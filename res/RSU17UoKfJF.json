{"notes": [{"id": "RSU17UoKfJF", "original": "FMeRkYMwi_i", "number": 1211, "cdate": 1601308135729, "ddate": null, "tcdate": 1601308135729, "tmdate": 1615500647471, "tddate": null, "forum": "RSU17UoKfJF", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "R-GAP: Recursive Gradient Attack on Privacy", "authorids": ["~Junyi_Zhu1", "~Matthew_B._Blaschko1"], "authors": ["Junyi Zhu", "Matthew B. Blaschko"], "keywords": ["privacy leakage from gradients", "federated learning", "collaborative learning"], "abstract": "Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP  works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network's security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|rgap_recursive_gradient_attack_on_privacy", "supplementary_material": "", "pdf": "/pdf/943e8621d1ff1fb8b9daa8cc68876c327966099c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021rgap,\ntitle={R-{\\{}GAP{\\}}: Recursive Gradient Attack on Privacy},\nauthor={Junyi Zhu and Matthew B. Blaschko},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RSU17UoKfJF}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "TuHGT5Iktm", "original": null, "number": 1, "cdate": 1610040394262, "ddate": null, "tcdate": 1610040394262, "tmdate": 1610473989111, "tddate": null, "forum": "RSU17UoKfJF", "replyto": "RSU17UoKfJF", "invitation": "ICLR.cc/2021/Conference/Paper1211/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The major criticism of this paper after the initial reviews was a lack of experimental results on deeper and more modern architectures that include skip connections.  The authors added results to the paper to address these issues."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "R-GAP: Recursive Gradient Attack on Privacy", "authorids": ["~Junyi_Zhu1", "~Matthew_B._Blaschko1"], "authors": ["Junyi Zhu", "Matthew B. Blaschko"], "keywords": ["privacy leakage from gradients", "federated learning", "collaborative learning"], "abstract": "Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP  works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network's security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|rgap_recursive_gradient_attack_on_privacy", "supplementary_material": "", "pdf": "/pdf/943e8621d1ff1fb8b9daa8cc68876c327966099c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021rgap,\ntitle={R-{\\{}GAP{\\}}: Recursive Gradient Attack on Privacy},\nauthor={Junyi Zhu and Matthew B. Blaschko},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RSU17UoKfJF}\n}"}, "tags": [], "invitation": {"reply": {"forum": "RSU17UoKfJF", "replyto": "RSU17UoKfJF", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040394249, "tmdate": 1610473989087, "id": "ICLR.cc/2021/Conference/Paper1211/-/Decision"}}}, {"id": "7zj1CqFpDEp", "original": null, "number": 3, "cdate": 1604802640497, "ddate": null, "tcdate": 1604802640497, "tmdate": 1607145915208, "tddate": null, "forum": "RSU17UoKfJF", "replyto": "RSU17UoKfJF", "invitation": "ICLR.cc/2021/Conference/Paper1211/-/Official_Review", "content": {"title": "Review", "review": "This paper proposes a new gradient attack method named R-GAP. R-GAP decomposites the DNN gradient attack problem into subproblems for each layer, and recursively solves each of them. The subproblem of each layer is formulated as a least-square reconstruction problem.\n\nThe authors further point out the rank of network weight matrix is (non-surprisingly) correlated with the difficulty of input recovery. Based on this finding, they design a metric based on the weight matrix rank to estimate the feasibility of fully recovering data.\nExperimental results on MNIST and CIFAR10 show that the proposed method R-GAP is comparable or superior to the classic DLG method. The authors also claimed the proposed method to be much faster than DLG baseline.\n\nStrengths:\n1. Gradient attacks raise data privacy concerns in many applications such as federated learning, making it an important problem.\n2. As an analytical method to solve the input reversion problem, R-GAP should have its intrinsic advantage over previous optimization-based gradient attack (O-GAP) methods such as DLG. For example, in my assumption (and also claimed by the authors), R-GAP can be much faster than O-GAP.\n3. R-GAP has much better performance than DLG on full-rank CNN6 networks, as shown both visually and numerically in Figure 3 and Table 1. This shows that if the attacked model satisfies the full-rank condition, R-GAP can be both faster and more effective than DLG. \n\nWeakness:\n1. Insufficient experiments.\nMy largest concern is over the lack of necessary experiments to show the advantage of R-GAP. \ni. Why only showing results on a self-designed CNN6 network? In order to fairly compare with DLG, and also to show the general effectiveness of the proposed method, the authors should also consider comparing with DLG on some standard network, such as the LeNet benchmarked in many previous gradient attack works [1,2].\nThis is very important also because we need to see whether the popular deep models such as LeNet, VGG, ResNet, etc. satisfy the full rank condition required by the proposed method. If not, the proposed R-GAP will have limited application scenarios. \nii. Why not compare with more recent gradient attack methods such as iDLG [2], which has been shown to also outperform the original DLG [1]?\niii. The authors claimed the proposed method is much faster than DLG. Although I agree this is intuitively true, I think it necessary to report the numbers in the paper. For example, how much time/FLOPs does it take to attack a single image for each method?\n\n2. Additional tricks used without detailed description in the Method section. \nIn Table 1, the authors show that R-GAP is largely outperformed by DLG on the rank-deficient network CNN6-d. However, according to the authors' vague descriptions, simply adding a smoothing operation can largely improve R-GAP performance. (I assume H-GAP = R-GAP + image smoothing?) Is the image smoothing the main technique making the proposed method effective?\nThis is really confusing since neither H-GAP nor image smoothing is mentioned in the method/related work sections. I suggest the authors to provide more descriptions about the H-GAP method and also provide explanations why it largely outperforms R-GAP.\n\nI'm willing to increase my score if these concerns are properly tackled during the rebuttal period.\n\nOther comments:\n1.  The artifacts of DLG reconstruction images are mainly located on the corner of the images (see Figure 2), while the artifacts of DLG are evenly distributed on the whole images (see Figure 3). Is this a general trend? If yes, any explanations or intuitions behind this?\n2. The proposed RA-i is only using matrix rank to predict the hardness of input recovery. In my view, it might be better to consider using matrix condition number as the metric. This is because the sub-problem at each layer is basically a least-square regression problem, and two least-square regression problems can have different difficulties when the matrix have identical ranks but different condition numbers. In other words, condition number contains more information than rank, and thus might be more useful. (Please point out if I'm wrong here.)\n\nReference\n\n[1] Deep leakage from gradients.\n\n[2] iDLG: Improved Deep Leakage from Gradients. \n\nUpdate: The authors have addressed my concerns and now I vote for acceptance.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1211/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1211/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "R-GAP: Recursive Gradient Attack on Privacy", "authorids": ["~Junyi_Zhu1", "~Matthew_B._Blaschko1"], "authors": ["Junyi Zhu", "Matthew B. Blaschko"], "keywords": ["privacy leakage from gradients", "federated learning", "collaborative learning"], "abstract": "Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP  works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network's security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|rgap_recursive_gradient_attack_on_privacy", "supplementary_material": "", "pdf": "/pdf/943e8621d1ff1fb8b9daa8cc68876c327966099c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021rgap,\ntitle={R-{\\{}GAP{\\}}: Recursive Gradient Attack on Privacy},\nauthor={Junyi Zhu and Matthew B. Blaschko},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RSU17UoKfJF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "RSU17UoKfJF", "replyto": "RSU17UoKfJF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1211/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124009, "tmdate": 1606915770977, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1211/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1211/-/Official_Review"}}}, {"id": "9DzFIJE_ERl", "original": null, "number": 1, "cdate": 1603853906627, "ddate": null, "tcdate": 1603853906627, "tmdate": 1606316000533, "tddate": null, "forum": "RSU17UoKfJF", "replyto": "RSU17UoKfJF", "invitation": "ICLR.cc/2021/Conference/Paper1211/-/Official_Review", "content": {"title": "This paper proposes a new gradient based attack which is able to reconstruct a data point when the gradient with respect to that data point is available", "review": "This paper studies the problem of gradient attack in deep learning models. In particular,  this paper tries to form a system of linear equations to find a training data point when the gradient of the deep learning model with respect to that data point is available. The algorithm for finding the data point is called R-GAP.\n\nStrengths: \n1. The idea of the paper is simple and interesting. The main idea is that for a deep learning model with $d$ layers, we can form $d$ systems of linear equations that can be solved recursively. Solving the last system of linear equations gives us the reconstructed data point. \n2. Moreover, because R-GAP is using systems of linear equations, it is very easy to know/analyze the feasibility of the R-GAP algorithm.\n3. In contrast to (Phong et al. (2018)), the proposed method works for CNNs as well. \n\nWeaknesses: \n1. The contribution of the paper seems limited. This paper extends the method of (Phong et al. (2018)). More precisely, (Phong et al. (2018)) forms a system of equations only with respect to the first layer. The current paper does the same thing for $d$ layers.\n\n2. This paper compares R-GAP with the DLG algorithm. However, there is an improved version of the DLG algorithm called iDLG (Zhao et al. (2020)). Therefore, more numerical examples are required to compare R-GAP with iDLG. \n3. This paper situates itself in the Federated Learning (FL) literature. However, in FL problems, each node reports the gradient with respect to the whole local training data points (not a single data point). Therefore, it is not realistic to assume that the central node or an adversary has access to the gradient with respect to one data point. I believe batch DLG (Zhu et al., 2019) is a better attack method in an FL setting. \n\nUpdate: I would like to increase the score to 7. The authors have improved the paper and addressed my concerns.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1211/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1211/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "R-GAP: Recursive Gradient Attack on Privacy", "authorids": ["~Junyi_Zhu1", "~Matthew_B._Blaschko1"], "authors": ["Junyi Zhu", "Matthew B. Blaschko"], "keywords": ["privacy leakage from gradients", "federated learning", "collaborative learning"], "abstract": "Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP  works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network's security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|rgap_recursive_gradient_attack_on_privacy", "supplementary_material": "", "pdf": "/pdf/943e8621d1ff1fb8b9daa8cc68876c327966099c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021rgap,\ntitle={R-{\\{}GAP{\\}}: Recursive Gradient Attack on Privacy},\nauthor={Junyi Zhu and Matthew B. Blaschko},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RSU17UoKfJF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "RSU17UoKfJF", "replyto": "RSU17UoKfJF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1211/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124009, "tmdate": 1606915770977, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1211/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1211/-/Official_Review"}}}, {"id": "MkcX25yB2Ch", "original": null, "number": 8, "cdate": 1606246007064, "ddate": null, "tcdate": 1606246007064, "tmdate": 1606246052396, "tddate": null, "forum": "RSU17UoKfJF", "replyto": "zYzc1F-S6Fg", "invitation": "ICLR.cc/2021/Conference/Paper1211/-/Official_Comment", "content": {"title": "Final revision", "comment": "We have uploaded our final revision, in this revision we:\n\n* insert some qualitative results to Figure 4.\n* add a new section Appendix E talking about improving the defendability of ResNet101 according to the rank analysis.\n\nWe would like to thank three reviewers for their work and valuable feedback."}, "signatures": ["ICLR.cc/2021/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1211/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "R-GAP: Recursive Gradient Attack on Privacy", "authorids": ["~Junyi_Zhu1", "~Matthew_B._Blaschko1"], "authors": ["Junyi Zhu", "Matthew B. Blaschko"], "keywords": ["privacy leakage from gradients", "federated learning", "collaborative learning"], "abstract": "Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP  works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network's security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|rgap_recursive_gradient_attack_on_privacy", "supplementary_material": "", "pdf": "/pdf/943e8621d1ff1fb8b9daa8cc68876c327966099c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021rgap,\ntitle={R-{\\{}GAP{\\}}: Recursive Gradient Attack on Privacy},\nauthor={Junyi Zhu and Matthew B. Blaschko},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RSU17UoKfJF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RSU17UoKfJF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1211/Authors|ICLR.cc/2021/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862343, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1211/-/Official_Comment"}}}, {"id": "zYzc1F-S6Fg", "original": null, "number": 7, "cdate": 1605641296863, "ddate": null, "tcdate": 1605641296863, "tmdate": 1605642391776, "tddate": null, "forum": "RSU17UoKfJF", "replyto": "RSU17UoKfJF", "invitation": "ICLR.cc/2021/Conference/Paper1211/-/Official_Comment", "content": {"title": "Revision and generic comment", "comment": "We thank the reviewers for their helpful comments and insightful reviews. With your feedback, we are able to provide better work.\n\nIn this revision we have added:\n* an experiment comparing R-GAP with DLG over LeNet. We further provide some insights of the instability of DLG through the condition number of constraints matrix A.\n* a section Appendix D about rank analysis of the skip connection. We analytically prove and empirically demonstrate that the skip connection can make rank-deficient bottle-neck again full rank. \n* an experiment about improving the security of ResNet18 using rank analysis. More importantly, we demonstrate that we can improve the defendability towards gradient attacks without sacrificing accuracy.\n\nWe note that in our response to AnonReviewer4, we referred to Figure 7 (old numbering).  In the revision, this now refers to Figure 8 (new numbering).\n\nWe have also edited:\n\n* section of the rank analysis, to make the explanation of virtual constraints more clear.\n* the abstract, introduction and results, conclusion to emphasize that the focus of this work include not only the attack approach but also a risk estimation tool and other theoretical understanding of gradient attacks."}, "signatures": ["ICLR.cc/2021/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1211/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "R-GAP: Recursive Gradient Attack on Privacy", "authorids": ["~Junyi_Zhu1", "~Matthew_B._Blaschko1"], "authors": ["Junyi Zhu", "Matthew B. Blaschko"], "keywords": ["privacy leakage from gradients", "federated learning", "collaborative learning"], "abstract": "Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP  works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network's security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|rgap_recursive_gradient_attack_on_privacy", "supplementary_material": "", "pdf": "/pdf/943e8621d1ff1fb8b9daa8cc68876c327966099c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021rgap,\ntitle={R-{\\{}GAP{\\}}: Recursive Gradient Attack on Privacy},\nauthor={Junyi Zhu and Matthew B. Blaschko},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RSU17UoKfJF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RSU17UoKfJF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1211/Authors|ICLR.cc/2021/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862343, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1211/-/Official_Comment"}}}, {"id": "KcRerV7lH3Q", "original": null, "number": 6, "cdate": 1605225008536, "ddate": null, "tcdate": 1605225008536, "tmdate": 1605225008536, "tddate": null, "forum": "RSU17UoKfJF", "replyto": "9DzFIJE_ERl", "invitation": "ICLR.cc/2021/Conference/Paper1211/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "1. R-GAP is far from a straightforward generalization of the bias attack (Phong et al., 2018) to $d$ layers. The bias attack works only on a fully connected network with bias terms. Therefore, understanding the bias attack cannot help us understand a gradient attack on a convolutional network. R-GAP utilizes much more information and is the first closed-form algorithm that works on both convolutional networks and fully connected networks with or without bias term. Through the study of R-GAP, we have provided many insights into gradient attacks, e.g. the architecture rather than the number of parameters is critical to gradient attacks, why DLG is sensitive to the initialization, why gradient attacks are robust to zero mean noise added to the gradients (this noise is averaged out over redundant estimates of the data either explicitly in the case of R-GAP or implicitly in the case of DLG).\n\n2. This point was also raised by AnonReviewer3. iDLG proposed a way to analytically derive the label rather than let DLG jointly reconstruct the label and data, which increases the stability of DLG. In our work, as the label is analytically retrievable, we always optimize DLG with the label fixed and let it recover only the image. Thus all of our comparisons are to iDLG and not to the original DLG approach. We will clarify this in a revision of the paper.\n\n3. In the batch setting, R-GAP in unmodified format returns a linear combination of training data.  This is already a violation of privacy.  A closed-form attack that recovers individual batch samples is an obvious target for future work, and will provide additional insights into the performance of DLG and in which cases recovery of individual samples is feasible.  We empirically observe that DLG has a tendency to mix samples in the batch setting (Figure 7), and it can be of interest to understand what causes this behavior. We do not claim that R-GAP beats DLG in all settings. DLG is a good optimization-based method but with comparatively limited theoretical insights. Through R-GAP we are able to provide many insights about gradient attacks, including DLG. We regard our work as a theory-oriented study, focused on providing insights into how gradient attacks work so that more secure federated learning algorithms can be developed."}, "signatures": ["ICLR.cc/2021/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1211/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "R-GAP: Recursive Gradient Attack on Privacy", "authorids": ["~Junyi_Zhu1", "~Matthew_B._Blaschko1"], "authors": ["Junyi Zhu", "Matthew B. Blaschko"], "keywords": ["privacy leakage from gradients", "federated learning", "collaborative learning"], "abstract": "Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP  works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network's security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|rgap_recursive_gradient_attack_on_privacy", "supplementary_material": "", "pdf": "/pdf/943e8621d1ff1fb8b9daa8cc68876c327966099c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021rgap,\ntitle={R-{\\{}GAP{\\}}: Recursive Gradient Attack on Privacy},\nauthor={Junyi Zhu and Matthew B. Blaschko},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RSU17UoKfJF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RSU17UoKfJF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1211/Authors|ICLR.cc/2021/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862343, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1211/-/Official_Comment"}}}, {"id": "axPCmeqxqQm", "original": null, "number": 3, "cdate": 1605221922793, "ddate": null, "tcdate": 1605221922793, "tmdate": 1605223888674, "tddate": null, "forum": "RSU17UoKfJF", "replyto": "7zj1CqFpDEp", "invitation": "ICLR.cc/2021/Conference/Paper1211/-/Official_Comment", "content": {"title": "Response to AnonReviewer3 comment 1", "comment": "W1: (i) The rank constraints are not only relevant to R-GAP, but to any gradient attack method.  Our rank analysis also predicts when the *optimization attack* (DLG) fails. We have identified three types of constraints in the gradient attack problem: gradient constraints, weight constraints and virtual constraints. The first two constraints are linear and can be easily recursively defined, and therefore have been taken into account by R-GAP, which regards the attack as a sequence of linear systems. Whereas, DLG is an optimization-based attack which implicitly takes all of them into account. However, R-GAP is a closed-form algorithm which is amenable to analysis and is not susceptible to local optima. As such, we should not view R-GAP as a direct competitor to DLG.  Indeed, we may consider a possible application of R-GAP as an initialization of DLG (see additional experiment in the next paragraph), although in our experiments R-GAP usually produces results that break privacy by itself.\n\nLeNet also satisfies the rank conditions for full recovery of the training data with R-GAP.  Our code release will include the calculation of the rank conditions, and we will include additional analysis for popular network architectures in the final version of the paper. The following table summarizes the results of an additional experiment on the LeNet architecture (evaluated on CIFAR-10).\n\nCondition Number:\n\nNetwork|      conv1        |       conv2       |        conv3     |\n\nLeNet | $1.8\\times 10^4 \\pm 29.1$            | $6.1 \\times 10^3 \\pm 3.1$             | $32.4 \\pm 2.9 \\times 10^{-3}$ |\n\nLeNet*| $1.2\\times 10^3 \\pm 2.0\\times 10^2$ | $1.3\\times 10^3 \\pm 2.3\\times 10^2$ | $14.2 \\pm 0.45$             |\n\nMSE:\n\nNetwork|         DLG         |       R-GAP      | R-GAP-> DLG |\n\nLeNet  |   $3.7\\times 10^{-8} \\pm 8.6\\times 10^{-9}$  |  $1.1\\times 10^{-4} \\pm 7.8 \\times 10^{-5}$  |   $1.1 \\times10^{-6} \\pm1.1 \\times 10^{-5}$  |\n\nLeNet*|   $5.2\\times 10^{-2}\\pm 2.9\\times10^{-2}$  | $1.5\\times 10^{-10}\\pm2.5\\times10^{-10}$ |    $4.8\\times10^{-4}\\pm 9.1 \\times 10^{-4}$  |\n\nLeNet* is identical to LeNet but uses a Leaky ReLU activation function instead of Sigmoid\n\n\nBoth DLG and R-GAP perform well on LeNet. Empirically, if the MSE is around or below $1\\times10^{-4}$, the difference of the reconstruction will be visually undetectable. However, we surprisingly find that by replacing the Sigmoid function with the Leaky ReLU, the reconstruction of DLG becomes much poorer. The condition number of the matrix $\\textbf{A}$ (from Algorithm 1 in the paper) changes significantly in this case. Since the Sigmoid function leads to a higher condition number at each layer, any reconstruction error in the subsequent layer will be amplified in the previous layer, therefore DLG is forced to converge to a better result. In contrast, R-GAP has an accumulated error and naturally performs much better on LeNet*. Additionally, we find R-GAP could be a good initiliazation tool for DLG. By initiliazing DLG with the reconstruction of R-GAP, and running 8\\% of the previous iterations, we achieve a visually indistinguishable result.  However, for LeNet*, we find that DLG reduces the reconstruction quality obtained by R-GAP, which further shows the instability of DLG.\n\n(ii) \"Improved Deep Leakage from Gradients\" (iDLG) proposed a way to analytically derive the label rather than let DLG jointly reconstruct the label and data. In our work, as the label can be analytically recovered, we are always providing DLG the ground-truth label and let it recover the image only. Therefore the experiment is actually comparing R-GAP with iDLG, we will add a note explaining this to the paper.\n\n(iii) The cost of running R-GAP is approximately the same order of magnitude as a single iteration of DLG.  A step of DLG is dominated by computing the gradient of the gradient w.r.t. the function input, while R-GAP is dominated by the matrix inversions at each step (cubic in the number of rows).  We note that both methods can run on a GPU.  The difference in run-time between the methods is orders of magnitude (not even close), and is dominated by the large number of optimization iterations required by DLG (typically hundreds as reported in their paper).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1211/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "R-GAP: Recursive Gradient Attack on Privacy", "authorids": ["~Junyi_Zhu1", "~Matthew_B._Blaschko1"], "authors": ["Junyi Zhu", "Matthew B. Blaschko"], "keywords": ["privacy leakage from gradients", "federated learning", "collaborative learning"], "abstract": "Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP  works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network's security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|rgap_recursive_gradient_attack_on_privacy", "supplementary_material": "", "pdf": "/pdf/943e8621d1ff1fb8b9daa8cc68876c327966099c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021rgap,\ntitle={R-{\\{}GAP{\\}}: Recursive Gradient Attack on Privacy},\nauthor={Junyi Zhu and Matthew B. Blaschko},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RSU17UoKfJF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RSU17UoKfJF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1211/Authors|ICLR.cc/2021/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862343, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1211/-/Official_Comment"}}}, {"id": "PDYMJOkEvDP", "original": null, "number": 5, "cdate": 1605223694862, "ddate": null, "tcdate": 1605223694862, "tmdate": 1605223694862, "tddate": null, "forum": "RSU17UoKfJF", "replyto": "C2-I6FaiyEc", "invitation": "ICLR.cc/2021/Conference/Paper1211/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "1. Eq. (10) shows that if the activation function is LeakyReLU or ReLU we can average different estimates of $\\frac{\\partial \\ell}{\\partial \\mu}\\mu$ computed at different layers in order to obtain a more robust estimate. If not, we can still use the result of the last layer as shown in Eq. (9).\n\n2. We will revise our explanation of the virtual constraints in the revision to increase clarity. When the activation function becomes non-linear, so do the virtual constraints. For a strictly monotonic activation function (which holds for most common choices), the virtual constraints can be expressed as $\\mathcal{V}\\sigma^{-1}(\\textbf{x}) = 0$.  We can still count the number of such constraints to provide an estimate of the data vulnerability.\n\n3. This is a very good question. The residual connection has some interesting traits in terms of the rank deficiency: if the section of the network it skips over is rank deficient, the resulting network can still be full rank and enable full recovery of the data. We will add a new subsection in the appendix discussing this, and we have already performed experiments showing the effect.\n\n4. See also the response to AnonReviewer3 and additional experiment.  Geiping et al. (2020) report that network depth does not tend to protect privacy in the context of DLG.  Although there may be some accumulation of error over the layers with R-GAP, this tends to be minimal for common choices of activation function.  For less stable activation functions (e.g. Sigmoid), we still get a recognizable reconstruction therefore breaking privacy.\n\n5. The variable names are consistent with preceding equations (9)-(16).  We will note the correspondence in the caption of the image in the revised version.\n\n6. See response to AnonReviewer3 regarding additional architectures."}, "signatures": ["ICLR.cc/2021/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1211/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "R-GAP: Recursive Gradient Attack on Privacy", "authorids": ["~Junyi_Zhu1", "~Matthew_B._Blaschko1"], "authors": ["Junyi Zhu", "Matthew B. Blaschko"], "keywords": ["privacy leakage from gradients", "federated learning", "collaborative learning"], "abstract": "Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP  works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network's security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|rgap_recursive_gradient_attack_on_privacy", "supplementary_material": "", "pdf": "/pdf/943e8621d1ff1fb8b9daa8cc68876c327966099c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021rgap,\ntitle={R-{\\{}GAP{\\}}: Recursive Gradient Attack on Privacy},\nauthor={Junyi Zhu and Matthew B. Blaschko},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RSU17UoKfJF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RSU17UoKfJF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1211/Authors|ICLR.cc/2021/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862343, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1211/-/Official_Comment"}}}, {"id": "_TntsNxoTDP", "original": null, "number": 4, "cdate": 1605222001168, "ddate": null, "tcdate": 1605222001168, "tmdate": 1605222239396, "tddate": null, "forum": "RSU17UoKfJF", "replyto": "axPCmeqxqQm", "invitation": "ICLR.cc/2021/Conference/Paper1211/-/Official_Comment", "content": {"title": "Response to AnonReviewer3 part 2", "comment": "* W2: H-GAP is simply a hybrid attack that selects the better output of either R-GAP or DLG.  The selection is done by rejecting the solution that has more salt-and-pepper type noise.  We measure this by the difference of the image and its smoothed version (achieved by a simple 3x3 averaging) and select the output with the smaller norm.  In this way, we can always select the better of the two outputs by taking into account a minimal amount of domain knowledge.  We note that other recent work has extended DLG to incorporate image priors, and our approach is a simple step in this vein.  We will improve the description in the paper.\n\n* C1: Figure 2 shows reconstruction artifacts when the network is rank deficient, while Figure 3 shows artifacts caused by convergence to a local minimum in the optimization process. For the rank deficient case, the convolution kernel performs less computation at the edges and corners, and therefore that is where the artifacts are located.\n\n* C2: We have included an analysis of the condition number in the above experiment.  We note that the condition number is data and network dependent. By contrast, rank analysis is an offline tool, which can be useful to understand risks inherent in certain network architectures.  Ultimately, it is not our goal to attack networks, but to show via these attacks when data are vulnerable (regardless of whether the optimization is hard or easy).  Our goal is also to provide a basis for analysis that will ultimately lead to a better understanding of how to design data-secure federated learning systems.\n\nBack to the goal of this work, DLG was published in NeurIPS 2019 last December.  Although it has received a bit less than 100 citations in less than a year, until now there is little theoretical understanding of it. Through our study, we found that a gradient attack is similar to solving a chain of linear equations and proposed the R-GAP based on this idea. R-GAP is more stable and consumes less time than the optimization-based attack, but does not take the non-linear constraints into account. However, R-GAP is a closed-form algorithm and easy to study. Therefore, we are able to provide insights on many important open questions of DLG, e.g. why DLG is sensitive to its initialization, local optima due to twin-data, how to estimate the risk of gradient attacks based on a network architecture, etc. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1211/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1211/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "R-GAP: Recursive Gradient Attack on Privacy", "authorids": ["~Junyi_Zhu1", "~Matthew_B._Blaschko1"], "authors": ["Junyi Zhu", "Matthew B. Blaschko"], "keywords": ["privacy leakage from gradients", "federated learning", "collaborative learning"], "abstract": "Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP  works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network's security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|rgap_recursive_gradient_attack_on_privacy", "supplementary_material": "", "pdf": "/pdf/943e8621d1ff1fb8b9daa8cc68876c327966099c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021rgap,\ntitle={R-{\\{}GAP{\\}}: Recursive Gradient Attack on Privacy},\nauthor={Junyi Zhu and Matthew B. Blaschko},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RSU17UoKfJF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "RSU17UoKfJF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1211/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1211/Authors|ICLR.cc/2021/Conference/Paper1211/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1211/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862343, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1211/-/Official_Comment"}}}, {"id": "C2-I6FaiyEc", "original": null, "number": 2, "cdate": 1603950802267, "ddate": null, "tcdate": 1603950802267, "tmdate": 1605024501528, "tddate": null, "forum": "RSU17UoKfJF", "replyto": "RSU17UoKfJF", "invitation": "ICLR.cc/2021/Conference/Paper1211/-/Official_Review", "content": {"title": "This paper addresses a distributed and federated learning based gradient attack techniques, which explains how gradients can lead to recovery of original data.", "review": "The authors proposed a theoretical explanation of the gradient attack on privacy. To be specific, a closed-form approach, namely recursive gradient attack on privacy has been proposed to explain how the gradients can be used to recover the original data. Moreover, the authors also presented a rank analysis method that is used to estimate the risk of being gradient attacked. Overall, this paper is easy to follow and well-written. I have a few concerns as follows:\n1. In Eqn.10, the authors assume the activation function is ReLU or LeakyReLU. In that case, the recursive form will be held. I guess whether the limitation for this method would be the activation function has to be ReLU or LeakyReLU? For instance, sigmoid and tanh activation functions are also widely used. Whether the proposed method is able to address those? If not, what would the reconstruction errors be.\n\n2. I am a little bit confused by the equations (19) and (20). When the activation functions are non-linear, how the authors define V? Also, how does V affect the layer rank? Maybe the conclusion below equation (20) is not very straightforward to me. \n\n3. As for the networks that used in this paper, I wonder when residual connections are used, how the rank deficiency problem would be?\n\n4. The follow-up question is when the network is very deep, like DenseNet and ResNet, whether the conclusions of this paper will hold? \n\n5. The symbols in Algorithm 1 should be consistent to the context. More importantly, I would suggest the authors highlight the correspondences between the equation and the algorithm diagram.\n\n6. Practical usage is one of my concern. In practice, researchers or engineers may use very deep models for practical problems. If the authors could demonstrate a more practical model, that would make this submission very strong. ", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1211/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1211/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "R-GAP: Recursive Gradient Attack on Privacy", "authorids": ["~Junyi_Zhu1", "~Matthew_B._Blaschko1"], "authors": ["Junyi Zhu", "Matthew B. Blaschko"], "keywords": ["privacy leakage from gradients", "federated learning", "collaborative learning"], "abstract": "Federated learning frameworks have been regarded as a promising approach to break the dilemma between demands on privacy and the promise of learning from large collections of distributed data. Many such frameworks only ask collaborators to share their local update of a common model, i.e. gradients with respect to locally stored data, instead of exposing their raw data to other collaborators. However, recent optimization-based gradient attacks show that raw data can often be accurately recovered from gradients. It has been shown that minimizing the Euclidean distance between true gradients and those calculated from estimated data is often effective in fully recovering private data. However, there is a fundamental lack of theoretical understanding of how and when gradients can lead to unique recovery of original data. Our research fills this gap by providing a closed-form recursive procedure to recover data from gradients in deep neural networks. We name it Recursive Gradient Attack on Privacy (R-GAP). Experimental results demonstrate that R-GAP  works as well as or even better than optimization-based approaches at a fraction of the computation under certain conditions. Additionally, we propose a Rank Analysis method, which can be used to estimate the risk of gradient attacks inherent in certain network architectures, regardless of whether an optimization-based or closed-form-recursive attack is used. Experimental results demonstrate the utility of the rank analysis towards improving the network's security. Source code is available for download from https://github.com/JunyiZhu-AI/R-GAP.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|rgap_recursive_gradient_attack_on_privacy", "supplementary_material": "", "pdf": "/pdf/943e8621d1ff1fb8b9daa8cc68876c327966099c.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhu2021rgap,\ntitle={R-{\\{}GAP{\\}}: Recursive Gradient Attack on Privacy},\nauthor={Junyi Zhu and Matthew B. Blaschko},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=RSU17UoKfJF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "RSU17UoKfJF", "replyto": "RSU17UoKfJF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1211/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538124009, "tmdate": 1606915770977, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1211/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1211/-/Official_Review"}}}], "count": 11}