{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1393889580000, "tcdate": 1393889580000, "number": 1, "id": "wg4jUf9Kjiw0E", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "NR4KjDE0w9RXD", "replyto": "FWcRWk_B136Of", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "My co-author David Warde-Farley chose the final hyperparameters for CIFAR-10. Our goal was simply to improve upon the state of the art with the limited computational resources that were available to us, so we did not run an exhaustive, automated search. Instead, both David and I guessed a small number of hyperparameter settings by hand, and we stopped working on that particular task after we started to get diminishing marginal utility from our time spent on it. The hyperparameters for the case with no data augmentation are probably particularly poor. We just used the best hyperparameters from the data augmentation case.\r\n\r\nI think if you want to do an explicit comparison between two methods, like maxout and probabilistic maxout, it's best to do an automated search, like we did for our comparison between maxout and rectifiers. Unfortunately, when we did this automated search, we had to do it for a small maxout model, since we wanted to compare maxout against a significantly larger rectifier model."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Deep Neural Networks with Probabilistic Maxout Units", "decision": "submitted, no decision", "abstract": "We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN).", "pdf": "https://arxiv.org/abs/1312.6116", "paperhash": "springenberg|improving_deep_neural_networks_with_probabilistic_maxout_units", "keywords": [], "conflicts": [], "authors": ["Jost Tobias Springenberg", "Martin Riedmiller"], "authorids": ["springj@informatik.uni-freiburg.de", "martin.riedmiller@deepmind.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1393552740000, "tcdate": 1393552740000, "number": 6, "id": "FWcRWk_B136Of", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "NR4KjDE0w9RXD", "replyto": "NR4KjDE0w9RXD", "signatures": ["Jost Tobias Springenberg"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "As mentioned in a previous comment I want to report back with\r\npreliminary results from the hyperparameter search I am conducting. \r\nEssentially I am optimizing over all notable parameters except the\r\nnumber of units in each layer - in order to fix the model size \r\n(More specifically this includes: learning rate,\r\nmomentum, pooling shape, pooling stride, size of the convolutional\r\nkernels). \r\n\r\nMy preliminary results suggest that on CIFAR-10 the best probout\r\nnetwork trained without data augmentation can achieve an error rate of <= 10.7% .\r\nDuring the hyperparameter search I however also became aware that the\r\nparameter search carried out for the original maxout paper was\r\nprobably  not very exhaustive, as improved performance can also\r\nbe achieved with a vanilla maxout model. \r\nWhile the difference in performance between probout and maxout appears\r\nto be observable for all hyperparameter settings, the best maxout model \r\nI obtained so far achieves 10.92% error on CIFAR-10 without data\r\naugmentation.\r\n\r\nInterestingly, the best hyperparameter settings found so far\r\nare much closer to the parameters that seem to be used in the 'Network\r\nin Network' paper (also submitted to ICLR) than the original settings\r\nused in the maxout paper (and maxout seems to perform on par\r\nwith 'Network in Network' when a fully connected layer is used after the convolutional\r\nlayers). I will cross-link these results in the discussion to that paper. \r\n\r\nI will include the results into the paper as soon as the full\r\nparameter search is finished and disclose the parameters found.\r\n\r\nIt would be interesting if Ian could jump in with a short comment on\r\nhow exactly he fixed hyperparameters for his original work."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Deep Neural Networks with Probabilistic Maxout Units", "decision": "submitted, no decision", "abstract": "We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN).", "pdf": "https://arxiv.org/abs/1312.6116", "paperhash": "springenberg|improving_deep_neural_networks_with_probabilistic_maxout_units", "keywords": [], "conflicts": [], "authors": ["Jost Tobias Springenberg", "Martin Riedmiller"], "authorids": ["springj@informatik.uni-freiburg.de", "martin.riedmiller@deepmind.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1393514100000, "tcdate": 1393514100000, "number": 10, "id": "D7M6i8b56ziN8", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "NR4KjDE0w9RXD", "replyto": "NR4KjDE0w9RXD", "signatures": ["Jost Tobias Springenberg"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "'For figure 4, I think it's important to note that your evaluation\r\nmethod only returns a low number if *all* of the units in a layer are\r\ninvariant to the studied transformation.'\r\n\r\nI agree and will add a sentence to the paper noting this.\r\nI actually thought about whether there is a better way to produce such\r\nplots than what was done in prior work  as well. Your suggestions\r\nseems solid and would be a nice addition to the whole layer analysis\r\nthat is depicted in Figure 4. I will try to get around to implementing it\r\nthis way (although this might have to wait a few days)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Deep Neural Networks with Probabilistic Maxout Units", "decision": "submitted, no decision", "abstract": "We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN).", "pdf": "https://arxiv.org/abs/1312.6116", "paperhash": "springenberg|improving_deep_neural_networks_with_probabilistic_maxout_units", "keywords": [], "conflicts": [], "authors": ["Jost Tobias Springenberg", "Martin Riedmiller"], "authorids": ["springj@informatik.uni-freiburg.de", "martin.riedmiller@deepmind.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1393513260000, "tcdate": 1393513260000, "number": 1, "id": "ttg76KBxN16Pj", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "NR4KjDE0w9RXD", "replyto": "wH1_HGT0Enlq-", "signatures": ["Jost Tobias Springenberg"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Hi Ian,\r\nthe maxout baseline is the maxout network without any sampling, i.e. a\r\nsimple forward pass through the net.\r\n\r\nEquation 3 was used for the two other curves (maxout + sampling,\r\nprobout) for both networks the dropout effect was removed by the\r\n`halving the weights trick`. I agree that it is not surprising that\r\nmaxout performs worse in combination with the sampling procedure as it\r\nwas never trained to compensate for/utilize the stochastic sampling\r\nprocedure. This experiment was meant as a simple control experiment and not\r\nmuch more. I did not mean to convey the idea that the result is\r\nsurprising or is a disatvantage of the maxout model. \r\n\r\nThe idea of also investigating different dropout masks was not really\r\nthe scope of this experiment, however it is a good idea and I will try\r\nto setup such an additional experiment. \r\n\r\nBtw, I will later today also report back with a few results from my\r\nlarge hyperparameter search which are somewhat interesting. I would\r\nbe very happy if you could comment on them as well."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Deep Neural Networks with Probabilistic Maxout Units", "decision": "submitted, no decision", "abstract": "We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN).", "pdf": "https://arxiv.org/abs/1312.6116", "paperhash": "springenberg|improving_deep_neural_networks_with_probabilistic_maxout_units", "keywords": [], "conflicts": [], "authors": ["Jost Tobias Springenberg", "Martin Riedmiller"], "authorids": ["springj@informatik.uni-freiburg.de", "martin.riedmiller@deepmind.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1393474680000, "tcdate": 1393474680000, "number": 9, "id": "gu83dns9QXd1k", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "NR4KjDE0w9RXD", "replyto": "NR4KjDE0w9RXD", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "For figure 4, I think it's important to note that your evaluation method only returns a low number if *all* of the units in a layer are invariant to the studied transformation. If the layer has a factored representation that represents the studied transformation with one set of units and represents other properties of the input with another, disjoint set of units, there will still be a large cosine difference between the representation of two transformed versions of the input, because the change in the portion of the representation that corresponds to the studied property will result in a change of the normalization constant and thus a change in all of the code elements. I think it would make more sense to normalize each unit separately based on a statistic such as its standard deviation across the training set, and then plot histograms showing how much each normalized unit changes as you vary the input. This way you still control for the possibility that the models operate on different scales, but you can also tell if the representation is factored or not."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Deep Neural Networks with Probabilistic Maxout Units", "decision": "submitted, no decision", "abstract": "We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN).", "pdf": "https://arxiv.org/abs/1312.6116", "paperhash": "springenberg|improving_deep_neural_networks_with_probabilistic_maxout_units", "keywords": [], "conflicts": [], "authors": ["Jost Tobias Springenberg", "Martin Riedmiller"], "authorids": ["springj@informatik.uni-freiburg.de", "martin.riedmiller@deepmind.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1393474020000, "tcdate": 1393474020000, "number": 8, "id": "wH1_HGT0Enlq-", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "NR4KjDE0w9RXD", "replyto": "NR4KjDE0w9RXD", "signatures": ["Ian Goodfellow"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "For fig 3b, what sampling distribution did you use for the maxout baseline? Are you sampling from the distribution defined in eqn 3? Why is this a meaningful thing to evaluate for maxout, which hasn't been trained to know you're going to sample in that particular way? It seems like it would be more fair to sample different dropout masks, since each of the maxout subnetworks have actually been trained to do the classification task. It's not very surprising to find that a neural net that has been trained to do task X performs better than a neural net that has not been trained to do task X."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Deep Neural Networks with Probabilistic Maxout Units", "decision": "submitted, no decision", "abstract": "We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN).", "pdf": "https://arxiv.org/abs/1312.6116", "paperhash": "springenberg|improving_deep_neural_networks_with_probabilistic_maxout_units", "keywords": [], "conflicts": [], "authors": ["Jost Tobias Springenberg", "Martin Riedmiller"], "authorids": ["springj@informatik.uni-freiburg.de", "martin.riedmiller@deepmind.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392997560000, "tcdate": 1392997560000, "number": 1, "id": "EEYZlvk8Zyz5h", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "NR4KjDE0w9RXD", "replyto": "NR4KjDE0w9RXD", "signatures": ["Jost Tobias Springenberg"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We want to point out that an updated version of the paper is available on arXiv. The main changes are:\r\n- Most minor comments of reviewers Anonymous f3f1 and Anonymous 4eb4 are adressed in the new version\r\n- The experiments section has been changed to more clearly reflect statistically ties between maxout and probout in the results\r\n- An additional experiment on invariance properties was added"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Deep Neural Networks with Probabilistic Maxout Units", "decision": "submitted, no decision", "abstract": "We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN).", "pdf": "https://arxiv.org/abs/1312.6116", "paperhash": "springenberg|improving_deep_neural_networks_with_probabilistic_maxout_units", "keywords": [], "conflicts": [], "authors": ["Jost Tobias Springenberg", "Martin Riedmiller"], "authorids": ["springj@informatik.uni-freiburg.de", "martin.riedmiller@deepmind.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392859380000, "tcdate": 1392859380000, "number": 2, "id": "aaq01-SNy58mX", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "NR4KjDE0w9RXD", "replyto": "NR4KjDE0w9RXD", "signatures": ["anonymous reviewer 2618"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Improving Deep Neural Networks with Probabilistic Maxout Units", "review": "Authors propose replacing max operation of maxout with probabilistic version, same what Zeiler did for spatial max-pooling in 'stochastic pooling' paper. \r\n\r\nFor inference, they run the network 50 times using the same sampling procedure, and average the outputs to get probabilities.\r\n\r\nThey add a 'temperature' parameter to allow to interpolate between maxout and 'uniform random' sampling, and find that annealing the temperature helps.\r\n\r\nAlso the analyze the per-layer optimal setting of the temperature and found that stochasticity is most important in first 2 convolutional layers, whereas in the last 2 layers, using probout did not give any advantage over maxout.\r\n\r\nThere are a few minor corrections, but overall this is a solid submission with high relevance for ICLR.\r\n\r\nIssues:\r\n\r\n- Minor style issue: use \text or mbox for 'softmax' and 'multinomial' inside formula\r\n\r\n- Table 3: could add 2.16% error rate obtained by another ICLR submission -- 'Multi-digit Number Recognition from Street View\r\nImagery using Deep Convolutional Neural Networks'\r\n\r\n- There's an explanation for why first layers benefit the most from stochasticity -- how stochasticity 'pulls the units in the subspace closer together.' This is unclear to me, and I would recommend expanding/explaining this view."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Deep Neural Networks with Probabilistic Maxout Units", "decision": "submitted, no decision", "abstract": "We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN).", "pdf": "https://arxiv.org/abs/1312.6116", "paperhash": "springenberg|improving_deep_neural_networks_with_probabilistic_maxout_units", "keywords": [], "conflicts": [], "authors": ["Jost Tobias Springenberg", "Martin Riedmiller"], "authorids": ["springj@informatik.uni-freiburg.de", "martin.riedmiller@deepmind.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392738660000, "tcdate": 1392738660000, "number": 5, "id": "SSRsSpe-wg_G6", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "NR4KjDE0w9RXD", "replyto": "NR4KjDE0w9RXD", "signatures": ["Jost Tobias Springenberg"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "First of all we want to thank both reviewers for their detailed comments. \r\nBefore giving a more elaborate response we want to mention that we have incorporated your suggestions in  a new version of the paper which should appear on arxiv as of Feb 19.\r\n\r\nTo both reviewers:\r\nAs already acknowledged in the paper we agree that our proposed method comes with an attached high computational cost during inference. While this cost, as you mentioned, can be seen as an important practical drawback we believe that explorative research towards novel stochastic regularization techniques (for which an efficient inference procedure is not immediately available) still constitutes a worthwhile research direction from which new, more efficient, regularization techniques could be obtained in the future.\r\n\r\nIn addition to this increase in computational cost you point out that the improvement over maxout achieved by our approach is minor/non-significant. Although this is acknowledged at the end of the introduction and in the discussion, we have now additionally rephrased the experiments section to reflect this fact more clearly. We want to reiterate that since both methods are tightly coupled - in both their motivation and computational properties as well as our parameter choices - we believe that our results still provide an interesting contribution and can serve as a starting point for future research on the impact of including subspace pooling in deep neural networks. \r\n\r\nTo reviewer 2 (Anonymous 4eb4):\r\n\r\nWe agree that the best hyperparameters for both maxout and probout cannot generally be assumed to coincide. However, as a full hyperparameter search for both methods on all datasets requires significant computational resources we decided to stick to the original parameters in an attempt to make a comparison that is at worst biased towards the maxout results (as we were concerned to not make an unfair comparison to previous results). We are currently running a parameter search on both CIFAR-10 and SVHN in a similar manner as you suggested and will include the results in an additional updated version in the coming days. \r\n\r\nAs you point out, our investigation of the invariance properties of the network in the original paper is only a qualitative one. We performed an additional quantitative analysis in the new version of the paper, comparing invariance properties of maxout and probout networks in a manner similar to [1,2].\r\n\r\n[1] Koray Kavukcuoglu, Marc'Aurelio Ranzato, Rob Fergus, and Yann LeCun, 'Learning Invariant Features through Topographic Filter Maps', in Proc. International Conference on Computer Vision and Pattern Recognition (CVPR'09), 2009.\r\n[2] Visualizing and Understanding Convolutional Networks M.D. Zeiler, R. Fergus Arxiv 1311.2901 (Nov 28, 2013)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Deep Neural Networks with Probabilistic Maxout Units", "decision": "submitted, no decision", "abstract": "We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN).", "pdf": "https://arxiv.org/abs/1312.6116", "paperhash": "springenberg|improving_deep_neural_networks_with_probabilistic_maxout_units", "keywords": [], "conflicts": [], "authors": ["Jost Tobias Springenberg", "Martin Riedmiller"], "authorids": ["springj@informatik.uni-freiburg.de", "martin.riedmiller@deepmind.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392035940000, "tcdate": 1392035940000, "number": 4, "id": "QTz-TsRb-_Qds", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "NR4KjDE0w9RXD", "replyto": "NR4KjDE0w9RXD", "signatures": ["anonymous reviewer 4eb4"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Improving Deep Neural Networks with Probabilistic Maxout Units", "review": "This manuscript extends the recently proposed \u201cmaxout\u201d scheme for neural networks by making the linear subspace pooling stochastic, i.e. by parameterizing the probability of activating each filter as the softmax of the filter responses and then sampling from the resulting discrete distribution. Experimental results are presented on CIFAR10/CIFAR100/SVHN and contrasted with the original maxout work.\r\n\r\nNovelty: low\r\nQuality: low to medium\r\n\r\nPros:\r\n- The idea is somewhat interesting and worth trying, and the manuscript itself is generally well-written\r\n- Experiments and hyperparameter choices are generally described in detail, including software packages used\r\n- Attempts a fair-handed comparison between the method and the one they are building off (though see below)\r\n\r\nCons:\r\n- The benchmark results are quite lackluster: the improvements are of questionable statistical significance (see below)\r\n- This questionable gain comes at a >=10x increase in computational cost. \r\n- The experimental comparisons have several shortcomings (not all of them strictly advantageous to the proposed method, however -- see below).\r\n- The abstract mentions enhancing invariance properties as the goal but there are no attempts to quantify the degree of learned invariance as has been examined in the literature. See, for example, Goodfellow et al\u2019s \u201cMeasuring Invariances in Deep Networks\u201d from NIPS 2009 for one such attempt. \r\n\r\nDetailed comments:\r\n\r\nThe procedure doesn\u2019t yield a single deterministic model at test time, which may be a significant practical drawback as compared with conventional maxout or relu networks.\r\n\r\nStill, a more significant drawback to the proposed method is that the test time computation involves several forward propagations through the entire network in order to have a noticeable (but statistically negligible) advantage over maxout. Figure 3b suggests that, on a relatively simple dataset, 10 or more fprops per example may be necessary to yield a significant advantage over the maxout baseline. This is in addition to the cost additional cost incurred by sampling pseudorandom variates as part of the inference process.\r\n\r\nA quick computation of a confidence interval (based on the confidence interval of a Bernoulli parameter, i.e. the probability of classifying an example incorrectly) for both the results reported in Goodfellow et al (2013) and this work reveals that the confidence intervals can be seen to overlap significantly:\r\n\r\n- CIFAR10 Maxout (no augmentation): 11.68% +/- 0.63%, Probout: 11.35% +/- 0.62%\r\n- CIFAR100 Maxout: 38.57% +/- 0.95%, Probout: 38.14% +/- 0.95%\r\n- SVHN Maxout: 2.47% +/- 0.19%, Probout: 2.39% +/- 0.19%\r\n\r\nThe same comparison between maxout and the competitors reported in the original work yields non-overlapping confidence intervals for all tasks above. The original maxout work does not achieve a statistically significant improvement over the existing state of the art for CIFAR10, and these authors report no improvement, suggesting the task is sufficiently well regularized by the data augmentation that their complementary regularization does not help.\r\n\r\nHyperparameter search: while the authors reused the exact same architectures and other hyperparameters employed by the original maxout manuscript in an attempt to be fair-handed, I believe this is, in truth, a mistake that disadvantages their method in the comparison. Certain hyperparameters, critically the learning rate and momentum, will be very sensitive to changes in the learning dynamics such as those introduced by the stochastic generalization of the activation function. Even the optimal network architecture may not be the same. The way I would suggest approaching this is with a randomized hyperparameter search (even one in the neighbourhood of the  original settings) wherein the same hyperparameters are tried for both methods, and each point in the shared hyperparameter space is further optimized via randomized search over the hyperparameters specific to probout. This gives the method in question a fairer shot by not insisting that the optimal hyperparameters for maxout be the same as those for probout (there is no a priori reason that they should be).\r\n\r\nThe choice of temperature schedules also seems like an area that should be further explored. It seems odd that increasing the temperature during training should help (the paper does not specify the length of the linear (inverse) decrease period, this should be noted). What is the intuition for why this helps? How could one validate these intuitions experimentally?\r\n\r\nThe claim that the stochastic procedure \u201cprevents [filters] from being unused by the network\u201d is dubious. Section 8.2 of the original maxout paper suggests that dropout SGD training alone is remarkably effective in this respect. This claim should be quantitatively investigated and verified if it is to be made at all.\r\n\r\nFinally, the paper motivates around probout units learning better invariances, but no attempt is made at quantitatively validating this claim. As it stands, there is a qualitative assessment of the first layer convolutional filters learned, noting that they appear to resemble easily recognizable transformations & quadrature pairs moreso than those learned by vanilla maxout, but I find this somewhat unconvincing. Just because the invariances learned by vanilla maxout are not always obvious to the human practitioner does not mean they are not useful invariances for the model to encode."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Deep Neural Networks with Probabilistic Maxout Units", "decision": "submitted, no decision", "abstract": "We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN).", "pdf": "https://arxiv.org/abs/1312.6116", "paperhash": "springenberg|improving_deep_neural_networks_with_probabilistic_maxout_units", "keywords": [], "conflicts": [], "authors": ["Jost Tobias Springenberg", "Martin Riedmiller"], "authorids": ["springj@informatik.uni-freiburg.de", "martin.riedmiller@deepmind.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391830920000, "tcdate": 1391830920000, "number": 3, "id": "r5RFq2vpx8rfK", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "NR4KjDE0w9RXD", "replyto": "NR4KjDE0w9RXD", "signatures": ["anonymous reviewer f3f1"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Improving Deep Neural Networks with Probabilistic Maxout Units", "review": "The paper introduces a generalization of the maxout unit, called probout. The output of a maxout unit is defined as the maximum of a set of linear filter responses. The output of a probout unit is sampled from a softmax defined on the linear responses. For vanishing temperature this turns into the maxout response.\r\n\r\nWhile the idea is probably not revolutionary, it seems reasonable and it seems to work fairly well on the datasets tried in the paper. \r\n\r\nIt is a bit unfortunate that unlike for dropout/maxout, there does not seem to be a closed-form, deterministic activation function at test time that works well. At least the authors did not find any.\r\n\r\nInstead they propose to average multiple outputs. This makes probout networks much slower at test time than a maxout network. It also puts into perspective the improved classification results over maxout. It seems unlikely that the common practice of halving weights at test time is exactly the optimal way of making predictions for a model trained with dropout. And it is conceivable that some kind of model averaging will be able to improve performance for those networks, too.\r\n\r\nIt is interesting that probout units with group size two tend to yield filter pairs in quadrature relationship, and much more clearly so than maxout. In that respect they behave similar to average pooling units. It would be interesting to investigate this further in future work."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Deep Neural Networks with Probabilistic Maxout Units", "decision": "submitted, no decision", "abstract": "We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN).", "pdf": "https://arxiv.org/abs/1312.6116", "paperhash": "springenberg|improving_deep_neural_networks_with_probabilistic_maxout_units", "keywords": [], "conflicts": [], "authors": ["Jost Tobias Springenberg", "Martin Riedmiller"], "authorids": ["springj@informatik.uni-freiburg.de", "martin.riedmiller@deepmind.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387812300000, "tcdate": 1387812300000, "number": 36, "id": "NR4KjDE0w9RXD", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "NR4KjDE0w9RXD", "signatures": ["springj@informatik.uni-freiburg.de"], "readers": ["everyone"], "content": {"title": "Improving Deep Neural Networks with Probabilistic Maxout Units", "decision": "submitted, no decision", "abstract": "We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN).", "pdf": "https://arxiv.org/abs/1312.6116", "paperhash": "springenberg|improving_deep_neural_networks_with_probabilistic_maxout_units", "keywords": [], "conflicts": [], "authors": ["Jost Tobias Springenberg", "Martin Riedmiller"], "authorids": ["springj@informatik.uni-freiburg.de", "martin.riedmiller@deepmind.com"]}, "writers": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 12}