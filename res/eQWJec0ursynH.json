{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1365114600000, "tcdate": 1365114600000, "number": 6, "id": "H3-iUVuyZzUgh", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "eQWJec0ursynH", "replyto": "eQWJec0ursynH", "signatures": ["Zhirong Yang"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Great work, congratulations! It seems we and you have simultaneously found essentially the same solution. Our paper and software are here:\r\n\r\nZhirong Yang, Jaakko Peltonen, Samuel Kaski. Scalable Optimization of Neighbor Embedding for Visualization. Accepted to ICML2013.\r\n\r\nPreprint and software: http://research.ics.aalto.fi/mi/software/ne/\r\n\r\n\r\nBest regards,\r\nZhirong, Jaakko, Samuel"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Barnes-Hut-SNE", "decision": "conferenceOral-iclr2013-conference", "abstract": "The paper presents an O(N log N)-implementation of t-SNE -- an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in O(N^2). The new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm used by astronomers to perform N-body simulations - to approximate the forces between the corresponding points in the embedding. Our experiments show that the new algorithm, called Barnes-Hut-SNE, leads to substantial computational advantages over standard t-SNE, and that it makes it possible to learn embeddings of data sets with millions of objects.", "pdf": "https://arxiv.org/abs/1301.3342", "paperhash": "maaten|barneshutsne", "authors": ["Laurens van der Maaten"], "authorids": ["lvdmaaten@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363113120000, "tcdate": 1363113120000, "number": 9, "id": "Hy8wy4X01CHmD", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "eQWJec0ursynH", "replyto": "eQWJec0ursynH", "signatures": ["Laurens van der Maaten"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "In typical applications of Barnes-Hut (like t-SNE), the force nearly vanishes in the far field, which allows for averaging those far-field forces without losing much accuracy.\r\n\r\nIn algorithms that minimize, e.g., the squared error between two sets of pairwise distances, I guess you could do the opposite. The force exerted on a point is then dominated by interactions with distant points, so you should be able to average over the interactions with nearby points without losing much accuracy. However, it's questionable whether such an approach would be as efficient because, in general, a point has far fewer points in its near field than in its far field (i.e. far fewer points for which we can average without losing accuracy). \r\n\r\nHaving said that, I have never tried, so I could be wrong."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Barnes-Hut-SNE", "decision": "conferenceOral-iclr2013-conference", "abstract": "The paper presents an O(N log N)-implementation of t-SNE -- an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in O(N^2). The new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm used by astronomers to perform N-body simulations - to approximate the forces between the corresponding points in the embedding. Our experiments show that the new algorithm, called Barnes-Hut-SNE, leads to substantial computational advantages over standard t-SNE, and that it makes it possible to learn embeddings of data sets with millions of objects.", "pdf": "https://arxiv.org/abs/1301.3342", "paperhash": "maaten|barneshutsne", "authors": ["Laurens van der Maaten"], "authorids": ["lvdmaaten@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362833640000, "tcdate": 1362833640000, "number": 5, "id": "AZcnMdQBqGZS4", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "eQWJec0ursynH", "replyto": "eQWJec0ursynH", "signatures": ["Alex Bronstein"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Laurens, have you thought about using similar ideas for embedding algorithms that also exploit global similarities (like multidimensional scaling)? I think in many types of data analysis, this can be extremely important."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Barnes-Hut-SNE", "decision": "conferenceOral-iclr2013-conference", "abstract": "The paper presents an O(N log N)-implementation of t-SNE -- an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in O(N^2). The new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm used by astronomers to perform N-body simulations - to approximate the forces between the corresponding points in the embedding. Our experiments show that the new algorithm, called Barnes-Hut-SNE, leads to substantial computational advantages over standard t-SNE, and that it makes it possible to learn embeddings of data sets with millions of objects.", "pdf": "https://arxiv.org/abs/1301.3342", "paperhash": "maaten|barneshutsne", "authors": ["Laurens van der Maaten"], "authorids": ["lvdmaaten@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362833520000, "tcdate": 1362833520000, "number": 8, "id": "24bs4th0sfgwE", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "eQWJec0ursynH", "replyto": "eQWJec0ursynH", "signatures": ["anonymous reviewer c262"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Barnes-Hut-SNE", "review": "The paper addresses the problem of low-dimensional data embedding for visualization purposes via stochastic neighbor embedding, in which Euclidean dissimilarities in the data space are modulated by the Gaussian kernel, and a configuration of points in the low-dimensional embedding space is found such that the new dissimilarities in the embedding space obtained via the Student-t kernel match the original ones as closely as possible in the sense of the KL divergence. While the original algorithm is O(n^2), the authors propose to use a fast multipole technique to reduce complexity to O(nlogn). The idea is original and the reported results are very convincing. I think it is probably one of the first instances in which an FMM technique is used to accelerate local embeddings.\r\n\r\nPros:\r\n\r\n1.\tThe idea is simple and is relatively easy to implement. The authors also provide code.\r\n2.\tThe experimental evaluation is large-scale, and the results are very convincing.\r\n\r\nCons:\r\n\r\n1.\tNo controllable tradeoff between the embedding error and acceleration.\r\n2.\tIn its current setting, the proposed approach is limited to local similarities only. Can it be extended to other settings in which global similarities are at least as important as the local ones? In other words, is it possible to apply a similar scheme for MDS-type global embedding algorithms?"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Barnes-Hut-SNE", "decision": "conferenceOral-iclr2013-conference", "abstract": "The paper presents an O(N log N)-implementation of t-SNE -- an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in O(N^2). The new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm used by astronomers to perform N-body simulations - to approximate the forces between the corresponding points in the embedding. Our experiments show that the new algorithm, called Barnes-Hut-SNE, leads to substantial computational advantages over standard t-SNE, and that it makes it possible to learn embeddings of data sets with millions of objects.", "pdf": "https://arxiv.org/abs/1301.3342", "paperhash": "maaten|barneshutsne", "authors": ["Laurens van der Maaten"], "authorids": ["lvdmaaten@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362758580000, "tcdate": 1362758580000, "number": 7, "id": "pA91py2CW8AQg", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "eQWJec0ursynH", "replyto": "eQWJec0ursynH", "signatures": ["Laurens van der Maaten"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I updated the paper according the reviewers' comments, and included results with a dual-tree implementation of t-SNE in the appendix. The updated paper should appear on Arxiv soon."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Barnes-Hut-SNE", "decision": "conferenceOral-iclr2013-conference", "abstract": "The paper presents an O(N log N)-implementation of t-SNE -- an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in O(N^2). The new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm used by astronomers to perform N-body simulations - to approximate the forces between the corresponding points in the embedding. Our experiments show that the new algorithm, called Barnes-Hut-SNE, leads to substantial computational advantages over standard t-SNE, and that it makes it possible to learn embeddings of data sets with millions of objects.", "pdf": "https://arxiv.org/abs/1301.3342", "paperhash": "maaten|barneshutsne", "authors": ["Laurens van der Maaten"], "authorids": ["lvdmaaten@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362421080000, "tcdate": 1362421080000, "number": 2, "id": "DyHSDHfKmbDPM", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "eQWJec0ursynH", "replyto": "eQWJec0ursynH", "signatures": ["Laurens van der Maaten"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I have experimented with dual-tree variants of my algorithm (which required only trivial changes in the existing code), experimenting with both quadtrees and kd-trees as the underlying tree structures. Perhaps surprisingly, the dual-tree algorithm has approximately the same accuracy-speed trade-off as the Barnes-Hut algorithm (even when redundant dual-tree computations are pruned) irrespective of what tree is used. \r\n\r\nI think the main reason for this result is that after computing an interaction between two cells, one still needs to figure out to which points this interaction needs to be added (i.e. which points are in the cell). This set of points can either be obtained using a full search of the tree corresponding to the cell, or by storing a list of children in each node during tree construction. Both these approaches are quite costly, and lead the computational advantages of the dual-tree algorithm to evaporate. (The dual-tree algorithm does provide a very cheap way to estimate the value of the t-SNE cost function though.)\r\n\r\nI will add these results in the final paper."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Barnes-Hut-SNE", "decision": "conferenceOral-iclr2013-conference", "abstract": "The paper presents an O(N log N)-implementation of t-SNE -- an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in O(N^2). The new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm used by astronomers to perform N-body simulations - to approximate the forces between the corresponding points in the embedding. Our experiments show that the new algorithm, called Barnes-Hut-SNE, leads to substantial computational advantages over standard t-SNE, and that it makes it possible to learn embeddings of data sets with millions of objects.", "pdf": "https://arxiv.org/abs/1301.3342", "paperhash": "maaten|barneshutsne", "authors": ["Laurens van der Maaten"], "authorids": ["lvdmaaten@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362330660000, "tcdate": 1362330660000, "number": 1, "id": "TTxAqxZdhgIV0", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "eQWJec0ursynH", "replyto": "eQWJec0ursynH", "signatures": ["Laurens van der Maaten"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Thanks a bunch for these insightful reviews and for the useful pointers to related work (some of which I was not aware of)!\r\n\r\nIn preliminary experiments, I compared locality-sensitive hashing and vantage-point trees in the initial nearest-neighbor (in the high-dimensional space). I found vantage-point trees to perform considerably better, which is why I used them in the final implementation. The strong performance I obtained when using metric trees appears to be in line with the results presented by Liu, Moore, Gray & Yang (2004). I agree with the first reviewer that there are many other (approximate) nearest-neighbor algorithms that could be used here instead. I will clarify this in the paper, and include references to relevant related work.\r\n\r\nThe work by Nando de Freitas's lab on n-body simulations is very interesting indeed. I don't think it can readily be applied to t-SNE though, as it appears to heavily rely on the (improved) fast Gauss transform, i.e. on the assumption that Gaussian kernels are used. To the best of my knowledge, there is no existing work that uses fast multipole methods to evaluate Student-t kernels (the fast Gauss transform is an example of a fast multipole method), so extension of this work to t-SNE appears non-trivial. It is also unclear whether fast multipole methods would actually outperform Barnes-Hut in practice, because multipole methods tend to have constants that are much worse. Having said that, this is indeed a very interesting direction for future work! I will clarify this in the paper, and make sure to include the relevant references.\r\n\r\nI was not aware of the work by Alex Gray's lab on dual-tree algorithms for n-body simulations; indeed, this work seems readily applicable to t-SNE. I'm presently coding up a dual-tree version of my algorithm, and will try to include empirical evaluations with the dual-tree approach in the final version of the paper. I hope to post an updated version of the paper with these results on Arxiv in a week or two.\r\n\r\nI agree with the first reviewer that it is interesting to study if the accuracy-speed trade-off can be adapted during the optimization, but I am not sure that I agree that looser bounds should be used in the beginning of the optimization. In fact, the first 100 or so iterations are essential in identifying the global structure of the data --- doing a poor job in those iterations often implies getting stuck in poor local optima. (I guess one can think of it as errors propagating over time in the optimization.) So an optimal strategy may actually be the opposite of what the reviewer suggests: use tight bounds in the early stages of the optimization and looser bounds later on. It's certainly an interesting direction for future work!"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Barnes-Hut-SNE", "decision": "conferenceOral-iclr2013-conference", "abstract": "The paper presents an O(N log N)-implementation of t-SNE -- an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in O(N^2). The new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm used by astronomers to perform N-body simulations - to approximate the forces between the corresponding points in the embedding. Our experiments show that the new algorithm, called Barnes-Hut-SNE, leads to substantial computational advantages over standard t-SNE, and that it makes it possible to learn embeddings of data sets with millions of objects.", "pdf": "https://arxiv.org/abs/1301.3342", "paperhash": "maaten|barneshutsne", "authors": ["Laurens van der Maaten"], "authorids": ["lvdmaaten@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362192420000, "tcdate": 1362192420000, "number": 4, "id": "2VfI2cAZSF2P0", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "eQWJec0ursynH", "replyto": "eQWJec0ursynH", "signatures": ["anonymous reviewer 7db1"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Barnes-Hut-SNE", "review": "The submitted paper proposes a more efficient implementation of the Student-t distributed version of SNE. t-SNE is O(n^2), and the proposed implementation is O(nlogn). This offers a substantial improvement in the efficiency, such that very large datasets may be embedded. Furthermore, the speed increase is obtained through 2 key approximations without incurring a penalty on accuracy of the embedding.\r\n\r\nThere are 2 approximations that are described. First, the input space nearest neighbors are approximated by building a vantage-point tree. Second, the approximation of the gradient of KL divergence is made by splitting the gradient into attractive and repulsive components and applying a Barnes-Hut algorithm to estimate the repulsive component. The Barnes Hut algorithm uses a hierarchical estimate of force. A quad-tree provides an efficient, hierarchical spatial representation. \r\n\r\nThe submission is well-written and seems to be accurate. The results validate the claim: the error of the embedding does not increase, and the computation time is decreased by an order of magnitude. The approach is tested on MNIST, NORB, TIMIT, and CIFAR. Overall, the contribution of the paper is fairly small, but the benefit is real, given the popularity of SNE. In addition, the topic is relevant for the ICLR audience."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Barnes-Hut-SNE", "decision": "conferenceOral-iclr2013-conference", "abstract": "The paper presents an O(N log N)-implementation of t-SNE -- an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in O(N^2). The new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm used by astronomers to perform N-body simulations - to approximate the forces between the corresponding points in the embedding. Our experiments show that the new algorithm, called Barnes-Hut-SNE, leads to substantial computational advantages over standard t-SNE, and that it makes it possible to learn embeddings of data sets with millions of objects.", "pdf": "https://arxiv.org/abs/1301.3342", "paperhash": "maaten|barneshutsne", "authors": ["Laurens van der Maaten"], "authorids": ["lvdmaaten@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362177000000, "tcdate": 1362177000000, "number": 3, "id": "Dkj3DFf4GZJPh", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "eQWJec0ursynH", "replyto": "eQWJec0ursynH", "signatures": ["anonymous reviewer d9db"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Barnes-Hut-SNE", "review": "Stochastic neighbour embedding (SNE) is a sound, probabilistic method for dimensionality reduction. One of its limitations is that  its complexity is O(N^2), where N is the, typically large, number of data points. To surmount this limitation, the this paper proposes computational methods to reduce the computational cost to O(NlogN), while only incurring an O(N) memory cost.\r\n\r\nIn the SNE variant discussed in this paper, the kernel in high dimensions is Gaussian, while the similarity in low dimensions is governed by a t-distribution. The proposed method consists of two components. First, the exponential decay of Gaussian measures is used to carry out truncation and construct a vantage-point tree for the data in high dimensions. This enables the authors to carry our nearest neighbour search in O(NlogN). The second component addreses the efficient computation of the gradient of SNE. Here, the paper proposes a 2D Barnes-hut algorithm to approximate the gradient in O(NlogN) steps. The Barnes-Hut algorithm is a well known method in N-body simulation, but it has not been used in this context previously to the best of my knowledge.\r\n\r\nThe paper is very well written. The contribution is correct and sound. Not surprisingly, the experiments show great improvements in computational performance, thus allowing for a good dimensionality reduction technique to become more broadly applicable.\r\n\r\nThe author ought to be commended for making the code available. He should also be commended for making the limitations of the approach very clear in the concluding remarks, namely that the current version is only for 2D-embeddings and that the method does not offer a way of controlling the error (e.g. via error bounds).\r\n\r\nMinor typo in Page 2 last line: to slow should be too slow.\r\n\r\nI believe the paper makes a good contribution. However, it has one crucial shortcoming that must be addressed by the author.  Specifically, there is a great body of literature on N-body methods for machine learning problems that the author does not seem to be aware of. I think this work should be placed in this context and that appropriate references and comparisons (for which I will point the author to online software) should be included in the final form in this paper. The relevant work includes:\r\n\r\n1. All the dual-tree approximations developed by Alex Gray at http://www.fast-lab.org/\r\nIn particular note that his methods apply to nearest neighbour search and the type of kernel density estimates required in the computation of the gradient. Dual trees also allow for the use of error bounds. For publications, see e.g.\r\nGray, Alexander G., and Andrew W. Moore. 'N-Body'problems in statistical learning.' Advances in neural information processing systems (2001): 521-527.\r\nLiu, Ting, Andrew W. Moore, Alexander Gray, and Ke Yang. 'An investigation of practical approximate nearest neighbor algorithms.' Advances in neural information processing systems 17 (2004): 825-832.\r\n\r\n2. The multipole methods developed in Ramani Duraiswami lab, including:\r\nYang, Changjiang, Ramani Duraiswami, Nail A. Gumerov, and Larry Davis. 'Improved fast gauss transform and efficient kernel density estimation.' In Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on, pp. 664-671. IEEE, 2003.\r\n\r\n3. The algorithms for fast kernel density estimates from Nando de Freitas' lab. See e.g.,\r\nMahdaviani, Maryam, Nando de Freitas, Bob Fraser, and Firas Hamze. 'Fast computational methods for visually guided robots.' In IEEE International Conference on Robotics and Automation, vol. 1, p. 138. IEEE; 1999, 2005.\r\nLang, Dustin, Mike Klaas, and Nando de Freitas. 'Empirical testing of fast kernel density estimation algorithms.' UBC Technical repor 2 (2005).\r\nOne of his papers does, in fact, discuss multipole methods for SNE and presents results using the fast Gauss transform:\r\nDe Freitas, Nando, Yang Wang, Maryam Mahdaviani, and Dustin Lang. 'Fast Krylov methods for N-body learning.' Advances in neural information processing systems 18 (2006): 251.\r\nThe code is available here: http://www.cs.ubc.ca/~awll/nbody_methods.html\r\n\r\n4. The cover tree for nearest neighbour search, introduced in:\r\nBeygelzimer, Alina, Sham Kakade, and John Langford. 'Cover trees for nearest neighbor.' In MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE-, vol. 23, p. 97. 2006.\r\nFor code, see the Wikipedia entry: http://en.wikipedia.org/wiki/Cover_tree\r\n\r\n5. FLANN - Fast Library for Approximate Nearest Neighbors developed by Marius Muja. This is a powerful library of methods including randomized kd-trees and k-means methods for fast nearest neighbour search. It is extremely popular in computer vision. For code and more info see: http://www.cs.ubc.ca/~mariusm/index.php/FLANN/FLANN\r\nYou could use this code easily to replace the nearest neighbour search and compare performance.\r\n\r\nFinally, there is something very interesting in this paper that is worth studying further. Assume we use an N-body method in the computation of the gradient, which has error bounds. Then, it seems to stand to reason that one ought to use loose bounds in the beginning of the gradient iterations and increase the precision as the algorithm progresses. This could allow for further improvements in computation. Moreover, using theoretical tools for studying the convergence of optimization algorithms, one could possibly address the theoretical analysis of this algorithm."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Barnes-Hut-SNE", "decision": "conferenceOral-iclr2013-conference", "abstract": "The paper presents an O(N log N)-implementation of t-SNE -- an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in O(N^2). The new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm used by astronomers to perform N-body simulations - to approximate the forces between the corresponding points in the embedding. Our experiments show that the new algorithm, called Barnes-Hut-SNE, leads to substantial computational advantages over standard t-SNE, and that it makes it possible to learn embeddings of data sets with millions of objects.", "pdf": "https://arxiv.org/abs/1301.3342", "paperhash": "maaten|barneshutsne", "authors": ["Laurens van der Maaten"], "authorids": ["lvdmaaten@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358343900000, "tcdate": 1358343900000, "number": 19, "id": "eQWJec0ursynH", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "eQWJec0ursynH", "signatures": ["lvdmaaten@gmail.com"], "readers": ["everyone"], "content": {"title": "Barnes-Hut-SNE", "decision": "conferenceOral-iclr2013-conference", "abstract": "The paper presents an O(N log N)-implementation of t-SNE -- an embedding technique that is commonly used for the visualization of high-dimensional data in scatter plots and that normally runs in O(N^2). The new implementation uses vantage-point trees to compute sparse pairwise similarities between the input data objects, and it uses a variant of the Barnes-Hut algorithm - an algorithm used by astronomers to perform N-body simulations - to approximate the forces between the corresponding points in the embedding. Our experiments show that the new algorithm, called Barnes-Hut-SNE, leads to substantial computational advantages over standard t-SNE, and that it makes it possible to learn embeddings of data sets with millions of objects.", "pdf": "https://arxiv.org/abs/1301.3342", "paperhash": "maaten|barneshutsne", "authors": ["Laurens van der Maaten"], "authorids": ["lvdmaaten@gmail.com"], "keywords": [], "conflicts": []}, "writers": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 10}