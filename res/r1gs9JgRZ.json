{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730185572, "tcdate": 1509059224064, "number": 203, "cdate": 1518730185559, "id": "r1gs9JgRZ", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "r1gs9JgRZ", "original": "H1kjqyxCZ", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Mixed Precision Training", "abstract": "Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.", "pdf": "/pdf/575abe1d433099e0cd046ec3bbcf10eaa1022ec6.pdf", "paperhash": "micikevicius|mixed_precision_training", "_bibtex": "@inproceedings{\nmicikevicius2018mixed,\ntitle={Mixed Precision Training},\nauthor={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1gs9JgRZ},\n}", "authors": ["Paulius Micikevicius", "Sharan Narang", "Jonah Alben", "Gregory Diamos", "Erich Elsen", "David Garcia", "Boris Ginsburg", "Michael Houston", "Oleksii Kuchaiev", "Ganesh Venkatesh", "Hao Wu"], "keywords": ["Half precision", "float16", "Convolutional neural networks", "Recurrent neural networks"], "authorids": ["pauliusm@nvidia.com", "sharan@baidu.com", "alben@nvidia.com", "gdiamos@baidu.com", "eriche@google.com", "dagarcia@nvidia.com", "bginsburg@nvidia.com", "mhouston@nvidia.com", "okuchaiev@nvidia.com", "gavenkatesh@nvidia.com", "skyw@nvidia.com"]}, "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260099927, "tcdate": 1517249253753, "number": 54, "cdate": 1517249253727, "id": "rJAJQ1prf", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "r1gs9JgRZ", "replyto": "r1gs9JgRZ", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "meta score: 8\n\nThe paper explores mixing 16- and 32-bit floating point arithmetic for NN training with CNN and LSTM experiments on a variety of tasks\n\nPros:\n - addresses an important practical problem\n - very wide range of experimentation, reported in depth\n\nCons:\n - one might say the novelty was minor, but the novelty comes from the extensive analysis and experiments", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixed Precision Training", "abstract": "Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.", "pdf": "/pdf/575abe1d433099e0cd046ec3bbcf10eaa1022ec6.pdf", "paperhash": "micikevicius|mixed_precision_training", "_bibtex": "@inproceedings{\nmicikevicius2018mixed,\ntitle={Mixed Precision Training},\nauthor={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1gs9JgRZ},\n}", "authors": ["Paulius Micikevicius", "Sharan Narang", "Jonah Alben", "Gregory Diamos", "Erich Elsen", "David Garcia", "Boris Ginsburg", "Michael Houston", "Oleksii Kuchaiev", "Ganesh Venkatesh", "Hao Wu"], "keywords": ["Half precision", "float16", "Convolutional neural networks", "Recurrent neural networks"], "authorids": ["pauliusm@nvidia.com", "sharan@baidu.com", "alben@nvidia.com", "gdiamos@baidu.com", "eriche@google.com", "dagarcia@nvidia.com", "bginsburg@nvidia.com", "mhouston@nvidia.com", "okuchaiev@nvidia.com", "gavenkatesh@nvidia.com", "skyw@nvidia.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642408464, "tcdate": 1511681822582, "number": 1, "cdate": 1511681822582, "id": "rJwXkeOgM", "invitation": "ICLR.cc/2018/Conference/-/Paper203/Official_Review", "forum": "r1gs9JgRZ", "replyto": "r1gs9JgRZ", "signatures": ["ICLR.cc/2018/Conference/Paper203/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Exhaustive experiments validate simple techniques", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper considers the problem of training neural networks in mixed precision (MP), using both 16-bit floating point (FP16) and 32-bit floating point (FP32). The paper proposes three techniques for training networks in mixed precision: first, keep a master copy of network parameters in FP32; second, use loss scaling to ensure that gradients are representable using the limited range of FP16; third, compute dot products and reductions with FP32 accumulation. \n\nUsing these techniques allows the authors to match the results of traditional FP32 training on a wide variety of tasks without modifying any training hyperparameters. The authors show results on ImageNet classification (with AlexNet, VGG, GoogLeNet, Inception-v1, Inception-v3, and ResNet-50), VOC object detection (with Faster R-CNN and Multibox SSD), speech recognition in English and Mandarin (with CNN+GRU), English to French machine translation (with multilayer LSTMs), language modeling on the 1 Billion Words dataset (with a bigLSTM), and generative adversarial networks on CelebFaces (with DCGAN).\n\nPros:\n- Three simple techniques to use for mixed-precision training\n- Matches performance of traditional FP32 training without modifying any hyperparameters\n- Very extensive experiments on a wide variety of tasks\n\nCons:\n- Experiments do not validate the necessity of FP32 accumulation\n- No comparison of training time speedup from mixed precision\n\nWith new hardware (such as NVIDIA\u2019s Volta architecture) providing large computational speedups for MP computation, I expect that MP training will become standard practice in deep learning in the near future. Naively porting FP32 training recipes can fail due to the reduced numeric range of FP16 arithmetic; however by adopting the techniques of this paper, practitioners will be able to migrate their existing FP32 training pipelines to MP without modifying any hyperparameters. I expect these techniques to be hugely impactful as more people begin migrating to new MP hardware.\n\nThe experiments in this paper are very exhaustive, covering nearly every major application of deep learning. Matching state-of-the-art results on so many tasks increases my confidence that I will be able to apply these techniques to my own tasks and architectures to achieve stable MP training.\n\nMy first concern with the paper is that there are no experiments to demonstrate the necessity of FP32 accumulation. With an FP32 master copy of the weights and loss scaling, can all arithmetic be performed solely in FP16, or are there some tasks where training will still diverge?\n\nMy second concern is that there is no comparison of training-time speedup using MP. The main reason that MP is interesting is because new hardware promises to accelerate it. If people are willing to endure the extra engineering overhead of implementing the techniques from this paper, what kind of practical speedups can they expect to see from their workloads? NVIDIA\u2019s marketing material claims that the Tensor Cores in the V100 offer an 8x speedup over its general-purpose CUDA cores (https://www.nvidia.com/en-us/data-center/tesla-v100/). Since in this paper some operations are performed in FP32 (weight updates, batch normalization) and other operations are bound by memory and not compute bandwidth, what kinds of speedups do you see in practice when moving from FP32 to MP on V100?\n\nMy other concerns are minor. Mandarin speech recognition results are reported on \u201cour internal test set\u201d. Is there any previously published work on this dataset, or any publicly available test set for this task?\n\nThe notation around the Inception architectures should be clarified. According to [3] and [4], \u201cInception-v1\u201d and \u201cGoogLeNet\u201d both refer to the architecture used in [1]. The architecture used in [2] is referred to as \u201cBN-Inception\u201d by [3] and \u201cInception-v2\u201d by [4]. \u201cInception-v3\u201d is the architecture from [3], which is not currently cited. To improve clarity in Table 1, I suggest renaming \u201cGoogLeNet\u201d to \u201cInception-v1\u201d, changing \u201cInception-v1\u201d to \u201cInception-v2\u201d, and adding explicit citations to all rows of the table.\n\nIn Section 4.3 the authors note that \u201chalf-precision storage format may act as a regularizer during training\u201d. Though the effect is most obvious from the speech recognition experiments in Section 4.3, MP also achieves slightly higher performance than baseline for all ImageNet models but Inception-v1 and for both object detection models; these results add support to the idea of FP16 as a regularizer.\n\nMinor typos:\nSection 3.3, Paragraph 3: \u201ceither FP16 or FP16 math\u201d -> \u201ceither FP16 or FP32 math\u201d\nSection 4.1, Paragraph 4: \u201c pre-ativation\u201d -> \u201cpre-activation\u201d\n\nOverall this is a strong paper, and I believe that it will be impactful as MP hardware becomes more widely used.\n\n\nReferences\n\n[1] Szegedy et al, \u201cGoing Deeper with Convolutions\u201d, CVPR 2015\n[2] Ioffe and Szegedy, \u201cBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\u201d, ICML 2015\n[3] Szegedy et al, \u201cRethinking the Inception Architecture for Computer Vision\u201d, CVPR 2016\n[4] Szegedy et al, \u201cInception-v4, Inception-ResNet and the Impact of Residual Connections on Learning\u201d, ICLR 2016 Workshop", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixed Precision Training", "abstract": "Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.", "pdf": "/pdf/575abe1d433099e0cd046ec3bbcf10eaa1022ec6.pdf", "paperhash": "micikevicius|mixed_precision_training", "_bibtex": "@inproceedings{\nmicikevicius2018mixed,\ntitle={Mixed Precision Training},\nauthor={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1gs9JgRZ},\n}", "authors": ["Paulius Micikevicius", "Sharan Narang", "Jonah Alben", "Gregory Diamos", "Erich Elsen", "David Garcia", "Boris Ginsburg", "Michael Houston", "Oleksii Kuchaiev", "Ganesh Venkatesh", "Hao Wu"], "keywords": ["Half precision", "float16", "Convolutional neural networks", "Recurrent neural networks"], "authorids": ["pauliusm@nvidia.com", "sharan@baidu.com", "alben@nvidia.com", "gdiamos@baidu.com", "eriche@google.com", "dagarcia@nvidia.com", "bginsburg@nvidia.com", "mhouston@nvidia.com", "okuchaiev@nvidia.com", "gavenkatesh@nvidia.com", "skyw@nvidia.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642408365, "id": "ICLR.cc/2018/Conference/-/Paper203/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper203/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper203/AnonReviewer1", "ICLR.cc/2018/Conference/Paper203/AnonReviewer3", "ICLR.cc/2018/Conference/Paper203/AnonReviewer2"], "reply": {"forum": "r1gs9JgRZ", "replyto": "r1gs9JgRZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper203/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642408365}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642408423, "tcdate": 1511817228832, "number": 2, "cdate": 1511817228832, "id": "SkSMlWcgG", "invitation": "ICLR.cc/2018/Conference/-/Paper203/Official_Review", "forum": "r1gs9JgRZ", "replyto": "r1gs9JgRZ", "signatures": ["ICLR.cc/2018/Conference/Paper203/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Mixed Precision Training", "rating": "5: Marginally below acceptance threshold", "review": "The paper provides methods for training deep networks using half-precision floating point numbers without losing model accuracy or changing the model hyper-parameters. The main ideas are to use a master copy of weights when updating the weights, scaling the loss before back-prop and using full precision variables to store products. Experiments are performed on a large number of state-of-art deep networks, tasks and datasets which show that the proposed mixed precision training does provide the same accuracy at half the memory.\n\nPositives\n- The experimental evaluation is fairly exhaustive on a large number of deep networks, tasks and datasets and the proposed training preserves the accuracy of all the tested networks at half the memory cost.\n\nNegatives\n- The overall technical contribution is fairly small and are ideas that are regularly implemented when optimizing systems.\n- The overall advantage is only a 2x reduction in memory which can be gained by using smaller batches at the cost of extra compute. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixed Precision Training", "abstract": "Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.", "pdf": "/pdf/575abe1d433099e0cd046ec3bbcf10eaa1022ec6.pdf", "paperhash": "micikevicius|mixed_precision_training", "_bibtex": "@inproceedings{\nmicikevicius2018mixed,\ntitle={Mixed Precision Training},\nauthor={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1gs9JgRZ},\n}", "authors": ["Paulius Micikevicius", "Sharan Narang", "Jonah Alben", "Gregory Diamos", "Erich Elsen", "David Garcia", "Boris Ginsburg", "Michael Houston", "Oleksii Kuchaiev", "Ganesh Venkatesh", "Hao Wu"], "keywords": ["Half precision", "float16", "Convolutional neural networks", "Recurrent neural networks"], "authorids": ["pauliusm@nvidia.com", "sharan@baidu.com", "alben@nvidia.com", "gdiamos@baidu.com", "eriche@google.com", "dagarcia@nvidia.com", "bginsburg@nvidia.com", "mhouston@nvidia.com", "okuchaiev@nvidia.com", "gavenkatesh@nvidia.com", "skyw@nvidia.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642408365, "id": "ICLR.cc/2018/Conference/-/Paper203/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper203/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper203/AnonReviewer1", "ICLR.cc/2018/Conference/Paper203/AnonReviewer3", "ICLR.cc/2018/Conference/Paper203/AnonReviewer2"], "reply": {"forum": "r1gs9JgRZ", "replyto": "r1gs9JgRZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper203/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642408365}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642408382, "tcdate": 1511989675010, "number": 3, "cdate": 1511989675010, "id": "SJQ3bonlG", "invitation": "ICLR.cc/2018/Conference/-/Paper203/Official_Review", "forum": "r1gs9JgRZ", "replyto": "r1gs9JgRZ", "signatures": ["ICLR.cc/2018/Conference/Paper203/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Computation improvements for training neural nets", "rating": "7: Good paper, accept", "review": "The paper presents three techniques to train and test neural networks using half precision format (FP16) while not losing accuracy. This allows to train and compute networks faster, and potentially create larger models that use less computation and energy.\n\nThe proposed techniques are rigorously evaluated in several tasks, including CNNs for classification and object detection, RNNs for machine translation, language generation and speech recognition, and generative adversarial networks. The paper consistently shows that the accuracy of training and validation matches the baseline using single precision (FP32), which is the common practice.\n\nThe paper is missing results comparing training and testing speeds in all these models, to illustrate the benefits of using the proposed techniques. It would be very valuable to add the baseline wall-time to the tables, together with the obtained wall-time for training and testing using the proposed techniques. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixed Precision Training", "abstract": "Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.", "pdf": "/pdf/575abe1d433099e0cd046ec3bbcf10eaa1022ec6.pdf", "paperhash": "micikevicius|mixed_precision_training", "_bibtex": "@inproceedings{\nmicikevicius2018mixed,\ntitle={Mixed Precision Training},\nauthor={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1gs9JgRZ},\n}", "authors": ["Paulius Micikevicius", "Sharan Narang", "Jonah Alben", "Gregory Diamos", "Erich Elsen", "David Garcia", "Boris Ginsburg", "Michael Houston", "Oleksii Kuchaiev", "Ganesh Venkatesh", "Hao Wu"], "keywords": ["Half precision", "float16", "Convolutional neural networks", "Recurrent neural networks"], "authorids": ["pauliusm@nvidia.com", "sharan@baidu.com", "alben@nvidia.com", "gdiamos@baidu.com", "eriche@google.com", "dagarcia@nvidia.com", "bginsburg@nvidia.com", "mhouston@nvidia.com", "okuchaiev@nvidia.com", "gavenkatesh@nvidia.com", "skyw@nvidia.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642408365, "id": "ICLR.cc/2018/Conference/-/Paper203/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper203/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper203/AnonReviewer1", "ICLR.cc/2018/Conference/Paper203/AnonReviewer3", "ICLR.cc/2018/Conference/Paper203/AnonReviewer2"], "reply": {"forum": "r1gs9JgRZ", "replyto": "r1gs9JgRZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper203/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642408365}}}, {"tddate": null, "ddate": null, "tmdate": 1515187536929, "tcdate": 1515187536929, "number": 5, "cdate": 1515187536929, "id": "rktIavaQz", "invitation": "ICLR.cc/2018/Conference/-/Paper203/Official_Comment", "forum": "r1gs9JgRZ", "replyto": "r1gs9JgRZ", "signatures": ["ICLR.cc/2018/Conference/Paper203/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper203/Authors"], "content": {"title": "New Revision of the paper", "comment": "We've added a new revision of the paper that addresses the following points:\n\n- Corrected the typos pointed out by reviewer #1\n- Added a reference to Inception v3, modified the classification CNN table to include references and both names for googlenet.  - Added improved accuracy numbers for ResNet-50\n- Added a paragraph to the conclusion on operation speedups, etc.\n\n\nWe thank the reviewers for their helpful comments and feedback. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixed Precision Training", "abstract": "Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.", "pdf": "/pdf/575abe1d433099e0cd046ec3bbcf10eaa1022ec6.pdf", "paperhash": "micikevicius|mixed_precision_training", "_bibtex": "@inproceedings{\nmicikevicius2018mixed,\ntitle={Mixed Precision Training},\nauthor={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1gs9JgRZ},\n}", "authors": ["Paulius Micikevicius", "Sharan Narang", "Jonah Alben", "Gregory Diamos", "Erich Elsen", "David Garcia", "Boris Ginsburg", "Michael Houston", "Oleksii Kuchaiev", "Ganesh Venkatesh", "Hao Wu"], "keywords": ["Half precision", "float16", "Convolutional neural networks", "Recurrent neural networks"], "authorids": ["pauliusm@nvidia.com", "sharan@baidu.com", "alben@nvidia.com", "gdiamos@baidu.com", "eriche@google.com", "dagarcia@nvidia.com", "bginsburg@nvidia.com", "mhouston@nvidia.com", "okuchaiev@nvidia.com", "gavenkatesh@nvidia.com", "skyw@nvidia.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737639, "id": "ICLR.cc/2018/Conference/-/Paper203/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r1gs9JgRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper203/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper203/Authors|ICLR.cc/2018/Conference/Paper203/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper203/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper203/Authors|ICLR.cc/2018/Conference/Paper203/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper203/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper203/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper203/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper203/Reviewers", "ICLR.cc/2018/Conference/Paper203/Authors", "ICLR.cc/2018/Conference/Paper203/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737639}}}, {"tddate": null, "ddate": null, "tmdate": 1515123891898, "tcdate": 1515123683379, "number": 3, "cdate": 1515123683379, "id": "SyoJNu2Xf", "invitation": "ICLR.cc/2018/Conference/-/Paper203/Official_Comment", "forum": "r1gs9JgRZ", "replyto": "SyetJ_Pbz", "signatures": ["ICLR.cc/2018/Conference/Paper203/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper203/Authors"], "content": {"title": "Thanks ", "comment": "Hello Stephen,\n\nThanks for pointing out that the advantage of this technique is not limited to reduction in memory only. We will add some more statements in the paper highlighting the potential speedup with hardware that supports mixed precision training. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixed Precision Training", "abstract": "Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.", "pdf": "/pdf/575abe1d433099e0cd046ec3bbcf10eaa1022ec6.pdf", "paperhash": "micikevicius|mixed_precision_training", "_bibtex": "@inproceedings{\nmicikevicius2018mixed,\ntitle={Mixed Precision Training},\nauthor={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1gs9JgRZ},\n}", "authors": ["Paulius Micikevicius", "Sharan Narang", "Jonah Alben", "Gregory Diamos", "Erich Elsen", "David Garcia", "Boris Ginsburg", "Michael Houston", "Oleksii Kuchaiev", "Ganesh Venkatesh", "Hao Wu"], "keywords": ["Half precision", "float16", "Convolutional neural networks", "Recurrent neural networks"], "authorids": ["pauliusm@nvidia.com", "sharan@baidu.com", "alben@nvidia.com", "gdiamos@baidu.com", "eriche@google.com", "dagarcia@nvidia.com", "bginsburg@nvidia.com", "mhouston@nvidia.com", "okuchaiev@nvidia.com", "gavenkatesh@nvidia.com", "skyw@nvidia.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737639, "id": "ICLR.cc/2018/Conference/-/Paper203/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r1gs9JgRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper203/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper203/Authors|ICLR.cc/2018/Conference/Paper203/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper203/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper203/Authors|ICLR.cc/2018/Conference/Paper203/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper203/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper203/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper203/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper203/Reviewers", "ICLR.cc/2018/Conference/Paper203/Authors", "ICLR.cc/2018/Conference/Paper203/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737639}}}, {"tddate": null, "ddate": null, "tmdate": 1515123813840, "tcdate": 1515123813840, "number": 4, "cdate": 1515123813840, "id": "SyCvNOnQf", "invitation": "ICLR.cc/2018/Conference/-/Paper203/Official_Comment", "forum": "r1gs9JgRZ", "replyto": "rJwXkeOgM", "signatures": ["ICLR.cc/2018/Conference/Paper203/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper203/Authors"], "content": {"title": "Review Response", "comment": "We thank the reviewer for the feedback.  As newer hardware such as Nvidia\u2019s V100 and TitanV become more widely available, we should be able to see a speedup in training time. Performance results for GEMM, RNN, and CNN layers are available at DeepBench. Depending on the layer size and batch size, MP hardware can achieve 2~6x speedup for a given layer, we will add this information to the paper. We are working on measuring the improvement in end-to-end model training using MP hardware and more optimized libraries and frameworks - the studies in this paper used either older hardware, or Volta GPUs but very early libraries and frameworks with MP support.  These measurements are targeted for a subsequent publication as they couldn\u2019t make the ICLR deadline. \n \nWhen it comes to the need for FP32 accumulation, while some networks did not need it others lost a few percentage points of accuracy when accumulating in fp16.  We will add this mention to the paper, but to maximize the success of initial training in MP we recommend employing all three proposed techniques.\n \nThank you for pointing out typos, we will address them in this paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixed Precision Training", "abstract": "Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.", "pdf": "/pdf/575abe1d433099e0cd046ec3bbcf10eaa1022ec6.pdf", "paperhash": "micikevicius|mixed_precision_training", "_bibtex": "@inproceedings{\nmicikevicius2018mixed,\ntitle={Mixed Precision Training},\nauthor={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1gs9JgRZ},\n}", "authors": ["Paulius Micikevicius", "Sharan Narang", "Jonah Alben", "Gregory Diamos", "Erich Elsen", "David Garcia", "Boris Ginsburg", "Michael Houston", "Oleksii Kuchaiev", "Ganesh Venkatesh", "Hao Wu"], "keywords": ["Half precision", "float16", "Convolutional neural networks", "Recurrent neural networks"], "authorids": ["pauliusm@nvidia.com", "sharan@baidu.com", "alben@nvidia.com", "gdiamos@baidu.com", "eriche@google.com", "dagarcia@nvidia.com", "bginsburg@nvidia.com", "mhouston@nvidia.com", "okuchaiev@nvidia.com", "gavenkatesh@nvidia.com", "skyw@nvidia.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737639, "id": "ICLR.cc/2018/Conference/-/Paper203/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r1gs9JgRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper203/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper203/Authors|ICLR.cc/2018/Conference/Paper203/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper203/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper203/Authors|ICLR.cc/2018/Conference/Paper203/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper203/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper203/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper203/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper203/Reviewers", "ICLR.cc/2018/Conference/Paper203/Authors", "ICLR.cc/2018/Conference/Paper203/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737639}}}, {"tddate": null, "ddate": null, "tmdate": 1515123522582, "tcdate": 1515123522582, "number": 2, "cdate": 1515123522582, "id": "ryjSm_hQz", "invitation": "ICLR.cc/2018/Conference/-/Paper203/Official_Comment", "forum": "r1gs9JgRZ", "replyto": "SkSMlWcgG", "signatures": ["ICLR.cc/2018/Conference/Paper203/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper203/Authors"], "content": {"title": "Review Response", "comment": "Thank you for your review and valuable feedback.  We are working on obtaining speedup numbers for mixed precision training with libraries and training frameworks that have been more extensively optimized for mixed precision (experiments in this study that were run on Volta GPUs used libraries and frameworks that had preliminary optimization for mixed precision).  \n\nInitial performance numbers are available in DeepBench which indicate a 2~6x speedup for an operation depending on layer size and batch size, as long as the layer is not limited by latency (as stated in the paper, mixed-precision improves performance for 2 out of 3 potential performance limiters - memory or arithmetic throughput, with latency being the third one).  For layers limited by memory bandwidth, as you point out, upper bound on speedup is 2x. The upper bound on speedups on Volta GPUs is 8x, if the operation is limited by floating point arithmetic. Full network speedups will be somewhat lower, depending on how many layers are limited by memory bandwidth or latency.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixed Precision Training", "abstract": "Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.", "pdf": "/pdf/575abe1d433099e0cd046ec3bbcf10eaa1022ec6.pdf", "paperhash": "micikevicius|mixed_precision_training", "_bibtex": "@inproceedings{\nmicikevicius2018mixed,\ntitle={Mixed Precision Training},\nauthor={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1gs9JgRZ},\n}", "authors": ["Paulius Micikevicius", "Sharan Narang", "Jonah Alben", "Gregory Diamos", "Erich Elsen", "David Garcia", "Boris Ginsburg", "Michael Houston", "Oleksii Kuchaiev", "Ganesh Venkatesh", "Hao Wu"], "keywords": ["Half precision", "float16", "Convolutional neural networks", "Recurrent neural networks"], "authorids": ["pauliusm@nvidia.com", "sharan@baidu.com", "alben@nvidia.com", "gdiamos@baidu.com", "eriche@google.com", "dagarcia@nvidia.com", "bginsburg@nvidia.com", "mhouston@nvidia.com", "okuchaiev@nvidia.com", "gavenkatesh@nvidia.com", "skyw@nvidia.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737639, "id": "ICLR.cc/2018/Conference/-/Paper203/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r1gs9JgRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper203/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper203/Authors|ICLR.cc/2018/Conference/Paper203/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper203/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper203/Authors|ICLR.cc/2018/Conference/Paper203/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper203/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper203/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper203/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper203/Reviewers", "ICLR.cc/2018/Conference/Paper203/Authors", "ICLR.cc/2018/Conference/Paper203/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737639}}}, {"tddate": null, "ddate": null, "tmdate": 1515123395905, "tcdate": 1515123395905, "number": 1, "cdate": 1515123395905, "id": "S1nTzd2Qf", "invitation": "ICLR.cc/2018/Conference/-/Paper203/Official_Comment", "forum": "r1gs9JgRZ", "replyto": "SJQ3bonlG", "signatures": ["ICLR.cc/2018/Conference/Paper203/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper203/Authors"], "content": {"title": "Review Response ", "comment": "Thank you for the review and comments.  The focus of the studies in the paper, as you point out, was to describe and validate the procedure for training with mixed precision without losing accuracy.  Experiments were run with libraries and frameworks that had preliminary support for mixed precision.  As shown in DeepBench (https://github.com/baidu-research/DeepBench), depending on the layer size and batch size, MP hardware can achieve 2~6x speedup for a layer that\u2019s not latency-limited. We will add this mention and pointer to DeepBench results to the paper.  Measuring end to end speedups with more optimized frameworks is the focus for future work.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixed Precision Training", "abstract": "Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.", "pdf": "/pdf/575abe1d433099e0cd046ec3bbcf10eaa1022ec6.pdf", "paperhash": "micikevicius|mixed_precision_training", "_bibtex": "@inproceedings{\nmicikevicius2018mixed,\ntitle={Mixed Precision Training},\nauthor={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1gs9JgRZ},\n}", "authors": ["Paulius Micikevicius", "Sharan Narang", "Jonah Alben", "Gregory Diamos", "Erich Elsen", "David Garcia", "Boris Ginsburg", "Michael Houston", "Oleksii Kuchaiev", "Ganesh Venkatesh", "Hao Wu"], "keywords": ["Half precision", "float16", "Convolutional neural networks", "Recurrent neural networks"], "authorids": ["pauliusm@nvidia.com", "sharan@baidu.com", "alben@nvidia.com", "gdiamos@baidu.com", "eriche@google.com", "dagarcia@nvidia.com", "bginsburg@nvidia.com", "mhouston@nvidia.com", "okuchaiev@nvidia.com", "gavenkatesh@nvidia.com", "skyw@nvidia.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825737639, "id": "ICLR.cc/2018/Conference/-/Paper203/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r1gs9JgRZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper203/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper203/Authors|ICLR.cc/2018/Conference/Paper203/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper203/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper203/Authors|ICLR.cc/2018/Conference/Paper203/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper203/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper203/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper203/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper203/Reviewers", "ICLR.cc/2018/Conference/Paper203/Authors", "ICLR.cc/2018/Conference/Paper203/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825737639}}}, {"tddate": null, "ddate": null, "tmdate": 1512697738105, "tcdate": 1512697719624, "number": 1, "cdate": 1512697719624, "id": "SyetJ_Pbz", "invitation": "ICLR.cc/2018/Conference/-/Paper203/Public_Comment", "forum": "r1gs9JgRZ", "replyto": "SkSMlWcgG", "signatures": ["~Stephen_Merity1"], "readers": ["everyone"], "writers": ["~Stephen_Merity1"], "content": {"title": "Overall advantage is both memory and speed", "comment": "You note in negatives that \"the overall advantage is only a 2x reduction in memory\". The paper notes (though only in the introduction section) that \"Performance (speed) ... is limited by one of three factors: arithmetic bandwidth, memory bandwidth, or latency\", with reduced precision helping two. Specifically, FP16 improves memory bandwidth by only requiring half the data to be shuffled about and that on modern GPUs the FP16 throughput can be 2 to 8 times faster than FP32. Hence, the potential benefit is actually far more than just reducing memory, though the methods and techniques noted in the paper are required in order to have models that can sanely train using FP16."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mixed Precision Training", "abstract": "Increasing the size of a neural network typically improves accuracy but also increases the memory and compute requirements for training the model. We introduce methodology for training deep neural networks using half-precision floating point numbers, without losing model accuracy or having to modify hyper-parameters. This nearly halves memory requirements and, on recent GPUs, speeds up arithmetic. Weights, activations, and gradients are stored in IEEE half-precision format. Since this format has a narrower range than single-precision we propose three techniques for preventing the loss of critical information. Firstly, we recommend maintaining a single-precision copy of weights that accumulates the gradients after each optimizer step (this copy is rounded to half-precision for the forward- and back-propagation). Secondly, we propose loss-scaling to preserve gradient values with small magnitudes. Thirdly, we use half-precision arithmetic that accumulates into single-precision outputs, which are converted to half-precision before storing to memory. We demonstrate that the proposed methodology works across a wide variety of tasks and modern large scale (exceeding 100 million parameters) model architectures, trained on large datasets.", "pdf": "/pdf/575abe1d433099e0cd046ec3bbcf10eaa1022ec6.pdf", "paperhash": "micikevicius|mixed_precision_training", "_bibtex": "@inproceedings{\nmicikevicius2018mixed,\ntitle={Mixed Precision Training},\nauthor={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=r1gs9JgRZ},\n}", "authors": ["Paulius Micikevicius", "Sharan Narang", "Jonah Alben", "Gregory Diamos", "Erich Elsen", "David Garcia", "Boris Ginsburg", "Michael Houston", "Oleksii Kuchaiev", "Ganesh Venkatesh", "Hao Wu"], "keywords": ["Half precision", "float16", "Convolutional neural networks", "Recurrent neural networks"], "authorids": ["pauliusm@nvidia.com", "sharan@baidu.com", "alben@nvidia.com", "gdiamos@baidu.com", "eriche@google.com", "dagarcia@nvidia.com", "bginsburg@nvidia.com", "mhouston@nvidia.com", "okuchaiev@nvidia.com", "gavenkatesh@nvidia.com", "skyw@nvidia.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791691148, "id": "ICLR.cc/2018/Conference/-/Paper203/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "r1gs9JgRZ", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper203/Authors", "ICLR.cc/2018/Conference/Paper203/Reviewers", "ICLR.cc/2018/Conference/Paper203/Area_Chair"], "cdate": 1512791691148}}}], "count": 11}