{"notes": [{"id": "tEFhwX8s1GN", "original": "WTZKFd7q0Zg", "number": 1043, "cdate": 1601308117656, "ddate": null, "tcdate": 1601308117656, "tmdate": 1614985692363, "tddate": null, "forum": "tEFhwX8s1GN", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Training By Vanilla SGD with Larger Learning Rates", "authorids": ["~Yueyao_Yu1", "~Jie_Wang9", "~Wenye_Li1", "~Yin_Zhang4"], "authors": ["Yueyao Yu", "Jie Wang", "Wenye Li", "Yin Zhang"], "keywords": [], "abstract": "The stochastic gradient descent (SGD) method, first proposed in 1950's, has been the foundation for deep-neural-network (DNN) training with numerous enhancements including adding a momentum or adaptively selecting learning rates, or using both strategies and more.  A common view for SGD is that the learning rate should be eventually made small in order to reach sufficiently good approximate solutions.  Another widely held view is that the vanilla SGD is out of fashion in comparison to many of its modern variations.  In this work, we provide a contrarian claim that, when training over-parameterized DNNs, the vanilla SGD can still compete well with, and oftentimes outperform, its more recent variations by simply using learning rates significantly larger than commonly used values.  We establish theoretical results to explain this local convergence behavior of SGD on nonconvex functions, and also present computational evidence, across multiple tasks including image classification, speech recognition and natural language processing, to support the practice of using larger learning rates.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|training_by_vanilla_sgd_with_larger_learning_rates", "supplementary_material": "/attachment/4009b64a2140f32e03816cade7a835e3f731480e.zip", "pdf": "/pdf/87bae18369ebd5602e1f2a5e4aa1fa362fc3c56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1T3P1FxDUd", "_bibtex": "@misc{\nyu2021training,\ntitle={Training By Vanilla {\\{}SGD{\\}} with Larger Learning Rates},\nauthor={Yueyao Yu and Jie Wang and Wenye Li and Yin Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=tEFhwX8s1GN}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "mdCbpAINoL", "original": null, "number": 1, "cdate": 1610040456422, "ddate": null, "tcdate": 1610040456422, "tmdate": 1610474059116, "tddate": null, "forum": "tEFhwX8s1GN", "replyto": "tEFhwX8s1GN", "invitation": "ICLR.cc/2021/Conference/Paper1043/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper primary theoretical contribution claim is to establish the constant size SGD converges linear to the optimal solution in non-convex settings. This is shown in the interpolation regime for over-parametrized situations when starting from points nearby to the optimum. The paper's empirical claim is to use relatively larger learning rates for SGD in common deep learning settings and claim that they can do well. \n\nMy recommendation is based on the overall low scores provided by the reviewers - which did not change post rebuttal. The concerns raised by the reviewers amounting to my decision recommendation is summarized below - \n\nOverall the reviewers found the connection between the theoretical results and the overall claims of the paper unconnected. The reviewers found the theoretical contribution of the local convergence weak - particularly in the context of an analysis of constant learning rates and taking into account existing work on the convex case for such results. Furthermore, the experimental contribution of the paper is incremental as the proposed algorithm is standard with just a larger than typical initial learning rates. This factor is usually searched over during Hyper Parameter sweeps in all the large scale learning setups. In this context, SGDL performing favorably, is an interesting observation but not enough of a contribution. Further the reviewers objected to the fact that SGDL does not connect with the theory presented as SGDL in experiments still uses learning rate schedules.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training By Vanilla SGD with Larger Learning Rates", "authorids": ["~Yueyao_Yu1", "~Jie_Wang9", "~Wenye_Li1", "~Yin_Zhang4"], "authors": ["Yueyao Yu", "Jie Wang", "Wenye Li", "Yin Zhang"], "keywords": [], "abstract": "The stochastic gradient descent (SGD) method, first proposed in 1950's, has been the foundation for deep-neural-network (DNN) training with numerous enhancements including adding a momentum or adaptively selecting learning rates, or using both strategies and more.  A common view for SGD is that the learning rate should be eventually made small in order to reach sufficiently good approximate solutions.  Another widely held view is that the vanilla SGD is out of fashion in comparison to many of its modern variations.  In this work, we provide a contrarian claim that, when training over-parameterized DNNs, the vanilla SGD can still compete well with, and oftentimes outperform, its more recent variations by simply using learning rates significantly larger than commonly used values.  We establish theoretical results to explain this local convergence behavior of SGD on nonconvex functions, and also present computational evidence, across multiple tasks including image classification, speech recognition and natural language processing, to support the practice of using larger learning rates.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|training_by_vanilla_sgd_with_larger_learning_rates", "supplementary_material": "/attachment/4009b64a2140f32e03816cade7a835e3f731480e.zip", "pdf": "/pdf/87bae18369ebd5602e1f2a5e4aa1fa362fc3c56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1T3P1FxDUd", "_bibtex": "@misc{\nyu2021training,\ntitle={Training By Vanilla {\\{}SGD{\\}} with Larger Learning Rates},\nauthor={Yueyao Yu and Jie Wang and Wenye Li and Yin Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=tEFhwX8s1GN}\n}"}, "tags": [], "invitation": {"reply": {"forum": "tEFhwX8s1GN", "replyto": "tEFhwX8s1GN", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040456409, "tmdate": 1610474059101, "id": "ICLR.cc/2021/Conference/Paper1043/-/Decision"}}}, {"id": "5z01jhbUF-L", "original": null, "number": 3, "cdate": 1603923485996, "ddate": null, "tcdate": 1603923485996, "tmdate": 1606770541175, "tddate": null, "forum": "tEFhwX8s1GN", "replyto": "tEFhwX8s1GN", "invitation": "ICLR.cc/2021/Conference/Paper1043/-/Official_Review", "content": {"title": "Reviews for The simpler the better: vanilla sgd revisited", "review": "This paper studies the smooth finite-sum problem under suitable conditions in the non-convex case. They show the necessary condition for the minimizer $x^*$ being a point of attraction, and Theorem 1 provides a sufficient condition for the strong minimizer $x^*$ to be a point of strong attraction with high probability. Based on the results, they introduce a modified SGD algorithm with a large initial learning rate (SGDL), and provide extensive experiments on various popular tasks and models in computer vision, audio recognition and natural language processing. \npros: 1, They give a sufficient condition for the strong minimizer $x^*$ to be a point of strong attraction with high probability. \n2, Extensive experiments are presented to show the effectiveness of SGDL. \n\ncons: 1, In Theorem 2, it is better to show how to choose the $\\epsilon$ explicitly and what linear convergence rate can be achieved, i.e., how small the parameter $\\rho$ can be. \n2, Even though this paper considers the non-convex case, the assumptions seems very restrictive. Assumption A.3 means that in a neighborhood of $x^*$, the objective function is essentially strongly convex. Furthermore, all $\\nabla f_i(x^*)$ need to be zero. \n\nminor comment: In the first inequality of the proof of Remark 2, why the bound is not zero? Since under the condition $\\tau=\\infty$, $||X_k-x^*||$ should be no larger than $\\epsilon$. \n\n---------------------After the rebuttal------\nThe authors partially addressed my concerns. I remain the current score.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1043/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1043/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training By Vanilla SGD with Larger Learning Rates", "authorids": ["~Yueyao_Yu1", "~Jie_Wang9", "~Wenye_Li1", "~Yin_Zhang4"], "authors": ["Yueyao Yu", "Jie Wang", "Wenye Li", "Yin Zhang"], "keywords": [], "abstract": "The stochastic gradient descent (SGD) method, first proposed in 1950's, has been the foundation for deep-neural-network (DNN) training with numerous enhancements including adding a momentum or adaptively selecting learning rates, or using both strategies and more.  A common view for SGD is that the learning rate should be eventually made small in order to reach sufficiently good approximate solutions.  Another widely held view is that the vanilla SGD is out of fashion in comparison to many of its modern variations.  In this work, we provide a contrarian claim that, when training over-parameterized DNNs, the vanilla SGD can still compete well with, and oftentimes outperform, its more recent variations by simply using learning rates significantly larger than commonly used values.  We establish theoretical results to explain this local convergence behavior of SGD on nonconvex functions, and also present computational evidence, across multiple tasks including image classification, speech recognition and natural language processing, to support the practice of using larger learning rates.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|training_by_vanilla_sgd_with_larger_learning_rates", "supplementary_material": "/attachment/4009b64a2140f32e03816cade7a835e3f731480e.zip", "pdf": "/pdf/87bae18369ebd5602e1f2a5e4aa1fa362fc3c56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1T3P1FxDUd", "_bibtex": "@misc{\nyu2021training,\ntitle={Training By Vanilla {\\{}SGD{\\}} with Larger Learning Rates},\nauthor={Yueyao Yu and Jie Wang and Wenye Li and Yin Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=tEFhwX8s1GN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tEFhwX8s1GN", "replyto": "tEFhwX8s1GN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1043/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128464, "tmdate": 1606915788467, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1043/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1043/-/Official_Review"}}}, {"id": "rgjVGmZ8jX", "original": null, "number": 4, "cdate": 1603926193601, "ddate": null, "tcdate": 1603926193601, "tmdate": 1606407454137, "tddate": null, "forum": "tEFhwX8s1GN", "replyto": "tEFhwX8s1GN", "invitation": "ICLR.cc/2021/Conference/Paper1043/-/Official_Review", "content": {"title": "The paper lacks of innovation and theoretical justification for their claiming points.", "review": "Main idea: As a classical and effective optimizer, vanilla SGD can always compete with or even outperform its momentum or adaptive variations when training over-parameterized DNNs. The paper aims to theoretically justify this claim and empirically compare the performance across multiple tasks.\n\n1.\tWhat is exactly the overall advantage or difference between the SGDL and the vanilla SGD? Particularly, in how to choose the stepsize alpha?\n2.\tThere is no theoretical comparisons between SGD and its momentum and adaptive variations. Because the paper claims to theoretically justify the claim that \u201cSGD is better\u201d, can authors point out how they justify it theoretically?\n3.\tRemark 1 that follows theorem 2 gives particular conditions to make Eq.(7) hold. Can authors explain more on how to derive these 3 conditions 8(a)-(8(c)?  Does the proof assume that the initial point is close enough to the optimal solution? How can SGDL globally converge (converge from any starting point)?  \n4.\tIn experiments, what is SGDM? \n5.\tIn experiments, the paper uses decaying learning rate, so a large initial stepsize can quickly decay into a small number, so how does this become an advantage?  SGD has better generalization which has been observed in many prior works.\n6.\tThe paper has some typos, and the meaning of some sentences is puzzling.  For instance, (1) there are multiple uses of N in definition 2; (2) the index used in the paper is not consistent, i=1:N and i=1,\u2026,N are both used in the manuscript and other format has been used in the proofs; (3) Eq.8(b) and the condition of theorem 2 are not consistent.\n\n#####################\nupdate:  I have read authors' response to my comments and also read other reviewers' comments and discussions.  The main concern of my comments is still not clear. I will keep my rating unchanged. \n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1043/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1043/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training By Vanilla SGD with Larger Learning Rates", "authorids": ["~Yueyao_Yu1", "~Jie_Wang9", "~Wenye_Li1", "~Yin_Zhang4"], "authors": ["Yueyao Yu", "Jie Wang", "Wenye Li", "Yin Zhang"], "keywords": [], "abstract": "The stochastic gradient descent (SGD) method, first proposed in 1950's, has been the foundation for deep-neural-network (DNN) training with numerous enhancements including adding a momentum or adaptively selecting learning rates, or using both strategies and more.  A common view for SGD is that the learning rate should be eventually made small in order to reach sufficiently good approximate solutions.  Another widely held view is that the vanilla SGD is out of fashion in comparison to many of its modern variations.  In this work, we provide a contrarian claim that, when training over-parameterized DNNs, the vanilla SGD can still compete well with, and oftentimes outperform, its more recent variations by simply using learning rates significantly larger than commonly used values.  We establish theoretical results to explain this local convergence behavior of SGD on nonconvex functions, and also present computational evidence, across multiple tasks including image classification, speech recognition and natural language processing, to support the practice of using larger learning rates.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|training_by_vanilla_sgd_with_larger_learning_rates", "supplementary_material": "/attachment/4009b64a2140f32e03816cade7a835e3f731480e.zip", "pdf": "/pdf/87bae18369ebd5602e1f2a5e4aa1fa362fc3c56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1T3P1FxDUd", "_bibtex": "@misc{\nyu2021training,\ntitle={Training By Vanilla {\\{}SGD{\\}} with Larger Learning Rates},\nauthor={Yueyao Yu and Jie Wang and Wenye Li and Yin Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=tEFhwX8s1GN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tEFhwX8s1GN", "replyto": "tEFhwX8s1GN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1043/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128464, "tmdate": 1606915788467, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1043/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1043/-/Official_Review"}}}, {"id": "wsVvomSMgTT", "original": null, "number": 2, "cdate": 1605954707679, "ddate": null, "tcdate": 1605954707679, "tmdate": 1605955737161, "tddate": null, "forum": "tEFhwX8s1GN", "replyto": "rgjVGmZ8jX", "invitation": "ICLR.cc/2021/Conference/Paper1043/-/Official_Comment", "content": {"title": "Reply to AnonReviewer3", "comment": "Thanks for your comments. Almost all the points you mentioned are corrected in our new paper. You can refer to the blue highlighted text in the revised paper, and now we will explain the corrected points briefly in the following.\n\n1.SGDL means the vanilla SGD with a relatively large learning rate. The scale of the learning rate for SGDL is discussed in the Appendix and now we add them in the Introduction section. Generally speaking, we say that a learning rate that is 10 times larger than that in SGD+momentum is a relatively large one.\n\n2.For proof:  SGD-CS cannot converge when starting from any initialization point in non-convex functions. In this work, our theoretical justification is to show the local ( last-iterate) convergence of SGD. The motivation is as follows:  once the initial point is bad, and it takes a long distance to a neighborhood around a strong minimizer, then the step size is quite close to 0. We show that there is a scale of constant step size for convergence when it is in the neighborhood. \n\n3.For assumptions: All assumptions are reasonable in some of the applications. The interpolation almost always holds in an over-parameterized DNN, which is useful for the computer vision and natural language model. For 8(c) Assumption(A.3), thanks to Review3\u2019s reminder. We study a regularized problem in our paper by adding an L2 regularization in the objective function $f$ to make the Hessian $H(x^*)$ positive definite, and we also use L2-regularizer in all our experiments.\n\n4.SGDM is SGD with momentum.\n\n5.Decaying the learning rate happens in our experiments, but it does not hurt our definition of large, since the learning rate is still 10 times larger than that in SGD + momentum.  Many prior works have already shown better generalization for the first order optimization in comparison to the adaptive optimization(Adam), but to our understanding, the above experimental results focus on the SGD with momentum, and our paper shows SGD without any modification can still have a quite competitive performance. \n\n6.Sincerely thanks for pointing out some typos. We have already modified it in our new version.\n\n7.unsolved one: For SGD and SGD+momentum, it is difficult to justify SGD is better than SGD + momentum theoretically, which is beyond the scope of this work. However, the extensive experimental results in this paper have already shown that vanilla SGD still can be used and its performance is quite comparable to its modern variants in many cases.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1043/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1043/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training By Vanilla SGD with Larger Learning Rates", "authorids": ["~Yueyao_Yu1", "~Jie_Wang9", "~Wenye_Li1", "~Yin_Zhang4"], "authors": ["Yueyao Yu", "Jie Wang", "Wenye Li", "Yin Zhang"], "keywords": [], "abstract": "The stochastic gradient descent (SGD) method, first proposed in 1950's, has been the foundation for deep-neural-network (DNN) training with numerous enhancements including adding a momentum or adaptively selecting learning rates, or using both strategies and more.  A common view for SGD is that the learning rate should be eventually made small in order to reach sufficiently good approximate solutions.  Another widely held view is that the vanilla SGD is out of fashion in comparison to many of its modern variations.  In this work, we provide a contrarian claim that, when training over-parameterized DNNs, the vanilla SGD can still compete well with, and oftentimes outperform, its more recent variations by simply using learning rates significantly larger than commonly used values.  We establish theoretical results to explain this local convergence behavior of SGD on nonconvex functions, and also present computational evidence, across multiple tasks including image classification, speech recognition and natural language processing, to support the practice of using larger learning rates.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|training_by_vanilla_sgd_with_larger_learning_rates", "supplementary_material": "/attachment/4009b64a2140f32e03816cade7a835e3f731480e.zip", "pdf": "/pdf/87bae18369ebd5602e1f2a5e4aa1fa362fc3c56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1T3P1FxDUd", "_bibtex": "@misc{\nyu2021training,\ntitle={Training By Vanilla {\\{}SGD{\\}} with Larger Learning Rates},\nauthor={Yueyao Yu and Jie Wang and Wenye Li and Yin Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=tEFhwX8s1GN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tEFhwX8s1GN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1043/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1043/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1043/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1043/Authors|ICLR.cc/2021/Conference/Paper1043/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1043/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864337, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1043/-/Official_Comment"}}}, {"id": "4MyY5A40aid", "original": null, "number": 5, "cdate": 1605954919424, "ddate": null, "tcdate": 1605954919424, "tmdate": 1605955103417, "tddate": null, "forum": "tEFhwX8s1GN", "replyto": "FJhwVdZHFb", "invitation": "ICLR.cc/2021/Conference/Paper1043/-/Official_Comment", "content": {"title": "Reply to AnonReviewer2", "comment": "Thanks for your comments. All the points you mentioned are corrected in our new paper. You can refer to the blue highlighted text in the revised paper, and now we will explain the corrected points briefly in the following.\n\n1.Thanks for your comments in the convergence analysis. SGD-CS cannot converge when starting from any initialization point in non-convex functions. In this work, our theoretical justify is to show the last-iterate convergence of SGD. The motivation is as follows: once the initial point is bad, and it takes a long distance to a neighborhood around a strong minimizer, then the step size is quite close to 0. We show that there is a scale of constant step size for convergence when it is in the neighborhood.\n\n2.For assumptions: All assumptions are reasonable in some of the applications. The interpolation almost always holds in an over-parameterized DNN, which is useful for the computer vision and natural language model. For 8(c) Assumption(A.3), thanks to Review3\u2019s reminder. We study a regularized problem in our paper by adding an L2 regularization in the objective function $f$ to make the Hessian $H(x^*)$ positive definite, and we also use L2-regularizer in all our experiments.\n\n3.The connection between the theoretical result and experiments is by a case study in section5, which is a special example of non-convex optimization, and it shows that as long as the learning rate is bounded by a threshold, the larger learning rate, the smaller spectral radius (which means a faster convergence rate). \n\n4.The scale of the learning rate for SGDL is discussed in the Appendix and now we add them in the Introduction section. Generally speaking, we say that a learning rate that is 10 times larger than that in SGD+momentum is a relatively large one. Decaying the learning rate happens in our experiments, but it does not hurt our definition of large, since the learning rate is still 10 times larger than that in SGD + momentum.  Many prior works have already shown better generalization for the first order optimization in comparison to the adaptive optimization(Adam), but to our understanding, the above experimental results focus on the SGD with momentum, and our paper shows SGD without any modification can still have a quite competitive performance.\n\n5. We modified all points raised in the clarity part of your comment.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1043/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1043/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training By Vanilla SGD with Larger Learning Rates", "authorids": ["~Yueyao_Yu1", "~Jie_Wang9", "~Wenye_Li1", "~Yin_Zhang4"], "authors": ["Yueyao Yu", "Jie Wang", "Wenye Li", "Yin Zhang"], "keywords": [], "abstract": "The stochastic gradient descent (SGD) method, first proposed in 1950's, has been the foundation for deep-neural-network (DNN) training with numerous enhancements including adding a momentum or adaptively selecting learning rates, or using both strategies and more.  A common view for SGD is that the learning rate should be eventually made small in order to reach sufficiently good approximate solutions.  Another widely held view is that the vanilla SGD is out of fashion in comparison to many of its modern variations.  In this work, we provide a contrarian claim that, when training over-parameterized DNNs, the vanilla SGD can still compete well with, and oftentimes outperform, its more recent variations by simply using learning rates significantly larger than commonly used values.  We establish theoretical results to explain this local convergence behavior of SGD on nonconvex functions, and also present computational evidence, across multiple tasks including image classification, speech recognition and natural language processing, to support the practice of using larger learning rates.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|training_by_vanilla_sgd_with_larger_learning_rates", "supplementary_material": "/attachment/4009b64a2140f32e03816cade7a835e3f731480e.zip", "pdf": "/pdf/87bae18369ebd5602e1f2a5e4aa1fa362fc3c56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1T3P1FxDUd", "_bibtex": "@misc{\nyu2021training,\ntitle={Training By Vanilla {\\{}SGD{\\}} with Larger Learning Rates},\nauthor={Yueyao Yu and Jie Wang and Wenye Li and Yin Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=tEFhwX8s1GN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tEFhwX8s1GN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1043/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1043/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1043/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1043/Authors|ICLR.cc/2021/Conference/Paper1043/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1043/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864337, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1043/-/Official_Comment"}}}, {"id": "qBMAvRP1vtN", "original": null, "number": 4, "cdate": 1605954813710, "ddate": null, "tcdate": 1605954813710, "tmdate": 1605954813710, "tddate": null, "forum": "tEFhwX8s1GN", "replyto": "eB6a-dDVRhy", "invitation": "ICLR.cc/2021/Conference/Paper1043/-/Official_Comment", "content": {"title": "Reply to AnonReviewer1", "comment": "Thanks for your comments. All the points you mentioned are corrected in our new paper. You can refer to the blue highlighted text in the revised paper, and now we will explain the corrected points briefly in the following.\nFor clarify:\n1.\tWe have added the SGC in our related work now.\n2.\tIn Figure 2, we use vanilla SGD with different learning rates. Yes, there are train and test perplexity results.\n3.\tYes, they are train and test perplexity.\n4.\tWe rewrite the last sentence.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1043/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1043/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training By Vanilla SGD with Larger Learning Rates", "authorids": ["~Yueyao_Yu1", "~Jie_Wang9", "~Wenye_Li1", "~Yin_Zhang4"], "authors": ["Yueyao Yu", "Jie Wang", "Wenye Li", "Yin Zhang"], "keywords": [], "abstract": "The stochastic gradient descent (SGD) method, first proposed in 1950's, has been the foundation for deep-neural-network (DNN) training with numerous enhancements including adding a momentum or adaptively selecting learning rates, or using both strategies and more.  A common view for SGD is that the learning rate should be eventually made small in order to reach sufficiently good approximate solutions.  Another widely held view is that the vanilla SGD is out of fashion in comparison to many of its modern variations.  In this work, we provide a contrarian claim that, when training over-parameterized DNNs, the vanilla SGD can still compete well with, and oftentimes outperform, its more recent variations by simply using learning rates significantly larger than commonly used values.  We establish theoretical results to explain this local convergence behavior of SGD on nonconvex functions, and also present computational evidence, across multiple tasks including image classification, speech recognition and natural language processing, to support the practice of using larger learning rates.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|training_by_vanilla_sgd_with_larger_learning_rates", "supplementary_material": "/attachment/4009b64a2140f32e03816cade7a835e3f731480e.zip", "pdf": "/pdf/87bae18369ebd5602e1f2a5e4aa1fa362fc3c56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1T3P1FxDUd", "_bibtex": "@misc{\nyu2021training,\ntitle={Training By Vanilla {\\{}SGD{\\}} with Larger Learning Rates},\nauthor={Yueyao Yu and Jie Wang and Wenye Li and Yin Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=tEFhwX8s1GN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tEFhwX8s1GN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1043/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1043/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1043/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1043/Authors|ICLR.cc/2021/Conference/Paper1043/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1043/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864337, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1043/-/Official_Comment"}}}, {"id": "4Bzdx08lpBK", "original": null, "number": 3, "cdate": 1605954761642, "ddate": null, "tcdate": 1605954761642, "tmdate": 1605954761642, "tddate": null, "forum": "tEFhwX8s1GN", "replyto": "5z01jhbUF-L", "invitation": "ICLR.cc/2021/Conference/Paper1043/-/Official_Comment", "content": {"title": "Reply to AnonReviewer4", "comment": "Thanks for your comments. All the points you mentioned are corrected in our new paper. You can refer to the blue highlighted text in the revised paper, and now we will explain the corrected points briefly in the following. \n\n1.\tHow to choose $\\epsilon$ was discussed in the Appendix previously, and now we move it into the main text (Theorem 2).\n2.\tFor assumptions: All assumptions are reasonable in some of the applications. The interpolation almost always holds in an over-parameterized DNN, which is useful for the computer vision and natural language model. For 8(c) Assumption(A.3), thanks to Review3\u2019s reminder. We study a regularized problem in our paper by adding an L2 regularization in the objective function $f$ to make the Hessian $H(x^*)$ positive definite, and we also use L2-regularizer in all our experiments.\n3.\tThanks for your minor comment. Now we corrected the proof of Remark 2 and the probability is at least $1-\\delta$.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1043/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1043/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training By Vanilla SGD with Larger Learning Rates", "authorids": ["~Yueyao_Yu1", "~Jie_Wang9", "~Wenye_Li1", "~Yin_Zhang4"], "authors": ["Yueyao Yu", "Jie Wang", "Wenye Li", "Yin Zhang"], "keywords": [], "abstract": "The stochastic gradient descent (SGD) method, first proposed in 1950's, has been the foundation for deep-neural-network (DNN) training with numerous enhancements including adding a momentum or adaptively selecting learning rates, or using both strategies and more.  A common view for SGD is that the learning rate should be eventually made small in order to reach sufficiently good approximate solutions.  Another widely held view is that the vanilla SGD is out of fashion in comparison to many of its modern variations.  In this work, we provide a contrarian claim that, when training over-parameterized DNNs, the vanilla SGD can still compete well with, and oftentimes outperform, its more recent variations by simply using learning rates significantly larger than commonly used values.  We establish theoretical results to explain this local convergence behavior of SGD on nonconvex functions, and also present computational evidence, across multiple tasks including image classification, speech recognition and natural language processing, to support the practice of using larger learning rates.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|training_by_vanilla_sgd_with_larger_learning_rates", "supplementary_material": "/attachment/4009b64a2140f32e03816cade7a835e3f731480e.zip", "pdf": "/pdf/87bae18369ebd5602e1f2a5e4aa1fa362fc3c56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1T3P1FxDUd", "_bibtex": "@misc{\nyu2021training,\ntitle={Training By Vanilla {\\{}SGD{\\}} with Larger Learning Rates},\nauthor={Yueyao Yu and Jie Wang and Wenye Li and Yin Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=tEFhwX8s1GN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "tEFhwX8s1GN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1043/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1043/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1043/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1043/Authors|ICLR.cc/2021/Conference/Paper1043/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1043/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864337, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1043/-/Official_Comment"}}}, {"id": "FJhwVdZHFb", "original": null, "number": 1, "cdate": 1602788318061, "ddate": null, "tcdate": 1602788318061, "tmdate": 1605024544643, "tddate": null, "forum": "tEFhwX8s1GN", "replyto": "tEFhwX8s1GN", "invitation": "ICLR.cc/2021/Conference/Paper1043/-/Official_Review", "content": {"title": "Weak analysis, strong and unrealisitc assumption, and unsupported claims", "review": "This paper investigates the SGD with constant step size (SGD-CS) on non-conex optimization problems. Theoretically, the paper shows the conditions under which a minimizer $x^*$ is a point of attraction in a local neighborhood under the algorithm SGD-CS with sufficiently small step-size. Furthermore, the paper experimentally shows that vanilla SGD-CS with relatively large step-size performs well, or even outperforms its momentum and/or adaptive counterparts, on several popular deep learning tasks.\n\n[Comments]:\n\n1: Claim and analysis are not consistent. The authors claim in the introduction that convergence of SGD-CS on non-convex functions is shown. However, the analysis only focuses on points of attraction and the stay within a local neighborhood. I would like to point out that the latter concepts are not equivalent to convergence. To show convergence of an algorithm, the missing piece of the paper is that, starting from the initialization point, the algorithm can be guaranteed to find such a local neighborhood around the point of attraction. Without this guarantee, the analysis of points of attraction is meaningless in the sense of algorithm convergence. Hence, I disagree that convergence of SGD-CS is theoretically shown in the paper.\n\n2: Considering the necessary condition for points of attraction, Theorem 1, the assumption A.3 barely holds true in practical cases. As stated in Theorem 1, $x^*$ being a point of attraction implies interpolation property. For most real world tasks, interpolation can be only achieved when the model is over-parameterized, i.e., number of parameters is greater than the number of data samples. (For example, consider solving a system of linear equations). As pointed out by the work [Liu et al. 2020], most of the minimizers are not isolated, instead they form a low-dimensional manifold. In this case, none of the minimizers satisfies Assumption A.3, because the Hessian matrice H at the minimizers always have zero-eigenvalues (flat directions).\n\n3: The main theoretical result, Theorem 2, relies on the fact that step-size is sufficiently small. However, one of the main claims of the paper is the convergence under large step size, as discussed in Section 5 and experimented in Section 6. I don\u2019t see the connection between the small step-size theoretical result and the large step-size experiments. The theory seems not to explain the experiments.\n\n4: The paper frequently talks about large learning rates. However, it is not clear to me what is the criteria to be large or small. Especially, in section 5, the paper provides a certain range of step size values (e.g., step size $t \\in (0, 1/\\lambda_{max}(H))$, within which the SGD-CS converges on a few simple examples. What are the reasons to claim these step-sizes are large?\n\n[About clarity]:\n\n1: It should be reader friendly to enlarge some of the figures.\n\n2: Providing an intuition of the error function, defined in Eq.(4), should be helpful.\n\n3: Notations can be improved.\n\n[References]:\n\n[Liu et al. 2020] Liu, Zhu, and Belkin. Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning. arXiv:2003.00307.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1043/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1043/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training By Vanilla SGD with Larger Learning Rates", "authorids": ["~Yueyao_Yu1", "~Jie_Wang9", "~Wenye_Li1", "~Yin_Zhang4"], "authors": ["Yueyao Yu", "Jie Wang", "Wenye Li", "Yin Zhang"], "keywords": [], "abstract": "The stochastic gradient descent (SGD) method, first proposed in 1950's, has been the foundation for deep-neural-network (DNN) training with numerous enhancements including adding a momentum or adaptively selecting learning rates, or using both strategies and more.  A common view for SGD is that the learning rate should be eventually made small in order to reach sufficiently good approximate solutions.  Another widely held view is that the vanilla SGD is out of fashion in comparison to many of its modern variations.  In this work, we provide a contrarian claim that, when training over-parameterized DNNs, the vanilla SGD can still compete well with, and oftentimes outperform, its more recent variations by simply using learning rates significantly larger than commonly used values.  We establish theoretical results to explain this local convergence behavior of SGD on nonconvex functions, and also present computational evidence, across multiple tasks including image classification, speech recognition and natural language processing, to support the practice of using larger learning rates.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|training_by_vanilla_sgd_with_larger_learning_rates", "supplementary_material": "/attachment/4009b64a2140f32e03816cade7a835e3f731480e.zip", "pdf": "/pdf/87bae18369ebd5602e1f2a5e4aa1fa362fc3c56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1T3P1FxDUd", "_bibtex": "@misc{\nyu2021training,\ntitle={Training By Vanilla {\\{}SGD{\\}} with Larger Learning Rates},\nauthor={Yueyao Yu and Jie Wang and Wenye Li and Yin Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=tEFhwX8s1GN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tEFhwX8s1GN", "replyto": "tEFhwX8s1GN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1043/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128464, "tmdate": 1606915788467, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1043/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1043/-/Official_Review"}}}, {"id": "eB6a-dDVRhy", "original": null, "number": 2, "cdate": 1603914933430, "ddate": null, "tcdate": 1603914933430, "tmdate": 1605024544575, "tddate": null, "forum": "tEFhwX8s1GN", "replyto": "tEFhwX8s1GN", "invitation": "ICLR.cc/2021/Conference/Paper1043/-/Official_Review", "content": {"title": "interesting (albeit a bit disconnected) theoretical and empirical results", "review": "This paper presents a theoretical analysis of SGD with constant step size (SGD-CS) and presents conditions under which SGD-CS leads to parameter updates that converge to a local minima, including parameters of non-convex functions.  The authors then show, in context of some special functions, that the step size can be fairly large yet convergence is achieved.  This is followed by a number of empirical studies of SGD with large (but annealed) step-size (SGDL) on a variety of tasks.\n\nI find the connection of SGD-CS with SGDL tenuous, and it is not clear that the theoretical analysis helps in selecting largest possible step size.  However, I do find the following contributions of the paper valuable:\na) Analysis of SGD-CS sheds lights on conditions under which the minimizers (local) of objective functions act as attractor (or strong attractor) of SGD updates.  This is worth sharing.\nb) The empirical results with SGDL show a very consistent pattern of SGDL outperforming other optimization approaches (including ADAM, SGD with momentum, etc.).  While this is not really SGD-CS, I find the consistent behavior of SGDL worth noting and sharing.\n\nSome clarifications and minor typographical errors:\n* SGC in last sentence of Section 3.1 is not defined.\n* Section 3.2 assumptions A.1 and A.2 should refer to Eq. (1b) and not (1a) I think\n* In Figure 2 which optimization approach is used to derive the curves?\n* In Fig. 6, the two curves in (a) and two in (b) \u2026 are they train and test perplexity results?\n* Last sentence of conclusions is unclear, please restate.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1043/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1043/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training By Vanilla SGD with Larger Learning Rates", "authorids": ["~Yueyao_Yu1", "~Jie_Wang9", "~Wenye_Li1", "~Yin_Zhang4"], "authors": ["Yueyao Yu", "Jie Wang", "Wenye Li", "Yin Zhang"], "keywords": [], "abstract": "The stochastic gradient descent (SGD) method, first proposed in 1950's, has been the foundation for deep-neural-network (DNN) training with numerous enhancements including adding a momentum or adaptively selecting learning rates, or using both strategies and more.  A common view for SGD is that the learning rate should be eventually made small in order to reach sufficiently good approximate solutions.  Another widely held view is that the vanilla SGD is out of fashion in comparison to many of its modern variations.  In this work, we provide a contrarian claim that, when training over-parameterized DNNs, the vanilla SGD can still compete well with, and oftentimes outperform, its more recent variations by simply using learning rates significantly larger than commonly used values.  We establish theoretical results to explain this local convergence behavior of SGD on nonconvex functions, and also present computational evidence, across multiple tasks including image classification, speech recognition and natural language processing, to support the practice of using larger learning rates.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yu|training_by_vanilla_sgd_with_larger_learning_rates", "supplementary_material": "/attachment/4009b64a2140f32e03816cade7a835e3f731480e.zip", "pdf": "/pdf/87bae18369ebd5602e1f2a5e4aa1fa362fc3c56d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=1T3P1FxDUd", "_bibtex": "@misc{\nyu2021training,\ntitle={Training By Vanilla {\\{}SGD{\\}} with Larger Learning Rates},\nauthor={Yueyao Yu and Jie Wang and Wenye Li and Yin Zhang},\nyear={2021},\nurl={https://openreview.net/forum?id=tEFhwX8s1GN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "tEFhwX8s1GN", "replyto": "tEFhwX8s1GN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1043/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538128464, "tmdate": 1606915788467, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1043/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1043/-/Official_Review"}}}], "count": 10}