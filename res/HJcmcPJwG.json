{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124462042, "tcdate": 1518463521705, "number": 226, "cdate": 1518463521705, "id": "HJcmcPJwG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "HJcmcPJwG", "signatures": ["~Sungyoon_Lee1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Defensive denoising methods against adversarial attack", "abstract": " Deep neural networks are highly vulnerable to adversarial examples. An adversarial example is an image with small perturbation designed to make the networks missclassify it. In this paper, we propose two defensive methods. First, we use denoising methods using ROF model and NL-means model before classification to remove adversarial noise. Second, we perturb images in certain directions to escape from the adversarial area. Experiments on the universal adversarial perturbations(UAP) show that proposed methods can remove adversarial noise and perform better classification.", "paperhash": "lee|defensive_denoising_methods_against_adversarial_attack", "_bibtex": "@misc{\n  lee2018defensive,\n  title={Defensive denoising methods against adversarial attack},\n  author={Sungyoon Lee and Jaewook Lee},\n  year={2018},\n  url={https://openreview.net/forum?id=HJcmcPJwG}\n}", "authorids": ["goman1934@snu.ac.kr", "jaewook@snu.ac.kr"], "authors": ["Sungyoon Lee", "Jaewook Lee"], "keywords": ["Deep Learning", "Adversarial attack"], "pdf": "/pdf/25aa126ba11db829456e42d8b6c74309ab6d3080.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582639036, "tcdate": 1520809512808, "number": 1, "cdate": 1520809512808, "id": "r1WEI4QKf", "invitation": "ICLR.cc/2018/Workshop/-/Paper226/Official_Review", "forum": "HJcmcPJwG", "replyto": "HJcmcPJwG", "signatures": ["ICLR.cc/2018/Workshop/Paper226/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper226/AnonReviewer3"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "In this paper, the authors consider the use of denoising methods for defense against adversarial attack. \n\nThe attack is obtained by adding noise using universal adversarial perturbations. This is defended against by considering classical denoising methods of ROF TV norm based denoising and NL means based denoising. \n\nThe authors also consider the case of adding a small amount of noise that is obtained through training can after denoising be added to further improve the performance.\n\nThorough comparison would perhaps benefit by considering which denoising methods adversely affect the method. One question is whether any such denoising be valid and considering a systematic evaluation for these. Another question is how sensitive is the method to the selection of parameters. Some analysis on this would also be relevant.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive denoising methods against adversarial attack", "abstract": " Deep neural networks are highly vulnerable to adversarial examples. An adversarial example is an image with small perturbation designed to make the networks missclassify it. In this paper, we propose two defensive methods. First, we use denoising methods using ROF model and NL-means model before classification to remove adversarial noise. Second, we perturb images in certain directions to escape from the adversarial area. Experiments on the universal adversarial perturbations(UAP) show that proposed methods can remove adversarial noise and perform better classification.", "paperhash": "lee|defensive_denoising_methods_against_adversarial_attack", "_bibtex": "@misc{\n  lee2018defensive,\n  title={Defensive denoising methods against adversarial attack},\n  author={Sungyoon Lee and Jaewook Lee},\n  year={2018},\n  url={https://openreview.net/forum?id=HJcmcPJwG}\n}", "authorids": ["goman1934@snu.ac.kr", "jaewook@snu.ac.kr"], "authors": ["Sungyoon Lee", "Jaewook Lee"], "keywords": ["Deep Learning", "Adversarial attack"], "pdf": "/pdf/25aa126ba11db829456e42d8b6c74309ab6d3080.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582638861, "id": "ICLR.cc/2018/Workshop/-/Paper226/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper226/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper226/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper226/AnonReviewer1"], "reply": {"forum": "HJcmcPJwG", "replyto": "HJcmcPJwG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper226/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper226/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582638861}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582631003, "tcdate": 1520818400500, "number": 2, "cdate": 1520818400500, "id": "HkOkFUmYz", "invitation": "ICLR.cc/2018/Workshop/-/Paper226/Official_Review", "forum": "HJcmcPJwG", "replyto": "HJcmcPJwG", "signatures": ["ICLR.cc/2018/Workshop/Paper226/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper226/AnonReviewer1"], "content": {"title": "Filtering Adversarial Examples", "rating": "4: Ok but not good enough - rejection", "review": "This work proposes a smoothing operation, effectively a low pass filter, on adversarial examples. The experimental evidence are scarce and unconvincing. There is no theoretical justification for why this would work. The manuscript is poorly written.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive denoising methods against adversarial attack", "abstract": " Deep neural networks are highly vulnerable to adversarial examples. An adversarial example is an image with small perturbation designed to make the networks missclassify it. In this paper, we propose two defensive methods. First, we use denoising methods using ROF model and NL-means model before classification to remove adversarial noise. Second, we perturb images in certain directions to escape from the adversarial area. Experiments on the universal adversarial perturbations(UAP) show that proposed methods can remove adversarial noise and perform better classification.", "paperhash": "lee|defensive_denoising_methods_against_adversarial_attack", "_bibtex": "@misc{\n  lee2018defensive,\n  title={Defensive denoising methods against adversarial attack},\n  author={Sungyoon Lee and Jaewook Lee},\n  year={2018},\n  url={https://openreview.net/forum?id=HJcmcPJwG}\n}", "authorids": ["goman1934@snu.ac.kr", "jaewook@snu.ac.kr"], "authors": ["Sungyoon Lee", "Jaewook Lee"], "keywords": ["Deep Learning", "Adversarial attack"], "pdf": "/pdf/25aa126ba11db829456e42d8b6c74309ab6d3080.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582638861, "id": "ICLR.cc/2018/Workshop/-/Paper226/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper226/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper226/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper226/AnonReviewer1"], "reply": {"forum": "HJcmcPJwG", "replyto": "HJcmcPJwG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper226/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper226/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582638861}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573587021, "tcdate": 1521573587021, "number": 188, "cdate": 1521573586681, "id": "S1iA0RCKG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "HJcmcPJwG", "replyto": "HJcmcPJwG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive denoising methods against adversarial attack", "abstract": " Deep neural networks are highly vulnerable to adversarial examples. An adversarial example is an image with small perturbation designed to make the networks missclassify it. In this paper, we propose two defensive methods. First, we use denoising methods using ROF model and NL-means model before classification to remove adversarial noise. Second, we perturb images in certain directions to escape from the adversarial area. Experiments on the universal adversarial perturbations(UAP) show that proposed methods can remove adversarial noise and perform better classification.", "paperhash": "lee|defensive_denoising_methods_against_adversarial_attack", "_bibtex": "@misc{\n  lee2018defensive,\n  title={Defensive denoising methods against adversarial attack},\n  author={Sungyoon Lee and Jaewook Lee},\n  year={2018},\n  url={https://openreview.net/forum?id=HJcmcPJwG}\n}", "authorids": ["goman1934@snu.ac.kr", "jaewook@snu.ac.kr"], "authors": ["Sungyoon Lee", "Jaewook Lee"], "keywords": ["Deep Learning", "Adversarial attack"], "pdf": "/pdf/25aa126ba11db829456e42d8b6c74309ab6d3080.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}