{"notes": [{"id": "HJxwvCEFvH", "original": "r1lNXODuvr", "number": 1178, "cdate": 1569439326976, "ddate": null, "tcdate": 1569439326976, "tmdate": 1577168214444, "tddate": null, "forum": "HJxwvCEFvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "SPECTRA: Sparse Entity-centric Transitions", "authors": ["Rim Assouel", "Yoshua Bengio"], "authorids": ["rim.assouel@hotmail.fr", "yoshua.bengio@mila.quebec"], "keywords": ["representation learning", "slot-structured representations", "sparse slot-structured transitions", "entity-centric representation", "unsupervised learning", "object-centric"], "TL;DR": "Sparse slot-structured transition model. Training is done such that such that latent slots correspond to relevant entities of the visual scene.", "abstract": "Learning an agent that interacts with objects is ubiquituous in many RL tasks. In most of them the  agent's actions have sparse effects : only a small subset of objects in the visual scene will be affected by the action taken. We introduce SPECTRA, a model for learning slot-structured transitions from raw visual observations that embodies this sparsity assumption. Our model is composed of a perception module that decomposes the visual scene into a set of  latent objects representations (i.e. slot-structured) and a transition module that predicts the next latent set slot-wise and in a sparse way. We show that learning a perception module jointly with a sparse slot-structured transition model not only  biases the model towards more entity-centric perceptual groupings  but also enables intrinsic exploration strategy that aims at maximizing the number of objects changed in the agent\u2019s trajectory.", "pdf": "/pdf/b4270b052a3a351200bd7d14c48b2b830b3e349f.pdf", "paperhash": "assouel|spectra_sparse_entitycentric_transitions", "original_pdf": "/attachment/b4270b052a3a351200bd7d14c48b2b830b3e349f.pdf", "_bibtex": "@misc{\nassouel2020spectra,\ntitle={{\\{}SPECTRA{\\}}: Sparse Entity-centric Transitions},\nauthor={Rim Assouel and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxwvCEFvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "knAaaNAuZ", "original": null, "number": 1, "cdate": 1576798716597, "ddate": null, "tcdate": 1576798716597, "tmdate": 1576800919922, "tddate": null, "forum": "HJxwvCEFvH", "replyto": "HJxwvCEFvH", "invitation": "ICLR.cc/2020/Conference/Paper1178/-/Decision", "content": {"decision": "Reject", "comment": "This paper introduces a model that learns a slot-based representation and its transition model to predict the representation changes over time. While all the reviewers agree that this paper is focusing on an important problem, they expressed multiple concerns regarding the novelty of the approach as well as lacking experiments. It certainly is missing multiple important relevant works, thereby overclaiming at a few places. The authors provided a short general response to compare their approach with some of the previous works and conduct stronger experiments for a future submission. We believe this paper is not at the stage to be published at this point.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SPECTRA: Sparse Entity-centric Transitions", "authors": ["Rim Assouel", "Yoshua Bengio"], "authorids": ["rim.assouel@hotmail.fr", "yoshua.bengio@mila.quebec"], "keywords": ["representation learning", "slot-structured representations", "sparse slot-structured transitions", "entity-centric representation", "unsupervised learning", "object-centric"], "TL;DR": "Sparse slot-structured transition model. Training is done such that such that latent slots correspond to relevant entities of the visual scene.", "abstract": "Learning an agent that interacts with objects is ubiquituous in many RL tasks. In most of them the  agent's actions have sparse effects : only a small subset of objects in the visual scene will be affected by the action taken. We introduce SPECTRA, a model for learning slot-structured transitions from raw visual observations that embodies this sparsity assumption. Our model is composed of a perception module that decomposes the visual scene into a set of  latent objects representations (i.e. slot-structured) and a transition module that predicts the next latent set slot-wise and in a sparse way. We show that learning a perception module jointly with a sparse slot-structured transition model not only  biases the model towards more entity-centric perceptual groupings  but also enables intrinsic exploration strategy that aims at maximizing the number of objects changed in the agent\u2019s trajectory.", "pdf": "/pdf/b4270b052a3a351200bd7d14c48b2b830b3e349f.pdf", "paperhash": "assouel|spectra_sparse_entitycentric_transitions", "original_pdf": "/attachment/b4270b052a3a351200bd7d14c48b2b830b3e349f.pdf", "_bibtex": "@misc{\nassouel2020spectra,\ntitle={{\\{}SPECTRA{\\}}: Sparse Entity-centric Transitions},\nauthor={Rim Assouel and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxwvCEFvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJxwvCEFvH", "replyto": "HJxwvCEFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727502, "tmdate": 1576800279756, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1178/-/Decision"}}}, {"id": "HylRsVvnjr", "original": null, "number": 1, "cdate": 1573840038416, "ddate": null, "tcdate": 1573840038416, "tmdate": 1573840038416, "tddate": null, "forum": "HJxwvCEFvH", "replyto": "HJxwvCEFvH", "invitation": "ICLR.cc/2020/Conference/Paper1178/-/Official_Comment", "content": {"title": "Thanks", "comment": "We would like to thank the reviewers for all their useful comments. We especially agree with Reviewer 2 regarding the prior work we claim to improve upon (representation for some RL downstream tasks vs accuracy of the forward model learned). We will come with a stronger body of experiments for a future submission. We also thank Reviewer 3 for his very detailed comments on all the Figures in the draft and Reviewer 1 for pointing out some missing related work. However regarding those, we were not aware of [C] until they released the arXiv version a few days ago, [B] uses ground truth objects segmentations to learn object-centric representations, [D] doesn\u2019t seem to have a bottleneck for each object and [E] seems to use high-level representations of the blocks and thus doesn't focus on representation learning."}, "signatures": ["ICLR.cc/2020/Conference/Paper1178/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1178/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SPECTRA: Sparse Entity-centric Transitions", "authors": ["Rim Assouel", "Yoshua Bengio"], "authorids": ["rim.assouel@hotmail.fr", "yoshua.bengio@mila.quebec"], "keywords": ["representation learning", "slot-structured representations", "sparse slot-structured transitions", "entity-centric representation", "unsupervised learning", "object-centric"], "TL;DR": "Sparse slot-structured transition model. Training is done such that such that latent slots correspond to relevant entities of the visual scene.", "abstract": "Learning an agent that interacts with objects is ubiquituous in many RL tasks. In most of them the  agent's actions have sparse effects : only a small subset of objects in the visual scene will be affected by the action taken. We introduce SPECTRA, a model for learning slot-structured transitions from raw visual observations that embodies this sparsity assumption. Our model is composed of a perception module that decomposes the visual scene into a set of  latent objects representations (i.e. slot-structured) and a transition module that predicts the next latent set slot-wise and in a sparse way. We show that learning a perception module jointly with a sparse slot-structured transition model not only  biases the model towards more entity-centric perceptual groupings  but also enables intrinsic exploration strategy that aims at maximizing the number of objects changed in the agent\u2019s trajectory.", "pdf": "/pdf/b4270b052a3a351200bd7d14c48b2b830b3e349f.pdf", "paperhash": "assouel|spectra_sparse_entitycentric_transitions", "original_pdf": "/attachment/b4270b052a3a351200bd7d14c48b2b830b3e349f.pdf", "_bibtex": "@misc{\nassouel2020spectra,\ntitle={{\\{}SPECTRA{\\}}: Sparse Entity-centric Transitions},\nauthor={Rim Assouel and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxwvCEFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJxwvCEFvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1178/Authors", "ICLR.cc/2020/Conference/Paper1178/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1178/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1178/Reviewers", "ICLR.cc/2020/Conference/Paper1178/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1178/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1178/Authors|ICLR.cc/2020/Conference/Paper1178/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504160033, "tmdate": 1576860561343, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1178/Authors", "ICLR.cc/2020/Conference/Paper1178/Reviewers", "ICLR.cc/2020/Conference/Paper1178/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1178/-/Official_Comment"}}}, {"id": "SkeIrpvpKH", "original": null, "number": 1, "cdate": 1571810621597, "ddate": null, "tcdate": 1571810621597, "tmdate": 1572972502569, "tddate": null, "forum": "HJxwvCEFvH", "replyto": "HJxwvCEFvH", "invitation": "ICLR.cc/2020/Conference/Paper1178/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a model that learns to disentangle visual scene into objects (slots), and simultaneously learns a dynamics model to capture how these objects interact with each other.  The authors demonstrated that the proposed model has the potentials to discover objects without supervision, and also enables an exploration strategy that maximizes the number of objects changed in the agents' trajectories.\n\nThis paper is for sure studying an important problem. The approach presented in the manuscript makes intuitive sense. The experimental results are reasonable but can be strengthened. However, I was shocked that the authors seem to be unaware of the abundant related work in this area (see below). I'd like to see the authors' responses regarding the missing related work. As of now, my recommendation is a clear reject.\n\nAs the authors mentioned, objects play a key role in human perception, and the problem of discovering objects from visual input is for sure an important problem. I commend the authors for pursuing research in this direction.\n\nThe experimental evaluations are however a little limited: it's restricted to games, where the visual appearance of objects is almost identical. In those cases, it'd be hard to access how the model may generalize to real-world data, where object appearances and texture can be complex. There are also no comparisons with published, SOTA methods.\n\nThe major problem of this manuscript, to me, is its ignorance of related work and, therefore, overclaiming at a few places. Most importantly, I suggest the authors cite, discuss, and ideally compare with many related works from Josh Tenenbaum's group and Sergey Levine's group. A few papers listed below, in particular [A] and [B], also formulated the problem in a similar fashion, where they decompose the scene into object-centric representations and learn the interactions among objects. [B] and [C] also explored how the model can be used in an RL setting. Further, [D] studied learning an object-oriented dynamics predictor in a similar context as presented in this paper. \n\nThe sparse effects have also been explored by Xia et al. [E]. Similarly, the work has built upon object-centric representations. I, therefore, wonder how the proposed method compares with all those published papers, both at the conceptual level and at the experimental level.\n\n[A] Wu et al. Learning to See Physics via Visual De-animation. NeurIPS 2017\n[B] Janner et al. Reasoning About Physical Interactions with Object-Oriented Prediction and Planning. ICLR 2019\n[C] Co-Reyes et al. Discovering, Predicting, and Planning with Objects. ICML 2019 Workshop, https://sites.google.com/view/dpppo/\n[D] Zhu et al. Object-Oriented Dynamics Predictor. NeurIPS 2018\n[E] Xia et al. Learning sparse relational transition models. ICLR 2019\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1178/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1178/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SPECTRA: Sparse Entity-centric Transitions", "authors": ["Rim Assouel", "Yoshua Bengio"], "authorids": ["rim.assouel@hotmail.fr", "yoshua.bengio@mila.quebec"], "keywords": ["representation learning", "slot-structured representations", "sparse slot-structured transitions", "entity-centric representation", "unsupervised learning", "object-centric"], "TL;DR": "Sparse slot-structured transition model. Training is done such that such that latent slots correspond to relevant entities of the visual scene.", "abstract": "Learning an agent that interacts with objects is ubiquituous in many RL tasks. In most of them the  agent's actions have sparse effects : only a small subset of objects in the visual scene will be affected by the action taken. We introduce SPECTRA, a model for learning slot-structured transitions from raw visual observations that embodies this sparsity assumption. Our model is composed of a perception module that decomposes the visual scene into a set of  latent objects representations (i.e. slot-structured) and a transition module that predicts the next latent set slot-wise and in a sparse way. We show that learning a perception module jointly with a sparse slot-structured transition model not only  biases the model towards more entity-centric perceptual groupings  but also enables intrinsic exploration strategy that aims at maximizing the number of objects changed in the agent\u2019s trajectory.", "pdf": "/pdf/b4270b052a3a351200bd7d14c48b2b830b3e349f.pdf", "paperhash": "assouel|spectra_sparse_entitycentric_transitions", "original_pdf": "/attachment/b4270b052a3a351200bd7d14c48b2b830b3e349f.pdf", "_bibtex": "@misc{\nassouel2020spectra,\ntitle={{\\{}SPECTRA{\\}}: Sparse Entity-centric Transitions},\nauthor={Rim Assouel and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxwvCEFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxwvCEFvH", "replyto": "HJxwvCEFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1178/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1178/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575675118974, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1178/Reviewers"], "noninvitees": [], "tcdate": 1570237741206, "tmdate": 1575675118989, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1178/-/Official_Review"}}}, {"id": "ByeZ0fDAFB", "original": null, "number": 2, "cdate": 1571873481217, "ddate": null, "tcdate": 1571873481217, "tmdate": 1572972502522, "tddate": null, "forum": "HJxwvCEFvH", "replyto": "HJxwvCEFvH", "invitation": "ICLR.cc/2020/Conference/Paper1178/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper introduces a model that learns a slot-based representation, along with a transition model to predict the evolution of these representations in a sparse fashion, all in a fully unsupervised way. This is done by leveraging a self-attention mechanism to decide which slots should be updated in a given transition, leaving the others untouched. The model learns to encode in a slot-wise and is trained on single step transitions.\n\nThis work tackles an important problem, and is very well motivated and presented in a very clear fashion. It reuses some known ideas and components, but combines them in a nice way. I especially liked the use of attention to select what to update, which is a good prior to have.\n\nHowever, the results presented unfortunately seem to fall a bit short in this current version, and some decisions might have had too much of an effect on some of these shortcomings. Given some improvements, this work might become quite promising, but for the time being I am leaning against publication.\n\n1.\tMost modeling decisions are clear and well-motivated, however the choice to make the transition model f_trans always be applied only \u201cslot-wise\u201d might be too restrictive. Indeed, for a given action, this means that 2 slots have to independently learn the effect of that action (e.g. as shown in the example in Figure 2. right), and that some interactions are ~impossible to learn (e.g. in Sokoban, pushing a box requires knowing about the location of both the agent and the box). This could have been alleviated if the transition had access to \u201cinteractions outcomes\u201d (e.g. if using a GraphNet, or in your model, if \\tilde{s} was provided to the transition function f_theta). Other works (including Zambaldi et al 2018, which is cited several times), handle this appropriately.\u2028Did you try to provide \\tilde{s} to the transformation operator (e.g. in Figure 7 left)?\n2.\tAdding a direct comparison to pure non-slotted versions of the model/baselines would have been quite useful, as currently it is unclear why certain things are failing.\n3.\tSimilarly, finding what was the output of the CNN encoder for pixel inputs was a bit too difficult. It is explained in the Appendix that one maps into 4x4 feature maps, but that might be too large for the current environments? Indeed for Sokoban, this means that any grid is partially supported by several \u201cslots\u201d, which may hurt the results more than they should (especially combined with the slot-wise transition constraints expressed above).\n4.\tThe early state of the current results are quite visible in all examples of the \u201cSeparate training\u201d model predictions (Figure 3, 5, 8, 9, 10 and 11). None of these actually show this model performing a \u201ccorrect\u201d prediction for t+1? They only predict no changes, or nonsensical interpolations\u2026 This is not sufficient to try to make an argument about the \u201cjoint training\u201d helping, and most discussions about \u201cwhat information they contain\u201d is strenuous at best.\n5.\tThe paper keeps mentioning that it \u201cimplicitly imposes transitions to be sparse\u201d, however it is never explained how that would come about? I understand that the softmax in the self-attention may tend to become \u201cpeaky\u201d and hence only affect a few slots (and the results do seem to confirm this observation), but I was expecting an explicit loss to enforce this fact. The current emphasis seems a bit ill-funded, so I would present more evidence to it or downplay it.\n6.\tSome of the results shown seem hard to interpret or provide only weak evidence for the proposed model:\n\ta.\tFigure 2. Left does seem to indicate a benefit in using the self-attention module, but it is hard to know how much of an effect the gap between the orange and red curves actually imply. This figure is overall a bit too small to interpret, and it might be better to split the 2 conditions into sub-plots. The names of the curves in the legend do not correspond to anything described in the text/caption (but I could understand them\u2026).\u2028I was expecting more discussion of the results in Figure 2, for example at the end of Section 4.1.\n\tb.\tAs explained above, Figure 3 only shows that the Joint Training can perform a 1-step prediction, which is ok but is the bare minimum.\u2028Does it handle multi-step rollouts?\n\tc.\tFigure 4 does not seem to provide any significant results or insights. I would interpret them as showing no significant difference between the curves, and they are too small to extract any information out of them. I would remove this figure fully.\n\td.\tFigure 5 is unclear about what f_k=0 really is, and once again just shows that Separate training does nothing. I expected it to be exactly reconstructing s_t (given that the others are trying to predict s_t+1)? But this is only the case for the joint training, and without knowing what x_t actually was, it is hard to trust. The fact that f_k changes what it does as it is being increased makes the whole point hard to interpret.\n7.\tFigure 6 was quite interesting, and I feel like this could be pushed forward in a quite interesting manner. The choice of \u201cmaximizing the number of entities selected\u201d was fair as a first try, but I could imagine it failing to generalize to more complex environments, or to be easily exploited if one ever decides to pass gradients back into the representation from the policy.\u2028It was unfortunate that the legends do not correspond to anything expressed in the main text or caption (e.g. why do you not reuse the \u201cvalid_move\u201d, \u2026, \u201dblocked_push\u201d names that you thoroughly introduce?). What is \u201crandom_XX\u201d?\u2028Could you comment on why the curves seem to have a large increase in variance along the 80000-100000 updates region?\n\n\nDetails:\n8.\tFigure 2 (right) was good to explain how the model worked, but I would actually change its location and try to move it into Figure 1. Similarly, Figure 7 in the Appendix seemed rather necessary to understand the model, and belongs in the main text in my opinion.\n9.\tWhen presenting the self-attention block, it would be good to directly state that these are MLPs receiving [s_t, a_t] (as done in the Appendix).\n10.\tIs sigma^2 in the decoder fixed? To which value?\n11.\tWhen presenting the \u201csparse\u201d and \u201cfull\u201d settings, having access to Figure 7 and the rest of the Appendix might be beneficial, it would be good to point forward to it.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1178/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1178/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SPECTRA: Sparse Entity-centric Transitions", "authors": ["Rim Assouel", "Yoshua Bengio"], "authorids": ["rim.assouel@hotmail.fr", "yoshua.bengio@mila.quebec"], "keywords": ["representation learning", "slot-structured representations", "sparse slot-structured transitions", "entity-centric representation", "unsupervised learning", "object-centric"], "TL;DR": "Sparse slot-structured transition model. Training is done such that such that latent slots correspond to relevant entities of the visual scene.", "abstract": "Learning an agent that interacts with objects is ubiquituous in many RL tasks. In most of them the  agent's actions have sparse effects : only a small subset of objects in the visual scene will be affected by the action taken. We introduce SPECTRA, a model for learning slot-structured transitions from raw visual observations that embodies this sparsity assumption. Our model is composed of a perception module that decomposes the visual scene into a set of  latent objects representations (i.e. slot-structured) and a transition module that predicts the next latent set slot-wise and in a sparse way. We show that learning a perception module jointly with a sparse slot-structured transition model not only  biases the model towards more entity-centric perceptual groupings  but also enables intrinsic exploration strategy that aims at maximizing the number of objects changed in the agent\u2019s trajectory.", "pdf": "/pdf/b4270b052a3a351200bd7d14c48b2b830b3e349f.pdf", "paperhash": "assouel|spectra_sparse_entitycentric_transitions", "original_pdf": "/attachment/b4270b052a3a351200bd7d14c48b2b830b3e349f.pdf", "_bibtex": "@misc{\nassouel2020spectra,\ntitle={{\\{}SPECTRA{\\}}: Sparse Entity-centric Transitions},\nauthor={Rim Assouel and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxwvCEFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxwvCEFvH", "replyto": "HJxwvCEFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1178/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1178/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575675118974, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1178/Reviewers"], "noninvitees": [], "tcdate": 1570237741206, "tmdate": 1575675118989, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1178/-/Official_Review"}}}, {"id": "HJgQLsmz5B", "original": null, "number": 3, "cdate": 1572121418968, "ddate": null, "tcdate": 1572121418968, "tmdate": 1572972502426, "tddate": null, "forum": "HJxwvCEFvH", "replyto": "HJxwvCEFvH", "invitation": "ICLR.cc/2020/Conference/Paper1178/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to use a \u2018slot-based\u2019 (factored) representation of a \u2018scene\u2019 s.t. a forward model learned over some observed transitions only requires sparse updates to the current representation. The results show that jointly learning the forward model and the scene representation encourages meaningful \u2018entities\u2019 to emerge in each slot. Additionally, the paper argues that this representation allows for better generalization and can also guide exploration by rewarding actions that change multiple entities\n\nI really like the overall idea i.e. jointly learning the scene representation and the transition model, while enforcing sparse transitions. The experiments in Sec 4.2 and the visual results in Fig 3 do clearly highlight the benefits of joint learning, and show that the emergent representations are more meaningful compared to learning representations independently.\n\nHowever, while the overall approach is intuitive and seems to yield desirable results, I have concerns regarding the experiments, comparisons to prior work, and the exact contributions of this work. Specifically:\n\n1) It is unclear what this paper is claiming to improve over prior work: is the goal to a) learn a good forward model, or b) show that emergent entities allow better downstream tasks. If it is \u2018a\u2019 , then there are several other ways of learning good forward models e.g. convolution flow-based [1], and simply showing comparisons to a naive baseline is not sufficient. Similarly, the only downstream task examined is exploration, and again the only comparison is against a variant of the method. Therefore, while the results regarding the emergent representation are good compared to a variant without joint training, they are not shown to be useful in context of any task when compared to the approaches in literature.\n\n2) Despite the motivation in the introduction regarding applications to RL, the paper essentially learns a specific form of factored forward model. There have been several prior works which also pursue a similar approach (though mainly in context of video prediction) e.g. [2], and I don\u2019t think this paper\u2019s approach is novel in context of these. In any case, some comparisons should be made to these form of models which also show emergent entities with a graph-structured transition model.\n\n3) While the experiments in Sec 4.2 clearly demonstrate the benefits of the approach, the ones in Sec 4.1 and 4.3 are less convincing:\n3a) Sec 4.1 shows that the slot based transition model generalizes better, but this is only in comparison to a naive fully-connected baseline. I feel any prediction model with some structure e.g. graph-based forward model, convolutional forward model etc. would also similarly generalize.\n3b) Sec 4.2 argues that this slot-based representation can help exploration, but this is in fact a chicken-and-egg problem, as one needs to have collected interesting transition samples for a good representation to emerge. In fact, the results shown in Fig 6b) show that actions affecting 3 blocks were not explored, perhaps because the random transitions did not sample these to begin with.\n\nOverall, the approach proposed is desirable, but there are closely related alternatives that exist in literature, and there need to be more concrete comparisons against these for either prediction quality or success in downstream task.\n\n---\nReferences:\n[1] Self-Supervised Visual Planning with Temporal Skip Connections, Ebert et. al, CORL 17\n[2] Learning to decompose and disentangle representations for video prediction, Hsieh et. al., NeurIPS 18"}, "signatures": ["ICLR.cc/2020/Conference/Paper1178/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1178/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SPECTRA: Sparse Entity-centric Transitions", "authors": ["Rim Assouel", "Yoshua Bengio"], "authorids": ["rim.assouel@hotmail.fr", "yoshua.bengio@mila.quebec"], "keywords": ["representation learning", "slot-structured representations", "sparse slot-structured transitions", "entity-centric representation", "unsupervised learning", "object-centric"], "TL;DR": "Sparse slot-structured transition model. Training is done such that such that latent slots correspond to relevant entities of the visual scene.", "abstract": "Learning an agent that interacts with objects is ubiquituous in many RL tasks. In most of them the  agent's actions have sparse effects : only a small subset of objects in the visual scene will be affected by the action taken. We introduce SPECTRA, a model for learning slot-structured transitions from raw visual observations that embodies this sparsity assumption. Our model is composed of a perception module that decomposes the visual scene into a set of  latent objects representations (i.e. slot-structured) and a transition module that predicts the next latent set slot-wise and in a sparse way. We show that learning a perception module jointly with a sparse slot-structured transition model not only  biases the model towards more entity-centric perceptual groupings  but also enables intrinsic exploration strategy that aims at maximizing the number of objects changed in the agent\u2019s trajectory.", "pdf": "/pdf/b4270b052a3a351200bd7d14c48b2b830b3e349f.pdf", "paperhash": "assouel|spectra_sparse_entitycentric_transitions", "original_pdf": "/attachment/b4270b052a3a351200bd7d14c48b2b830b3e349f.pdf", "_bibtex": "@misc{\nassouel2020spectra,\ntitle={{\\{}SPECTRA{\\}}: Sparse Entity-centric Transitions},\nauthor={Rim Assouel and Yoshua Bengio},\nyear={2020},\nurl={https://openreview.net/forum?id=HJxwvCEFvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJxwvCEFvH", "replyto": "HJxwvCEFvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1178/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1178/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575675118974, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1178/Reviewers"], "noninvitees": [], "tcdate": 1570237741206, "tmdate": 1575675118989, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1178/-/Official_Review"}}}], "count": 6}