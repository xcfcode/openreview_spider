{"notes": [{"id": "Hyed4i05KX", "original": "HyxHHinuFQ", "number": 13, "cdate": 1538087727751, "ddate": null, "tcdate": 1538087727751, "tmdate": 1545355391425, "tddate": null, "forum": "Hyed4i05KX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Interpreting Layered Neural Networks via Hierarchical Modular Representation", "abstract": "Interpreting the prediction mechanism of complex models is currently one of the most important tasks in the machine learning field, especially with layered neural networks, which have achieved high predictive performance with various practical data sets. To reveal the global structure of a trained neural network in an interpretable way, a series of clustering methods have been proposed, which decompose the units into clusters according to the similarity of their inference roles. The main problems in these studies were that (1) we have no prior knowledge about the optimal resolution for the decomposition, or the appropriate number of clusters, and (2) there was no method with which to acquire knowledge about whether the outputs of each cluster have a positive or negative correlation with the input and output dimension values. \nIn this paper, to solve these problems, we propose a method for obtaining a hierarchical modular representation of a layered neural network. The application of a hierarchical clustering method to a trained network reveals a tree-structured relationship among hidden layer units, based on their feature vectors defined by their correlation with the input and output dimension values. ", "paperhash": "watanabe|interpreting_layered_neural_networks_via_hierarchical_modular_representation", "keywords": ["interpretabile machine learning", "neural network", "hierarchical clustering"], "authorids": ["watanabe.chihiro@lab.ntt.co.jp"], "authors": ["Chihiro Watanabe"], "TL;DR": "A method for obtaining a hierarchical cluster structure of a trained layered neural network", "pdf": "/pdf/482b96f95283bdda7d6ec0427b77369b0ba558a0.pdf", "_bibtex": "@misc{\nwatanabe2019interpreting,\ntitle={Interpreting Layered Neural Networks via Hierarchical Modular Representation},\nauthor={Chihiro Watanabe},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyed4i05KX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "rJlp94hge4", "original": null, "number": 1, "cdate": 1544762516617, "ddate": null, "tcdate": 1544762516617, "tmdate": 1545354519497, "tddate": null, "forum": "Hyed4i05KX", "replyto": "Hyed4i05KX", "invitation": "ICLR.cc/2019/Conference/-/Paper13/Meta_Review", "content": {"metareview": "All reviewers agree to reject. While there were many positive points to this work, reviewers believed that it was not yet ready for acceptance.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Meta-Review for Hierarchical Modular Representation paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper13/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper13/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpreting Layered Neural Networks via Hierarchical Modular Representation", "abstract": "Interpreting the prediction mechanism of complex models is currently one of the most important tasks in the machine learning field, especially with layered neural networks, which have achieved high predictive performance with various practical data sets. To reveal the global structure of a trained neural network in an interpretable way, a series of clustering methods have been proposed, which decompose the units into clusters according to the similarity of their inference roles. The main problems in these studies were that (1) we have no prior knowledge about the optimal resolution for the decomposition, or the appropriate number of clusters, and (2) there was no method with which to acquire knowledge about whether the outputs of each cluster have a positive or negative correlation with the input and output dimension values. \nIn this paper, to solve these problems, we propose a method for obtaining a hierarchical modular representation of a layered neural network. The application of a hierarchical clustering method to a trained network reveals a tree-structured relationship among hidden layer units, based on their feature vectors defined by their correlation with the input and output dimension values. ", "paperhash": "watanabe|interpreting_layered_neural_networks_via_hierarchical_modular_representation", "keywords": ["interpretabile machine learning", "neural network", "hierarchical clustering"], "authorids": ["watanabe.chihiro@lab.ntt.co.jp"], "authors": ["Chihiro Watanabe"], "TL;DR": "A method for obtaining a hierarchical cluster structure of a trained layered neural network", "pdf": "/pdf/482b96f95283bdda7d6ec0427b77369b0ba558a0.pdf", "_bibtex": "@misc{\nwatanabe2019interpreting,\ntitle={Interpreting Layered Neural Networks via Hierarchical Modular Representation},\nauthor={Chihiro Watanabe},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyed4i05KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper13/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353370600, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hyed4i05KX", "replyto": "Hyed4i05KX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper13/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper13/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper13/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353370600}}}, {"id": "ByxTjCE-pQ", "original": null, "number": 3, "cdate": 1541652132586, "ddate": null, "tcdate": 1541652132586, "tmdate": 1541652132586, "tddate": null, "forum": "Hyed4i05KX", "replyto": "Hyed4i05KX", "invitation": "ICLR.cc/2019/Conference/-/Paper13/Official_Review", "content": {"title": "Not convinced", "review": "Sorry, I am not convinced by this paper.\n\nI just don't believe that one can really gain any useful insight into neural networks by this kind of visualization.  In my opinion, all these kinds of visualization can give is the false believe that one understands what the network is doing.  (If you think about it, understanding itself is a rather vague and subjective term).  I guess my point is, these kinds of visualization don't seem to generate any actionable knowledge.  And how would one even meaningfully compare the outputs of competing methods of this general type?\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper13/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpreting Layered Neural Networks via Hierarchical Modular Representation", "abstract": "Interpreting the prediction mechanism of complex models is currently one of the most important tasks in the machine learning field, especially with layered neural networks, which have achieved high predictive performance with various practical data sets. To reveal the global structure of a trained neural network in an interpretable way, a series of clustering methods have been proposed, which decompose the units into clusters according to the similarity of their inference roles. The main problems in these studies were that (1) we have no prior knowledge about the optimal resolution for the decomposition, or the appropriate number of clusters, and (2) there was no method with which to acquire knowledge about whether the outputs of each cluster have a positive or negative correlation with the input and output dimension values. \nIn this paper, to solve these problems, we propose a method for obtaining a hierarchical modular representation of a layered neural network. The application of a hierarchical clustering method to a trained network reveals a tree-structured relationship among hidden layer units, based on their feature vectors defined by their correlation with the input and output dimension values. ", "paperhash": "watanabe|interpreting_layered_neural_networks_via_hierarchical_modular_representation", "keywords": ["interpretabile machine learning", "neural network", "hierarchical clustering"], "authorids": ["watanabe.chihiro@lab.ntt.co.jp"], "authors": ["Chihiro Watanabe"], "TL;DR": "A method for obtaining a hierarchical cluster structure of a trained layered neural network", "pdf": "/pdf/482b96f95283bdda7d6ec0427b77369b0ba558a0.pdf", "_bibtex": "@misc{\nwatanabe2019interpreting,\ntitle={Interpreting Layered Neural Networks via Hierarchical Modular Representation},\nauthor={Chihiro Watanabe},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyed4i05KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper13/Official_Review", "cdate": 1542234557810, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hyed4i05KX", "replyto": "Hyed4i05KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper13/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335629421, "tmdate": 1552335629421, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper13/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BygtYjnwhm", "original": null, "number": 2, "cdate": 1541028736802, "ddate": null, "tcdate": 1541028736802, "tmdate": 1541534360648, "tddate": null, "forum": "Hyed4i05KX", "replyto": "Hyed4i05KX", "invitation": "ICLR.cc/2019/Conference/-/Paper13/Official_Review", "content": {"title": "Unpromising approach to an important problem, understanding processing in feed-forward MLPs", "review": "Pros\n\n1.\tThe paper is fairly clear.\n2.\tThe problem is important: analyzing the internal computations of layered networks.\n3.\tThe method seems to be a slight improvement on an existing method: the use of hierarchical clustering is nice.\n4.\tFigs. 3 and 5 superimposing the analyzed clusters on top of the network diagram are cool.\n\nCons\n\n5.\tThe paper wastes valuable space writing out in detail the equations for backpropagation in a standard feed-forward MLP.\n6.\tThe paper does not have an acceptable review of relevant prior work. This is particularly problematic as the proposal seems to be a rather small tweak to prior work of two 2018 papers by Watanabe et al. But there is extensive other literature attempting to address this problem, especially in the vision domain, where their main example - poor over-worked MNIST - resides.\n7.\tIn my view, attempts to understand processing in NNs exclusively at the individual-unit level are essentially doomed at the outset. These networks crucially represent their information in distributed representations and it is joint action by multiple units rather than action by individual units that drives processing. Consider the \u201ceffect\u201d variable analyzed in this paper, which is a simple correlation between the activity of a target hidden unit and the activity of a particular input or output unit. Suppose whenever hidden unit i is active, hidden unit j is also active, and vice versa. Now suppose j strongly drives output unit k via a connection with a large weight, while unit i has no connection at all to unit k. Then i will have a strong \u201ceffect\u201d on k! The correlations between the activity of i and k is the same as the correlation between j and k, even though the causal interaction between i and k is nil, while the causal interaction between j and k is strong. In this especially transparent situation, it is the joint action of i and j that matters, and it so happens that this joint action has no contribution from i.\n8.\tSo in addition to the problems arising from analyzing exclusively at the individual-unit level, there is the problem of defining \u201ceffect\u201d by correlation instead of causation.\n9.\tI don\u2019t myself gain any insight into how the MNIST network is working by looking at the clusters diagrammed in Fig. 4. There is no discussion of the fact that nearly all of their input-\u201ceffect\u201d maps look like a slanted oval which is either on-center-off-surround or the reverse (no comment on the superficial, at least, connection to the receptive fields of neurons in the early mammalian visual system). Just how do these cluster maps explain anything? \n10.\tThe maps for the other example, time-series of prices of root vegetables, are even more baffling, but, superficially at least, the input maps suggest the hidden units are doing Fourier analysis; even this obvious observation is not made in the paper, however.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper13/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpreting Layered Neural Networks via Hierarchical Modular Representation", "abstract": "Interpreting the prediction mechanism of complex models is currently one of the most important tasks in the machine learning field, especially with layered neural networks, which have achieved high predictive performance with various practical data sets. To reveal the global structure of a trained neural network in an interpretable way, a series of clustering methods have been proposed, which decompose the units into clusters according to the similarity of their inference roles. The main problems in these studies were that (1) we have no prior knowledge about the optimal resolution for the decomposition, or the appropriate number of clusters, and (2) there was no method with which to acquire knowledge about whether the outputs of each cluster have a positive or negative correlation with the input and output dimension values. \nIn this paper, to solve these problems, we propose a method for obtaining a hierarchical modular representation of a layered neural network. The application of a hierarchical clustering method to a trained network reveals a tree-structured relationship among hidden layer units, based on their feature vectors defined by their correlation with the input and output dimension values. ", "paperhash": "watanabe|interpreting_layered_neural_networks_via_hierarchical_modular_representation", "keywords": ["interpretabile machine learning", "neural network", "hierarchical clustering"], "authorids": ["watanabe.chihiro@lab.ntt.co.jp"], "authors": ["Chihiro Watanabe"], "TL;DR": "A method for obtaining a hierarchical cluster structure of a trained layered neural network", "pdf": "/pdf/482b96f95283bdda7d6ec0427b77369b0ba558a0.pdf", "_bibtex": "@misc{\nwatanabe2019interpreting,\ntitle={Interpreting Layered Neural Networks via Hierarchical Modular Representation},\nauthor={Chihiro Watanabe},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyed4i05KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper13/Official_Review", "cdate": 1542234557810, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hyed4i05KX", "replyto": "Hyed4i05KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper13/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335629421, "tmdate": 1552335629421, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper13/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJe9AyUUh7", "original": null, "number": 1, "cdate": 1540935634136, "ddate": null, "tcdate": 1540935634136, "tmdate": 1541534360440, "tddate": null, "forum": "Hyed4i05KX", "replyto": "Hyed4i05KX", "invitation": "ICLR.cc/2019/Conference/-/Paper13/Official_Review", "content": {"title": "I recommend rejection of this paper, because 1) I do not think the proposed method achieve its purpose; 2) It is not appropriately compared with existing methods; and 3) I am not convinced that the method is designed properly.", "review": "In this paper, the authors try to interpret the prediction mechanism of Layered Neural Networks (LNNs). The authors proposed to first define a feature vector that represents the roles of each hidden layer unit, via computing Pearson correlation coefficient. Then a hierarchical clustering method is applied to the generated feature vectors, such that tree-structured relationships among hidden layer units are revealed.\n\nThe purpose of the paper is to understand the prediction mechanism of Layered Neural Networks (LNNs). But based on the results in the experiments, I do not think the model achieves this purpose. Given the tree structure of LNN for the MNIST data set, I am still not able to understand how this LNN distinguishes the digit 0 from other digits. I am also not able to understand why a particular sample is classified as 0 rather than 6.\n\nIn Section 1, the authors mension that there are existing clustering-based methods that interpret LNN. The authors do not compare the proposed methods with these existing methods, either quantitatively or qualitatively. So I am also not sure the contribution of this paper, provided the existing methods.\n\nIn Section 3.1, the authors state that \"there is no method that can reveal whether an increase in the input dimension value has a positive or negative effect on the output value of a hidden layer unit\". I do not agree with this statement, because Ross et.al (2017) has proposed to measure it via gradient, although they are trying to solve a slightly different problem. Since the output of a hidden unit is a non-linear function of the input, I am not convinced that the proposed method that computes Pearson correlation coefficient is better choise than computing the gradient.\n\nThe proposed method provides a tree structure to describe the relationships between the hidden layer units. The authors also do not illustrate why learning the tree structure is particularly important. We can also run k-means with cosine similarity on the generated vector $v$, and learn the number of clusters via Bayesian information criterion (BIC). The authors do not explain why the tree-structured clustering results are more superior than the k-means clustering results.\n\nIn summary, I recommend rejection of this paper, because 1) I do not think the proposed method achieve its purpose; 2) It is not appropriately compared with existing methods; and 3) I am not convinced that the method is designed properly.\n\n\nReferences\nRoss, Andrew Slavin, Michael C. Hughes, and Finale Doshi-Velez. \"Right for the right reasons: training differentiable models by constraining their explanations.\" Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI). 2017.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper13/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Interpreting Layered Neural Networks via Hierarchical Modular Representation", "abstract": "Interpreting the prediction mechanism of complex models is currently one of the most important tasks in the machine learning field, especially with layered neural networks, which have achieved high predictive performance with various practical data sets. To reveal the global structure of a trained neural network in an interpretable way, a series of clustering methods have been proposed, which decompose the units into clusters according to the similarity of their inference roles. The main problems in these studies were that (1) we have no prior knowledge about the optimal resolution for the decomposition, or the appropriate number of clusters, and (2) there was no method with which to acquire knowledge about whether the outputs of each cluster have a positive or negative correlation with the input and output dimension values. \nIn this paper, to solve these problems, we propose a method for obtaining a hierarchical modular representation of a layered neural network. The application of a hierarchical clustering method to a trained network reveals a tree-structured relationship among hidden layer units, based on their feature vectors defined by their correlation with the input and output dimension values. ", "paperhash": "watanabe|interpreting_layered_neural_networks_via_hierarchical_modular_representation", "keywords": ["interpretabile machine learning", "neural network", "hierarchical clustering"], "authorids": ["watanabe.chihiro@lab.ntt.co.jp"], "authors": ["Chihiro Watanabe"], "TL;DR": "A method for obtaining a hierarchical cluster structure of a trained layered neural network", "pdf": "/pdf/482b96f95283bdda7d6ec0427b77369b0ba558a0.pdf", "_bibtex": "@misc{\nwatanabe2019interpreting,\ntitle={Interpreting Layered Neural Networks via Hierarchical Modular Representation},\nauthor={Chihiro Watanabe},\nyear={2019},\nurl={https://openreview.net/forum?id=Hyed4i05KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper13/Official_Review", "cdate": 1542234557810, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hyed4i05KX", "replyto": "Hyed4i05KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper13/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335629421, "tmdate": 1552335629421, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper13/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}