{"notes": [{"id": "HylNWkHtvB", "original": "H1xn-9iuvS", "number": 1540, "cdate": 1569439484194, "ddate": null, "tcdate": 1569439484194, "tmdate": 1577168240705, "tddate": null, "forum": "HylNWkHtvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Domain-Independent Dominance of Adaptive Methods", "authors": ["Pedro Savarese", "David McAllester", "Sudarshan Babu", "Michael Maire"], "authorids": ["savarese@ttic.edu", "mcallester@ttic.edu", "sudarshan@ttic.edu", "mmaire@uchicago.edu"], "keywords": [], "abstract": "From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. This later observation, alongside of AvaGrad's decoupling of hyperparameters, could make it the preferred optimizer for deep learning, replacing both SGD and Adam.", "pdf": "/pdf/9af302ed6da4c59db377e6e67f134d42c2426975.pdf", "paperhash": "savarese|domainindependent_dominance_of_adaptive_methods", "original_pdf": "/attachment/1676dc516783c5377fcdff65c8f566d994c341af.pdf", "_bibtex": "@misc{\nsavarese2020domainindependent,\ntitle={Domain-Independent Dominance of Adaptive Methods},\nauthor={Pedro Savarese and David McAllester and Sudarshan Babu and Michael Maire},\nyear={2020},\nurl={https://openreview.net/forum?id=HylNWkHtvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "rcRPTcLTD_", "original": null, "number": 1, "cdate": 1576798725992, "ddate": null, "tcdate": 1576798725992, "tmdate": 1576800910502, "tddate": null, "forum": "HylNWkHtvB", "replyto": "HylNWkHtvB", "invitation": "ICLR.cc/2020/Conference/Paper1540/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes an adaptive gradient method for optimization in deep learning called AvaGrad.  The authors argue that AvaGrad greatly simplifies hyperparameter search (over e.g. ADAM) and demonstrate competitive performance on benchmark image and text problems.  In thorough reviews, thorough author response and discussion by the reviewers (which are are all appreciated) a few concerns about the work came to light and were debated.  One reviewer was compelled by the author response to raise their recommendation to weak accept.  However, none of the reviewers felt strongly enough to champion the paper for acceptance and even the reviewer assigning the highest score had reservations.  A major issue of debate was the treatment of hyperparameters, i.e. that the authors tuned hyperparameters on a smaller problem and then assumed these would extrapolate to larger problems. In a largely empirical paper this does seem to be a significant concern.  The space of adaptive optimizers for deep learning is a crowded one and thus the empirical (or theoretical) burden of proof of superiority is high.  The authors state regarding a concurrent submission: \"when hyperparameters are properly tuned, echoing our results on this matter\", however, it seems that the reviewers disagree that the hyperparameters are indeed properly tuned in this paper.  It's due to these remaining reservations that the recommendation is to reject.  ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Independent Dominance of Adaptive Methods", "authors": ["Pedro Savarese", "David McAllester", "Sudarshan Babu", "Michael Maire"], "authorids": ["savarese@ttic.edu", "mcallester@ttic.edu", "sudarshan@ttic.edu", "mmaire@uchicago.edu"], "keywords": [], "abstract": "From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. This later observation, alongside of AvaGrad's decoupling of hyperparameters, could make it the preferred optimizer for deep learning, replacing both SGD and Adam.", "pdf": "/pdf/9af302ed6da4c59db377e6e67f134d42c2426975.pdf", "paperhash": "savarese|domainindependent_dominance_of_adaptive_methods", "original_pdf": "/attachment/1676dc516783c5377fcdff65c8f566d994c341af.pdf", "_bibtex": "@misc{\nsavarese2020domainindependent,\ntitle={Domain-Independent Dominance of Adaptive Methods},\nauthor={Pedro Savarese and David McAllester and Sudarshan Babu and Michael Maire},\nyear={2020},\nurl={https://openreview.net/forum?id=HylNWkHtvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HylNWkHtvB", "replyto": "HylNWkHtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717871, "tmdate": 1576800268250, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1540/-/Decision"}}}, {"id": "rklCY7VJ9r", "original": null, "number": 3, "cdate": 1571926917550, "ddate": null, "tcdate": 1571926917550, "tmdate": 1574436962188, "tddate": null, "forum": "HylNWkHtvB", "replyto": "HylNWkHtvB", "invitation": "ICLR.cc/2020/Conference/Paper1540/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "In this paper, the authors present a new adaptive gradient method AvaGrad. The authors claim the proposed method is less sensitive to its hyperparameters, compared to previous algorithms, and this is due to decoupling the learning rate and the damping parameter.\n\nOverall, the paper is well written, and is on an important topic. However, I have a few concerns about the paper, which I will list below.\n\n1. The fact that adaptive gradient methods converge with a fast rate when the sum in the denominator is taken till the t-1th iterate has appeared in previous papers before [1]. The convergence rate analysis for this case is fairly simple, and I am not sure if analyzing RMSProp/Adam in this setting should be considered a significant contributions of the paper.\n\n2. The proposed algorithm AvaGrad is a simple but interesting idea. I have a number questions about the experimental evaluation though, which makes it hard for me to evaluate the significance of the results presented:\n\na) Was momentum used with SGD?\n\nb) How is the optimal hyperparameters (learning rate and damping, i..e, epsilon, parameters) selected?\n\nc) Do any of these conclusions change when trying out a very small or very large batch size?\n\nd) I am not convinced that using the same optimal hyperparams as the WRN-28-4 task on the WRN-28-10 and ResNet50 models is a reasonable experiment. Why is this a good idea? While this does support the claim that adaptive gradient methods are less sensitive to hyperparameter settings, but makes the other claim about AvaGrad generalizing just as well as SGD weaker?\n\ne) One of the key claims that adaptive gradient methods generalize better when using a large damping (epsilon) parameter has appeared in previous papers as well [2, 3].\n\n\nOverall, in my view, this is a borderline paper mostly because I think a number of the results presented have been shown in other recent papers. My score reflects this. However, I think decoupling the learning rate and the damping parameter by normalizing the preconditioner is a simple but interesting idea, and I am willing to increase my score based on the discussion with the authors and other reviewers.\n\n\n[1] X. Li and F. Orabona. On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes. In AISTATS 2019\n[2] M. Zaheer, S. Reddi, D. Sachan, S. Kale, and S. Kumar. Adaptive methods for nonconvex optimization. in NeurIPS 2018.\n[3] S. De, A. Mukherjee, and E. Ullah. Convergence guarantees for rmsprop and adam in non-convex optimization and an empirical comparison to nesterov acceleration. arXiv:1807.06766, 2018.\n\nA few more minor comments:\n\n1. The authors say that methods like AMSGrad fail to match the convergence rate of SGD. But this statement seems misleading since it is not clear whether the worse rate is due to the analysis (which gives an upper bound) or the algorithm?\n\n2. In the related work section, the authors discuss convergence rates of algorithms with constant and decreasing step sizes together. This can be confusing to the reader, and it is best to explicitly mention the setting under which the different results were derived.\n\n=======================================\n\nEdit after rebuttal:\nI thank the authors for the detailed response and for the updated version of the paper. After discussion with other reviewers, we are still not convinced that the hyperparameter tuning in the experiments (especially the baselines) is rigorous enough. This is especially important given that this paper proposes a new optimizer. We are also concerned about the novelty of the results, and believe most of these results have appeared in previous work. So I am not increasing my score. I would encourage the authors to do a more rigorous experimental evaluation of the proposed algorithm.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1540/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1540/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Independent Dominance of Adaptive Methods", "authors": ["Pedro Savarese", "David McAllester", "Sudarshan Babu", "Michael Maire"], "authorids": ["savarese@ttic.edu", "mcallester@ttic.edu", "sudarshan@ttic.edu", "mmaire@uchicago.edu"], "keywords": [], "abstract": "From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. This later observation, alongside of AvaGrad's decoupling of hyperparameters, could make it the preferred optimizer for deep learning, replacing both SGD and Adam.", "pdf": "/pdf/9af302ed6da4c59db377e6e67f134d42c2426975.pdf", "paperhash": "savarese|domainindependent_dominance_of_adaptive_methods", "original_pdf": "/attachment/1676dc516783c5377fcdff65c8f566d994c341af.pdf", "_bibtex": "@misc{\nsavarese2020domainindependent,\ntitle={Domain-Independent Dominance of Adaptive Methods},\nauthor={Pedro Savarese and David McAllester and Sudarshan Babu and Michael Maire},\nyear={2020},\nurl={https://openreview.net/forum?id=HylNWkHtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylNWkHtvB", "replyto": "HylNWkHtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575599942915, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1540/Reviewers"], "noninvitees": [], "tcdate": 1570237735895, "tmdate": 1575599942928, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1540/-/Official_Review"}}}, {"id": "HkgBFoJCKS", "original": null, "number": 2, "cdate": 1571842940700, "ddate": null, "tcdate": 1571842940700, "tmdate": 1574193362746, "tddate": null, "forum": "HylNWkHtvB", "replyto": "HylNWkHtvB", "invitation": "ICLR.cc/2020/Conference/Paper1540/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "In this paper the authors develop variants of Adam which corrects for the relationship of the gradient and adaptive terms that causes convergence issues, naming them Delayed Adam and AvaGrad. They also provide proofs demonstrating they solve the convergence issues of Adam in O(1/sqrt(T)) time. They also introduce a convex problem where Adam fails to converge to a stationary point.\n\nThis paper is clearly written and has reasonable experimental support of its claims. However in terms of novelty, AdaShift was published at ICLR last year (https://openreview.net/forum?id=HkgTkhRcKQ) and seems to include a closely related analysis and update rule of your proposed optimizers. In AdaShift instead of correcting for the correlation between the gradient and eta_t, they correct for the relationship between the gradient and the second moment term v_t. Could you further clarify the differences between the two, both in your approach to deriving the new update rule and the algorithms themselves? Additionally, their Theorem 1 could be compared to yours, but noting the differences for these seems less important. If the optimizers are unique enough, including AdaShift in your experiments would be very useful for demonstrating their differences.\n\nRegarding experiments, while it is true that adaptive methods are supposed to be less \u201csensitive\u201d to hyperparameter choices, the limits of the feasible ranges for each hyperparameter could vary drastically across problems (especially, as previously demonstrated, across different batch sizes.) Thus, not retuning across experiments seems like it could negatively affect performance for any of the transferred hyperparameter settings. Instead of demonstrating hyperparameter insensitivity by carrying over hyperparameter settings, one could instead retune for each problem and show that a higher percent of hyperparameter combinations result in the same/similar best performance (similar to what is done in Fig. 2, but also showing a (1-dimensional) SGD version which would presumably contain fewer high performing settings.)\n\nSome additional comments:\n-The contribution of Theorem 1 is a nice addition to the literature.\n-Your tuning of epsilon is great! I believe more papers should include epsilon in their hyperparameter sweeps.\n-Scaling epsilon with step size makes sense when considering that Adam is similar to a trust region method, where epsilon is inversely proportional to the trust region radius. However, in section 5 implying that epsilon should be as large as possible in the worst case seems like an odd result given that this would always diminish your second moment term as much as possible, defeating the point of the additional Adam-like adaptivity. Can you comment on why this diminished adaptivity would be desirable in the worst case scenario analyzed?\n-The synthetic toy problem is much appreciated, more papers should start with a small, interpretable experiment.\n-Was SGD with momentum used? If not, this may not be a fair comparison, as I believe it is much more common to use momentum with SGD. If momentum was used, was the momentum hyperparameter tuned? If not, this may be advantageous to the Adam based methods, as they have more versatile adaptability and thus may not need as much care with their selection of momentum values.\n-Was a validation set used for CIFAR? You note in appendix C that there are 50k train and 10k test. You mention validation performance in the main text, so this is just double checking.\n-The demonstration in figures 2, 3 of decoupling the step size and epsilon in interesting! Given that the best performing values seem to be on the edges of the ranges tested, I would be curious if the trend continues for more extreme values of alpha and epsilon (one could sparsely search along the predicted trendlines.)\n\nNits:\n-\u201cVanilla SGD is still prevalent, in spite of the development of seemingly more sophisticated adaptive alternatives...\u201d This could use some citations to back up the claim, because as far as I know it is much more common to use SGD with momentum and is actually rare to use vanilla SGD (the DenseNets and Resnets citations use momentum=0.9.)\n-It would be nice to highlight in color the diff from vanilla Adam in the Algorithm sections.\n-It is not super clear from the text how in eq 26 you get \\sum{E[f(w_t)|Z]} = f(w_1)\n-I may be misreading something, but I believe the H in the leftmost term in the last line of eq 33 should be an L.\n-In section 5, \u201cfor a fixed learning rate (as is typically done in practice, except for discrete decays during training)\u201d seems like an overly broad claim, given that authors commonly use polynomial, linear, exponential, cosine, or other learning rate decay/warmups. Granted for some CIFAR and ImageNet benchmarks there are more common discrete learning rate schedules, but that does not seem to be the overwhelmingly prevalent technique.\n\nOverall, while this area of analyzing Adam and proposing modifications is a popular and crowded subject, I believe this paper may contribute to it if my concerns are addressed. While I currently do not recommend acceptance, I am open to changing my score after considering the author comments!\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1540/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1540/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Independent Dominance of Adaptive Methods", "authors": ["Pedro Savarese", "David McAllester", "Sudarshan Babu", "Michael Maire"], "authorids": ["savarese@ttic.edu", "mcallester@ttic.edu", "sudarshan@ttic.edu", "mmaire@uchicago.edu"], "keywords": [], "abstract": "From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. This later observation, alongside of AvaGrad's decoupling of hyperparameters, could make it the preferred optimizer for deep learning, replacing both SGD and Adam.", "pdf": "/pdf/9af302ed6da4c59db377e6e67f134d42c2426975.pdf", "paperhash": "savarese|domainindependent_dominance_of_adaptive_methods", "original_pdf": "/attachment/1676dc516783c5377fcdff65c8f566d994c341af.pdf", "_bibtex": "@misc{\nsavarese2020domainindependent,\ntitle={Domain-Independent Dominance of Adaptive Methods},\nauthor={Pedro Savarese and David McAllester and Sudarshan Babu and Michael Maire},\nyear={2020},\nurl={https://openreview.net/forum?id=HylNWkHtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylNWkHtvB", "replyto": "HylNWkHtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575599942915, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1540/Reviewers"], "noninvitees": [], "tcdate": 1570237735895, "tmdate": 1575599942928, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1540/-/Official_Review"}}}, {"id": "BJggfpXoiH", "original": null, "number": 11, "cdate": 1573760264303, "ddate": null, "tcdate": 1573760264303, "tmdate": 1573760264303, "tddate": null, "forum": "HylNWkHtvB", "replyto": "H1gL5F3Yir", "invitation": "ICLR.cc/2020/Conference/Paper1540/-/Official_Comment", "content": {"title": "Response to response to reviewer 3 [*/2]", "comment": "Thank you very much for the detailed responses! These are very thorough in addressing the concerns raised, I appreciate (and sympathize with the work involved in) the quick turn around in adding additional experiments. I will definitely be raising my score."}, "signatures": ["ICLR.cc/2020/Conference/Paper1540/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1540/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Independent Dominance of Adaptive Methods", "authors": ["Pedro Savarese", "David McAllester", "Sudarshan Babu", "Michael Maire"], "authorids": ["savarese@ttic.edu", "mcallester@ttic.edu", "sudarshan@ttic.edu", "mmaire@uchicago.edu"], "keywords": [], "abstract": "From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. This later observation, alongside of AvaGrad's decoupling of hyperparameters, could make it the preferred optimizer for deep learning, replacing both SGD and Adam.", "pdf": "/pdf/9af302ed6da4c59db377e6e67f134d42c2426975.pdf", "paperhash": "savarese|domainindependent_dominance_of_adaptive_methods", "original_pdf": "/attachment/1676dc516783c5377fcdff65c8f566d994c341af.pdf", "_bibtex": "@misc{\nsavarese2020domainindependent,\ntitle={Domain-Independent Dominance of Adaptive Methods},\nauthor={Pedro Savarese and David McAllester and Sudarshan Babu and Michael Maire},\nyear={2020},\nurl={https://openreview.net/forum?id=HylNWkHtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylNWkHtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference/Paper1540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1540/Reviewers", "ICLR.cc/2020/Conference/Paper1540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1540/Authors|ICLR.cc/2020/Conference/Paper1540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154505, "tmdate": 1576860550903, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference/Paper1540/Reviewers", "ICLR.cc/2020/Conference/Paper1540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1540/-/Official_Comment"}}}, {"id": "SJe0hjnFoH", "original": null, "number": 10, "cdate": 1573665718110, "ddate": null, "tcdate": 1573665718110, "tmdate": 1573665718110, "tddate": null, "forum": "HylNWkHtvB", "replyto": "HylNWkHtvB", "invitation": "ICLR.cc/2020/Conference/Paper1540/-/Official_Comment", "content": {"title": "Comments on revision and concurrent submission", "comment": "We thank the reviewers.\n\nWe would like to emphasize that our main contribution is not Delayed Adam and its convergence analysis, but our newly-proposed optimizer, AvaGrad.  Additionally, we establish the empirical finding that adaptive methods can outperform SGD across different tasks/datasets, when training distinct architectures -- even in settings where SGD has been universally adopted, such as image classification on ImageNet. These findings require proper tuning of \\epsilon, whose optimal values are as large as \\epsilon=10, a value 9 orders of magnitude larger than the recommended. Without the decoupling between the learning rate and \\epsilon offered by AvaGrad, proper tuning can be extremely costly: in total, we performed over 450 runs to assess the validation performance with different settings for \\alpha and \\epsilon for each adaptive method. \n\nWe have also revised the paper, incorporating changes suggested in the reviews:\n\n   - Our experimental setup now includes AdaShift, following the same protocol we used for other adaptive methods. CIFAR and PTB results are shown in Table 1; ImageNet results will be added to the camera-ready version.\n\n   - We re-tuned SGD on the large-scale experiments (Wide ResNet 28-10 on CIFAR, 4x1000 LSTM on Penn Treebank) to make the comparison against adaptive methods stronger. In all cases, the learning rate that performed the best was the same one chosen from the previous protocol (the one that performed best in the small-scale experiment), hence the numbers are the same.\n\n\nReviewers may want to take note of the following concurrent ICLR 2020 submission and the discussion surrounding it:\n\nOn Empirical Comparisons of Optimizers for Deep Learning\nhttps://openreview.net/forum?id=HygrAR4tPS\n\nIt makes similar observations about the superiority of existing adaptive optimizers when hyperparameters are properly tuned, echoing our results on this matter.  Our contributions, however, go beyond this mere observation, as we also propose, theoretically motivate, and experimentally evaluate a better adaptive optimizer: AvaGrad.\n\nFinally, we are preparing a code repository that we will make publicly available in the near future."}, "signatures": ["ICLR.cc/2020/Conference/Paper1540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Independent Dominance of Adaptive Methods", "authors": ["Pedro Savarese", "David McAllester", "Sudarshan Babu", "Michael Maire"], "authorids": ["savarese@ttic.edu", "mcallester@ttic.edu", "sudarshan@ttic.edu", "mmaire@uchicago.edu"], "keywords": [], "abstract": "From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. This later observation, alongside of AvaGrad's decoupling of hyperparameters, could make it the preferred optimizer for deep learning, replacing both SGD and Adam.", "pdf": "/pdf/9af302ed6da4c59db377e6e67f134d42c2426975.pdf", "paperhash": "savarese|domainindependent_dominance_of_adaptive_methods", "original_pdf": "/attachment/1676dc516783c5377fcdff65c8f566d994c341af.pdf", "_bibtex": "@misc{\nsavarese2020domainindependent,\ntitle={Domain-Independent Dominance of Adaptive Methods},\nauthor={Pedro Savarese and David McAllester and Sudarshan Babu and Michael Maire},\nyear={2020},\nurl={https://openreview.net/forum?id=HylNWkHtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylNWkHtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference/Paper1540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1540/Reviewers", "ICLR.cc/2020/Conference/Paper1540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1540/Authors|ICLR.cc/2020/Conference/Paper1540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154505, "tmdate": 1576860550903, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference/Paper1540/Reviewers", "ICLR.cc/2020/Conference/Paper1540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1540/-/Official_Comment"}}}, {"id": "HJeDIo3Kir", "original": null, "number": 9, "cdate": 1573665615061, "ddate": null, "tcdate": 1573665615061, "tmdate": 1573665615061, "tddate": null, "forum": "HylNWkHtvB", "replyto": "rklCY7VJ9r", "invitation": "ICLR.cc/2020/Conference/Paper1540/-/Official_Comment", "content": {"title": "Response to reviewer 4 [1/2]", "comment": "Thank you for the review and your comments. We address your points individually below \u2014 please let us know if we can clarify or address any further concerns.\n\n\n\n\u201cI am not sure if analyzing RMSProp/Adam in this setting should be considered a significant contributions of the paper\u201d\n\nThe convergence rate of Delayed Adam is not the main point of the paper. Its form \u2014 which depends explicitly on the norm of \\eta \u2014 is what motivates the design of AvaGrad, hence the role of the analysis was to, first of all, inspire the design of a better adaptive method. AvaGrad, a new adaptive algorithm with different characteristics and increased performance, is the major contribution of the paper.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cWas momentum used with SGD?\u201d\n\nWe used SGD with nesterov momentum of 0.9, closely following Zagoruyko & Komodakis for the CIFAR Wide ResNet experiments,  Merity et al. for the Penn Treebank LSTM experiments, and He et al. for the ResNet ImageNet experiments.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cHow is the optimal hyperparameters (learning rate and damping, i..e, epsilon, parameters) selected?\u201d\n\nWe first performed extensive grid search on smaller-scale experiments, training a Wide ResNet 28-4 on CIFAR, and a 4-layer, 300 units per layer LSTM on Penn Treebank, evaluating the performance on the validation set \u2014 the results for Adam and AvaGrad are given in Figure 2 and Figure 3. Next, we used the values that yielded the best validation performance to train a Wide ResNet 28-10 on CIFAR, a 4-layer, 1000 units LSTM on Penn Treebank, and a ResNet 50 on ImageNet, achieving the results presented in Table 1. The protocol is described in Section 6.2 (paragraph 3 and the two following bullet points) and Section 6.3 (paragraph 2).\n\nIn particular, for CIFAR we searched over:\n\n  - For AvaGrad and AvaGradW:\nLearning rate in {0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 100.0, 500.0, 1000.0, 5000.0}\nEpsilon in {1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0}\n\n  - For Adam, AMSGrad, AdamW, AdaBound:\nLearning rate in {0.00005, 0.00001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0, 10.0, 100.0, 500.0}\nEpsilon in {1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0}\n\nThe best learning rate and epsilon, for each method, was:\n\nAdam (0.1, 0.1)\nAMSGrad (0.1, 0.1)\nAvaGrad (1.0, 10.0)\nAvaGradW (1.0, 0.1)\nAdamW(0.5, 10.0)\nAdaBound(0.005, 0.01)\n\nFor Penn Treebank, we searched over:\n\n  - For AvaGrad and AvaGradW:\nLearning rate in {0.2, 2.0, 20.0, 200.0, 2000.0}\nEpsilon in {1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0}\n\n  - For Adam, AMSGrad, AdamW, AdaBound:\nLearning rate in {0.0002, 0.002, 0.02, 0.2, 2.0, 20.0}\nEpsilon in {1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0, 100.0}\n\nThe best learning rate and epsilon (\\alpha, \\epsilon), for each method, was:\n\nAdam (0.002, 1e-8)\nAMSGrad (0.002, 1e-8)\nAvaGrad (200, 1e-8)\nAvaGradW (200, 1e-6)\nAdamW(0.002, 1e-5)\nAdaBound(0.002, 1e-8)\n\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cDo any of these conclusions change when trying out a very small or very large batch size?\u201d\n\nAlthough an interesting question, it was out of scope with respect to our initial goal of evaluating whether adaptive methods can outperform SGD using the recommended hyperparameters i.e. the batch sizes used in Zagoruyko & Komodakis, Merity et al., and He et al. Unfortunately we cannot re-run our experiments by the rebuttal deadline, but we do believe batch size would be interesting to investigate.  For the camera-ready version, we will generate heatmaps, like the ones in Figure 2, that explore batch size as a hyperparameter.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Independent Dominance of Adaptive Methods", "authors": ["Pedro Savarese", "David McAllester", "Sudarshan Babu", "Michael Maire"], "authorids": ["savarese@ttic.edu", "mcallester@ttic.edu", "sudarshan@ttic.edu", "mmaire@uchicago.edu"], "keywords": [], "abstract": "From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. This later observation, alongside of AvaGrad's decoupling of hyperparameters, could make it the preferred optimizer for deep learning, replacing both SGD and Adam.", "pdf": "/pdf/9af302ed6da4c59db377e6e67f134d42c2426975.pdf", "paperhash": "savarese|domainindependent_dominance_of_adaptive_methods", "original_pdf": "/attachment/1676dc516783c5377fcdff65c8f566d994c341af.pdf", "_bibtex": "@misc{\nsavarese2020domainindependent,\ntitle={Domain-Independent Dominance of Adaptive Methods},\nauthor={Pedro Savarese and David McAllester and Sudarshan Babu and Michael Maire},\nyear={2020},\nurl={https://openreview.net/forum?id=HylNWkHtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylNWkHtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference/Paper1540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1540/Reviewers", "ICLR.cc/2020/Conference/Paper1540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1540/Authors|ICLR.cc/2020/Conference/Paper1540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154505, "tmdate": 1576860550903, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference/Paper1540/Reviewers", "ICLR.cc/2020/Conference/Paper1540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1540/-/Official_Comment"}}}, {"id": "SJlv1snKjS", "original": null, "number": 8, "cdate": 1573665502878, "ddate": null, "tcdate": 1573665502878, "tmdate": 1573665502878, "tddate": null, "forum": "HylNWkHtvB", "replyto": "rklCY7VJ9r", "invitation": "ICLR.cc/2020/Conference/Paper1540/-/Official_Comment", "content": {"title": "Response to reviewer 4 [2/2]", "comment": "\u201cusing the same optimal hyperparams as the WRN-28-4 task (...) makes the other claim about AvaGrad generalizing just as well as SGD weaker\u201d\n\nThis is an important observation. We re-tuned the learning rate of SGD for CIFAR 10/100  with the Wide ResNet 28-10, and for PTB with the 4x1000 LSTM, yielding the following performance for each learning rate:\n\nC10 (val error):\n1.0:  \t8.84%\n0.5:  \t4.98%\n0.1:  \t3.86%\n0.05: \t4.20%\n0.01: \t5.14%\n\nC100 (val error):\n1.0:  \t37.13%\n0.5:  \t23.25%\n0.1:  \t19.05%\n0.05: \t19.92%\n0.01: \t22.51%\n\nPTB (bits per character, lower is better):\n100.0: \t1.473 bpc\n20.0:  \t1.238 bpc\n10.0:  \t1.253 bpc\n2.0:\t\t1.298 bpc\n1.0: \t\t1.348 bpc\n\nThe best learning rates, even when re-tuning SGD on the large experiments, were the same as the ones we used to get the results in Table 1. Performing the same level of tuning we did for the Wide ResNet 28-4 and 4x300 LSTM models for adaptive methods is unfeasible, as it consisted of over 150 runs for each algorithm. Since adaptive methods, even when not re-tuned, still outperform a re-tuned SGD, our findings remain consistent.\n\nIn practice, it is common to tune hyperparameters in smaller models and translate these to large-scale experiments \u2014 moreover, the hyperparameters used for SGD coincide with the ones extensively used in the literature, e.g. the ResNet (He et al.\u201915), DenseNet (Huang et al.\u201916), Wide ResNet (Zagoruyko & Komodakis\u201916) and ResNeXt (Xie et al.\u201916) papers.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cOne of the key claims that adaptive gradient methods generalize better when using a large damping (epsilon) parameter has appeared in previous papers as well [2, 3].\u201d\n\nThe values for epsilon explored in [2] and [3] are at most 1e-3, a value 100 times smaller than the one we found to be optimal for Adam/AMSGrad and 10,000 times smaller than the optimal one for AvaGrad/AdamW.\n\nTo clarify our contribution: we observe that the optimal hyperparameters for popular adaptive methods can be many orders of magnitude larger than the ones typically explored in the literature. Tuning them to such extreme ranges had not been done before due to the computational costs of grid search, especially since the optimal values for epsilon are strongly coupled with the learning rate (Figure 2 and 3, left plots) in a non-linear manner (more specifically, there are two visible \u2018regimes\u2019 that determine how epsilon and the learning rate interact).\n\nOur proposed method, AvaGrad, effectively decouples the two (Figure 2 and 3, right plots), hence hyperparameter tuning can be broken into two line searches. The precise interaction between the learning rate and epsilon, as shown in our figures, has not appeared in previous works, nor has an effective method to tune both hyperparameters without yielding quadratic time complexity. Lastly, showing that adaptive methods \u2014 even Adam \u2014  can outperform SGD on ImageNet, without extra caveats such as involved warmup schedules, is an important and novel experimental result.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cit is not clear whether the worse rate is due to the analysis\u201d / \u201ccan be confusing to the reader, and it is best to explicitly mention the setting under which the different results were derived\u201d\n\nWe agree that these can be sources of confusion to the readers, and we have clarified this in the revised version of the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Independent Dominance of Adaptive Methods", "authors": ["Pedro Savarese", "David McAllester", "Sudarshan Babu", "Michael Maire"], "authorids": ["savarese@ttic.edu", "mcallester@ttic.edu", "sudarshan@ttic.edu", "mmaire@uchicago.edu"], "keywords": [], "abstract": "From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. This later observation, alongside of AvaGrad's decoupling of hyperparameters, could make it the preferred optimizer for deep learning, replacing both SGD and Adam.", "pdf": "/pdf/9af302ed6da4c59db377e6e67f134d42c2426975.pdf", "paperhash": "savarese|domainindependent_dominance_of_adaptive_methods", "original_pdf": "/attachment/1676dc516783c5377fcdff65c8f566d994c341af.pdf", "_bibtex": "@misc{\nsavarese2020domainindependent,\ntitle={Domain-Independent Dominance of Adaptive Methods},\nauthor={Pedro Savarese and David McAllester and Sudarshan Babu and Michael Maire},\nyear={2020},\nurl={https://openreview.net/forum?id=HylNWkHtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylNWkHtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference/Paper1540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1540/Reviewers", "ICLR.cc/2020/Conference/Paper1540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1540/Authors|ICLR.cc/2020/Conference/Paper1540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154505, "tmdate": 1576860550903, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference/Paper1540/Reviewers", "ICLR.cc/2020/Conference/Paper1540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1540/-/Official_Comment"}}}, {"id": "H1xb4q2toB", "original": null, "number": 7, "cdate": 1573665320839, "ddate": null, "tcdate": 1573665320839, "tmdate": 1573665320839, "tddate": null, "forum": "HylNWkHtvB", "replyto": "HkgBFoJCKS", "invitation": "ICLR.cc/2020/Conference/Paper1540/-/Official_Comment", "content": {"title": "Response to reviewer 3 [1/2] ", "comment": "Thank you for the review and your comments. We address your points individually below \u2014 please let us know if we can clarify or address any further concerns.\n\n\n\n\u201cAdaShift was published at ICLR last year and seems to include a closely related analysis and update rule\u201d\n\nThe differences in the analysis and update rule are significant. AvaGrad, our main contribution, includes the convergence fix of Delayed Adam \u2014 a delay in the computation of v_t \u2014  but, more significantly, also applies a new adaptive scaling factor to gradients.  This new adaptive scaling rule is responsible for AvaGrad\u2019s superior performance on real datasets and its better hyperparameter separation.\n\nContrasting Delayed Adam with AdaShift, the update rule in AdaShift applies a delay in the computation of v_t and, at the same time, a limited horizon on the computation of m_t. If we set n=1 and \\phi as the identity function, then we recover an update rule that is similar to Delayed Adam, but with \\beta_1 = 0 (that is, we lose first-order momentum). In the general setting where n>1, AdaShift requires storing a history of the past n gradients, and there is little relation to the update rule of Delayed Adam.\n\nAvaGrad, and not Delayed Adam, is our actual contribution in terms of a new adaptive method. AvaGrad\u2019s key significance is that the parameter-wise learning rates \\eta_t are normalized, which is precisely what decouples the learning rate and epsilon. In our presentation, Delayed Adam serves as a motivation for the design of AvaGrad: the normalization of \\eta is inspired by the convergence rate of Delayed Adam.\n\nIn terms of analysis, Zhou et al. analyze AdaShift in the online convex optimization framework, while we provide convergence rates in the smooth stochastic non-convex setting: both the implications and the proof technique differ significantly between the two.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cCould you further clarify the differences between the two\u201d\n\nFor any update rule where the gradient and \\eta are uncorrelated, we can use Theorem 2 (considering standard assumptions) to assure convergence regardless of how \\eta is computed. On the other hand, if \\eta has a different form, then having the gradient and v_t to be uncorrelated might not be enough to guarantee convergence. In other words, to guarantee convergence, correcting for the gradient and \\eta is sufficient for any functional form of \\eta, while correcting for the gradient and v_t is might not be sufficient (it is sufficient, however, when \\eta_t is only a function of v_t).\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201ctheir Theorem 1 could be compared to yours\u201d\n\nAlthough this might be a minor concern, the differences are significant and important: their Theorem 1 is a statement about regret in the online convex optimization framework, while our Theorem 1 is about stationarity in the stochastic smooth non-convex setting.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cincluding AdaShift in your experiments would be very useful for demonstrating their differences\u201d\n\nThanks for the suggestion -- we have now added AdaShift to our experiments. Following the same protocol we used for all adaptive methods, we first performed grid search over the learning rate and epsilon on CIFAR with a Wide ResNet 28-4, where AdaShift performed best with lr = epsilon = 1.0. Next, we trained a Wide ResNet 28-10, where AdaShift achieved 4.08% and 18.88% error on CIFAR10 and CIFAR100 (outperforming every adaptive method except for AvaGrad on the latter). Following the same protocol n Penn Treebank, it yielded a bpc of 1.274 with a 4-layer LSTM with 1000 units per layer. We won\u2019t have ImageNet results for AdaShift by the rebuttal deadline, but we will add them to the camera-ready version of the paper.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cretuning across experiments\u201d\n\nThis is an important observation. We re-tuned the learning rate of SGD for CIFAR 10/100  with the Wide ResNet 28-10, and for PTB with the 4x1000 LSTM, yielding the following performance for each learning rate:\n\nC10 (val error):\n1.0:  \t8.84%\n0.5:  \t4.98%\n0.1:  \t3.86%\n0.05: \t4.20%\n0.01: \t5.14%\n\nC100 (val error):\n1.0:  \t37.13%\n0.5:  \t23.25%\n0.1:  \t19.05%\n0.05: \t19.92%\n0.01: \t22.51%\n\nPTB (bits per character, lower is better):\n100.0: \t1.473 bpc\n20.0:  \t1.238 bpc\n10.0:  \t1.253 bpc\n2.0:\t\t1.298 bpc\n1.0: \t\t1.348 bpc\n\nThe best learning rates, even when re-tuning SGD on the large experiments, were the same as the ones we used to get the results in Table 1. Performing the same level of tuning we did for the Wide ResNet 28-4 and 4x300 LSTM models for adaptive methods is unfeasible, as it consisted of over 150 runs for each algorithm. Since adaptive methods, even when not re-tuned, still outperform a re-tuned SGD, our findings remain consistent.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Independent Dominance of Adaptive Methods", "authors": ["Pedro Savarese", "David McAllester", "Sudarshan Babu", "Michael Maire"], "authorids": ["savarese@ttic.edu", "mcallester@ttic.edu", "sudarshan@ttic.edu", "mmaire@uchicago.edu"], "keywords": [], "abstract": "From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. This later observation, alongside of AvaGrad's decoupling of hyperparameters, could make it the preferred optimizer for deep learning, replacing both SGD and Adam.", "pdf": "/pdf/9af302ed6da4c59db377e6e67f134d42c2426975.pdf", "paperhash": "savarese|domainindependent_dominance_of_adaptive_methods", "original_pdf": "/attachment/1676dc516783c5377fcdff65c8f566d994c341af.pdf", "_bibtex": "@misc{\nsavarese2020domainindependent,\ntitle={Domain-Independent Dominance of Adaptive Methods},\nauthor={Pedro Savarese and David McAllester and Sudarshan Babu and Michael Maire},\nyear={2020},\nurl={https://openreview.net/forum?id=HylNWkHtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylNWkHtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference/Paper1540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1540/Reviewers", "ICLR.cc/2020/Conference/Paper1540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1540/Authors|ICLR.cc/2020/Conference/Paper1540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154505, "tmdate": 1576860550903, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference/Paper1540/Reviewers", "ICLR.cc/2020/Conference/Paper1540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1540/-/Official_Comment"}}}, {"id": "H1gL5F3Yir", "original": null, "number": 6, "cdate": 1573665165702, "ddate": null, "tcdate": 1573665165702, "tmdate": 1573665165702, "tddate": null, "forum": "HylNWkHtvB", "replyto": "HkgBFoJCKS", "invitation": "ICLR.cc/2020/Conference/Paper1540/-/Official_Comment", "content": {"title": "Response to reviewer 3 [2/2]", "comment": "\u201cCan you comment on why this diminished adaptivity would be desirable in the worst case scenario analyzed?\u201d\n\nExactly characterizing why adaptivity is undesirable in theory is beyond the scope of this paper, but the same observation is present in previous papers in the literature, for example:\n\nStaib et al. - Escaping Saddle Points with Adaptive Gradient Methods (check Section 5.1)\nDe et al. - Convergence guarantees for RMSProp and ADAM in non-convex optimization and an empirical comparison to Nesterov acceleration\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cWas SGD with momentum used?\u201d\n\nWe used SGD with a Nesterov momentum of 0.9, following seminal works such as the ResNet (He et al.\u201915), DenseNet (Huang et al.\u201916), Wide ResNet (Zagoruyko & Komodakis\u201916) and ResNeXt (Xie et al.\u201916) papers.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cWas a validation set used for CIFAR?\u201d\n\nWe used a validation set of 5k examples sampled from the training set for the hyperparameter search and the results in Figure 2 and 3. For the final results in Table 1, we used the actual test set composed of 10k images. We have updated the paper to clarify this.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cI would be curious if the trend continues for more extreme values of alpha and epsilon\u201d:\n\nWe have performed preliminary experiments and observed that the trend indeed continues for extreme values of alpha and epsilon when training with Adam or AMSGrad. In particular, the performance with alpha = c * epsilon for some fixed c is nearly identical regardless of alpha and epsilon, as long as they are large enough. For a motivation why this happens, note that if epsilon is large, then \\eta_t = 1 / (\\sqrt(v_t) + \\epsilon) is approximately 1 / \\epsilon. If we set \\alpha = c * \\epsilon, then the update rule becomes w_{t+1} = w_t - c * \\epsilon * g_t / \\epsilon = w_t - c * g_t, regardless of \\alpha and \\epsilon.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cit is much more common to use SGD with momentum\u201d\n\nBy \u2018vanilla SGD\u2019 we meant SGD with momentum. We use SGD with momentum for all experiments in the paper. We have changed \u2018vanilla SGD\u2019 to \u2018SGD\u2019 in the paper.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cIt would be nice to highlight in color the diff from vanilla Adam in the Algorithm sections.\u201d\n\nWe have put the differences in red in the revised version of the paper.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cIt is not super clear from the text how in eq 26 you get \\sum{E[f(w_t)|Z]} = f(w_1)\u201d\n\nHere, f(w_1) - E[f(w_{T+1})|Z] is the result of a telescoping sum (more specifically, the sum in the right-hand side of the first line in eq 26). First, we have that E_S[E_{s_t}[f(w_{t+1})]|Z] = E_S[f(w_{t+1})] due to the assumption. With this, the sum has the form \\sum_{t=1}^T a_t - a_{t+1} (where a_t is the expectation of f(w_t)) and the sum of all terms equals to a_1 - a_{T+1} by telescoping sum. Since w_1 does not depend on how points are sampled, we have E[f(w_1)] = f(w_1), finally yielding f(w_1) - E[f(w_{T+1})|Z]. We have updated the paper to make this clearer, adding an additional step in the derivation.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cI believe the H in the leftmost term in the last line of eq 33 should be an L.\u201d\n\nThis was indeed a typo, which we have fixed in the revised version \u2014 thanks for pointing it out.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cauthors commonly use polynomial, linear, exponential, cosine, or other learning rate decay/warmups\u201d\n\nWe have changed \u2018typically\u2019 to \u2018not uncommonly\u2019 to be more precise. In this statement we were referring specifically to the papers that presented our baselines (Zagoruyko & Komodakis, Merity et al.), which all use a step-wise learning rate schedule.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Independent Dominance of Adaptive Methods", "authors": ["Pedro Savarese", "David McAllester", "Sudarshan Babu", "Michael Maire"], "authorids": ["savarese@ttic.edu", "mcallester@ttic.edu", "sudarshan@ttic.edu", "mmaire@uchicago.edu"], "keywords": [], "abstract": "From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. This later observation, alongside of AvaGrad's decoupling of hyperparameters, could make it the preferred optimizer for deep learning, replacing both SGD and Adam.", "pdf": "/pdf/9af302ed6da4c59db377e6e67f134d42c2426975.pdf", "paperhash": "savarese|domainindependent_dominance_of_adaptive_methods", "original_pdf": "/attachment/1676dc516783c5377fcdff65c8f566d994c341af.pdf", "_bibtex": "@misc{\nsavarese2020domainindependent,\ntitle={Domain-Independent Dominance of Adaptive Methods},\nauthor={Pedro Savarese and David McAllester and Sudarshan Babu and Michael Maire},\nyear={2020},\nurl={https://openreview.net/forum?id=HylNWkHtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylNWkHtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference/Paper1540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1540/Reviewers", "ICLR.cc/2020/Conference/Paper1540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1540/Authors|ICLR.cc/2020/Conference/Paper1540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154505, "tmdate": 1576860550903, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference/Paper1540/Reviewers", "ICLR.cc/2020/Conference/Paper1540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1540/-/Official_Comment"}}}, {"id": "SyltgthtiH", "original": null, "number": 5, "cdate": 1573665008885, "ddate": null, "tcdate": 1573665008885, "tmdate": 1573665008885, "tddate": null, "forum": "HylNWkHtvB", "replyto": "r1lrEWi3YB", "invitation": "ICLR.cc/2020/Conference/Paper1540/-/Official_Comment", "content": {"title": "Response to reviewer 2 [1/2] ", "comment": "Thank you for the review. We address your points individually below \u2014 please let us know if we can clarify or address any further concerns.\n\n\n\n\"If the Adam-type algorithms are the delayed version in Table 1?\u201d\n\nNo, the adaptive methods in Table 1 are not the delayed versions. In Table 1, the only method that applies a delay in the computation of v_t is AvaGrad (and, consequently, AvaGradW). Delayed Adam was only used in the synthetic experiment of Figure 1, for the purpose of theoretical motivation and evaluation.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cIt is not compatible with AdamW\u201d\n\nThere is no issue of compatibility: we are proposing a new optimizer, not modifying existing ones. We do not modify Adam or AdamW in any form for the results in Table 1: other than AvaGrad and AvaGradW, we use PyTorch built-in or official implementations for each method, and the results were achieved when running with hyperparameters found after extensive grid search (Figures 2 and 3).\n\nThe optimizer referred as \u2018AvaGradW\u2019 in Table 1, which yielded the best results on Penn Treebank, is the result of applying weight decay as in Loshchilov & Hutter to AvaGrad, our newly proposed optimizer. \n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cAre you still using bias correction in the proposed method?\u201d\n\nWe use bias correction similarly to Adam, dividing m_t by 1 - \\beta_1^t, and v_{t-1} by 1 - \\beta_2^{t-1}.The norm of v_{t-1}, used to scale the learning rate, is computed from the bias-corrected v_{t-1}.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cDo you update the model for the first step?\u201d\nWe do not update the model in the first step.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n\u201cThe results on the image datasets seem too good to be true.\u201d\n\nOur results are correct as reported and highlight the value of our proposed optimizer, which improves results across different datasets and architectures.\n\nTo achieve improved results with existing optimizers, we had to search over a hyperparameter space larger than typically employed, which may be why those best case results appear surprisingly good. See an example run with hyperparameter settings where Adam outperforms SGD at the end of our reply."}, "signatures": ["ICLR.cc/2020/Conference/Paper1540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Independent Dominance of Adaptive Methods", "authors": ["Pedro Savarese", "David McAllester", "Sudarshan Babu", "Michael Maire"], "authorids": ["savarese@ttic.edu", "mcallester@ttic.edu", "sudarshan@ttic.edu", "mmaire@uchicago.edu"], "keywords": [], "abstract": "From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. This later observation, alongside of AvaGrad's decoupling of hyperparameters, could make it the preferred optimizer for deep learning, replacing both SGD and Adam.", "pdf": "/pdf/9af302ed6da4c59db377e6e67f134d42c2426975.pdf", "paperhash": "savarese|domainindependent_dominance_of_adaptive_methods", "original_pdf": "/attachment/1676dc516783c5377fcdff65c8f566d994c341af.pdf", "_bibtex": "@misc{\nsavarese2020domainindependent,\ntitle={Domain-Independent Dominance of Adaptive Methods},\nauthor={Pedro Savarese and David McAllester and Sudarshan Babu and Michael Maire},\nyear={2020},\nurl={https://openreview.net/forum?id=HylNWkHtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylNWkHtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference/Paper1540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1540/Reviewers", "ICLR.cc/2020/Conference/Paper1540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1540/Authors|ICLR.cc/2020/Conference/Paper1540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154505, "tmdate": 1576860550903, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference/Paper1540/Reviewers", "ICLR.cc/2020/Conference/Paper1540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1540/-/Official_Comment"}}}, {"id": "H1g0TuhKir", "original": null, "number": 4, "cdate": 1573664966104, "ddate": null, "tcdate": 1573664966104, "tmdate": 1573664966104, "tddate": null, "forum": "HylNWkHtvB", "replyto": "r1lrEWi3YB", "invitation": "ICLR.cc/2020/Conference/Paper1540/-/Official_Comment", "content": {"title": "Response to reviewer 2 [2/2]", "comment": "\u201cImplementation Issue\u201d\n\nThe learning rates of Adam and Delayed Adam are not equivalent. As an informal motivation to see this, consider a sequence of many samples with small gradients, followed by a sample with a large gradient. For Adam, the large gradient will be immediately used to update v_t, which will increase, hence decreasing \\eta_t and consequently also the step size. On the other hand, for Delayed Adam, the large gradient will not affect \\eta_t, but only \\eta_{t+1}, hence the step size will be larger than the one which Adam would have computed. In practice, to achieve similar behavior with Delayed Adam, one should use a smaller learning rate than the one used with Adam.\n\nWe have performed runs using the same codebase (https://github.com/LiyuanLucasLiu/RAdam/tree/master/cifar_imagenet) so that results can be more easily replicated. For Delayed Adam, we performed the steps 2 to 4 in your review so that we have a matching implementation (which is equivalent to the one used in the experimental results in our paper), referred to as \u2018dadam\u2019 when chosen by command-line arguments below. To control the epsilon parameter, we added a command-line argument args.eps:\n\nparser.add_argument('--eps', default=1e-8, type=float, help='epsilon parameter for adaptive methods')\n\nWhich is used to instantiate the adaptive methods, e.g.:\n\noptimizer = optim.Adam(model.parameters(), lr=args.lr, betas=(args.beta1, args.beta2), weight_decay=args.weight_decay, eps=args.eps)\n\nTo facilitate reproducibility, we remove cudnn.benchmark=True in cifar.py (right after model = torch.nn.DataParallel(model).cuda()), replacing it with:\n\ncudnn.deterministic = True\ncudnn.benchmark = False\n\nWe also use a manual seed 0 for all experiments. The commands, followed by the given results, were:\n\nAdam, lr=0.001, eps=1e-8\npython cifar.py -a resnet --depth 20 --epochs 164 --schedule 81 122 --gamma 0.1 --wd 1e-4 --optimizer adam  --beta1 0.9 --beta2 0.999  --checkpoint ./logdir --gpu-id 0 --model_name adam_001 --lr 0.001 --manualSeed 0 --eps 1e-8\nBest acc: 90.86%\n\nDelayed Adam, lr=0.001, eps=1e-8\npython cifar.py -a resnet --depth 20 --epochs 164 --schedule 81 122 --gamma 0.1 --wd 1e-4 --optimizer dadam  --beta1 0.9 --beta2 0.999  --checkpoint ./logdir --gpu-id 0 --model_name dadam_001 --lr 0.001 --manualSeed 0 --eps 1e-8\nBest acc: 90.69%\n\nThese show that Adam and Delayed Adam perform similarly given standard values for the learning rate.\n\nRegarding the \u2018too good to be true results\u2019, here there are commands with hyperparameter settings showing Adam can outperform SGD for ResNet training on CIFAR. We used the same values for the learning rate and epsilon as in our paper \u2014 no extra hyperparameter search was performed.\n\nAdam, lr=0.1, eps=0.1\npython cifar.py -a resnet --depth 20 --epochs 164 --schedule 81 122 --gamma 0.1 --wd 1e-4 --optimizer adam  --beta1 0.9 --beta2 0.999  --checkpoint ./logdir --gpu-id 0 --model_name adam_01 --lr 0.1 --manualSeed 0 --eps 0.1\nBest acc: 92.21%\n\nSGD, lr=0.1\npython cifar.py -a resnet --depth 20 --epochs 164 --schedule 81 122 --gamma 0.1 --wd 1e-4 --optimizer sgd  --checkpoint ./logdir --gpu-id 0 --model_name sgd_01 --lr 0.1 --manualSeed 0\nBest acc: 91.91%\n\nTo further facilitate reproducibility, we are preparing a codebase to be publicly available with our implementation of AvaGrad and the code to run our experiments. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1540/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Independent Dominance of Adaptive Methods", "authors": ["Pedro Savarese", "David McAllester", "Sudarshan Babu", "Michael Maire"], "authorids": ["savarese@ttic.edu", "mcallester@ttic.edu", "sudarshan@ttic.edu", "mmaire@uchicago.edu"], "keywords": [], "abstract": "From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. This later observation, alongside of AvaGrad's decoupling of hyperparameters, could make it the preferred optimizer for deep learning, replacing both SGD and Adam.", "pdf": "/pdf/9af302ed6da4c59db377e6e67f134d42c2426975.pdf", "paperhash": "savarese|domainindependent_dominance_of_adaptive_methods", "original_pdf": "/attachment/1676dc516783c5377fcdff65c8f566d994c341af.pdf", "_bibtex": "@misc{\nsavarese2020domainindependent,\ntitle={Domain-Independent Dominance of Adaptive Methods},\nauthor={Pedro Savarese and David McAllester and Sudarshan Babu and Michael Maire},\nyear={2020},\nurl={https://openreview.net/forum?id=HylNWkHtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylNWkHtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference/Paper1540/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1540/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1540/Reviewers", "ICLR.cc/2020/Conference/Paper1540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1540/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1540/Authors|ICLR.cc/2020/Conference/Paper1540/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504154505, "tmdate": 1576860550903, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1540/Authors", "ICLR.cc/2020/Conference/Paper1540/Reviewers", "ICLR.cc/2020/Conference/Paper1540/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1540/-/Official_Comment"}}}, {"id": "r1lrEWi3YB", "original": null, "number": 1, "cdate": 1571758381062, "ddate": null, "tcdate": 1571758381062, "tmdate": 1572972455198, "tddate": null, "forum": "HylNWkHtvB", "replyto": "HylNWkHtvB", "invitation": "ICLR.cc/2020/Conference/Paper1540/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThis paper proposes a new adaptive method, which is called AvaGrad. The authors first show that Adam may not converge to a stationary point for a stochastic convex optimization in Theorem1, which is closely related to [1]. They then show that by simply making $eta_t$ to be independent of the sample $s_t$, Adam is able to converge just like SGD in Theorem2. Theorem2 follows the standard SGD techniques. Next, they propose AVAGRAD, which is based on the idea of getting rid of the effect of $\\epsilon$. \n\nStrength:\nThe experiment results are impressive. They show that Adam can outperform SGD on vision tasks.\nRecently, people have found out that $\\epsilon$ is a very sensitive hyper-parameter. It is good to see some research directly addresses this problem. \n\nWeakness:\nThe word \"domain\" is confusing. \nIf the Adam-type algorithms are the delayed version in Table 1?\nIt is not compatible with AdamW. \nThe results on the image datasets seem too good to be true.\n\nImplementation Issue:\n***Many implementation details in the below discussion are different from the paper (e.g. hyperparameters and network architecture). So the following experiment results may not be used for assessment of the quality of the proposed method.***\nI tried the proposed Delayed Adam on CIFAR-10 using the codebase in (https://github.com/LiyuanLucasLiu/RAdam/tree/master/cifar_imagenet). The performance seems the same as Adam. Delayed Adam even leads to a *divergence* problem, especially with a large learning rate (0.03). The divergence problem never happens when using Adam and AdamW with the same hyperparameters.\nImplementation details: \n1. Replace the optimizer with the original PyTorch Adam implementation. (https://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py) \n2. Swap line 96 and 108 as suggested in the paper. \n3. Modified line 89 (bias_correction2=1 - beta2 ** (state['step']-1)) \n4. Do not run line 97-107 when state['step']==1. \n5. Run the following code: python cifar.py -a resnet --depth 20 --epochs 164 --schedule 81 122 --gamma 0.1 --wd 1e-4 --optimizer adam  --beta1 0.9 --beta2 0.999  --checkpoint ./logdir --gpu-id 0 --model_name adam_003 --lr 0.03\n\nIf the authors can provide more implementation details, I would promote my rating. \ne.g., \n1. Are you still using bias correction in the proposed method? If so how do you use them?\n2. Do you update the model for the first step?\n\nReference:\n[1] On the convergence of adam and beyond\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1540/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1540/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Domain-Independent Dominance of Adaptive Methods", "authors": ["Pedro Savarese", "David McAllester", "Sudarshan Babu", "Michael Maire"], "authorids": ["savarese@ttic.edu", "mcallester@ttic.edu", "sudarshan@ttic.edu", "mmaire@uchicago.edu"], "keywords": [], "abstract": "From a simplified analysis of adaptive methods, we derive AvaGrad, a new optimizer which outperforms SGD on vision tasks when its adaptability is properly tuned. We observe that the power of our method is partially explained by a decoupling of learning rate and adaptability, greatly simplifying hyperparameter search. In light of this observation, we demonstrate that, against conventional wisdom, Adam can also outperform SGD on vision tasks, as long as the coupling between its learning rate and adaptability is taken into account. In practice, AvaGrad matches the best results, as measured by generalization accuracy, delivered by any existing optimizer (SGD or adaptive) across image classification (CIFAR, ImageNet) and character-level language modelling (Penn Treebank) tasks. This later observation, alongside of AvaGrad's decoupling of hyperparameters, could make it the preferred optimizer for deep learning, replacing both SGD and Adam.", "pdf": "/pdf/9af302ed6da4c59db377e6e67f134d42c2426975.pdf", "paperhash": "savarese|domainindependent_dominance_of_adaptive_methods", "original_pdf": "/attachment/1676dc516783c5377fcdff65c8f566d994c341af.pdf", "_bibtex": "@misc{\nsavarese2020domainindependent,\ntitle={Domain-Independent Dominance of Adaptive Methods},\nauthor={Pedro Savarese and David McAllester and Sudarshan Babu and Michael Maire},\nyear={2020},\nurl={https://openreview.net/forum?id=HylNWkHtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylNWkHtvB", "replyto": "HylNWkHtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1540/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575599942915, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1540/Reviewers"], "noninvitees": [], "tcdate": 1570237735895, "tmdate": 1575599942928, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1540/-/Official_Review"}}}], "count": 13}