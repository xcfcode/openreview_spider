{"notes": [{"id": "B1l1b205KX", "original": "HJxST2XqY7", "number": 1132, "cdate": 1538087926992, "ddate": null, "tcdate": 1538087926992, "tmdate": 1545355399074, "tddate": null, "forum": "B1l1b205KX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Unsupervised Disentangling Structure and Appearance", "abstract": "It is challenging to disentangle an object into two orthogonal spaces of structure and appearance since each can influence the visual observation in a different and unpredictable way. It is rare for one to have access to a large number of data to help separate the influences. In this paper, we present a novel framework to learn this disentangled representation in a completely unsupervised manner. We address this problem in a two-branch Variational Autoencoder framework. For the structure branch, we project the latent factor into a soft structured point tensor and constrain it with losses derived from prior knowledge. This encourages the branch to distill geometry information. Another branch learns the complementary appearance information. The two branches form an effective framework that can disentangle object's structure-appearance representation without any human annotation. We evaluate our approach on four image datasets, on which we demonstrate the superior disentanglement and visual analogy quality both in synthesis and real-world data. We are able to generate photo-realistic images with 256*256 resolution that are clearly disentangled in structure and appearance.", "keywords": ["disentangled representations", "VAE", "generative models", "unsupervised learning"], "authorids": ["wuwenyan@sensetime.com", "kaidicao@cs.stanford.edu", "chengli@sensetime.com", "qianchen@sensetime.com", "ccloy225@gmail.com"], "authors": ["Wayne Wu", "Kaidi Cao", "Cheng Li", "Chen Qian", "Chen Change Loy"], "TL;DR": "We present a novel framework to learn the disentangled representation of structure and appearance in a completely unsupervised manner. ", "pdf": "/pdf/e4e29d156e204dca1bf7c15a20486b2687fa4fa8.pdf", "paperhash": "wu|unsupervised_disentangling_structure_and_appearance", "_bibtex": "@misc{\nwu2019unsupervised,\ntitle={Unsupervised Disentangling Structure and Appearance},\nauthor={Wayne Wu and Kaidi Cao and Cheng Li and Chen Qian and Chen Change Loy},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l1b205KX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HkgdmSBxlN", "original": null, "number": 1, "cdate": 1544733983746, "ddate": null, "tcdate": 1544733983746, "tmdate": 1545354513181, "tddate": null, "forum": "B1l1b205KX", "replyto": "B1l1b205KX", "invitation": "ICLR.cc/2019/Conference/-/Paper1132/Meta_Review", "content": {"metareview": "With an average review score of 4.67 and a short review for the one positive review it is just not possible to accept the paper.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "The aggregate assessment of reviewers is just not strong enough to warrant acceptance"}, "signatures": ["ICLR.cc/2019/Conference/Paper1132/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1132/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Disentangling Structure and Appearance", "abstract": "It is challenging to disentangle an object into two orthogonal spaces of structure and appearance since each can influence the visual observation in a different and unpredictable way. It is rare for one to have access to a large number of data to help separate the influences. In this paper, we present a novel framework to learn this disentangled representation in a completely unsupervised manner. We address this problem in a two-branch Variational Autoencoder framework. For the structure branch, we project the latent factor into a soft structured point tensor and constrain it with losses derived from prior knowledge. This encourages the branch to distill geometry information. Another branch learns the complementary appearance information. The two branches form an effective framework that can disentangle object's structure-appearance representation without any human annotation. We evaluate our approach on four image datasets, on which we demonstrate the superior disentanglement and visual analogy quality both in synthesis and real-world data. We are able to generate photo-realistic images with 256*256 resolution that are clearly disentangled in structure and appearance.", "keywords": ["disentangled representations", "VAE", "generative models", "unsupervised learning"], "authorids": ["wuwenyan@sensetime.com", "kaidicao@cs.stanford.edu", "chengli@sensetime.com", "qianchen@sensetime.com", "ccloy225@gmail.com"], "authors": ["Wayne Wu", "Kaidi Cao", "Cheng Li", "Chen Qian", "Chen Change Loy"], "TL;DR": "We present a novel framework to learn the disentangled representation of structure and appearance in a completely unsupervised manner. ", "pdf": "/pdf/e4e29d156e204dca1bf7c15a20486b2687fa4fa8.pdf", "paperhash": "wu|unsupervised_disentangling_structure_and_appearance", "_bibtex": "@misc{\nwu2019unsupervised,\ntitle={Unsupervised Disentangling Structure and Appearance},\nauthor={Wayne Wu and Kaidi Cao and Cheng Li and Chen Qian and Chen Change Loy},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l1b205KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1132/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352953042, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1l1b205KX", "replyto": "B1l1b205KX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1132/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1132/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1132/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352953042}}}, {"id": "BJeGI4xR37", "original": null, "number": 3, "cdate": 1541436489835, "ddate": null, "tcdate": 1541436489835, "tmdate": 1541533394913, "tddate": null, "forum": "B1l1b205KX", "replyto": "B1l1b205KX", "invitation": "ICLR.cc/2019/Conference/-/Paper1132/Official_Review", "content": {"title": "Interesting work", "review": "This paper proposes an unsupervised method to disentangle the latent code of VAE. Overall, it is novel and well written. The experiment has shown good performance of the proposed method.\n \nI have some concerns as follows:\n1.  In Eq.(1), y is assumed to follow a Gaussian distribution. Is it possible that y follows a multinomial distribution? Then, this model can be used for clustering.\n\n2. In section 2.3, the concatenation between z and y is used to learn a complementary of y.  Why does the concatenation encourage  to learn the complementary of y? More explanations are needed.  Additionally, some experiments are needed to verify this claim.\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1132/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Disentangling Structure and Appearance", "abstract": "It is challenging to disentangle an object into two orthogonal spaces of structure and appearance since each can influence the visual observation in a different and unpredictable way. It is rare for one to have access to a large number of data to help separate the influences. In this paper, we present a novel framework to learn this disentangled representation in a completely unsupervised manner. We address this problem in a two-branch Variational Autoencoder framework. For the structure branch, we project the latent factor into a soft structured point tensor and constrain it with losses derived from prior knowledge. This encourages the branch to distill geometry information. Another branch learns the complementary appearance information. The two branches form an effective framework that can disentangle object's structure-appearance representation without any human annotation. We evaluate our approach on four image datasets, on which we demonstrate the superior disentanglement and visual analogy quality both in synthesis and real-world data. We are able to generate photo-realistic images with 256*256 resolution that are clearly disentangled in structure and appearance.", "keywords": ["disentangled representations", "VAE", "generative models", "unsupervised learning"], "authorids": ["wuwenyan@sensetime.com", "kaidicao@cs.stanford.edu", "chengli@sensetime.com", "qianchen@sensetime.com", "ccloy225@gmail.com"], "authors": ["Wayne Wu", "Kaidi Cao", "Cheng Li", "Chen Qian", "Chen Change Loy"], "TL;DR": "We present a novel framework to learn the disentangled representation of structure and appearance in a completely unsupervised manner. ", "pdf": "/pdf/e4e29d156e204dca1bf7c15a20486b2687fa4fa8.pdf", "paperhash": "wu|unsupervised_disentangling_structure_and_appearance", "_bibtex": "@misc{\nwu2019unsupervised,\ntitle={Unsupervised Disentangling Structure and Appearance},\nauthor={Wayne Wu and Kaidi Cao and Cheng Li and Chen Qian and Chen Change Loy},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l1b205KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1132/Official_Review", "cdate": 1542234298811, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1l1b205KX", "replyto": "B1l1b205KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1132/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335879835, "tmdate": 1552335879835, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1132/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Bylmo-s237", "original": null, "number": 2, "cdate": 1541349787474, "ddate": null, "tcdate": 1541349787474, "tmdate": 1541533394702, "tddate": null, "forum": "B1l1b205KX", "replyto": "B1l1b205KX", "invitation": "ICLR.cc/2019/Conference/-/Paper1132/Official_Review", "content": {"title": "A conditional deep generative model that lacks in comparison with the state of the art and some experimental results are not convincing", "review": "In this paper,  a conditional deep generative model is proposed for disentangling structure (more precisely shape)  and appearance.  The architecture of the proposed system is very similar to that in [A], however, in this paper different applications are considered. The paper is relatively well-written and a number of experiments are presented. However, they are that convincing.\n\nI have two main concerns regarding this paper.\n\n1)\tThe authors have not taken into account recently proposed deep generative models for disentangling shape and appearance along other modes of visual variations. A non-exhaustive list is as follows:\n\n*GAGAN: Geometry-Aware Generative Adversarial Networks\n*Geometry-Contrastive GAN for Facial Expression Transfer\n*Cross-View Image Synthesis using Conditional GANs\n*Deforming Autoencoders: Unsupervised Disentangling of Shape and Appearance\n*Neural Face Editing with Intrinsic Image Disentangling\n\n\nThe authors should discuss how the proposed method is different from the above-mentioned ones and compare the performance of the proposed model against that obtained by GAGAN and Deforming Autoencoders, which are very relevant to the proposed one models.\n\n2)\tSome of the experimental results are not convincing. For example, in Fig. 6 it seems to me that all the chairs produced by the proposed method are identical. In the same figure, Jakab\u2019s method seems to produce more meaningful results than the proposed method which appears to implement texture style transfer, rather than shape transfer.\n\nConsidering all the above, I believe the paper needs substantial improvement prior to being considered for publication.\n\n\nReference\n\n[\u0391] Tomas Jakab, Ankush Gupta, Hakan Bilen, and Andrea Vedaldi. Conditional image generation for\nlearning the structure of visual objects. NIPS, 2018. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1132/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Disentangling Structure and Appearance", "abstract": "It is challenging to disentangle an object into two orthogonal spaces of structure and appearance since each can influence the visual observation in a different and unpredictable way. It is rare for one to have access to a large number of data to help separate the influences. In this paper, we present a novel framework to learn this disentangled representation in a completely unsupervised manner. We address this problem in a two-branch Variational Autoencoder framework. For the structure branch, we project the latent factor into a soft structured point tensor and constrain it with losses derived from prior knowledge. This encourages the branch to distill geometry information. Another branch learns the complementary appearance information. The two branches form an effective framework that can disentangle object's structure-appearance representation without any human annotation. We evaluate our approach on four image datasets, on which we demonstrate the superior disentanglement and visual analogy quality both in synthesis and real-world data. We are able to generate photo-realistic images with 256*256 resolution that are clearly disentangled in structure and appearance.", "keywords": ["disentangled representations", "VAE", "generative models", "unsupervised learning"], "authorids": ["wuwenyan@sensetime.com", "kaidicao@cs.stanford.edu", "chengli@sensetime.com", "qianchen@sensetime.com", "ccloy225@gmail.com"], "authors": ["Wayne Wu", "Kaidi Cao", "Cheng Li", "Chen Qian", "Chen Change Loy"], "TL;DR": "We present a novel framework to learn the disentangled representation of structure and appearance in a completely unsupervised manner. ", "pdf": "/pdf/e4e29d156e204dca1bf7c15a20486b2687fa4fa8.pdf", "paperhash": "wu|unsupervised_disentangling_structure_and_appearance", "_bibtex": "@misc{\nwu2019unsupervised,\ntitle={Unsupervised Disentangling Structure and Appearance},\nauthor={Wayne Wu and Kaidi Cao and Cheng Li and Chen Qian and Chen Change Loy},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l1b205KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1132/Official_Review", "cdate": 1542234298811, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1l1b205KX", "replyto": "B1l1b205KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1132/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335879835, "tmdate": 1552335879835, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1132/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkgHZE592X", "original": null, "number": 1, "cdate": 1541215228876, "ddate": null, "tcdate": 1541215228876, "tmdate": 1541533394491, "tddate": null, "forum": "B1l1b205KX", "replyto": "B1l1b205KX", "invitation": "ICLR.cc/2019/Conference/-/Paper1132/Official_Review", "content": {"title": "Paper does not justify its modifications to the VAE formulation", "review": "Summary: The authors present an autoencoding strategy for disentangling structure and appearance in image data. They achieve this using a learned, spatial prior in the VAE framework.\n\nWriting: The paper contains grammatical errors and I found Section 2 (2.1 and 2.2, specifically) to be a bit confusing as many ideas were described with words when they could have been outlined more precisely mathematically. \n\nMajor comments:\nThe paper takes ideas from Zhang et al and Jakab et al and put them in a VAE context. The paper, however, constructs the ELBO in such a way that distance it from many key ideas of the VAE. Particularly, the paper decomposes the ELBO into three terms and proceed to define these terms as they wish. In this way, the novel contributions of this paper are left unclear and many decisions left unjustified.\n\n- Incorporating landmark/spatial information into autoencoders is not a new idea. Zhang et al and Jakab et al both train autoencoders with disentangled structure, and Finn, 2016 [1] uses a similar spatial landmark strategy when learning representations. Incorporating structural information as a prior distribution (as in this paper) is an interesting idea. However, it is not clear that the defined prior log p(y) is a proper density (integrates to 1). Given the importance of a prior distribution in VAEs, this choice should be precisely justified (maybe relate it to beta-VAE or just use a properly normalizable distribution)\n\n- The paper chooses variational distributions in such a way that removes the entropy term from the KL divergences. Specifically, both q(z | x, y) and p(z | y) have fixed, identity covariance, resulting in the KL divergences equating to L_2 distance. An important part of the VAE is the explicit incorporation of uncertainty by means of learned variances. Although this can sometimes be problematic (see beta-VAE), neglecting to include these variances at all removes an important aspect of VAE.\n\n- The paper includes a likelihood model which also throws away key ideas from VAE. Although some neural likelihood models as in the VAE have issues with blurriness, the reasons are still not completely understood. However, there are well explored alternatives to what the authors propose. For example, many image-based VAEs use Bernoulli likelihoods [2, 3] or autoregressive likelihoods [4]. Autoregressive models, especially, can produce sharp images. The authors introduce a unnormalizable likelihood which combines L1 loss with a function that incorporates L1 distance in VGG space. Using VGG in the likelihood model is unjustified and seems unnecessarily complicated, given the existence of powerful decoders that already exist in VAE literature.\n\n- The authors incorporate various connections and concatenations between neural networks and distributions that further complicate the variational lower bound. For example, p(z | y) is concatenated to q(z | x, y) in addition to acting as a prior on q(z | x, y). This introduces a dependency between the likelihood model and the variational posterior which normally does not exist. Furthermore skip connections are introduced between E_\\theta and D_\\theta, which complicate the ELBO further. The authors should explicitly write out the loss function they are optimizing at this point or describe how they are modifying ELBO to justify these chocise.\n\nOverall, I find it difficult to call this a variational autoencoder given the liberal modifications to the evidence lower-bound. However, even if I was to interpret this work as an autoencoder with a custom loss function, this model ends up very similar to that in Zhang et al with the main differences being the inclusion of an equivariance constraint in Zhang that is not present in this paper and that Zhang et al use a feature map that\u2019s multiplied by landmarks to incorporate appearance information whereas this paper uses the z representation as appearance information. \n\nThe qualitative results of this paper, although good looking, are very similar to qualitative results in Zhang et al and Jakab et al. A quick comment: in Figure 6, you should use the same celebrity faces when comparing Jakab against your own work. The quantitative results only compare to the same model trained without the KL loss term; this, in my mind, is more of a sanity check than a fair baseline. The authors should be comparing against alternate strategies that incorporate spatial information, such as Zhang et al and Jakab et al.\n\n[1] Finn, Chelsea, et al. \"Deep spatial autoencoders for visuomotor learning.\" 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2016.\n[2] Chen, Xi, et al. \"Variational lossy autoencoder.\" arXiv preprint arXiv:1611.02731 (2016).\n[3] http://ruishu.io/2018/03/19/bernoulli-vae/\n[4] van den Oord, Aaron, et al. \"Conditional image generation with pixelcnn decoders.\" Advances in Neural Information Processing Systems. 2016.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1132/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Disentangling Structure and Appearance", "abstract": "It is challenging to disentangle an object into two orthogonal spaces of structure and appearance since each can influence the visual observation in a different and unpredictable way. It is rare for one to have access to a large number of data to help separate the influences. In this paper, we present a novel framework to learn this disentangled representation in a completely unsupervised manner. We address this problem in a two-branch Variational Autoencoder framework. For the structure branch, we project the latent factor into a soft structured point tensor and constrain it with losses derived from prior knowledge. This encourages the branch to distill geometry information. Another branch learns the complementary appearance information. The two branches form an effective framework that can disentangle object's structure-appearance representation without any human annotation. We evaluate our approach on four image datasets, on which we demonstrate the superior disentanglement and visual analogy quality both in synthesis and real-world data. We are able to generate photo-realistic images with 256*256 resolution that are clearly disentangled in structure and appearance.", "keywords": ["disentangled representations", "VAE", "generative models", "unsupervised learning"], "authorids": ["wuwenyan@sensetime.com", "kaidicao@cs.stanford.edu", "chengli@sensetime.com", "qianchen@sensetime.com", "ccloy225@gmail.com"], "authors": ["Wayne Wu", "Kaidi Cao", "Cheng Li", "Chen Qian", "Chen Change Loy"], "TL;DR": "We present a novel framework to learn the disentangled representation of structure and appearance in a completely unsupervised manner. ", "pdf": "/pdf/e4e29d156e204dca1bf7c15a20486b2687fa4fa8.pdf", "paperhash": "wu|unsupervised_disentangling_structure_and_appearance", "_bibtex": "@misc{\nwu2019unsupervised,\ntitle={Unsupervised Disentangling Structure and Appearance},\nauthor={Wayne Wu and Kaidi Cao and Cheng Li and Chen Qian and Chen Change Loy},\nyear={2019},\nurl={https://openreview.net/forum?id=B1l1b205KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1132/Official_Review", "cdate": 1542234298811, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1l1b205KX", "replyto": "B1l1b205KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1132/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335879835, "tmdate": 1552335879835, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1132/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}