{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028606067, "tcdate": 1490028606067, "number": 1, "id": "Hk8rOYasx", "invitation": "ICLR.cc/2017/workshop/-/paper109/acceptance", "forum": "HkXKUTVFl", "replyto": "HkXKUTVFl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Explaining the Learning Dynamics of Direct Feedback Alignment", "abstract": "    Two recently developed methods, Feedback Alignment (FA) and Direct Feedback Alignment (DFA), have been shown to obtain surprising performance on vision tasks by replacing the traditional backpropagation update with a random feedback update. However, it is still not clear what mechanisms allow learning to happen with these random updates. \n    In this work we argue that DFA can be viewed as a noisy variant of a layer-wise training method we call Linear Aligned Feedback Systems (LAFS). We support this connection theoretically by comparing the update rules for the two methods.  We additionally empirically verify that the random update matrices used in DFA work effectively as readout matrices, and that strong correlations exist between the error vectors used in the DFA and LAFS updates. With this new connection between DFA and LAFS we are able to explain why the \"alignment\" happens in DFA. ", "pdf": "/pdf/6a720648546d2209dd351e9bd75d07b1e9742573.pdf", "TL;DR": "We interpret DFA as a noisy version of a layer-wise training method we call Linear Aligned Feedback Systems (LAFS). ", "paperhash": "gilmer|explaining_the_learning_dynamics_of_direct_feedback_alignment", "conflicts": ["google.com"], "keywords": ["Theory", "Supervised Learning", "Optimization"], "authors": ["Justin Gilmer", "Colin Raffel", "Samuel S. Schoenholz", "Maithra Raghu", "and Jascha Sohl-Dickstein"], "authorids": ["gilmer@google.com", "craffel@google.com", "schsam@google.com", "maithra@google.com", "jaschasd@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028606638, "id": "ICLR.cc/2017/workshop/-/paper109/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HkXKUTVFl", "replyto": "HkXKUTVFl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028606638}}}, {"tddate": null, "tmdate": 1489453314686, "tcdate": 1489453314686, "number": 1, "id": "H1oZWpVsg", "invitation": "ICLR.cc/2017/workshop/-/paper109/public/comment", "forum": "HkXKUTVFl", "replyto": "SyPh7Dlse", "signatures": ["~Justin_Gilmer1"], "readers": ["everyone"], "writers": ["~Justin_Gilmer1"], "content": {"title": "Adding additional related work, and other uses of DFA", "comment": "Thank you for your review! Based upon your feedback, we have now updated our abstract to include further background on the usefulness of DFA and related techniques. In particular, DFA allows for parallelized updates to weight matrices which means that it could yield greater computational efficiency than traditional BP. We have also expanded our discussion on related works to include additional work on biologically plausible machine learning. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Explaining the Learning Dynamics of Direct Feedback Alignment", "abstract": "    Two recently developed methods, Feedback Alignment (FA) and Direct Feedback Alignment (DFA), have been shown to obtain surprising performance on vision tasks by replacing the traditional backpropagation update with a random feedback update. However, it is still not clear what mechanisms allow learning to happen with these random updates. \n    In this work we argue that DFA can be viewed as a noisy variant of a layer-wise training method we call Linear Aligned Feedback Systems (LAFS). We support this connection theoretically by comparing the update rules for the two methods.  We additionally empirically verify that the random update matrices used in DFA work effectively as readout matrices, and that strong correlations exist between the error vectors used in the DFA and LAFS updates. With this new connection between DFA and LAFS we are able to explain why the \"alignment\" happens in DFA. ", "pdf": "/pdf/6a720648546d2209dd351e9bd75d07b1e9742573.pdf", "TL;DR": "We interpret DFA as a noisy version of a layer-wise training method we call Linear Aligned Feedback Systems (LAFS). ", "paperhash": "gilmer|explaining_the_learning_dynamics_of_direct_feedback_alignment", "conflicts": ["google.com"], "keywords": ["Theory", "Supervised Learning", "Optimization"], "authors": ["Justin Gilmer", "Colin Raffel", "Samuel S. Schoenholz", "Maithra Raghu", "and Jascha Sohl-Dickstein"], "authorids": ["gilmer@google.com", "craffel@google.com", "schsam@google.com", "maithra@google.com", "jaschasd@google.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487357563687, "tcdate": 1487357563687, "id": "ICLR.cc/2017/workshop/-/paper109/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper109/reviewers"], "reply": {"forum": "HkXKUTVFl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487357563687}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1489452744624, "tcdate": 1487357562935, "number": 109, "id": "HkXKUTVFl", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "HkXKUTVFl", "signatures": ["~Justin_Gilmer1"], "readers": ["everyone"], "content": {"title": "Explaining the Learning Dynamics of Direct Feedback Alignment", "abstract": "    Two recently developed methods, Feedback Alignment (FA) and Direct Feedback Alignment (DFA), have been shown to obtain surprising performance on vision tasks by replacing the traditional backpropagation update with a random feedback update. However, it is still not clear what mechanisms allow learning to happen with these random updates. \n    In this work we argue that DFA can be viewed as a noisy variant of a layer-wise training method we call Linear Aligned Feedback Systems (LAFS). We support this connection theoretically by comparing the update rules for the two methods.  We additionally empirically verify that the random update matrices used in DFA work effectively as readout matrices, and that strong correlations exist between the error vectors used in the DFA and LAFS updates. With this new connection between DFA and LAFS we are able to explain why the \"alignment\" happens in DFA. ", "pdf": "/pdf/6a720648546d2209dd351e9bd75d07b1e9742573.pdf", "TL;DR": "We interpret DFA as a noisy version of a layer-wise training method we call Linear Aligned Feedback Systems (LAFS). ", "paperhash": "gilmer|explaining_the_learning_dynamics_of_direct_feedback_alignment", "conflicts": ["google.com"], "keywords": ["Theory", "Supervised Learning", "Optimization"], "authors": ["Justin Gilmer", "Colin Raffel", "Samuel S. Schoenholz", "Maithra Raghu", "and Jascha Sohl-Dickstein"], "authorids": ["gilmer@google.com", "craffel@google.com", "schsam@google.com", "maithra@google.com", "jaschasd@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "tmdate": 1489167279206, "tcdate": 1489167279206, "number": 2, "id": "SyPh7Dlse", "invitation": "ICLR.cc/2017/workshop/-/paper109/official/review", "forum": "HkXKUTVFl", "replyto": "HkXKUTVFl", "signatures": ["ICLR.cc/2017/workshop/paper109/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper109/AnonReviewer1"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "This paper argues that Direct Feedback Alignment (DFA) has a very similar update rule to Linear Aligned Feedback Systems (LAFTS) which is a layer-wise training method. Part of the argument is through similarities in the update equation and they perform experiments to verify that the different part in the update rule behaves similarly in DFA and LAFST. Unfortunately, since I'm not familiar with previous work on methods similar to DFA, I don't understand why they are useful in general (I'm not convinced that being biologically plausible is an acceptable reason for ML research).", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Explaining the Learning Dynamics of Direct Feedback Alignment", "abstract": "    Two recently developed methods, Feedback Alignment (FA) and Direct Feedback Alignment (DFA), have been shown to obtain surprising performance on vision tasks by replacing the traditional backpropagation update with a random feedback update. However, it is still not clear what mechanisms allow learning to happen with these random updates. \n    In this work we argue that DFA can be viewed as a noisy variant of a layer-wise training method we call Linear Aligned Feedback Systems (LAFS). We support this connection theoretically by comparing the update rules for the two methods.  We additionally empirically verify that the random update matrices used in DFA work effectively as readout matrices, and that strong correlations exist between the error vectors used in the DFA and LAFS updates. With this new connection between DFA and LAFS we are able to explain why the \"alignment\" happens in DFA. ", "pdf": "/pdf/6a720648546d2209dd351e9bd75d07b1e9742573.pdf", "TL;DR": "We interpret DFA as a noisy version of a layer-wise training method we call Linear Aligned Feedback Systems (LAFS). ", "paperhash": "gilmer|explaining_the_learning_dynamics_of_direct_feedback_alignment", "conflicts": ["google.com"], "keywords": ["Theory", "Supervised Learning", "Optimization"], "authors": ["Justin Gilmer", "Colin Raffel", "Samuel S. Schoenholz", "Maithra Raghu", "and Jascha Sohl-Dickstein"], "authorids": ["gilmer@google.com", "craffel@google.com", "schsam@google.com", "maithra@google.com", "jaschasd@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489167280197, "id": "ICLR.cc/2017/workshop/-/paper109/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper109/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper109/AnonReviewer2", "ICLR.cc/2017/workshop/paper109/AnonReviewer1"], "reply": {"forum": "HkXKUTVFl", "replyto": "HkXKUTVFl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper109/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper109/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489167280197}}}, {"tddate": null, "tmdate": 1488687397588, "tcdate": 1488687397588, "number": 1, "id": "rkRXbMF9x", "invitation": "ICLR.cc/2017/workshop/-/paper109/official/review", "forum": "HkXKUTVFl", "replyto": "HkXKUTVFl", "signatures": ["ICLR.cc/2017/workshop/paper109/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper109/AnonReviewer2"], "content": {"title": "Interesting work", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This paper provides a constructive proof to indicate that  Direct Feedback\nAlignment (DFA) can be viewed as a noisy variant of a layer-wise training method called Linear Aligned Feedback Systems (LAFS). It also empirically verified that the random update matrices used in DFA are effectively readout matrices.\n\nThis work is interesting because it explained why DFA works and what is the limitation of DFA.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Explaining the Learning Dynamics of Direct Feedback Alignment", "abstract": "    Two recently developed methods, Feedback Alignment (FA) and Direct Feedback Alignment (DFA), have been shown to obtain surprising performance on vision tasks by replacing the traditional backpropagation update with a random feedback update. However, it is still not clear what mechanisms allow learning to happen with these random updates. \n    In this work we argue that DFA can be viewed as a noisy variant of a layer-wise training method we call Linear Aligned Feedback Systems (LAFS). We support this connection theoretically by comparing the update rules for the two methods.  We additionally empirically verify that the random update matrices used in DFA work effectively as readout matrices, and that strong correlations exist between the error vectors used in the DFA and LAFS updates. With this new connection between DFA and LAFS we are able to explain why the \"alignment\" happens in DFA. ", "pdf": "/pdf/6a720648546d2209dd351e9bd75d07b1e9742573.pdf", "TL;DR": "We interpret DFA as a noisy version of a layer-wise training method we call Linear Aligned Feedback Systems (LAFS). ", "paperhash": "gilmer|explaining_the_learning_dynamics_of_direct_feedback_alignment", "conflicts": ["google.com"], "keywords": ["Theory", "Supervised Learning", "Optimization"], "authors": ["Justin Gilmer", "Colin Raffel", "Samuel S. Schoenholz", "Maithra Raghu", "and Jascha Sohl-Dickstein"], "authorids": ["gilmer@google.com", "craffel@google.com", "schsam@google.com", "maithra@google.com", "jaschasd@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489167280197, "id": "ICLR.cc/2017/workshop/-/paper109/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper109/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper109/AnonReviewer2", "ICLR.cc/2017/workshop/paper109/AnonReviewer1"], "reply": {"forum": "HkXKUTVFl", "replyto": "HkXKUTVFl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper109/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper109/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489167280197}}}], "count": 5}