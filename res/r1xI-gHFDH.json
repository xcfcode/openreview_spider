{"notes": [{"id": "r1xI-gHFDH", "original": "rygfDleKPB", "number": 2137, "cdate": 1569439742123, "ddate": null, "tcdate": 1569439742123, "tmdate": 1577168216948, "tddate": null, "forum": "r1xI-gHFDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "How can we generalise learning distributed representations of graphs?", "authors": ["Paul M Scherer", "Pietro Lio"], "authorids": ["pms69@cam.ac.uk", "pl219@cam.ac.uk"], "keywords": ["graphs", "distributed representations", "similarity learning"], "TL;DR": "We propose a general framework for building models that can learn distributed representations of discrete structures and test this on graphs.", "abstract": "We propose a general framework to construct unsupervised models capable of learning distributed representations of discrete structures such as graphs based on R-Convolution kernels and distributed semantics research. Our framework combines the insights and observations of Deep Graph Kernels and Graph2Vec towards a unified methodology for performing similarity learning on graphs of arbitrary size. This is exemplified by our own instance G2DR which extends Graph2Vec from labelled graphs towards unlabelled graphs and tackles issues of diagonal dominance through pruning of the subgraph vocabulary composing graphs. These changes produce new state of the art results in the downstream application of G2DR embeddings in graph classification tasks over datasets with small labelled graphs in binary classification to multi-class classification on large unlabelled graphs using an off-the-shelf support vector machine. ", "code": "https://github.com/ANON-ICLR2020/ICLR2020-G2DR", "pdf": "/pdf/bc2489ef3429cb51829b11e40e089e6f99493b8b.pdf", "paperhash": "scherer|how_can_we_generalise_learning_distributed_representations_of_graphs", "original_pdf": "/attachment/528d963af170b0a070c0343c3c64049733972d3c.pdf", "_bibtex": "@misc{\nscherer2020how,\ntitle={How can we generalise learning distributed representations of graphs?},\nauthor={Paul M Scherer and Pietro Lio},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xI-gHFDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "UiUyNvGyv7", "original": null, "number": 1, "cdate": 1576798741489, "ddate": null, "tcdate": 1576798741489, "tmdate": 1576800894753, "tddate": null, "forum": "r1xI-gHFDH", "replyto": "r1xI-gHFDH", "invitation": "ICLR.cc/2020/Conference/Paper2137/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposed a general framework to construct unsupervised models for representation learning of discrete structures. The reviewers feel that the approach is taken directly from graph kernels, and the novelty is not high enough. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How can we generalise learning distributed representations of graphs?", "authors": ["Paul M Scherer", "Pietro Lio"], "authorids": ["pms69@cam.ac.uk", "pl219@cam.ac.uk"], "keywords": ["graphs", "distributed representations", "similarity learning"], "TL;DR": "We propose a general framework for building models that can learn distributed representations of discrete structures and test this on graphs.", "abstract": "We propose a general framework to construct unsupervised models capable of learning distributed representations of discrete structures such as graphs based on R-Convolution kernels and distributed semantics research. Our framework combines the insights and observations of Deep Graph Kernels and Graph2Vec towards a unified methodology for performing similarity learning on graphs of arbitrary size. This is exemplified by our own instance G2DR which extends Graph2Vec from labelled graphs towards unlabelled graphs and tackles issues of diagonal dominance through pruning of the subgraph vocabulary composing graphs. These changes produce new state of the art results in the downstream application of G2DR embeddings in graph classification tasks over datasets with small labelled graphs in binary classification to multi-class classification on large unlabelled graphs using an off-the-shelf support vector machine. ", "code": "https://github.com/ANON-ICLR2020/ICLR2020-G2DR", "pdf": "/pdf/bc2489ef3429cb51829b11e40e089e6f99493b8b.pdf", "paperhash": "scherer|how_can_we_generalise_learning_distributed_representations_of_graphs", "original_pdf": "/attachment/528d963af170b0a070c0343c3c64049733972d3c.pdf", "_bibtex": "@misc{\nscherer2020how,\ntitle={How can we generalise learning distributed representations of graphs?},\nauthor={Paul M Scherer and Pietro Lio},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xI-gHFDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1xI-gHFDH", "replyto": "r1xI-gHFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795707794, "tmdate": 1576800256065, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2137/-/Decision"}}}, {"id": "H1lWHaNhir", "original": null, "number": 8, "cdate": 1573829945449, "ddate": null, "tcdate": 1573829945449, "tmdate": 1573833292551, "tddate": null, "forum": "r1xI-gHFDH", "replyto": "r1xI-gHFDH", "invitation": "ICLR.cc/2020/Conference/Paper2137/-/Official_Comment", "content": {"title": "Uploaded Revision", "comment": "We would like to thank all of the reviewers for reading our work and providing feedback to improve our work and correct mistakes.\n\nWe have taken these into consideration and uploaded a revision. On top of including as many of the pointers and promised revisions as possible, we have changed parts of the presentation to be clearer on the different contributions. \n\nMain points of revision: \n- We have updated the abstract to be clearer about the specific contributions of this work. (unfortunately we cannot update the abstract on this webpage.)\n\n- We have included suggested related work from Reviewer #3 and made comments within the paper, as well as including their results on our selection of benchmark datasets.\n\n- We have expanded on our heuristic to prune the subgraph pattern vocabulary to handle the indirect influence of diagonal dominance as suggested by Reviewer #2 within Section 4.2.1. We point out how this helps the skipgram model learn a more useful representations (for the downstream graph classification task).  \n\n- We have attempted to better separate the description of the framework for building models which can learn distributed representations of graphs, and the presentation of an extended version of Graph2Vec, described using this framework.\n\n- Changes to the section titles and seperation of some previous subsections into their own to reflect above point, and make the content clearer.\n\n- Removal of figure 1 and Algorithm 2, replaced with equation of the objective function to be optimised in the learning phase to save space and include revisions promised elsewhere.\n\n- Updated the results tables, as we previously presented the standard deviation of the downstream SVM instead of multiple iterations of the entire system as pointed out by Reviewer #1. We have also included the suggested models of Reviewer #3.\n\n- We have updated sections 5.2 and 5.3 to give a better discussion on the results, comparison with other approaches, and a comment on the PTC dataset and the challenge it poses to graph classification systems.\n\nOnce again we would like to thank all who have read our work, and provided pointers for improvement in the revision and future."}, "signatures": ["ICLR.cc/2020/Conference/Paper2137/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How can we generalise learning distributed representations of graphs?", "authors": ["Paul M Scherer", "Pietro Lio"], "authorids": ["pms69@cam.ac.uk", "pl219@cam.ac.uk"], "keywords": ["graphs", "distributed representations", "similarity learning"], "TL;DR": "We propose a general framework for building models that can learn distributed representations of discrete structures and test this on graphs.", "abstract": "We propose a general framework to construct unsupervised models capable of learning distributed representations of discrete structures such as graphs based on R-Convolution kernels and distributed semantics research. Our framework combines the insights and observations of Deep Graph Kernels and Graph2Vec towards a unified methodology for performing similarity learning on graphs of arbitrary size. This is exemplified by our own instance G2DR which extends Graph2Vec from labelled graphs towards unlabelled graphs and tackles issues of diagonal dominance through pruning of the subgraph vocabulary composing graphs. These changes produce new state of the art results in the downstream application of G2DR embeddings in graph classification tasks over datasets with small labelled graphs in binary classification to multi-class classification on large unlabelled graphs using an off-the-shelf support vector machine. ", "code": "https://github.com/ANON-ICLR2020/ICLR2020-G2DR", "pdf": "/pdf/bc2489ef3429cb51829b11e40e089e6f99493b8b.pdf", "paperhash": "scherer|how_can_we_generalise_learning_distributed_representations_of_graphs", "original_pdf": "/attachment/528d963af170b0a070c0343c3c64049733972d3c.pdf", "_bibtex": "@misc{\nscherer2020how,\ntitle={How can we generalise learning distributed representations of graphs?},\nauthor={Paul M Scherer and Pietro Lio},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xI-gHFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xI-gHFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2137/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2137/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2137/Authors|ICLR.cc/2020/Conference/Paper2137/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145790, "tmdate": 1576860560385, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2137/-/Official_Comment"}}}, {"id": "rJgHTiE2sr", "original": null, "number": 7, "cdate": 1573829564723, "ddate": null, "tcdate": 1573829564723, "tmdate": 1573829723594, "tddate": null, "forum": "r1xI-gHFDH", "replyto": "H1lnJeRqiH", "invitation": "ICLR.cc/2020/Conference/Paper2137/-/Official_Comment", "content": {"title": "Building blocks", "comment": "In general I am ok with comments of the authors. \n\nHowever, I think that it is rather obvious (taking into account so many papers on this topic) a general structure of such type of algorithms, what kind of building blocks we should use. I think that the main remaining issue here is to improve some of the blocks and prove under which conditions those modifications can have some impact on theoretical properties, e.g. ability to solve graph isomorphism.\n\nTherefore, I do not think I can increase my grade significantly."}, "signatures": ["ICLR.cc/2020/Conference/Paper2137/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2137/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How can we generalise learning distributed representations of graphs?", "authors": ["Paul M Scherer", "Pietro Lio"], "authorids": ["pms69@cam.ac.uk", "pl219@cam.ac.uk"], "keywords": ["graphs", "distributed representations", "similarity learning"], "TL;DR": "We propose a general framework for building models that can learn distributed representations of discrete structures and test this on graphs.", "abstract": "We propose a general framework to construct unsupervised models capable of learning distributed representations of discrete structures such as graphs based on R-Convolution kernels and distributed semantics research. Our framework combines the insights and observations of Deep Graph Kernels and Graph2Vec towards a unified methodology for performing similarity learning on graphs of arbitrary size. This is exemplified by our own instance G2DR which extends Graph2Vec from labelled graphs towards unlabelled graphs and tackles issues of diagonal dominance through pruning of the subgraph vocabulary composing graphs. These changes produce new state of the art results in the downstream application of G2DR embeddings in graph classification tasks over datasets with small labelled graphs in binary classification to multi-class classification on large unlabelled graphs using an off-the-shelf support vector machine. ", "code": "https://github.com/ANON-ICLR2020/ICLR2020-G2DR", "pdf": "/pdf/bc2489ef3429cb51829b11e40e089e6f99493b8b.pdf", "paperhash": "scherer|how_can_we_generalise_learning_distributed_representations_of_graphs", "original_pdf": "/attachment/528d963af170b0a070c0343c3c64049733972d3c.pdf", "_bibtex": "@misc{\nscherer2020how,\ntitle={How can we generalise learning distributed representations of graphs?},\nauthor={Paul M Scherer and Pietro Lio},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xI-gHFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xI-gHFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2137/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2137/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2137/Authors|ICLR.cc/2020/Conference/Paper2137/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145790, "tmdate": 1576860560385, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2137/-/Official_Comment"}}}, {"id": "BkxnW6eCFS", "original": null, "number": 3, "cdate": 1571847427553, "ddate": null, "tcdate": 1571847427553, "tmdate": 1573829668849, "tddate": null, "forum": "r1xI-gHFDH", "replyto": "r1xI-gHFDH", "invitation": "ICLR.cc/2020/Conference/Paper2137/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "N/A", "title": "Official Blind Review #3", "review": "The paper presents an unsupervised method for graph embedding. \n\nDespite having good experimental results, the paper is not of the quality to be accepted to the conference yet. The approach is rather a mix of previous works and hence not novel. \n\nIn particular, the algorithm for WL decomposition is almost fully taken from the original paper with a slight modification. Advantage of using it for unlabeled data is poorly motivated as unlabeled graphs can easily take statistics such as degree as the node labels, which was shown well in practice. \n\nModified PV-DBOW is in fact the same algorithm as the original CBOW model but applied to different context. It has been used in many papers, including Deep GK, graph2vec, anonymous walks. \n\nAlso, the Figure 1. is taken from the original paper of WL kernel. The algorithms 1 and 2 are taken from the original papers with slight modifications. \n\nThere is no discussion of [1], which uses CBOW framework, has theoretical properties, and produces good results in experiments. There is no comparison with GNN models such as [2]. \n\nI would be more interested to see explanation of the obtained results for each particular dataset (e.g. why MUTAG has 92% accuracy and PTC 67%); what so different about dataset and whether we reached a limit on most commonly used datasets. \n\n[1] Anonymous Walk Embeddings? ICML 2018, Ivanov et. al. \n[2] How Powerful are Graph Neural Networks? ICLR 2019, Xu et. al.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper2137/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2137/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How can we generalise learning distributed representations of graphs?", "authors": ["Paul M Scherer", "Pietro Lio"], "authorids": ["pms69@cam.ac.uk", "pl219@cam.ac.uk"], "keywords": ["graphs", "distributed representations", "similarity learning"], "TL;DR": "We propose a general framework for building models that can learn distributed representations of discrete structures and test this on graphs.", "abstract": "We propose a general framework to construct unsupervised models capable of learning distributed representations of discrete structures such as graphs based on R-Convolution kernels and distributed semantics research. Our framework combines the insights and observations of Deep Graph Kernels and Graph2Vec towards a unified methodology for performing similarity learning on graphs of arbitrary size. This is exemplified by our own instance G2DR which extends Graph2Vec from labelled graphs towards unlabelled graphs and tackles issues of diagonal dominance through pruning of the subgraph vocabulary composing graphs. These changes produce new state of the art results in the downstream application of G2DR embeddings in graph classification tasks over datasets with small labelled graphs in binary classification to multi-class classification on large unlabelled graphs using an off-the-shelf support vector machine. ", "code": "https://github.com/ANON-ICLR2020/ICLR2020-G2DR", "pdf": "/pdf/bc2489ef3429cb51829b11e40e089e6f99493b8b.pdf", "paperhash": "scherer|how_can_we_generalise_learning_distributed_representations_of_graphs", "original_pdf": "/attachment/528d963af170b0a070c0343c3c64049733972d3c.pdf", "_bibtex": "@misc{\nscherer2020how,\ntitle={How can we generalise learning distributed representations of graphs?},\nauthor={Paul M Scherer and Pietro Lio},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xI-gHFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1xI-gHFDH", "replyto": "r1xI-gHFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2137/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2137/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575859714019, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2137/Reviewers"], "noninvitees": [], "tcdate": 1570237727177, "tmdate": 1575859714030, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2137/-/Official_Review"}}}, {"id": "H1lnJeRqiH", "original": null, "number": 6, "cdate": 1573736420247, "ddate": null, "tcdate": 1573736420247, "tmdate": 1573746032092, "tddate": null, "forum": "r1xI-gHFDH", "replyto": "H1eWRE9ciS", "invitation": "ICLR.cc/2020/Conference/Paper2137/-/Official_Comment", "content": {"title": "Thank you for your comment", "comment": "Thank you for your comment\n\n\"- as we already discussed, the approach is the mix of several existing approaches\"\n\nAs in the first response, the paper does not describe is not a single approach or single model exclusively, we are highlighting a common framework/workflow utilised by other models which learn distributed representations of graphs (deep graph kernels, graph2vec, anonymous walk embeddings can all be described within this framework). The second portion presents an extended version of Graph2Vec described using this framework as an example, we call this G2DR for easier reference. We will revise the abstract to make this more clear from the start.\n\n\"-one of the main differences of the proposed approach is that the authors use some other method (which is already known) to construct a vocabulary of subtree patterns for large graphs\"\n\nYes one extension to the current implementation of Graph2Vec is to use the more general WL node relabeling algorithm described by Shervashidze et al. We don't believe the use of a good algorithm described by a well known paper in graph kernels is a bad thing. \n\n\"- the computational complexity of the proposed approach is high due to high computational complexity of constructing the vocabulary. Is it worth using this approach due to its high computational cost?\"\n\nThe construction of the substructure vocabulary is O(|G|dm) where |G| is the number of graphs in the dataset, d is the highest degree of subtree pattern we wish to extract, and m is the highest number of edges in a graph within the dataset. An advantage of using the distributed approach is clear associations that can be drawn between graphs that are deemed similar as we can expect and actually present the different subgraph patterns as they are recorded as contexts. This can be useful subsequent analysis on the motifs present in different classes of graphs.\n\n\"- the experimental section is rather weak. Since the approach is based on using different building blocks, it is necessary to know which of building blocks is the most important and provides the most contribution to increase in accuracy. Is it due to the new vocabulary? Or doc2vec? or what?\"\n\nThe presented results are a result of the pruned vocabulary. This is the only operation that differs from Graph2Vec that directly affects the learning model (skipgram). \n\nThe other \"building blocks\" such as using Shershavidze et al's WL-relabeling algorithm and suggestion on labeling nodes by degree allows application of the model to learn distributed representations of unlabelled graphs; hence the results on the Reddit datasets. Otherwise the WL algorithm produces exactly the same subtrees as the WL algorithm described in Graph2Vec in the labeled case; which then get pruned based on their frequency. \n\nOnce again thank you for your comment. Hopefully this addresses some of the new questions."}, "signatures": ["ICLR.cc/2020/Conference/Paper2137/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How can we generalise learning distributed representations of graphs?", "authors": ["Paul M Scherer", "Pietro Lio"], "authorids": ["pms69@cam.ac.uk", "pl219@cam.ac.uk"], "keywords": ["graphs", "distributed representations", "similarity learning"], "TL;DR": "We propose a general framework for building models that can learn distributed representations of discrete structures and test this on graphs.", "abstract": "We propose a general framework to construct unsupervised models capable of learning distributed representations of discrete structures such as graphs based on R-Convolution kernels and distributed semantics research. Our framework combines the insights and observations of Deep Graph Kernels and Graph2Vec towards a unified methodology for performing similarity learning on graphs of arbitrary size. This is exemplified by our own instance G2DR which extends Graph2Vec from labelled graphs towards unlabelled graphs and tackles issues of diagonal dominance through pruning of the subgraph vocabulary composing graphs. These changes produce new state of the art results in the downstream application of G2DR embeddings in graph classification tasks over datasets with small labelled graphs in binary classification to multi-class classification on large unlabelled graphs using an off-the-shelf support vector machine. ", "code": "https://github.com/ANON-ICLR2020/ICLR2020-G2DR", "pdf": "/pdf/bc2489ef3429cb51829b11e40e089e6f99493b8b.pdf", "paperhash": "scherer|how_can_we_generalise_learning_distributed_representations_of_graphs", "original_pdf": "/attachment/528d963af170b0a070c0343c3c64049733972d3c.pdf", "_bibtex": "@misc{\nscherer2020how,\ntitle={How can we generalise learning distributed representations of graphs?},\nauthor={Paul M Scherer and Pietro Lio},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xI-gHFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xI-gHFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2137/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2137/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2137/Authors|ICLR.cc/2020/Conference/Paper2137/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145790, "tmdate": 1576860560385, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2137/-/Official_Comment"}}}, {"id": "H1eWRE9ciS", "original": null, "number": 5, "cdate": 1573721289405, "ddate": null, "tcdate": 1573721289405, "tmdate": 1573721289405, "tddate": null, "forum": "r1xI-gHFDH", "replyto": "Sylfcv2bjS", "invitation": "ICLR.cc/2020/Conference/Paper2137/-/Official_Comment", "content": {"title": "Still I am not convinced", "comment": "- as we already discussed, the approach is the mix of several existing approaches\n- one of the main differences of the proposed approach is that the authors use some other method (which is already known) to construct a vocabulary of subtree patterns for large graphs\n- the computational complexity of the proposed approach is high due to high computational complexity of constructing the vocabulary. Is it worth using this approach due to its high computational cost?\n- the experimental section is rather weak. Since the approach is based on using different building blocks, it is necessary to know which of building blocks is the most important and provides the most contribution to increase in accuracy. Is it due to the new vocabulary? Or doc2vec? or what?\n- as a results still I am not convinced in the provided comments."}, "signatures": ["ICLR.cc/2020/Conference/Paper2137/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2137/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How can we generalise learning distributed representations of graphs?", "authors": ["Paul M Scherer", "Pietro Lio"], "authorids": ["pms69@cam.ac.uk", "pl219@cam.ac.uk"], "keywords": ["graphs", "distributed representations", "similarity learning"], "TL;DR": "We propose a general framework for building models that can learn distributed representations of discrete structures and test this on graphs.", "abstract": "We propose a general framework to construct unsupervised models capable of learning distributed representations of discrete structures such as graphs based on R-Convolution kernels and distributed semantics research. Our framework combines the insights and observations of Deep Graph Kernels and Graph2Vec towards a unified methodology for performing similarity learning on graphs of arbitrary size. This is exemplified by our own instance G2DR which extends Graph2Vec from labelled graphs towards unlabelled graphs and tackles issues of diagonal dominance through pruning of the subgraph vocabulary composing graphs. These changes produce new state of the art results in the downstream application of G2DR embeddings in graph classification tasks over datasets with small labelled graphs in binary classification to multi-class classification on large unlabelled graphs using an off-the-shelf support vector machine. ", "code": "https://github.com/ANON-ICLR2020/ICLR2020-G2DR", "pdf": "/pdf/bc2489ef3429cb51829b11e40e089e6f99493b8b.pdf", "paperhash": "scherer|how_can_we_generalise_learning_distributed_representations_of_graphs", "original_pdf": "/attachment/528d963af170b0a070c0343c3c64049733972d3c.pdf", "_bibtex": "@misc{\nscherer2020how,\ntitle={How can we generalise learning distributed representations of graphs?},\nauthor={Paul M Scherer and Pietro Lio},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xI-gHFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xI-gHFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2137/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2137/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2137/Authors|ICLR.cc/2020/Conference/Paper2137/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145790, "tmdate": 1576860560385, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2137/-/Official_Comment"}}}, {"id": "rylK_u2bsH", "original": null, "number": 4, "cdate": 1573140593341, "ddate": null, "tcdate": 1573140593341, "tmdate": 1573140593341, "tddate": null, "forum": "r1xI-gHFDH", "replyto": "ryexkwlRKr", "invitation": "ICLR.cc/2020/Conference/Paper2137/-/Official_Comment", "content": {"title": "Response for Blind Review #1", "comment": "First of all thank you very much for your review of this work, we will attempt to address some of the comments and questions individually\n\n\u201cThis paper studied unsupervised graph representation learning. The authors combined the techniques for Deep Graph Kernels and Graph2Vec, which essential extract substructures as words and the whole graph as documents and use doc2vec for learning the representations of both graphs and substructures.\u201d\n\nYou are correct in this summary, however we may have not adequately stressed that the approach more generally highlights that graphs may be represented distributively via its internal substructure patterns (such as walks, nodes, induced subgraphs, subtrees, etc. as highlighted in Deep Graph Kernels [DGK]) as context. This allows a variety of embedding methods which exploit the distributive hypothesis to be applied on the graph-subpattern context pairs to learn vector representations of graphs (skipgram, cbow, GLOVE, pmi) etc. The revision will try to make this distinction clearer in the introduction. The intended contribution is an acknowledgement of the observation that many kernels fall under the R-Convolution framework (Haussler 1999) in DGK (Yanardag and Vishwanathan 2015); and generalisation beyond building representation with edit distance matrices and word2vec towards all embedding methods which exploit the distributive hypothesis.\n\nThe 2nd half of this work presents G2DR (which is a straight-forward extension of the Graph2Vec model) to exemplify an instance of this framework using a decomposition of graphs to subtrees using the WL algorithm and building distributed representations with a skipgram model. This is just one possible instance of the approach above, and we chose to extend Graph2Vec (could have been called Graph2Vec2 but felt G2DR was more appropriate whilst acknowledging the previous work) as it could be modified to be utilised on a wider set of graph types.\n\n\u201cHowever, the novelty of the proposed method is very marginal. Comparing to the Deep Graph kernel methods, the authors simply changed from the word2vec style methods to doc2vec style methods.\u201c\n\nYou are correct that Graph2Vec extends deep graph kernels through use of a WL subtree contexts followed by a skipgram architecture posed in the form of doc2vec. The contribution of G2DR is to modify the implementation the WL subtree decomposition with that in WL Kernel (Shervashidze et al, 2011) and take on their suggestion of relabeling unlabelled graphs by degree to allow building representations of unlabelled graphs (REDDIT graphs, for example). Furthermore we also attempt to lessen the problem of diagonal dominance in DGK/Graph2Vec by pruning the vocabulary of context subgraph patterns to improve downstream classification performance.\n\nOnto some of the questions:\n\u201c(1) The data sets used in this paper are too small. For unsupervised pretraining methods, much larger data sets are expected. \u201c\nIndeed it is difficult to find good large/public/popular datasets for comparative analysis with related works. As reported in the work we have sourced our datasets from https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets (Kersting et al, 2016) around 2018. As on the list the REDDIT datasets are still the (2nd?) largest in this list and are regularly used in related literature which motivates its use in our work (as well as the fact it has unlabelled nodes).\n\n\u201c(2) The results in Table 1 are really weird. Why do the performance of your method have a much lower standard deviation? It is really hard to believe the proposed methods have much stable performance compare to other methods.  Can you explain this?\u201d\n\nThank you for this observation! You are the only one that noticed this in the results table. The authors sincerely apologise for this mistake, the presented standard deviation comes from the 10 Fold SVM CV on the same embeddings output by G2DR through an old development file that reused pretrained embeddings for experiments. The revision will present the standard deviation of 10 Fold SVM CV being run on 10 trained embedding outputs of G2DR. This should give a more realistic picture of expected performance on the benchmarks. \n\nWe hope that this clarifies some points and thank the reviewer for constructive feedback, and would be happy to discuss more.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2137/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How can we generalise learning distributed representations of graphs?", "authors": ["Paul M Scherer", "Pietro Lio"], "authorids": ["pms69@cam.ac.uk", "pl219@cam.ac.uk"], "keywords": ["graphs", "distributed representations", "similarity learning"], "TL;DR": "We propose a general framework for building models that can learn distributed representations of discrete structures and test this on graphs.", "abstract": "We propose a general framework to construct unsupervised models capable of learning distributed representations of discrete structures such as graphs based on R-Convolution kernels and distributed semantics research. Our framework combines the insights and observations of Deep Graph Kernels and Graph2Vec towards a unified methodology for performing similarity learning on graphs of arbitrary size. This is exemplified by our own instance G2DR which extends Graph2Vec from labelled graphs towards unlabelled graphs and tackles issues of diagonal dominance through pruning of the subgraph vocabulary composing graphs. These changes produce new state of the art results in the downstream application of G2DR embeddings in graph classification tasks over datasets with small labelled graphs in binary classification to multi-class classification on large unlabelled graphs using an off-the-shelf support vector machine. ", "code": "https://github.com/ANON-ICLR2020/ICLR2020-G2DR", "pdf": "/pdf/bc2489ef3429cb51829b11e40e089e6f99493b8b.pdf", "paperhash": "scherer|how_can_we_generalise_learning_distributed_representations_of_graphs", "original_pdf": "/attachment/528d963af170b0a070c0343c3c64049733972d3c.pdf", "_bibtex": "@misc{\nscherer2020how,\ntitle={How can we generalise learning distributed representations of graphs?},\nauthor={Paul M Scherer and Pietro Lio},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xI-gHFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xI-gHFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2137/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2137/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2137/Authors|ICLR.cc/2020/Conference/Paper2137/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145790, "tmdate": 1576860560385, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2137/-/Official_Comment"}}}, {"id": "rkxdXdnbor", "original": null, "number": 3, "cdate": 1573140511901, "ddate": null, "tcdate": 1573140511901, "tmdate": 1573140556472, "tddate": null, "forum": "r1xI-gHFDH", "replyto": "S1xV-886FS", "invitation": "ICLR.cc/2020/Conference/Paper2137/-/Official_Comment", "content": {"title": "Response for Blind Review #2", "comment": "Thank you very much for reading this work and providing feedback. We will attempt to address each point individually.\n\n\u201c1. The main issue with this method is the computational complexity due to exponential growth of vocabulary of subtree patterns size for large graphs. Particularly , for experiments with unlabeled graphs, the performance is significantly worse than CNN based models. How would the performance be on unlabeled small graphs? For example, have you verified the performance on small graphs of section 4.2 when labels are ignored? (downstream clustering task)\u201d\n\nIndeed the computational complexity of this approach is high in the embedding learning stage due to the exponential growth of the subtree patterns extracted as the graphs get larger and more heterogeneous in terms of node labels. However we believe it is nonetheless interesting to look at alternative inductive biases (such as a distributive one, with various definitions of context) to learn representations of graphs. We believe intelligent definitions of \u201ccontext\u201d or vocabulary pruning can help significantly in this regard.\n\nWe have not tried applying this to small unlabelled graphs. If time permits this will be in the revision (within an appendix) with the labeled datasets such as Mutag. Thank you for this suggestion.\n\n\u201c  2. The neural language models rely on the concept of context in documents. How the concept of context defined for subtree patterns extracted by Weisfeiler-Lehman algorithm?\u201d\n\nYes defining the context is very important for learning useful distributive representations, and there are many different ways this can be done in natural language processing. For learning whole graph representations the context for a graph was its induced subtree patterns (which are extracted using the WL algorithm).\n\n\u201c3. The issue of diagonal dominance should be clarified. How does the pruning tackles this issue?\u201d\n\nWe will attempt to describe the issue of diagonal dominance more concretely in the revision. Essentially diagonal dominance is related to the explosive increase of unique induced subgraph patterns when building our vocabularies. An example of this can be seen for our work, using the WL relabeling algorithm for the NCI1 dataset on the first iteration there are 267 subtrees, on the second there are 4033, and in the third iteration 22923 subtrees patterns within the graphs of NCI1. Consequently as the number of features (vocabulary size) grows, we run into the sparsity problem, where only a few substructures will be common across the graphs. This leads to the phenomenon known as diagonal dominance, where graphs become more similar to themselves but more distant from other graphs in the dataset. The naive pruning directly tackles this approach by removing dimensions along vocabulary instances that only appear a few times. Smarter ways of reducing this effect would lead to better distributed representations as we lightly touch upon in the discussion of the final results. We will try to make this more apparent in the revision. Thank you for this comment.\n\nWe thank the reviewer for reading our work and the constructive feedback, we will work to integrate some of the comments into our revision.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2137/Authors"], "readers": ["ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How can we generalise learning distributed representations of graphs?", "authors": ["Paul M Scherer", "Pietro Lio"], "authorids": ["pms69@cam.ac.uk", "pl219@cam.ac.uk"], "keywords": ["graphs", "distributed representations", "similarity learning"], "TL;DR": "We propose a general framework for building models that can learn distributed representations of discrete structures and test this on graphs.", "abstract": "We propose a general framework to construct unsupervised models capable of learning distributed representations of discrete structures such as graphs based on R-Convolution kernels and distributed semantics research. Our framework combines the insights and observations of Deep Graph Kernels and Graph2Vec towards a unified methodology for performing similarity learning on graphs of arbitrary size. This is exemplified by our own instance G2DR which extends Graph2Vec from labelled graphs towards unlabelled graphs and tackles issues of diagonal dominance through pruning of the subgraph vocabulary composing graphs. These changes produce new state of the art results in the downstream application of G2DR embeddings in graph classification tasks over datasets with small labelled graphs in binary classification to multi-class classification on large unlabelled graphs using an off-the-shelf support vector machine. ", "code": "https://github.com/ANON-ICLR2020/ICLR2020-G2DR", "pdf": "/pdf/bc2489ef3429cb51829b11e40e089e6f99493b8b.pdf", "paperhash": "scherer|how_can_we_generalise_learning_distributed_representations_of_graphs", "original_pdf": "/attachment/528d963af170b0a070c0343c3c64049733972d3c.pdf", "_bibtex": "@misc{\nscherer2020how,\ntitle={How can we generalise learning distributed representations of graphs?},\nauthor={Paul M Scherer and Pietro Lio},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xI-gHFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xI-gHFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2137/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2137/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2137/Authors|ICLR.cc/2020/Conference/Paper2137/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145790, "tmdate": 1576860560385, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2137/-/Official_Comment"}}}, {"id": "Sylfcv2bjS", "original": null, "number": 2, "cdate": 1573140362040, "ddate": null, "tcdate": 1573140362040, "tmdate": 1573140362040, "tddate": null, "forum": "r1xI-gHFDH", "replyto": "SyxdvPnZjr", "invitation": "ICLR.cc/2020/Conference/Paper2137/-/Official_Comment", "content": {"title": "Response for Blind Review #3 Part 2 (of 2)", "comment": "\u201cAlso, the Figure 1. is taken from the original paper of WL kernel. The algorithms 1 and 2 are taken from the original papers with slight modifications. \u201c\n\nAs previously we explicitly say we use the algorithms from their respective papers with acknowledgement to aid description of the G2DR and Graph2Vec approaches with notational changes for consistency in the explanations. \n\n-For algorithm 1 \u201cWL-Relabel\u201d in section 3.1.1. <<... This is achieved as a byproduct of WL test\u2019s node relabeling (Shervashidze et al,. 2011) and is fully described in algorithm 1 ...\u201d>>\n\n-For algorithm 2 \u201cTrain-Graph2Vec\u201d in section 3.1.3 <<... We follow Graph2Vec (Narayanan et al., 2017) and use a PV-DBOW\u2026 as outlined in algorithm 2>>\n\nWe think presenting the algorithms helps the reader refer to details within the paper itself to get more exposition if they wish to do so with notation that is consistent within this work. However together with the comment on the PVDBOW name being an ill-suited name for the method we may remove algorithm 2 and replace it with the objective function of the word2vec/graph2vec algorithm to save space and address other points. Similarly for the figure 1 which was used for explanation purpose (it actually has an additional node number 5 in comparison to the figure in the WL Kernel, so the extracted subtree is accordingly different as well), we may remove this in favour of addressing some of the other points in the reviews, or make a showcase of the subtree extraction on a more obviously different graph. Thank you for the comment and we will revise the document as necessary.\n\n\u201cThere is no discussion of [1], which uses CBOW framework, has theoretical properties, and produces good results in experiments. There is no comparison with GNN models such as [2]. \u201c\n\nThank you for introducing us to [1] (AWE). We have simply not come across this work during the time working on this project. This is a very nice paper with clear parallels to this work as it has to DGK and Graph2Vec as well. In fact the style of this work is very similar to Graph2Vec with the usage of anonymous walks instead of subtree patterns as input into context based language models. In one sense the AWE is another method that can fall under the framework described in the introduction and section 3 alongside DGK, Graph2Vec, G2DR. This is very neat and thank you for pointing this out, we will try to include it in the revision and results tables.\n\nThank you for pointing out [2]. Our original comparison for GNN/convolution based methods was to compare between DiffPool and PATCHYSAN as described in the paper. Because DiffPool had only published results for one of the datasets within our selection without standard deviations we did not include this, in favour of PATCHY-SAN which did cover all the datasets with appropriate standard deviations. The GIN in [2] seems to have results for almost all of the datasets so we will include it in the results table of the revision. To help clarify relations with the GNN based methods we will retitle section 2.2 as \u201cDeep learning approaches: GNNs and convolutional approaches\u201d.\n\n\u201cI would be more interested to see explanation of the obtained results for each particular dataset (e.g. why MUTAG has 92% accuracy and PTC 67%); what so different about dataset and whether we reached a limit on most commonly used datasets. \u201c\n\nYes thank you, this is an interesting question on what defines the difficulty of the classification tasks based on the properties of the dataset. From the point of view of building distributed representations we think an interesting way to look at it would be the characterisation of the substructure pattern distributions for graphs of different classifications. In PTC there may be clear overlap between the distributions which makes it hard to make representations that are easy to seperate downstream. If time permits within the revision period we will either answer directly on a comment or within an appendix section.\n\nWe thank the reviewer for reading our work and the constructive feedback. We will work to incorporate tips from the comments into the revision. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2137/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How can we generalise learning distributed representations of graphs?", "authors": ["Paul M Scherer", "Pietro Lio"], "authorids": ["pms69@cam.ac.uk", "pl219@cam.ac.uk"], "keywords": ["graphs", "distributed representations", "similarity learning"], "TL;DR": "We propose a general framework for building models that can learn distributed representations of discrete structures and test this on graphs.", "abstract": "We propose a general framework to construct unsupervised models capable of learning distributed representations of discrete structures such as graphs based on R-Convolution kernels and distributed semantics research. Our framework combines the insights and observations of Deep Graph Kernels and Graph2Vec towards a unified methodology for performing similarity learning on graphs of arbitrary size. This is exemplified by our own instance G2DR which extends Graph2Vec from labelled graphs towards unlabelled graphs and tackles issues of diagonal dominance through pruning of the subgraph vocabulary composing graphs. These changes produce new state of the art results in the downstream application of G2DR embeddings in graph classification tasks over datasets with small labelled graphs in binary classification to multi-class classification on large unlabelled graphs using an off-the-shelf support vector machine. ", "code": "https://github.com/ANON-ICLR2020/ICLR2020-G2DR", "pdf": "/pdf/bc2489ef3429cb51829b11e40e089e6f99493b8b.pdf", "paperhash": "scherer|how_can_we_generalise_learning_distributed_representations_of_graphs", "original_pdf": "/attachment/528d963af170b0a070c0343c3c64049733972d3c.pdf", "_bibtex": "@misc{\nscherer2020how,\ntitle={How can we generalise learning distributed representations of graphs?},\nauthor={Paul M Scherer and Pietro Lio},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xI-gHFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xI-gHFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2137/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2137/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2137/Authors|ICLR.cc/2020/Conference/Paper2137/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145790, "tmdate": 1576860560385, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2137/-/Official_Comment"}}}, {"id": "SyxdvPnZjr", "original": null, "number": 1, "cdate": 1573140320136, "ddate": null, "tcdate": 1573140320136, "tmdate": 1573140320136, "tddate": null, "forum": "r1xI-gHFDH", "replyto": "BkxnW6eCFS", "invitation": "ICLR.cc/2020/Conference/Paper2137/-/Official_Comment", "content": {"title": "Response for blind review #3 Part 1", "comment": "First of all thank you very much for your review of this work, we will attempt to address some of the comments and questions below. \n\n\u201cDespite having good experimental results, the paper is not of the quality to be accepted to the conference yet. The approach is rather a mix of previous works and hence not novel.\u201d \nAnd\n\u201cIn particular, the algorithm for WL decomposition is almost fully taken from the original paper with a slight modification... \u201c\n\nThis paper relies on previous models such as Deep Graph Kernels and Graph2Vec to extract and explicitly specify a general pipeline for building models capable of learning distributed representations of graphs.  The pipeline is based on two parts: the decompositions of graphs into substructures (walks, subtrees, nodes, etc) and the learning distributed representations using such substructures with different definitions of context and associated embedding methods (word2vec, GLoVe, etc.).\n\nThe second half of the write-up focuses on G2DR (explicitly stated as an extension of Graph2Vec) as an instance of this pipeline described above. G2DR is a straightforward extension of the Graph2Vec to more graph types (unlabelled graphs) through adoption of Shervashidze et al\u2019s WL algorithm to find subtree patterns, we have put it in this work with minor modification for notation because otherwise it wouldn\u2019t be the same WL algorithm. We believe in keeping the algorithm in the paper as it aids description of the specific implementation used and is correctly acknowledged as being the Shervashidze WL algorithm within the paper (section 3.1.1). We are afraid that simply pointing to the Shervashidze et al\u2019s exact presentation would detract from the reading and flow of the paper as different notation is used. \n\nTo summarise we can garner two contributions here:\nSpecification of a general pipeline for building models capable of learning distributed representations of graphs.\nAn extended version of Graph2Vec, called G2DR which is applicable to unlabelled graphs and is also more amenable to diagonal dominance through pruning of the subgraph vocabularies. This makes it perform better on larger graphs/datasets. \n\n\u201dAdvantage of using it for unlabeled data is poorly motivated as unlabeled graphs can easily take statistics such as degree as the node labels, which was shown well in practice.\u201d\n\nWe explicitly state our use of Shervashize et al\u2019s suggestion to label unlabelled nodes initially by their degree, otherwise the WL algorithm cannot be run for the unlabelled graphs such as the Reddit datasets. The contribution here is the application of this suggestion within another existing algorithm (Graph2Vec) to expand its applicability to more graph types and improve the performance of the GetSubgraph() (which is their rendition of the subtree decomposition algorithm) algorithm stated in Graph2Vec. \n\nOnce the unlabelled nodes are labelled by their degree, the motivation of using the WL algorithm falls upon motivating the usage of the rooted subtree patterns extracted. We touch upon this section 3.1.1 and is potentially better covered in the WL Kernel and Graph2Vec works. Essentially the motivation is that they are higher order substructures (than nodes), non-linear around definition of the neighbourhood around a node (as compared to a random walk), and the exhaustive nature of decomposition for subtree patterns for every node in the graph is useful to characterise all the patterns (subtree patterns) within a given graph. Another pragmatic motivation is that the WL Kernel has been shown to work well in graph classification tasks. We will try to make these motivations more clear in the paper, thank you for this comment and suggestion. \n\n\u201cModified PV-DBOW is in fact the same algorithm as the original CBOW model but applied to different context. It has been used in many papers, including Deep GK, graph2vec, anonymous walks. \u201c\n\nYes you are completely correct! We explicitly say that we are using the embedding method from Graph2Vec (hence the name of the algorithm also being TrainGraph2Vec). We kept the misleading Doc2Vec analogies used in Graph2Vec as it aided exposition of how one can think of a graph as composition of substructures, like documents being compositions of words. As the contexts of the graphs are defined as the subtree patterns within it, it is actually more similar to training a word2vec model as you mention. To make this clear we will change the title of this section in the revision. Thank you for this comment.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2137/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How can we generalise learning distributed representations of graphs?", "authors": ["Paul M Scherer", "Pietro Lio"], "authorids": ["pms69@cam.ac.uk", "pl219@cam.ac.uk"], "keywords": ["graphs", "distributed representations", "similarity learning"], "TL;DR": "We propose a general framework for building models that can learn distributed representations of discrete structures and test this on graphs.", "abstract": "We propose a general framework to construct unsupervised models capable of learning distributed representations of discrete structures such as graphs based on R-Convolution kernels and distributed semantics research. Our framework combines the insights and observations of Deep Graph Kernels and Graph2Vec towards a unified methodology for performing similarity learning on graphs of arbitrary size. This is exemplified by our own instance G2DR which extends Graph2Vec from labelled graphs towards unlabelled graphs and tackles issues of diagonal dominance through pruning of the subgraph vocabulary composing graphs. These changes produce new state of the art results in the downstream application of G2DR embeddings in graph classification tasks over datasets with small labelled graphs in binary classification to multi-class classification on large unlabelled graphs using an off-the-shelf support vector machine. ", "code": "https://github.com/ANON-ICLR2020/ICLR2020-G2DR", "pdf": "/pdf/bc2489ef3429cb51829b11e40e089e6f99493b8b.pdf", "paperhash": "scherer|how_can_we_generalise_learning_distributed_representations_of_graphs", "original_pdf": "/attachment/528d963af170b0a070c0343c3c64049733972d3c.pdf", "_bibtex": "@misc{\nscherer2020how,\ntitle={How can we generalise learning distributed representations of graphs?},\nauthor={Paul M Scherer and Pietro Lio},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xI-gHFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1xI-gHFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2137/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2137/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2137/Authors|ICLR.cc/2020/Conference/Paper2137/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145790, "tmdate": 1576860560385, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2137/Authors", "ICLR.cc/2020/Conference/Paper2137/Reviewers", "ICLR.cc/2020/Conference/Paper2137/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2137/-/Official_Comment"}}}, {"id": "S1xV-886FS", "original": null, "number": 1, "cdate": 1571804667610, "ddate": null, "tcdate": 1571804667610, "tmdate": 1572972378394, "tddate": null, "forum": "r1xI-gHFDH", "replyto": "r1xI-gHFDH", "invitation": "ICLR.cc/2020/Conference/Paper2137/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a framework for learning distributional representations of graphs in the following way: First, each graph is represented as a collection of subtree patterns. Second, the neural language model of doc2vec is applied to these collections of patterns to learn graph embeddings. These embeddings are then exploited in downstream analyses such as classification. Overall, the idea of formulating graph representation learning as a language model is interesting. The experiments show that it perform better than kernel methods. I have the following major comments:\n\n1. The main issue with this method is the computational complexity due to exponential growth of vocabulary of subtree patterns size for large graphs. Particularly , for experiments with unlabeled graphs, the performance is significantly worse than CNN based models. How would the performance be on unlabeled small graphs? For example, have you verified the performance on small graphs of section 4.2 when labels are ignored? (downstream clustering task)\n\n2. The neural language models rely on the concept of context in documents. How the concept of context defined for subtree patterns extracted by Weisfeiler-Lehman algorithm?\n\n3. The issue of diagonal dominance should be clarified. How does the pruning tackles this issue?\n\n "}, "signatures": ["ICLR.cc/2020/Conference/Paper2137/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2137/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How can we generalise learning distributed representations of graphs?", "authors": ["Paul M Scherer", "Pietro Lio"], "authorids": ["pms69@cam.ac.uk", "pl219@cam.ac.uk"], "keywords": ["graphs", "distributed representations", "similarity learning"], "TL;DR": "We propose a general framework for building models that can learn distributed representations of discrete structures and test this on graphs.", "abstract": "We propose a general framework to construct unsupervised models capable of learning distributed representations of discrete structures such as graphs based on R-Convolution kernels and distributed semantics research. Our framework combines the insights and observations of Deep Graph Kernels and Graph2Vec towards a unified methodology for performing similarity learning on graphs of arbitrary size. This is exemplified by our own instance G2DR which extends Graph2Vec from labelled graphs towards unlabelled graphs and tackles issues of diagonal dominance through pruning of the subgraph vocabulary composing graphs. These changes produce new state of the art results in the downstream application of G2DR embeddings in graph classification tasks over datasets with small labelled graphs in binary classification to multi-class classification on large unlabelled graphs using an off-the-shelf support vector machine. ", "code": "https://github.com/ANON-ICLR2020/ICLR2020-G2DR", "pdf": "/pdf/bc2489ef3429cb51829b11e40e089e6f99493b8b.pdf", "paperhash": "scherer|how_can_we_generalise_learning_distributed_representations_of_graphs", "original_pdf": "/attachment/528d963af170b0a070c0343c3c64049733972d3c.pdf", "_bibtex": "@misc{\nscherer2020how,\ntitle={How can we generalise learning distributed representations of graphs?},\nauthor={Paul M Scherer and Pietro Lio},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xI-gHFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1xI-gHFDH", "replyto": "r1xI-gHFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2137/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2137/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575859714019, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2137/Reviewers"], "noninvitees": [], "tcdate": 1570237727177, "tmdate": 1575859714030, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2137/-/Official_Review"}}}, {"id": "ryexkwlRKr", "original": null, "number": 2, "cdate": 1571845847693, "ddate": null, "tcdate": 1571845847693, "tmdate": 1572972378336, "tddate": null, "forum": "r1xI-gHFDH", "replyto": "r1xI-gHFDH", "invitation": "ICLR.cc/2020/Conference/Paper2137/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Strength:\n-- The paper is well written and easy to follow\n--  Learning the unsupervised graph representation learning is a very important problem\n-- The proposed approach seems effective on some data sets.\n\nWeakness:\n-- The novelty of the proposed approach is very marginal\n-- The experiments are very weak. \n\nThis paper studied unsupervised graph representation learning. The authors combined the techniques for Deep Graph Kernels and Graph2Vec, which essential extract substructures as words and the whole graph as documents and use doc2vec for learning the representations of both graphs and substructures. Experimental results on a few data sets prove the effectiveness of the proposed approach. \n\nOverall, the paper is well written and easy to follow. Learning unsupervised graph representation learning is a very important problem, especially for predicting the chemical properties of molecular structures. However, the novelty of the proposed method is very marginal. Comparing to the Deep Graph kernel methods, the authors simply changed from the word2vec style methods to doc2vec style methods. The paper could be better fit to a more applied conference. Moreover,  I have some concerns on the experiments. \n(1) The data sets used in this paper are too small. For unsupervised pretraining methods, much larger data sets are expected. \n\n(2) The results in Table 1 are really weird. Why do the performance of your method have a much lower standard deviation? It is really hard to believe the proposed methods have much stable performance compare to other methods.  Can you explain this?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2137/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2137/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "How can we generalise learning distributed representations of graphs?", "authors": ["Paul M Scherer", "Pietro Lio"], "authorids": ["pms69@cam.ac.uk", "pl219@cam.ac.uk"], "keywords": ["graphs", "distributed representations", "similarity learning"], "TL;DR": "We propose a general framework for building models that can learn distributed representations of discrete structures and test this on graphs.", "abstract": "We propose a general framework to construct unsupervised models capable of learning distributed representations of discrete structures such as graphs based on R-Convolution kernels and distributed semantics research. Our framework combines the insights and observations of Deep Graph Kernels and Graph2Vec towards a unified methodology for performing similarity learning on graphs of arbitrary size. This is exemplified by our own instance G2DR which extends Graph2Vec from labelled graphs towards unlabelled graphs and tackles issues of diagonal dominance through pruning of the subgraph vocabulary composing graphs. These changes produce new state of the art results in the downstream application of G2DR embeddings in graph classification tasks over datasets with small labelled graphs in binary classification to multi-class classification on large unlabelled graphs using an off-the-shelf support vector machine. ", "code": "https://github.com/ANON-ICLR2020/ICLR2020-G2DR", "pdf": "/pdf/bc2489ef3429cb51829b11e40e089e6f99493b8b.pdf", "paperhash": "scherer|how_can_we_generalise_learning_distributed_representations_of_graphs", "original_pdf": "/attachment/528d963af170b0a070c0343c3c64049733972d3c.pdf", "_bibtex": "@misc{\nscherer2020how,\ntitle={How can we generalise learning distributed representations of graphs?},\nauthor={Paul M Scherer and Pietro Lio},\nyear={2020},\nurl={https://openreview.net/forum?id=r1xI-gHFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1xI-gHFDH", "replyto": "r1xI-gHFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2137/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2137/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575859714019, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2137/Reviewers"], "noninvitees": [], "tcdate": 1570237727177, "tmdate": 1575859714030, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2137/-/Official_Review"}}}], "count": 13}