{"notes": [{"id": "BJlguT4YPr", "original": "H1laAtowPr", "number": 619, "cdate": 1569439079799, "ddate": null, "tcdate": 1569439079799, "tmdate": 1583912044807, "tddate": null, "forum": "BJlguT4YPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["wcohen@google.com", "haitiansun@google.com", "rofer@google.com", "msiegler@google.com"], "title": "Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base", "authors": ["William W. Cohen", "Haitian Sun", "R. Alex Hofer", "Matthew Siegler"], "pdf": "/pdf/0a0b8ae9915a6c580db0233edb49d0b07e5b4a82.pdf", "TL;DR": "A scalable differentiable neural module that implements reasoning on symbolic KBs.", "abstract": "We describe a novel way of representing a symbolic knowledge base (KB) called a sparse-matrix reified KB.  This representation enables neural modules that are fully differentiable, faithful to the original semantics of the KB, expressive enough to model multi-hop inferences, and scalable enough to use with realistically large KBs. The sparse-matrix reified KB can be distributed across multiple GPUs, can scale to tens of millions of entities and facts, and is orders of magnitude faster than naive sparse-matrix implementations.  The reified KB enables very simple end-to-end architectures to obtain competitive performance on several benchmarks representing two families of tasks: KB completion, and learning semantic parsers from denotations.", "keywords": ["question-answering", "knowledge base completion", "neuro-symbolic reasoning", "multihop reasoning"], "paperhash": "cohen|scalable_neural_methods_for_reasoning_with_a_symbolic_knowledge_base", "_bibtex": "@inproceedings{\nCohen2020Scalable,\ntitle={Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base},\nauthor={William W. Cohen and Haitian Sun and R. Alex Hofer and Matthew Siegler},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlguT4YPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/42e786f408feef1f99420b17dacab9d930c4d412.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "O5-NRxbiO", "original": null, "number": 1, "cdate": 1576798701555, "ddate": null, "tcdate": 1576798701555, "tmdate": 1576800934447, "tddate": null, "forum": "BJlguT4YPr", "replyto": "BJlguT4YPr", "invitation": "ICLR.cc/2020/Conference/Paper619/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper proposes an approach to representing a symbolic knowledge base as a sparse matrix, which enables the use of  differentiable neural modules for inference. This approach scales to large knowledge bases and is demonstrated on several tasks.   \n\nPost-discussion and rebuttal, all three reviewers are in agreement that this is an interesting and useful paper. There was intiially some concern about clarity and polish, but these have been resolved upon rebuttal and discussion. Therefore I recommend acceptance. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wcohen@google.com", "haitiansun@google.com", "rofer@google.com", "msiegler@google.com"], "title": "Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base", "authors": ["William W. Cohen", "Haitian Sun", "R. Alex Hofer", "Matthew Siegler"], "pdf": "/pdf/0a0b8ae9915a6c580db0233edb49d0b07e5b4a82.pdf", "TL;DR": "A scalable differentiable neural module that implements reasoning on symbolic KBs.", "abstract": "We describe a novel way of representing a symbolic knowledge base (KB) called a sparse-matrix reified KB.  This representation enables neural modules that are fully differentiable, faithful to the original semantics of the KB, expressive enough to model multi-hop inferences, and scalable enough to use with realistically large KBs. The sparse-matrix reified KB can be distributed across multiple GPUs, can scale to tens of millions of entities and facts, and is orders of magnitude faster than naive sparse-matrix implementations.  The reified KB enables very simple end-to-end architectures to obtain competitive performance on several benchmarks representing two families of tasks: KB completion, and learning semantic parsers from denotations.", "keywords": ["question-answering", "knowledge base completion", "neuro-symbolic reasoning", "multihop reasoning"], "paperhash": "cohen|scalable_neural_methods_for_reasoning_with_a_symbolic_knowledge_base", "_bibtex": "@inproceedings{\nCohen2020Scalable,\ntitle={Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base},\nauthor={William W. Cohen and Haitian Sun and R. Alex Hofer and Matthew Siegler},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlguT4YPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/42e786f408feef1f99420b17dacab9d930c4d412.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJlguT4YPr", "replyto": "BJlguT4YPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713484, "tmdate": 1576800263110, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper619/-/Decision"}}}, {"id": "BklxFnq2YS", "original": null, "number": 2, "cdate": 1571757176097, "ddate": null, "tcdate": 1571757176097, "tmdate": 1574445470101, "tddate": null, "forum": "BJlguT4YPr", "replyto": "BJlguT4YPr", "invitation": "ICLR.cc/2020/Conference/Paper619/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "The paper proposes sparse-matrix KB representation for end-to-end KB reasoning tasks. They demonstrate that their algorithm is scalable to large knowledge graphs which is the central contribution of the paper.\nThey apply this to a bunch of tasks such as KB Completion and KBQA. This is done by mapping the query to a set (weights) of relations over which reasoning is performed. \n \nI would first like to comment that I found the paper very hard to read. Thus due to my difficulty in understanding the paper, it is possible that I might have misunderstood parts of the paper. \n \nThe notations are overly complex. In my opinion, the notations can be simplified to a considerable extent. I would suggest a Table of notations or a small figure explaining the model. The paper, in my view, requires  considerable rewriting.\n \nThe paper states that the proposed approach encodes three floating point values and 6 integers for each triple. Is this true for KBC task? Because I am quite surprised that ReifKB approaches SOTA which use hundreds of floats for representing each entity and relation (e.g. DistMult/ComplEx).  \n \nHere are somethings which are not very clear to me KBQA tasks: It is not fully clear how the query is mapped to r. In my understanding r is a set of relations. Is the output of linear function taken as weights? \nKBC Task: How do you ensure that the N chains are distinct? Do you have N different linear functions (f_i)? Also why is x_i^t added to follow in KBC task?\n \nThe paper needs considerable rewriting and therefore I cannot recommend this paper for acceptance at this stage.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper619/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper619/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wcohen@google.com", "haitiansun@google.com", "rofer@google.com", "msiegler@google.com"], "title": "Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base", "authors": ["William W. Cohen", "Haitian Sun", "R. Alex Hofer", "Matthew Siegler"], "pdf": "/pdf/0a0b8ae9915a6c580db0233edb49d0b07e5b4a82.pdf", "TL;DR": "A scalable differentiable neural module that implements reasoning on symbolic KBs.", "abstract": "We describe a novel way of representing a symbolic knowledge base (KB) called a sparse-matrix reified KB.  This representation enables neural modules that are fully differentiable, faithful to the original semantics of the KB, expressive enough to model multi-hop inferences, and scalable enough to use with realistically large KBs. The sparse-matrix reified KB can be distributed across multiple GPUs, can scale to tens of millions of entities and facts, and is orders of magnitude faster than naive sparse-matrix implementations.  The reified KB enables very simple end-to-end architectures to obtain competitive performance on several benchmarks representing two families of tasks: KB completion, and learning semantic parsers from denotations.", "keywords": ["question-answering", "knowledge base completion", "neuro-symbolic reasoning", "multihop reasoning"], "paperhash": "cohen|scalable_neural_methods_for_reasoning_with_a_symbolic_knowledge_base", "_bibtex": "@inproceedings{\nCohen2020Scalable,\ntitle={Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base},\nauthor={William W. Cohen and Haitian Sun and R. Alex Hofer and Matthew Siegler},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlguT4YPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/42e786f408feef1f99420b17dacab9d930c4d412.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlguT4YPr", "replyto": "BJlguT4YPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper619/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper619/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575669557638, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper619/Reviewers"], "noninvitees": [], "tcdate": 1570237749507, "tmdate": 1575669557650, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper619/-/Official_Review"}}}, {"id": "S1xi_7koYB", "original": null, "number": 1, "cdate": 1571644274965, "ddate": null, "tcdate": 1571644274965, "tmdate": 1573830476736, "tddate": null, "forum": "BJlguT4YPr", "replyto": "BJlguT4YPr", "invitation": "ICLR.cc/2020/Conference/Paper619/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "N/A", "review_assessment:_thoroughness_in_paper_reading": "N/A", "title": "Official Blind Review #1", "review": "The paper provides a way to represent symbolic KBs called sparse matrix reified. Relations and entities' types are modelled using sparse matrices. This modelization allows distributing the computation on many GPUs. A neural model is used to manage these matrices. The trained model can be used to perform multi-hop queries. The proposed system has been compared with related systems. The results show that it achieves hits@1 that are comparable with the others.\n\nThe proposed approach seems promising, however, I feel that the paper is not ready for publication. The experiments lack a scalability test with related systems, the scalability test included in the paper only takes into account other definitions of the system. Also, comparison on the run time should be performed to see if the lower performance in terms of hits@1 (which are good anyway) is balanced by a better run time.\n\nTherefore, it is difficult to see whether the proposed system is good or not.\n\nAs for the description of the system, it seems to me to be quite foggy. In my opinion, the neural model should be better described to show how the sparse matrices are mapped into the model.\nAlso, how are the training and testing sentences created? How would the output of a query look?\n\nAfter reading the paper I have the feeling that it was written in a bit of a hurry, without working on the details. There are several typos, parentheses not correctly opened or closed, out of place commas that make some sentences seem unrelated to the rest of the paragraph. However, here last problems are easily fixable.\n\nTo sum up, I am conflicted about the choice of the final score. On the one hand, the approach is interesting, on the other hand the article seems to me not mature enough and strong enough from the point of view of organization, contents and experimental results shown.\n\nMinor problems\nOn page 4, the size of the matrices XM_k uses the factor b that is introduced later.\nIn the introduction, next to footnote 1, I suggest specifying that A is the first query, the one about Tarantino's movies.\nAlso, his surname is misspelt throughout the paper. The correct one is Tarantino.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper619/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper619/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wcohen@google.com", "haitiansun@google.com", "rofer@google.com", "msiegler@google.com"], "title": "Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base", "authors": ["William W. Cohen", "Haitian Sun", "R. Alex Hofer", "Matthew Siegler"], "pdf": "/pdf/0a0b8ae9915a6c580db0233edb49d0b07e5b4a82.pdf", "TL;DR": "A scalable differentiable neural module that implements reasoning on symbolic KBs.", "abstract": "We describe a novel way of representing a symbolic knowledge base (KB) called a sparse-matrix reified KB.  This representation enables neural modules that are fully differentiable, faithful to the original semantics of the KB, expressive enough to model multi-hop inferences, and scalable enough to use with realistically large KBs. The sparse-matrix reified KB can be distributed across multiple GPUs, can scale to tens of millions of entities and facts, and is orders of magnitude faster than naive sparse-matrix implementations.  The reified KB enables very simple end-to-end architectures to obtain competitive performance on several benchmarks representing two families of tasks: KB completion, and learning semantic parsers from denotations.", "keywords": ["question-answering", "knowledge base completion", "neuro-symbolic reasoning", "multihop reasoning"], "paperhash": "cohen|scalable_neural_methods_for_reasoning_with_a_symbolic_knowledge_base", "_bibtex": "@inproceedings{\nCohen2020Scalable,\ntitle={Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base},\nauthor={William W. Cohen and Haitian Sun and R. Alex Hofer and Matthew Siegler},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlguT4YPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/42e786f408feef1f99420b17dacab9d930c4d412.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlguT4YPr", "replyto": "BJlguT4YPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper619/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper619/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575669557638, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper619/Reviewers"], "noninvitees": [], "tcdate": 1570237749507, "tmdate": 1575669557650, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper619/-/Official_Review"}}}, {"id": "rJl7JTbijS", "original": null, "number": 6, "cdate": 1573752027505, "ddate": null, "tcdate": 1573752027505, "tmdate": 1573752027505, "tddate": null, "forum": "BJlguT4YPr", "replyto": "S1xi_7koYB", "invitation": "ICLR.cc/2020/Conference/Paper619/-/Official_Comment", "content": {"title": "Response to Blind Review #1", "comment": "Thank you for your thoughtful review.  We respond below to the your\ncomments.\n\nRESPONSE TO \"experiments lack a scalability test\": We made two major\nchanges in the revised paper.\n\n1) We compared to key-value memory networks in Figure 1, demonstrating\nthat reified KBs are much faster for many relations, and much more\nmemory-efficient.\n\n2) We compared the run-time efficiency of ReifKB with several baseline\nmodels on the metaQA tasks. The performance and running time on 10k\nsamples are shown in the table below, which is also given in the\nAppendix, and a bubble chart figure is presented in Table 5.\n\nTime (sec)  (hits@1)  \tMethod\t  \n---------------------------------\n72.6\t \t 79.7\t\t Reif KB\t    \n189.8\t    10.1\t\t KV-mem\t    \n28.9\t    \t 77.2\t\t GRAFT-Net   \n1131\t    91.4      \tPullNet   \n\n\nIn summary compared to Key-Value Memory Network (KV-mem), ReifKB is\nmore efficient in run-time and achieves much better\nperformance. GRAFT-Net is a pipeline approach which runs some\nheuristic-based retrieval algorithm to retrieve 500 triples from the\nKB for each question in MetaQA separately and then applies an\nexpensive Graph-CNN model to find answers. It\u2019s encouraging to see\nthat ReifKB is only two times slower than GRAFT-Net while running on a\nKB which is 400 times larger!  PullNet is the state-of-the-art system\non MetaQA recently proposed in EMNLP 2019 that includes a complicated\nlearned iterative process of retrieval and classification. PullNet has\nbetter performance than ReifKB but is significantly slower.\n\nRESPONSE TO \"the neural model should be better described to show how\nthe sparse matrices are mapped into the model.\"  Briefly, the\nconnection to the sparse matrices is given by the follow(x,r)\noperation that appears in each model.  To clarify this in the\nrevision, we \n\n1) Discuss the application to neural modeling briefly when we define\nthe follow operation in Eq 2; \n\n2) Mention the connection to the follow operation after the first\nmodel we present (for MetaQA) \n\n3) Include a paragraph of additional explanation after the first model\nwe present.\n\nRESPONSE TO \"Also, how are the training and testing sentences created?\nHow would the output of a query look?\" We added some examples to the\nappendix.  For example, a 2-hop question in MetaQA could be \u201cWho\nco-start with Robert Downey Jr. in their movies?\u201d, and the answers is\na list of actors (entities) \u201cChris Hemsworth\u201d, \u201cThomas Stanley\u201d,\netc. Triples in the knowledge base are represented as <subject,\nrelation, object>, such as, <Robert Downey Jr., act_in, Avengers:\nEndgame>, <Avengers: Endgame, stars, Thomas Stanley>, <Avengers:\nEndgame, published_in, 2019>, etc.\n\nRESPONSE TO: \"There are several typos, ...\": We have fixed the minor\nproblems you noticed --- thank you! --- and believe the revised\nversion is much clearer.\n\nRESPONSE TO: \"To sum up, I am conflicted about the choice of the final\nscore. On the one hand, the approach is interesting, on the other hand\nthe article seems to me not mature enough and strong enough from the\npoint of view of organization, contents and experimental results\nshown.\"  Thanks for finding the work interesting - we hope the many\nchanges in the revision will improve your view of the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper619/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper619/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wcohen@google.com", "haitiansun@google.com", "rofer@google.com", "msiegler@google.com"], "title": "Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base", "authors": ["William W. Cohen", "Haitian Sun", "R. Alex Hofer", "Matthew Siegler"], "pdf": "/pdf/0a0b8ae9915a6c580db0233edb49d0b07e5b4a82.pdf", "TL;DR": "A scalable differentiable neural module that implements reasoning on symbolic KBs.", "abstract": "We describe a novel way of representing a symbolic knowledge base (KB) called a sparse-matrix reified KB.  This representation enables neural modules that are fully differentiable, faithful to the original semantics of the KB, expressive enough to model multi-hop inferences, and scalable enough to use with realistically large KBs. The sparse-matrix reified KB can be distributed across multiple GPUs, can scale to tens of millions of entities and facts, and is orders of magnitude faster than naive sparse-matrix implementations.  The reified KB enables very simple end-to-end architectures to obtain competitive performance on several benchmarks representing two families of tasks: KB completion, and learning semantic parsers from denotations.", "keywords": ["question-answering", "knowledge base completion", "neuro-symbolic reasoning", "multihop reasoning"], "paperhash": "cohen|scalable_neural_methods_for_reasoning_with_a_symbolic_knowledge_base", "_bibtex": "@inproceedings{\nCohen2020Scalable,\ntitle={Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base},\nauthor={William W. Cohen and Haitian Sun and R. Alex Hofer and Matthew Siegler},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlguT4YPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/42e786f408feef1f99420b17dacab9d930c4d412.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlguT4YPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper619/Authors", "ICLR.cc/2020/Conference/Paper619/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper619/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper619/Reviewers", "ICLR.cc/2020/Conference/Paper619/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper619/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper619/Authors|ICLR.cc/2020/Conference/Paper619/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168762, "tmdate": 1576860548727, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper619/Authors", "ICLR.cc/2020/Conference/Paper619/Reviewers", "ICLR.cc/2020/Conference/Paper619/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper619/-/Official_Comment"}}}, {"id": "HJgpu3Wiir", "original": null, "number": 5, "cdate": 1573751925096, "ddate": null, "tcdate": 1573751925096, "tmdate": 1573751925096, "tddate": null, "forum": "BJlguT4YPr", "replyto": "BklxFnq2YS", "invitation": "ICLR.cc/2020/Conference/Paper619/-/Official_Comment", "content": {"title": "Response to Blind Review #2", "comment": "Thank you for your thoughtful review.  We respond below to the your\ncomments and try and answer your questions.\n\nRESPONSE TO \"notations are overly complex\": We thank you for the\ncomment and believe the paper is now much clearer.  In the revised\nversion, we made a number of simplifying changes.  We removed all\ndiscussion of entity and relation types, and removed the omega[x in X]\nnotation and the delta[a=b] notation. We put the discussion of types\nwhich are used mainly in the WebQuestionsSP experiment, into an\nappendix, and also moved a discussion of how to use soft KBs into an\nappendix.  We made the definition of M_R into a separate Equation.  We\nalso relegated some details, such as minibatching and how distributed\nmatrix multiplication is performed, to appendices.  As requested, we\nadded a table of notation.  ONLY 15 NOTATIONAL CONVENTIONS ARE NOW\nDEFINED IN THE BODY OF THE PAPER including the convention of calling\nentities x.  (The several task-specific models we introduce still\nrequire some notation to define, but the use of this notation is very\nlocal and shouldn't cause much confusion to the reader.)\n \nRESPONSE TO \"I am quite surprised that ReifKB approaches SOTA which\nuse hundreds of floats for representing each entity and relation\":\nYes, this is also true for the KBC task!  It\u2019s a little surprising but\none of the strengths of the method.  The reason for the difference is\nthat ReifKB doesn\u2019t try and produce a generalizable embedded version\nof every entity in the KB - instead it builds a soft version of a\nsymbolic KB query that approximates the target relation, so the model\nthat it constructs is only a few parameters for each relation it is\nlearning about.  We added the following discussion in the revision,\nwhen we discuss the KBC results, to emphasize this: \"The competitive\nperformance of the ReifKB model is perhaps surprising, since it has\nmany fewer parameters than the baseline models---only one float and\ntwo integers per KB triple, plus a small number of parameters to\ndefine the $f_i^t$ functions for each relation.  The ability to use\nfewer parameters is directly related to the fact that our model\n\\emph{directly uses inference on the existing symbolic KB} in its\nmodel, rather than having to learn embeddings that approximate this\ninference.  Or course, since the KB is incomplete, some learning is\nstill required, but for KBC the ability to perform inference on the\nincomplete KB ``out of the box'' appears to be very advantageous.\"\n \nRESPONSE TO SPECIFIC QUESTIONS: \n\nQ: Here are somethings which are not very clear to me KBQA tasks: It is\nnot fully clear how the query is mapped to r. In my understanding r is\na set of relations. Is the output of linear function taken as weights?\n\nA: Your interpretation is exactly right: r is a vector of weights over\nrelations, which are then used as vectors \\textbf{r} in Eq (1) ---\ni.e., they are interpreted as weights used to mix the sparse-matrix\nencoding of the relations in the KB.\n\nQ: KBC Task: How do you ensure that the N chains are distinct? Do you\nhave N different linear functions (f_i)? \n\nA: The N chains don\u2019t need to be distinct: if you don\u2019t need all N\nchains to model a relation the optimizer could chose to duplicate\nthem.  We do indeed have multiple linear functions - a total of N*T of\nthem.  \n\nQ: Also why is x_i^t added to follow in KBC task?\n\nA: Why adding x_i^t works is difficult to answer succinctly: briefly,\nexpanding the sum out recursively means that the model includes many\ndiffent types of paths through the KB.  We added a final subsection of\nAppendix E that gives some intuition of why we this is true.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper619/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper619/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wcohen@google.com", "haitiansun@google.com", "rofer@google.com", "msiegler@google.com"], "title": "Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base", "authors": ["William W. Cohen", "Haitian Sun", "R. Alex Hofer", "Matthew Siegler"], "pdf": "/pdf/0a0b8ae9915a6c580db0233edb49d0b07e5b4a82.pdf", "TL;DR": "A scalable differentiable neural module that implements reasoning on symbolic KBs.", "abstract": "We describe a novel way of representing a symbolic knowledge base (KB) called a sparse-matrix reified KB.  This representation enables neural modules that are fully differentiable, faithful to the original semantics of the KB, expressive enough to model multi-hop inferences, and scalable enough to use with realistically large KBs. The sparse-matrix reified KB can be distributed across multiple GPUs, can scale to tens of millions of entities and facts, and is orders of magnitude faster than naive sparse-matrix implementations.  The reified KB enables very simple end-to-end architectures to obtain competitive performance on several benchmarks representing two families of tasks: KB completion, and learning semantic parsers from denotations.", "keywords": ["question-answering", "knowledge base completion", "neuro-symbolic reasoning", "multihop reasoning"], "paperhash": "cohen|scalable_neural_methods_for_reasoning_with_a_symbolic_knowledge_base", "_bibtex": "@inproceedings{\nCohen2020Scalable,\ntitle={Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base},\nauthor={William W. Cohen and Haitian Sun and R. Alex Hofer and Matthew Siegler},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlguT4YPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/42e786f408feef1f99420b17dacab9d930c4d412.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlguT4YPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper619/Authors", "ICLR.cc/2020/Conference/Paper619/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper619/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper619/Reviewers", "ICLR.cc/2020/Conference/Paper619/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper619/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper619/Authors|ICLR.cc/2020/Conference/Paper619/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168762, "tmdate": 1576860548727, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper619/Authors", "ICLR.cc/2020/Conference/Paper619/Reviewers", "ICLR.cc/2020/Conference/Paper619/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper619/-/Official_Comment"}}}, {"id": "Bkefz3bsoB", "original": null, "number": 4, "cdate": 1573751817555, "ddate": null, "tcdate": 1573751817555, "tmdate": 1573751817555, "tddate": null, "forum": "BJlguT4YPr", "replyto": "S1lWRZMTYS", "invitation": "ICLR.cc/2020/Conference/Paper619/-/Official_Comment", "content": {"title": "Response to Blind Review #3", "comment": "Thank you for your thoughtful review.  We respond below to the\ndisadvantages you list.\n\nRESPONSE TO DISADVANTAGE 1) \"... cover and discuss more existing\nmethods:\" In the first submission, we did discuss this briefly in the\nrelated work section, saying: \"Neural architectures like memory\nnetworks (Weston et al., 2014), or other architectures that use\nattention over some data structure approximating assertions (Andreas\net al., 2016; Gupta & Lewis, 2018) can be used to build soft versions\nof relation-set following: however, they also do not scale well to\nlarge KBs, so they are typically used either with a non-differentiable\nad hoc retrieval mechanism, or else in cases where a small amount of\ninformation is relevant to a question (e.g., (Weston et al., 2015;\nZhong et al., 2017)).\" \n\nIn the revision we extended the discussion in related works of\nembedding methods and how our work is different.  Also, we added more\nmaterial to the experimental section to further clarify the relation\nto key-value networks.  We plotted time in qps for a key-value network\nin Figure 1, and verified that runtime memory consumption is much\nworse than the reified KB---the model we used can only be used on 1/20\nthe relations and 1/10 the triples before completely exhausting a 12Gb\nGPU memory.  We added this text to Section 3.2, where we discuss\nwebQuestionsSP: This dataset [webQuestionsSP] is a good illustration\nof the scalability issues associated with prior approaches to\nincluding a KB in a model, such as key-value memory networks.  A\nkey-value network that appends relations and subject entities as a key\nand has an object embedding as the value could be trained to implement\nsomething similar to relation-set following.  If we assume 64-float\nembeddings for the 12.9M entities, the full KB of 43.7M facts would be\n67Gb in size, which is impractical, and a softmax over the 43.7M keys\nwould be prohibitively expensive.  We provide more detailed timing\ninformation for key-value networks on MetaQA in a new figure in Table\n5 (the precise numbers are given in Appendix E).\n\nRESPONSE TO DISADVANTAGE 2) \"[the description] lacks some guidance\nfor the readers to understand the motivation behind the scene. For\nexample, it would be helpful to discuss why representing relations as\nsparse matrices is necessary\".  The original paper discussed this\nbriefly, and partly in a footnote: we did say in sec 2.1: \"For all but\nthe smallest KBs, a relation matrix must be implemented using a sparse\nmatrix data structure, as explicitly storing all N\u03c41 \u00d7 N\u03c42 values is\nimpractical [4]\" with footnote 4 saying: \"For instance, if a KB\ncontains 10,000 movie entities and 100,000 person entities, then a\nrelationship like writer_of would require storing 1 billion values\u2014far\nmore than few tens of thousands of writer_of facts that would be in\nthe KB (since most movies have only one or writers.)\"\n\nIn the revision, we expanded this discussion to a paragraph as\nfollows: \"Scalably representing a large KB requires careful\nconsideration of the implementation.  One important issue is that for\nall but the smallest KBs, a relation matrix must be implemented using\na sparse matrix data structure, as explicitly storing all N_E x N_E\nvalues is impractical.  For instance, consider a KB containing 10,000\nmovie entities and 100,000 person entities.  A relationship like\nwriter_of would have only a few tens of thousands of facts, since most\nmovies have only one or two writers, but a dense matrix would have\nmore than 1 billion values.\"  \n\nWe also give more concrete experimental comparisons to key-value\nmemory networks, which are a dense-matrix alternative to the reified\nKB, and give more run-time experiments --- see our response below to\nreviewer 1.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper619/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper619/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wcohen@google.com", "haitiansun@google.com", "rofer@google.com", "msiegler@google.com"], "title": "Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base", "authors": ["William W. Cohen", "Haitian Sun", "R. Alex Hofer", "Matthew Siegler"], "pdf": "/pdf/0a0b8ae9915a6c580db0233edb49d0b07e5b4a82.pdf", "TL;DR": "A scalable differentiable neural module that implements reasoning on symbolic KBs.", "abstract": "We describe a novel way of representing a symbolic knowledge base (KB) called a sparse-matrix reified KB.  This representation enables neural modules that are fully differentiable, faithful to the original semantics of the KB, expressive enough to model multi-hop inferences, and scalable enough to use with realistically large KBs. The sparse-matrix reified KB can be distributed across multiple GPUs, can scale to tens of millions of entities and facts, and is orders of magnitude faster than naive sparse-matrix implementations.  The reified KB enables very simple end-to-end architectures to obtain competitive performance on several benchmarks representing two families of tasks: KB completion, and learning semantic parsers from denotations.", "keywords": ["question-answering", "knowledge base completion", "neuro-symbolic reasoning", "multihop reasoning"], "paperhash": "cohen|scalable_neural_methods_for_reasoning_with_a_symbolic_knowledge_base", "_bibtex": "@inproceedings{\nCohen2020Scalable,\ntitle={Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base},\nauthor={William W. Cohen and Haitian Sun and R. Alex Hofer and Matthew Siegler},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlguT4YPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/42e786f408feef1f99420b17dacab9d930c4d412.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJlguT4YPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper619/Authors", "ICLR.cc/2020/Conference/Paper619/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper619/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper619/Reviewers", "ICLR.cc/2020/Conference/Paper619/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper619/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper619/Authors|ICLR.cc/2020/Conference/Paper619/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168762, "tmdate": 1576860548727, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper619/Authors", "ICLR.cc/2020/Conference/Paper619/Reviewers", "ICLR.cc/2020/Conference/Paper619/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper619/-/Official_Comment"}}}, {"id": "S1lWRZMTYS", "original": null, "number": 3, "cdate": 1571787208928, "ddate": null, "tcdate": 1571787208928, "tmdate": 1572972572629, "tddate": null, "forum": "BJlguT4YPr", "replyto": "BJlguT4YPr", "invitation": "ICLR.cc/2020/Conference/Paper619/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes an efficient sparse-matrix based representation for symbolic knowledge bases. This representation enables fully differentiable neural modules to model multi-hop inferences, which is designed to be scalable to handle realistically large knowledge bases. Experiments using the proposed method with end-to-end architectures on downstream KB tasks demonstrate its effectiveness and efficiency. Overall, this paper makes clear contributions and can inspire other researchers in the community to apply this KB representation for various learning / reasoning tasks on knowledge bases. However, considering the readability, I would like to recommend a weak accept for this paper.\n\nAdvantages of this paper: 1) The proposed method employs three sparse matrices to represent all KB relations, which is more efficient than existing work such as TensorLog and can support relation sets; 2) The reified KB representation is scalable, which can be distributed across multiple GPUs, making it much faster than the naive implementation; 3) The proposed method can be naturally used in end-to-end neural models and efficiently trained with gradient-based approaches.\n\nDisadvantages of this paper: 1) In the introduction part, the authors mention that existing neural KB reasoning methods generally require some non-differentiable mechanism to retrieve small question-dependent subset of the KB, but there exist some existing methods such as memory networks that are fully differentiable for the KBQA task. It would be better to cover and discuss more existing methods when summarizing their properties; 2) In the methodology part (Section 2), the description is clear but lacks some guidance for the readers to understand the motivation behind the scene. For example, it would be helpful to discuss why representing relations as sparse matrices is necessary, whether there is any other choices, and the benefit of making the current choice. This might seem obvious for the authors, but can help readers that are not familiar with the context access the methodology more easily and understand the motivation better."}, "signatures": ["ICLR.cc/2020/Conference/Paper619/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper619/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["wcohen@google.com", "haitiansun@google.com", "rofer@google.com", "msiegler@google.com"], "title": "Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base", "authors": ["William W. Cohen", "Haitian Sun", "R. Alex Hofer", "Matthew Siegler"], "pdf": "/pdf/0a0b8ae9915a6c580db0233edb49d0b07e5b4a82.pdf", "TL;DR": "A scalable differentiable neural module that implements reasoning on symbolic KBs.", "abstract": "We describe a novel way of representing a symbolic knowledge base (KB) called a sparse-matrix reified KB.  This representation enables neural modules that are fully differentiable, faithful to the original semantics of the KB, expressive enough to model multi-hop inferences, and scalable enough to use with realistically large KBs. The sparse-matrix reified KB can be distributed across multiple GPUs, can scale to tens of millions of entities and facts, and is orders of magnitude faster than naive sparse-matrix implementations.  The reified KB enables very simple end-to-end architectures to obtain competitive performance on several benchmarks representing two families of tasks: KB completion, and learning semantic parsers from denotations.", "keywords": ["question-answering", "knowledge base completion", "neuro-symbolic reasoning", "multihop reasoning"], "paperhash": "cohen|scalable_neural_methods_for_reasoning_with_a_symbolic_knowledge_base", "_bibtex": "@inproceedings{\nCohen2020Scalable,\ntitle={Scalable Neural Methods for Reasoning With a Symbolic Knowledge   Base},\nauthor={William W. Cohen and Haitian Sun and R. Alex Hofer and Matthew Siegler},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlguT4YPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/42e786f408feef1f99420b17dacab9d930c4d412.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlguT4YPr", "replyto": "BJlguT4YPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper619/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper619/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575669557638, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper619/Reviewers"], "noninvitees": [], "tcdate": 1570237749507, "tmdate": 1575669557650, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper619/-/Official_Review"}}}], "count": 8}