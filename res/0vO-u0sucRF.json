{"notes": [{"id": "0vO-u0sucRF", "original": "m8fRJFH_N2K", "number": 2372, "cdate": 1601308261535, "ddate": null, "tcdate": 1601308261535, "tmdate": 1614985672769, "tddate": null, "forum": "0vO-u0sucRF", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Information Theoretic Meta Learning with Gaussian Processes", "authorids": ["~Michalis_Titsias1", "snikolou@aueb.gr", "~Alexandre_Galashov1"], "authors": ["Michalis Titsias", "Sotirios Nikoloutsopoulos", "Alexandre Galashov"], "keywords": ["Meta Learning", "Information Bottleneck", "Gaussian Processes", "Few-shot learning", "Variational Inference"], "abstract": "We formulate meta learning using information theoretic concepts such as mutual information and the information bottleneck. The idea is to learn a stochastic representation or encoding of the task description, given by a training or support set, that is highly informative about predicting the validation set. By making use of variational approximations to the mutual information, we derive a general and tractable framework for meta learning. We  particularly develop new memory-based meta learning algorithms based on Gaussian processes and derive extensions that combine memory and gradient-based meta learning. We demonstrate our method on few-shot regression and classification by using standard benchmarks such as Omniglot, mini-Imagenet and Augmented Omniglot.\n", "one-sentence_summary": "Meta learning using information bottleneck and combination of memory and gradient-based techniques using Gaussian processes. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "titsias|information_theoretic_meta_learning_with_gaussian_processes", "pdf": "/pdf/5ab73c2b33e07eaf54bb442b5f013e689025036b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Gc5Drbr45H", "_bibtex": "@misc{\ntitsias2021information,\ntitle={Information Theoretic Meta Learning with Gaussian Processes},\nauthor={Michalis Titsias and Sotirios Nikoloutsopoulos and Alexandre Galashov},\nyear={2021},\nurl={https://openreview.net/forum?id=0vO-u0sucRF}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "EwPfvVf0rno", "original": null, "number": 1, "cdate": 1610040483634, "ddate": null, "tcdate": 1610040483634, "tmdate": 1610474088819, "tddate": null, "forum": "0vO-u0sucRF", "replyto": "0vO-u0sucRF", "invitation": "ICLR.cc/2021/Conference/Paper2372/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Information bottleneck is a well-known principle that is used for clustering, dimensionality reduction, and recently deep learning. It finds a compressed representation of input X while retaining most information on the response Y. This paper addresses an attempt to interpret the meta-learning using the information bottleneck. In addition, a GP-based meta-learning method is also proposed. \nThe topic itself is interesting without any doubt. However, most of reviewers have serious concerns about this work, which is summarized below. First of all, two components of this paper (IB and GP-based meta-learning) do not provide a coherent message.  While the IB interpretation is emphasized in the beginning of this paper, the main point seems to that GP-based methods can be more data efficient than gradient-based meta learning. There does not much point to GP+MAML or IB interpretation of MAML.  Experiments are not strong enough, although a few ones are added during the author responses. During the discussion with reviewers, no one support this work, so I do not have choice but to suggest rejection.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Meta Learning with Gaussian Processes", "authorids": ["~Michalis_Titsias1", "snikolou@aueb.gr", "~Alexandre_Galashov1"], "authors": ["Michalis Titsias", "Sotirios Nikoloutsopoulos", "Alexandre Galashov"], "keywords": ["Meta Learning", "Information Bottleneck", "Gaussian Processes", "Few-shot learning", "Variational Inference"], "abstract": "We formulate meta learning using information theoretic concepts such as mutual information and the information bottleneck. The idea is to learn a stochastic representation or encoding of the task description, given by a training or support set, that is highly informative about predicting the validation set. By making use of variational approximations to the mutual information, we derive a general and tractable framework for meta learning. We  particularly develop new memory-based meta learning algorithms based on Gaussian processes and derive extensions that combine memory and gradient-based meta learning. We demonstrate our method on few-shot regression and classification by using standard benchmarks such as Omniglot, mini-Imagenet and Augmented Omniglot.\n", "one-sentence_summary": "Meta learning using information bottleneck and combination of memory and gradient-based techniques using Gaussian processes. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "titsias|information_theoretic_meta_learning_with_gaussian_processes", "pdf": "/pdf/5ab73c2b33e07eaf54bb442b5f013e689025036b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Gc5Drbr45H", "_bibtex": "@misc{\ntitsias2021information,\ntitle={Information Theoretic Meta Learning with Gaussian Processes},\nauthor={Michalis Titsias and Sotirios Nikoloutsopoulos and Alexandre Galashov},\nyear={2021},\nurl={https://openreview.net/forum?id=0vO-u0sucRF}\n}"}, "tags": [], "invitation": {"reply": {"forum": "0vO-u0sucRF", "replyto": "0vO-u0sucRF", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040483621, "tmdate": 1610474088803, "id": "ICLR.cc/2021/Conference/Paper2372/-/Decision"}}}, {"id": "4S-Ag-rNZl1", "original": null, "number": 2, "cdate": 1603817743182, "ddate": null, "tcdate": 1603817743182, "tmdate": 1606764633670, "tddate": null, "forum": "0vO-u0sucRF", "replyto": "0vO-u0sucRF", "invitation": "ICLR.cc/2021/Conference/Paper2372/-/Official_Review", "content": {"title": "Interesting ways of combining information theory, variational inference, and Bayesian non-parametric for meta-learning.", "review": "Summary:\n\nThe paper proposed variational approximations to the information bottleneck objective functions for meta-learning.\nThe authors then provided three different settings using their variational loss functions, namely SMAML, GP, and GP + MAML.\nThe authors' motivations for these three settings were to study the effect of stochastic gradient-based method, non-parametric method, and the combination of gradient-based method with non-parametric method.\n\nReason for the score:\n\nI find the paper interesting for establishing a variational information theoretic objective function for meta-learning.\nI also enjoyed the comparisons among purely gradient-based method with non-parametric method and their hybrid.\nYet, I did find some details where the motivations are a bit difficult to understand. It would be great if the authors could\nclarify them.\n\nPro:\n- The authors provided clear derivations for their variational objective functions.\n- The authors provided clear kernel setups for their GP models.\n- The authors provided evaluations for their three settings on both regression and classification tasks.\n- The paper is easy to follow and I did not find typos.\n\nCon / Questions:\n- For the objective function in Eq.1, is there a particular reason for assuming conditional independence for $q(Z| D^{t}) $?\n- For Eq.5 and Eq.6, are the models of $p_\\theta (D_i^v | Z_i)$ and $p_\\theta (Z_i)$ related in some ways? It seems that they are sharing the parameters $\\theta$. \n- I did not find Section 3.2's motivation very convincing. The authors mentioned that the hope for combining MAML with GP is to reduce inner steps for MAML and possibly achieve better results. Yet, in Figure 4, it seems that pure GP is basically on-par with GP+MAML and outperforms GP+MAML when more inner steps are introduced for MAML. Could the authors help me with some insights for this observation?\n- It seems that SMAML performed worse than MAML consistently for regression tasks and occasionally better for classification tasks. I observe that SMAML differ from MAML in two aspects, being stochastic and having the extra KL regularizer controlled by $\\beta$. Are the worsened performance due to being stochastic or too much regularization from the KL term?\n- On the classification tasks, it seems that GP models perform better than the standard MAML mostly when $K$ is large. It is difficult to conclude that the results are better thanks to GP being non-parametric or the variational loss function being better. This is also somewhat true for the regression tasks because we know GP is smooth and interpolates better.\n\n-----------------------------------------------------------------\nPost Rebuttal:\n\nMany thanks for the authors to update their original paper addressing some of my questions and concerns.\nUnfortunately, I still think that some aspects could be better analysed,\nit is not crystal clear to me if the improvements come from the GP or their proposed variational information\ntheoretic function. I am keeping my original score.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2372/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2372/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Meta Learning with Gaussian Processes", "authorids": ["~Michalis_Titsias1", "snikolou@aueb.gr", "~Alexandre_Galashov1"], "authors": ["Michalis Titsias", "Sotirios Nikoloutsopoulos", "Alexandre Galashov"], "keywords": ["Meta Learning", "Information Bottleneck", "Gaussian Processes", "Few-shot learning", "Variational Inference"], "abstract": "We formulate meta learning using information theoretic concepts such as mutual information and the information bottleneck. The idea is to learn a stochastic representation or encoding of the task description, given by a training or support set, that is highly informative about predicting the validation set. By making use of variational approximations to the mutual information, we derive a general and tractable framework for meta learning. We  particularly develop new memory-based meta learning algorithms based on Gaussian processes and derive extensions that combine memory and gradient-based meta learning. We demonstrate our method on few-shot regression and classification by using standard benchmarks such as Omniglot, mini-Imagenet and Augmented Omniglot.\n", "one-sentence_summary": "Meta learning using information bottleneck and combination of memory and gradient-based techniques using Gaussian processes. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "titsias|information_theoretic_meta_learning_with_gaussian_processes", "pdf": "/pdf/5ab73c2b33e07eaf54bb442b5f013e689025036b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Gc5Drbr45H", "_bibtex": "@misc{\ntitsias2021information,\ntitle={Information Theoretic Meta Learning with Gaussian Processes},\nauthor={Michalis Titsias and Sotirios Nikoloutsopoulos and Alexandre Galashov},\nyear={2021},\nurl={https://openreview.net/forum?id=0vO-u0sucRF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0vO-u0sucRF", "replyto": "0vO-u0sucRF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2372/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097869, "tmdate": 1606915795738, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2372/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2372/-/Official_Review"}}}, {"id": "zIjnLsiBtLR", "original": null, "number": 4, "cdate": 1605712203894, "ddate": null, "tcdate": 1605712203894, "tmdate": 1605732673486, "tddate": null, "forum": "0vO-u0sucRF", "replyto": "Pqf44PumjU1", "invitation": "ICLR.cc/2021/Conference/Paper2372/-/Official_Comment", "content": {"title": "response to the main comments ", "comment": "We would like to response to the main comments (1 to 7) \n\n1.  The main objective of our paper is to derive an information bottleneck principle for meta learning. We believe this is a valid methodological contribution. Then to justify this as a valid principle we have fully applied it to GPs. Our method does not provide SOTA classification results (although we are not aware of any published work reporting better accuracy in Augmented Omniglot than our GP method; see Table 8 in Appendix for results with the more advanced architecture). We believe that a clear message from our experiments is that our GP method can be significantly more data efficient than gradient-based methods such as MAML, with better calibrated probabilities.  We have updated the paper to add evidence that supports this claim.  We would be happy to compare with Snell & Zemel 2020. Although given that this work is co-current with ours,  we are not aware if there is code available to be able to re-run this method and include it our ablation study.  We can speculate that Snell & Zemel 2020 provides better classification accuracy due to this really efficient One-vs-Each P\u00f3lya-Gamma Augmentation, which  however is very specific to classification. \n \n2. As mentioned above we have updated the paper to emphasise the data efficiency aspect of our method and also its ability to provide  better predictive uncertainties. \n \n3. Regarding Bertinetto et al 2018 work their results are not comparable to our results in Table 1 because they use a different architecture than  Finn et al (2017),  that includes Dropout, different convolution layer sizes and leaky-Relu; see Section 4.2 in  Bertinetto et al 2018. Our results are based exactly on the same architecture and experimental setup in  Finn et al (2017). In Table 9 in the Appendix we report significant better results with more advanced architecture (which obviously for fair comparison should be compared only with methods using the same architecture and experimental setup).  We will  add a reference to Bertinetto et al 2018 regarding the similarly of the linear deep kernel.  \n \n 4. We could quote classification accuracies from other papers as suggested by the reviewer. But does this add any useful information for our paper? Notice that for MAML we have fully reproduced the MAML results and get for example NNL scores on top of accuracies.     \n  \n5. Regarding Augmented Omniglot for meta-training we used only the 300 task support points without further data augmentation. This proves to be sufficient for meta-training of the deep kernel feature vector, without data augmentation. Given that a GP is memory-based system then if we use data augmentation on top of the initial 300 task support points, then GP will need to memorise more data points, which while could improve further performance it could make meta-training more expensive.  However, for meta testing GP uses data augmentation exactly as MAML and both methods process exactly the same amount of data  as shown in Fig2. We also want to point out that MAML without data augmentation in meta-training will work much worse than the results reported.    \n\n6. We prefer the linear kernel since it makes the GP as close as possible to a non-GP system that uses the same DNN feature vector. The linear kernel  it is also computationally convenient. However, while exact Bayesian linear regression can be done linearly wrt the data points N, it has O(M^3) complexity over the size of feature vector so it is often much slower than O(N^3) in few-shot learning. For example, in mini-Imagenet N=5,K=1 the feature vector has size M=801, while N=5, so it is clearly preferable to decompose (with Cholesky) a 5 x 5 matrix than a 801 x 801 one. The linear kernel allows to choose between O(M^3) and O(N^3) based on whatever is faster at each occasion. A nice discussion that we find useful on this issue is from \"Prediction with Gaussian Processes: From Linear Regression to Linear Prediction and Beyond C. K. I. Williams Aston University, UK. In \"Learning and Inference in Graphical Models\", ed. M. I. Jordan, Kluwer, 1998.\"    \n   \n7. We have added mini-Imagenet in this plot and more discussion in the experiments. The message we wish to convey is that a purely memory-based system can be more data efficient than a gradient-based system, thus we prefer to keep only GP in this plot. GP+MAML can be better than GP, but at the same time since GP+MAML is also a gradient-based system its behaviour will depend on the learning rate and the inner loop size.   Fig4 in the Appendix shows how GP+MAML  compares with GP as a function of the inner loop size of GP+MAML on mini-Imagenet.  \n\nAlso to clarify 4) from \"Other points of feedback:\" Meta-training is done  in a completely end-to-end fashion maximising VIB over the neural representation $\\phi(x;\\theta)$ using automatic differentiation. Appendix C fully describes this. In particular, Appendix C.4 gives the full objective for few-shot classification.    "}, "signatures": ["ICLR.cc/2021/Conference/Paper2372/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper2372/Area_Chairs", "ICLR.cc/2021/Conference/Paper2372/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2372/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Meta Learning with Gaussian Processes", "authorids": ["~Michalis_Titsias1", "snikolou@aueb.gr", "~Alexandre_Galashov1"], "authors": ["Michalis Titsias", "Sotirios Nikoloutsopoulos", "Alexandre Galashov"], "keywords": ["Meta Learning", "Information Bottleneck", "Gaussian Processes", "Few-shot learning", "Variational Inference"], "abstract": "We formulate meta learning using information theoretic concepts such as mutual information and the information bottleneck. The idea is to learn a stochastic representation or encoding of the task description, given by a training or support set, that is highly informative about predicting the validation set. By making use of variational approximations to the mutual information, we derive a general and tractable framework for meta learning. We  particularly develop new memory-based meta learning algorithms based on Gaussian processes and derive extensions that combine memory and gradient-based meta learning. We demonstrate our method on few-shot regression and classification by using standard benchmarks such as Omniglot, mini-Imagenet and Augmented Omniglot.\n", "one-sentence_summary": "Meta learning using information bottleneck and combination of memory and gradient-based techniques using Gaussian processes. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "titsias|information_theoretic_meta_learning_with_gaussian_processes", "pdf": "/pdf/5ab73c2b33e07eaf54bb442b5f013e689025036b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Gc5Drbr45H", "_bibtex": "@misc{\ntitsias2021information,\ntitle={Information Theoretic Meta Learning with Gaussian Processes},\nauthor={Michalis Titsias and Sotirios Nikoloutsopoulos and Alexandre Galashov},\nyear={2021},\nurl={https://openreview.net/forum?id=0vO-u0sucRF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0vO-u0sucRF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2372/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2372/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2372/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2372/Authors|ICLR.cc/2021/Conference/Paper2372/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2372/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849160, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2372/-/Official_Comment"}}}, {"id": "18uoy0CjRU", "original": null, "number": 5, "cdate": 1605717914143, "ddate": null, "tcdate": 1605717914143, "tmdate": 1605729041164, "tddate": null, "forum": "0vO-u0sucRF", "replyto": "VHWtzAG0Q5x", "invitation": "ICLR.cc/2021/Conference/Paper2372/-/Official_Comment", "content": {"title": "response to the main points", "comment": "\"The paper seems to lack focus. It consists of two orthogonal components: (1) applying the information bottleneck to meta-learning and (2) a GP-based meta-learning algorithm. ...\"\n\nThe main objective of our paper is to derive  a general information bottleneck principle for meta learning. We believe this is a valid methodological contribution. Then to justify this as a valid principle we have fully applied it to GPs. We believe that information bottleneck allows to connect and unify memory-based and gradient-based meta learning under the concept of \"encoding in the bottleneck\", so that memory-based methods use parametric encodings while gradient-based methods use parametric encodings. We believe that the GP is an instance of the framework that helps to show this connection.    \n   \n\"The interpretation of MAML as a special case of information bottleneck (section 2.2) is not particularly novel: as the paper mentions in section 4, any previous works (LLAMA, probabilistic MAML, bayesian MAML, iMAML)....\"\n\nThanks for this comment. Yes we agree that there exists different ways to obtain MAML.  However, we do claim that the information bottleneck view of MAML, as explained in Section 2.2, is an interesting and simple way to view  MAML.  Notice also the information \nbottleneck provides a rather simple way to understand/interpret  transductive and non-transductive settings (an issue often causing confusion in the literature) as we discuss in Appendix B.\n\n\"The experimental results are mixed, and the paper doesn't give much of an interpretation of these results... \"\n\nWe have updated the experiments to add more results regarding the data efficiency aspect of GP method versus MAML. We also added Table 2 that shows the proposed GP method can give better calibrated probabilities as measured  by negative log-likelihood scores. From Fig2 our conclusion is that regarding data efficiency the GP shows a similar ability for both regression and classification problems.    \n\n\"Given the relation to other probabilistic extensions of MAML, perhaps comparing against at least one of them would have made experiments more informative.\"\n\nWe will try to reproduce and re-run one of these methods based on available code. We prefer not to quote results from other papers unless we are certain that the neural architecture (including how batch-norm was used etc) and experimental protocol  was exactly the same as Finn et al that is currently the basis for all comparisons.    \n\n\"The experiments about the efficiency regarding the amount of data (Figure 2 right) are promising, but this claim would be more significant if evaluated on multiple tasks.\"\n\nWe  added mini-Imagenet in this plot and more discussion in the experiments; see last paragraph in this section called \"Data efficiency in meta-testing of gradient-based vs GP-based meta learning\". A message we wish to convey is that a purely memory-based system can be more data efficient than a gradient-based system.  "}, "signatures": ["ICLR.cc/2021/Conference/Paper2372/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper2372/Area_Chairs", "ICLR.cc/2021/Conference/Paper2372/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2372/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Meta Learning with Gaussian Processes", "authorids": ["~Michalis_Titsias1", "snikolou@aueb.gr", "~Alexandre_Galashov1"], "authors": ["Michalis Titsias", "Sotirios Nikoloutsopoulos", "Alexandre Galashov"], "keywords": ["Meta Learning", "Information Bottleneck", "Gaussian Processes", "Few-shot learning", "Variational Inference"], "abstract": "We formulate meta learning using information theoretic concepts such as mutual information and the information bottleneck. The idea is to learn a stochastic representation or encoding of the task description, given by a training or support set, that is highly informative about predicting the validation set. By making use of variational approximations to the mutual information, we derive a general and tractable framework for meta learning. We  particularly develop new memory-based meta learning algorithms based on Gaussian processes and derive extensions that combine memory and gradient-based meta learning. We demonstrate our method on few-shot regression and classification by using standard benchmarks such as Omniglot, mini-Imagenet and Augmented Omniglot.\n", "one-sentence_summary": "Meta learning using information bottleneck and combination of memory and gradient-based techniques using Gaussian processes. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "titsias|information_theoretic_meta_learning_with_gaussian_processes", "pdf": "/pdf/5ab73c2b33e07eaf54bb442b5f013e689025036b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Gc5Drbr45H", "_bibtex": "@misc{\ntitsias2021information,\ntitle={Information Theoretic Meta Learning with Gaussian Processes},\nauthor={Michalis Titsias and Sotirios Nikoloutsopoulos and Alexandre Galashov},\nyear={2021},\nurl={https://openreview.net/forum?id=0vO-u0sucRF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0vO-u0sucRF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2372/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2372/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2372/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2372/Authors|ICLR.cc/2021/Conference/Paper2372/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2372/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849160, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2372/-/Official_Comment"}}}, {"id": "vikicQIIJAS", "original": null, "number": 6, "cdate": 1605722992327, "ddate": null, "tcdate": 1605722992327, "tmdate": 1605729007534, "tddate": null, "forum": "0vO-u0sucRF", "replyto": "4S-Ag-rNZl1", "invitation": "ICLR.cc/2021/Conference/Paper2372/-/Official_Comment", "content": {"title": "response to the main points", "comment": "\n\"For the objective function in Eq.1, is there a particular reason for assuming conditional independence..\"\n\nThe main motivation of $q(Z|D^t)$ is that at test time you will only be able to observe the training set $D^t$ and be asked to predict $D^v$. In general, the rationale behind  building the encoder $q(Z| \\cdot)$ is that you would like to condition only on things that you will observe at test time. For example, in supervised meta learning at test time you will observe a task labelled training set $D^t = (y^t, X^t)$ but also unlabelled validation/test inputs $X^v$, so in such case $q(Z | y^t, X^t, X^v)$.    \n\n\"For Eq.5 and Eq.6, are the models of  and  related in some ways? It seems that they are sharing the parameters\"\n\nWe have made this choice mainly for notational simplicity. Since both $p(Z)$ and $p(D^v |  Z)$ are part of the decoding/generative process, as opposed to the encoding  or recognition model $q(Z|D^t)$ that goes in reverse, they could naturally share some tunable parameters (although this is not necessary). Note that as we mention in footnote 1, all derivations are valid irrespectively of whether $\\theta$ and $w$ have shared components or not.      \n\n\"I did not find Section 3.2's motivation very convincing. The authors mentioned that the hope for combining MAML with GP is to reduce inner steps for MAML and possibly achieve better results. Yet, in Figure 4, it seems that pure GP is basically on-par with GP+MAML and outperforms GP+MAML when more inner steps are introduced for MAML. Could the authors help me with some insights for this observation?\"\n\nOur main motivation of GP+MAML is to be able to show that based on the information bottleneck we are able to combine  memory-based and gradient-based meta learning. The reason of the behaviour in Fig4 is that GP+MAML, due its MAML component, is optimised during meta-training to reconstruct well the validation set (so essentially to have good performance on the test data) after performing a fixed number of gradients steps for the feature vector $\\phi(x;\\theta)$ using the support data. So somehow the system is meta-trained to provide the best performance not with the initial $\\theta$ but with the $\\psi_i$ obtained from $\\theta$ after applying a **fixed** number of SGD steps. After this number of steps the performance can deteriorate, simply because the $\\psi_i$ can start overfitting due to the fact that  for this new task we have very few data (say only N=5,K=1 in mini-Imagenet) and we keep cycling with SGD updates over it.  Overfitting will not occur if for some reason for the new task we have much more data, so the performance of GP+MAML in such case could keep improving. We believe what we observe in Fig4 can be a typical behaviour of any MAML method.    \n\n\"It seems that SMAML performed worse than MAML consistently for regression tasks and occasionally better for classification tasks. I observe that SMAML differ from MAML in two aspects, .....GP is smooth and interpolates better.\"\n\nWe have included some additional experiments that show that our GP method can be significantly more data efficient than gradient-based methods such as MAML, with better calibrated probabilities. Note also that the information bottleneck plays a crucial role, especially via the value of $\\beta$, in the GP performance. In Fig5 in the Appendix we include an ablation study for $\\beta$ that shows that small values for $\\beta$ for the classification problems are preferred. Values of $\\beta$ close to $1$ (that will make training more like Bayesian variational inference) provide the worse performance.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2372/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper2372/Area_Chairs", "ICLR.cc/2021/Conference/Paper2372/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2372/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Meta Learning with Gaussian Processes", "authorids": ["~Michalis_Titsias1", "snikolou@aueb.gr", "~Alexandre_Galashov1"], "authors": ["Michalis Titsias", "Sotirios Nikoloutsopoulos", "Alexandre Galashov"], "keywords": ["Meta Learning", "Information Bottleneck", "Gaussian Processes", "Few-shot learning", "Variational Inference"], "abstract": "We formulate meta learning using information theoretic concepts such as mutual information and the information bottleneck. The idea is to learn a stochastic representation or encoding of the task description, given by a training or support set, that is highly informative about predicting the validation set. By making use of variational approximations to the mutual information, we derive a general and tractable framework for meta learning. We  particularly develop new memory-based meta learning algorithms based on Gaussian processes and derive extensions that combine memory and gradient-based meta learning. We demonstrate our method on few-shot regression and classification by using standard benchmarks such as Omniglot, mini-Imagenet and Augmented Omniglot.\n", "one-sentence_summary": "Meta learning using information bottleneck and combination of memory and gradient-based techniques using Gaussian processes. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "titsias|information_theoretic_meta_learning_with_gaussian_processes", "pdf": "/pdf/5ab73c2b33e07eaf54bb442b5f013e689025036b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Gc5Drbr45H", "_bibtex": "@misc{\ntitsias2021information,\ntitle={Information Theoretic Meta Learning with Gaussian Processes},\nauthor={Michalis Titsias and Sotirios Nikoloutsopoulos and Alexandre Galashov},\nyear={2021},\nurl={https://openreview.net/forum?id=0vO-u0sucRF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0vO-u0sucRF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2372/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2372/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2372/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2372/Authors|ICLR.cc/2021/Conference/Paper2372/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2372/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849160, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2372/-/Official_Comment"}}}, {"id": "CngCP8Y8nVi", "original": null, "number": 7, "cdate": 1605728155342, "ddate": null, "tcdate": 1605728155342, "tmdate": 1605728980946, "tddate": null, "forum": "0vO-u0sucRF", "replyto": "KNPt__knvzr", "invitation": "ICLR.cc/2021/Conference/Paper2372/-/Official_Comment", "content": {"title": "response to the main comments", "comment": "\"One of the things that it is missing is that the authors failed to mention that the connection between meta-learning and the information bottleneck principle had been previously explored in the literature (see [2]). While the setting is slightly different, it would make sense for the authors to address how the two papers are related, especially since the GP method proposed by the authors can be used in the transductive settings as well.\"\n\nThank you for this comment and for pointing to this previous work that we were not aware of.  We have added the following paragraph to the related work section that we believe it describes accurately the connection with this previous work:\n*A meta learning method that connects with the information bottleneck was recently proposed by Hu et al. (2020). There the information bottleneck was used to analyze the generalization of a variational Bayesian inference objective suitable for transductive supervised few-shot learning. Note that the information bottleneck derived  in Hu et al. (2020) (theorem 1) is not the same as the information bottleneck objective used here (i.e.\\ the objective in 7 for the supervised learning case), since they differ in the second term. In addition, our framework  expresses a general information bottleneck principle for meta learning applicable beyond transductive supervised learning.*   \n\n\"The weakest part of the paper is the experiments. The authors use MAML as the unique baseline ignoring more up to date meta-learning methods which undermines the message of the paper. It would be really interesting to see how the method generalises and compares against other state of the art methods. For example, there has been several methods that try to leverage uncertainty into MAML (e.g. [3]). This would be an interesting comparison since it would help to disentangle the impact of a parametric/nonparametric encoder versus probabilistic/deterministic encoders. In addition, there are several recent methods specifically developed for transductive meta-learning that would be interesting to include in the comparison (e.g. [4]). Finally, I would appreciate a more in depth analysis of why the combined approach works better or worse than the GP one in different settings (see some questions below).\"\n\nThen main reason we focus on MAML is that it is still very competitive in these few-shot learning and many other methods build on MAML.   \nWe will try to reproduce and re-run one of the probabilistic or Bayesian MAML methods based on available code. Note that while our framework can explain transductive settings, our current experiments do not use any specialised transductive procedures (as those in [2] or [4]). For instance, if you see the GP predictive posterior $q(f_{i,j}^v)$ (below equation 11) essentially implies a non- transductive setting, since this posterior depends on the union of the single validation input and the support set. The only transductive component of the method comes from the batch-normalisation which is part of the neural architecture of Finn et al (2017) that we use in all experiments without any modification. To build a more specialised transductive procedure we will need to significantly change the way we currently set the encoder (i.e. the way we amortise the GP; see Appendix C.1), and we feel this is beyond the scope of this paper.     \n  \nAlso note that in the currently revised version we have included  additional experiments that show that our GP method can be significantly more data efficient in meta-testing than gradient-based methods such as MAML, with better calibrated probabilities.  This is shown in Fig2 and described in the last paragraph in the experiments section.  \n\n\"In section 2.2 the authors cast MAML under their framework...\"\n\nWe agree this is relevant for stochastic MAML method where $\\beta>0$. We can clarify this. \n\n\"To build the stochastic extension of MAML, the authors use a stochastic encoder...... (there is an ablation study in D.3 but it is not specifically addressing this and it is not clear what is the model they use for the study). Also it would be interesting a small discussion about the importance of tying the variances of the prior and the encoder\" \n\nThe model we use for $\\beta$ ablation is just the GP, i.e. the only memory-based system. For GP+MAML the ablation study \nfor $\\beta$ shows a very similar picture with the GP. We haven't done any ablation study for Stochastic MAML but we plan to follow \nreviewer's suggestion to study the effect of tying the variances. as well the effect of $\\beta_\\psi =0$ vs $\\beta_\\psi>0$ in GP+MAML.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2372/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper2372/Area_Chairs", "ICLR.cc/2021/Conference/Paper2372/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2372/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Meta Learning with Gaussian Processes", "authorids": ["~Michalis_Titsias1", "snikolou@aueb.gr", "~Alexandre_Galashov1"], "authors": ["Michalis Titsias", "Sotirios Nikoloutsopoulos", "Alexandre Galashov"], "keywords": ["Meta Learning", "Information Bottleneck", "Gaussian Processes", "Few-shot learning", "Variational Inference"], "abstract": "We formulate meta learning using information theoretic concepts such as mutual information and the information bottleneck. The idea is to learn a stochastic representation or encoding of the task description, given by a training or support set, that is highly informative about predicting the validation set. By making use of variational approximations to the mutual information, we derive a general and tractable framework for meta learning. We  particularly develop new memory-based meta learning algorithms based on Gaussian processes and derive extensions that combine memory and gradient-based meta learning. We demonstrate our method on few-shot regression and classification by using standard benchmarks such as Omniglot, mini-Imagenet and Augmented Omniglot.\n", "one-sentence_summary": "Meta learning using information bottleneck and combination of memory and gradient-based techniques using Gaussian processes. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "titsias|information_theoretic_meta_learning_with_gaussian_processes", "pdf": "/pdf/5ab73c2b33e07eaf54bb442b5f013e689025036b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Gc5Drbr45H", "_bibtex": "@misc{\ntitsias2021information,\ntitle={Information Theoretic Meta Learning with Gaussian Processes},\nauthor={Michalis Titsias and Sotirios Nikoloutsopoulos and Alexandre Galashov},\nyear={2021},\nurl={https://openreview.net/forum?id=0vO-u0sucRF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "0vO-u0sucRF", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2372/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2372/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2372/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2372/Authors|ICLR.cc/2021/Conference/Paper2372/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2372/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923849160, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2372/-/Official_Comment"}}}, {"id": "KNPt__knvzr", "original": null, "number": 1, "cdate": 1603737201172, "ddate": null, "tcdate": 1603737201172, "tmdate": 1605024226467, "tddate": null, "forum": "0vO-u0sucRF", "replyto": "0vO-u0sucRF", "invitation": "ICLR.cc/2021/Conference/Paper2372/-/Official_Review", "content": {"title": "Review of Information Theoretic Meta Learning with Gaussian Processes:  Interesting idea. Experimental part is weak.", "review": "## Summary\n\nThe paper derives a meta-learning framework based on the information bottleneck principle. By adapting the variational approximation proposed in [1] to the meta-learning setting, the authors come up with a tractable objective that generalises both gradient based and memory meta-learning methods. Based on this framework, the authors proposed a new memory based meta-learning algorithm by using a GP with a deep kernel and an extension that combines this memory based method with MAML. The authors show that the method outperforms MAML in several standard meta-learning  tasks, especially in regression and many shots classification problems.\n\n## Comments\n\nThe motivation of the paper is really clear and the paper is extremely well-written. \n\nFrom the technical perspective, the paper has two main contributions:\n\n1. The development of a general meta-learning framework based on the information bottleneck that generalizes gradient and memory based meta-Learning\n2. Two new methods that resort to GPs with deep kernels to implement the encoder in the aforementioned framework\n\nRegarding the framework, the extension of the variational information bottleneck to the meta-learning setting is not particularly challenging being a straightforward extension of [1]. However, it is an interesting observation that this extension subsumes gradient based methods (by using parametric encodings) and memory based ones (by using nonparametric encodings). One of the things that it is missing is that the authors failed to mention that the connection between meta-learning and the information bottleneck principle had been previously explored in the literature (see [2]). While the setting is slightly different, it would make sense for the authors to address how the two papers are related, especially since the GP method proposed by the authors can be used in the transductive settings as well. \n\nRegarding the new methods, I think that the use of a GP encoder with a deep kernel is interesting but again not particularly challenging from the technical perspective. However, I think the combination of MAML + GP it is a very interesting observation that can open new research avenues.\n\nThe weakest part of the paper is the experiments. The authors use MAML as the unique baseline ignoring more up to date meta-learning methods which undermines the message of the paper. It would be really interesting to see how the method generalises and compares against other state of the art methods. For example, there has been several methods that try to leverage uncertainty into MAML (e.g. [3]). This would be an interesting comparison since it would help to disentangle the impact of a parametric/nonparametric encoder versus probabilistic/deterministic encoders. In addition, there are several recent methods specifically developed for transductive meta-learning that would be interesting to include in the comparison (e.g. [4]). Finally, I would appreciate a more in depth analysis of why the combined approach works better or worse than the GP one in different settings (see some questions below).\n\n## Questions/Minors\n\n* In section 2.2 the authors cast MAML under their framework. Two do so, they assume Z_i=\\psi_i leading to p_\\theta(\\mathcal{D}_i^v|\\psi_i) and p_\\theta(\\psi_i). However, the second approximation is irrelevant since further on they assume \\beta=0.\n* To build the stochastic extension of MAML, the authors use a stochastic encoder. However, in addition they use \\beta \\neq 0. It would be nice to see an ablation study to see the contribution of these separate extensions (there is an ablation study in D.3 but it is not specifically addressing this and it is not clear what is the model they use for the study). Also it would be interesting a small discussion about the importance of tying the variances of the prior and the encoder\n* In section 3.1 it should be mention that it is a degenerate GP.\n* Toward the end of section 3.2 a bunch of simplifications are done in order to come up with the final version of the combined algorithm: \\beta to \\beta_f and \\beta_\\psi and \\beta_psi=0 which leads to a deterministic encoder and remove the need of defining a prior. However, these simplifications goes in the opposite direction of the extensions made to come up with the stochastic version of MAML. It would be great if the authors could elaborate on why that is the case and the contribution to the final performance of each of this simplifications.\n\n## References\n\n[1] Deep variational information bottleneck\n[2] Empirical Bayes Transductive Meta-Learning with Synthetic Gradients\n[3] Recasting Gradient-Based Meta-Learning as Hierarchical Bayes\n[4] Learning to Propagate Labels: Transductive Propagation Network for Few-shot Learning\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2372/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2372/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Meta Learning with Gaussian Processes", "authorids": ["~Michalis_Titsias1", "snikolou@aueb.gr", "~Alexandre_Galashov1"], "authors": ["Michalis Titsias", "Sotirios Nikoloutsopoulos", "Alexandre Galashov"], "keywords": ["Meta Learning", "Information Bottleneck", "Gaussian Processes", "Few-shot learning", "Variational Inference"], "abstract": "We formulate meta learning using information theoretic concepts such as mutual information and the information bottleneck. The idea is to learn a stochastic representation or encoding of the task description, given by a training or support set, that is highly informative about predicting the validation set. By making use of variational approximations to the mutual information, we derive a general and tractable framework for meta learning. We  particularly develop new memory-based meta learning algorithms based on Gaussian processes and derive extensions that combine memory and gradient-based meta learning. We demonstrate our method on few-shot regression and classification by using standard benchmarks such as Omniglot, mini-Imagenet and Augmented Omniglot.\n", "one-sentence_summary": "Meta learning using information bottleneck and combination of memory and gradient-based techniques using Gaussian processes. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "titsias|information_theoretic_meta_learning_with_gaussian_processes", "pdf": "/pdf/5ab73c2b33e07eaf54bb442b5f013e689025036b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Gc5Drbr45H", "_bibtex": "@misc{\ntitsias2021information,\ntitle={Information Theoretic Meta Learning with Gaussian Processes},\nauthor={Michalis Titsias and Sotirios Nikoloutsopoulos and Alexandre Galashov},\nyear={2021},\nurl={https://openreview.net/forum?id=0vO-u0sucRF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0vO-u0sucRF", "replyto": "0vO-u0sucRF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2372/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097869, "tmdate": 1606915795738, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2372/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2372/-/Official_Review"}}}, {"id": "VHWtzAG0Q5x", "original": null, "number": 3, "cdate": 1603868865161, "ddate": null, "tcdate": 1603868865161, "tmdate": 1605024226345, "tddate": null, "forum": "0vO-u0sucRF", "replyto": "0vO-u0sucRF", "invitation": "ICLR.cc/2021/Conference/Paper2372/-/Official_Review", "content": {"title": "Review", "review": "This paper formulates meta-learning using the information bottleneck. The method develops a meta-learning algorithm based on Gaussian Processes, which they also interpret as a memory-based algorithm. They further describe an extension that combines the GP-based method and MAML.\n\nThe paper seems to lack focus. It consists of two orthogonal components: (1) applying the information bottleneck to meta-learning and (2) a GP-based meta-learning algorithm. Each of these components is a separate meta-learning algorithm: (1) alone can extend MAML to \"stochastic MAML\" ($\\beta > 0$) and (2) alone is \"GP\" with hyperparameter $\\beta=0$. The paper does not make a sufficient case for why these two components should be used in tandem (e.g. GP w/ $\\beta > 0$), nor does it perform ablation experiments that each positively contributes to some measure of meta-learning performance.\n\nThe interpretation of MAML as a special case of information bottleneck (section 2.2) is not particularly novel: as the paper mentions in section 4, any previous works (LLAMA, probabilistic MAML, bayesian MAML, iMAML) have built on the interpretation of the initial parameter of MAML as encoding a prior distribution over parameters, and the gradient step as posterior inference. The fact that information bottleneck with $\\beta=0$ reduces to standard learning is well-known.\n\nThe experimental results are mixed, and the paper doesn't give much of an interpretation of these results. GP and GP+MAML outperform MAML by an order of magnitude on the sinusoid regression problem but is outperformed by MAML in two of the five classification tasks considered. The experiments in the appendix are similarly mixed. GP wins on Augmented Omniglot but loses on ImageNet variants. Is the GP approach vastly superior on only regression problems, or is the difference just due to tuning and the fact that sinusoid regression is a toy problem? \n\nGiven the relation to other probabilistic extensions of MAML, perhaps comparing against at least one of them would have made experiments more informative.\n\nThe experiments about the efficiency regarding the amount of data (Figure 2 right) are promising, but this claim would be more significant if evaluated on multiple tasks.\n\nminor\n\nThere is a link formatting issue btw pages 2 and 3", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2372/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2372/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Meta Learning with Gaussian Processes", "authorids": ["~Michalis_Titsias1", "snikolou@aueb.gr", "~Alexandre_Galashov1"], "authors": ["Michalis Titsias", "Sotirios Nikoloutsopoulos", "Alexandre Galashov"], "keywords": ["Meta Learning", "Information Bottleneck", "Gaussian Processes", "Few-shot learning", "Variational Inference"], "abstract": "We formulate meta learning using information theoretic concepts such as mutual information and the information bottleneck. The idea is to learn a stochastic representation or encoding of the task description, given by a training or support set, that is highly informative about predicting the validation set. By making use of variational approximations to the mutual information, we derive a general and tractable framework for meta learning. We  particularly develop new memory-based meta learning algorithms based on Gaussian processes and derive extensions that combine memory and gradient-based meta learning. We demonstrate our method on few-shot regression and classification by using standard benchmarks such as Omniglot, mini-Imagenet and Augmented Omniglot.\n", "one-sentence_summary": "Meta learning using information bottleneck and combination of memory and gradient-based techniques using Gaussian processes. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "titsias|information_theoretic_meta_learning_with_gaussian_processes", "pdf": "/pdf/5ab73c2b33e07eaf54bb442b5f013e689025036b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Gc5Drbr45H", "_bibtex": "@misc{\ntitsias2021information,\ntitle={Information Theoretic Meta Learning with Gaussian Processes},\nauthor={Michalis Titsias and Sotirios Nikoloutsopoulos and Alexandre Galashov},\nyear={2021},\nurl={https://openreview.net/forum?id=0vO-u0sucRF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0vO-u0sucRF", "replyto": "0vO-u0sucRF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2372/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097869, "tmdate": 1606915795738, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2372/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2372/-/Official_Review"}}}, {"id": "Pqf44PumjU1", "original": null, "number": 4, "cdate": 1604280204874, "ddate": null, "tcdate": 1604280204874, "tmdate": 1605024226279, "tddate": null, "forum": "0vO-u0sucRF", "replyto": "0vO-u0sucRF", "invitation": "ICLR.cc/2021/Conference/Paper2372/-/Official_Review", "content": {"title": "Interesting but not sure of advantages of proposed method vs. earlier methods", "review": "The paper presents a method for Bayesian meta-learning. This method combines a NN feature extractor with a Gaussian Process on top. The GP kernel is linear. The information bottleneck is used to motivate a choice of approximate posterior. Using MAML to adapt the NN feature extractor weights improves performance of the composite model for few-shot learning.\n\n---\nThings I liked:\n- This paper was interesting and on an important topic. \n- The information bottleneck seems some to fix some issues with variational inference takes on Bayesian meta learning (e.g. Amortized Bayesian Meta-Learning, Ravi & Beatson, 2019). The VI approach in that paper doesn't distinguish train/support and validation/query sets, and doesn't motivate the weight beta which is placed on the prior, while the information bottleneck does.\n- The particular information bottleneck motivated posterior used with the GP is not specific to classification, regression, or other particular structure of prediction problem.\n- I liked that the paper had plots of performance vs number of data points, as opposed to only reporting error on the full set of training tasks\n- The proposed method leads to a bump in accuracy over MAML for augmented omniglot\n\n---\nThere are potential areas of concern which I would like the authors to respond to. If the concerns are sufficiently addressed (or turn out to be due to my misunderstanding), I could be willing to significantly increase my score.\n\nMain concerns:\n1. What is the objective of the proposed method? Is it to get better test error than MAML, or to quantify uncertainty, or something else? If the point is to get better test error, it seems unfair to compare only to vanilla MAML given the considerable amount of literature and number of improvements in the years since. To build a case for an advantage in terms of accuracy, the paper should compare to a wider range of stronger/more recent baselines (e.g. \"Bayesian Few-Shot Classification with One-vs-Each\nP\u00f3lya-Gamma Augmented Gaussian Processes\", Snell & Zemel 2020, does a good job of this).\n2. If the motivation for the current proposed method is more than just improving the test error on these benchmarks, this motivation should be clearly explained and/or measured. The method proposed in the paper adds a lot of complexity on top of MAML and variants, so I think the paper needs to build a strong case and clarify exactly which settings this method is preferable for.\n3.  One paper the authors could compare to and cite is \"Meta-learning with differentiable closed-form solvers\", Bertinetto et al 2018, which -- as with the current paper -- combines NN features with a closed-form linear model, and which significantly outperforms MAML.\n4. The authors could also compare to the other papers they cite in Section 4, e.g. Snell & Zemel 2020 significantly outperform all the baselines and the proposed method from the current paper's results.\n5. Can the authors discuss the lack of data augmentation used for Augmented Omniglot for the GP methods? It strikes me that the data augmentation is likely part of what makes Augmented Omniglot hard. In this case, removing the data augmentation for the GP methods would be giving the GP methods an unfair advantage, which could explain almost all of the GP methods' superiority on this task. A fair comparison should also compare to MAML(+variants) without data aug, and/or GP methods with data aug\n6. Can the authors comment on the choice of a linear kernel for the GP? This surprised me, as for regression using a GP with a linear kernel is the same thing as doing Bayesian linear regression. Bayesian linear regression can be done exactly in O(N) time instead of O(N^3) for linear-kernel GP, so it seems strange not to use the LR formulation. It also seems surprising to have to jump through hoops to use an information bottleneck-motivated posterior in a case where we could do exact Bayesian inference cheaply. Of course, in a classification setting one needs to use approximations like Polya-Gamma augmentation, variational inference, or similar.\n7. Why only show accuracy vs data points (fig 2 right) for GP not GP-MAML, and why only for augmented omniglot? This is an interesting plot which I would like to see more of in few-shot learning papers.\n\n---\nOther points of feedback:\n1. I think that whenever the GP method is referred to (title, abstract, text, results tables, plots...), it should be made clear that this is a composition of a neural network feature extractor and a GP. Yes, the NN feature extractor can be seen as part of a very particular choice of kernel for the GP, and this is explained in Section 3. However, someone skimming the paper might not realize the existence of this NN at all, which would make the results very surprising. I think the paper should endeavor to give readers the correct impression on a skim. \n2. The set up in Section 2 (and \"Stochastic MAML\") is extremely similar to Amortized Bayesian Meta-Learning, Ravi & Beatson, 2019. As mentioned, the current submission fixes some issues with that paper, but due the similarity it wouldn't hurt to cite them.\n3. The paper should explain the Augmented Omniglot benchmark better in the text (or at least refer the reader to the appendix, where it is explained). Flennerhag et al who introduced the benchmark did not use the term \"Augmented Omniglot\", so simply citing that paper when the term is mentioned is insufficient and left me confused before I found the discussion in the appendix, although I do think this is a good name for the benchmark.\n4. How was the NN+GP method trained? I imagine this was done by backpropogating through the GP fitting procedure to get the gradient of the query-set error for training tasks (or of the IB objective for training tasks) and using this to optimize the NN weights. It would be nice to have this spelled out, as well as how this gradient was computed (whether it is simple or required some non-trivial tricks).", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2372/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2372/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Meta Learning with Gaussian Processes", "authorids": ["~Michalis_Titsias1", "snikolou@aueb.gr", "~Alexandre_Galashov1"], "authors": ["Michalis Titsias", "Sotirios Nikoloutsopoulos", "Alexandre Galashov"], "keywords": ["Meta Learning", "Information Bottleneck", "Gaussian Processes", "Few-shot learning", "Variational Inference"], "abstract": "We formulate meta learning using information theoretic concepts such as mutual information and the information bottleneck. The idea is to learn a stochastic representation or encoding of the task description, given by a training or support set, that is highly informative about predicting the validation set. By making use of variational approximations to the mutual information, we derive a general and tractable framework for meta learning. We  particularly develop new memory-based meta learning algorithms based on Gaussian processes and derive extensions that combine memory and gradient-based meta learning. We demonstrate our method on few-shot regression and classification by using standard benchmarks such as Omniglot, mini-Imagenet and Augmented Omniglot.\n", "one-sentence_summary": "Meta learning using information bottleneck and combination of memory and gradient-based techniques using Gaussian processes. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "titsias|information_theoretic_meta_learning_with_gaussian_processes", "pdf": "/pdf/5ab73c2b33e07eaf54bb442b5f013e689025036b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Gc5Drbr45H", "_bibtex": "@misc{\ntitsias2021information,\ntitle={Information Theoretic Meta Learning with Gaussian Processes},\nauthor={Michalis Titsias and Sotirios Nikoloutsopoulos and Alexandre Galashov},\nyear={2021},\nurl={https://openreview.net/forum?id=0vO-u0sucRF}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "0vO-u0sucRF", "replyto": "0vO-u0sucRF", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2372/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097869, "tmdate": 1606915795738, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2372/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2372/-/Official_Review"}}}], "count": 10}