{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458272022247, "tcdate": 1458272022247, "id": "lx9KN5ozRt2OVPy8CvgD", "invitation": "ICLR.cc/2016/workshop/-/paper/175/review/11", "forum": "ROVmA279BsvnM0J1IpNn", "replyto": "ROVmA279BsvnM0J1IpNn", "signatures": ["ICLR.cc/2016/workshop/paper/175/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/175/reviewer/11"], "content": {"title": "The paper proses a novel variant to the standard neural networks which can potentially be used for fast inference in structured prediction settings. However its usefulness is questionable. ", "rating": "5: Marginally below acceptance threshold", "review": "Comment: Summary:\nThe paper presents a novel variant to the standard neural network architecture. Under certain constrains the proposed architecture is convex in their input space. This convexity property facilitates fast inference over a subset of input variables making them highly suitable for structured prediction problems. The authors report rather simplistic experiments to back their claim. \n\nNovelty: The ideas proposed in the paper are fairly novel and very well motivated. \nClarity: The paper is very well written and easy to read. \nSignificance: While the ideas proposed in the paper sound quite impactful especially in the structured prediction setting, I have some serious reservations with respect to their actual utility. For starters, the constraints specified with the model are rather too restrictive. There are a large number of non-linear activation and pooling functions which one will not be able to use. In addition the non-negativity constraint on the parameters is even more restrictive. As a result the usefulness of the proposed model is not very convincing. \nQuality: While the paper is well written, the experimental section is extremely weak: almost non-existent. It is a bit strange that the authors motivate their proposed model by listing its extreme usefulness for structure prediction problems. However they validate their claim on two rather simplistic dataset: a toy dataset, and mnist. I wonder why. \n\nPros: The paper presents a novel model which could potentially be used for fast structure prediction using deep networks. The paper is very well written and easy to read. \nCons: While the model is interesting, it has some rather significant drawbacks. The constraints are quite limiting to make the model of any use for a real problem. In addition the experimental section of the paper is almost non-existent: the authors train and test their model on a synthetic data set and an mnist dataset. First, the authors do not compare their model against any other baseline. Second, the model was motivated to be useful for structure prediction problems, however it is tested on something completely different. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Input-Convex Deep Networks", "abstract": "This paper introduces a new class of neural networks that we\nrefer to as input-convex neural networks, networks that are convex in\ntheir inputs (as opposed to their parameters).  We discuss the nature and\nrepresentational power of these networks, illustrate how the prediction\n(inference) problem can be solved via convex optimization, and discuss their\napplication to structured prediction problems.  We highlight a few simple\nexamples of these networks applied to classification tasks, where we illustrate\nthat the networks perform substantially better than any other approximator we\nare aware of that is convex in its inputs.", "pdf": "/pdf/ROVmA279BsvnM0J1IpNn.pdf", "paperhash": "amos|inputconvex_deep_networks", "conflicts": ["cmu.edu"], "authors": ["Brandon Amos", "J. Zico Kolter"], "authorids": ["bamos@cs.cmu.edu", "zkolter@cs.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580084431, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580084431, "id": "ICLR.cc/2016/workshop/-/paper/175/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "ROVmA279BsvnM0J1IpNn", "replyto": "ROVmA279BsvnM0J1IpNn", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/175/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458271769747, "tcdate": 1458271769747, "id": "k80JNQvPPCOYKX7ji4Ny", "invitation": "ICLR.cc/2016/workshop/-/paper/175/comment", "forum": "ROVmA279BsvnM0J1IpNn", "replyto": "ZY9AEEoqYs5Pk8ELfEy0", "signatures": ["~Sumit_Chopra1"], "readers": ["everyone"], "writers": ["~Sumit_Chopra1"], "content": {"title": "Interesting variant of deep networks for potential use in structure prediction problems. ", "comment": "The paper proposes a novel neural network model which can be potentially used for structure prediction problems. The convexity property over its inputs leads to fast inference during test time. However the constraints the model needs to satisfy seem too restrictive. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Input-Convex Deep Networks", "abstract": "This paper introduces a new class of neural networks that we\nrefer to as input-convex neural networks, networks that are convex in\ntheir inputs (as opposed to their parameters).  We discuss the nature and\nrepresentational power of these networks, illustrate how the prediction\n(inference) problem can be solved via convex optimization, and discuss their\napplication to structured prediction problems.  We highlight a few simple\nexamples of these networks applied to classification tasks, where we illustrate\nthat the networks perform substantially better than any other approximator we\nare aware of that is convex in its inputs.", "pdf": "/pdf/ROVmA279BsvnM0J1IpNn.pdf", "paperhash": "amos|inputconvex_deep_networks", "conflicts": ["cmu.edu"], "authors": ["Brandon Amos", "J. Zico Kolter"], "authorids": ["bamos@cs.cmu.edu", "zkolter@cs.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455831659154, "ddate": null, "super": null, "final": null, "tcdate": 1455831659154, "id": "ICLR.cc/2016/workshop/-/paper/175/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "ROVmA279BsvnM0J1IpNn", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/175/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457625460338, "tcdate": 1457625460338, "id": "ZY9AEEoqYs5Pk8ELfEy0", "invitation": "ICLR.cc/2016/workshop/-/paper/175/review/10", "forum": "ROVmA279BsvnM0J1IpNn", "replyto": "ROVmA279BsvnM0J1IpNn", "signatures": ["ICLR.cc/2016/workshop/paper/175/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/175/reviewer/10"], "content": {"title": "Input-convex neural networks are proposed but could be investigated more carefully to provide additional insights", "rating": "5: Marginally below acceptance threshold", "review": "The submission investigates an interesting variant of neural networks, referred to as `input-convex.' Hereby, the composite function of a standard neural network, or more generally a neural network for structured output spaces, is restricted to be convex in the output space, i.e., the variable of interest for prediction, and optionally the input space. This translates into non-negativity constraints on some trainable parameters, as well as a convexity and non-decreasing assumption on the employed activation functions.\n\nSummary:\n--------\nClarity: The paper is well written and the idea is easy to follow.\nQuality: The idea is generally well demonstrated, but some experiments are missing in order to judge the efficacy of the proposed modifications, e.g., providing inference and training time on MNIST as well as adding some more baselines.\nOriginality: The investigated variant is new but related to recent work which should be reviewed more adequately. See comments below for details.\nSignificance: Due to some missing important experiments (timing), it's hard to judge the significance of this work at the moment.\n\nPros: Convexity in the output space recovers some guarantees for inference.\nCons: Need for guarantees during inference hasn't been demonstrated and time for both inference and learning might be prohibitively expensive at the moment.\n\nComments:\n---------\n1. Why did the authors choose the name `input-convex' if convexity in the output space is the most desirable property? I think the title might be slightly confusing.\n\n2. Using the rectified linear unit as the activation function allows to rephrase inference as a large linear program. Did the authors investigate non-linear activation functions where inference amounts to solving constrained optimization problems?\n\n3. The non-negativity constraint on the parameters \\theta is missing in Eq. 5.\n\n4. For a reader it is desirable to get to know the difference in training and inference time between standard neural networks and the proposed `input-convex networks.' Hence, providing error over time in addition to Fig. 3 as well as a small table containing inference times seems worthwhile. I suspect inference and hence training to be time consuming for larger models, but an investigation is missing at the moment.\n\n5. Admittedly, 4 pages are rather constraining, but I think there is significant amount of very related work that should therefore be mentioned. E.g., work by D. Belanger and A. McCallum, `Structured Prediction Energy Networks,' and also recent work combining structured prediction with deep learning, e.g., by M. Jaderberg et al. (Deep Structured Output Learning for Unconstrained Text Recognition), S. Zheng et al. (Conditional Random Fields as Recurrent Neural Networks), L.-C. Chen et al. (Learning Deep Structured Models), A. Schwing and R. Urtasun (Fully Connected Deep Structured Networks) and references therein.\n\nMinor comments:\n---------------\n- Aside from one note, the output space (\\cal Y) is never defined formally. It might be worthwhile to at least specify it explicitly for the experiments.\n- I wasn't able to open the document in Acrobat. The authors may want to check.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Input-Convex Deep Networks", "abstract": "This paper introduces a new class of neural networks that we\nrefer to as input-convex neural networks, networks that are convex in\ntheir inputs (as opposed to their parameters).  We discuss the nature and\nrepresentational power of these networks, illustrate how the prediction\n(inference) problem can be solved via convex optimization, and discuss their\napplication to structured prediction problems.  We highlight a few simple\nexamples of these networks applied to classification tasks, where we illustrate\nthat the networks perform substantially better than any other approximator we\nare aware of that is convex in its inputs.", "pdf": "/pdf/ROVmA279BsvnM0J1IpNn.pdf", "paperhash": "amos|inputconvex_deep_networks", "conflicts": ["cmu.edu"], "authors": ["Brandon Amos", "J. Zico Kolter"], "authorids": ["bamos@cs.cmu.edu", "zkolter@cs.cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580085150, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580085150, "id": "ICLR.cc/2016/workshop/-/paper/175/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "ROVmA279BsvnM0J1IpNn", "replyto": "ROVmA279BsvnM0J1IpNn", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/175/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455831657208, "tcdate": 1455831657208, "id": "ROVmA279BsvnM0J1IpNn", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "ROVmA279BsvnM0J1IpNn", "signatures": ["~Brandon_Amos1"], "readers": ["everyone"], "writers": ["~Brandon_Amos1"], "content": {"CMT_id": "", "title": "Input-Convex Deep Networks", "abstract": "This paper introduces a new class of neural networks that we\nrefer to as input-convex neural networks, networks that are convex in\ntheir inputs (as opposed to their parameters).  We discuss the nature and\nrepresentational power of these networks, illustrate how the prediction\n(inference) problem can be solved via convex optimization, and discuss their\napplication to structured prediction problems.  We highlight a few simple\nexamples of these networks applied to classification tasks, where we illustrate\nthat the networks perform substantially better than any other approximator we\nare aware of that is convex in its inputs.", "pdf": "/pdf/ROVmA279BsvnM0J1IpNn.pdf", "paperhash": "amos|inputconvex_deep_networks", "conflicts": ["cmu.edu"], "authors": ["Brandon Amos", "J. Zico Kolter"], "authorids": ["bamos@cs.cmu.edu", "zkolter@cs.cmu.edu"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 4}