{"notes": [{"id": "Z4YatHL7aq", "original": "NZ7CnL5Gr70", "number": 221, "cdate": 1601308033228, "ddate": null, "tcdate": 1601308033228, "tmdate": 1614985620696, "tddate": null, "forum": "Z4YatHL7aq", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Semantically-Adaptive Upsampling for Layout-to-Image Translation", "authorids": ["~Hao_Tang6", "~Nicu_Sebe1"], "authors": ["Hao Tang", "Nicu Sebe"], "keywords": ["Feature upsampling", "semantically-adaptive", "layout-to-image translation"], "abstract": "We propose the Semantically-Adaptive UpSampling (SA-UpSample), a general and highly effective upsampling method for the layout-to-image translation task. SA-UpSample has three advantages: 1) Global view. Unlike traditional upsampling methods (e.g., Nearest-neighbor) that only exploit local neighborhoods, SA-UpSample can aggregate semantic information in a global view. \n2) Semantically adaptive.  Instead of using a fixed kernel for all locations (e.g., Deconvolution), SA-UpSample enables semantic class-specific upsampling via generating adaptive kernels for different locations. 3) Efficient. Unlike Spatial Attention which uses a fully-connected strategy to connect all the pixels, SA-UpSample only considers the most relevant pixels, introducing little computational overhead. We observe that SA-UpSample achieves consistent and substantial gains on six popular datasets. The source code will be made publicly available.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tang|semanticallyadaptive_upsampling_for_layouttoimage_translation", "one-sentence_summary": "A novel feature upsampling method for layout-to-image translation method.", "supplementary_material": "/attachment/7ac9ab83310792277773d86772b2ec23090cde85.zip", "pdf": "/pdf/2f477861ed02c3ae6a9efbbbf6072f587693e92a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7aXgBtdDr", "_bibtex": "@misc{\ntang2021semanticallyadaptive,\ntitle={Semantically-Adaptive Upsampling for Layout-to-Image Translation},\nauthor={Hao Tang and Nicu Sebe},\nyear={2021},\nurl={https://openreview.net/forum?id=Z4YatHL7aq}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "woR_yDUm7SR", "original": null, "number": 1, "cdate": 1610040538555, "ddate": null, "tcdate": 1610040538555, "tmdate": 1610474148776, "tddate": null, "forum": "Z4YatHL7aq", "replyto": "Z4YatHL7aq", "invitation": "ICLR.cc/2021/Conference/Paper221/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The paper proposes an upsampling layer design for converting layouts to images. Three reviewers rate the paper below the bar, while one reviewer rates the paper marginally above the bar. The main concern that several reviewers raise is the novelty. Particularly, R1 and R3 point out that the proposed method shares great similarity to CARAFE [Wang et al. ICCV 2019]. The AC agrees with the reviewers.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantically-Adaptive Upsampling for Layout-to-Image Translation", "authorids": ["~Hao_Tang6", "~Nicu_Sebe1"], "authors": ["Hao Tang", "Nicu Sebe"], "keywords": ["Feature upsampling", "semantically-adaptive", "layout-to-image translation"], "abstract": "We propose the Semantically-Adaptive UpSampling (SA-UpSample), a general and highly effective upsampling method for the layout-to-image translation task. SA-UpSample has three advantages: 1) Global view. Unlike traditional upsampling methods (e.g., Nearest-neighbor) that only exploit local neighborhoods, SA-UpSample can aggregate semantic information in a global view. \n2) Semantically adaptive.  Instead of using a fixed kernel for all locations (e.g., Deconvolution), SA-UpSample enables semantic class-specific upsampling via generating adaptive kernels for different locations. 3) Efficient. Unlike Spatial Attention which uses a fully-connected strategy to connect all the pixels, SA-UpSample only considers the most relevant pixels, introducing little computational overhead. We observe that SA-UpSample achieves consistent and substantial gains on six popular datasets. The source code will be made publicly available.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tang|semanticallyadaptive_upsampling_for_layouttoimage_translation", "one-sentence_summary": "A novel feature upsampling method for layout-to-image translation method.", "supplementary_material": "/attachment/7ac9ab83310792277773d86772b2ec23090cde85.zip", "pdf": "/pdf/2f477861ed02c3ae6a9efbbbf6072f587693e92a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7aXgBtdDr", "_bibtex": "@misc{\ntang2021semanticallyadaptive,\ntitle={Semantically-Adaptive Upsampling for Layout-to-Image Translation},\nauthor={Hao Tang and Nicu Sebe},\nyear={2021},\nurl={https://openreview.net/forum?id=Z4YatHL7aq}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Z4YatHL7aq", "replyto": "Z4YatHL7aq", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040538542, "tmdate": 1610474148759, "id": "ICLR.cc/2021/Conference/Paper221/-/Decision"}}}, {"id": "0aRleGLPEHb", "original": null, "number": 4, "cdate": 1604033438422, "ddate": null, "tcdate": 1604033438422, "tmdate": 1606975989118, "tddate": null, "forum": "Z4YatHL7aq", "replyto": "Z4YatHL7aq", "invitation": "ICLR.cc/2021/Conference/Paper221/-/Official_Review", "content": {"title": "Incremental novelty and limited performance improvement", "review": "##########################################################################\n\nSummary:\n\nThis paper proposes a semantically-adaptive upsampling approach for layout-to-image translation. It uses the semantic label map to predict spatially-adaptive upsampling kernels for feature map upsampling.  Compared with traditional upsampling methods, it has a larger receptive field to focus on not only nearby pixels, but also semantically-related pixels at a longer distance. Experiments are conducted on Cityscapes, ADE20K, COCO-Stuff, DeepFashion, CelebAMask-HQ, and Facades datasets, and the proposed approach achieves better results compared with the baseline.\n\n##########################################################################\n\nPros: \n\n1. The semantically-adaptive upsampling approach considers the semantic information for upsampling. It has a larger receptive field and can take far-away pixels which are semantically related for upsampling, so that it can better preserve the semantic consistency within the instance or stuff with the same semantic label.\n\n2. The proposed semantically-adaptive upsampling is efficient compared with spatial attention or prediction convolution kernels.\n\n3. The writting and explanations are clear.\n \n##########################################################################\n\nCons: \n\n1. In Figure.2, both the SAFU branch and the SAKG branch take the feature f as input. Based on my understanding of this paper, the input of the SAKG branch should be the semantic layout map, rather than the feature map f. I think this figure is a little confusing and misleading.\n\n2. The novelty of the semantically-adaptive upsampling is limited. On the one hand, semantic-adaptive operations have been proposed in previous work SPADE (semantically-adaptive normalization) and CC-FPSE (semantically-adaptive convolution). The difference from the conditional convolution in CC-FPSE is that the semantically-adaptive upsampling does not learn the feature transformation across channels, so the parameter size to be predicted is smaller. On the other hand, content-adaptive upsampling operations have also been explored in previous work CARAFE [A]. Especially the architecture design of this paper is almost the same as CARAFE (even with the same submodule names such as \"channel compression\" and \"channel-wise normalization\"). The only difference from CARAFE is that the proposed method uses the semantic label maps rather than the feature maps to predict the upsampling kernels.\n\n[A] CARAFE: Content-Aware ReAssembly of FEatures, ICCV 2019\n\n3. The authors claim that the proposed upsampling approach can aggregate information in a global view, but the receptive field of the upsampling layer is still limited to the kernel size of the upsampling kernels. The kernel size used in this paper is k=5. Have the authors experiment using different kernel sizes to see the effect of kernel size on the image generation performance?\n\n4. The visual results do not seem to be significantly better than previous methods.\n\n##########################################################################\n\nReasons for score: \n\nMy main reason for rating this paper as below the acceptance threshold is that the noveltly is limited (see the 2nd point in Cons for explanation), and there is not a large improvement on the quality of the synthesized images.\n\n##########################################################################\n\nQuestions during the rebuttal period: \n\nPlease address my concerns in the Cons part.\n\n##########################################################################\n\nPost-rebuttal:\n\nI have read other reviewers' comments. Since the authors did not provide feedback to our reviews, I would change my score from 5 to 4.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper221/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper221/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantically-Adaptive Upsampling for Layout-to-Image Translation", "authorids": ["~Hao_Tang6", "~Nicu_Sebe1"], "authors": ["Hao Tang", "Nicu Sebe"], "keywords": ["Feature upsampling", "semantically-adaptive", "layout-to-image translation"], "abstract": "We propose the Semantically-Adaptive UpSampling (SA-UpSample), a general and highly effective upsampling method for the layout-to-image translation task. SA-UpSample has three advantages: 1) Global view. Unlike traditional upsampling methods (e.g., Nearest-neighbor) that only exploit local neighborhoods, SA-UpSample can aggregate semantic information in a global view. \n2) Semantically adaptive.  Instead of using a fixed kernel for all locations (e.g., Deconvolution), SA-UpSample enables semantic class-specific upsampling via generating adaptive kernels for different locations. 3) Efficient. Unlike Spatial Attention which uses a fully-connected strategy to connect all the pixels, SA-UpSample only considers the most relevant pixels, introducing little computational overhead. We observe that SA-UpSample achieves consistent and substantial gains on six popular datasets. The source code will be made publicly available.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tang|semanticallyadaptive_upsampling_for_layouttoimage_translation", "one-sentence_summary": "A novel feature upsampling method for layout-to-image translation method.", "supplementary_material": "/attachment/7ac9ab83310792277773d86772b2ec23090cde85.zip", "pdf": "/pdf/2f477861ed02c3ae6a9efbbbf6072f587693e92a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7aXgBtdDr", "_bibtex": "@misc{\ntang2021semanticallyadaptive,\ntitle={Semantically-Adaptive Upsampling for Layout-to-Image Translation},\nauthor={Hao Tang and Nicu Sebe},\nyear={2021},\nurl={https://openreview.net/forum?id=Z4YatHL7aq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Z4YatHL7aq", "replyto": "Z4YatHL7aq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper221/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147793, "tmdate": 1606915810741, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper221/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper221/-/Official_Review"}}}, {"id": "ASEhtbaYljc", "original": null, "number": 2, "cdate": 1603910397516, "ddate": null, "tcdate": 1603910397516, "tmdate": 1606959999507, "tddate": null, "forum": "Z4YatHL7aq", "replyto": "Z4YatHL7aq", "invitation": "ICLR.cc/2021/Conference/Paper221/-/Official_Review", "content": {"title": "Clarification and comparison with prior art needed", "review": "**Summary**\nThis paper presents a content-aware upsampling approach for layout-to-image translation problems. The core idea is to learn a spatially varying upsampling kernel. The paper applied the proposed upsampling method to GauGAN and showed improved performance over the nearest neighbor upsampling operators on six layout-to-image datasets.\n\n**Strength**\n+ The results clearly show that the proposed upsampling operator can improve the visual quality over the baseline nearest neighbor upsampling. The quantitative results also support the effectiveness of the proposed upsampling operation. \n+ The motivation and the differences over alternative methods is clear (Figure 1).\n+ The overview figure helps readers understand the high-level ideas. \n\n**Weakness**\n\n== Exposition ==\n\nThe exposition of the proposed method can be improved. \nFor examples, \n-\tit\u2019s unclear how the \u201cSemantic Kernel Generation\u201d is implemented. I can probably guess this step is essentially a 1 x 1 convolution, but it would be better to fill in the details. \n-\tIn the introduction, the paper mentioned that the translated images often contain artifacts due to the nearest-neighbor upsampling. Yet, in the Feature Spatial Expansion step it also used nearest-neighbor interpolation. This needs some more justification.\n-\tFor Figure 3, it\u2019s unclear what the learned semantically adaptive kernel visualization means. At each upsampling layer, the kernel is only K x K. \n-\tAt the end of Section 2, I do not understand what it means by \u201cthe semantics of the upsampled feature map can be stronger than the original one\u201d. \n-\tThe proposed upsampling layer is called \u201csemantically aware\u201d. However, I do not see anything that\u2019s related to the semantics other than the fact that the input is a semantic map. I would suggest that this should be called \u201ccontent aware\u201d instead. \n\n== Technical novelty ==\n-\tMy main concern about the paper lies in its technical novelty. There have been multiple papers that proposed content aware filter. As far as I know, the first one is [Xu et al. 2016].\nJia, Xu, et al. \"Dynamic filter networks.\" Advances in neural information processing systems. 2016.\n\n- The work most relevant to this paper is the CARAFE [Wang et al. ICCV 2019], which can be viewed as a special case of the dynamic filter network for feature upsampling. By comparing Figure 2 in this paper and Figure 2 in the CARAFE paper [Wang et al. ICCV 2019], it seems to me that the two methods are * the same*. The only difference is the application of the layout-to-image task. The paper in the related work section suggests, \u201cthe settings of these tasks are significantly different from ours, making their methods cannot be used directly.\u201d I respectfully disagree with this statement because both methods take in a feature tensor and produce an upsampled feature tensor. I believe that the application would be straightforward without any modification. Given the similarity to prior work, it would be great for the authors to (1) describe in detail the differences (if any) and (2) compare with prior work that also uses spatially varying upsampling kernel.\n\nMinor:\n-\tCARAFE: Content-Aware ReAssembly of Features. The paper is in ICCV 2019, not CVPR 2019\n\nIn sum, I think the idea of the spatially adaptive upsampling kernel is technically sound. I also like the extensive evaluation in this paper. However, I have concerns about the high degree of similarity with the prior method and the lack of comparison with CARAFE. \n\n** After discussions **\n\nI have read other reviewers' comments. Many of the reviewers share similar concerns regarding the technical novelty of this work. I don't find sufficient ground to recommend acceptance of this paper. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper221/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper221/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantically-Adaptive Upsampling for Layout-to-Image Translation", "authorids": ["~Hao_Tang6", "~Nicu_Sebe1"], "authors": ["Hao Tang", "Nicu Sebe"], "keywords": ["Feature upsampling", "semantically-adaptive", "layout-to-image translation"], "abstract": "We propose the Semantically-Adaptive UpSampling (SA-UpSample), a general and highly effective upsampling method for the layout-to-image translation task. SA-UpSample has three advantages: 1) Global view. Unlike traditional upsampling methods (e.g., Nearest-neighbor) that only exploit local neighborhoods, SA-UpSample can aggregate semantic information in a global view. \n2) Semantically adaptive.  Instead of using a fixed kernel for all locations (e.g., Deconvolution), SA-UpSample enables semantic class-specific upsampling via generating adaptive kernels for different locations. 3) Efficient. Unlike Spatial Attention which uses a fully-connected strategy to connect all the pixels, SA-UpSample only considers the most relevant pixels, introducing little computational overhead. We observe that SA-UpSample achieves consistent and substantial gains on six popular datasets. The source code will be made publicly available.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tang|semanticallyadaptive_upsampling_for_layouttoimage_translation", "one-sentence_summary": "A novel feature upsampling method for layout-to-image translation method.", "supplementary_material": "/attachment/7ac9ab83310792277773d86772b2ec23090cde85.zip", "pdf": "/pdf/2f477861ed02c3ae6a9efbbbf6072f587693e92a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7aXgBtdDr", "_bibtex": "@misc{\ntang2021semanticallyadaptive,\ntitle={Semantically-Adaptive Upsampling for Layout-to-Image Translation},\nauthor={Hao Tang and Nicu Sebe},\nyear={2021},\nurl={https://openreview.net/forum?id=Z4YatHL7aq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Z4YatHL7aq", "replyto": "Z4YatHL7aq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper221/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147793, "tmdate": 1606915810741, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper221/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper221/-/Official_Review"}}}, {"id": "EuNEqGztJmZ", "original": null, "number": 3, "cdate": 1603959461690, "ddate": null, "tcdate": 1603959461690, "tmdate": 1605268919828, "tddate": null, "forum": "Z4YatHL7aq", "replyto": "Z4YatHL7aq", "invitation": "ICLR.cc/2021/Conference/Paper221/-/Official_Review", "content": {"title": "Some details of the proposed method are unclear", "review": "This paper proposes the Semantic-Adaptive UpSampling method to do feature upsample in layout-to-image translation task. The SA-UpSample module exploit the semantic information to learn adaptive upsample kernels for different input features. The proposed method has the advantage of global view, semantically-adaptive and efficient. Extensive experimental results are shown to prove the effectiveness of the proposed method. The paper is well organized and easy to follow.\n\nStrength:\n\n(+) The experiments are sufficient and extensive. The proposed method is evaluated on many different datasets and generate satisfactory results in all of these datasets. Both qualitative and Quantitative results are shown, along with user study and visualization. Comparison results with different upsampling methods and state-of-the-art methods guarantee the effectiveness of the proposed methods.\n\n(+) This paper is well organized and the description of the proposed method is easy to understand.\n\nWeakness:\n\n(-) How to determine some hyper-parameters of the proposed method is not clear. E.g. C\u2019, k and s are very important hyper-parameters of SA-UpSample. The authors set C\u2019=64, k=5 and s=2. Are they determined by manual or from experimental results? \n\n(-) The proposed method is efficient because it has a feature channel compression step. If we ignore the computational complexity and enlarge the channel number of this step (i.e. enlarge C\u2019), will the proposed method generate better results (images with more realistic)? Whether the authors choose a trade off between effectiveness and efficiency by setting a relatively small C\u2019?", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper221/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper221/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantically-Adaptive Upsampling for Layout-to-Image Translation", "authorids": ["~Hao_Tang6", "~Nicu_Sebe1"], "authors": ["Hao Tang", "Nicu Sebe"], "keywords": ["Feature upsampling", "semantically-adaptive", "layout-to-image translation"], "abstract": "We propose the Semantically-Adaptive UpSampling (SA-UpSample), a general and highly effective upsampling method for the layout-to-image translation task. SA-UpSample has three advantages: 1) Global view. Unlike traditional upsampling methods (e.g., Nearest-neighbor) that only exploit local neighborhoods, SA-UpSample can aggregate semantic information in a global view. \n2) Semantically adaptive.  Instead of using a fixed kernel for all locations (e.g., Deconvolution), SA-UpSample enables semantic class-specific upsampling via generating adaptive kernels for different locations. 3) Efficient. Unlike Spatial Attention which uses a fully-connected strategy to connect all the pixels, SA-UpSample only considers the most relevant pixels, introducing little computational overhead. We observe that SA-UpSample achieves consistent and substantial gains on six popular datasets. The source code will be made publicly available.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tang|semanticallyadaptive_upsampling_for_layouttoimage_translation", "one-sentence_summary": "A novel feature upsampling method for layout-to-image translation method.", "supplementary_material": "/attachment/7ac9ab83310792277773d86772b2ec23090cde85.zip", "pdf": "/pdf/2f477861ed02c3ae6a9efbbbf6072f587693e92a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7aXgBtdDr", "_bibtex": "@misc{\ntang2021semanticallyadaptive,\ntitle={Semantically-Adaptive Upsampling for Layout-to-Image Translation},\nauthor={Hao Tang and Nicu Sebe},\nyear={2021},\nurl={https://openreview.net/forum?id=Z4YatHL7aq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Z4YatHL7aq", "replyto": "Z4YatHL7aq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper221/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147793, "tmdate": 1606915810741, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper221/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper221/-/Official_Review"}}}, {"id": "muVc4QLdZ3A", "original": null, "number": 1, "cdate": 1603887104017, "ddate": null, "tcdate": 1603887104017, "tmdate": 1605024737103, "tddate": null, "forum": "Z4YatHL7aq", "replyto": "Z4YatHL7aq", "invitation": "ICLR.cc/2021/Conference/Paper221/-/Official_Review", "content": {"title": "Novelty is limited", "review": "This paper presents a module named semantically-adaptive upsampling (SA-UpSample) to achieve layout-to-image translation. The proposed method is able to aggregate semantic information in the layout input and adaptively conducts class-specific upsampling in the translation process. Experiments on six datasets demonstrate the effectiveness of the proposed mehtod. However, there are still several issues to be addressed.\n\n1. Novelty. The proposed semantically-adaptive kernel generation branch and semantically-adpative feature upsampling branch share very similar idea as the filter generation network and dynamic filtering layer in [a], and adaptive interpolation kernel generation and adaptive image resampling in [b], except that the input of generation branch is layout map.\n\n[a] B. De Brabandere et al., Dynamic Filter Networks, NIPS 2016.\n\n[b] X. Jia et al., Super-Resolution with Deep Adaptive Image Resampling, CoRR abs/1712.06463, 2017\n\n2. What if applying the proposed module to also the place after conv layers that corresponds to 1x upsampling? Will performance be further improved.\n\n3. Given the experiments in Fig. 5 (comparing 3rd column and the 4th column), I wonder whether there is any overfitting with the results.\n\n4. It only shows that it is effective in image translation but does not mention how efficient the proposed module is.\n\n5. In Table 3, it seems that the semantic consistency with layout input is worse than TSIT and LGGAN. It is not clear why the row of the proposed method is marked bold although it is not the best in terms of some metrics.\n ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper221/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper221/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Semantically-Adaptive Upsampling for Layout-to-Image Translation", "authorids": ["~Hao_Tang6", "~Nicu_Sebe1"], "authors": ["Hao Tang", "Nicu Sebe"], "keywords": ["Feature upsampling", "semantically-adaptive", "layout-to-image translation"], "abstract": "We propose the Semantically-Adaptive UpSampling (SA-UpSample), a general and highly effective upsampling method for the layout-to-image translation task. SA-UpSample has three advantages: 1) Global view. Unlike traditional upsampling methods (e.g., Nearest-neighbor) that only exploit local neighborhoods, SA-UpSample can aggregate semantic information in a global view. \n2) Semantically adaptive.  Instead of using a fixed kernel for all locations (e.g., Deconvolution), SA-UpSample enables semantic class-specific upsampling via generating adaptive kernels for different locations. 3) Efficient. Unlike Spatial Attention which uses a fully-connected strategy to connect all the pixels, SA-UpSample only considers the most relevant pixels, introducing little computational overhead. We observe that SA-UpSample achieves consistent and substantial gains on six popular datasets. The source code will be made publicly available.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "tang|semanticallyadaptive_upsampling_for_layouttoimage_translation", "one-sentence_summary": "A novel feature upsampling method for layout-to-image translation method.", "supplementary_material": "/attachment/7ac9ab83310792277773d86772b2ec23090cde85.zip", "pdf": "/pdf/2f477861ed02c3ae6a9efbbbf6072f587693e92a.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7aXgBtdDr", "_bibtex": "@misc{\ntang2021semanticallyadaptive,\ntitle={Semantically-Adaptive Upsampling for Layout-to-Image Translation},\nauthor={Hao Tang and Nicu Sebe},\nyear={2021},\nurl={https://openreview.net/forum?id=Z4YatHL7aq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Z4YatHL7aq", "replyto": "Z4YatHL7aq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper221/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147793, "tmdate": 1606915810741, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper221/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper221/-/Official_Review"}}}], "count": 6}