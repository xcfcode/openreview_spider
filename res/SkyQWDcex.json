{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396536647, "tcdate": 1486396536647, "number": 1, "id": "Sk-t3MI_e", "invitation": "ICLR.cc/2017/conference/-/paper357/acceptance", "forum": "SkyQWDcex", "replyto": "SkyQWDcex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The program committee appreciates the authors' response to concerns raised in the reviews. Unfortunately, all reviewers are leaning against accepting the paper. Authors are encouraged to incorporate reviewer feedback in future iterations of this work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Context-aware Attention Network for Interactive Question Answering", "abstract": "We develop a new model for Interactive Question Answering (IQA), using Gated-Recurrent-Unit recurrent networks (GRUs) as encoders for statements and questions, and another GRU as a decoder for outputs. Distinct from previous work, our approach employs context-dependent word-level attention for more accurate statement representations and question-guided sentence-level attention for better context modeling. Employing these mechanisms, our model accurately understands when it can output an answer or when it requires generating a supplementary question for additional input. When available, user's feedback is encoded and directly applied to update sentence-level attention to infer the answer. Extensive experiments on QA and IQA datasets demonstrate quantitatively the effectiveness of our model with significant improvement over conventional QA models.", "pdf": "/pdf/060e7dd9c5df591171ee87bfff2b0456c7a317d4.pdf", "TL;DR": "A self-adaptive QA model aware of what it knows and what it does not know for interactive question answering.", "paperhash": "li|a_contextaware_attention_network_for_interactive_question_answering", "keywords": ["Deep learning", "Natural language processing", "Applications"], "conflicts": ["uncc.edu", "email.arizona.edu", "nec-labs.com"], "authors": ["Huayu Li", "Martin Renqiang Min", "Yong Ge", "Asim Kadav"], "authorids": ["hli38@uncc.edu", "renqiang@nec-labs.com", "yongge@email.arizona.edu", "asim@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396537628, "id": "ICLR.cc/2017/conference/-/paper357/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SkyQWDcex", "replyto": "SkyQWDcex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396537628}}}, {"tddate": null, "tmdate": 1482429955185, "tcdate": 1482429955185, "number": 6, "id": "HkiZL9YEg", "invitation": "ICLR.cc/2017/conference/-/paper357/public/comment", "forum": "SkyQWDcex", "replyto": "r1PJRDf4g", "signatures": ["~Huayu_Li1"], "readers": ["everyone"], "writers": ["~Huayu_Li1"], "content": {"title": "Comments on Review", "comment": "Thanks a lot for your review.\n\n1. Thanks a lot for your suggestion of providing more details about the bAbI extension. We will improve our paper to talk more about this new dataset. Basically, the extended datasets follow standard bAbI settings, but the difference is that we added some supporting question templates, such as \u201cWho is REFERENCE?\u201d, \u201cWhich OBJECT, CHOICE_1 or CHOICE_2 one?\u201d, \u201cWhat objects are you referring to?\u201d. The feedback is randomly picked from the objects in the statements.\n\nFor DMN+ and MemN2N on IQA tasks, they do not use supporting question and feedback and we will make it more clear in the paper. We did this because they cannot handle the feedback by design.\n\n\n2. We agree that the supporting question like \"which\" or \"which bedroom\" is also sufficient for most users. But generating the template \"Which OBJECT, CHOICE_1 or CHOICE_2 one?\u201d can increase the diversity of supporting questions and at the same time can test the capability of the decoder to output a statement-dependent question.\n\n\n3. Our model is aware of what it knows and what it does not know. When it does not know, it outputs a question for additional support. The user then provides a feedback. This feedback will be used to update the model and output the answer. We think this interactive process is novel, natural and effective for solving real-world QA problems.\n\n\n4. Our three tasks simulate different scenarios with incomplete information. Specifically, task 1 focuses on ambiguous actor problem. Task 4 represents ambiguous object problem. Task 7 is to ask further information that assists answer prediction. We think all of these three task are representative for IQA problems. We do not need to modify each of the 20 bAbI task to make it interactive, because some extensions are not natural.\n\n\n5. Creating a new task and a new dataset is one of the major contributions of this paper. The purpose is to inform the research community of the importance of modeling unknown state in QA systems.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Context-aware Attention Network for Interactive Question Answering", "abstract": "We develop a new model for Interactive Question Answering (IQA), using Gated-Recurrent-Unit recurrent networks (GRUs) as encoders for statements and questions, and another GRU as a decoder for outputs. Distinct from previous work, our approach employs context-dependent word-level attention for more accurate statement representations and question-guided sentence-level attention for better context modeling. Employing these mechanisms, our model accurately understands when it can output an answer or when it requires generating a supplementary question for additional input. When available, user's feedback is encoded and directly applied to update sentence-level attention to infer the answer. Extensive experiments on QA and IQA datasets demonstrate quantitatively the effectiveness of our model with significant improvement over conventional QA models.", "pdf": "/pdf/060e7dd9c5df591171ee87bfff2b0456c7a317d4.pdf", "TL;DR": "A self-adaptive QA model aware of what it knows and what it does not know for interactive question answering.", "paperhash": "li|a_contextaware_attention_network_for_interactive_question_answering", "keywords": ["Deep learning", "Natural language processing", "Applications"], "conflicts": ["uncc.edu", "email.arizona.edu", "nec-labs.com"], "authors": ["Huayu Li", "Martin Renqiang Min", "Yong Ge", "Asim Kadav"], "authorids": ["hli38@uncc.edu", "renqiang@nec-labs.com", "yongge@email.arizona.edu", "asim@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287609825, "id": "ICLR.cc/2017/conference/-/paper357/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkyQWDcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper357/reviewers", "ICLR.cc/2017/conference/paper357/areachairs"], "cdate": 1485287609825}}}, {"tddate": null, "tmdate": 1482429800080, "tcdate": 1482429800080, "number": 5, "id": "r1lOHcFNl", "invitation": "ICLR.cc/2017/conference/-/paper357/public/comment", "forum": "SkyQWDcex", "replyto": "rJvuW5GNl", "signatures": ["~Huayu_Li1"], "readers": ["everyone"], "writers": ["~Huayu_Li1"], "content": {"title": "Comments on Review", "comment": "Thanks a lot for your review.\n\n1. In general, most QA systems can be described to have input module, question module, and answer module in a cartoon style as in Figure 1 (the interactive module is new).  However, the technical difference between our model CAN and previous model DMN+ is very clear, which will be described as follows and further emphasized in the revision of our paper.\n\n1) Completely different attention mechanisms: CAN has a well-motivated two-level context-aware attention mechanism including context-dependent word attention and question-dependent sentence attention, which requires transformations between two different embedding spaces. DMN+ only has attention calculated at a standard fact/sentence level. This is the key difference between CAN and DMN+.\n\n2) Just due to different attention mechanisms, CAN does not need multi-hop attention and calculation, but DMN+ does require it. It is hard to determine the optimal number of hops for each task, which is clearly an overhead for DMN+. Just because our model CAN does not have multi-hop attention, we don't need to worry about complex issues such as ReLU with parameter sharing/nonsharing for computing episode memories in DMN+.\n\n3) Significant performance improvement: Out of 20 bAbI tasks (using 1% error rate as cutoff), our model CAN only failed on 1 task, but DMN+ failed on 5 tasks.\n\nIn details, in task 7 (CAN is 0.3% while DMN+ is 2.4%), task 17 (CAN is 0.2% while DMN+ is 4.2%), and task 18 (CAN is 0.5% while DMN+ is 2.1%).\n\nIn summary, from a technical perspective, our model CAN is very different from DMN+.\n\n\n2. Our IQA task is different from the dialog task in (Weston, 2016) and (Bordes et al., 2016). Our IQA focuses on conventional textual statement-question-answer triplets and effectively solves real-world QA problems with incomplete or ambiguous information. In other words, our system behaves in a way that it knows what it knows and knows what it does not know.\n\nPlease note that (Weston, 2016) was just published at NIPS this December and (Bordes et al., 2016) is a concurrent submission to ICLR 2017. We will cite these two papers in the revision, but will mention that our research was performed at the same time.\n\n\n3. To the best of our knowledge, our work is the first to augment encoder-decoder framework for IQA with incomplete/ambiguous information and use the IQA concept to improve QA accuracy.\n\nThis new framework could be extended to other models, but we are the first to propose/conduct this line of research. Although most models can be trained to output question, how to effectively utilize user\u2019s feedback to update model and output answer will lead to completely different papers. It is true that the baseline model, EncDec*, can be easily extended to generate supporting questions, however, it cannot use the feedback to generate answers.\n\nIt is not clear that how DMN+ can be simply extended to solve the IQA problem proposed by us here. CAN is context-aware and readily models contextual information by design, which is natural for assessing whether there is any ambiguity or missing information. There is no motivation to consider how DMN+ should be extended for modeling ambiguous/incomplete information.\n\n\n4. Enumerating all the questions in the training and taking softmax over them to generate question can be regarded as one classification solution. But this kind of solution cannot address statement-dependent supporting questions. For example, for supporting question \u201cwhich bedroom, master or guest one?\u201c, \u2018master\u2019 and \u2018guest\u2019 are generated according to statements. If the testing instance includes new examples, the classification solution is incapable of classifying a new supporting question.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Context-aware Attention Network for Interactive Question Answering", "abstract": "We develop a new model for Interactive Question Answering (IQA), using Gated-Recurrent-Unit recurrent networks (GRUs) as encoders for statements and questions, and another GRU as a decoder for outputs. Distinct from previous work, our approach employs context-dependent word-level attention for more accurate statement representations and question-guided sentence-level attention for better context modeling. Employing these mechanisms, our model accurately understands when it can output an answer or when it requires generating a supplementary question for additional input. When available, user's feedback is encoded and directly applied to update sentence-level attention to infer the answer. Extensive experiments on QA and IQA datasets demonstrate quantitatively the effectiveness of our model with significant improvement over conventional QA models.", "pdf": "/pdf/060e7dd9c5df591171ee87bfff2b0456c7a317d4.pdf", "TL;DR": "A self-adaptive QA model aware of what it knows and what it does not know for interactive question answering.", "paperhash": "li|a_contextaware_attention_network_for_interactive_question_answering", "keywords": ["Deep learning", "Natural language processing", "Applications"], "conflicts": ["uncc.edu", "email.arizona.edu", "nec-labs.com"], "authors": ["Huayu Li", "Martin Renqiang Min", "Yong Ge", "Asim Kadav"], "authorids": ["hli38@uncc.edu", "renqiang@nec-labs.com", "yongge@email.arizona.edu", "asim@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287609825, "id": "ICLR.cc/2017/conference/-/paper357/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkyQWDcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper357/reviewers", "ICLR.cc/2017/conference/paper357/areachairs"], "cdate": 1485287609825}}}, {"tddate": null, "tmdate": 1482429520542, "tcdate": 1482429520542, "number": 4, "id": "r1_L4qt4l", "invitation": "ICLR.cc/2017/conference/-/paper357/public/comment", "forum": "SkyQWDcex", "replyto": "H16tHzL4l", "signatures": ["~Huayu_Li1"], "readers": ["everyone"], "writers": ["~Huayu_Li1"], "content": {"title": "Comments on Review", "comment": "Thanks a lot for your review.\n\n1. In general, most QA systems can be described to have input module, question module, and answer module in a cartoon style as in Figure 1 (the interactive module is new).  However, the technical difference between our model CAN and previous model DMN+ is very clear, which will be described as follows and further emphasized in the revision of our paper.\n\n1) Completely different attention mechanisms: CAN has a well-motivated two-level context-aware attention mechanism including context-dependent word attention and question-dependent sentence attention, which requires transformations between two different embedding spaces. DMN+ only has attention calculated at a standard fact/sentence level. This is the key difference between CAN and DMN+.\n\n2) Just due to different attention mechanisms, CAN does not need multi-hop attention and calculation, but DMN+ does require it. It is hard to determine the optimal number of hops for each task, which is clearly an overhead for DMN+. Just because our model CAN does not have multi-hop attention, we don't need to worry about complex issues such as ReLU with parameter sharing/nonsharing for computing episode memories in DMN+.\n\n3) Significant performance improvement: Out of 20 bAbI tasks (using 1% error rate as cutoff), our model CAN only failed on 1 task, but DMN+ failed on 5 tasks.\n\nIn details, in task 7 (CAN is 0.3% while DMN+ is 2.4%), task 17 (CAN is 0.2% while DMN+ is 4.2%), and task 18 (CAN is 0.5% while DMN+ is 2.1%).\n\nIn summary, from a technical perspective, our model CAN is very different from DMN+.\n\n2. We use IQA datasets to train our model. Our IQA training datasets have both supporting questions and user\u2019s feedbacks. When testing, we use the trained model to handle test examples which are in the form of statements-question. As our model is aware of what it knows and it does not know, it outputs a question for additional support if it does not know. The user then provides a feedback. This feedback will be used to update the model and output an answer.  For example, in the test phase, when the system outputs a supporting question, \u201cwhich bedroom, master or guest one?\u201d. The user will provide either \u201cmaster bedroom\u201d or \u201cguest bedroom\u201d.  After receiving this feedback, the system will update the attention weights over sentences and output an estimated answer. Table 2 provides an example of the interactive process."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Context-aware Attention Network for Interactive Question Answering", "abstract": "We develop a new model for Interactive Question Answering (IQA), using Gated-Recurrent-Unit recurrent networks (GRUs) as encoders for statements and questions, and another GRU as a decoder for outputs. Distinct from previous work, our approach employs context-dependent word-level attention for more accurate statement representations and question-guided sentence-level attention for better context modeling. Employing these mechanisms, our model accurately understands when it can output an answer or when it requires generating a supplementary question for additional input. When available, user's feedback is encoded and directly applied to update sentence-level attention to infer the answer. Extensive experiments on QA and IQA datasets demonstrate quantitatively the effectiveness of our model with significant improvement over conventional QA models.", "pdf": "/pdf/060e7dd9c5df591171ee87bfff2b0456c7a317d4.pdf", "TL;DR": "A self-adaptive QA model aware of what it knows and what it does not know for interactive question answering.", "paperhash": "li|a_contextaware_attention_network_for_interactive_question_answering", "keywords": ["Deep learning", "Natural language processing", "Applications"], "conflicts": ["uncc.edu", "email.arizona.edu", "nec-labs.com"], "authors": ["Huayu Li", "Martin Renqiang Min", "Yong Ge", "Asim Kadav"], "authorids": ["hli38@uncc.edu", "renqiang@nec-labs.com", "yongge@email.arizona.edu", "asim@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287609825, "id": "ICLR.cc/2017/conference/-/paper357/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkyQWDcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper357/reviewers", "ICLR.cc/2017/conference/paper357/areachairs"], "cdate": 1485287609825}}}, {"tddate": null, "tmdate": 1482200452875, "tcdate": 1482200452875, "number": 3, "id": "H16tHzL4l", "invitation": "ICLR.cc/2017/conference/-/paper357/official/review", "forum": "SkyQWDcex", "replyto": "SkyQWDcex", "signatures": ["ICLR.cc/2017/conference/paper357/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper357/AnonReviewer2"], "content": {"title": "", "rating": "5: Marginally below acceptance threshold", "review": "1. the QA model is not novel, very similar to the existing model.\n2. The IQA model is very confusing. If it needs human interactive in the training process, how could it be practical to ask human to join the training in each iteration? It sounds impractical. If the human interactive questions are predefined, then it is not interactive at all, since it is not based on the current state of model output.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Context-aware Attention Network for Interactive Question Answering", "abstract": "We develop a new model for Interactive Question Answering (IQA), using Gated-Recurrent-Unit recurrent networks (GRUs) as encoders for statements and questions, and another GRU as a decoder for outputs. Distinct from previous work, our approach employs context-dependent word-level attention for more accurate statement representations and question-guided sentence-level attention for better context modeling. Employing these mechanisms, our model accurately understands when it can output an answer or when it requires generating a supplementary question for additional input. When available, user's feedback is encoded and directly applied to update sentence-level attention to infer the answer. Extensive experiments on QA and IQA datasets demonstrate quantitatively the effectiveness of our model with significant improvement over conventional QA models.", "pdf": "/pdf/060e7dd9c5df591171ee87bfff2b0456c7a317d4.pdf", "TL;DR": "A self-adaptive QA model aware of what it knows and what it does not know for interactive question answering.", "paperhash": "li|a_contextaware_attention_network_for_interactive_question_answering", "keywords": ["Deep learning", "Natural language processing", "Applications"], "conflicts": ["uncc.edu", "email.arizona.edu", "nec-labs.com"], "authors": ["Huayu Li", "Martin Renqiang Min", "Yong Ge", "Asim Kadav"], "authorids": ["hli38@uncc.edu", "renqiang@nec-labs.com", "yongge@email.arizona.edu", "asim@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512613182, "id": "ICLR.cc/2017/conference/-/paper357/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper357/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper357/AnonReviewer3", "ICLR.cc/2017/conference/paper357/AnonReviewer1", "ICLR.cc/2017/conference/paper357/AnonReviewer2"], "reply": {"forum": "SkyQWDcex", "replyto": "SkyQWDcex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper357/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper357/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512613182}}}, {"tddate": null, "tmdate": 1481970116427, "tcdate": 1481970031067, "number": 2, "id": "rJvuW5GNl", "invitation": "ICLR.cc/2017/conference/-/paper357/official/review", "forum": "SkyQWDcex", "replyto": "SkyQWDcex", "signatures": ["ICLR.cc/2017/conference/paper357/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper357/AnonReviewer1"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "This work describes \n\n1: a two stage encoding of stories in bAbI like setups, where a GRU is used to encode a sentence, word by word, conditioned on a sentence level GRU, and the sentence level GRU keeps track of a sentence level encoding.  Each is used\n\n2: modifying the bAbI tasks so it is necessary to ask a question to correctly solve the problem\n\nI am not convinced by the papers results:\n\n1:   The new architecture does not do significantly better than DMN+, and in my view, is similar to DMN+.   What problem with DMN+ does your architecture solve?   \n\n2:  There are now several papers doing the second thing, for example \"Dialog-based Language Learning\" by Weston and  \"Learning End-to-End Goal-Oriented Dialog\" by Bordes and Weston, and I think doing it more carefully and in more compelling ways.   In the current work, the correct answer to the question seems given independent of the what the agent asks, so any model that can output \"unknown\" and then input the extra response has an advantage.  Essentially all of the architectures that are used to solve bAbI can be modified to do this...  Indeed, the enc-dec* accuracies in appendix A show that this sort of module can be appended to any other model.  All of the standard models can be trained to output questions as a sequence of words.    Furthermore, I suspect you could generate the questions  in the authors' setting just by enumerating all the questions that occur in training, and taking a softmax over them, instead of generating word-by-word.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Context-aware Attention Network for Interactive Question Answering", "abstract": "We develop a new model for Interactive Question Answering (IQA), using Gated-Recurrent-Unit recurrent networks (GRUs) as encoders for statements and questions, and another GRU as a decoder for outputs. Distinct from previous work, our approach employs context-dependent word-level attention for more accurate statement representations and question-guided sentence-level attention for better context modeling. Employing these mechanisms, our model accurately understands when it can output an answer or when it requires generating a supplementary question for additional input. When available, user's feedback is encoded and directly applied to update sentence-level attention to infer the answer. Extensive experiments on QA and IQA datasets demonstrate quantitatively the effectiveness of our model with significant improvement over conventional QA models.", "pdf": "/pdf/060e7dd9c5df591171ee87bfff2b0456c7a317d4.pdf", "TL;DR": "A self-adaptive QA model aware of what it knows and what it does not know for interactive question answering.", "paperhash": "li|a_contextaware_attention_network_for_interactive_question_answering", "keywords": ["Deep learning", "Natural language processing", "Applications"], "conflicts": ["uncc.edu", "email.arizona.edu", "nec-labs.com"], "authors": ["Huayu Li", "Martin Renqiang Min", "Yong Ge", "Asim Kadav"], "authorids": ["hli38@uncc.edu", "renqiang@nec-labs.com", "yongge@email.arizona.edu", "asim@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512613182, "id": "ICLR.cc/2017/conference/-/paper357/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper357/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper357/AnonReviewer3", "ICLR.cc/2017/conference/paper357/AnonReviewer1", "ICLR.cc/2017/conference/paper357/AnonReviewer2"], "reply": {"forum": "SkyQWDcex", "replyto": "SkyQWDcex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper357/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper357/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512613182}}}, {"tddate": null, "tmdate": 1481960927289, "tcdate": 1481960927289, "number": 1, "id": "r1PJRDf4g", "invitation": "ICLR.cc/2017/conference/-/paper357/official/review", "forum": "SkyQWDcex", "replyto": "SkyQWDcex", "signatures": ["ICLR.cc/2017/conference/paper357/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper357/AnonReviewer3"], "content": {"title": "review", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes an \"interactive\" version of the bAbI dataset by adding supporting questions/answers to the dataset in cases where there is not enough information to answer the question. Interactive QA is certainly an interesting problem and is well-motivated by the paper. However, I don't feel like the bAbI extension is adequately explained. For example, the baseline DMN and MemN2N models on the IQA task are \"take both statements and question as input and then\nestimate an answer.\" Their task is then fundamentally more difficult from the CAN's because they do not distinguish \"feedback\" from the original context; perhaps a more fair approach would be to treat **every** question (both supporting and original questions) as individual instances. Also, how were the supporting questions and the user feedback generated? How many templates / words were used to create them? The dataset creation details are missing, and if space is an issue, a lot of basic exposition on things like GRU / sentence encodings can be cut (or at least greatly shortened) and replaced with pointers to the original papers. \n\nAnother issue I had is that the model attempts to generate these synthetic questions; if there are just one or two templates, why not just predict the values that fill these templates? So instead of generating \"Which bedroom, master one or guest one?\" with an RNN decoder, just predict \"which\" or \"which bedroom\"... isn't this sufficient? In the end, these just seem like more supporting facts, not actual interaction with users, and the fact that it is run on only three of the original twenty tasks make the conclusions hard to trust.\n\nIn conclusion, I think the paper has a strong idea and motivation, but the experiments are not convincing for the paper to be accepted at ICLR.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Context-aware Attention Network for Interactive Question Answering", "abstract": "We develop a new model for Interactive Question Answering (IQA), using Gated-Recurrent-Unit recurrent networks (GRUs) as encoders for statements and questions, and another GRU as a decoder for outputs. Distinct from previous work, our approach employs context-dependent word-level attention for more accurate statement representations and question-guided sentence-level attention for better context modeling. Employing these mechanisms, our model accurately understands when it can output an answer or when it requires generating a supplementary question for additional input. When available, user's feedback is encoded and directly applied to update sentence-level attention to infer the answer. Extensive experiments on QA and IQA datasets demonstrate quantitatively the effectiveness of our model with significant improvement over conventional QA models.", "pdf": "/pdf/060e7dd9c5df591171ee87bfff2b0456c7a317d4.pdf", "TL;DR": "A self-adaptive QA model aware of what it knows and what it does not know for interactive question answering.", "paperhash": "li|a_contextaware_attention_network_for_interactive_question_answering", "keywords": ["Deep learning", "Natural language processing", "Applications"], "conflicts": ["uncc.edu", "email.arizona.edu", "nec-labs.com"], "authors": ["Huayu Li", "Martin Renqiang Min", "Yong Ge", "Asim Kadav"], "authorids": ["hli38@uncc.edu", "renqiang@nec-labs.com", "yongge@email.arizona.edu", "asim@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512613182, "id": "ICLR.cc/2017/conference/-/paper357/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper357/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper357/AnonReviewer3", "ICLR.cc/2017/conference/paper357/AnonReviewer1", "ICLR.cc/2017/conference/paper357/AnonReviewer2"], "reply": {"forum": "SkyQWDcex", "replyto": "SkyQWDcex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper357/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper357/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512613182}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1480959184215, "tcdate": 1478287639519, "number": 357, "id": "SkyQWDcex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SkyQWDcex", "signatures": ["~Huayu_Li1"], "readers": ["everyone"], "content": {"title": "A Context-aware Attention Network for Interactive Question Answering", "abstract": "We develop a new model for Interactive Question Answering (IQA), using Gated-Recurrent-Unit recurrent networks (GRUs) as encoders for statements and questions, and another GRU as a decoder for outputs. Distinct from previous work, our approach employs context-dependent word-level attention for more accurate statement representations and question-guided sentence-level attention for better context modeling. Employing these mechanisms, our model accurately understands when it can output an answer or when it requires generating a supplementary question for additional input. When available, user's feedback is encoded and directly applied to update sentence-level attention to infer the answer. Extensive experiments on QA and IQA datasets demonstrate quantitatively the effectiveness of our model with significant improvement over conventional QA models.", "pdf": "/pdf/060e7dd9c5df591171ee87bfff2b0456c7a317d4.pdf", "TL;DR": "A self-adaptive QA model aware of what it knows and what it does not know for interactive question answering.", "paperhash": "li|a_contextaware_attention_network_for_interactive_question_answering", "keywords": ["Deep learning", "Natural language processing", "Applications"], "conflicts": ["uncc.edu", "email.arizona.edu", "nec-labs.com"], "authors": ["Huayu Li", "Martin Renqiang Min", "Yong Ge", "Asim Kadav"], "authorids": ["hli38@uncc.edu", "renqiang@nec-labs.com", "yongge@email.arizona.edu", "asim@nec-labs.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1480956946426, "tcdate": 1480739170055, "number": 3, "id": "SJ9PFTyQe", "invitation": "ICLR.cc/2017/conference/-/paper357/public/comment", "forum": "SkyQWDcex", "replyto": "B1T-puJ7x", "signatures": ["~Huayu_Li1"], "readers": ["everyone"], "writers": ["~Huayu_Li1"], "content": {"title": "Comments on questions.", "comment": "Thanks for your comments.\n\n1. Different from Memory Networks, our model does not use supporting facts during the training of both QA and IQA. For standard QA, we train the model end-to-end in the form of statements-question-answer triplets. For IQA, we use additional information obtained from users during training to update sentence-level attention. From this perspective, we agree that our model is not trained end-to-end for IQA because of interactivity. We will revise our paper to improve the model description to avoid confusions.\n\n2. \\mathbf{u} is encoded in sentence-level space. Word-level representation space is different from sentence-level space. Therefore, we use a two-layer MLP to transform a word representation into a sentence-level space and get a context-aware vector in equation 5. This vector is then used to calculate the attention weight over a word shown in equation 6 by incorporating question and context information.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Context-aware Attention Network for Interactive Question Answering", "abstract": "We develop a new model for Interactive Question Answering (IQA), using Gated-Recurrent-Unit recurrent networks (GRUs) as encoders for statements and questions, and another GRU as a decoder for outputs. Distinct from previous work, our approach employs context-dependent word-level attention for more accurate statement representations and question-guided sentence-level attention for better context modeling. Employing these mechanisms, our model accurately understands when it can output an answer or when it requires generating a supplementary question for additional input. When available, user's feedback is encoded and directly applied to update sentence-level attention to infer the answer. Extensive experiments on QA and IQA datasets demonstrate quantitatively the effectiveness of our model with significant improvement over conventional QA models.", "pdf": "/pdf/060e7dd9c5df591171ee87bfff2b0456c7a317d4.pdf", "TL;DR": "A self-adaptive QA model aware of what it knows and what it does not know for interactive question answering.", "paperhash": "li|a_contextaware_attention_network_for_interactive_question_answering", "keywords": ["Deep learning", "Natural language processing", "Applications"], "conflicts": ["uncc.edu", "email.arizona.edu", "nec-labs.com"], "authors": ["Huayu Li", "Martin Renqiang Min", "Yong Ge", "Asim Kadav"], "authorids": ["hli38@uncc.edu", "renqiang@nec-labs.com", "yongge@email.arizona.edu", "asim@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287609825, "id": "ICLR.cc/2017/conference/-/paper357/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkyQWDcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper357/reviewers", "ICLR.cc/2017/conference/paper357/areachairs"], "cdate": 1485287609825}}}, {"tddate": null, "tmdate": 1480719621234, "tcdate": 1480719621230, "number": 1, "id": "B1T-puJ7x", "invitation": "ICLR.cc/2017/conference/-/paper357/pre-review/question", "forum": "SkyQWDcex", "replyto": "SkyQWDcex", "signatures": ["ICLR.cc/2017/conference/paper357/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper357/AnonReviewer2"], "content": {"title": "Few questions", "question": "1. It is said a end-to-end model for IQA, but in the training IQA, it is not just given question and answer. For each step, it is supervised with the feedback questions, which is similar to the Memory Network paper, not the End-to-end one.\n\n2. In the attention model, at equ 6 and 9, it used the question representation U to calculate both word-level and sentence -level attention, why U could represent information in both word-level and sentence level, or word-leve and sentence-level information are in the same space?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Context-aware Attention Network for Interactive Question Answering", "abstract": "We develop a new model for Interactive Question Answering (IQA), using Gated-Recurrent-Unit recurrent networks (GRUs) as encoders for statements and questions, and another GRU as a decoder for outputs. Distinct from previous work, our approach employs context-dependent word-level attention for more accurate statement representations and question-guided sentence-level attention for better context modeling. Employing these mechanisms, our model accurately understands when it can output an answer or when it requires generating a supplementary question for additional input. When available, user's feedback is encoded and directly applied to update sentence-level attention to infer the answer. Extensive experiments on QA and IQA datasets demonstrate quantitatively the effectiveness of our model with significant improvement over conventional QA models.", "pdf": "/pdf/060e7dd9c5df591171ee87bfff2b0456c7a317d4.pdf", "TL;DR": "A self-adaptive QA model aware of what it knows and what it does not know for interactive question answering.", "paperhash": "li|a_contextaware_attention_network_for_interactive_question_answering", "keywords": ["Deep learning", "Natural language processing", "Applications"], "conflicts": ["uncc.edu", "email.arizona.edu", "nec-labs.com"], "authors": ["Huayu Li", "Martin Renqiang Min", "Yong Ge", "Asim Kadav"], "authorids": ["hli38@uncc.edu", "renqiang@nec-labs.com", "yongge@email.arizona.edu", "asim@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959324871, "id": "ICLR.cc/2017/conference/-/paper357/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper357/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper357/AnonReviewer2"], "reply": {"forum": "SkyQWDcex", "replyto": "SkyQWDcex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper357/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper357/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959324871}}}, {"tddate": null, "tmdate": 1479835164887, "tcdate": 1479835164882, "number": 2, "id": "SyS7RxGMx", "invitation": "ICLR.cc/2017/conference/-/paper357/public/comment", "forum": "SkyQWDcex", "replyto": "BkxOulC-g", "signatures": ["~Huayu_Li1"], "readers": ["everyone"], "writers": ["~Huayu_Li1"], "content": {"title": "Fix Typos", "comment": "Thanks for your comments. We have corrected the typos in the revision."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Context-aware Attention Network for Interactive Question Answering", "abstract": "We develop a new model for Interactive Question Answering (IQA), using Gated-Recurrent-Unit recurrent networks (GRUs) as encoders for statements and questions, and another GRU as a decoder for outputs. Distinct from previous work, our approach employs context-dependent word-level attention for more accurate statement representations and question-guided sentence-level attention for better context modeling. Employing these mechanisms, our model accurately understands when it can output an answer or when it requires generating a supplementary question for additional input. When available, user's feedback is encoded and directly applied to update sentence-level attention to infer the answer. Extensive experiments on QA and IQA datasets demonstrate quantitatively the effectiveness of our model with significant improvement over conventional QA models.", "pdf": "/pdf/060e7dd9c5df591171ee87bfff2b0456c7a317d4.pdf", "TL;DR": "A self-adaptive QA model aware of what it knows and what it does not know for interactive question answering.", "paperhash": "li|a_contextaware_attention_network_for_interactive_question_answering", "keywords": ["Deep learning", "Natural language processing", "Applications"], "conflicts": ["uncc.edu", "email.arizona.edu", "nec-labs.com"], "authors": ["Huayu Li", "Martin Renqiang Min", "Yong Ge", "Asim Kadav"], "authorids": ["hli38@uncc.edu", "renqiang@nec-labs.com", "yongge@email.arizona.edu", "asim@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287609825, "id": "ICLR.cc/2017/conference/-/paper357/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkyQWDcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper357/reviewers", "ICLR.cc/2017/conference/paper357/areachairs"], "cdate": 1485287609825}}}, {"tddate": null, "tmdate": 1479571560366, "tcdate": 1479571560360, "number": 1, "id": "BkxOulC-g", "invitation": "ICLR.cc/2017/conference/-/paper357/public/comment", "forum": "SkyQWDcex", "replyto": "SkyQWDcex", "signatures": ["~Erfan_Noury1"], "readers": ["everyone"], "writers": ["~Erfan_Noury1"], "content": {"title": "One or two typos", "comment": "Hello,\n\nIt's an interesting paper. However, I think there are a number of typos in the paper that fixing them would make the paper more clear.\n1. In Equation 14, both 't' should be uppercase, to indicate transpose operation, i.e. it should be changed to $\\mathbf{u}^T \\mathbf{s}_t + \\mathbf{u}^T \\mathbf{r}$.\n2. In section 4.5, \"Training Procedure\", you have indicated that a single GRU is used to encode the question, input sentences, and user's feedback. However, above Equation 11 you indicate that hidden state size of feedback sentence is $K_f$. As it is not indicated that $K_f$ is equal to $K_h$, therefore it is either a typo or you should indicate in section 4.5 that by taking these two hidden sizes to be equal you have been able to use a single GRU to encode all these sequences."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Context-aware Attention Network for Interactive Question Answering", "abstract": "We develop a new model for Interactive Question Answering (IQA), using Gated-Recurrent-Unit recurrent networks (GRUs) as encoders for statements and questions, and another GRU as a decoder for outputs. Distinct from previous work, our approach employs context-dependent word-level attention for more accurate statement representations and question-guided sentence-level attention for better context modeling. Employing these mechanisms, our model accurately understands when it can output an answer or when it requires generating a supplementary question for additional input. When available, user's feedback is encoded and directly applied to update sentence-level attention to infer the answer. Extensive experiments on QA and IQA datasets demonstrate quantitatively the effectiveness of our model with significant improvement over conventional QA models.", "pdf": "/pdf/060e7dd9c5df591171ee87bfff2b0456c7a317d4.pdf", "TL;DR": "A self-adaptive QA model aware of what it knows and what it does not know for interactive question answering.", "paperhash": "li|a_contextaware_attention_network_for_interactive_question_answering", "keywords": ["Deep learning", "Natural language processing", "Applications"], "conflicts": ["uncc.edu", "email.arizona.edu", "nec-labs.com"], "authors": ["Huayu Li", "Martin Renqiang Min", "Yong Ge", "Asim Kadav"], "authorids": ["hli38@uncc.edu", "renqiang@nec-labs.com", "yongge@email.arizona.edu", "asim@nec-labs.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287609825, "id": "ICLR.cc/2017/conference/-/paper357/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkyQWDcex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper357/reviewers", "ICLR.cc/2017/conference/paper357/areachairs"], "cdate": 1485287609825}}}], "count": 12}