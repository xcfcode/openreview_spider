{"notes": [{"id": "ByghKiC5YX", "original": "HJewSzbPK7", "number": 481, "cdate": 1538087811858, "ddate": null, "tcdate": 1538087811858, "tmdate": 1545355392685, "tddate": null, "forum": "ByghKiC5YX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 32, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "H1xfMh9lgN", "original": null, "number": 1, "cdate": 1544756233879, "ddate": null, "tcdate": 1544756233879, "tmdate": 1545354518488, "tddate": null, "forum": "ByghKiC5YX", "replyto": "ByghKiC5YX", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Meta_Review", "content": {"metareview": "I appreciate the willingness of the authors to engage in vigorous discussion about their paper. Although several reviewers support accepting this submission, I do not find their arguments for acceptance convincing. The paper considers automated methods for finding errors in text classification models. I believe it is valuable to study the errors our models make in order to understand when they work well and how to improve them. Crucially, in the later case, we should demonstrate how to use the errors we find to close the loop and create better models.\n\nA paper about techniques to find errors for text models should make a sufficiently large contribution to be accepted. I view the following hypothetical contributions as the most salient in this specific case thus my decision reduces to determining if any of these conditions have been met. A paper need not achieve all of these things, any one of them would suffice:\n\n1. Show that the errors found can be used to meaningfully improve the models. \n\nThis requires building a better model than the one probed by the method and convincingly demonstrating that it is superior in an important way that is relevant to the original goals of the application. Ideally it would also consider alternative, simpler ways to improve the models (e.g. making them larger).\n\n2. Show that errors are difficult to find, but that the proposed method is nonetheless capable of finding errors and that the method is non-obvious to a researcher in the field.\n\nThis is not applicable here because errors are extremely easy to find on the test set and from labeling more data. If we demand an automated method, then the greedy algorithm does not qualify as sufficiently non-obvious and it seems to work fine, making the Gumbel method unnecessary.\n\n3. Show that the particular specific errors found are qualitatively different from other errors in their implications and that they provide a unique and important insight.\n\nI do not believe this submission attempts to show this type of contribution. One example of this type of paper would be a paper that does a comparative study of the errors that different models make and finds something interesting (potentially yielding a path to improved models).\n\n4. Generate a new, more difficult/interesting, dataset by finding errors of one or more trained models\n\nGiven that the authors use human labelers to validate examples this is potentially another path. Here is an example of a paper using adversarial techniques in this way: https://arxiv.org/abs/1808.05326\nHowever, I believe the paper would need to be rethought and rewritten to make this sort of contribution.\n\n\nUltimately, the authors and reviews supporting acceptance must explain the contribution succinctly and convincingly. The reviewers most strongly advocating for accepting this submission seem to be saying that there is a valuable new method and probabilistic framework proposed here for finding model errors. I believe researchers in the field could have easily come up with the greedy algorithm (a standard approach to discrete optimization problems) proposed here without needing to read the paper. Furthermore, I believe the other more complicated Gumbel algorithm proposed is not necessary given the similarly effective and simpler greedy algorithm. If the authors believe that the Gumbel algorithm provides application-relevant advantages over the greedy algorithm, then they should specify how these errors will be used and rewrite the paper to make the greedy algorithm a baseline. However, I do not believe the experimental results support this idea.\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Although several reviewers support accepting this submission, I do not find their arguments for acceptance convincing"}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper481/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353202059, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": "ByghKiC5YX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353202059}}}, {"id": "rJlr2RR-xV", "original": null, "number": 30, "cdate": 1544838828985, "ddate": null, "tcdate": 1544838828985, "tmdate": 1544838828985, "tddate": null, "forum": "ByghKiC5YX", "replyto": "ByghKiC5YX", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "Why is the topic (finding nearby errors) important? (Part 2) ", "comment": "4. In terms of security, finding the error is a way to attack the model.\n\nFrom the security perspective, the ability of finding nearby errors (adversarial examples) leads to many security threats. Several important applications including attacking self-driving cars (slightly perturbed traffic sign can fool self-driving cars), and malicious ads (small change to ads pictures can pass the ML-based blocking or ranking models, see a nice new paper at https://arxiv.org/abs/1811.03194). \n\nWe believe this motivation is still valid for text data. For instance, if an attacker has a spam email to send out but the original email cannot bypass an ML-based spam filter, he or she can slightly perturb the text to bypass spam filter. The same thing can be done in many other online applications.  \n\nOr, for instance, A sends B a message or a document, and an attacker between A, B can change a small number of bits in the message to make the ML model in B mis-classify. Number of bits could correspond to the edit distance in text data, which is one motivation for using edit distance. \n\n5. Is \u201cedit distance\u201d the best distance measurement for text adversarial example? \n\nWe think edit distance is a natural way as a distance measurement for text, but of course it might not be perfect. In general, even in image adversarial examples, there\u2019s debate on which norm should be used.  FGSM ( https://arxiv.org/pdf/1412.6572.pdf) aims to control the L_\\infty norm perturbation; C&W works for different L_p norm (https://arxiv.org/abs/1608.04644); recent papers also proposed more complicated norms such as L1-L2 norm (https://arxiv.org/abs/1709.04114), one pixel change (https://arxiv.org/abs/1710.08864), rotate/shift (https://arxiv.org/pdf/1712.02779.pdf) and semantic similarity (https://arxiv.org/pdf/1804.00499.pdf). We agree that exploiting different similarity measurement will be important, but in this paper we just focus on the most intuitive distance measurement in text and develop algorithms to find minimal adversarial perturbation. Also, we conduct human evaluation to show that minimizing edit distance works to some extent to achieve the goal that \u201cthe semantic meaning of the sentence is not changed\u201d. "}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "HJgu9RRWxE", "original": null, "number": 29, "cdate": 1544838799960, "ddate": null, "tcdate": 1544838799960, "tmdate": 1544838799960, "tddate": null, "forum": "ByghKiC5YX", "replyto": "ByghKiC5YX", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "Why is the topic (finding nearby errors) important? (Part 1)", "comment": "1. Finding the error is the first step toward fixing it.\n\nIn general, given a sample x with correct prediction, the model is robust if it outputs the same correct label for all the nearby points *within a small distance*. The \u201cdefense\u201d algorithms are proposed to improve the robustness of models, but before doing defense, it\u2019s necessary to know *how to evaluate the robustness of a model*. \n\n**There is no way to improve robustness if you don\u2019t know how to measure it.**\n\nTherefore, to evaluate the robustness of models, we need to *find nearby errors* for a given x. The AC might think doing random perturbation will work, but based on the experiments, simple random perturbation does not work for text (see our explanations in the \u201cMotivation\u201d threads) and also doesn\u2019t work for image applications. Therefore, finding nearby errors is the first step before fixing it, so this has become an important task in our community. See the list of attacks for text data in our \u201cMotivation\u201d thread, and there\u2019s much longer list of work on computer vision applications. \n\nMoreover, recently researchers also found that *the error with small perturbation can be used to improve robustness*. The strategy is called \u201cadversarial training\u201d: When training the model, we can keep finding adversarial examples and adding them into training data to train the model. Random perturbation again doesn\u2019t work well in this case, so the state-of-the-art method now works like 1) finds nearby adversarial samples based on current batch 2) run SGD on adversarial samples. See a seminal work (https://arxiv.org/abs/1412.6572) and one of the state-of-the-art methods (https://arxiv.org/abs/1706.06083). This is another reason that we want to find adversarial examples (nearby errors). \n\n2. It\u2019s not surprising that such nearby error exists. The question is how to find it. \n\nWe totally agree that it\u2019s not surprising that such nearby error exists, and it\u2019s also not our focus to show such nearby error exists. What we are doing in this paper is to propose an efficient way to find such error, which can be used to measure the robustness and identify the blind spots of the model (see our point 1).\n\n3. Attacks can often be reduced to an optimization problem. Does this mean attacks are trivial? \n\nAs the AC pointed out and we also agreed, finding adversarial example (based on edit distance) for text classification can be formulated as a discrete optimization problem. Actually this is the case for most attacks. To attack a machine learning model, we want to \n\nfind x\u2019 \\in Ball(x, \\epsilon) to *maximize* Loss(f(x\u2019), y)\n\nAnd this is naturally a constrained optimization problem. For image classification, x\u2019 is in the continuous and bounded space. The seminal paper (https://arxiv.org/pdf/1312.6199.pdf) proposed to solve this by LBFGS. State-of-the-art  C&W (https://arxiv.org/abs/1608.04644) attack is based on the similar formulation with different loss functions. Given this, there are still tons of papers proposing different attacks in ICLR, in both black-box and white-box settings. \n\nThe AC pointed out that \u201cSince a standard greedy algorithm works, there can't be anything special about this particular optimization problem that standard methods can't handle.\u201d So does this imply there\u2019s no contribution to apply an existing optimization algorithm to solve an ML problem? We disagree. \n\nIn general, we think \u201cshowing that an optimization algorithm works well for an ML problem\u201d itself is an important contribution. There has been many works trying to apply existing optimization algorithms to ML models, such as SVM optimization, graphical models, sparse recovery, low-rank recovery, and these work have led to faster training/inference and many easy-to-use ML packages that were really beneficial to the community. Furthermore, when people apply some existing algorithm to some ML problem, they usually need to slightly change the algorithm to exploit the structure of problem, which is very important in practice. \n\nWe can see the same trend in the research of adversarial attacks, in both white-box setting (LBFGS is used initially, and then gradient descent, and then Adam), and black-box setting (coordinate descent is used initially, and then NES, genetic algorithm, etc). In our case, the algorithm is based on greedy optimization but has some treatments to make it adapt to text attack, see the discussion thread \u201cEfficiency of Gumbel Attack; Difference and Connections in Discrete Optimization and Adversarial Attack.\u201d Furthermore we show how to attack efficiently using the Gumbel trick, which is also an interesting finding. \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "Ske46vsZeE", "original": null, "number": 4, "cdate": 1544824764233, "ddate": null, "tcdate": 1544824764233, "tmdate": 1544833167200, "tddate": null, "forum": "ByghKiC5YX", "replyto": "HJl-l_1-lN", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Public_Comment", "content": {"comment": "I'm not reviewing this paper, but I'm interested in this discussion. It sounds like you are arguing that the topic of small worst-case perturbations is interesting because lots of people find it interesting. Can you be more specific? I'm not asking about adv ex as a whole, but more specifically papers which identify the minimum perturbation without much other contribution. What are we learning from the minimum perturbation, other than there are inputs for which the model makes a mistake and we found the nearest one? You are saying the distance of the minimum perturbation isn't surprising and I agree, so then what is the point of identifying the minimum perturbation?\n\nI'm all for an error analysis assuming we learn something from them. Maybe for text we could identify inherent biases of the model. For example, it's probably not a coincidence that in Table 3 modifying a word associated with sentiment (better) changed the model prediction. Just speculating but perhaps if the authors reran the random replacement experiment but with a bias towards inserting words associated with sentiment they would find random replacement would be more likely to degrade the performance of the model. What happens if you randomly append a bunch of sentiment related words as the last sentence of the paragraph? Identifying such a sentiment bias could be interesting and potentially useful.\n\nAs a concrete example for computer vision, I really liked this paper which identifies a texture bias for computer vision models: https://openreview.net/pdf?id=Bygh9j09KX. ", "title": "I don't understand either"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311830237, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ByghKiC5YX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311830237}}}, {"id": "r1gzkf3Zx4", "original": null, "number": 27, "cdate": 1544827353891, "ddate": null, "tcdate": 1544827353891, "tmdate": 1544827792001, "tddate": null, "forum": "ByghKiC5YX", "replyto": "Ske46vsZeE", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "Your proposed experiments are interesting!", "comment": "Wow! Thanks for proposing these interesting experiments! I personally agree with you that they are worth to investigate. Actually your thoughts point to some directions we are thinking about. I'd like to share with you some more of my personal thoughts, not necessarily related to the paper. \n\nYou mentioned \"papers which identify the minimum perturbation without much other contribution\" may not be interesting. I agree with you. Experiments also need to be carried out to see whether humans can make the right decision after this, how to do this efficiently in the setting of adversarial examples, etc. \n\nAs AC said: \" this is not surprising in the discrete case because the models considered certainly have a non-trivial amount of test error in the data distribution.\" Yeah, that's truth! But does it suffice to know the test error exists? No, it is not. We need to investigate what the error is, or 'what is the inherent bias' as you mentioned, more importantly, 'how to characterize that bias?' Finally, 'how to fix that?'\n\nFundamentally, the first thing we care about is: \"How do we define the bias of a model?\" I think maybe we can summarize in an inaccurate language: if most humans think the label of an instance is A, but the label is B, then the model has a bias. After that, we can proceed to find out \"what is a precise way to summarize the bias of existing models\". Maybe we can use math language, maybe we can summarize with other domain-specific abstraction (e.g.: texture). In the end, equipped with the knowledge, we proceed to fix the models. Perhaps one can use adversarial training? Or add some simple rules? So many potentially interesting directions there waiting for us!\n\nOne more thing, although you pointed me to a nice paper, I feel sad about the cat in that paper who was enforced to wear the elephant skin:)\n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "HJl-l_1-lN", "original": null, "number": 26, "cdate": 1544775656778, "ddate": null, "tcdate": 1544775656778, "tmdate": 1544775656778, "tddate": null, "forum": "ByghKiC5YX", "replyto": "S1xyRzcxeV", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "of course minimum perturbations will be smaller than random perturbations ", "comment": "I did not mean to say that it's surprising that minimum adversarial perturbations are smaller than random perturbations. My comment was meant to second Nicholas in the importance of minimum adversarial perturbations. Just like him I'd like to ask you whether your arguments against the importance of this work would also hold for any paper on adversarials, whether on text or images? For image-based adversarial attacks and defenses there have been dozens of papers this year and the topic is important to many."}, "signatures": ["ICLR.cc/2019/Conference/Paper481/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "S1xyRzcxeV", "original": null, "number": 25, "cdate": 1544753862540, "ddate": null, "tcdate": 1544753862540, "tmdate": 1544753862540, "tddate": null, "forum": "ByghKiC5YX", "replyto": "Skg9KAvmTm", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "I still don't understand your argument", "comment": "Are you saying that finding nearby (in edit distance) errors has specific value? When do we need this in text?\n\nAre you saying that we should be surprised that the minimum perturbation in the Lp ball case is smaller than randomly found perturbations? If so, I disagree that this is surprising (see the thread above with Nicholas Carlini as well as the convincing paper he linked that supports this point). I also don't think this is surprising in the discrete case because the models considered certainly have a non-trivial amount of test error in the data distribution."}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "HkxPdeCh14", "original": null, "number": 24, "cdate": 1544507503413, "ddate": null, "tcdate": 1544507503413, "tmdate": 1544507503413, "tddate": null, "forum": "ByghKiC5YX", "replyto": "r1lwJY3tyE", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "We agree to disagree.", "comment": "We express our sincere thanks to Reviewer 3 for the support. We think adversarial examples on texts are interesting to study, as is explained by Reviewer 3, as well as for the reasons we explained in previous posts.\n\nOn the other hand, we also understand it is natural for different people to be excited about different areas, and to feel certain pieces of work less interesting. We still appreciate Reviewer 4 for reading our rebuttal."}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "r1lwJY3tyE", "original": null, "number": 23, "cdate": 1544304862947, "ddate": null, "tcdate": 1544304862947, "tmdate": 1544304862947, "tddate": null, "forum": "ByghKiC5YX", "replyto": "BkeaGveDJV", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "Do the same arguments apply to vision?", "comment": "Your arguments strike me as being equally applicable to adversarial images: there, a fairly small amount of salt & pepper noise is usually sufficient to fool DNN classifiers on ImageNet. Still, there are literally dozens of publications each conference looking at this problem. I fail to see why adversarials on text are less interesting than adversarials in the image domain."}, "signatures": ["ICLR.cc/2019/Conference/Paper481/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "BkeaGveDJV", "original": null, "number": 22, "cdate": 1544124181197, "ddate": null, "tcdate": 1544124181197, "tmdate": 1544124181197, "tddate": null, "forum": "ByghKiC5YX", "replyto": "Syg18fj11E", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "I'm then not convinced that \"adversarial examples\" in this context are interesting to study.", "comment": "\"First, we agree with the reviewer that adversarial attack on texts is at a relatively new stage compared to the counterpart on images.\"\n\nI brought this point up not because the methods introduced in this paper are not efficient enough at producing adversarial examples. I brought it up because I am not convinced that the distortions proposed in this paper, called adversarial examples by the authors, are significant enough for an ICLR acceptance. \n\n\"As an example, it does not make sense to reject a paper because their model achieves a lower accuracy on ImageNet than the accuracy of a very simple model on MNIST. \"\n\nI agree that these distortions on text data are not quantitatively comparable to similar distortions on image data. My concern is that finding distortions that fool text classifiers by itself is not a significant enough development. Almost all machine learning models fail to generalize to some small distortions (\"small\" as defined by distortions that would not fool humans). The authors present the distortions in this paper as especially worthy of study, as they fall under the category of \"adversarial examples\". I unfortunately do not find this convincing.    "}, "signatures": ["ICLR.cc/2019/Conference/Paper481/AnonReviewer4"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/AnonReviewer4", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "Syg18fj11E", "original": null, "number": 17, "cdate": 1543643718739, "ddate": null, "tcdate": 1543643718739, "tmdate": 1543986669519, "tddate": null, "forum": "ByghKiC5YX", "replyto": "HyeQu_ns07", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We thank the reviewer for reading our paper and giving detailed comments on our paper. However, we observe the review is posted after the deadline of paper modification has passed, and hope to address a few points of this review. \n\nIn short,\n1. We think the comparison between adversarial attack on images and on texts is unfair.\n2. Some of the questions in the review are highly correlated with AC\u2019s questions and we have answered in our previous rebuttal. \n\nWe now address them in details. \n\nFirst, we agree with the reviewer that adversarial attack on texts is at a relatively new stage compared to the counterpart on images. However, to evaluate our work, we think it makes more sense to compare our methods with the best methods in this area, instead of with methods on a different data set. As an example, it does not make sense to reject a paper because their model achieves a lower accuracy on ImageNet than the accuracy of a very simple model on MNIST. \nWe have shown that our method outperforms previous text adversarial attack algorithms in Figure 3, and we have even compared all methods under human evaluation in Appendix B, which indicates that humans are least sensitive to adversarial examples generated by our algorithm. So we believe our method can advance state-of-the-art in attacks on texts. \n\n\u201cThe attacks on character-based models are closer to adversarial examples from this perspective.\u201d\nWe aim to propose a general mathematical framework to generate adversarial examples for models with discrete input. Thus, the same algorithm works for both character-based and word-based models, and could be potentially useful for other NLP models such as word-piece models. We are happy to see that the reviewer consider character-based adversarial examples more interesting.\n\n\u201cThe Greedy attack is a straightforward application of greedy optimization on discrete data and is not very novel or interesting.\u201d\nSee our rebuttal to AC (point 2) in  \u201cEfficiency of Gumbel Attack; Difference and Connections in Discrete Optimization and Adversarial Attack.\u201d We will also elaborate this in our final version.\n\nThe reviewer also proposed to include an experiment on how our attacks perform on models trained with data augmentation techniques. We agree with the reviewer that the proposed experiment can be interesting. However, given the timeline, we are not able to update the paper now. We are willing to add it in our final version."}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "HyeQu_ns07", "original": null, "number": 4, "cdate": 1543387243234, "ddate": null, "tcdate": 1543387243234, "tmdate": 1543387243234, "tddate": null, "forum": "ByghKiC5YX", "replyto": "ByghKiC5YX", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Review", "content": {"title": "Review", "review": "This paper introduces two new methods for generating adversarial examples for text classification models. The paper is well written, the introduced algorithms and experiments are easy to understand. \n\nHowever, I do not believe that these two methods are sufficiently significant. First of all, I am not convinced that the attacks can be classified as \u201cadversarial examples\u201d, especially the ones on the word-based models. The community originally got interested in adversarial examples because while they can easily be classified correctly by humans, they seemed to fool machine learning models with high efficiency. For example, the PGD attack by Madry et al. can reduce the accuracy of a CIFAR-10 model to 0% by using distortions that are not at all noticeable to humans. In the case of the word-based task studied here, human accuracy drops by 8-11%. \n\nWhile the question of whether adversarial examples are actually a security threat is under debate, the attacks on the word-based models here do not even classify as adversarial examples. Of course, it is interesting that the ML models are much less robust to these distortions than humans are, however, this is a well known problem. This paper did not perform comprehensive experiments to investigate this phenomenon. For example, they could have evaluated a wide range of distortions (including random distortions), and then check if training with all of these distortions makes the network more robust \u2026 etc (for example, see [1]).  \n\nThe attacks on character-based models are closer to adversarial examples from this perspective. However, the performance of the Gumbel Attack is significantly worse on character-based models than an attack as simple as the Delete-1 attack. The Greedy attack is more successful than the Delete-1 attack, however it is a straight-forward application of greedy optimization on discrete data and is not very novel or interesting. \n\n[1] Generalisation in humans and deep neural networks, arXiv:1808.08750 ", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper481/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Review", "cdate": 1542234451584, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByghKiC5YX", "replyto": "ByghKiC5YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335734034, "tmdate": 1552335734034, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkgBBnMApX", "original": null, "number": 16, "cdate": 1542495293108, "ddate": null, "tcdate": 1542495293108, "tmdate": 1542495293108, "tddate": null, "forum": "ByghKiC5YX", "replyto": "H1e_TlAhTQ", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "Efficiency of Gumbel Attack; Difference and Connections in Discrete Optimization and Adversarial Attack.", "comment": "We thank the area chair for reading the reviews and our rebuttals carefully. We will answer the questions of Area Chair from the authors\u2019 perspective. \n\nThe area chair proposes two questions:\n1) Why do we need Gumbel as a new discrete optimization algorithm?\n2) Have we improved \u201cthe state-of-the-art\u201d in discrete optimization?\n\nThe short reply is\n1)  Gumbel attack is efficient. The efficiency can be a practical concern in the setting of adversarial attack.\n2) We propose better algorithms in terms of accuracy or efficiency in the regime of adversarial attack. This regime is not exactly the same as discrete optimization. \n\nWe address the details below. \n1)  \n1.1 Gumbel attack is efficient both in terms of the number of model evaluations and in terms of real time. First, no model evaluation is required during the attack stage. Also, Figure 4 in the manuscript provides a comparison of real-time efficiency, which shows Gumbel attack is orders-of-magnitude faster. (Gumbel attack is around 10^-2 seconds per sample while FGSM, Delete-1 Score and other methods are between 10^-1s and 1s per sample on Yahoo! Answers.)\n1.2 In practice, attackers may not be able to conduct many model evaluations to attack a real system.\n1.3 It may also help design more efficient adversarial training algorithms. \n\n2)\n2.1 We first address the difference between Greedy attack and standard greedy methods. \nThe most standard greedy methods choose the first perturbation by evaluating models d * V times, where d is the length of the sentence/paragraph and V is the size of dictionary, and choose the next perturbation with complexity (d-1) *V, etc. Greedy attack follows a two-stage procedure motivated from a probabilistic framework, and takes O(d + k*V) evaluations in total (k being the number of perturbations). Moreover, Greedy attack is easier to parallelize. Given the efficiency concern of adversarial attacks, it can be more practical. \n2.2 The area of adversarial attack is not exactly the same as discrete optimization. \nWe formulate the problem of adversarial attack as a constrained discrete optimization problem. The true constraint here is that \u201chumans will not change their decisions\u201d, which we approximate by constraining the number of perturbed words.  Experiments involving human subjects have been carried out to validate the effectiveness of approximation. \n2.3 We only show the superior performance of our algorithms to algorithms in adversarial attack ([1-4]), and we do not have the intention to claim it achieves the state-of-the-art in discrete optimization. \n\n[1] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial text sequences to evade deep learning classifiers. arXiv preprint arXiv:1801.04354, 2018.\n[2] Jiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation erasure. arXiv preprint arXiv:1612.08220, 2016.\n[3] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adversarial input sequences for recurrent neural networks. In Military Communications Conference, MILCOM 2016-2016 IEEE, pp. 49\u201354. IEEE, 2016.\n[4] Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. Deep text classification can be fooled. arXiv preprint arXiv:1704.08006, 2017."}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "ryeJ9zR3pQ", "original": null, "number": 3, "cdate": 1542410887490, "ddate": null, "tcdate": 1542410887490, "tmdate": 1542410887490, "tddate": null, "forum": "ByghKiC5YX", "replyto": "HyxnjDahaX", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Public_Comment", "content": {"comment": "There's actually a paper under submission that makes exactly this argument ( https://openreview.net/forum?id=S1xoy3CcYX ). I definitely agree that it should not be surprising that models have such low accuracy when you adversarially select noise to maximize classification error rate in light of this phenomenon.\n\nI don't think this is actually contradictory to adversarial examples as a line of research. In particular, one of the main reasons I see adversarial example work as interesting is that it gives us an estimate of the worst-case accuracy. Just like average-case accuracy is useful in many situations (and standard 'accuracy-on-test-set' measures do this for us), worst-case accuracy is also useful in other cases.\n\nTo relate it back to this paper's topic of discrete data (again, I haven't read the paper) a classifier for malware that worked 100% of the time on \"normal\" data would be useless if it worked 0% of the time on adversarial data---because the only data it will ever see, malware, is by definition adversarial. The same argument applies to spam, and to a lesser extent various other written text attempting to avoid detection (e.g., the recent hate speech detectors).\n\nJust to clarify, though: it sounds like your \"Why is the task important?\" question is generally directed at the adversarial example research as a whole. Is this right? There are, by my count, at least 60 papers under submission to ICLR this year that focus explicitly on the problem of adversarial examples (explaining their existence, approaches for generating them, and approaches for defending against them). Would you have a similar complaint about any of these other papers?", "title": "I agree with your perspective"}, "signatures": ["~Nicholas_Carlini1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Nicholas_Carlini1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311830237, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ByghKiC5YX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311830237}}}, {"id": "H1x-WZR2T7", "original": null, "number": 15, "cdate": 1542410489404, "ddate": null, "tcdate": 1542410489404, "tmdate": 1542410489404, "tddate": null, "forum": "ByghKiC5YX", "replyto": "SkxNEow7Tm", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "Thanks for elaborating", "comment": "Thanks for elaborating on the motivation you have for the work. It is very helpful."}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "H1e_TlAhTQ", "original": null, "number": 14, "cdate": 1542410432388, "ddate": null, "tcdate": 1542410432388, "tmdate": 1542410432388, "tddate": null, "forum": "ByghKiC5YX", "replyto": "ByghKiC5YX", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "Can the reviewers please clarify the contribution(s)?", "comment": "As defined in this paper, an adversarial attack is just solving an optimization problem. For discrete sequence inputs, the paper considers a constrained discrete optimization problem. Discrete optimization is well studied and greedy algorithms for discrete optimization are also well-known and well-studied methods. They are obvious to machine learning practitioners as well. The particular greedy algorithm the authors use seems to be effective for this problem and does not require any special tricks.\n\nCould the reviewers especially please comment on the following questions:\n\n1. Is the Gumbel algorithm proposed necessary here or, more generally, is a new discrete optimization algorithm needed here?\n\n2. The discussion section of the paper says:\n\"We have proposed a probabilistic framework for generating adversarial examples on discrete data, based on which we have derived two algorithms. Greedy Attack improves the state-of-the-art across several widely-used language models, and Gumbel Attack provides a scalable method for real-time generation of adversarial examples.\"\n\nThe paper claims to improve the state of the art. Can any of the reviewers comment on whether the paper advanced the state of the art in discrete optimization? Or, more generally, how should we read the claim above? Since a standard greedy algorithm works, there can't be anything special about this particular optimization problem that standard methods can't handle.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "HyxnjDahaX", "original": null, "number": 13, "cdate": 1542408099676, "ddate": null, "tcdate": 1542408099676, "tmdate": 1542408099676, "tddate": null, "forum": "ByghKiC5YX", "replyto": "BJgwIBu7am", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "thanks for your comment", "comment": "Thanks for weighing in, Nicolas, but I'm not sure I understand your argument.\n\nNeither of your first two points should surprise us when the models have substantial test error.\n\nTo put it another way, 50% of random sigma=.2 perturbations are misclassified for ImageNet and adversarially chosen errors can be 20x closer than these randomly found errors. Of *course* the nearest error is going to be significantly closer than randomly found errors, the nearest error is, by construction, the nearest error! Why is 20x closer unusually close in a high (~150,000) dimensional space? \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "BJxl0Dtjpm", "original": null, "number": 12, "cdate": 1542326215576, "ddate": null, "tcdate": 1542326215576, "tmdate": 1542326299830, "tddate": null, "forum": "ByghKiC5YX", "replyto": "ByghKiC5YX", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "UPDATED: Elaborate Section 3.1 and add new human evaluation based on the reviewers\u2019 suggestions.", "comment": "We have elaborated the Greedy attack with a clearer presentation in Section 3.1. First, we have adopted the common notations and marked each expectation with a subscript to indicate the source of the expectation. Second, in the updated version, we have added a detailed explanation of the approximation in Equation (5, 7).  To summarize, when one assumes other features are perturbed adversarially, the Greedy Attack can be interpreted as maximizing a lower bound of the original objectives.\n\nWe have added another experiment to compare various algorithms with human evaluation on the IMDB movie review data set. On each instance, we increase the number of words to be perturbed until the prediction of the model changes. Then we ask humans to label original texts and perturbed texts. Greedy attack yields the best performance in the experiment. Please see Appendix B of the updated version for details.\n\nWe again express our sincere thanks to all the reviewers, who have provided very useful suggestions for helping build our manuscript into a better shape."}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "Hkx5iwFoam", "original": null, "number": 11, "cdate": 1542326178172, "ddate": null, "tcdate": 1542326178172, "tmdate": 1542326178172, "tddate": null, "forum": "ByghKiC5YX", "replyto": "HJlmF5xq27", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for the detailed and encouraging comments! \n\nTo address the reviewer\u2019s concern on Equation 5, we have added a more rigorous and detailed explanation of the approximation. Roughly, when one assumes other features are perturbed adversarially, the Greedy Attack can be interpreted as maximizing a lower bound of the original objectives. Details can be found in Section 3.1 of the updated version. "}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "S1x8tPKiam", "original": null, "number": 10, "cdate": 1542326142360, "ddate": null, "tcdate": 1542326142360, "tmdate": 1542326142360, "tddate": null, "forum": "ByghKiC5YX", "replyto": "S1gSsdb537", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for the encouraging comments and the help in addressing the importance of the task!\n\nWhat\u2019s the \u201crandom attack\u201d baseline in these tasks? In computer vision it\u2019s often sufficient to add a little bit of salt-and-pepper noise or Gaussian noise to change the model decision.\n\nWe define \u201crandom attack\u201d as randomly sample k positions in the sentence, and replace them with randomly sampled words. We run random perturbation on the test set of the IMDB movie review dataset used in our paper. The average consistency of the predictions of the model from the perturbed and the original instances is 99.9% after k = 10 words are changed, and 92%, 90.4% after k = 50, 100 words are changed respectively. See the following link for a plot of comparison with our algorithms (on the first five words): https://drive.google.com/file/d/1T6UJQPz4iDFqsK9XQZ0nYv-bBcYxWraP/view?usp=sharing. \nWe conclude that random perturbation does not work.  \n\n\u201cWhat the human evaluation scores would be on adversarials from other adversarial attacks?\u201d \n\nWe have added another experiment to compare various algorithms with human evaluation on the IMDB movie review data set. On each instance, we increase the number of words to be perturbed until the prediction of the model changes. Then we ask humans to label original texts and perturbed texts. Greedy attack yields the best performance in the experiment. Please see Appendix B of the updated version for details.\n\n\u201cAre you planning to release the code? Will it be part of CleverHans or Foolbox?\u201d\n\nYes, we plan to release the code. We will either release the code in a stand-alone github repository or merge it into CleverHans."}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "B1lXIwYsT7", "original": null, "number": 9, "cdate": 1542326090778, "ddate": null, "tcdate": 1542326090778, "tmdate": 1542326090778, "tddate": null, "forum": "ByghKiC5YX", "replyto": "BJekkfSq3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thanks the reviewer for the comments, and the explanations on the motivation of the task. We have improved the clarity of Section 3.1 based on the reviewer\u2019s suggestions. Below we respond in detail to the reviewer\u2019s comments:\n\n\u201cthe Gumbel method performs poorly compared to other baselines\u201d\n\nWe agree that the performance of the Gumbel method is comparable to previous methods. However, its running time is significantly shorter than all the previous methods and our Greedy Attack method (See Figure 4). Thus, Gumbel Attack is the most efficient one across all methods even after taking into account the training stage. The efficiency of generating adversarial examples is an important factor for large-scale data.  \n\n\"what is causing their greedy approach to perform better\u201d than \u201csome gradient based adversarial attacks\u201d?\n\nWhile gradient-based methods have led to several successful algorithms in the continuous domain (e.g., natural images), they have been observed to be less effective compared to discrete methods (e.g., [1]). It is mainly because gradient based methods focus on the sensitivity of response to each feature in the infinitesimal space, while perturbation is carried out in discrete space. \n\n\u201cit is egregiously difficult to read in parts and is poorly written\u201d\n\nWe apologize for the difficulty of reading and have addressed the problem carefully. First, we have adopted the common notations and marked each expectation with a subscript to indicate the source of the expectation. Second, in the updated version, we have added a clearer and more detailed explanation of the approximation in Equation (5, 7).  To summarize, when one assumes other features are perturbed adversarially, the Greedy Attack can be interpreted as maximizing a lower bound of the original objectives.\n\n\u201cThe argument about approximation to the objective by considering the i positions independently is not convincing\u201d\n\nWe agree with the reviewer that this is an unnecessary assumption and have removed it from our framework (but still keep it in the design of Gumbel attack.) The independence assumption is used in Gumbel Attack for the sake of efficiency. This can be interpreted as a constraint on the search space so that decisions can be made in parallel. It can be a promising future direction to consider a framework where features are perturbed sequentially, with a termination gate [2] to control when to stop the perturbation. The latter enables the use of variable sizes of perturbation, instead of top-k perturbation.\n\n[1] Gao, Ji, et al. \"Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers.\" arXiv preprint arXiv:1801.04354 (2018)\n[2] Shen, Yelong, et al. \"Reasonet: Learning to stop reading in machine comprehension.\" Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2017."}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "H1exhKOmp7", "original": null, "number": 8, "cdate": 1541798311811, "ddate": null, "tcdate": 1541798311811, "tmdate": 1541798311811, "tddate": null, "forum": "ByghKiC5YX", "replyto": "SJx8hts16X", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "Poor mathematical exposition", "comment": "Organizationally, the paper is fine and sections are presented in a logical manner. But as I mentioned in my review, the mathematical exposition is certainly non-conventional and maybe even wrong. Their notations (Expectation symbols, conditional symbols etc.) have serious issues and I found their arguments about approximation and assumptions hard to follow. I'm not convinced about their argument of  approximating their proposed schemes by considering each position independently (partly because I can't clearly follow their argument) and moreover I believe that their original non-approximated probabilistic (unclear) formulation is unnecessary because it doesn't add anything to the paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper481/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "SJepNwO7pX", "original": null, "number": 7, "cdate": 1541797685382, "ddate": null, "tcdate": 1541797685382, "tmdate": 1541797685382, "tddate": null, "forum": "ByghKiC5YX", "replyto": "Hkxb5b3kpX", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "Adversarial examples help find systems' blind spots", "comment": "The task is important because it focuses on small perturbations to text that change the classifier decision. These changes, if undetected by humans,  exhibit clear brittleness of a classifier and will aid in better design of a more robust classifier. This is the motivation for generating adversarial attacks in general: perturb the input ever so slightly in a manner that is in general undetectable to the human eye but results in drastic change in model's predictions. This has implications ranging from interpretablility and reliability of the model to security and privacy issues.\nIn the Mturk experiments, the authors do demonstrate that a lot of the perturbations produced by their models do not change the human's decision on sentiment classification but does change the model prediction."}, "signatures": ["ICLR.cc/2019/Conference/Paper481/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "BJgwIBu7am", "original": null, "number": 2, "cdate": 1541797199061, "ddate": null, "tcdate": 1541797199061, "tmdate": 1541797199061, "tddate": null, "forum": "ByghKiC5YX", "replyto": "Hkxb5b3kpX", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Public_Comment", "content": {"comment": "[Disclaimer: I have not read the paper. This comment is solely intended to respond to the AC asking why the problem domain is important.]\n\nAdversarial example research papers have always had to deal with this question: why is this interesting? we already know classifiers make mistakes!\n\nThere are at least a few common counter arguments:\n\n-  Yes, models make mistakes, but they are on average quite good. The interesting property of adversarial examples is that you can take an arbitrary input, that is very clearly Class A, and make the model produce the label for Class B. You can do this even when the object in Class A is the most A-like in the entire dataset. And even if class B resembles nothing like class A. That's what makes the domain interesting.\n\n- Why bother trying to find strong attacks if random noise might work? The main counter-argument here is that random noise often has to have a significantly larger distortion than adversarial noise. With Gaussian noise with sigma=0.2 on ImageNet, models still reach modest (50%+) accuracy. Adversarial noise with norm 20x smaller can reduce model accuracy to <1%.\n\n- Is this actually a security problem? It depends on the situation. For a nice treatment of this question see https://arxiv.org/abs/1807.06732", "title": "Motivating adversarial example research"}, "signatures": ["~Nicholas_Carlini1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Nicholas_Carlini1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311830237, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ByghKiC5YX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311830237}}}, {"id": "Skg9KAvmTm", "original": null, "number": 6, "cdate": 1541795458029, "ddate": null, "tcdate": 1541795458029, "tmdate": 1541795458029, "tddate": null, "forum": "ByghKiC5YX", "replyto": "SkxNEow7Tm", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "I second the importance", "comment": "I'd like to second the importance of this work. Of course random perturbations at some point will also do the trick - but the same is true in computer vision applications where often small amounts of Gaussian noise lead to misclassifications. Nonetheless, many people in CV study adversarial perturbations as a means to understand what concepts network models have learnt and how susceptible they really are. Minimum adversarial perturbations are often several orders of magnitude smaller than random noise in CV, and the same seems to be true on discrete data like text."}, "signatures": ["ICLR.cc/2019/Conference/Paper481/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "SkxNEow7Tm", "original": null, "number": 4, "cdate": 1541794603589, "ddate": null, "tcdate": 1541794603589, "tmdate": 1541794603589, "tddate": null, "forum": "ByghKiC5YX", "replyto": "Hkxb5b3kpX", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "Motivation", "comment": "Dear Area Chair and Anonymous Reader:\n\nThanks for your questions on the motivation of adversarial attack for discrete data. Below we briefly explain the motivation, followed by the evidence that simple random perturbation does not work. \n\nIn summary, the area chair and another reader posed the following questions\uff1a\n\n1. Why does one need to study the phenomenon of adversarial examples on discrete data?\n2. Why is this paper worth reading?\n3. Do simple methods like random perturbation work on text data?   \n\nIn short, our reply is\n\n1. Robustness is an important criterion for models on discrete data. The generation of adversarial examples can be used to evaluate robustness or even improve robustness.\n2. In this paper, our goal is to propose methods with better performance (Greedy attack) or with higher efficiency (Gumbel attack).\n3. We provide evidence that simple methods like random perturbation do not work. \n\nBelow are concrete details:\n\nRobustness is an important criterion for the application of machine learning models in critical areas such as medicine, financial markets, recommendation systems, and criminal justice. Adversarial examples have been used to evaluate the (adversarial) robustness of models (e.g., [1, 2, 5]) and have also been applied to train robust models (e.g., [3, 4]).\n\nThe phenomenon of adversarial examples was first found in state-of-the-art deep neural network models for classifying images (e.g., [5, 6, 2]), where small perturbations unobservable by human can easily fool neural networks. Similar to image data, the problem of adversarial perturbation on discrete data can be defined as altering the prediction of a model via minimal perturbation to an original sample (e.g., [7-14]).  \n\nWhile there have been many pioneered and interesting papers in this area (e.g., [7-14]), we proposed Greedy attack, a method to increase the misclassification rate of a model with a comparable scale of perturbation, and Gumbel attack, a method to improve the efficiency of generating adversarial examples, (It just happens to be fashionable :) ).\n\nIt is natural to ask how the simplest algorithm, random perturbation, works before one is persuaded to read our paper. We compare our methods with random perturbation on the test set of the IMDB movie review dataset used in our paper. For each instance, we randomly sample k positions in the sentence, and replace them with randomly sampled words. The average consistency of the predictions of the model from the perturbed and the original instances is 99.9% after k = 10 words are changed, and 92%, 90.4% after k = 50, 100 words are changed respectively. See the following link for a plot of comparison: https://drive.google.com/file/d/1T6UJQPz4iDFqsK9XQZ0nYv-bBcYxWraP/view?usp=sharing. \nWe conclude that random perturbation does not work. \n\n[1] Carlini, Nicholas, and David Wagner. \"Towards evaluating the robustness of neural networks.\" 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017.\n[2] Agarwal, Chirag, et al. \"An Explainable Adversarial Robustness Metric for Deep Learning Neural Networks.\" arXiv preprint arXiv:1806.01477 (2018).\n[3] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. ICLR (2018).\n[4] Alex Kurakin, Ian Goodfellow, Samy Bengio. Adversarial machine learning at scale. ICLR 2017. \n[5] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. ICLR, 2015.\n[6] Moosavi-Dezfooli, Seyed-Mohsen, Alhussein Fawzi, and Pascal Frossard. \"Deepfool: a simple and accurate method to fool deep neural networks.\" CVPR, 2016.\n[7] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial text sequences to evade deep learning classifiers. IEEE Security and Privacy Workshops (SPW), 2018.\n[8] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2021\u20132031, 2017.\n[9] Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. IJCAI, 2018. \n[10] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adversarial input sequences for recurrent neural networks. In Military Communications Conference, MILCOM 2016-2016 IEEE, 2016.\n[11] Suranjana Samanta and Sameep Mehta. Towards crafting text adversarial samples. arXiv preprint arXiv:1707.02812, 2017.\n[12] Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, and Cho-Jui Hsieh. Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples. arXiv preprint arXiv:1803.01128, 2018.\n[13] Javid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou. Hotflip:White-box adversarial examples for text classification. ACL, 2018. \n[14] Jiwei Li, Will Monroe, Dan Jurafsky. Understanding neural networks through representation erasure.  arXiv preprint arXiv:1612.08220, 2016."}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "B1xno5Dz6X", "original": null, "number": 1, "cdate": 1541728931954, "ddate": null, "tcdate": 1541728931954, "tmdate": 1541728931954, "tddate": null, "forum": "ByghKiC5YX", "replyto": "Hkxb5b3kpX", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Public_Comment", "content": {"comment": "Random perturbations are enough to fool text classifiers so why are the authors doing this? Because Gumbel-Softmax is fashionable?", "title": "Indeed, why is this problem important?"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311830237, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ByghKiC5YX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311830237}}}, {"id": "Hkxb5b3kpX", "original": null, "number": 3, "cdate": 1541550472735, "ddate": null, "tcdate": 1541550472735, "tmdate": 1541550472735, "tddate": null, "forum": "ByghKiC5YX", "replyto": "BJekkfSq3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "Why is the task important?", "comment": "Can you please clarify why you say the task is important? It is very easy to generate errors for models of text. Attackers would not need the methods in this paper to produce Yelp reviews that a state-of-the-art text sentiment classifier got wrong. They would not need any knowledge of machine learning at all to find errors for these text classifiers."}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "SJx8hts16X", "original": null, "number": 1, "cdate": 1541548461554, "ddate": null, "tcdate": 1541548461554, "tmdate": 1541548461554, "tddate": null, "forum": "ByghKiC5YX", "replyto": "BJekkfSq3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "content": {"title": "writing quality is extremely important", "comment": "A poorly written manuscript is sufficient reason, by itself, to recommend rejecting a paper.\n\nCan you clarify how detrimental these writing problems are? Are they problems at the section and organizational level? The paragraph level in constructing clear prose? The sentence level? All of the above? Is the logical structure of the argument well-organized and easy to follow?"}, "signatures": ["ICLR.cc/2019/Conference/Paper481/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper481/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619372, "tddate": null, "super": null, "final": null, "reply": {"forum": "ByghKiC5YX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper481/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper481/Authors|ICLR.cc/2019/Conference/Paper481/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers", "ICLR.cc/2019/Conference/Paper481/Authors", "ICLR.cc/2019/Conference/Paper481/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619372}}}, {"id": "BJekkfSq3Q", "original": null, "number": 3, "cdate": 1541194198706, "ddate": null, "tcdate": 1541194198706, "tmdate": 1541533959750, "tddate": null, "forum": "ByghKiC5YX", "replyto": "ByghKiC5YX", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Review", "content": {"title": "Important task; very poorly written", "review": "This paper addresses the problem of generating adversarial examples for discrete domains like text. They propose two simple techniques:\n1) Greedy: two stage process- first stage involves finding the k words in the sentence/paragraph to perturb and second step changes the word in the positions identified in step 1.\n2) Gumbel: first approach amortized over datasets where first and second steps are parametrized and learned over the dataset with the loss being the probability of flipping the decision.\nSpecifically, for the Gumbel approach, the authors use the non-differentiable top-k-argmax output to train the module in the second step which is not ideal and it would be better to train both first and second steps jointly in an end-to-end differentiable manner.\n\nThe results show that Greedy approach is able to significantly affect the accuracy of the systems compared to other adversarial baselines. Mturk evaluation shows that for tasks like sentiment analysis, humans weren't as confused as the systems were when the selected words were changed which is encouraging. However, the Gumbel method performs poorly compared to other baselines.\nMoreover, a thorough analysis of why Greedy is doing better than some gradient based adversarial attacks is needed in the paper because it is unclear what is causing their greedy approach to perform well; is it the two-stage nature of the process?\n\nMy major gripe with the paper is that it is egregiously difficult to read in parts and is poorly written. There are dangling conditional bars in many equations (5, 7, Greedy attack etc.), unclear \"expectation (E)\" signs and many other confusing notational choices which make the math difficult to parse. I am not even sure if those equations are correctly conveying the idea they are meant to convey. I found  the algorithms to be more clearly written and realize that the text in the models and equations is unnecessarily complicated. The argument about approximation to the objective by considering the i positions independently is not convincing and their is nothing in the paper to show if the assumption is reasonable.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper481/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Review", "cdate": 1542234451584, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByghKiC5YX", "replyto": "ByghKiC5YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335734034, "tmdate": 1552335734034, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1gSsdb537", "original": null, "number": 2, "cdate": 1541179548860, "ddate": null, "tcdate": 1541179548860, "tmdate": 1541533959537, "tddate": null, "forum": "ByghKiC5YX", "replyto": "ByghKiC5YX", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Review", "content": {"title": "Exciting advance in discrete adversarial attacks", "review": "In this work the authors introduce two new state-of-the-art adversarial attacks on discrete data based on a two-stage probabilistic process: the first step identifies key features which are then replaced in the second step through choices from a dictionary.\n\nOverall the manuscript is very well written and easy to follow. The evaluation is extensive and contains all previous attacks I am aware of. The greedy attack outperforms all prior work by a large margin while the Gumbel attack works on par with the previous state-of-the-art while being significantly faster. \n\nI only have a few questions and remarks:\n\n* What\u2019s the \u201crandom attack\u201d baseline in these tasks? In computer vision it\u2019s often sufficient to add a little bit of salt-and-pepper noise or Gaussian noise to change the model decision.\n\n* Another thing I am wondering is what the human evaluation scores would be on adversarials from other adversarial attacks? Adversarial attacks in general (e.g. in computer vision) can work in two ways: one being actually changing the semantic content (thus also \u201cfooling humans) while the other changes background features / add noise to which humans are pretty insensitive (unless you add too much of it). The greedy attack does seem to change some semantics as can be seen in the increased error rate of humans (which is pretty rare for computer vision adversarials). It might be that other attacks are rather changing words or characters which are not as semantically meaningful, as would be revealed by the accompanying human scores.\n\n* Are you planning to release the code? Will it be part of CleverHans or Foolbox?\n\nOverall, I find this work to be a really exciting advance on discrete adversarial attacks.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper481/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Review", "cdate": 1542234451584, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByghKiC5YX", "replyto": "ByghKiC5YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335734034, "tmdate": 1552335734034, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJlmF5xq27", "original": null, "number": 1, "cdate": 1541175930775, "ddate": null, "tcdate": 1541175930775, "tmdate": 1541533959322, "tddate": null, "forum": "ByghKiC5YX", "replyto": "ByghKiC5YX", "invitation": "ICLR.cc/2019/Conference/-/Paper481/Official_Review", "content": {"title": "Novel probabilistic framework for making adversarial attacks on deep networks with discrete valued inputs; flexible framework that allows solving the trade-off between attack success rate and computation time", "review": "The authors proposed a novel probabilistic framework to model adversarial attacks on deep networks with discrete inputs such as text. The proposed framework assumes a two step construction of an adversarial perturbation: 1) finding relevant features (or dimensions) to perturb (Eq. 3); 2) finding values to replace the features that are selected in step 1 (Eq. 4). The authors approximate some terms in these two equations to make the optimization easier. For example, it is *implicitly* assumed that given the i-th feature is removed from consideration, the probability of attack success does not change *on average* under probabilistic *adversarial* attack on other features (Eq. 5). It is not clear why that should hold and under what conditions that assumption would be reasonable (given that the attacks on other features are adversarial, although being probabilistic). \nThe proposed framework allows one to solve the computation vs. success rate trade-off by either estimating the best attack from the network (called greedy attack Eq. 6) or using a parametric estimation that does not require model evaluation (called Gumbel attack). Experimental results suggest that Gumbel attack has better or competitive attack rate on models developed for text classification while having the most computationally efficiency among other methods. It is also noticeable that the greedy attack achieves the best success rate with a large margin among all the tested methods. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper481/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "TL;DR": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "pdf": "/pdf/76ce00e3c687855e8d1cad2e874d8fa8b537e7ca.pdf", "paperhash": "yang|greedy_attack_and_gumbel_attack_generating_adversarial_examples_for_discrete_data", "_bibtex": "@misc{\nyang2019greedy,\ntitle={Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data},\nauthor={Puyudi Yang and Jianbo Chen and Cho-Jui Hsieh and Jane-Ling Wang and Michael I. Jordan},\nyear={2019},\nurl={https://openreview.net/forum?id=ByghKiC5YX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper481/Official_Review", "cdate": 1542234451584, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ByghKiC5YX", "replyto": "ByghKiC5YX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper481/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335734034, "tmdate": 1552335734034, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper481/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 33}