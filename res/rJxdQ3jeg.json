{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488552971879, "tcdate": 1478374247930, "number": 573, "id": "rJxdQ3jeg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rJxdQ3jeg", "signatures": ["~Johannes_Ball\u00e91"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "nonreaders": null, "tmdate": 1487528431481, "tcdate": 1487528334762, "number": 1, "id": "Skv9ZPDFx", "invitation": "ICLR.cc/2017/conference/-/paper573/public/review", "forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "signatures": ["~Jeremy_Noring1"], "readers": ["everyone"], "writers": ["~Jeremy_Noring1"], "content": {"title": "Very interesting results", "rating": "7: Good paper, accept", "review": "Two things I'd like to see.\n\n1) Specifics about the JPEG and JPEG2000 implementations used, and how they were configured.  One major weakness I see in many papers is they do not include specific encoders and configuration used in comparisons.  Without knowing this, it's hard to know if the comparison was done with a suitably strong JPEG implementation that was properly configured, for example.\n\n2) The comparison to JPEG2000 is unfortunately not that interesting, since that codec does not have widespread usage and likely never will.  A better comparison would be with WebP performance.  Or, even better, both.\n\nVery nice results.  Is a software implementation of this available to play with?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1487528336198, "id": "ICLR.cc/2017/conference/-/paper573/public/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "noninvitees": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu", "ICLR.cc/2017/conference/paper573/reviewers", "ICLR.cc/2017/conference/paper573/areachairs", "~Jeremy_Noring1"], "cdate": 1487528336198}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396684610, "tcdate": 1486396684610, "number": 1, "id": "SkSfpGIue", "invitation": "ICLR.cc/2017/conference/-/paper573/acceptance", "forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This is one of the two top papers in my stack and I recommend it for oral presentation. The reviewers were particularly careful and knowledgable of the topic.", "decision": "Accept (Oral)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396685197, "id": "ICLR.cc/2017/conference/-/paper573/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396685197}}}, {"tddate": null, "tmdate": 1484136067936, "tcdate": 1484136067936, "number": 1, "id": "ry3K05XLx", "invitation": "ICLR.cc/2017/conference/-/paper573/official/comment", "forum": "rJxdQ3jeg", "replyto": "HyQHhc2Bl", "signatures": ["ICLR.cc/2017/conference/paper573/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper573/AnonReviewer4"], "content": {"title": "Update score", "comment": "Great, thanks for providing more detail. I updated my score."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287515662, "id": "ICLR.cc/2017/conference/-/paper573/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "rJxdQ3jeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper573/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper573/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper573/reviewers", "ICLR.cc/2017/conference/paper573/areachairs"], "cdate": 1485287515662}}}, {"tddate": null, "tmdate": 1484136033004, "tcdate": 1482954592422, "number": 3, "id": "BkdDD9-Be", "invitation": "ICLR.cc/2017/conference/-/paper573/official/review", "forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "signatures": ["ICLR.cc/2017/conference/paper573/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper573/AnonReviewer4"], "content": {"title": "Great progress performance-wise but missing details", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper extends an approach to rate-distortion optimization to deep encoders and decoders, and from a simple entropy encoding scheme to adaptive entropy coding. In addition, the paper discusses the approach\u2019s relationship to variational autoencoders.\n\nGiven that the approach to rate-distortion optimization has already been published, the novelty of this submission is arguably not very high (correct me if I missed a new trick). In some ways, this paper even represents a step backward, since earlier work optimized for a perceptual metric where here MSE is used. However, the results are a visible improvement over JPEG 2000, and I don\u2019t know of any other learned encoding which has been shown to achieve this level of performance. The paper is very well written.\n\nEquation 10 appears to be wrong and I believe the partition function should depend on g_s(y; theta). This would mean that the approach is not equivalent to a VAE for non-Euclidean metrics.\n\nWhat was the reason for optimizing MSE rather than a perceptual metric as in previous work? Given the author\u2019s backgrounds, it is surprising that even the evaluation was only performed in terms of PSNR.\n\nWhat is the contribution of adaptive entropy coding versus the effect of deeper encoders and decoders? This seems like an important piece of information, so it would be interesting to see the performance without adaptation as in the previous paper. More detail on the adaptive coder and its effects should be provided, and I will be happy to give a higher score when the authors do.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483449280640, "id": "ICLR.cc/2017/conference/-/paper573/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper573/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper573/AnonReviewer2", "ICLR.cc/2017/conference/paper573/AnonReviewer1", "ICLR.cc/2017/conference/paper573/AnonReviewer4", "ICLR.cc/2017/conference/paper573/AnonReviewer3"], "reply": {"forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper573/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper573/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483449280640}}}, {"tddate": null, "tmdate": 1484092491251, "tcdate": 1484092491251, "number": 6, "id": "rJQUVgQUl", "invitation": "ICLR.cc/2017/conference/-/paper573/public/comment", "forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "signatures": ["~Johannes_Ball\u00e91"], "readers": ["everyone"], "writers": ["~Johannes_Ball\u00e91"], "content": {"title": "Revised paper uploaded", "comment": "We have uploaded a revised version of the paper.  Major changes are as follows:\n\n* Introduction/Discussion: we\u2019ve made some adjustments to the text, further motivating the use of MSE (instead of perceptual error), emphasizing that the addition of uniform noise is used only for optimization (all compression results are based on quantized and entropy-coded values), and elaborating on the visual appearance of the compressed results.\n\n* Images: we\u2019ve now run tests on additional images, including the \u201cclassic\u201d examples {Barbara, Lena, Mandrill, Peppers}, as well as more of our own photographs.  All examples have been added to the online set, which has been consolidated at http://www.cns.nyu.edu/~lcv/iclr2017/ (note that results on the Kodak set are unchanged from those uploaded at the time of initial submission).  We\u2019ve also made a few changes to the choice of images shown in the paper.\n\n* Adaptive entropy coder: we\u2019ve added some details about the entropy coder to the appendix, and have included a comparison to a non-adaptive entropy estimate.\n\n* Averaged rate-distortion comparison: The averaged R-D curves originally shown in fig 5 was meant to provide a summary of typical performance, but we had been concerned from the outset that performance is quite variable across images, and that there\u2019s no well-defined method of combining such results across images. In the specific case of JPEG2000, the implementation that we used controls the R-D tradeoff by specification of either the rate (R) or the distortion (D), whereas our own coder is controlled via lambda (which specifies the coder parameters).  Averaging R-D results over images compressed at the same R does *not* give the same results as averaging them over images compressed at the same D, and neither of these is the same as averaging over images compressed with the same lambda.  Note that this does *not* present a problem for comparison of R-D curves on individual images (which, for the Kodak set, have been available online since the initial paper submission).  We\u2019ve now moved the summary plot to the appendix, augmented it to include two curves for JPEG 2000 (one averaged over images at the same quality, the other at the same rate), and we provide an explanation of these issues.  In the end, we strongly recommend that readers look at the individual compressed images and associated R-D curves online.\n\n* MS-SSIM comparisons:  We\u2019ve added MS-SSIM rate-distortion plots to each of the image pages in the appendix, as well as all online pages. These confirm what we see in the compressed images: the coder offers substantial visual improvement over JPEG2000 and JPEG, for all images and bitrates.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287515787, "id": "ICLR.cc/2017/conference/-/paper573/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJxdQ3jeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper573/reviewers", "ICLR.cc/2017/conference/paper573/areachairs"], "cdate": 1485287515787}}}, {"tddate": null, "tmdate": 1483676730751, "tcdate": 1483676730751, "number": 5, "id": "HyQHhc2Bl", "invitation": "ICLR.cc/2017/conference/-/paper573/public/comment", "forum": "rJxdQ3jeg", "replyto": "BkdDD9-Be", "signatures": ["~Johannes_Ball\u00e91"], "readers": ["everyone"], "writers": ["~Johannes_Ball\u00e91"], "content": {"title": "response to reviewer 4", "comment": "Thanks for your feedback, which prompted us to look more carefully at several aspects of the paper.\n\nRegarding novelty: You are right that the overall framework for rate--distortion optimization is the same as that presented at the Picture Coding Symposium last month, but please note that the PCS paper is very brief (5 pages), and that the PCS conference audience has essentially no overlap with that of ICLR. More importantly, the ICLR paper is significantly more detailed, includes all the additions you mention (use of deep transformations, a comparison to variational autoencoders, and an actual entropy coder), incorporates a variety of improvements in the optimization scheme (documented in the appendix), and shows results that are dramatically improved both in terms of rate--distortion values as well as visual appearance. Given all of this, we think our ICLR submission represents a significant advance over the PCS paper, and more generally, a significant advance in the field of image compression.\n\nRegarding eq. 10: You are right, the equation is missing a dependency of the partition function on the parameters -- thanks for pointing that out. The equivalence to VAE holds at least for any norm, and any affine and invertible perceptual transform (in which case the equation is correct) but is not as general as what we stated. We will correct this in the revised paper.\n\nRegarding perceptual distortion: We are quite interested in optimization for a perpetual metric (this was the emphasis of the PCS paper), and will at some point test this with the current system. But for the current work, optimization for MSE offered two practical advantages. First, we do not yet have a reliable perceptual metric that includes color, and we wanted to include examples of color images. Second, use of MSE in the objective allowed for a more fair and interpretable comparison to existing coders, which are essentially all optimized for MSE. Optimizing for a perpetual metric would surely have reduced the MSE performance of our coder, and a comparison to other coders (whether in terms of MSE, a metric like SSIM, or actual human subject ratings) would be \u201capples vs. oranges\u201d and thus a bit tricky to interpret. Once we ran our experiments, we were quite surprised to find that the subjective quality of the coded images was so good. Had we optimized for a perceptual metric, it wouldn't have been obvious that the parametric nonlinear transformations in our network are able to push the solution toward one that is perceptually better, even when optimized for plain old MSE!\n\nRegarding the adaptive entropy coder: We included an entropy code to provide a fair comparison to real-world coders (JPEG and JPEG 2000), and to ensure that our rate--distortion claims correspond to a realizable coder. It is worth noting that adaptive entropy coding is quite standard these days. In particular, JPEG 2000 uses an adaptive arithmetic coder. Note that in the PCS paper, we did not compare to other coding methods, but only to other transforms (in particular, the DCT). As for turning off the adaptivity, the CABAC coder we used in the this paper is adaptive, and not so easy to modify. Since we don't make use of any context dependencies, we were not expecting the adaptivity to contribute much to the reported results. To verify this, we have now measured the entropy of the encoded test images from the Kodak set, assuming the probability model learned on the training set. These values should very closely approximate the performance of a good non-adaptive arithmetic code. The results, averaged over the full test set, are plotted in this graph: [http://www.cns.nyu.edu/~balle/kodak-entropy2-psnr.pdf]. As expected, there is a relatively small improvement due to the adaptivity. Hence, most of the rate--distortion performance gain our system achieves over JPEG 2000 stems from the optimized 3-stage GDN transform.\n\nAs mentioned in our response to reviewer 3, we will include additional detail about the design of the entropy code in the revised appendix, and we will also include the plot linked above. We expect to upload the revision within the next few days.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287515787, "id": "ICLR.cc/2017/conference/-/paper573/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJxdQ3jeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper573/reviewers", "ICLR.cc/2017/conference/paper573/areachairs"], "cdate": 1485287515787}}}, {"tddate": null, "tmdate": 1483479806720, "tcdate": 1483479806720, "number": 4, "id": "BkwWjqtrx", "invitation": "ICLR.cc/2017/conference/-/paper573/public/comment", "forum": "rJxdQ3jeg", "replyto": "HJ2CvbMNx", "signatures": ["~Johannes_Ball\u00e91"], "readers": ["everyone"], "writers": ["~Johannes_Ball\u00e91"], "content": {"title": "Response to official review", "comment": "Thanks for your thorough feedback.\n\nWe agree that it might, in principle, be possible to develop a more complex transform which achieves uniform probability over quantization bins when optimized only for distortion, and which performs equally well or better than our solution. However, we also don't see a way to prove that such a transform exists.\n\nIn any case, restricting to the type of transform that we have used, the optimized solutions exhibit strongly non-uniform probability, which implies that any solution imposing uniform probabilities will be sub-optimal."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287515787, "id": "ICLR.cc/2017/conference/-/paper573/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJxdQ3jeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper573/reviewers", "ICLR.cc/2017/conference/paper573/areachairs"], "cdate": 1485287515787}}}, {"tddate": null, "tmdate": 1483449577217, "tcdate": 1483449280079, "number": 4, "id": "SJO6XQYHl", "invitation": "ICLR.cc/2017/conference/-/paper573/official/review", "forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "signatures": ["ICLR.cc/2017/conference/paper573/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper573/AnonReviewer3"], "content": {"title": "Very good paper", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This is the most convincing paper on image compression with deep neural networks that I have read so far. The paper is very well written, the use of the rate-distortion theory in the objective fits smoothly in the framework. The paper is compared to a reasonable baseline (JPEG2000, as opposed to previous papers only considering JPEG). I would expect this paper to have a very good impact. \n\nYes, please include results on Lena/Barbara/Baboon (sorry, not Gibbons), along with state-of-the-art references with more classical methods such as the one I mentioned in my questions. I think it is important to clearly state how NN compare to best previous methods. From the submitted version, I still don't know how both categories of methods are positioned. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483449280640, "id": "ICLR.cc/2017/conference/-/paper573/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper573/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper573/AnonReviewer2", "ICLR.cc/2017/conference/paper573/AnonReviewer1", "ICLR.cc/2017/conference/paper573/AnonReviewer4", "ICLR.cc/2017/conference/paper573/AnonReviewer3"], "reply": {"forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper573/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper573/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483449280640}}}, {"tddate": null, "tmdate": 1481934803810, "tcdate": 1481934803810, "number": 2, "id": "HJ2CvbMNx", "invitation": "ICLR.cc/2017/conference/-/paper573/official/review", "forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "signatures": ["ICLR.cc/2017/conference/paper573/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper573/AnonReviewer1"], "content": {"title": "Official review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This is a nice paper that demonstrates an end-to-end trained image compression and decompression system, which achieves better bit-rate vs quality trade-offs than established image compression algorithms (like JPEG-2000). In addition to showing the efficacy of 'deep learning' for a new application, a key contribution of the paper is the introduction of a differentiable version of \"rate\" function, which the authors show can be used for effective training with different rate-distortion trade-offs. I expect this will have impact beyond the compression application itself---for other tasks that might benefit from differentiable approximations to similar functions.\n\nThe authors provided a thoughtful response to my pre-review question. I would still argue that to minimize distortion under a fixed range and quantization, a sufficiently complex network would learn automatically produce  codes within a fixed range with the highest-possible entropy (i.e., it would meet the upper bound). But the second argument is convincing---doing so forces a specific \"form\" on how the compressor output is used, which to match the effective compression of the current system, would require a more complex network that is able to carry out the computations currently being done by a separate variable rate encoder used to store q.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483449280640, "id": "ICLR.cc/2017/conference/-/paper573/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper573/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper573/AnonReviewer2", "ICLR.cc/2017/conference/paper573/AnonReviewer1", "ICLR.cc/2017/conference/paper573/AnonReviewer4", "ICLR.cc/2017/conference/paper573/AnonReviewer3"], "reply": {"forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper573/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper573/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483449280640}}}, {"tddate": null, "tmdate": 1481886718376, "tcdate": 1481886718376, "number": 1, "id": "HkLZ3SbVl", "invitation": "ICLR.cc/2017/conference/-/paper573/official/review", "forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "signatures": ["ICLR.cc/2017/conference/paper573/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper573/AnonReviewer2"], "content": {"title": "A good paper with an interesting premise, some novel methods and good results", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This nicely written paper presents an end-to-end learning method for image compression. By optimizing for rate-distortion performance and a clever relaxation the method is able to learn an efficient image compression method by optimizing over a database of natural images.\n\nAs the method is interesting, results are interesting and analysis is quite thorough it's easy for me to recommend acceptance.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483449280640, "id": "ICLR.cc/2017/conference/-/paper573/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper573/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper573/AnonReviewer2", "ICLR.cc/2017/conference/paper573/AnonReviewer1", "ICLR.cc/2017/conference/paper573/AnonReviewer4", "ICLR.cc/2017/conference/paper573/AnonReviewer3"], "reply": {"forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper573/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper573/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483449280640}}}, {"tddate": null, "tmdate": 1481584269444, "tcdate": 1481584269439, "number": 3, "id": "r1HcCo37l", "invitation": "ICLR.cc/2017/conference/-/paper573/public/comment", "forum": "rJxdQ3jeg", "replyto": "HyXSc4k7g", "signatures": ["~Johannes_Ball\u00e91"], "readers": ["everyone"], "writers": ["~Johannes_Ball\u00e91"], "content": {"title": "response to AnonReviewer3", "comment": "1) Our arithmetic code handles each of the quantized values independently from all the others. The binarization is very similar to the way transform coefficients in H.264 are handled (essentially, a zero bit, a sign bit, and a truncated unary code followed by an exponential Golomb code \u2013 this is described in the IEEE CSVT paper we cited). Given the focus on the rate\u2013distortion optimization, providing full details of the entropy code in the main text seem a bit of a distraction, but we\u2019ll add a short description to the appendix.\n\n2) We haven't, but we will include Lena and Barbara as additional examples in the web site we created for this paper. We don't know the Gibbons image (at least not by that name) \u2013 can you provide a reference/source?\n\n3) Our aim was to compare to well-known methods that are representative of certain classes of linear transform coders. In that respect, JPEG and JPEG 2000 are widely used benchmarks, even though they are not quite state-of-the-art. We do plan to do a more thorough comparison to other methods, including state-of-the-art methods (e.g., HEVC intra, http://www.slideshare.net/touradj_ebrahimi/icip2016-image-compression-grand-challenge-66475960). Because the time is very limited, we don't think we will be able provide these results by the deadline, but we might be able to show some at the meeting.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287515787, "id": "ICLR.cc/2017/conference/-/paper573/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJxdQ3jeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper573/reviewers", "ICLR.cc/2017/conference/paper573/areachairs"], "cdate": 1485287515787}}}, {"tddate": null, "tmdate": 1481581042668, "tcdate": 1481581042660, "number": 2, "id": "BkseMshQl", "invitation": "ICLR.cc/2017/conference/-/paper573/public/comment", "forum": "rJxdQ3jeg", "replyto": "B1U53jkme", "signatures": ["~Johannes_Ball\u00e91"], "readers": ["everyone"], "writers": ["~Johannes_Ball\u00e91"], "content": {"title": "response to AnonReviewer1", "comment": "You are correct: we optimize the encoder/decoder parameters for each rate\u2013distortion ratio (i.e., each choice of lambda).\n\nIn general, to optimize the rate\u2013distortion tradeoff, one can either optimize the weighted sum of the two terms, or optimize either one while holding the other fixed (there is a long history of doing this in the coding community for particular types of coders). But fixing the range and the number of bins is not equivalent to fixing the rate.\n\nFirst, this would only place an upper bound on the rate (as you say), since the actual rate depends on the distribution. In general, the loss function comprising only a distortion term would then not be able to distinguish between two different sets of encoders/decoders with the same distortion, but different rates.\n\nSecond, fixing the range and number of bins as a way of controlling the rate imposes unnecessary constraints. It is easy to imagine entire classes of encoders/decoders that all achieve a given desired rate, but do not fit the constraint of having bounded transform values. In particular, the optimized marginal distributions we observe are heavy-tailed, and have widely varying ranges across feature maps. If we impose all feature maps to have the same range bounds, we are not just imposing an upper bound on the rate, but we are prescribing particular properties of the marginal distributions, which is likely to lead to suboptimal results. If the intention behind imposing the constraints you suggest was to end up with uniform distributions (in order to achieve the rate bound), then this point is particularly important.\n\nWe think it is ultimately more straight-forward to directly optimize for weighted rate\u2013distortion, imposing as few additional constraints as possible."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287515787, "id": "ICLR.cc/2017/conference/-/paper573/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJxdQ3jeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper573/reviewers", "ICLR.cc/2017/conference/paper573/areachairs"], "cdate": 1485287515787}}}, {"tddate": null, "tmdate": 1481567793287, "tcdate": 1481567793280, "number": 1, "id": "BJKNAvh7e", "invitation": "ICLR.cc/2017/conference/-/paper573/public/comment", "forum": "rJxdQ3jeg", "replyto": "H1zimb3Xx", "signatures": ["~Johannes_Ball\u00e91"], "readers": ["everyone"], "writers": ["~Johannes_Ball\u00e91"], "content": {"title": "answer to AnonReviewer2", "comment": "1) Thanks for bringing this paper to our attention \u2013 it seems interesting, but perhaps not so directly relevant to the main topic of our paper (rate\u2013distortion optimization). Note that we have not cited much of our own previous work, in which we have used GSMs as image priors (Wainwright & Simoncelli 99, Portilla et al. 2003) including some with student-T mixing distributions (Lyu & Simoncelli 08; Lyu 11), as well as normalization for compression (Buccigrossi & Simoncelli 99, Malo et al. 06).\n\n2) The filters in the first stage are indeed mostly oriented and come in different sizes (although the fixed 9x9 size prevents it from achieving the many octaves of scaling implemented in a Steerable Pyramid). We are currently working on methods of visualizing the filters in later stages.\n\n3) We agree that the generalized normalization transform (which we have previously used for image density modeling \u2013 see ICLR 2016) may be useful for other image processing tasks such as denoising,  deblurring/super-resolution, or dynamic range decompression. But we prefer to do so by optimizing it for each of them directly."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287515787, "id": "ICLR.cc/2017/conference/-/paper573/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJxdQ3jeg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper573/reviewers", "ICLR.cc/2017/conference/paper573/areachairs"], "cdate": 1485287515787}}}, {"tddate": null, "tmdate": 1481540505819, "tcdate": 1481540505814, "number": 3, "id": "H1zimb3Xx", "invitation": "ICLR.cc/2017/conference/-/paper573/pre-review/question", "forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "signatures": ["ICLR.cc/2017/conference/paper573/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper573/AnonReviewer2"], "content": {"title": "Some minor questions", "question": "1) I think a relevant paper to cite is one by Van der Oord and Schrauwen:\nhttp://jmlr.org/papers/volume15/vandenoord14a/vandenoord14a.pdf\n\n2) It would be great to get a visualization of some of the filters the network learns - can they be compared to other known transforms like the steerable pyramid etc.?\n\n3) Given the learned representation is capable of modeling long range dependencies and subtle image structure, can it be used to other image restoration tasks such as denoising?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481540506334, "id": "ICLR.cc/2017/conference/-/paper573/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper573/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper573/AnonReviewer3", "ICLR.cc/2017/conference/paper573/AnonReviewer1", "ICLR.cc/2017/conference/paper573/AnonReviewer2"], "reply": {"forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper573/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper573/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481540506334}}}, {"tddate": null, "tmdate": 1480731789905, "tcdate": 1480731789900, "number": 2, "id": "B1U53jkme", "invitation": "ICLR.cc/2017/conference/-/paper573/pre-review/question", "forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "signatures": ["ICLR.cc/2017/conference/paper573/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper573/AnonReviewer1"], "content": {"title": "Optimizing different tradeoff vs distortion for different rates", "question": "There's a certain degree of complexity introduced by the choice of putting an expression for rate/entropy in the training objective. If I understand correctly, you still train different encoders-decoders for a set of different weighted combinations of rate and distortion. \n\nI wonder then, if it wouldn't have been easier to minimize distortion for a set of different rates. Fixing the rate could've been achieved by restricting y to be in [0,1] (for e.g., by using a sigmoid), and then adding different levels of random noise (like you already do) to simulate different number of quantization levels. \n\nPresumably, for each choice of number of quantization levels, the encoder-decoder pair would learn to minimize distortion, and would achieve a rate no worse than that corresponding to a uniform distribution over the quantization levels."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481540506334, "id": "ICLR.cc/2017/conference/-/paper573/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper573/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper573/AnonReviewer3", "ICLR.cc/2017/conference/paper573/AnonReviewer1", "ICLR.cc/2017/conference/paper573/AnonReviewer2"], "reply": {"forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper573/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper573/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481540506334}}}, {"tddate": null, "tmdate": 1480702523349, "tcdate": 1480702523345, "number": 1, "id": "HyXSc4k7g", "invitation": "ICLR.cc/2017/conference/-/paper573/pre-review/question", "forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "signatures": ["ICLR.cc/2017/conference/paper573/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper573/AnonReviewer3"], "content": {"title": "Comparison and details", "question": "1) In my opinion, the way the CABAC is fed with quantization values lacks of details. Could you please elaborate a bit?\n\n2) Have you measured achievable rate-distortion compromises on Lena, Barbara, Gibbons, for which the reader may have more comparison points with the most recent compression techniques than the Kodak dataset? \n\n3) Similarly, for the sake of completness, could you please include a comparison with methods developed after JPEG2000, like the oriented Wavelet Transform (http://vivien.chappelier.free.fr/owavelets/). \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "End-to-end Optimized Image Compression", "abstract": "We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.", "pdf": "/pdf/d7df16ec78247c2550d3a0b658b90e6d755089c0.pdf", "paperhash": "ball\u00e9|endtoend_optimized_image_compression", "keywords": [], "conflicts": ["nyu.edu", "rwth-aachen.de", "uv.es"], "authors": ["Johannes Ball\u00e9", "Valero Laparra", "Eero P. Simoncelli"], "authorids": ["johannes.balle@nyu.edu", "valero.laparra@uv.es", "eero.simoncelli@nyu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481540506334, "id": "ICLR.cc/2017/conference/-/paper573/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper573/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper573/AnonReviewer3", "ICLR.cc/2017/conference/paper573/AnonReviewer1", "ICLR.cc/2017/conference/paper573/AnonReviewer2"], "reply": {"forum": "rJxdQ3jeg", "replyto": "rJxdQ3jeg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper573/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper573/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481540506334}}}], "count": 17}