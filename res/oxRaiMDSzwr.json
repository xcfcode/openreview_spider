{"notes": [{"id": "oxRaiMDSzwr", "original": "UZyepJWE8Th", "number": 3374, "cdate": 1601308374321, "ddate": null, "tcdate": 1601308374321, "tmdate": 1614985739673, "tddate": null, "forum": "oxRaiMDSzwr", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "NETWORK ROBUSTNESS TO PCA PERTURBATIONS", "authorids": ["~Anan_Kabaha3", "~Dana_Drachsler_Cohen1"], "authors": ["Anan Kabaha", "Dana Drachsler Cohen"], "keywords": ["neural network robustness", "adversarial examples"], "abstract": "A key challenge in analyzing neural networks' robustness is identifying input features for which networks are robust to perturbations. Existing work focuses on direct perturbations to the inputs, thereby studies network robustness to the lowest-level features. In this work, we take a new approach and study the robustness of networks to the inputs' semantic features. We show a black-box approach to determine features for which a network is robust or weak.  We leverage these features to obtain provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. We evaluate our approach with PCA features. We show (1) provably robust neighborhoods are larger: on average by 1.8x and up to 4.5x, compared to the standard neighborhoods, and (2) our adversarial examples are generated using at least 8.7x fewer queries and have at least 2.8x lower L2 distortion compared to state-of-the-art. We further show that our attack is effective even against ensemble adversarial training. ", "one-sentence_summary": "A framework to detect robust and weak features of DNNs and to use them to define robustness neighborhoods and adversarial examples.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kabaha|network_robustness_to_pca_perturbations", "pdf": "/pdf/cba4351e8db45d410cea7d661c31f2821011e291.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHU5RFA97P", "_bibtex": "@misc{\nkabaha2021network,\ntitle={{\\{}NETWORK{\\}} {\\{}ROBUSTNESS{\\}} {\\{}TO{\\}} {\\{}PCA{\\}} {\\{}PERTURBATIONS{\\}}},\nauthor={Anan Kabaha and Dana Drachsler Cohen},\nyear={2021},\nurl={https://openreview.net/forum?id=oxRaiMDSzwr}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "66jkgYjbeP", "original": null, "number": 1, "cdate": 1610040398613, "ddate": null, "tcdate": 1610040398613, "tmdate": 1610473994177, "tddate": null, "forum": "oxRaiMDSzwr", "replyto": "oxRaiMDSzwr", "invitation": "ICLR.cc/2021/Conference/Paper3374/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "I agree with the majority of reviews that this paper is not sufficiently convincing. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK ROBUSTNESS TO PCA PERTURBATIONS", "authorids": ["~Anan_Kabaha3", "~Dana_Drachsler_Cohen1"], "authors": ["Anan Kabaha", "Dana Drachsler Cohen"], "keywords": ["neural network robustness", "adversarial examples"], "abstract": "A key challenge in analyzing neural networks' robustness is identifying input features for which networks are robust to perturbations. Existing work focuses on direct perturbations to the inputs, thereby studies network robustness to the lowest-level features. In this work, we take a new approach and study the robustness of networks to the inputs' semantic features. We show a black-box approach to determine features for which a network is robust or weak.  We leverage these features to obtain provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. We evaluate our approach with PCA features. We show (1) provably robust neighborhoods are larger: on average by 1.8x and up to 4.5x, compared to the standard neighborhoods, and (2) our adversarial examples are generated using at least 8.7x fewer queries and have at least 2.8x lower L2 distortion compared to state-of-the-art. We further show that our attack is effective even against ensemble adversarial training. ", "one-sentence_summary": "A framework to detect robust and weak features of DNNs and to use them to define robustness neighborhoods and adversarial examples.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kabaha|network_robustness_to_pca_perturbations", "pdf": "/pdf/cba4351e8db45d410cea7d661c31f2821011e291.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHU5RFA97P", "_bibtex": "@misc{\nkabaha2021network,\ntitle={{\\{}NETWORK{\\}} {\\{}ROBUSTNESS{\\}} {\\{}TO{\\}} {\\{}PCA{\\}} {\\{}PERTURBATIONS{\\}}},\nauthor={Anan Kabaha and Dana Drachsler Cohen},\nyear={2021},\nurl={https://openreview.net/forum?id=oxRaiMDSzwr}\n}"}, "tags": [], "invitation": {"reply": {"forum": "oxRaiMDSzwr", "replyto": "oxRaiMDSzwr", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040398598, "tmdate": 1610473994160, "id": "ICLR.cc/2021/Conference/Paper3374/-/Decision"}}}, {"id": "UcSpguEQmx", "original": null, "number": 4, "cdate": 1604001028496, "ddate": null, "tcdate": 1604001028496, "tmdate": 1606774476618, "tddate": null, "forum": "oxRaiMDSzwr", "replyto": "oxRaiMDSzwr", "invitation": "ICLR.cc/2021/Conference/Paper3374/-/Official_Review", "content": {"title": "Confusing setup and experiments", "review": "The present paper proposes to consider features derived from PCA for the purposes of adversarial attack and defense. They argue that, based on these features, they can verify larger neighborhoods and provide stronger attacks. The neighborhoods are based on the ERAN verifier and the attacks are in comparison to a recent attack called AutoZoom. Defense performance is measured in number of images in the neighborhood, and attack performance in terms of L2 distance.\n\nI was, in general, confused by many of the choices in the paper. The paper proposes to use PCA features, but then measure attack in L2 distance; if the argument is that PCA is a better basis, then why use the pixel-space L2 metric? It seems this could only help to the extent that methods such as projected gradient descent (PGD) are failing to find the optimal L2 attack. Measuring defense capability in terms of number of images also seems like an odd choice, as usually we choose some perceptually meaningful norm and measure according to that. I had also not been familiar with the ERAN verifier prior to this paper, despite being an expert on neural network verification, and it isn't obvious to me whether we should consider it to be competitive with other SDP and LP-based verification methods. Moreover, the PCA features are only used to determine the side lengths of a hyperrectangle that is provided to the verifier, so that this is a fairly indirect test of whether PCA features are useful for defense (indeed, I would like to see a more crisp formulation of what \"useful for defense\" is meant to mean, since they aren't used to change the model itself).\n\nIn the experimental comparison of attacks, standard baselines such as PGD and C&W are missing, such that it is difficult to interpret the results. I checked the AutoZoom paper (the main method compared to) and it also does not compare to PGD, so I feel that the present paper does not provide evidence that the method performs better than baselines.\n\nI think the authors could improve the paper by more crisply articulating its goals, and either using more standard experimental setups and metrics or defending its deviation from these.\n\n== Update after author response ==\n\nThanks for your response. I believe it is widely agreed upon that black box evaluation is not meaningful in security settings, and that we should use white box attacks. Therefore, I don't find the justification for omitting PGD and CW convincing. I also still do not feel that counting images in pixel space is a meaningful metric. Therefore, I have kept my original score.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3374/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3374/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK ROBUSTNESS TO PCA PERTURBATIONS", "authorids": ["~Anan_Kabaha3", "~Dana_Drachsler_Cohen1"], "authors": ["Anan Kabaha", "Dana Drachsler Cohen"], "keywords": ["neural network robustness", "adversarial examples"], "abstract": "A key challenge in analyzing neural networks' robustness is identifying input features for which networks are robust to perturbations. Existing work focuses on direct perturbations to the inputs, thereby studies network robustness to the lowest-level features. In this work, we take a new approach and study the robustness of networks to the inputs' semantic features. We show a black-box approach to determine features for which a network is robust or weak.  We leverage these features to obtain provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. We evaluate our approach with PCA features. We show (1) provably robust neighborhoods are larger: on average by 1.8x and up to 4.5x, compared to the standard neighborhoods, and (2) our adversarial examples are generated using at least 8.7x fewer queries and have at least 2.8x lower L2 distortion compared to state-of-the-art. We further show that our attack is effective even against ensemble adversarial training. ", "one-sentence_summary": "A framework to detect robust and weak features of DNNs and to use them to define robustness neighborhoods and adversarial examples.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kabaha|network_robustness_to_pca_perturbations", "pdf": "/pdf/cba4351e8db45d410cea7d661c31f2821011e291.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHU5RFA97P", "_bibtex": "@misc{\nkabaha2021network,\ntitle={{\\{}NETWORK{\\}} {\\{}ROBUSTNESS{\\}} {\\{}TO{\\}} {\\{}PCA{\\}} {\\{}PERTURBATIONS{\\}}},\nauthor={Anan Kabaha and Dana Drachsler Cohen},\nyear={2021},\nurl={https://openreview.net/forum?id=oxRaiMDSzwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oxRaiMDSzwr", "replyto": "oxRaiMDSzwr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3374/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077067, "tmdate": 1606915772214, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3374/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3374/-/Official_Review"}}}, {"id": "UwP9cRuyw0q", "original": null, "number": 3, "cdate": 1603917842664, "ddate": null, "tcdate": 1603917842664, "tmdate": 1606587448653, "tddate": null, "forum": "oxRaiMDSzwr", "replyto": "oxRaiMDSzwr", "invitation": "ICLR.cc/2021/Conference/Paper3374/-/Official_Review", "content": {"title": "An interesting direction, requires further investigation", "review": "The authors study the problem of adversarial robustness, aiming to find regions of the input space for which a classifier is robust. Instead of the standard approach of defining a neighborhood around each data point based on some $\\ell_p$-norm, they use PCA to identify directions along which the model is robust or brittle. They then use these methods to identify large regions of input space for which models are robust and, in a complementary direction, to craft imperceptible adversarial examples with few model queries.\n\nIn general, understanding the set of perturbations that our models are robust to is an important research question. Unfortunately, the current paper does not go into significant depth. \n\n**Robustness neighborhoods.** \u03a4he input robustness regions computed using this approach are rather unintuitive. What insight is gained by reading Table 1? Do these robustness regions correspond to something concrete and meaningful (e.g., diagonal stripes, brightness) that we can convey to human users of the model? What fundamentally new understanding do we obtain via this analysis?\n\n**New adversarial attacks.**  The space of existing adversarial attacks is huge. By now, there exist so many different approaches for computing examples that fool models. From that perspective, it is not clear what the new attack proposed offers. Similarly to the point above, since these directions do not necessarily capture something human-understandable it is unclear how they reveal a fundamentally new model vulnerability.\n\nOverall, while the research direction is interesting, further exploration is needed to reach novel insights about these models.\n\nOther comments (not affecting score):\n- The large numbers representing \"number of images\" are somewhat misleading. How can we quantify whether these images are actually distinct. I do not think that these number convey significant information and I would thus recommend removing them.\n- https://arxiv.org/abs/1807.04200 and https://distill.pub/2019/advex-bugs-discussion/response-3/ also study model robustness to PCA-based perturbations and might thus be worth discussing.\n\n====== POST-RESPONSE UPDATE ======\n\nI appreciate the author's response and the additional illustrations provided. At the same time, my concerns remain:\n- I still disagree with the claim that PCA directions are \"semantic and meaningful.\" Yes, some of them might correspond to image changes that are intuitive, e.g., Figure 4, but it is still impossible to draw any such conclusions without manually inspecting individual directions. In other words, what do I learn about my model by reading Table 1?\n- I understand the process of counting the number of images. However, I still think that it is a fundamentally flawed metric. Based on this definition of \"distinct\", if I change the value of a single pixel by 1/255 I get a distinct image. This is clearly not an intuitive behavior. As a model designer, what do I understand about my model by look at these astronomical numbers.\n\nOverall, while I still find the broad direction interesting, I believe that the paper has fundamental issues and is hence unsuitable for publication.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3374/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3374/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK ROBUSTNESS TO PCA PERTURBATIONS", "authorids": ["~Anan_Kabaha3", "~Dana_Drachsler_Cohen1"], "authors": ["Anan Kabaha", "Dana Drachsler Cohen"], "keywords": ["neural network robustness", "adversarial examples"], "abstract": "A key challenge in analyzing neural networks' robustness is identifying input features for which networks are robust to perturbations. Existing work focuses on direct perturbations to the inputs, thereby studies network robustness to the lowest-level features. In this work, we take a new approach and study the robustness of networks to the inputs' semantic features. We show a black-box approach to determine features for which a network is robust or weak.  We leverage these features to obtain provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. We evaluate our approach with PCA features. We show (1) provably robust neighborhoods are larger: on average by 1.8x and up to 4.5x, compared to the standard neighborhoods, and (2) our adversarial examples are generated using at least 8.7x fewer queries and have at least 2.8x lower L2 distortion compared to state-of-the-art. We further show that our attack is effective even against ensemble adversarial training. ", "one-sentence_summary": "A framework to detect robust and weak features of DNNs and to use them to define robustness neighborhoods and adversarial examples.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kabaha|network_robustness_to_pca_perturbations", "pdf": "/pdf/cba4351e8db45d410cea7d661c31f2821011e291.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHU5RFA97P", "_bibtex": "@misc{\nkabaha2021network,\ntitle={{\\{}NETWORK{\\}} {\\{}ROBUSTNESS{\\}} {\\{}TO{\\}} {\\{}PCA{\\}} {\\{}PERTURBATIONS{\\}}},\nauthor={Anan Kabaha and Dana Drachsler Cohen},\nyear={2021},\nurl={https://openreview.net/forum?id=oxRaiMDSzwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oxRaiMDSzwr", "replyto": "oxRaiMDSzwr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3374/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077067, "tmdate": 1606915772214, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3374/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3374/-/Official_Review"}}}, {"id": "xwpT9wc11q6", "original": null, "number": 5, "cdate": 1606236931295, "ddate": null, "tcdate": 1606236931295, "tmdate": 1606237039588, "tddate": null, "forum": "oxRaiMDSzwr", "replyto": "z5Y_khAH6en", "invitation": "ICLR.cc/2021/Conference/Paper3374/-/Official_Comment", "content": {"title": "Response to the Cons ", "comment": "Thank you for your kind review and suggestions. We next address your points raised in Cons: \n\n\n(1) While theoretically Alg1 has a high running time, we use it in a practical way by tuning the parameters so that overall, on state-of-the-art models and datasets, it completes within minutes. We note that other attacks also have a one-time preprocessing step (e.g., AutoZoom trains an Autoencoder). \n\n\n(2) Note that our recovery employs an optimization to mitigate the computation time. Further, we show that even with this recovery step, the number of queries we pose to attack ImageNet models is in line with state-of-the-art black-box attacks. Additionally, one may stop the recovery at any point -- we report the L2 distortion of the adversarial examples we generate without the recovery, and it is not too large (Table 2).\n\n\n(3) We are currently exploring other approaches to extract semantic features such as nonlinear PCA and neural network-based approach. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3374/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3374/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK ROBUSTNESS TO PCA PERTURBATIONS", "authorids": ["~Anan_Kabaha3", "~Dana_Drachsler_Cohen1"], "authors": ["Anan Kabaha", "Dana Drachsler Cohen"], "keywords": ["neural network robustness", "adversarial examples"], "abstract": "A key challenge in analyzing neural networks' robustness is identifying input features for which networks are robust to perturbations. Existing work focuses on direct perturbations to the inputs, thereby studies network robustness to the lowest-level features. In this work, we take a new approach and study the robustness of networks to the inputs' semantic features. We show a black-box approach to determine features for which a network is robust or weak.  We leverage these features to obtain provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. We evaluate our approach with PCA features. We show (1) provably robust neighborhoods are larger: on average by 1.8x and up to 4.5x, compared to the standard neighborhoods, and (2) our adversarial examples are generated using at least 8.7x fewer queries and have at least 2.8x lower L2 distortion compared to state-of-the-art. We further show that our attack is effective even against ensemble adversarial training. ", "one-sentence_summary": "A framework to detect robust and weak features of DNNs and to use them to define robustness neighborhoods and adversarial examples.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kabaha|network_robustness_to_pca_perturbations", "pdf": "/pdf/cba4351e8db45d410cea7d661c31f2821011e291.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHU5RFA97P", "_bibtex": "@misc{\nkabaha2021network,\ntitle={{\\{}NETWORK{\\}} {\\{}ROBUSTNESS{\\}} {\\{}TO{\\}} {\\{}PCA{\\}} {\\{}PERTURBATIONS{\\}}},\nauthor={Anan Kabaha and Dana Drachsler Cohen},\nyear={2021},\nurl={https://openreview.net/forum?id=oxRaiMDSzwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "oxRaiMDSzwr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3374/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3374/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3374/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3374/Authors|ICLR.cc/2021/Conference/Paper3374/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3374/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838251, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3374/-/Official_Comment"}}}, {"id": "vNRyUOQW7H", "original": null, "number": 4, "cdate": 1606236827305, "ddate": null, "tcdate": 1606236827305, "tmdate": 1606236827305, "tddate": null, "forum": "oxRaiMDSzwr", "replyto": "7C7OVt7HjM", "invitation": "ICLR.cc/2021/Conference/Paper3374/-/Official_Comment", "content": {"title": "Explanations about the significance of our insights and comparison to existing attacks", "comment": "Thank you for the review and the suggestions. We next address your questions and concerns.\n\n\nQ: What is the significance of PCA-guided neighborhoods?\n\nA: Our robustness neighborhoods which are defined using *semantic, human-understandable features* make the network robustness more transparent to the user (for example, Figure 1 shows the network is robust to sky color perturbations). To emphasize this, we added an illustration of the meaning of the features mentioned in Figure 1 and 2 (Figure 1(h) and Figure 2(f)) and a short description of the features in Table 1 (Figure 4). \n\nBy showing that we obtain larger neighborhoods than distance-based neighorboods, we show that our definition is better suited to interpret the network robustness to perturbations. By the way, we are not the first to identify the need in studying non-distance based neighborhoods, see for example, Fundamental Tradeoffs between Invariance and Sensitivity to Adversarial Perturbations, Tram\u00e8r et al., ICML 2020. \n\nQ: Related work is missing.\n\n A: We present a practical, black-box attack and thus compare to state-of-the-art practical black-box attacks - AutoZoom and GenAttack. In Related Work, we discuss popular attacks. If there is another state-of-the-art black-box attack or another attack which the reviewer believes we should include, we would be happy to add and compare to. We note that the first reviewer requested a comparison to C&W and we added it (Table 2).\n\n\nQ: Why is the number of queries required to compute the weak PCA features counted separately?\n\n A: Computing the most robust and weakest features is a one-time computation per model, after which we can compute attacks for as many inputs as we want. Thus, we report the amortized number of queries. It is a common practice to separate between preprocessing steps and the attack itself (e.g., AutoZoom trains an autoencoder, and does not consider it when reporting the attack results).\n        \nQ:  Why not focus only on the attack?\n\nA: We believe part of the strength of our approach is to show that the same kind of features may be used to prove robustness (if the network is robust to them) or fool the network (if the network is weak with respect to them). \n\nModifications to the paper relevant to this review: Figure 1(h), Figure 2(f), Figure 4, Table 2 (C&W), and Related Work.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3374/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3374/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK ROBUSTNESS TO PCA PERTURBATIONS", "authorids": ["~Anan_Kabaha3", "~Dana_Drachsler_Cohen1"], "authors": ["Anan Kabaha", "Dana Drachsler Cohen"], "keywords": ["neural network robustness", "adversarial examples"], "abstract": "A key challenge in analyzing neural networks' robustness is identifying input features for which networks are robust to perturbations. Existing work focuses on direct perturbations to the inputs, thereby studies network robustness to the lowest-level features. In this work, we take a new approach and study the robustness of networks to the inputs' semantic features. We show a black-box approach to determine features for which a network is robust or weak.  We leverage these features to obtain provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. We evaluate our approach with PCA features. We show (1) provably robust neighborhoods are larger: on average by 1.8x and up to 4.5x, compared to the standard neighborhoods, and (2) our adversarial examples are generated using at least 8.7x fewer queries and have at least 2.8x lower L2 distortion compared to state-of-the-art. We further show that our attack is effective even against ensemble adversarial training. ", "one-sentence_summary": "A framework to detect robust and weak features of DNNs and to use them to define robustness neighborhoods and adversarial examples.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kabaha|network_robustness_to_pca_perturbations", "pdf": "/pdf/cba4351e8db45d410cea7d661c31f2821011e291.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHU5RFA97P", "_bibtex": "@misc{\nkabaha2021network,\ntitle={{\\{}NETWORK{\\}} {\\{}ROBUSTNESS{\\}} {\\{}TO{\\}} {\\{}PCA{\\}} {\\{}PERTURBATIONS{\\}}},\nauthor={Anan Kabaha and Dana Drachsler Cohen},\nyear={2021},\nurl={https://openreview.net/forum?id=oxRaiMDSzwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "oxRaiMDSzwr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3374/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3374/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3374/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3374/Authors|ICLR.cc/2021/Conference/Paper3374/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3374/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838251, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3374/-/Official_Comment"}}}, {"id": "iir-OS2xkfa", "original": null, "number": 3, "cdate": 1606236599964, "ddate": null, "tcdate": 1606236599964, "tmdate": 1606236599964, "tddate": null, "forum": "oxRaiMDSzwr", "replyto": "UwP9cRuyw0q", "invitation": "ICLR.cc/2021/Conference/Paper3374/-/Official_Comment", "content": {"title": "Further explanations about the paper\u2019s key insights", "comment": "Thank you for the review and the suggestions. We next address your questions and concerns.\n\n\nQ: Do the robustness neighborhoods correspond to concrete and meaningful perturbations?\n\nA: Yes, our robustness neighborhoods consist of inputs differing with respect to *semantic features* and are thus meaningful and human interpretable. Figure 1 and 2 illustrate the kind of features we use to define our neighborhoods (sky color and lighting conditions, respectively).  To emphasize this, we added an illustration of the meaning of the features mentioned in Figure 1 and 2 (Figure 1(h) and Figure 2(f)) and a short description of the features in Table 1 (Figure 4).\n\n\nQ: What is the advantage of your attack? Do the attack directions capture something human-understandable?\n\nA: First, our black-box attack requires fewer queries and distortion compared to state-of-the-art practical black-box attacks. Second, our attack exposes a new vulnerability of models to small perturbations of weak PCA features. Although the PCA features often capture meaningful features, we do not believe this is essential for attacks, as the goal of attacks is to find imperceivable perturbations that fool models and not human-understandable perturbations. \n\n\nQ: How can we quantify the number of distinct images in a neighborhood? Why using this metric?\n\nA: The number of images in neighborhoods is well-defined mathematically and in particular counts *distinct images*. We provide here the definition. Given a neighborhood defined around an input X with N pixels (dimensions), where the pixel i ranges over $[X_{down}(i), X_{up}(i)]$ and each pixel domain is [0,1], the number of distinct images in the neighborhood is:  $\\prod_{i=1}^{N}{1+\\lfloor{255\\ast(X_{up}(i)-X(i))}\\rfloor+\\lfloor{255\\ast(X(i)-X_{down}(i))}\\rfloor}$. \n\nThis formula reflects the transition from a continuous range [0,1] to the discrete pixel representation 1-255.\n\nFor example, assume the following neighborhood around $X: [[X(1), X(1)+1/255] ,[X(2),X(2)+1/255]]$ (i.e., the neighborhood consists of images with two pixels, each pixel can have one of two possible discrete values). The neighborhood has 4 distinct images:   $[X(1),X(2)],   [X(1)+1/255,X(2)]$,    $[X(1),X(2)+1/255]$,     $[X(2)1/255,X(2)+1/255]$.\n\nTo compare our neighborhoods with distance-based neighborhoods, we must use a metric which can quantify both kinds. To this end, we count the number of images. This metric is independent of the neighborhood shape, yet is capable of truly reflecting the neighborhood size. We believe that counting the number of images is better suited to measure neighborhood sizes. For example, reporting that we proved a distance-based neighborhood with epsilon=0.005 (as we have in Table 1) might mislead one to think this neighborhood is larger than one with epsilon=0.0000001. In reality, both neighborhoods contain a single image and are thus practically identical. \n\nQ: Two references are missing.\n\nA: Thanks for the references, we added them to Related Work. The first work (https://arxiv.org/abs/1807.04200) studies classification quality versus vulnerability to adversarial attacks through principal directions and curvatures of the decision boundary. \nThe second paper (https://distill.pub/2019/advex-bugs-discussion/response-3) is a followup on Ilyas et al. (2019) (discussed in our paper). This work considers correlation between features and dataset labels (that is, the model is unknown). This setting is different from our black-box setting (see https://arxiv.org/pdf/1804.00097.pdf)\n\n\nModifications to the paper relevant to this review: Figure 1(h), Figure 2(f), Figure 4, and Related Work.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3374/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3374/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK ROBUSTNESS TO PCA PERTURBATIONS", "authorids": ["~Anan_Kabaha3", "~Dana_Drachsler_Cohen1"], "authors": ["Anan Kabaha", "Dana Drachsler Cohen"], "keywords": ["neural network robustness", "adversarial examples"], "abstract": "A key challenge in analyzing neural networks' robustness is identifying input features for which networks are robust to perturbations. Existing work focuses on direct perturbations to the inputs, thereby studies network robustness to the lowest-level features. In this work, we take a new approach and study the robustness of networks to the inputs' semantic features. We show a black-box approach to determine features for which a network is robust or weak.  We leverage these features to obtain provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. We evaluate our approach with PCA features. We show (1) provably robust neighborhoods are larger: on average by 1.8x and up to 4.5x, compared to the standard neighborhoods, and (2) our adversarial examples are generated using at least 8.7x fewer queries and have at least 2.8x lower L2 distortion compared to state-of-the-art. We further show that our attack is effective even against ensemble adversarial training. ", "one-sentence_summary": "A framework to detect robust and weak features of DNNs and to use them to define robustness neighborhoods and adversarial examples.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kabaha|network_robustness_to_pca_perturbations", "pdf": "/pdf/cba4351e8db45d410cea7d661c31f2821011e291.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHU5RFA97P", "_bibtex": "@misc{\nkabaha2021network,\ntitle={{\\{}NETWORK{\\}} {\\{}ROBUSTNESS{\\}} {\\{}TO{\\}} {\\{}PCA{\\}} {\\{}PERTURBATIONS{\\}}},\nauthor={Anan Kabaha and Dana Drachsler Cohen},\nyear={2021},\nurl={https://openreview.net/forum?id=oxRaiMDSzwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "oxRaiMDSzwr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3374/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3374/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3374/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3374/Authors|ICLR.cc/2021/Conference/Paper3374/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3374/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838251, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3374/-/Official_Comment"}}}, {"id": "6VXf4uoZfj5", "original": null, "number": 2, "cdate": 1606234847816, "ddate": null, "tcdate": 1606234847816, "tmdate": 1606235157954, "tddate": null, "forum": "oxRaiMDSzwr", "replyto": "UcSpguEQmx", "invitation": "ICLR.cc/2021/Conference/Paper3374/-/Official_Comment", "content": {"title": "Explained setup and experiments", "comment": "Thank you for the review and the suggestions. We next address your questions and concerns.\n\nQ: Why report the L2 distortion for the pixel-space and not the PCA-space?\n\nA: The reported L2 distortion is required to properly compare our attack with previous attacks (which have not used the PCA domain). Note that the attack is computed in the PCA domain, and then the perturbation is transformed back to the image domain. However, to address your concern, we added the PCA distortion (Appendix E, Table 5).\n\n\nQ: Why measure the number of images in a neighborhood and not a meaningful norm? \n\nA: Our work defines robustness neighborhoods based on *meaningful semantic features*. To emphasize this, we added an illustration of the meaning of the features in Figure 1 and 2 and a short description of the features in Table 1 (Figure 4).\nBy the way, we are not the first to identify the need in studying non-distance based neighborhoods, see for example, Fundamental Tradeoffs between Invariance and Sensitivity to Adversarial Perturbations, Tram\u00e8r et al., ICML 2020. \nTo compare our neighborhoods with distance-based neighborhoods, we must use a metric which can quantify both kinds. To this end, we count the number of images. This metric is independent of the neighborhood shape, yet is capable of truly reflecting the neighborhood size. We believe that counting the number of images is better suited to measure neighborhood sizes. For example, reporting that we proved a distance-based neighborhood with epsilon=0.005 (as we have in Table 1) might mislead one to think this neighborhood is larger than one with epsilon=0.0000001. In reality, both neighborhoods contain a single image and are thus practically identical. \n\n\nQ: Why ERAN?\n\nA: ERAN is a state-of-the-art analyzer that outperforms LP-based and SDP approaches on many benchmarks -- see the competition results https://sites.google.com/view/vnn20/vnncomp. As far as we are aware, SDP approaches do not scale to large networks (see https://files.sri.inf.ethz.ch/website/papers/neurips19_krelu.pdf, end of page 2). \n \n     \nQ: Are PCA features useful for defense?\n\nA: Studying how PCA can be useful for defense is out of scope for our work, but is very interesting as future work. We believe one may employ similar ideas as in Adversarial Training and Provable Defenses: Bridging the Gap, Balunovic, ICLR 2020.\n\n\nQ: Comparison to PGD and C&W is missing.\n\nA: PGD and C&W are white-box attacks, i.e., they assume the attacker has *full access* to the model's weights and architecture, an assumption which is not always true in practice. In contrast, our work is a practical black-box attack where the attacker only has access to the model\u2019s input and output layers. To compare apples to apples, we compare to state-of-the-art practical black-box attacks - AutoZoom and GenAttack. It is a common practice to compare black-box attacks only to black-box attacks (see https://arxiv.org/pdf/1804.00097.pdf). Our results show that our attack computes adversarial examples with fewer queries and less distortion than parallel practical black-box approaches. Nevertheless, to address your concern, we added a comparison to C&W (Table 2) and discussed the difference of white-box attacks and black-box attacks (Related Work).\n\nModifications to the paper relevant to this review: Figure 1(h), Figure 2(f), Figure 4, Table 2 (C&W column), Appendix E, and the adversarial examples paragraph in Related Work."}, "signatures": ["ICLR.cc/2021/Conference/Paper3374/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3374/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK ROBUSTNESS TO PCA PERTURBATIONS", "authorids": ["~Anan_Kabaha3", "~Dana_Drachsler_Cohen1"], "authors": ["Anan Kabaha", "Dana Drachsler Cohen"], "keywords": ["neural network robustness", "adversarial examples"], "abstract": "A key challenge in analyzing neural networks' robustness is identifying input features for which networks are robust to perturbations. Existing work focuses on direct perturbations to the inputs, thereby studies network robustness to the lowest-level features. In this work, we take a new approach and study the robustness of networks to the inputs' semantic features. We show a black-box approach to determine features for which a network is robust or weak.  We leverage these features to obtain provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. We evaluate our approach with PCA features. We show (1) provably robust neighborhoods are larger: on average by 1.8x and up to 4.5x, compared to the standard neighborhoods, and (2) our adversarial examples are generated using at least 8.7x fewer queries and have at least 2.8x lower L2 distortion compared to state-of-the-art. We further show that our attack is effective even against ensemble adversarial training. ", "one-sentence_summary": "A framework to detect robust and weak features of DNNs and to use them to define robustness neighborhoods and adversarial examples.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kabaha|network_robustness_to_pca_perturbations", "pdf": "/pdf/cba4351e8db45d410cea7d661c31f2821011e291.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHU5RFA97P", "_bibtex": "@misc{\nkabaha2021network,\ntitle={{\\{}NETWORK{\\}} {\\{}ROBUSTNESS{\\}} {\\{}TO{\\}} {\\{}PCA{\\}} {\\{}PERTURBATIONS{\\}}},\nauthor={Anan Kabaha and Dana Drachsler Cohen},\nyear={2021},\nurl={https://openreview.net/forum?id=oxRaiMDSzwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "oxRaiMDSzwr", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3374/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3374/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3374/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3374/Authors|ICLR.cc/2021/Conference/Paper3374/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3374/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923838251, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3374/-/Official_Comment"}}}, {"id": "z5Y_khAH6en", "original": null, "number": 1, "cdate": 1603681194496, "ddate": null, "tcdate": 1603681194496, "tmdate": 1605024012450, "tddate": null, "forum": "oxRaiMDSzwr", "replyto": "oxRaiMDSzwr", "invitation": "ICLR.cc/2021/Conference/Paper3374/-/Official_Review", "content": {"title": "This paper proposed a feature perturbation procedure with given feature mappings, which can be used to select robust/weak features and generate adversarial attacks. ", "review": "The overall quality of the paper is good. This paper proposed a feature perturbation procedure, as a comparison to the commonly used perturbation to the original input data. Given access to a feature mapping and a black-box classifier, the proposed procedure is able to select the most robust/weak features. This then can be used for two important tasks: to determine a robust neighborhood for a data point using the robust features and to design adversarial examples using the weak features. For the first task, the feature-based robust neighborhood proposed by this paper is shown by experiments to contain far more points than the traditional input-based neighborhood. For the second task, the feature-based adversarial examples require less query to the black-box classifier and have less distortion from the original data points compared with other competitive methods, and thus are more human-imperceptible. These characteristics make the procedure appealing.\nTherefore, the main contribution of the paper (i.e., the perturbation procedure) is important to the ML community and worth further explorations. \nThe clarity of the paper is good. There is no difficulty in understanding the content and experimental details are provided. \n\nCons:\n1. The overall running time of Alg 1 is a concern. \n2. When generating the adversarial examples, a greedy recovery is performed which may be time-consuming when the data dimension is high. \n3. The effectiveness of the proposed procedure seems to strongly depend on the feature mappings. The performance under mapping other than PCA is unknown. \n\nOther comments:\n1. The experiment showed good results with PCA feature mapping. Are there any other feature mappings that might work well with this proposed procedure (Alg 1)?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3374/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3374/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK ROBUSTNESS TO PCA PERTURBATIONS", "authorids": ["~Anan_Kabaha3", "~Dana_Drachsler_Cohen1"], "authors": ["Anan Kabaha", "Dana Drachsler Cohen"], "keywords": ["neural network robustness", "adversarial examples"], "abstract": "A key challenge in analyzing neural networks' robustness is identifying input features for which networks are robust to perturbations. Existing work focuses on direct perturbations to the inputs, thereby studies network robustness to the lowest-level features. In this work, we take a new approach and study the robustness of networks to the inputs' semantic features. We show a black-box approach to determine features for which a network is robust or weak.  We leverage these features to obtain provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. We evaluate our approach with PCA features. We show (1) provably robust neighborhoods are larger: on average by 1.8x and up to 4.5x, compared to the standard neighborhoods, and (2) our adversarial examples are generated using at least 8.7x fewer queries and have at least 2.8x lower L2 distortion compared to state-of-the-art. We further show that our attack is effective even against ensemble adversarial training. ", "one-sentence_summary": "A framework to detect robust and weak features of DNNs and to use them to define robustness neighborhoods and adversarial examples.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kabaha|network_robustness_to_pca_perturbations", "pdf": "/pdf/cba4351e8db45d410cea7d661c31f2821011e291.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHU5RFA97P", "_bibtex": "@misc{\nkabaha2021network,\ntitle={{\\{}NETWORK{\\}} {\\{}ROBUSTNESS{\\}} {\\{}TO{\\}} {\\{}PCA{\\}} {\\{}PERTURBATIONS{\\}}},\nauthor={Anan Kabaha and Dana Drachsler Cohen},\nyear={2021},\nurl={https://openreview.net/forum?id=oxRaiMDSzwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oxRaiMDSzwr", "replyto": "oxRaiMDSzwr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3374/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077067, "tmdate": 1606915772214, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3374/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3374/-/Official_Review"}}}, {"id": "7C7OVt7HjM", "original": null, "number": 2, "cdate": 1603886465296, "ddate": null, "tcdate": 1603886465296, "tmdate": 1605024012387, "tddate": null, "forum": "oxRaiMDSzwr", "replyto": "oxRaiMDSzwr", "invitation": "ICLR.cc/2021/Conference/Paper3374/-/Official_Review", "content": {"title": "The work proposes a potentially interesting improved black-box attack, but comparison to existing attacks would have to be much more convincing.", "review": "This work proposes a method to find weak and robust features via sampling, and utilises the method to (a) define feature-guided neighborhoods and (b) to improve score-based black-box adversarial attacks.\n\nI examine the two main contributions (a) and (b) separately. The feature-guided neighourhoods are basically defined as 1D linear subspaces around a given sample x. The subspaces are usually defined along PCA-projections of the data set, although the definitions would also allow non-linear subspaces. The work claims that along some of these directions a classifier D is robust, and that using verification techniques one can show that these robust subspaces can be much larger than simple epsilon-ball neighbourhoods. While this claim might be true, the significance of this insight is unclear to me. Of course, if carefully chosen, one can find large neighbourhoods in which a classifier does not change its decision - but I fail to see what insights can be taken away.\n\nIn a second step, this work computes weak features, i.e. directions in the pixel space along which the classifier D is generally susceptible, and uses this to perform a score-based adversarial attack. The work claims higher robustness with fewer iterations than AutoZOOM and GenAttack. This is indeed an interesting direction, but the work misses a lot of relevant prior work on this area that should be discussed and compared against (in particular many other score-based but also decision-based attacks, one would need to take into account the number of queries needed to find the weak PCA directions to compare against other attacks, etc.).\n\nIt might be beneficial to remove the discussion of the neighbourhoods and to concentrate on the improvement of black-box attacks (which would have be shown much more convincingly).", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3374/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3374/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NETWORK ROBUSTNESS TO PCA PERTURBATIONS", "authorids": ["~Anan_Kabaha3", "~Dana_Drachsler_Cohen1"], "authors": ["Anan Kabaha", "Dana Drachsler Cohen"], "keywords": ["neural network robustness", "adversarial examples"], "abstract": "A key challenge in analyzing neural networks' robustness is identifying input features for which networks are robust to perturbations. Existing work focuses on direct perturbations to the inputs, thereby studies network robustness to the lowest-level features. In this work, we take a new approach and study the robustness of networks to the inputs' semantic features. We show a black-box approach to determine features for which a network is robust or weak.  We leverage these features to obtain provably robust neighborhoods defined using robust features and adversarial examples defined by perturbing weak features. We evaluate our approach with PCA features. We show (1) provably robust neighborhoods are larger: on average by 1.8x and up to 4.5x, compared to the standard neighborhoods, and (2) our adversarial examples are generated using at least 8.7x fewer queries and have at least 2.8x lower L2 distortion compared to state-of-the-art. We further show that our attack is effective even against ensemble adversarial training. ", "one-sentence_summary": "A framework to detect robust and weak features of DNNs and to use them to define robustness neighborhoods and adversarial examples.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kabaha|network_robustness_to_pca_perturbations", "pdf": "/pdf/cba4351e8db45d410cea7d661c31f2821011e291.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=oHU5RFA97P", "_bibtex": "@misc{\nkabaha2021network,\ntitle={{\\{}NETWORK{\\}} {\\{}ROBUSTNESS{\\}} {\\{}TO{\\}} {\\{}PCA{\\}} {\\{}PERTURBATIONS{\\}}},\nauthor={Anan Kabaha and Dana Drachsler Cohen},\nyear={2021},\nurl={https://openreview.net/forum?id=oxRaiMDSzwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "oxRaiMDSzwr", "replyto": "oxRaiMDSzwr", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3374/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538077067, "tmdate": 1606915772214, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3374/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3374/-/Official_Review"}}}], "count": 10}