{"notes": [{"id": "HyeJB8LKOV", "original": "SyglmHrO_N", "number": 40, "cdate": 1553716790732, "ddate": null, "tcdate": 1553716790732, "tmdate": 1562083049005, "tddate": null, "forum": "HyeJB8LKOV", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "content": {"title": "Compositional GAN (Extended Abstract): Learning Image-Conditional Binary Composition", "authors": ["Samaneh Azadi", "Deepak Pathak", "Sayna Ebrahimi", "Trevor Darrell"], "authorids": ["sazadi@berkeley.edu", "pathak@berkeley.edu", "sayna@berkeley.edu", "trevor@eecs.berkeley.edu"], "keywords": [], "abstract": "Generative Adversarial Networks (GANs) can produce images of surprising complexity and realism but are generally structured to sample from a single latent source ignoring the explicit spatial interaction between multiple entities that could be present in a scene. Capturing such complex interactions between different objects in the world, including their relative scaling, spatial layout, occlusion, or viewpoint transformation is a challenging problem. In this work, we compose a pair of objects in a conditional GAN framework using a novel self-consistent composition-by-decomposition network. Given object images from two distinct distributions, our model can generate a realistic composite image from their joint distribution following the texture and shape of the input objects. Our results reveal that the learned model captures potential interactions between the two object domains, and can output their realistic composed scene at test time.", "pdf": "/pdf/83aab07f85e50629b5b36322f99a1605bb6e433d.pdf", "paperhash": "azadi|compositional_gan_extended_abstract_learning_imageconditional_binary_composition"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "cdate": 1547567085825, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": [".*"]}, "writers": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1547567085825, "tmdate": 1555704438520, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}}, "tauthor": "OpenReview.net"}, {"id": "B1eWsUBNcE", "original": null, "number": 2, "cdate": 1555482264760, "ddate": null, "tcdate": 1555482264760, "tmdate": 1556906150986, "tddate": null, "forum": "HyeJB8LKOV", "replyto": "HyeJB8LKOV", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper40/Official_Review", "content": {"title": "interesting problem and reasonable approach", "review": "The paper tackles the problem of combining two images into one in a sensible way. In particular, the inputs are two objects (e.g., a bottle and a basket) and the output is an image containing both objects (e.g., a bottle in the basket). The challenge is that there aren't enough paired inputs and output. The authors proposed to 1) generate noisy examples by segmentation and inpainting and 2) adding a \"self-consistency\" loss to encourage that objects occurred in the inputs also occur in the output and segments in the output is close to the corresponding objects in the input. The \"self-consistency\" loss is also applied at test time to refine the output.\n\nI find the problem pretty interesting. The model needs to learn the relative positions of the two objects as well as proper occlusion. The approach is pretty reasonable as well. I only have a couple questions / comments below.\n\n- Aside from the paired examples, there is nothing in the loss function encouraging the model to *compose* the inputs with natural occlusion and position etc. So I'd like to see how many paired examples are needed to achieve the reported results, and how the results change with varying numbers of paired examples.\n\n- It would be cool to control the relative position of the two objects.", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper40/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper40/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compositional GAN (Extended Abstract): Learning Image-Conditional Binary Composition", "authors": ["Samaneh Azadi", "Deepak Pathak", "Sayna Ebrahimi", "Trevor Darrell"], "authorids": ["sazadi@berkeley.edu", "pathak@berkeley.edu", "sayna@berkeley.edu", "trevor@eecs.berkeley.edu"], "keywords": [], "abstract": "Generative Adversarial Networks (GANs) can produce images of surprising complexity and realism but are generally structured to sample from a single latent source ignoring the explicit spatial interaction between multiple entities that could be present in a scene. Capturing such complex interactions between different objects in the world, including their relative scaling, spatial layout, occlusion, or viewpoint transformation is a challenging problem. In this work, we compose a pair of objects in a conditional GAN framework using a novel self-consistent composition-by-decomposition network. Given object images from two distinct distributions, our model can generate a realistic composite image from their joint distribution following the texture and shape of the input objects. Our results reveal that the learned model captures potential interactions between the two object domains, and can output their realistic composed scene at test time.", "pdf": "/pdf/83aab07f85e50629b5b36322f99a1605bb6e433d.pdf", "paperhash": "azadi|compositional_gan_extended_abstract_learning_imageconditional_binary_composition"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper40/Official_Review", "cdate": 1554234172294, "reply": {"forum": "HyeJB8LKOV", "replyto": "HyeJB8LKOV", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper40/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper40/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234172294, "tmdate": 1556906091969, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper40/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "S1gpjKyptV", "original": null, "number": 1, "cdate": 1554999717229, "ddate": null, "tcdate": 1554999717229, "tmdate": 1556906150763, "tddate": null, "forum": "HyeJB8LKOV", "replyto": "HyeJB8LKOV", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper40/Official_Review", "content": {"title": "clear and interesting submission", "review": "The authors propose a loss function to generate natural images that include two separate objects via a GAN.  The loss uses a sort of self-supervision by noting that the decomposition of a natural image with two objects into individual object images should match closely to the original object images.  The decomposition and composition network are then learned jointly.  Additionally, at test time, the authors provide a loss that tunes pixels to preserve color and texture.\n\nI thought this short paper was quite clear (given space constraints) --- the objective was presented and described well.\n\nThe authors claim originality that the composition self-consistency loss is a new insight --- I am not familiar with work that conflicts with that claim, though I cannot be certain.\n\nQuestions/comments\n- What are the qualitative and quantitative differences between the $\\hat{c}^{after}$ and $\\hat{c}^{after}_s$ images?  This should be made a bit more clear in the text.\n- In the CelebA + Glasses experiment, what were the composite images used to train?", "rating": "5: Top 15% of accepted papers, strong accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper40/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper40/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compositional GAN (Extended Abstract): Learning Image-Conditional Binary Composition", "authors": ["Samaneh Azadi", "Deepak Pathak", "Sayna Ebrahimi", "Trevor Darrell"], "authorids": ["sazadi@berkeley.edu", "pathak@berkeley.edu", "sayna@berkeley.edu", "trevor@eecs.berkeley.edu"], "keywords": [], "abstract": "Generative Adversarial Networks (GANs) can produce images of surprising complexity and realism but are generally structured to sample from a single latent source ignoring the explicit spatial interaction between multiple entities that could be present in a scene. Capturing such complex interactions between different objects in the world, including their relative scaling, spatial layout, occlusion, or viewpoint transformation is a challenging problem. In this work, we compose a pair of objects in a conditional GAN framework using a novel self-consistent composition-by-decomposition network. Given object images from two distinct distributions, our model can generate a realistic composite image from their joint distribution following the texture and shape of the input objects. Our results reveal that the learned model captures potential interactions between the two object domains, and can output their realistic composed scene at test time.", "pdf": "/pdf/83aab07f85e50629b5b36322f99a1605bb6e433d.pdf", "paperhash": "azadi|compositional_gan_extended_abstract_learning_imageconditional_binary_composition"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper40/Official_Review", "cdate": 1554234172294, "reply": {"forum": "HyeJB8LKOV", "replyto": "HyeJB8LKOV", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper40/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper40/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234172294, "tmdate": 1556906091969, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper40/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "HkxY9mdPq4", "original": null, "number": 1, "cdate": 1555690384797, "ddate": null, "tcdate": 1555690384797, "tmdate": 1556906150550, "tddate": null, "forum": "HyeJB8LKOV", "replyto": "HyeJB8LKOV", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper40/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Compositional GAN (Extended Abstract): Learning Image-Conditional Binary Composition", "authors": ["Samaneh Azadi", "Deepak Pathak", "Sayna Ebrahimi", "Trevor Darrell"], "authorids": ["sazadi@berkeley.edu", "pathak@berkeley.edu", "sayna@berkeley.edu", "trevor@eecs.berkeley.edu"], "keywords": [], "abstract": "Generative Adversarial Networks (GANs) can produce images of surprising complexity and realism but are generally structured to sample from a single latent source ignoring the explicit spatial interaction between multiple entities that could be present in a scene. Capturing such complex interactions between different objects in the world, including their relative scaling, spatial layout, occlusion, or viewpoint transformation is a challenging problem. In this work, we compose a pair of objects in a conditional GAN framework using a novel self-consistent composition-by-decomposition network. Given object images from two distinct distributions, our model can generate a realistic composite image from their joint distribution following the texture and shape of the input objects. Our results reveal that the learned model captures potential interactions between the two object domains, and can output their realistic composed scene at test time.", "pdf": "/pdf/83aab07f85e50629b5b36322f99a1605bb6e433d.pdf", "paperhash": "azadi|compositional_gan_extended_abstract_learning_imageconditional_binary_composition"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper40/Decision", "cdate": 1554814603387, "reply": {"forum": "HyeJB8LKOV", "replyto": "HyeJB8LKOV", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554814603387, "tmdate": 1556906101697, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}], "count": 4}