{"notes": [{"id": "fylclEqgvgd", "original": "TkY-LX9-OIoT", "number": 3581, "cdate": 1601308398004, "ddate": null, "tcdate": 1601308398004, "tmdate": 1616028776360, "tddate": null, "forum": "fylclEqgvgd", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "9yxu3284Atv", "original": null, "number": 1, "cdate": 1610040433421, "ddate": null, "tcdate": 1610040433421, "tmdate": 1610474033739, "tddate": null, "forum": "fylclEqgvgd", "replyto": "fylclEqgvgd", "invitation": "ICLR.cc/2021/Conference/Paper3581/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The authors have done a very thorough job of responding to the comments from reviewers. The paper has a clear contribution, namely that attention maps predict contacts as well as existing unsupervised pipelines. This paper deserves to be published.\n\nIn the final version, the authors should discuss briefly \"BERTology Meets Biology: Interpreting Attention in Protein Language Models\"(https://openreview.net/forum?id=YWtLZvLmud7) and \"Improving Generalizability of Protein Sequence Models via Data Augmentations\" (https://openreview.net/forum?id=Kkw3shxszSd). However, the authors should also make sure that the final version respects the ICLR length limits.\n\nI am recommending poster acceptance because the results are anticlimactic given the recent success of Deepmind at CASP 2020.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "tags": [], "invitation": {"reply": {"forum": "fylclEqgvgd", "replyto": "fylclEqgvgd", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040433408, "tmdate": 1610474033723, "id": "ICLR.cc/2021/Conference/Paper3581/-/Decision"}}}, {"id": "a2VIZand6bJ", "original": null, "number": 1, "cdate": 1602947717907, "ddate": null, "tcdate": 1602947717907, "tmdate": 1606495740297, "tddate": null, "forum": "fylclEqgvgd", "replyto": "fylclEqgvgd", "invitation": "ICLR.cc/2021/Conference/Paper3581/-/Official_Review", "content": {"title": "Using Transformers for protein contact prediction is not new", "review": "## Summary\nThe paper shows that Transformers trained unsupervised on millions of protein sequences learn information about protein contacts by using attention maps for contact prediction. The paper is mostly clearly written and discusses server interesting ablation experiments. However, two recent papers that appeared on arXiv before the ICLR submission deadlines also use Transformers for protein contact prediction. These papers and other methods for contact prediction beyond Gremlin are not described. I therefore consider the contributions as insufficient for an ICLR submission.\n\n## Major comments\n\n1. Using Transformer attention maps for protein contact prediction is not new. See Rives et al, 2020, \u2018Biological structures and functions emerge\u2026\u2019, section 5.2, and Vig et al, 2020, \u2018Bertology\u2019 section 4.2. Both publications appeared on arXiv at least one month before the ICLR submission deadline and are not clearly discussed in the paper.\n\n2. The introductions discusses existing work on Transformers for protein languages models. Existing methods for contact prediction (beyond Gremlin), however, are not described sufficiently.\n\n3. It is unclear which sequences were used for training the Transformer models and how similar they are to test sequences.\n\n4. The paper compares Transformers to Gremlin. However, it is unclear how well they perform to the CASP state-of-the art (see also Rives et al, 2020).\n\n5. Section 3.4  does not describe clearly enough how attention maps were used for predicting contact maps. How were attention maps symmetrized? Which layers and heads were used and how were they aggregated? What is the number of resulting features that were used to train the logistic regression model? APC is not described or cited.\n\n6. Section 4.5 discusses that Transformers can be also used for secondary structure prediction. This is not new (see Rives 2020 and Vig 2020) and does not fit well to the rest of the paper, which is about contact prediction. \n\n6. Section 4.8: Using transformers for generating proteins with natural properties is not new (see Madani et al, 2020, \u2018ProGen\u2019 or Rives et al, 2020). \u2018Wang & Cho\u2019 were not the first who used Transformers generativity (see Vaswani, 2017).\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3581/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fylclEqgvgd", "replyto": "fylclEqgvgd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3581/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073318, "tmdate": 1606915781870, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3581/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3581/-/Official_Review"}}}, {"id": "AsQVaEgEL_", "original": null, "number": 2, "cdate": 1603839187818, "ddate": null, "tcdate": 1603839187818, "tmdate": 1606350160970, "tddate": null, "forum": "fylclEqgvgd", "replyto": "fylclEqgvgd", "invitation": "ICLR.cc/2021/Conference/Paper3581/-/Official_Review", "content": {"title": "Review for TRANSFORMER PROTEIN LANGUAGE MODELS ARE UNSUPERVISED STRUCTURE LEARNERS", "review": "In this paper, the authors show that transformer protein language models can learn protein contacts from the unsupervised language modelling objectives. They also show that the residue-residue contacts can be extracted by sparse logistic regression to learn coefficients on the attention heads. One of the advantages of using transformers models is that they do not require an alignment step nor the use of specialized bioinformatics tools (which are computationally expensive). When compared to a method based on multiple sequence alignment, the transformers models can obtain a similar or higher precision.\n\nContributions of this paper are:\n- showing that the attention maps built in Transformer-based protein languages learn protein contacts, and when extracted, they perform competitively for protein contact prediction;\n- a method for extracting attention maps from Transformer models;\n- a comparison between a recent protein transformer protein language model (which does dot require sequence alignment), and a pseudo-likelihood-based optimization method that uses multiple sequence alignment;\n- an analysis of how much the supervised learning (logistic regression) contributes to the results.\n\nThe paper covers a relevant topic and it is easy to read. \n\nHowever, I have a number of concerns. The main contribution of the paper is that attention maps built in Transformer-based protein languages learn protein contacts and can be used for protein contact prediction. However, this was reported before in Rives et al.(2019) (doi: 10.1101/622803). Also, several methods have been developed for this problem, but are not included in the comparisons. Finally, the provided implementation details are not sufficient to reproduce the results of the paper. \nI detail some of these concerns below, together with questions/suggestions for improvements:\n\n1) I would recommend comparing transformers to other methods besides Gremlin, or justify why other methods were not included. This review can be helpful:\n\n(Adhikari B, Cheng J., 2016.. doi: 10.1007/978-1-4939-3572-7_24)\n\nAlso, more recent methods that were published after the review are:\n\n(Badri Adhikari, 2020. https://doi.org/10.1093/bioinformatics/btz593)\n\n(Luttrell  et al., 2019. https://doi.org/10.1186/s12859-019-2627-6)\n\n(Gao et al.,2019. https://doi.org/10.1038/s41598-019-40314-1)\n\n(Ji S et al., 2019. https://doi.org/10.1371/journal.pone.0205214)\n\n2) On page 7, the authors state that \"We find that the logistic regression probabilities are reasonably well calibrated estimators of true contact probability and can be used directly as a measure of the model's confidence (Figure 10a)\". However, from the plot in Figure 10a, it is not totally clear that the probabilities are well calibrated. Could the authors add more justifications of why they consider it well calibrated? Could they also show a comparison of the calibration of the other transformer models, perhaps using MSE as a calibration metric?\n\n3) To understand the occurence of false positives, the authors analyze the Manhattan distance between the predicted contact and the true contact, which is between 1 and 4 for most false positives. They also show an example of a homodimer, for which predictions were far from the true contacts, and explain that the model is picking up inter-chain interactions. Could the authors report how many predictions have a Manhattan distance larger than 4? Is this one example representative of the group of false positives far from the true contact? Maybe the authors could analyse whether this happens in most of the cases.\n\n4) While ESM-1 is open-source and publicly available, this is not the case for ESM-1b. In section A.5, the authors provide implementation details as differences between ESM-1 and ESM-1b, stating \u201cCompared to ESM-1, the main changes in ESM-1b are: higher learning rate; dropout after word embedding; learned positional embeddings; final layer norm before the output; and tied input/output word embeddings. The weights of all ESM models throughout the training process were provided to us by the authors.\u201d. In my opinion, this is not enough to reproduce the results in this paper. To make it reproducible, the authors need to provide a detailed enough description of the differences to make the reader able to implement ESM-1b, or provide the weights and hyperparameters required to reproduce their results.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3581/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fylclEqgvgd", "replyto": "fylclEqgvgd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3581/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073318, "tmdate": 1606915781870, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3581/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3581/-/Official_Review"}}}, {"id": "goICfmHPbVf", "original": null, "number": 3, "cdate": 1603965711938, "ddate": null, "tcdate": 1603965711938, "tmdate": 1606244280676, "tddate": null, "forum": "fylclEqgvgd", "replyto": "fylclEqgvgd", "invitation": "ICLR.cc/2021/Conference/Paper3581/-/Official_Review", "content": {"title": "Interesting analyses, but has overall limited novelty", "review": "**Summary**\nThe paper performs a number of analyses centered around the ability of transformer-based language models trained on protein sequence data to learn representations useful for predicting protein secondary and tertiary structure (the latter as contact maps). Specifically, the paper studies several pre-trained transformer models by fitting an L1-penalized logistic regression to amino acid pair contacts. Several experiments are performed to showcase that (i) transformer-based representations can outperform state-of-the art methods based on MSA in terms of contact prediction precision; (ii) that the necessary information for contact predictions in these representations is learned in an unsupervised manner (and not by the logistic regression put on top of these representations); and (iii) that the contact prediction probabilities are reasonably well calibrated.\n\n**Score justification**\nIn its current form the paper presents interesting analyses, but has overall limited novelty. The ability of transformer models to learn representations predictive of secondary and tertiary structure has been demonstrated before (including in the papers proposing the models used by the authors). Furthermore, I have some questions regarding the methodology employed by the authors.\n\n\n**Major comments**\n* The main metric employed by the authors is the precision of the top L (protein length) contact prediction for a given range (P@L). I wonder why the authors do not also consider recall at L as an accompanying metric for reporting the results.\n* When comparing ESM to the baseline Gremlin method, the authors consider two scenarios: (i) Gremlin trained on the trRosetta data; and (ii) Gremlin trained on the same data as the ESM transformer model. Overall, Gremlin trained on the ESM data - which is arguably the correct baseline for the ESM model -  performs worse than Gremlin trained on the trRosetta data. Why is that the case? How does the procedure for preparing MSA for the ESM data compare to that of the trRosetta data? Can it be tuned to improve Gremlin's performance?\n* The paper compares several transformer models that differ primarily in the model size, dataset size and hyper-parameters. As can be seen from Table 1 of the manuscript, these differences are clearly important for the contact prediction task and thus should be summarized and discussed in more detail.\n* From what I understand the sequences from the testing set of the contact prediction problem (or sequences highly similar to them) could appear in the training sets of the considered transformer models. This creates some information leakage. It's unclear from the results presented in the paper whether it is an issue or not - how does contact prediction precision / recall change as sequence similarity to the ESM training set drops?\n* The authors present analysis on the usefulness of the representations learned by various attention heads for contact prediction; and on robustness of such predictions. I wonder how robust the results of these analyses are - they appear to have been performed using a single checkpoint of the ESM model, which is a result of stochastic training from random initialization.\n* In the Appendix the authors talk about the benefit of using predicted contact maps for inferring the all-atom protein 3D structure. However no results on this are presented. I would be very eager to see the comparison of 3D structure accuracy inferred with ESM-predicted and Gremlin-predicted contacts.\n\n**Minor comments**\n* Introduction talks about the ESM-1b model but (as far as I can tell) a reference isn't provided until a later section.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3581/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fylclEqgvgd", "replyto": "fylclEqgvgd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3581/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073318, "tmdate": 1606915781870, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3581/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3581/-/Official_Review"}}}, {"id": "jkW2ikDBLQ_", "original": null, "number": 19, "cdate": 1606227237372, "ddate": null, "tcdate": 1606227237372, "tmdate": 1606227281080, "tddate": null, "forum": "fylclEqgvgd", "replyto": "5eYq0uVDx2S", "invitation": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment", "content": {"title": "Substantial improvements to the manuscript", "comment": "The authors have addressed all the comments appropriately and have made substantial modifications to the paper considering my comments. \n\nIn my opinion, the related work section and the clear explanation on the supervised vs unsupervised contact prediction literature greatly improved the manuscript .\n\nI am still slightly concerned about reproducibility, as the modifications to the ESM architecture are not very clear to me. However, The authors have promised to share the weights, .\n\nIn the new version of the paper it is still not clear how many predictions have a Manhattan distance larger than 4. It would be good if the authors could  provide a figure, or a table, detailing the distribution of predicted contacts and their respective Manhattan distances to the closest true contact. I appreciate that they provided the proportion of proteins with at least one predicted contact > 4 at different thresholds for contact probability, but in my opinion this does not give a clear enough picture.\n\nAll things considered, I believe the paper has improved substantially, and I am willing to increase my score.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3581/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fylclEqgvgd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3581/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3581/Authors|ICLR.cc/2021/Conference/Paper3581/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836041, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment"}}}, {"id": "z_nyYFw_nG", "original": null, "number": 14, "cdate": 1605466403988, "ddate": null, "tcdate": 1605466403988, "tmdate": 1606149751238, "tddate": null, "forum": "fylclEqgvgd", "replyto": "fylclEqgvgd", "invitation": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment", "content": {"title": "Updated Revisions", "comment": "In order to address concerns regarding related work and evaluation, we have updated our submission with the following changes:\n\n1. New Related Work section, describing unsupervised and supervised contact prediction along with relevant citations.\n2. CASP 13 Evaluations (Section A.6, Table 4, and Figure 5).\n3. Comparisons with mfDCA and PSICOV baselines.\n4. Comparisons with Rives et al. 2020 supervised bilinear model on CASP13.\n5. Computed mean squared error for calibration analysis, and show comparison on ESM models.\n6. Removed speculative wording around sources of alignment failures.\n7. Bootstrapping analysis (Section A.10)\n8. Secondary structure analysis moved to appendix\n9. Additional discovered mode of false-positive contacts shown"}, "signatures": ["ICLR.cc/2021/Conference/Paper3581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fylclEqgvgd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3581/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3581/Authors|ICLR.cc/2021/Conference/Paper3581/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836041, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment"}}}, {"id": "II-nlX4-ZR7", "original": null, "number": 10, "cdate": 1605151267031, "ddate": null, "tcdate": 1605151267031, "tmdate": 1605152626106, "tddate": null, "forum": "fylclEqgvgd", "replyto": "a2VIZand6bJ", "invitation": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "\\>\\>The paper shows that Transformers trained unsupervised on millions of protein sequences learn information about protein contacts by using attention maps for contact prediction...\n\nWe thank the reviewer for their time, interest in the paper, and constructive feedback.\n\n\\>\\>However, two recent papers that appeared on arXiv before the ICLR submission deadlines also use Transformers for protein contact prediction. See Rives et al, 2020, \u2018Biological structures and functions emerge\u2026\u2019, section 5.2, and Vig et al, 2020, \u2018Bertology\u2019 section 4.2. \n\nThis is the first paper to show state-of-the-art results for unsupervised contact prediction from a transformer protein language model. Prior work Rives et al. 2020 benchmarks supervised contact prediction with deep residual networks. Vig et al. 2020 show that one specific head of the TAPE transformer is correlated with contacts (see Vig et al. 2020 Fig 4), but make no comparison to state-of-the-art methods for unsupervised contact prediction. In contrast, our work provides a new method that results in state-of-the-art performance on the unsupervised contact prediction problem. \n\nMoreover this is the first paper to demonstrate a state-of-the-art result for contact prediction from protein language modeling -- this is an important result for protein language modeling as previous work e.g. Rao et al. 2019, and Rives et al. 2020 have shown that protein language models fall well below state-of-the-art performance on supervised contact prediction tasks.\n\n\\>\\>These papers and other methods for contact prediction beyond Gremlin are not described...\n\nWe will add a more thorough description of related work addressing unsupervised and supervised contact prediction.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fylclEqgvgd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3581/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3581/Authors|ICLR.cc/2021/Conference/Paper3581/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836041, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment"}}}, {"id": "-_tHCjK8_O-", "original": null, "number": 11, "cdate": 1605151285543, "ddate": null, "tcdate": 1605151285543, "tmdate": 1605152556456, "tddate": null, "forum": "fylclEqgvgd", "replyto": "II-nlX4-ZR7", "invitation": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment", "content": {"title": "Response to Reviewer 1 (Continued)", "comment": "\\>\\>Major comments\n\n\\>\\>1. Using Transformer attention maps for protein contact prediction is not new...\n\nWe address novelty of this work above and in the comment to all reviewers. Thank you for pointing out the need for a more extensive related work section, we agree and will add this section discussing both Rives, et al. 2020 and Vig, et al. 2020 as well as references suggested by the other reviewers. \n\n\\>\\>2. ...existing methods for contact prediction (beyond Gremlin), however, are not described sufficiently.\n\nWe agree a more detailed background is necessary and will add this to the manuscript.\n\n\\>\\>3. It is unclear which sequences were used for training the Transformer models and how similar they are to test sequences.\n\nESM transformers were trained using UniRef 50, which is noted in the introduction and in Figure 1. Note that the models and baselines are given access to the same set of sequences during test time. Since the number of similar sequences in the training set can be judged by the MSA depth of the sequence, we see in Fig 6 that as expected both the baseline and ESM perform better when there are more similar sequences available in the training set. However we show that ESM performs better than the baseline when fewer similar sequences are available in the ESM training set.\n\nWe will also add new experiments on CASP13 proteins to the revision. Since the model was trained on data available prior to CASP13, these sequences will not be in the training set.\n\n\\>\\>4. ...it is unclear how well they perform to the CASP state-of-the art (see also Rives et al. 2020).\n\nWe will add a comparison of unsupervised methods on CASP13 comparing the transformer attention maps, pseudolikelihood methods (Gremlin), mean field methods (Evcouplings), and sparse inverse covariance estimation (Psicov). We note that Gremlin is considered a state-of-the-art method for this problem.\n\n\\>\\>5. Section 3.4 does not describe clearly enough how attention maps were used for predicting contact maps...\n\nWe provide a much more detailed explanation of the logistic regression in Appendix Section A.6. Attention maps are 2D matrices, so we symmetrize via 0.5 * (A + A^T). All layers and heads were used as input features, for a total of 660 features in the ESM-1b model (33 layers * 20 heads). We describe and cite APC in Appendix section A.2. Thank you for pointing out that APC is not cited in the main text -- we will add this citation.\n\n\\>\\>6. Section 4.5 discusses that Transformers can be also used for secondary structure prediction. This is not new...\n\nWe agree that this is a peripheral result and for that reason the figures relating to secondary structure are in the appendix already. We thought it was interesting to describe as this is a different and more interpretable way to extract secondary structure from Transformer models than used in previous work e.g. Rives et al. 2019 and Vig et al. 2020 both of which did not use the attention maps. We note that local contacts (within a sequence separation of 6) can correspond to secondary structure, and so we use secondary structure as a proxy for analyzing the accuracy of contacts within this sequence separation range.\n\n\\>\\>7. Section 4.8: Using transformers for generating proteins with natural properties is not new (see Madani et al. 2020, \u2018ProGen\u2019 or Rives et al. 2020). \u2018Wang & Cho\u2019 were not the first who used Transformers generativity (see Vaswani, 2017).\n\nVaswani et al. use an autoregressive decoder transformer as opposed to an encoder. We cite Wang & Cho as the first to generate from a non-autoregressive encoder transformer trained with a masked language modeling objective. Rives et al. 2019 and Rives et al. 2020 do not show results on generating proteins. Madani et al. 2020 show that it is possible to generate proteins that might preserve natural properties. However there are key differences. First, they use autoregressive decoder transformers, rather than bidirectional encoders. Our analysis demonstrates that bidirectional encoder transformers can also be used to generate proteins with natural properties. Additionally, our approach can generate proteins in the neighborhood of an existing protein, which may be highly useful for tasks such as protein engineering. Finally our analysis shows that generated sequences directly preserve the statistics needed to infer protein contacts which is not shown by Madani et al. 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper3581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fylclEqgvgd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3581/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3581/Authors|ICLR.cc/2021/Conference/Paper3581/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836041, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment"}}}, {"id": "L8vzLw3gWz_", "original": null, "number": 7, "cdate": 1605150775024, "ddate": null, "tcdate": 1605150775024, "tmdate": 1605152476241, "tddate": null, "forum": "fylclEqgvgd", "replyto": "goICfmHPbVf", "invitation": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for their time and interest in the paper and for helpful comments.\n\n\\>\\>Several experiments are performed to showcase that (i) transformer-based representations can outperform state-of-the art methods based on MSA in terms of contact prediction precision; (ii) that the necessary information for contact predictions in these representations is learned in an unsupervised manner (and not by the logistic regression put on top of these representations); and (iii) that the contact prediction probabilities are reasonably well calibrated.\n\n\\>\\>In its current form the paper presents interesting analyses, but has overall limited novelty. The ability of transformer models to learn representations predictive of secondary and tertiary structure has been demonstrated before \n\nThe reviewer\u2019s main concern appears to be novelty. However we note that the reviewer agrees in point (i) above that the paper shows  \u201ctransformer-based representations can outperform state-of-the art methods based on MSA in terms of contact prediction precision.\u201d No prior work has shown state-of-the-art performance for unsupervised contact prediction from a protein language model.\n\nUnsupervised contact prediction is an important and well studied problem (discussed in more depth in the comment to all reviewers) that has seen little progress since the introduction of pseudolikelihood maximization -- the state-of-the-art baseline we use in this paper. Additionally in point (ii) the reviewer agrees \u201cthat the necessary information for contact predictions in these representations is learned in an unsupervised manner.\u201d This is also an important contribution of the work -- this paper is the first to show that state-of-the-art contacts are learned by Transformer language models in an unsupervised and interpretable manner.\n\nWe realize we have not well situated the paper w.r.t. the supervised contact prediction literature, and prior work with protein language models in the supervised setting. We will endeavor to address this in the revision incorporating feedback from reviewers and additional references.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fylclEqgvgd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3581/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3581/Authors|ICLR.cc/2021/Conference/Paper3581/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836041, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment"}}}, {"id": "akhnP6uu9HV", "original": null, "number": 6, "cdate": 1605150295026, "ddate": null, "tcdate": 1605150295026, "tmdate": 1605152434426, "tddate": null, "forum": "fylclEqgvgd", "replyto": "x0Fqg1hrZXh", "invitation": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment", "content": {"title": "Response to Reviewer 4 (Continued)", "comment": "Major comments\n\\>>1. Missing related work...\n\nThank you for the suggestions and we agree that there is significant prior work in protein contact prediction that should be cited. We will add a Related Work incorporating these suggestions and additional references. While Risselman et al. 2018 and Bepler & Berger 2019 show that deep unsupervised models may learn structural information, our work demonstrates that this information is interpretable and accessible with little or no supervision required.\n\n\\>>Before this work, others have looked at fine tuning language models for contact prediction...\n\nAs far as we are aware, previous approaches using protein language models for contact prediction (including Bepler & Berger 2018, Rives et al. 2019, and Rao et al. 2019) consider the supervised contact prediction problem. Here, we focus on the model\u2019s ability to learn contacts without supervision. In particular, our top-1, 5, and 10 head results show that the model does not require any supervision at all to predict contacts.\n\n\\>>Many methods have surpassed GREMLIN for contact prediction using evolutionary couplings\u2026. \n\nMany supervised contact prediction methods have surpassed Gremlin for contact prediction, including those using evolutionary couplings, however we believe pseudolikelihood maximization (which Gremlin implements) is still considered state-of-the-art for unsupervised contact prediction. We will add results on the CASP data to make this point more clear.\n\n\\>>Minor Comments:\n\n\\>>Although multiple sequence alignment methods have challenges...\n\nDickson & Gloor (2012) find that errors in the alignment can cause errors in downstream coevolution analyses. Malinverni & Barducci (2019) find that alignments that mix sub-families in an MSA cause errors in coevolution-based contact prediction. We will add references to these works and remove speculative comments.\n\n\\>>1. The authors use the language model without fine tuning...on MSAs...\n\nWe do perform this experiment (we call it \u201cevolutionary finetuning\u201d as proposed by Alley et al. 2019).  We discuss it briefly in Section 4.2 and in further detail in section A.11. We find that fine-tuning on individual MSAs leads to a minimal increase in performance, likely due to rapid overfitting of the large model. We note that this analysis is limited -- it is possible that fine tuning only certain layers or otherwise limiting the model\u2019s ability to overfit may improve performance. We leave this to future work. We also show that averaging over sequences in the MSA can provide similar benefits without the costs of fine-tuning.\n\n\\>>2. Eight iterations of jackhmmer is a lot...\n\nThank you for this insightful comment. We propose to re-do this experiment following the procedure of Zhang et al. 2019, performing jackhmmer iterations until an Neff of 128 is reached.\n\n\\>>3. How are sequence depths in Figure 3 calculated?\n\nWe use the raw number of sequences in Figure 3.\nThings that would improve my rating:\n\n\\>>1. Provide a more comprehensive background review.\n\nThank you for the references -- we agree and will include this in the revision.\n\n\\>>2. Compare with state-of-the-art evolutionary coupling-based contact prediction methods.\n\nWe believe that pseudolikelihood maximization as implemented by Gremlin is the current state-of-the-art unsupervised contact prediction method. We note that two new methods for fitting an MRF to an alignment have been proposed (Vorberg et al. 2018, Figliuzzi et al. 2018), but have been shown to have nearly identical performance to pseudolikelihood maximization.\n\nWe specifically do not claim to achieve a state-of-the-art supervised contact prediction method. Instead, we claim that as with pseudolikelihood maximization, protein contacts naturally emerge from the unsupervised training signal in an interpretable and highly accessible manner. Therefore we do not believe that comparison to supervised methods, which incorporate significantly more information, is warranted.\n\n\\>>3. Compare with other language model-based contact prediction methods.\n\nSame as response to 2. Since prior language model-based contact prediction methods are trained with thousands of structures, it would be inappropriate to compare this setting (large models with millions of parameters trained with thousands of protein structures) with the unsupervised setting (logistic regression fit with zero to twenty proteins); these are fundamentally different problem settings.\n\n\\>>4. What should interest the general machine learning community about this paper?\n\nPlease see the note above and in the response to all reviewers. We view this paper as arguing for a fundamentally different interpretation of learned representations in transformers, one that is highly interpretable and directly maps to physical structures.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fylclEqgvgd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3581/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3581/Authors|ICLR.cc/2021/Conference/Paper3581/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836041, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment"}}}, {"id": "5eYq0uVDx2S", "original": null, "number": 9, "cdate": 1605151077760, "ddate": null, "tcdate": 1605151077760, "tmdate": 1605151077760, "tddate": null, "forum": "fylclEqgvgd", "replyto": "AsQVaEgEL_", "invitation": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for their time, interest, and helpful critique.\n\n\\>\\>Contributions of this paper are [...] showing that the attention maps built in Transformer-based protein languages learn protein contacts, and when extracted, they perform competitively for protein contact prediction ...\n\nWe agree on the contributions the reviewer has identified.\n\n\\>\\>However, I have a number of concerns.\n\nBelow and in the comment to all reviewers we outline a plan to address these concerns.\n\n\\>\\>However, this was reported before in Rives et al (2019)\n\nRives et al. 2019 does not report an analysis of attention maps. Rather, it uses the output from the final layer for supervised contact prediction.\n\n\\>\\>Also, several methods have been developed for this problem, but are not included in the comparisons.\n\nWe believe that pseudolikelihood maximization (and by extension Gremlin, which implements this method) is the current state-of-the-art for unsupervised contact prediction. To address the concern we will add a comparison to the Evcouplings implementation of mean-field inference, and to the Psicov implementation of sparse inverse covariance matrix estimation.\n\n\\>\\>I would recommend comparing transformers to other methods besides Gremlin...\n\nThank you for the suggestion. Section 2.2. of Adhikari 2016 describes evolutionary coupling-based methods. We note that Gremlin is an implementation of the pseudolikelihood based methods discussed in this section. The mean field approximation is also discussed here, as well as sparse inverse covariance matrix estimation. We propose adding comparisons to the mean field approximation (Evcouplings implementation) and the sparse inverse covariance matrix (Psicov implementation).\n\n\\>\\>Also, more recent methods that were published after the review are...\n\nThese citations are for supervised contact prediction methods which are all deep neural networks trained with supervision from many protein structures. A comparison to our unsupervised contact prediction method is not appropriate as the problem settings are fundamentally distinct. We will add a discussion of supervised methods to the related work section.\n\n\\>\\>However, from the plot in Figure 10a, it is not totally clear that the probabilities are well calibrated...\n\nWe agree that asserting the model is \u201cwell calibrated\u201d is unclear without a baseline. Since it is not obvious what the correct baseline should be, we will reword this \u201cwe see that the model\u2019s predicted probability is correlated with the actual contact probability.\u201d We will add Pearson correlation between predicted and actual contact probability for the ESM1b model as well as the other transformer models as suggested.\n\n\\>\\>Could the authors report how many predictions have a Manhattan distance larger than 4...\n\nWe appreciate the suggestions and will update the manuscript with the number of predictions with Manhattan distance greater than 4. After the submission deadline we have analyzed an alternate failure mode where the hallucinated contact is not representative of a true contact. We will add an example of this failure mode as well. We note that in both the old and new failure mode, hallucinated contacts appear in the Gremlin contacts as well. An analysis of which failure modes are most common is a very interesting idea, but would require too much manual work to be completed in the rebuttal period.\n\n\\>\\>To make it reproducible...\n\nWe agree on the importance of reproducibility. We will make available contact prediction weights for ESM-1 and ESM-1b models allowing loading the models released by the ESM authors, along with a `predict_contacts` API. If you would like to review the code yourself, we will make the effort to anonymize it as much as possible.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fylclEqgvgd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3581/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3581/Authors|ICLR.cc/2021/Conference/Paper3581/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836041, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment"}}}, {"id": "ofLAPKInKfW", "original": null, "number": 8, "cdate": 1605150803098, "ddate": null, "tcdate": 1605150803098, "tmdate": 1605150803098, "tddate": null, "forum": "fylclEqgvgd", "replyto": "L8vzLw3gWz_", "invitation": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment", "content": {"title": "Response to Reviewer 2 (Continued)", "comment": "Major comments\n\n\\>\\>The main metric\nPrecision at L is the standard contact prediction metric across the literature. Because having a small number of highly accurate contacts is useful (Skolnik et al. 1997, Kim et al. 2014), the field has standardized around this metric.\n\n\\>\\>When comparing ESM to the baseline Gremlin method\n\nGremlin generally performs well when sequences are filtered using an identity cutoff of 80-90% similarity. As per Rives et al. 2020, ESM used a sequence identity clustering at 50% to train their model, which might not be optimal for Gremlin. The trRosetta data is our attempt to optimize Gremlin performance as much as possible. We believe this is a very strong baseline since these MSAs were used to achieve sota results for supervised contact prediction in Yang et al. 2020. Note it uses an optimal sequence identity cutoff for Gremlin, along with a series of e-value similarity thresholds. Finally, it augments smaller MSAs with additional metagenomic data. \n\n\\>\\>The paper compares several transformer models\n\nThank you for the suggestion. We will add a table describing the differences in more detail and discussion to the paper describing how factors that vary between the models influence the results. We describe some ESM-1b changes in section A.5. The ESM-1b authors have made the model publicly available at https://github.com/facebookresearch/esm\n\n\\>\\>the sequences from the testing set of the contact prediction problem (or sequences highly similar to them) could appear in the training sets of the considered transformer models\n\nPlease note that there isn\u2019t an information leakage problem here in the sense that the baseline has access to the very same or strictly more sequences than our model was trained on. From Figure 7 we see that there is clearly a correlation between MSA depth and performance for both ESM and the baseline. MSA depth should provide a good proxy not just for sequence similarity (which is merely distance to the nearest sequence) but for the density of similar sequences present in the training set. We find that ESM outperforms the baseline significantly when the MSA depth is low (few similar sequences in the training dataset), and believe this is one of the strengths of our approach.\n\nTo address this overlap more clearly, we will add results on CASP13 to the revision. Since training data for ESM-1b was generated prior to CASP13, these sequences will not have appeared in the training set. \n\n\\>\\>I wonder how robust the results of these analyses are\n\nIn order to improve analysis of robustness, we will show bootstrapped results for training 100 different logistic regressions using randomly sampled training proteins.\n\nIt is computationally expensive to train transformer protein language models, and so few are available for evaluation. We do show contact prediction results on multiple distinct transformer models, including models trained with different architectures by different groups on different data (ESM-1 6, 12, 34 layers, ESM-1b, ProtBERT-BFD, and TAPE).\n\n\\>\\>I would be very eager to see the comparison of 3D structure accuracy inferred with ESM-predicted and Gremlin-predicted contacts.\n\nWe would also be very interested in comparing 3D structure inferred with ESM versus Gremlin contacts. This is likely beyond the scope of a revision and would be interesting for future work."}, "signatures": ["ICLR.cc/2021/Conference/Paper3581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fylclEqgvgd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3581/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3581/Authors|ICLR.cc/2021/Conference/Paper3581/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836041, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment"}}}, {"id": "x0Fqg1hrZXh", "original": null, "number": 5, "cdate": 1605150070831, "ddate": null, "tcdate": 1605150070831, "tmdate": 1605150520859, "tddate": null, "forum": "fylclEqgvgd", "replyto": "34HFsILFxM", "invitation": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We thank the reviewer for their time and attention to the paper and for detailed comments.\n\n\\>>The general concept of fine tuning protein language models for contact prediction has circulated for some time which lessens the core contribution,\n\\>>The existence of previous language model-based contact prediction methods reduces the novelty of this work, especially given that the model used here is from Rives et al. 2019, who already look at contact prediction.\n\nWhile protein language models have been studied for contact prediction, e.g. Rives et al. 2019, Rao et al. 2019, this has been in the supervised setting. No existing work applies the models to the unsupervised contact prediction problem. This is the first work to demonstrate that unsupervised learning from a protein language model exceeds performance of state-of-the-art evolutionary couplings based unsupervised contact prediction.\n\n\\>>Overall this is an interesting work, though there is quite a bit of background on contact prediction missing.\n\nThank you for pointing out additional references. We will add a related work section covering contact prediction and other topics.\n\n\\>>This paper is also very application specific and may not present new machine learning methods of general interest to the ICLR community.\n\\>>With this in mind, the manuscript may be better suited to submission at a biology specific venue.\n\nWe respectfully disagree. In this paper we propose an interpretable machine learning model that achieves state-of-the-art performance on an important unsupervised learning task in structural biology. This provides strong evidence that attention-based representations produced by unsupervised language modeling objectives can directly represent physical structures, which is of interest to the ICLR community.\n\n\\>>Furthermore, no comparisons with state-of-the-art evolutionary coupling-based or language model-based contact prediction methods are performed.\n\nPseudolikelihood maximization is the current state-of-the-art for unsupervised contact prediction (we use the Gremlin implementation). We will also add mean-field DCA (as implemented by Evcouplings) and sparse inverse covariance (Psicov implementation) as comparisons. There are no unsupervised language model-based contact prediction methods for comparison."}, "signatures": ["ICLR.cc/2021/Conference/Paper3581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fylclEqgvgd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3581/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3581/Authors|ICLR.cc/2021/Conference/Paper3581/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836041, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment"}}}, {"id": "ivFah-V-rHc", "original": null, "number": 4, "cdate": 1605149938284, "ddate": null, "tcdate": 1605149938284, "tmdate": 1605150477853, "tddate": null, "forum": "fylclEqgvgd", "replyto": "rFdUCNIlG_R", "invitation": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment", "content": {"title": "References", "comment": "[1] Rives et al. (2020). Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences.\n\n[2] Rao et al. (2019). Evaluating Protein Transfer Learning with TAPE.\n\n[3] Vig et al. (2020). BERTology Meets Biology: Interpreting Attention in Protein Language Models.\n\n[4] Bepler & Berger (2019). Learning protein sequence embeddings using information from structure.\n\n[5] Hopf et al. (2018). The EVcouplings Python framework for coevolutionary sequence analysis.\n\n[6] Lapedes et al. (1999). Correlated Mutations in Models of Protein Sequences: Phylogenetic and Structural Effects.\n\n[7] Thomas et al. (2008). Graphical Models of Residue Coupling in Protein Families. \n\n[8] Weigt et al. (2009). Identification of direct residue contacts in protein-protein interaction message passing.\n\n[9] Jones et al. (2012). PSICOV: precise structural contact prediction using sparse inverse covariance estimation on large multiple sequence alignments.\n\n[10] Alley et al. (2019). Unified rational protein engineering with sequence-based deep representational learning.\n\n[11] Zhang et al. (2019). DeepMSA: constructing deep multiple sequence alignment to improve contact prediction and fold-recognition for distant-homology proteins.\n\n[12] Skolnik et al. (1997). MONSSTER: a method for folding globular proteins with a small number of distance restraints.\n\n[13] Kim et al. (2014). One contact for every twelve residues allows robust and accurate topology-level protein structure modeling.\n\n[14] Dickson & Gloor (2012). Protein Sequence Alignment Analysis by Local Covariation: Coevolution Statistics Detect Benchmark Alignment Errors.\n\n[15] Malinverni & Barducci (2019). Coevolutionary Analysis of Protein Subfamilies by Sequence Reweighting.\n\n[16] Yang et al. (2020). Improved protein structure prediction using predicted interresidue orientations.\n\n[17] Vaswani et al. (2017). Attention is all you need.\n\n[18] Wang & Cho (2018). BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model.\n\n[19] Madani et al. (2020). ProGen: Language Modeling for Protein Generation.\n\n[20] Balakrishnan et al. (2011). Learning generative models for protein fold families.\n\n[21] Seemayer et al. (2014). CCMpred--fast and precise prediction of protein residue-residue contacts from correlated mutations.\n\n[22] Vorberg et al. (2018). Synthetic protein alignments by CCMgen quantify noise in residue-residue contact prediction.\n\n[23] Figliuzzi et al. (2018). How pairwise coevolutionary models capture the\ncollective residue variability in proteins.\n\n[24] Morcos et al. (2011). Direct-coupling analysis of residue coevolution captures native contacts across many protein families.\n\n[25] Ekeberg et al. (2013). Improved contact prediction in proteins: Using pseudolikelihoods to infer Potts models.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fylclEqgvgd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3581/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3581/Authors|ICLR.cc/2021/Conference/Paper3581/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836041, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment"}}}, {"id": "rFdUCNIlG_R", "original": null, "number": 3, "cdate": 1605149910231, "ddate": null, "tcdate": 1605149910231, "tmdate": 1605149910231, "tddate": null, "forum": "fylclEqgvgd", "replyto": "fylclEqgvgd", "invitation": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment", "content": {"title": "Response to All Reviewers", "comment": "We thank all reviewers for their thoughtful comments and suggestions. We are pleased to see that every reviewer considers this an interesting work.\n\nIn particular, we are glad that all reviewers appreciate that this work demonstrates transformers learn contacts in an unsupervised manner outperforming state-of-the-art unsupervised pipelines (R4: \u201csurprisingly data efficient and accurate\u201d, R2:  \u201ccontact predictions in these representations is learned in an unsupervised manner\u201d; R3: \u201cperform competitively for protein contact prediction [...] does not require sequence alignments\u201d; R1: \u201clearn information about protein contacts by using attention maps\u201d).\n\nThe primary concern highlighted by all reviewers appears to be the novelty of this work. The reviewers point out that significant prior work exists around contact prediction from protein language models, e.g. Bepler and Berger 2019, Rives et al. 2019, Rao et al. 2019, Rives et al. 2020. We agree with the reviewers that protein language modeling has been applied to contact prediction in the past; however prior work focuses on the **supervised contact prediction** problem.\n\nThe main novelty of this work is that it focuses on the **unsupervised contact prediction** problem showing state-of-the-art performance. Our work is the first to propose an interpretable unsupervised contact prediction method from protein language models. The use of attention maps in our method distinguishes it from the approaches used in prior work in the supervised setting. Our approach is also completely different to all evolutionary-coupling methods for unsupervised contact prediction, is competitive with pseudolikelihood maximization at all MSA depths, and especially improves on pseudolikelihood maximization for shallow MSAs (see Fig 3).\n\nUnsupervised contact prediction is well recognized as an important problem in its own right, evidenced by the breadth of prior work. Direct coupling analysis was initially described in Lapedes et al. 1999 and reintroduced by Thomas et al. 2008 and Weigt et al. 2009. Various methods have been developed to fit the underlying Markov Random Field, including inverse covariance (Morcos et al. 2011), sparse inverse covariance (Jones et al. 2012) and pseudolikelihood maximization (Balakrishnan et al. 2011, Seemayer et al. 2014, Ekeberg et al. 2013). Pseudolikelihood maximization is generally considered state-of-the-art for unsupervised contact prediction and is used as the baseline throughout.  In order to provide a more thorough comparison to prior methods, we will also add mean-field DCA and sparse inverse covariance as additional baselines.\n\nNo prior protein language modeling work directly considers the unsupervised contact prediction problem and benchmarks against the current state-of-the-art. Rives et al. 2019 fits linear projections and deep residual networks to the final hidden representation of the language model, demonstrating that information about contacts is encoded in the model and can be identified by supervision. Both Rao et al. 2019 and Rives et al. 2020 consider the supervised contact prediction application using deep residual networks and benchmark against supervised methods. Vig et al. 2020 Fig 4 shows that a particular head of the TAPE transformer correlates with contacts. They do not extract contact predictions using the attention maps, nor do they report contact precision based on attention maps. In Fig 16, they do report contact precisions, but these are fit from the hidden representations using supervision from many structures.\n\nThe reviewers have pointed out that a better discussion of prior work is needed. We acknowledge we have not properly positioned our work with respect to the literature on supervised contact prediction. We will add a thorough discussion incorporating the suggested references, discussing supervised and unsupervised approaches, and clearly delineating the unsupervised problem.\n\nWe believe this work is relevant to the ICLR community. Unsupervised learning is a core topic within the conference. The combination of unsupervised representation learning at scale with interpretability in a state-of-the-art method has broad interdisciplinary interest. It is of particular relevance to ICLR that representations learned from unlabeled sequence data map directly to underlying physical structures.\n\nWe propose the following plan to address the feedback from reviewers:\nNew related work section, with discussion of unsupervised and supervised contact prediction methods.\nAdditional unsupervised contact prediction baselines (mean field DCA, and sparse inverse covariance).\nResults on CASP13 to enable easier comparison with other methods.\nMisc additional experiments detailed inline in response to reviewer comments.\n\nWe appreciate the thoughtfulness of the reviewers and believe that these changes will significantly improve the paper. We welcome any additional comments or suggestions from the reviewers or broader community."}, "signatures": ["ICLR.cc/2021/Conference/Paper3581/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fylclEqgvgd", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3581/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3581/Authors|ICLR.cc/2021/Conference/Paper3581/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836041, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3581/-/Official_Comment"}}}, {"id": "34HFsILFxM", "original": null, "number": 4, "cdate": 1604283148514, "ddate": null, "tcdate": 1604283148514, "tmdate": 1605023974422, "tddate": null, "forum": "fylclEqgvgd", "replyto": "fylclEqgvgd", "invitation": "ICLR.cc/2021/Conference/Paper3581/-/Official_Review", "content": {"title": "Interesting idea, but background and comparisons are lacking", "review": "In this manuscript, the authors present a method for predicting residue-residue contacts within protein structures using the attention layers learned by transformer language models. Using the largest transformer language models trained to data, the authors show good performance for contact prediction. The paper is clearly written and easy to follow.\n\nThe general concept of fine tuning protein language models for contact prediction has circulated for some time which lessens the core contribution, but the authors approach is surprisingly data efficient and accurate. Overall this is an interesting work, though there is quite a bit of background on contact prediction missing. This paper is also very application specific and may not present new machine learning methods of general interest to the ICLR community. The existence of previous language model-based contact prediction methods reduces the novelty of this work, especially given that the model used here is from Rives et al. 2019, who already look at contact prediction. Furthermore, no comparisons with state-of-the-art evolutionary coupling-based or language model-based contact prediction methods are performed. With this in mind, the manuscript may be better suited to submission at a biology specific venue.\n\nAdditional specific comments follow below.\n\nMajor comments:\n1.\tMissing related work: there are a number of highly relevant prior works that are not mentioned/discussed. In particular, \u201cDeep generative models of genetic variation capture the effects of mutations\u201d \u2013 Riesselman et al. 2018 was, as far as I know, the first paper to show that deep generative models capture structure information (see Figure 6). Following that, \u201cLearning protein sequence embeddings using information from structure\u201d \u2013 Bepler & Berger 2019 was, to my knowledge, the first paper to propose deep language models (alignment free) for learning protein sequence representations and used those unsupervised representations for contact prediction. Furthermore, there has been extensive work in improving contact prediction using sequence + co-evolutionary features. See, for example, \u201cEnhancing Evolutionary Couplings with Deep Convolutional Neural Networks\u201d Liu et al. 2018 and \u201cAccurate De Novo Prediction of Protein Contact Map by Ultra-Deep Learning Model\u201d Wang et al. 2017. Other papers looking at protein structure prediction from sequence with deep learning, though they are less directly relevant, include \u201cEnd-to-End Differentiable Learning of Protein Structure\u201d AlQuraishi 2018 and \u201cLearning Protein Structure with a Differentiable Simulator\u201d Ingraham 2019.\n2.\tBefore this work, others have looked at fine tuning language models for contact prediction. How do those approaches compare with the approach presented here? Rives et al look at contact prediction in their manuscript describing the transformer model (which is the same model used here) on CASP 11-13 (see Table 5 in their manuscript). How does that approach compare with this one? Likewise for Bepler & Berger\n3.\tMany methods have surpassed GREMLIN for contact prediction using evolutionary couplings. How do those approaches compare with this one? It would be helpful to see how this approach compares with truly state-of-the-art contact prediction methods. Reporting results on the CASP data would help to make this comparison.\n\nMinor Comments:\n1.\tAlthough multiple sequence alignment methods have challenges especially as related to evolutionary coupling prediction, these methods have been heavily optimized for decades. The authors should provide citations for claimed failings such as \u201cfailure to find an optimal alignment\u201d and \u201csuboptimality of the substitution matrix and gap penalty.\u201d Certainly, these may be sources of error in alignments, but I am not aware of any studies of the frequency or impacts of these errors on evolutionary coupling analysis. If these studies exist, I encourage the authors to cite them. If they do not exist, I suggest the authors focus on well known sources of error here (namely, alignment depth) and provide references.\n2.\tThe authors use the language model without fine tuning, but the model could be fine tuned for each protein using its MSA. It\u2019s great that contacts can be predicted without fine tuning, but it would be interesting to investigate whether additional gains can be made.\n3.\tEight iterations of jackhmmer is a lot. In my personal experience, jackhmmer often diverges at 3+ iterations. By this I mean, the set of sequences and HMM learned by jackhmmer drift far away from the original sequence/family. Did the authors perform and quality checks of these alignments to ensure jackhmmer did not diverge?\n4.\tHow are sequence depths in Figure 3 calculated? Is this the raw number of sequences in each MSA or is it after applying some sort of neighborhood weighting to calculate an effective number of sequences? \n\nThings that would improve my rating:\n1.\tProvide a more comprehensive background review.\n2.\tCompare with state-of-the-art evolutionary coupling-based contact prediction methods.\n3.\tCompare with other language model-based contact prediction methods.\n4.\tWhat should interest the general machine learning community about this paper? What can we learn that might lead to better ML methods in the future? Convince me that this doesn\u2019t belong in a bioinformatics venue!\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3581/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3581/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Transformer protein language models are unsupervised structure learners", "authorids": ["~Roshan_Rao1", "~Joshua_Meier1", "~Tom_Sercu1", "~Sergey_Ovchinnikov1", "~Alexander_Rives1"], "authors": ["Roshan Rao", "Joshua Meier", "Tom Sercu", "Sergey Ovchinnikov", "Alexander Rives"], "keywords": ["proteins", "language modeling", "structure prediction", "unsupervised learning", "explainable"], "abstract": "Unsupervised contact prediction is central to uncovering physical, structural, and functional constraints for protein structure determination and design. For decades, the predominant approach has been to infer evolutionary constraints from a set of related sequences. In the past year, protein language models have emerged as a potential alternative, but performance has fallen short of state-of-the-art approaches in bioinformatics. In this paper we demonstrate that Transformer attention maps learn contacts from the unsupervised language modeling objective. We find the highest capacity models that have been trained to date already outperform a state-of-the-art unsupervised contact prediction pipeline, suggesting these pipelines can be replaced with a single forward pass of an end-to-end model.", "one-sentence_summary": "Transformer attention maps directly represent protein contacts with state-of-the-art unsupervised precision.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "rao|transformer_protein_language_models_are_unsupervised_structure_learners", "pdf": "/pdf/df3dd5269b8d22f21bb9984deb359af57f4dc0e6.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nrao2021transformer,\ntitle={Transformer protein language models are unsupervised structure learners},\nauthor={Roshan Rao and Joshua Meier and Tom Sercu and Sergey Ovchinnikov and Alexander Rives},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=fylclEqgvgd}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fylclEqgvgd", "replyto": "fylclEqgvgd", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3581/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073318, "tmdate": 1606915781870, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3581/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3581/-/Official_Review"}}}], "count": 17}