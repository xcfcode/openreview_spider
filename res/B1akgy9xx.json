{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396388951, "tcdate": 1486396388951, "number": 1, "id": "H1aJhfLOe", "invitation": "ICLR.cc/2017/conference/-/paper157/acceptance", "forum": "B1akgy9xx", "replyto": "B1akgy9xx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "No reviewer was willing to champion the paper and the authors did not adequately address reviewer comments in a revision. Recommend rejection."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Stochastic Neural Networks from Deterministic Ones", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN\n-> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.", "pdf": "/pdf/7e54b645b9aa1f5d582c664a72efdf8027099b94.pdf", "paperhash": "lee|making_stochastic_neural_networks_from_deterministic_ones", "keywords": ["Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["kaist.ac.kr", "kaist.edu"], "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "authorids": ["kiminlee@kaist.ac.kr", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu", "jinwoos@kaist.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396390989, "id": "ICLR.cc/2017/conference/-/paper157/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "B1akgy9xx", "replyto": "B1akgy9xx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396390989}}}, {"tddate": null, "tmdate": 1482768410557, "tcdate": 1482768241621, "number": 10, "id": "By5uya0Vx", "invitation": "ICLR.cc/2017/conference/-/paper157/public/comment", "forum": "B1akgy9xx", "replyto": "ByXsQisNe", "signatures": ["~Jinwoo_Shin1"], "readers": ["everyone"], "writers": ["~Jinwoo_Shin1"], "content": {"title": "All the comments for weakness can be well justified through the recent SFNN literature", "comment": "First, we very much appreciate the reviewer's comments on the strengths of our work. \n\nFor comments for weakness, we would like to refer some of recent works whose motivations are similar to ours, i.e., studying efficient training methods for SFNNs having binary random latent units.\n\n[a] \"Learning Stochastic Feedforward Neural Networks\" (Tang, Y. and Salakhutdinov, R.R., NIPS 2013)\n[b] \"Techniques for learning binary stochastic feedforward neural networks\" (Raiko, T., Berglund, M., Alain, G. and Dinh, L., ICLR 2015)\n[c] \"MuProp: Unbiased Backpropagation for Stochastic Neural Networks\" (Gu, S., Levine, S., Sutskever, I. and Mnih, A., ICLR 2016)\n\nThe above papers as well as all existing other SFNN works studying binary random latent units can be also commonly criticized by the reviewer's comments for weakness. However, we believe that they are still important contributions since the main focuses of our paper as well as [a], [b], [c] are neither \"scalability for large training set\" nor \"modeling uncertainty\". Instead, they are \"scalability for larger networks\" or \"modeling multi-modality\".\n\nIn what follows, we address the main concerns of the reviewer in more details, and will incorporate them in the final draft.\n\nQ1: no results are reported on real tasks with large training set. not clear exploration on the scalability of the learning methods when training data becomes larger\n\nA1: We expect that the training complexity grows linearly with respect to the size of training dataset. We did not explore too much about this issue since our main focus is handling large-scale networks efficiently, i.e.,  our work is the first training large-scale stochastic deep neural networks having binary latent units (e.g, WRN having 28 layers and 36 million parameters), while most prior related works including [a], [b], [c] have considered small models of at most 3 or 4 layers. Furthermore, the related works typically considered MNIST or TFD datasets, while we also used CIFAR-10, CIFAR-100 and SVHN. Namely, in terms of training datasets, our work is also much superior to them. Nevertheless, we can train SFNN using the ImageNet dataset following the reviewer's suggestion, and we will report its performance in the final draft.\n\nQ2: when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in \u201cPattern Recognition and Computer Vision\u201d, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as \u201cexplaining away\u201d. would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers\n\nA2: We agree that the uncertainty representation is a benefit of stochastic networks. However, for the purpose, it is more natural to consider SFNNs having continuous random latent units instead of discrete ones since the former is much easier to train using reparameterization tricks, e.g., also see https://arxiv.org/abs/1611.00712. Our main focus is training SFNNs having binary random latent units as like [a], [b], [c], which is for handling multi-modal tasks and regularizing better than DNNs. In addition, one can think SFNNs considered in our paper and variational autoencoders (VAEs) are similar, but the major difference is that VAEs typically consider continuous random latent units and can be trained using reparameterization tricks. We will incorporate the comments in the final draft.\n\nThanks again for the valuable comments, and in summary, we think the reviewer's concerns are well justified through the recent SFNN literature including [a], [b], [c].\n\nSincerely,\nJinwoo Shin\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Stochastic Neural Networks from Deterministic Ones", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN\n-> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.", "pdf": "/pdf/7e54b645b9aa1f5d582c664a72efdf8027099b94.pdf", "paperhash": "lee|making_stochastic_neural_networks_from_deterministic_ones", "keywords": ["Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["kaist.ac.kr", "kaist.edu"], "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "authorids": ["kiminlee@kaist.ac.kr", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu", "jinwoos@kaist.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287707700, "id": "ICLR.cc/2017/conference/-/paper157/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1akgy9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper157/reviewers", "ICLR.cc/2017/conference/paper157/areachairs"], "cdate": 1485287707700}}}, {"tddate": null, "tmdate": 1482564507220, "tcdate": 1482564507220, "number": 3, "id": "ByXsQisNe", "invitation": "ICLR.cc/2017/conference/-/paper157/official/review", "forum": "B1akgy9xx", "replyto": "B1akgy9xx", "signatures": ["ICLR.cc/2017/conference/paper157/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper157/AnonReviewer2"], "content": {"title": "The connection between different models is interesting, except for Bayesian net which is superficial and need to discuss more; MNIST results are interesting but more tasks need to be explored.", "rating": "5: Marginally below acceptance threshold", "review": "Strengths\n\n- interesting to explore the connection between ReLU DNN and simplified SFNN\n- small task (MNIST)  is used to demonstrate the usefulness of the proposed training methods experimentally\n- the proposed, multi-stage training methods are simple to implement (despite lacking theoretical rigor)\n\n\nWeaknesses\n\n-no results are reported on real tasks with large training set\n\n-not clear exploration on the scalability of the learning methods when training data becomes larger\n\n-when the hidden layers become stochastic, the model shares uncertainty representation with deep Bayes networks or deep generative models (Deep Discriminative and Generative Models for Pattern Recognition , book chapter in \u201cPattern Recognition and Computer Vision\u201d, November 2015, Download PDF). Such connections should be discussed, especially wrt the use of uncertainty representation to benefit pattern recognition (i.e. supervised learning via Bayes rule) and to benefit the use of domain knowledge such as \u201cexplaining away\u201d.\n\n-would like to see connections with variational autoencoder models and training, which is also stochastic with hidden layers\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Stochastic Neural Networks from Deterministic Ones", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN\n-> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.", "pdf": "/pdf/7e54b645b9aa1f5d582c664a72efdf8027099b94.pdf", "paperhash": "lee|making_stochastic_neural_networks_from_deterministic_ones", "keywords": ["Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["kaist.ac.kr", "kaist.edu"], "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "authorids": ["kiminlee@kaist.ac.kr", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu", "jinwoos@kaist.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482564507734, "id": "ICLR.cc/2017/conference/-/paper157/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper157/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper157/AnonReviewer1", "ICLR.cc/2017/conference/paper157/AnonReviewer3", "ICLR.cc/2017/conference/paper157/AnonReviewer2"], "reply": {"forum": "B1akgy9xx", "replyto": "B1akgy9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper157/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper157/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482564507734}}}, {"tddate": null, "tmdate": 1482131672282, "tcdate": 1482122929068, "number": 8, "id": "BkFhI1r4e", "invitation": "ICLR.cc/2017/conference/-/paper157/public/comment", "forum": "B1akgy9xx", "replyto": "Sk8GEQ1Ve", "signatures": ["~Kimin_Lee1"], "readers": ["everyone"], "writers": ["~Kimin_Lee1"], "content": {"title": "Our Response for AnonReviewer1", "comment": "First, we very much appreciate the reviewer's valuable comments, efforts and times on our paper. We believe that our work provides a new angle for training stochastic neural networks and has much more potential under various future directions.\n\nWe will incorporate all editorial suggestions in the final draft. In what follows, we address the main concerns of the reviewer. \n\nQ1: In Table 1, why does the 3 hidden layer SFNN initialized from a ReLU DNN have so much worse of a test NLL than the 2 hidden layer SFNN initialized from a ReLU DNN?\n\nA1: The performance of SFNN initialized from ReLU DNN strongly depends on the pre-trained model and the network structure. We expect that it is because of the unbounded property of ReLU. This issue is out of our scope, but we will try more experiments to provide more intuitive explanations for it in the final draft.\n\nQ2: When you transfer weights back from the simplified SFNN to the DNN* model, do you need to perform some sort of rescaling that undoes the operations in Equation (8) in the paper?\n\nA2: When we transfer weights back from the Simplified-SFNN to the DNN*, rescaling was not performed (we only used the operations in Equation (9) and recall that Equation (8) is for from DNN to Simplified-SFNN). Therefore, in this case, we can not guarantee a network knowledge transferring such as Theorem 1. Nevertheless, one can also investigate some ways of rescaling parameters for rigorous guarantees, but we do not try it since we empirically found that even such a simple transformation using Equation (9) reasonably works well. DNN* is worthwhile to try since the inference time of DNN* is much faster than those of stochastic models. For example, DropOut essentially trains a stochastic model, but it also uses a deterministic approximate inference procedure in practice.\n\nQ3: What does NCSFNN stand for in the supplementary material?\n\nA3: Sorry for the typo. NCSFNN in Lemma 4 should be replaced by Simplified-SFNN.\n\nQ4: The results CIFAR-10, CIFAR-100, and SVHN would be more convincing if the baselines used dropout and batch normalization.  \n\nA4: For NIN and WRN, we indeed train both the baselines and Simplified-SFNNs using dropout and batch normalization. Namely, our baselines are the strongest for CIFAR-10, CIFAR-100, and SVHN. We will emphasize it again in the draft. In addition, we note that dropout often does not work for large-scale CNNs. Since Simplified-SFNN consistently improves the performance of tested CNNs, we think it is still valuable even when its improved margin is relatively smaller in some cases compared to those of fully-connected DNNs on MNIST classification.\n\nThanks,\nKimin Lee, Jaehyung Kim, Song Chong, Jinwoo Shin"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Stochastic Neural Networks from Deterministic Ones", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN\n-> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.", "pdf": "/pdf/7e54b645b9aa1f5d582c664a72efdf8027099b94.pdf", "paperhash": "lee|making_stochastic_neural_networks_from_deterministic_ones", "keywords": ["Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["kaist.ac.kr", "kaist.edu"], "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "authorids": ["kiminlee@kaist.ac.kr", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu", "jinwoos@kaist.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287707700, "id": "ICLR.cc/2017/conference/-/paper157/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1akgy9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper157/reviewers", "ICLR.cc/2017/conference/paper157/areachairs"], "cdate": 1485287707700}}}, {"tddate": null, "tmdate": 1482123854680, "tcdate": 1482123854680, "number": 9, "id": "SyPIqyrVe", "invitation": "ICLR.cc/2017/conference/-/paper157/public/comment", "forum": "B1akgy9xx", "replyto": "S1h7tgZEx", "signatures": ["~Kimin_Lee1"], "readers": ["everyone"], "writers": ["~Kimin_Lee1"], "content": {"title": "Our Response for AnonReviewer3", "comment": "First, we very much appreciate the reviewer's comment on the novelty of our work. We believe that our work provides a new angle for training stochastic neural networks and has much more potential under various future directions.\n\nQ1: I think the connection between sigmoid DNN and Simplified SFNN is the same as mean-field approximation that has been known for decades. However, the connection between ReLU DNN and simplified SFNN is novel.\n\nA1: Right, the connection between sigmoid DNN and Simplified SFNN is essentially same as the mean-filed approximation. However, as the reviewer pointed out, the connection between ReLU DNN and simplified SFNN is new. We will add this comment in the final draft.\n\nQ2: My main concern is whether the proposed approach is useful when attacking real tasks with large training set. For tasks with small training set I can see that stochastic units would help generalize well.\n\nA2: Training Simplified-SFNN is typically 2-3 times slower than doing the same structure of DNN, but it is much faster (often more than 10 times) than doing the same structure of SFNN. Since the stochastic networks have several advantages beyond deterministic ones, we believe that such a trade-off is worthwhile depending on the target tasks. We emphasize again that our work is the first training large-scale stochastic deep neural networks (e.g, WRN having 28 layers and 36 million parameters), while most prior works on stochastic neural networks have considered small models of at most 3 or 4 layers.\n\nThanks,\nKimin Lee, Jaehyung Kim, Song Chong, Jinwoo Shin\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Stochastic Neural Networks from Deterministic Ones", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN\n-> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.", "pdf": "/pdf/7e54b645b9aa1f5d582c664a72efdf8027099b94.pdf", "paperhash": "lee|making_stochastic_neural_networks_from_deterministic_ones", "keywords": ["Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["kaist.ac.kr", "kaist.edu"], "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "authorids": ["kiminlee@kaist.ac.kr", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu", "jinwoos@kaist.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287707700, "id": "ICLR.cc/2017/conference/-/paper157/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1akgy9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper157/reviewers", "ICLR.cc/2017/conference/paper157/areachairs"], "cdate": 1485287707700}}}, {"tddate": null, "tmdate": 1481865507814, "tcdate": 1481865507814, "number": 2, "id": "S1h7tgZEx", "invitation": "ICLR.cc/2017/conference/-/paper157/official/review", "forum": "B1akgy9xx", "replyto": "B1akgy9xx", "signatures": ["ICLR.cc/2017/conference/paper157/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper157/AnonReviewer3"], "content": {"title": "interesting connection between DNN and simplified SFNN but its practical significance is unknown", "rating": "6: Marginally above acceptance threshold", "review": "This paper builds connections between DNN, simplified stochastic neural network (SFNN) and SFNN and proposes to use DNN as the initialization model for simplified SFNN. The authors evaluated their model on several small tasks with positive results.\n\nThe connection between different models is interesting. I think the connection between sigmoid DNN and Simplified SFNN is the same as mean-field approximation that has been known for decades. However, the connection between ReLU DNN and simplified SFNN is novel.\n\nMy main concern is whether the proposed approach is useful when attacking real tasks with large training set. For tasks with small training set I can see that stochastic units would help generalize well.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Stochastic Neural Networks from Deterministic Ones", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN\n-> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.", "pdf": "/pdf/7e54b645b9aa1f5d582c664a72efdf8027099b94.pdf", "paperhash": "lee|making_stochastic_neural_networks_from_deterministic_ones", "keywords": ["Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["kaist.ac.kr", "kaist.edu"], "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "authorids": ["kiminlee@kaist.ac.kr", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu", "jinwoos@kaist.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482564507734, "id": "ICLR.cc/2017/conference/-/paper157/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper157/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper157/AnonReviewer1", "ICLR.cc/2017/conference/paper157/AnonReviewer3", "ICLR.cc/2017/conference/paper157/AnonReviewer2"], "reply": {"forum": "B1akgy9xx", "replyto": "B1akgy9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper157/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper157/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482564507734}}}, {"tddate": null, "tmdate": 1481795113560, "tcdate": 1481792239069, "number": 7, "id": "rJvlj01Ve", "invitation": "ICLR.cc/2017/conference/-/paper157/public/comment", "forum": "B1akgy9xx", "replyto": "Sk8GEQ1Ve", "signatures": ["~Kimin_Lee1"], "readers": ["everyone"], "writers": ["~Kimin_Lee1"], "content": {"title": " Our baselines on CIFAR-10, CIFAR-100, and SVHN are the strongest", "comment": "For NIN and WRN, we indeed train both the baselines and Simplified-SFNNs using dropout and batch normalization. We will emphasize it again in the draft.\n\nFor other comments, we will upload our responses soon.\n\nThanks,\n\n-Kimin"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Stochastic Neural Networks from Deterministic Ones", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN\n-> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.", "pdf": "/pdf/7e54b645b9aa1f5d582c664a72efdf8027099b94.pdf", "paperhash": "lee|making_stochastic_neural_networks_from_deterministic_ones", "keywords": ["Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["kaist.ac.kr", "kaist.edu"], "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "authorids": ["kiminlee@kaist.ac.kr", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu", "jinwoos@kaist.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287707700, "id": "ICLR.cc/2017/conference/-/paper157/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1akgy9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper157/reviewers", "ICLR.cc/2017/conference/paper157/areachairs"], "cdate": 1485287707700}}}, {"tddate": null, "tmdate": 1481615578191, "tcdate": 1481610444988, "number": 6, "id": "r1rR4z67e", "invitation": "ICLR.cc/2017/conference/-/paper157/public/comment", "forum": "B1akgy9xx", "replyto": "SJG5N_nQe", "signatures": ["~Kimin_Lee1"], "readers": ["everyone"], "writers": ["~Kimin_Lee1"], "content": {"title": "The reasons why we replace only the first hidden layer by stochastic one for Table 1", "comment": "For all experiments reported in Table 1, the only first hidden layer of DNN is replaced by stochastic one. We will emphasize it again in the draft. One of the reasons why we replace only the first hidden layer by stochastic one is that  the approximation gap from (2) increases as the number of deterministic layers between a stochastic layer and output layer increases. Therefore, in order to investigate a loss from the approximation gap, the only first hidden layer is replaced by stochastic hidden one. Note that we considered two stochastic layers for WRN.\n\nThanks for clarification,\nKimin."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Stochastic Neural Networks from Deterministic Ones", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN\n-> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.", "pdf": "/pdf/7e54b645b9aa1f5d582c664a72efdf8027099b94.pdf", "paperhash": "lee|making_stochastic_neural_networks_from_deterministic_ones", "keywords": ["Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["kaist.ac.kr", "kaist.edu"], "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "authorids": ["kiminlee@kaist.ac.kr", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu", "jinwoos@kaist.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287707700, "id": "ICLR.cc/2017/conference/-/paper157/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1akgy9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper157/reviewers", "ICLR.cc/2017/conference/paper157/areachairs"], "cdate": 1485287707700}}}, {"tddate": null, "tmdate": 1481610467398, "tcdate": 1480476575160, "number": 3, "id": "SywswTofg", "invitation": "ICLR.cc/2017/conference/-/paper157/public/comment", "forum": "B1akgy9xx", "replyto": "BybnSXsGe", "signatures": ["~Kimin_Lee1"], "readers": ["everyone"], "writers": ["~Kimin_Lee1"], "content": {"title": "Network settings considred in our experiemnts are different for designing \"simpler\" Simplified-SFNN", "comment": "The stochastic network settings considered in our paper and Raiko et al., 2014 are a bit different. First, we considered both two and three hidden layers, while Riko et al., 2014 only did two hidden layers. More importantly, in our paper, the first hidden layer is the only stochastic one with 200 stochastic units. On the other hand, Riko et al., 2014 used a stochastic network where each hidden layer consists of 40 stochastic units and 160 deterministic ones. (The total number of stochastic units considered in our paper is larger.) Namely, they considered two hybrid hidden layers consisting of both deterministic and stochastic units. The main reason why we did not consider the same setting of Riko et al., 2014 is that if there exist consecutive stochastic layers, the definition of Simplified-SFNN becomes a bit more complex (although it is still doable in theory). Due to this, for simplicity, we did not consider consecutive stochastic units for all experiments in this paper (note that we considered two stochastic layers for WRN, but they are not consecutive).\n\nThanks for clarification,\nKimin."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Stochastic Neural Networks from Deterministic Ones", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN\n-> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.", "pdf": "/pdf/7e54b645b9aa1f5d582c664a72efdf8027099b94.pdf", "paperhash": "lee|making_stochastic_neural_networks_from_deterministic_ones", "keywords": ["Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["kaist.ac.kr", "kaist.edu"], "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "authorids": ["kiminlee@kaist.ac.kr", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu", "jinwoos@kaist.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287707700, "id": "ICLR.cc/2017/conference/-/paper157/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1akgy9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper157/reviewers", "ICLR.cc/2017/conference/paper157/areachairs"], "cdate": 1485287707700}}}, {"tddate": null, "tmdate": 1481610171802, "tcdate": 1481610171796, "number": 5, "id": "HyVpQGpmx", "invitation": "ICLR.cc/2017/conference/-/paper157/public/comment", "forum": "B1akgy9xx", "replyto": "H10T7vh7l", "signatures": ["~Kimin_Lee1"], "readers": ["everyone"], "writers": ["~Kimin_Lee1"], "content": {"title": "Citation Format", "comment": "We' ve uploaded the revised version with correct citations.\n\nThanks,\nKimin."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Stochastic Neural Networks from Deterministic Ones", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN\n-> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.", "pdf": "/pdf/7e54b645b9aa1f5d582c664a72efdf8027099b94.pdf", "paperhash": "lee|making_stochastic_neural_networks_from_deterministic_ones", "keywords": ["Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["kaist.ac.kr", "kaist.edu"], "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "authorids": ["kiminlee@kaist.ac.kr", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu", "jinwoos@kaist.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287707700, "id": "ICLR.cc/2017/conference/-/paper157/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1akgy9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper157/reviewers", "ICLR.cc/2017/conference/paper157/areachairs"], "cdate": 1485287707700}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481610071470, "tcdate": 1478254564972, "number": 157, "id": "B1akgy9xx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "B1akgy9xx", "signatures": ["~Kimin_Lee1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Making Stochastic Neural Networks from Deterministic Ones", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN\n-> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.", "pdf": "/pdf/7e54b645b9aa1f5d582c664a72efdf8027099b94.pdf", "paperhash": "lee|making_stochastic_neural_networks_from_deterministic_ones", "keywords": ["Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["kaist.ac.kr", "kaist.edu"], "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "authorids": ["kiminlee@kaist.ac.kr", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu", "jinwoos@kaist.ac.kr"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1480963127680, "tcdate": 1480963127674, "number": 4, "id": "ByeSEVmXg", "invitation": "ICLR.cc/2017/conference/-/paper157/public/comment", "forum": "B1akgy9xx", "replyto": "Bk6o_IyQe", "signatures": ["~Kimin_Lee1"], "readers": ["everyone"], "writers": ["~Kimin_Lee1"], "content": {"title": "SFNN is also Bayesian, but its goal is different", "comment": "The stochastic feedforward neural network (SFNN) considered in our paper is also a Bayesian network (although Simplified-SFNN is not). Our main motivation for SFNN is on supervised learning of a complex (e.g., multi-modal) mapping p(y|x), while that for generative deep Bayesian network such as the switching state model is usually on unsupervised (or semi-supervised) learning of a complex distribution p(x). (Here, x is the input data and y is its target.)\n\nThanks,\nKimin."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Stochastic Neural Networks from Deterministic Ones", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN\n-> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.", "pdf": "/pdf/7e54b645b9aa1f5d582c664a72efdf8027099b94.pdf", "paperhash": "lee|making_stochastic_neural_networks_from_deterministic_ones", "keywords": ["Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["kaist.ac.kr", "kaist.edu"], "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "authorids": ["kiminlee@kaist.ac.kr", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu", "jinwoos@kaist.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287707700, "id": "ICLR.cc/2017/conference/-/paper157/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1akgy9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper157/reviewers", "ICLR.cc/2017/conference/paper157/areachairs"], "cdate": 1485287707700}}}, {"tddate": null, "tmdate": 1480710309015, "tcdate": 1480710308863, "number": 2, "id": "Bk6o_IyQe", "invitation": "ICLR.cc/2017/conference/-/paper157/pre-review/question", "forum": "B1akgy9xx", "replyto": "B1akgy9xx", "signatures": ["ICLR.cc/2017/conference/paper157/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper157/AnonReviewer2"], "content": {"title": "Generative nature of Stochastic DNN vs. Generative Deep Bayesian Network", "question": "Can you compare the generative nature of Stochastic neural nets discussed in this paper with its counterpart in Generative Deep Bayesian Network(e.g., A multimodal variational approach to learning and inference in switching state space models, IEEE-ICASSP, 2004)? What are the commonality and differences between these two classes of deep generative models from both theoretical and practical standpoints? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Stochastic Neural Networks from Deterministic Ones", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN\n-> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.", "pdf": "/pdf/7e54b645b9aa1f5d582c664a72efdf8027099b94.pdf", "paperhash": "lee|making_stochastic_neural_networks_from_deterministic_ones", "keywords": ["Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["kaist.ac.kr", "kaist.edu"], "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "authorids": ["kiminlee@kaist.ac.kr", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu", "jinwoos@kaist.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959433818, "id": "ICLR.cc/2017/conference/-/paper157/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper157/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper157/AnonReviewer1", "ICLR.cc/2017/conference/paper157/AnonReviewer2"], "reply": {"forum": "B1akgy9xx", "replyto": "B1akgy9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper157/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper157/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959433818}}}, {"tddate": null, "tmdate": 1478668410202, "tcdate": 1478668410195, "number": 2, "id": "H1MteEebx", "invitation": "ICLR.cc/2017/conference/-/paper157/public/comment", "forum": "B1akgy9xx", "replyto": "ryVLpIClx", "signatures": ["~Kimin_Lee1"], "readers": ["everyone"], "writers": ["~Kimin_Lee1"], "content": {"title": "Paper format", "comment": "Dear PC,\n\nWe've uploaded the revised version with the correct margins. \n\nThanks,\nKimin"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Stochastic Neural Networks from Deterministic Ones", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN\n-> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.", "pdf": "/pdf/7e54b645b9aa1f5d582c664a72efdf8027099b94.pdf", "paperhash": "lee|making_stochastic_neural_networks_from_deterministic_ones", "keywords": ["Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["kaist.ac.kr", "kaist.edu"], "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "authorids": ["kiminlee@kaist.ac.kr", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu", "jinwoos@kaist.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287707700, "id": "ICLR.cc/2017/conference/-/paper157/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1akgy9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper157/reviewers", "ICLR.cc/2017/conference/paper157/areachairs"], "cdate": 1485287707700}}}, {"tddate": null, "tmdate": 1478554467495, "tcdate": 1478548812454, "number": 1, "id": "ryVLpIClx", "invitation": "ICLR.cc/2017/conference/-/paper157/public/comment", "forum": "B1akgy9xx", "replyto": "B1akgy9xx", "signatures": ["~Tara_N_Sainath1"], "readers": ["everyone"], "writers": ["~Tara_N_Sainath1"], "content": {"title": "ICLR Paper Format", "comment": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the margins to the correct spacing for your submission to be considered. Thank you!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Making Stochastic Neural Networks from Deterministic Ones", "abstract": "It has been believed that stochastic feedforward neural networks (SFNN) have several advantages beyond deterministic deep neural networks (DNN): they have more expressive power allowing multi-modal mappings and regularize better due to their stochastic nature. However, training SFNN is notoriously harder. In this paper, we aim at developing efficient training methods for large-scale SFNN, in particular using known architectures and pre-trained parameters of DNN. To this end, we propose a new intermediate stochastic model, called Simplified-SFNN, which can be built upon any baseline DNN and approximates certain SFNN by simplifying its upper latent units above stochastic ones. The main novelty of our approach is in establishing the connection between three models, i.e., DNN\n-> Simplified-SFNN -> SFNN, which naturally leads to an efficient training procedure of the stochastic models utilizing pre-trained parameters of DNN. Using several popular DNNs, we show how they can be effectively transferred to the corresponding stochastic models for both multi-modal and classification tasks on MNIST, TFD, CIFAR-10, CIFAR-100 and SVHN datasets. In particular, our stochastic model built from the wide residual network has 28 layers and 36 million parameters, where the former consistently outperforms the latter for the classification tasks on CIFAR-10 and CIFAR-100 due to its stochastic regularizing effect.", "pdf": "/pdf/7e54b645b9aa1f5d582c664a72efdf8027099b94.pdf", "paperhash": "lee|making_stochastic_neural_networks_from_deterministic_ones", "keywords": ["Deep learning", "Multi-modal learning", "Structured prediction"], "conflicts": ["kaist.ac.kr", "kaist.edu"], "authors": ["Kimin Lee", "Jaehyung Kim", "Song Chong", "Jinwoo Shin"], "authorids": ["kiminlee@kaist.ac.kr", "jaehyungkim@kaist.ac.kr", "songchong@kaist.edu", "jinwoos@kaist.ac.kr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287707700, "id": "ICLR.cc/2017/conference/-/paper157/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1akgy9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper157/reviewers", "ICLR.cc/2017/conference/paper157/areachairs"], "cdate": 1485287707700}}}], "count": 15}