{"notes": [{"id": "SkejkR4KDr", "original": "Syehq2G_vH", "number": 904, "cdate": 1569439202656, "ddate": null, "tcdate": 1569439202656, "tmdate": 1577168245200, "tddate": null, "forum": "SkejkR4KDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Layer Flexible Adaptive Computation Time for Recurrent Neural Networks", "authors": ["Lida Zhang", "Diego Klabjan"], "authorids": ["lidazhang2018@u.northwestern.edu", "d-klabjan@northwestern.edu"], "keywords": [], "abstract": "Deep recurrent neural networks perform well on sequence data and are the model of choice. However, it is a daunting task to decide the structure of the networks, i.e. the number of layers, especially considering different computational needs of a sequence. We propose a layer flexible recurrent neural network with adaptive computation time, and expand it to a sequence to sequence model. Different from the adaptive computation time model, our model has a dynamic number of transmission states which vary by step and sequence. We evaluate the model on a financial data set and Wikipedia language modeling. Experimental results show the performance improvement of 7% to 12% and indicate the model's ability to dynamically change the number of layers along with the computational steps.", "pdf": "/pdf/f7ab0ca74f6518eb34f587a8c7204e339107466e.pdf", "paperhash": "zhang|layer_flexible_adaptive_computation_time_for_recurrent_neural_networks", "original_pdf": "/attachment/f7ab0ca74f6518eb34f587a8c7204e339107466e.pdf", "_bibtex": "@misc{\nzhang2020layer,\ntitle={Layer Flexible Adaptive Computation Time for Recurrent Neural Networks},\nauthor={Lida Zhang and Diego Klabjan},\nyear={2020},\nurl={https://openreview.net/forum?id=SkejkR4KDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "iRWQxcYTQv", "original": null, "number": 1, "cdate": 1576798709258, "ddate": null, "tcdate": 1576798709258, "tmdate": 1576800927089, "tddate": null, "forum": "SkejkR4KDr", "replyto": "SkejkR4KDr", "invitation": "ICLR.cc/2020/Conference/Paper904/-/Decision", "content": {"decision": "Reject", "comment": "All reviewers assessed this paper as a weak reject.\nThe AC recommends rejection.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer Flexible Adaptive Computation Time for Recurrent Neural Networks", "authors": ["Lida Zhang", "Diego Klabjan"], "authorids": ["lidazhang2018@u.northwestern.edu", "d-klabjan@northwestern.edu"], "keywords": [], "abstract": "Deep recurrent neural networks perform well on sequence data and are the model of choice. However, it is a daunting task to decide the structure of the networks, i.e. the number of layers, especially considering different computational needs of a sequence. We propose a layer flexible recurrent neural network with adaptive computation time, and expand it to a sequence to sequence model. Different from the adaptive computation time model, our model has a dynamic number of transmission states which vary by step and sequence. We evaluate the model on a financial data set and Wikipedia language modeling. Experimental results show the performance improvement of 7% to 12% and indicate the model's ability to dynamically change the number of layers along with the computational steps.", "pdf": "/pdf/f7ab0ca74f6518eb34f587a8c7204e339107466e.pdf", "paperhash": "zhang|layer_flexible_adaptive_computation_time_for_recurrent_neural_networks", "original_pdf": "/attachment/f7ab0ca74f6518eb34f587a8c7204e339107466e.pdf", "_bibtex": "@misc{\nzhang2020layer,\ntitle={Layer Flexible Adaptive Computation Time for Recurrent Neural Networks},\nauthor={Lida Zhang and Diego Klabjan},\nyear={2020},\nurl={https://openreview.net/forum?id=SkejkR4KDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkejkR4KDr", "replyto": "SkejkR4KDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728189, "tmdate": 1576800280554, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper904/-/Decision"}}}, {"id": "S1li9bECtB", "original": null, "number": 1, "cdate": 1571860883073, "ddate": null, "tcdate": 1571860883073, "tmdate": 1572972537635, "tddate": null, "forum": "SkejkR4KDr", "replyto": "SkejkR4KDr", "invitation": "ICLR.cc/2020/Conference/Paper904/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThe paper proposes to use adaptive computation time (ACT) for the number of layers (instead of the number of timesteps in the original paper) for a RNN. The intuition is that some inputs are more complex and may need more processing compare to other inputs. \n\nComments on the paper,\n\n-  I am slightly confused over figure 2. For example, fig 2.c is the model performance over different steps. Does the number of steps here mean the number of time-steps per sequence? It seems that the performance of the proposed model and the RNN baseline matches at step 13. It seems to be natural to run RNNs over a sequence length of 13. it is not clear to me what is the advantage of using the proposed model in this case. \n\n- It is also unclear to me how many layers does the RNN baseline have and how the results changes wrt the number of the layers for the baselines, i would imagine that this would get better as the number of layers increases. Can the authors compare the proposed model to RNN baselines trained with different number of layers?\n\n\n- It is unclear to me what RNN-based models are, it seems that they are GRU models reading from section 4.1, but it seems to be implicit. Is there a reason to use GRUs compared to LSTMs? Do they achieve similar results?\n\n- I am not sure the proposed datasets have been picked. Is there a reason why each dataset is picked, what would the authors expect to see (hypothesis to test) from each dataset?\n\n- Some analytical experiments to help better understand the model would be nice. For example, some datasets would need more computational resources for some steps, but not others. Would the model be able to learn and pick that up? A really simple example if the copy task, would the model to learn to just have minimal layers for the 0's that does not contain information and would it use more layers for the steps that needs more processings (the non-zeros for example).\n\n\nMinor comments:\n\n1. some typos. P6, section 4.1, \"length\" was incorrectly spelled.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper904/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper904/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer Flexible Adaptive Computation Time for Recurrent Neural Networks", "authors": ["Lida Zhang", "Diego Klabjan"], "authorids": ["lidazhang2018@u.northwestern.edu", "d-klabjan@northwestern.edu"], "keywords": [], "abstract": "Deep recurrent neural networks perform well on sequence data and are the model of choice. However, it is a daunting task to decide the structure of the networks, i.e. the number of layers, especially considering different computational needs of a sequence. We propose a layer flexible recurrent neural network with adaptive computation time, and expand it to a sequence to sequence model. Different from the adaptive computation time model, our model has a dynamic number of transmission states which vary by step and sequence. We evaluate the model on a financial data set and Wikipedia language modeling. Experimental results show the performance improvement of 7% to 12% and indicate the model's ability to dynamically change the number of layers along with the computational steps.", "pdf": "/pdf/f7ab0ca74f6518eb34f587a8c7204e339107466e.pdf", "paperhash": "zhang|layer_flexible_adaptive_computation_time_for_recurrent_neural_networks", "original_pdf": "/attachment/f7ab0ca74f6518eb34f587a8c7204e339107466e.pdf", "_bibtex": "@misc{\nzhang2020layer,\ntitle={Layer Flexible Adaptive Computation Time for Recurrent Neural Networks},\nauthor={Lida Zhang and Diego Klabjan},\nyear={2020},\nurl={https://openreview.net/forum?id=SkejkR4KDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkejkR4KDr", "replyto": "SkejkR4KDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper904/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper904/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575215888476, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper904/Reviewers"], "noninvitees": [], "tcdate": 1570237745289, "tmdate": 1575215888491, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper904/-/Official_Review"}}}, {"id": "H1xYOcu0YS", "original": null, "number": 2, "cdate": 1571879536596, "ddate": null, "tcdate": 1571879536596, "tmdate": 1572972537591, "tddate": null, "forum": "SkejkR4KDr", "replyto": "SkejkR4KDr", "invitation": "ICLR.cc/2020/Conference/Paper904/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary:\n\nThe authors propose Layer Flexible Adaptive Computation Time, an RNN-esque sequence model with varying depth at each time step. The idea is that the model can adaptively choose how much computational effort to spend on each example. The authors evaluate the model empirically on a financial dataset and Wikipedia language modeling, and find that it outperforms a vanilla RNN and the original adaptive computation time (ACT) model.\n\nUnfortunately, the presentation of the idea is unclear, the idea itself is not very novel, and the experimental evaluation is lacking. These weaknesses lead me to vote for a weak reject.\n\nI address specific clarity points below.\n\nIn regards to the novelty claim, there have been several developments of depth-based (as opposed to time-based) adaptive computation time in the literature, for example:\n\n[1] McGill et al 2017 \"Deciding How to Decide: Dynamic Routing in Artificial Neural Networks\"\n[2] Bolukbasi et al 2017 \"Adaptive Neural Networks for Efficient Inference\"\n[3] Figurnov et al 2017 \"Spatially Adaptive Computation Time for Residual Networks\" (this paper is cited by the authors)\n\nThese papers do not present sequence models, but the ideas in them readily apply to sequence models. Thus, the main novelty in the authors' paper is handling the different number of hidden states at each timestep via their 'attention mechanism'. While this is definitely a contribution, the unclear presentation and lacking experimental evaluation combine to decrease the value of the paper.\n\nExperimentally, the authors evaluate on a financial time-series dataset and Wikipedia language modeling. They compare to Adaptive computation time and a standard RNN. While the experiments demonstrate a modest improvement over ACT and an RNN, they do not compare on larger, more standard datasets such as the WMT datasets, etc... Additionally, they do not compare with other, more powerful models. Both are required to thoroughly demonstrate their model's effectiveness.\n\nTo change my mind the authors would have to (in order of importance):\n1) Add experiments on WMT or other bigger datasets and compare with current SOTA models.\n2) Thoroughly edit their paper for clarity (specific points below).\n\nThe authors may want also add explorations of how much computation time can be saved using their model versus others, as this is very common in the ACT-esque literature.\n\nSpecific points:\n* While the original ACT paper does use 'mean-field' to denote the convex combination of states at a current timestep, that term has a specific technical meaning different from how it is used here. I would suggest using a different word.\n* The introduction is too long and repeats itself in several places.\n* There are repeated citations in the literature review.\n* In figure 1 sometimes nodes denote functions and sometimes nodes denote outputs. Sometimes the nodes are round and sometimes they are rectangular. Sometimes arrows denote inputs to a function, and sometimes they denote multiplication. These inconsistencies make the figure very hard to decipher.\n* The text description of your model is confusing. Specifically, distinguishing between the functions of u_t, \\hat{u}_t and \\overline{u}_t was difficult.\n* In your experiments section the plots are very difficult to interpret because they are phrase as 'improvement over x'. The standard presentation is a table of absolute results. Furthermore, bar charts can be misleading because the scale can make improvements seem bigger than they actually are. The figures should stand alone without having to read the text.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper904/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper904/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer Flexible Adaptive Computation Time for Recurrent Neural Networks", "authors": ["Lida Zhang", "Diego Klabjan"], "authorids": ["lidazhang2018@u.northwestern.edu", "d-klabjan@northwestern.edu"], "keywords": [], "abstract": "Deep recurrent neural networks perform well on sequence data and are the model of choice. However, it is a daunting task to decide the structure of the networks, i.e. the number of layers, especially considering different computational needs of a sequence. We propose a layer flexible recurrent neural network with adaptive computation time, and expand it to a sequence to sequence model. Different from the adaptive computation time model, our model has a dynamic number of transmission states which vary by step and sequence. We evaluate the model on a financial data set and Wikipedia language modeling. Experimental results show the performance improvement of 7% to 12% and indicate the model's ability to dynamically change the number of layers along with the computational steps.", "pdf": "/pdf/f7ab0ca74f6518eb34f587a8c7204e339107466e.pdf", "paperhash": "zhang|layer_flexible_adaptive_computation_time_for_recurrent_neural_networks", "original_pdf": "/attachment/f7ab0ca74f6518eb34f587a8c7204e339107466e.pdf", "_bibtex": "@misc{\nzhang2020layer,\ntitle={Layer Flexible Adaptive Computation Time for Recurrent Neural Networks},\nauthor={Lida Zhang and Diego Klabjan},\nyear={2020},\nurl={https://openreview.net/forum?id=SkejkR4KDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkejkR4KDr", "replyto": "SkejkR4KDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper904/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper904/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575215888476, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper904/Reviewers"], "noninvitees": [], "tcdate": 1570237745289, "tmdate": 1575215888491, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper904/-/Official_Review"}}}, {"id": "ryxhOoWN9S", "original": null, "number": 3, "cdate": 1572244339898, "ddate": null, "tcdate": 1572244339898, "tmdate": 1572972537547, "tddate": null, "forum": "SkejkR4KDr", "replyto": "SkejkR4KDr", "invitation": "ICLR.cc/2020/Conference/Paper904/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a layer-flexible adaptive computation time model which enables learning with a different number of layers at each time step.  It proposed a set of mechanisms to make the variable layer possible. It uses attention to re-arrange the hidden states in different layers into a different number of hidden states, thus allowing the hidden layers to be variable between different time steps. It also augmented the RNN to have a transmission state that transmits layer information to the next time step. \n\nIn the LFACT model equation after Eq.4 on Page 4, the lower layer hidden states are incorporated with the previous time step hidden states through the function g, before they were sent into the RNN cell f. This makes it different than a more straight forward setting in stacked RNN where the lower layer outputs are sent to the upper layer cell directly. I am wondering why this indirect way of stacking? will it work well if the layers are stacked as the normal stacked RNNs do (i.e., u_t^n takes the lower layer hidden state u_t^{n-1} as input)?\n\nIn the experiments, it seems that the N_t is very stable through time steps. Most of them are 1 or 2 layers. Although the LFACT outperforms the RNN and ACT significantly, that could be a factor of hyperparameter tuning or model structure advantage. Have the authors tried to fix N_t as a constant (let\u2019s say 2), and then perform the whole thing on the same setting again? I highly doubt that it will yield worse results. \n\nFor the N_t, is there any implication on what kinds of time steps should have more pondering steps and how does the model\u2019s choice match with the expected pondering time steps? Or  at least, if we are in an unsupervised setting so that we don\u2019t really have the \u201cexpected pondering time steps\u201d to check the quality of adaptive layers, we should run multiple replicas of the model with different parameter initializations and show that the distribution of N_t on each of the time steps are not uniform. If the \u201cpondering time\u201d (N_t\u2019s) are playing a role in processing the data, they should at least show some patterns that are related to the corresponding input, either decipherable or not. \n\nLearning through a different number of layers between different time steps is not a novel idea. For example, the TARDIS model (https://arxiv.org/abs/1701.08718) has set the number of hidden layers variable. \n\nIn general, I think the authors have provided an interesting idea and the experiments are well performed. I\u2019d expect the authors to isolate more factors from the model in order to show the effectiveness of the adaptive layer mechanism.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper904/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper904/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer Flexible Adaptive Computation Time for Recurrent Neural Networks", "authors": ["Lida Zhang", "Diego Klabjan"], "authorids": ["lidazhang2018@u.northwestern.edu", "d-klabjan@northwestern.edu"], "keywords": [], "abstract": "Deep recurrent neural networks perform well on sequence data and are the model of choice. However, it is a daunting task to decide the structure of the networks, i.e. the number of layers, especially considering different computational needs of a sequence. We propose a layer flexible recurrent neural network with adaptive computation time, and expand it to a sequence to sequence model. Different from the adaptive computation time model, our model has a dynamic number of transmission states which vary by step and sequence. We evaluate the model on a financial data set and Wikipedia language modeling. Experimental results show the performance improvement of 7% to 12% and indicate the model's ability to dynamically change the number of layers along with the computational steps.", "pdf": "/pdf/f7ab0ca74f6518eb34f587a8c7204e339107466e.pdf", "paperhash": "zhang|layer_flexible_adaptive_computation_time_for_recurrent_neural_networks", "original_pdf": "/attachment/f7ab0ca74f6518eb34f587a8c7204e339107466e.pdf", "_bibtex": "@misc{\nzhang2020layer,\ntitle={Layer Flexible Adaptive Computation Time for Recurrent Neural Networks},\nauthor={Lida Zhang and Diego Klabjan},\nyear={2020},\nurl={https://openreview.net/forum?id=SkejkR4KDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkejkR4KDr", "replyto": "SkejkR4KDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper904/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper904/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575215888476, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper904/Reviewers"], "noninvitees": [], "tcdate": 1570237745289, "tmdate": 1575215888491, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper904/-/Official_Review"}}}], "count": 5}