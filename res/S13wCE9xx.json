{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396445259, "tcdate": 1486396445259, "number": 1, "id": "rJBQnfLux", "invitation": "ICLR.cc/2017/conference/-/paper226/acceptance", "forum": "S13wCE9xx", "replyto": "S13wCE9xx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The paper is mostly clearly written. The observation made in the paper that word-embedding models based on optimizing skip-gram negative sampling objective function can be formulated as a low-rank matrix estimation problem, and solved using manifold optimization techniques, is sound. However, this observation by itself is not new and has come up in various other contexts such as matrix completion. As such the reviewers do not see sufficient novelty in the algorithmic aspects of the paper, and empirical evaluation on the specific problem of learning word embeddings does not show striking enough gains relative to standard SGD methods. The authors are encouraged to explore complimentary algorithmic angles and benefits that their approach provides for this specific class of applications."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Optimization for Skip-Gram Negative Sampling", "abstract": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \"word2vec\" software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.", "pdf": "/pdf/bfeab1f950d56cf2a80a7c038bb1d915a501be99.pdf", "TL;DR": "We train word embeddings optimizing Skip-Gram Negative Sampling objective (known by word2vec) via Riemannian low-rank optimization framework", "paperhash": "fonarev|riemannian_optimization_for_skipgram_negative_sampling", "keywords": ["Natural language processing", "Unsupervised Learning"], "conflicts": ["yandex-team.ru", "skoltech.ru", "skolkovotech.ru"], "authors": ["Alexander Fonarev", "Alexey Grinchuk", "Gleb Gusev", "Pavel Serdyukov", "Ivan Oseledets"], "authorids": ["newo@newo.su", "oleksii.hrinchuk@skolkovotech.ru", "gleb57@yandex-team.ru", "pavser@yandex-team.ru", "ioseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396445743, "id": "ICLR.cc/2017/conference/-/paper226/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S13wCE9xx", "replyto": "S13wCE9xx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396445743}}}, {"tddate": null, "tmdate": 1484949884335, "tcdate": 1484949884335, "number": 4, "id": "rJNYF-xvg", "invitation": "ICLR.cc/2017/conference/-/paper226/public/comment", "forum": "S13wCE9xx", "replyto": "H1dYu3xre", "signatures": ["~Alexander_Fonarev1"], "readers": ["everyone"], "writers": ["~Alexander_Fonarev1"], "content": {"title": "Asnwer", "comment": "Thank you for your reply!\n\nWe think that the contribution of our paper is not only in the new application of the existing Riemannian optimization technique but also in the clear reformulation of the word embedding learning problem (see previous comments for details).\n\nSpeaking of the computational complexity, you are right \u2014 the complexity of our algorithm is not perfect and the algorithm can not be applied to large-scale datasets with millions of unique tokens in the training corpus. We are going to improve the efficiency of the algorithm in our future work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Optimization for Skip-Gram Negative Sampling", "abstract": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \"word2vec\" software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.", "pdf": "/pdf/bfeab1f950d56cf2a80a7c038bb1d915a501be99.pdf", "TL;DR": "We train word embeddings optimizing Skip-Gram Negative Sampling objective (known by word2vec) via Riemannian low-rank optimization framework", "paperhash": "fonarev|riemannian_optimization_for_skipgram_negative_sampling", "keywords": ["Natural language processing", "Unsupervised Learning"], "conflicts": ["yandex-team.ru", "skoltech.ru", "skolkovotech.ru"], "authors": ["Alexander Fonarev", "Alexey Grinchuk", "Gleb Gusev", "Pavel Serdyukov", "Ivan Oseledets"], "authorids": ["newo@newo.su", "oleksii.hrinchuk@skolkovotech.ru", "gleb57@yandex-team.ru", "pavser@yandex-team.ru", "ioseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287676155, "id": "ICLR.cc/2017/conference/-/paper226/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S13wCE9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper226/reviewers", "ICLR.cc/2017/conference/paper226/areachairs"], "cdate": 1485287676155}}}, {"tddate": null, "tmdate": 1482897536144, "tcdate": 1482897536144, "number": 3, "id": "H1dYu3xre", "invitation": "ICLR.cc/2017/conference/-/paper226/official/review", "forum": "S13wCE9xx", "replyto": "S13wCE9xx", "signatures": ["ICLR.cc/2017/conference/paper226/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper226/AnonReviewer1"], "content": {"title": "Not convincing ", "rating": "4: Ok but not good enough - rejection", "review": "The paper considers Grassmannian SGD to optimize the skip gram negative sampling (SGNS) objective for learning better word embeddings. It is not clear why the proposed optimization approach has any advantage over the existing vanilla SGD-based approach - neither approach comes with theoretical guarantees - the empirical comparisons show marginal improvements. Furthermore, the key idea here - that of projector splitting algorithm - has been applied on numerous occasions to machine learning problems - see references by Vandereycken on matrix completion and by Sepulchre on matrix factorization. \n\nThe computational cost of the two approaches is not carefully discussed. For instance, how expensive is the SVD in (7)? One can always perform an efficient low-rank update to the SVD - therefore, a rank one update requires O(nd) operations. What is the computational cost of each iteration of the proposed approach? \n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Optimization for Skip-Gram Negative Sampling", "abstract": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \"word2vec\" software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.", "pdf": "/pdf/bfeab1f950d56cf2a80a7c038bb1d915a501be99.pdf", "TL;DR": "We train word embeddings optimizing Skip-Gram Negative Sampling objective (known by word2vec) via Riemannian low-rank optimization framework", "paperhash": "fonarev|riemannian_optimization_for_skipgram_negative_sampling", "keywords": ["Natural language processing", "Unsupervised Learning"], "conflicts": ["yandex-team.ru", "skoltech.ru", "skolkovotech.ru"], "authors": ["Alexander Fonarev", "Alexey Grinchuk", "Gleb Gusev", "Pavel Serdyukov", "Ivan Oseledets"], "authorids": ["newo@newo.su", "oleksii.hrinchuk@skolkovotech.ru", "gleb57@yandex-team.ru", "pavser@yandex-team.ru", "ioseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482897536794, "id": "ICLR.cc/2017/conference/-/paper226/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper226/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper226/AnonReviewer2", "ICLR.cc/2017/conference/paper226/AnonReviewer3", "ICLR.cc/2017/conference/paper226/AnonReviewer1"], "reply": {"forum": "S13wCE9xx", "replyto": "S13wCE9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper226/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper226/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482897536794}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481925192787, "tcdate": 1478278756228, "number": 226, "id": "S13wCE9xx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "S13wCE9xx", "signatures": ["~Alexander_Fonarev1"], "readers": ["everyone"], "content": {"title": "Riemannian Optimization for Skip-Gram Negative Sampling", "abstract": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \"word2vec\" software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.", "pdf": "/pdf/bfeab1f950d56cf2a80a7c038bb1d915a501be99.pdf", "TL;DR": "We train word embeddings optimizing Skip-Gram Negative Sampling objective (known by word2vec) via Riemannian low-rank optimization framework", "paperhash": "fonarev|riemannian_optimization_for_skipgram_negative_sampling", "keywords": ["Natural language processing", "Unsupervised Learning"], "conflicts": ["yandex-team.ru", "skoltech.ru", "skolkovotech.ru"], "authors": ["Alexander Fonarev", "Alexey Grinchuk", "Gleb Gusev", "Pavel Serdyukov", "Ivan Oseledets"], "authorids": ["newo@newo.su", "oleksii.hrinchuk@skolkovotech.ru", "gleb57@yandex-team.ru", "pavser@yandex-team.ru", "ioseledets@skoltech.ru"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481915975684, "tcdate": 1481915915372, "number": 2, "id": "SJmfRhWEg", "invitation": "ICLR.cc/2017/conference/-/paper226/official/review", "forum": "S13wCE9xx", "replyto": "S13wCE9xx", "signatures": ["ICLR.cc/2017/conference/paper226/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper226/AnonReviewer3"], "content": {"title": "still somewhat confused", "rating": "5: Marginally below acceptance threshold", "review": "Dear authors,\n\nThe authors' response clarified some of my confusion. But I still have the following question:\n\n-- The response said a first contribution is a different formulation: you divide the word embedding learning into two steps, step 1 looks for a low-rank X (by Riemannian optimization), step 2 factorizes X into two matrices (W, C). You are claiming that your model outperforms previous approaches that directly optimizes over (W, C). But since the end result (the factors) is the same, can the authors provide some intuition and justification why the proposed method works better?\n\nAs far as I can see, though parameterized differently, the first step of your method and previous methods (SGD) are both optimizing over low-rank matrices. Admittedly, Riemannian optimization avoids the rotational degree of freedom (the invertible matrix S you are mentioning in sec 2.3), but I am not 100% certain at this point this is the source of your gain; learning curves of objectives would help to see if Riemannian optimization is indeed more effective. \n\n-- Another detail I could not easily find is the following. You said a disadvantage of other approaches is that their factors W and C do not directly reflect similarity. Did you try to multiply the factors W and C from other optimizers and then factorize the product using the method in section 2.3, and use the new W for your downstream tasks? I am not sure if this would cause much difference in the performance.\n\nOverall, I think it is always interesting to apply advanced optimization techniques to machine learning problems. The current paper would be stronger from the machine learning perspective, if more thorough comparison and discussion (as mentioned above) are provided. On the other hand, my expertise is not in NLP and I leave it to other reviewers to decide the significance in experimental results.\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Optimization for Skip-Gram Negative Sampling", "abstract": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \"word2vec\" software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.", "pdf": "/pdf/bfeab1f950d56cf2a80a7c038bb1d915a501be99.pdf", "TL;DR": "We train word embeddings optimizing Skip-Gram Negative Sampling objective (known by word2vec) via Riemannian low-rank optimization framework", "paperhash": "fonarev|riemannian_optimization_for_skipgram_negative_sampling", "keywords": ["Natural language processing", "Unsupervised Learning"], "conflicts": ["yandex-team.ru", "skoltech.ru", "skolkovotech.ru"], "authors": ["Alexander Fonarev", "Alexey Grinchuk", "Gleb Gusev", "Pavel Serdyukov", "Ivan Oseledets"], "authorids": ["newo@newo.su", "oleksii.hrinchuk@skolkovotech.ru", "gleb57@yandex-team.ru", "pavser@yandex-team.ru", "ioseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482897536794, "id": "ICLR.cc/2017/conference/-/paper226/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper226/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper226/AnonReviewer2", "ICLR.cc/2017/conference/paper226/AnonReviewer3", "ICLR.cc/2017/conference/paper226/AnonReviewer1"], "reply": {"forum": "S13wCE9xx", "replyto": "S13wCE9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper226/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper226/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482897536794}}}, {"tddate": null, "tmdate": 1481840532634, "tcdate": 1481840532627, "number": 3, "id": "HkTcv5eNx", "invitation": "ICLR.cc/2017/conference/-/paper226/public/comment", "forum": "S13wCE9xx", "replyto": "SkD87Ly4x", "signatures": ["~Alexander_Fonarev1"], "readers": ["everyone"], "writers": ["~Alexander_Fonarev1"], "content": {"title": "Answer", "comment": "Thank you for your reply! I have commented it in the answer to your other message."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Optimization for Skip-Gram Negative Sampling", "abstract": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \"word2vec\" software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.", "pdf": "/pdf/bfeab1f950d56cf2a80a7c038bb1d915a501be99.pdf", "TL;DR": "We train word embeddings optimizing Skip-Gram Negative Sampling objective (known by word2vec) via Riemannian low-rank optimization framework", "paperhash": "fonarev|riemannian_optimization_for_skipgram_negative_sampling", "keywords": ["Natural language processing", "Unsupervised Learning"], "conflicts": ["yandex-team.ru", "skoltech.ru", "skolkovotech.ru"], "authors": ["Alexander Fonarev", "Alexey Grinchuk", "Gleb Gusev", "Pavel Serdyukov", "Ivan Oseledets"], "authorids": ["newo@newo.su", "oleksii.hrinchuk@skolkovotech.ru", "gleb57@yandex-team.ru", "pavser@yandex-team.ru", "ioseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287676155, "id": "ICLR.cc/2017/conference/-/paper226/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S13wCE9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper226/reviewers", "ICLR.cc/2017/conference/paper226/areachairs"], "cdate": 1485287676155}}}, {"tddate": null, "tmdate": 1481759956381, "tcdate": 1481759956373, "number": 2, "id": "BJn0nLkEl", "invitation": "ICLR.cc/2017/conference/-/paper226/public/comment", "forum": "S13wCE9xx", "replyto": "H1KdABJQe", "signatures": ["~Alexander_Fonarev1"], "readers": ["everyone"], "writers": ["~Alexander_Fonarev1"], "content": {"title": "Answer", "comment": "Thank you for your question!\n\nThe goal of any SGNS optimization method (Step 1 from the introduction section) is to find a good solution in terms of SNGS objective. From that point of view, our approach achieves the significant improvement (see Table 1). Moreover, the improvement on Step 1 entails the improvement on Step 2 (see Table 2). So, all experimental results together show that our approach makes sense, what, most importantly, opens the way to applying even more advanced approaches based on the proposed two-step decomposition of the problem (e.g., more advanced Riemannian optimization techniques for Step 1 or a more sophisticated treatment of Step 2)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Optimization for Skip-Gram Negative Sampling", "abstract": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \"word2vec\" software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.", "pdf": "/pdf/bfeab1f950d56cf2a80a7c038bb1d915a501be99.pdf", "TL;DR": "We train word embeddings optimizing Skip-Gram Negative Sampling objective (known by word2vec) via Riemannian low-rank optimization framework", "paperhash": "fonarev|riemannian_optimization_for_skipgram_negative_sampling", "keywords": ["Natural language processing", "Unsupervised Learning"], "conflicts": ["yandex-team.ru", "skoltech.ru", "skolkovotech.ru"], "authors": ["Alexander Fonarev", "Alexey Grinchuk", "Gleb Gusev", "Pavel Serdyukov", "Ivan Oseledets"], "authorids": ["newo@newo.su", "oleksii.hrinchuk@skolkovotech.ru", "gleb57@yandex-team.ru", "pavser@yandex-team.ru", "ioseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287676155, "id": "ICLR.cc/2017/conference/-/paper226/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S13wCE9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper226/reviewers", "ICLR.cc/2017/conference/paper226/areachairs"], "cdate": 1485287676155}}}, {"tddate": null, "tmdate": 1481759715008, "tcdate": 1481759714998, "number": 1, "id": "S1oyh8kNx", "invitation": "ICLR.cc/2017/conference/-/paper226/public/comment", "forum": "S13wCE9xx", "replyto": "Sk7OP_JQx", "signatures": ["~Alexander_Fonarev1"], "readers": ["everyone"], "writers": ["~Alexander_Fonarev1"], "content": {"title": "Answer", "comment": "Thank you for the questions!\n\n1. The paper's novelty is in the demonstration that the word embeddings learning consists of two separate steps (see Section 1). Step 1: searching for a matrix with a low-rank constraint that is good in terms of SGNS objective, and Step 2: searching for a good factorization of that matrix in terms of the target linguistic quality metric of word embeddings. Unfortunately, most previous approaches mixed these two steps into a single one, what entails a not completely correct formulation of the optimization problem (see Sections 1 and 2.2.2). In our paper, we demonstrated that the use of this two-step reformulation may increase the quality of embeddings. \n\nAs an example, we showed that the proper optimization over low-rank matrix X=WC instead of optimization over factors W and C independently increases the quality of Step 1 (see Table 1). Of course, Step 2 may be improved as well, but we regard this as a direction of future work. Hopefully, this two-step perspective will be helpful for future research on the topic of word embeddings learning. We will highlight the ideas described above in the next revision of the paper in order to make our contribution more clear.\n\n2. Yes, our Riemannian optimization based method outperforms the method based on SVD according to our experiments. That is what we expected to gain from the reformulation of the embeddings learning problem into two steps with clear objectives. \n\n3. We have also experimented with the blockwise alternating optimization over factors W and C. The alternating minimization led to almost the same results as SGD, and both of these techniques appeared to be worse than the Riemannian optimization based method. We decided not to include the alternating minimization results into the paper because the goal of the paper is to demonstrate the superiority of two-step problem reformulation, and not necessarily to compare all most advanced optimization methods in terms of SGNS minimization (nevertheless, more advanced Riemannian optimization techniques significantly outperform advanced component-wise methods in similar tasks, e.g. see [1]). That is why we compared the proposed method only to the most popular one based on SGD. Anyway, we implemented the alternating minimization and we will highlight that the alternating minimization approach is outperformed, as well as SGD and SVD, in the next revision of the paper.\n\n[1] Tan, Mingkui, et al. \"Riemannian Pursuit for Big Matrix Recovery.\" ICML 2014."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Optimization for Skip-Gram Negative Sampling", "abstract": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \"word2vec\" software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.", "pdf": "/pdf/bfeab1f950d56cf2a80a7c038bb1d915a501be99.pdf", "TL;DR": "We train word embeddings optimizing Skip-Gram Negative Sampling objective (known by word2vec) via Riemannian low-rank optimization framework", "paperhash": "fonarev|riemannian_optimization_for_skipgram_negative_sampling", "keywords": ["Natural language processing", "Unsupervised Learning"], "conflicts": ["yandex-team.ru", "skoltech.ru", "skolkovotech.ru"], "authors": ["Alexander Fonarev", "Alexey Grinchuk", "Gleb Gusev", "Pavel Serdyukov", "Ivan Oseledets"], "authorids": ["newo@newo.su", "oleksii.hrinchuk@skolkovotech.ru", "gleb57@yandex-team.ru", "pavser@yandex-team.ru", "ioseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287676155, "id": "ICLR.cc/2017/conference/-/paper226/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S13wCE9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper226/reviewers", "ICLR.cc/2017/conference/paper226/areachairs"], "cdate": 1485287676155}}}, {"tddate": null, "tmdate": 1481757519161, "tcdate": 1481757519155, "number": 1, "id": "SkD87Ly4x", "invitation": "ICLR.cc/2017/conference/-/paper226/official/review", "forum": "S13wCE9xx", "replyto": "S13wCE9xx", "signatures": ["ICLR.cc/2017/conference/paper226/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper226/AnonReviewer2"], "content": {"title": "Elegant method, not sure about the practical benefits", "rating": "6: Marginally above acceptance threshold", "review": "This paper presents a principled optimization method for SGNS (word2vec).\n\nWhile the proposed method is elegant from a theoretical perspective, I am not sure what the tangible benefits of this approach are. For example, does using Riemannian optimization allow the model to converge faster than the alternatives? The evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\", (Levy et al., 2015)). The theoretical connection to Riemannian optimization is nice though, and it might be useful for understanding related methods in the future.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Optimization for Skip-Gram Negative Sampling", "abstract": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \"word2vec\" software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.", "pdf": "/pdf/bfeab1f950d56cf2a80a7c038bb1d915a501be99.pdf", "TL;DR": "We train word embeddings optimizing Skip-Gram Negative Sampling objective (known by word2vec) via Riemannian low-rank optimization framework", "paperhash": "fonarev|riemannian_optimization_for_skipgram_negative_sampling", "keywords": ["Natural language processing", "Unsupervised Learning"], "conflicts": ["yandex-team.ru", "skoltech.ru", "skolkovotech.ru"], "authors": ["Alexander Fonarev", "Alexey Grinchuk", "Gleb Gusev", "Pavel Serdyukov", "Ivan Oseledets"], "authorids": ["newo@newo.su", "oleksii.hrinchuk@skolkovotech.ru", "gleb57@yandex-team.ru", "pavser@yandex-team.ru", "ioseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482897536794, "id": "ICLR.cc/2017/conference/-/paper226/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper226/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper226/AnonReviewer2", "ICLR.cc/2017/conference/paper226/AnonReviewer3", "ICLR.cc/2017/conference/paper226/AnonReviewer1"], "reply": {"forum": "S13wCE9xx", "replyto": "S13wCE9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper226/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper226/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482897536794}}}, {"tddate": null, "tmdate": 1480718187258, "tcdate": 1480718187249, "number": 2, "id": "Sk7OP_JQx", "invitation": "ICLR.cc/2017/conference/-/paper226/pre-review/question", "forum": "S13wCE9xx", "replyto": "S13wCE9xx", "signatures": ["ICLR.cc/2017/conference/paper226/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper226/AnonReviewer3"], "content": {"title": "main contribution?", "question": "I still need to read the paper more carefully. My main concern at this moment is what is the main contribution of this paper.\n\n1. Does the current paper have significant novelty in terms of optimization, or is it applying relatively mature techniques to the low-rank optimization problem of SGNS?\n\n2. Is the main contribution about properly optimization the SGNS objective, which was previously done only approximately by SVD? In that case, it is easy to believe that the paper can obtain improvement over SVD factorization. \n\n3. Have you compared with other popular optimization algorithms in machine learning for the SGNS objective, such as gradient descent/block coordinate descent or CG/L-BFGS? I am not asking for all comparison; and it appears the authors compared with SGD in the experiments. But if the contribution is mainly proper optimization, it makes sense to compare different algorithm in terms of efficiency (e.g., it would be good to show some learning curves). A comparison with gradient descent would be informative as we will be able to see the effect of Euclidean geometry vs Riemannian manifold geometry.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Optimization for Skip-Gram Negative Sampling", "abstract": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \"word2vec\" software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.", "pdf": "/pdf/bfeab1f950d56cf2a80a7c038bb1d915a501be99.pdf", "TL;DR": "We train word embeddings optimizing Skip-Gram Negative Sampling objective (known by word2vec) via Riemannian low-rank optimization framework", "paperhash": "fonarev|riemannian_optimization_for_skipgram_negative_sampling", "keywords": ["Natural language processing", "Unsupervised Learning"], "conflicts": ["yandex-team.ru", "skoltech.ru", "skolkovotech.ru"], "authors": ["Alexander Fonarev", "Alexey Grinchuk", "Gleb Gusev", "Pavel Serdyukov", "Ivan Oseledets"], "authorids": ["newo@newo.su", "oleksii.hrinchuk@skolkovotech.ru", "gleb57@yandex-team.ru", "pavser@yandex-team.ru", "ioseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959394968, "id": "ICLR.cc/2017/conference/-/paper226/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper226/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper226/AnonReviewer2", "ICLR.cc/2017/conference/paper226/AnonReviewer3"], "reply": {"forum": "S13wCE9xx", "replyto": "S13wCE9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper226/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper226/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959394968}}}, {"tddate": null, "tmdate": 1480707696817, "tcdate": 1480707696813, "number": 1, "id": "H1KdABJQe", "invitation": "ICLR.cc/2017/conference/-/paper226/pre-review/question", "forum": "S13wCE9xx", "replyto": "S13wCE9xx", "signatures": ["ICLR.cc/2017/conference/paper226/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper226/AnonReviewer2"], "content": {"title": "How fast is the new algorithm?", "question": "Does using Riemannian optimization allow the model to converge faster than the alternatives?\nI'm asking this because the evaluation doesn't show a dramatic advantage to RO-SGNS; the 1% difference on the word similarity benchmarks is within the range of hyperparameter effects (see \"Improving Distributional Similarity with Lessons Learned from Word Embeddings\", (Levy et al., 2015)). The proposed method is elegant from a theoretical perspective, but I was wondering if there are also some tangible benefits to this approach."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Riemannian Optimization for Skip-Gram Negative Sampling", "abstract": "Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in \"word2vec\" software, is usually optimized by stochastic gradient descent. It can be shown that optimizing for SGNS objective can be viewed as an optimization problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.", "pdf": "/pdf/bfeab1f950d56cf2a80a7c038bb1d915a501be99.pdf", "TL;DR": "We train word embeddings optimizing Skip-Gram Negative Sampling objective (known by word2vec) via Riemannian low-rank optimization framework", "paperhash": "fonarev|riemannian_optimization_for_skipgram_negative_sampling", "keywords": ["Natural language processing", "Unsupervised Learning"], "conflicts": ["yandex-team.ru", "skoltech.ru", "skolkovotech.ru"], "authors": ["Alexander Fonarev", "Alexey Grinchuk", "Gleb Gusev", "Pavel Serdyukov", "Ivan Oseledets"], "authorids": ["newo@newo.su", "oleksii.hrinchuk@skolkovotech.ru", "gleb57@yandex-team.ru", "pavser@yandex-team.ru", "ioseledets@skoltech.ru"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959394968, "id": "ICLR.cc/2017/conference/-/paper226/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper226/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper226/AnonReviewer2", "ICLR.cc/2017/conference/paper226/AnonReviewer3"], "reply": {"forum": "S13wCE9xx", "replyto": "S13wCE9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper226/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper226/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959394968}}}], "count": 11}