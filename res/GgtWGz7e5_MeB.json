{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1362021540000, "tcdate": 1362021540000, "number": 2, "id": "A6lxA54Jzv1yo", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "GgtWGz7e5_MeB", "replyto": "GgtWGz7e5_MeB", "signatures": ["anonymous reviewer a273"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Joint Space Neural Probabilistic Language Model for Statistical Machine\r\n      Translation", "review": "Author proposes 'n-gram HMM language model', which is inconsistent with the name of the paper. Also, the introduction is confusing and misleading.\r\n\r\nOverall the paper presents weak results. For example in Section 4 - the perplexity results are insignificantly better than from n-gram models, and most importantly are not reproducible: it is not even mentioned what is the amount of the training data, what is the order of n-gram models etc.\r\n\r\nAuthor uses irsltm, altough Srilm is cited too (giving it credit for n-gram language modeling, for some unknown reason); overall, many citations are unjustified, and unrelated to the paper itself (probably the only reason is to make everyone happy).\r\n\r\n0.2 bleu improvement is generally supposed to be insignificant.\r\n\r\nI don't see any useful information in the paper that can help others to improve their work (rather opposite). Unless author can obtain better results (which I honestly believe is not possible, with the explored approach), I don't see a reason why this work should be published."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Joint Space Neural Probabilistic Language Model for Statistical Machine\r\n      Translation", "abstract": "A neural probabilistic language model (NPLM) provides an idea to achieve the better perplexity than n-gram language model and their smoothed language models. This paper investigates application area in bilingual NLP, specifically Statistical Machine Translation (SMT). We focus on the perspectives that NPLM has potential to open the possibility to complement potentially `huge' monolingual resources into the `resource-constraint' bilingual resources. We introduce an ngram-HMM language model as NPLM using the non-parametric Bayesian construction. In order to facilitate the application to various tasks, we propose the joint space model of ngram-HMM language model. We show an experiment of system combination in the area of SMT. One discovery was that our treatment of noise improved the results 0.20 BLEU points if NPLM is trained in relatively small corpus, in our case 500,000 sentence pairs, which is often the case due to the long training time of NPLM.", "pdf": "https://arxiv.org/abs/1301.3614", "paperhash": "okita|joint_space_neural_probabilistic_language_model_for_statistical_machine_translation", "authors": ["Tsuyoshi Okita"], "authorids": ["tsuyoshi.okita2@mail.dcu.ie"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1361986980000, "tcdate": 1361986980000, "number": 3, "id": "Ezy1znNS-ZwLb", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "GgtWGz7e5_MeB", "replyto": "GgtWGz7e5_MeB", "signatures": ["anonymous reviewer 5a64"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Joint Space Neural Probabilistic Language Model for Statistical Machine\r\n      Translation", "review": "To quote the authors, this paper introduces a n-gram-HMM language\r\nmodel as neural probabilistic language model (NPLM) using the\r\nnon-parametric Bayesian construction. This article is really confused\r\nand describes a messy mix of different approaches. At the end, it is\r\nvery hard to understand what the author wanted to do and what he has\r\ndone. This paper can be improved in many ways before it could be\r\npublished: authors must clarify their motivations; the interaction\r\nbetween neural and HMM models could be describe more precisely;\r\n\r\n\r\nIn the reading order: \r\n\r\nIn the introduction, authors mistake the MERT process with the\r\ntranslation process. While MERT is used to tune the weights of a\r\nlog-linear combination of models in order to optimize BLEU for\r\ninstance, the NPLM are used as an additionnal model to re-rank nbest\r\nlists. Moreover, the correct citation for MERT is the ACL 2003 paper\r\nof F. Och.\r\n\r\nIn section 2, the authors introduce a HMM language model. A lot of\r\nquestions remain: What does the hidden states intend to capture ?\r\nWhat is the motivation ? What is the joint distribution associated to\r\nthe graphical model of figure 1 ? How the word n-gram distributions\r\nare related to the hidden states ?\r\n\r\nIn section 3, the authors enhance their HMM LM with an additional row\r\nof hidden states (joint space HMM). At this point the overall goal of\r\nthe paper is for me totally unclear. \r\n\r\nFor the following experimental sections, a lot of information on the\r\nset put is missing. Experiments cannot be reproduced given based on\r\nthe content of that paper. For example, the intrinsic evaluation\r\nintroduces ngram-HMM with one or two features. The very confused\r\nexplanation of these features is provided further in the\r\narticle. Authors do not describe the data-set (there are a lot of\r\neuroparl version), nor the order of the LMs under consideration.\r\n\r\nIn section 5, the following sentence puzzled me: 'Note that although\r\nthis experiment was done using the ngram-HMM language model, any NPLM\r\nmay be sufficient for this purpose. In this sense, we use the term\r\nNPLM instead of ngram-HMM language model.' Moreover, the first feature\r\nis derived from a NPLM, but how this NPLM is learnt, on which dataset,\r\nwhat are the parameters, the order of the model and how this feature\r\nis derived. I could not find the answers in this article. The rest of\r\nthe paper is more and more unclear. At the end, authors shows a BLEU\r\nimprovement of 0.2 on a system combination task. While I don't\r\nunderstand the models used, the gain is really small and I wonder if\r\nit is significant. For comparison's sake, MBR decoding usually provide\r\na BLEU improvement of at least 0.2."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Joint Space Neural Probabilistic Language Model for Statistical Machine\r\n      Translation", "abstract": "A neural probabilistic language model (NPLM) provides an idea to achieve the better perplexity than n-gram language model and their smoothed language models. This paper investigates application area in bilingual NLP, specifically Statistical Machine Translation (SMT). We focus on the perspectives that NPLM has potential to open the possibility to complement potentially `huge' monolingual resources into the `resource-constraint' bilingual resources. We introduce an ngram-HMM language model as NPLM using the non-parametric Bayesian construction. In order to facilitate the application to various tasks, we propose the joint space model of ngram-HMM language model. We show an experiment of system combination in the area of SMT. One discovery was that our treatment of noise improved the results 0.20 BLEU points if NPLM is trained in relatively small corpus, in our case 500,000 sentence pairs, which is often the case due to the long training time of NPLM.", "pdf": "https://arxiv.org/abs/1301.3614", "paperhash": "okita|joint_space_neural_probabilistic_language_model_for_statistical_machine_translation", "authors": ["Tsuyoshi Okita"], "authorids": ["tsuyoshi.okita2@mail.dcu.ie"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1360788360000, "tcdate": 1360788360000, "number": 1, "id": "MUE4IYdQ_XMbN", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "GgtWGz7e5_MeB", "replyto": "GgtWGz7e5_MeB", "signatures": ["anonymous reviewer 5328"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Joint Space Neural Probabilistic Language Model for Statistical Machine\r\n      Translation", "review": "The paper describes a Bayesian nonparametric HMM augmented with a hierarchical Pitman-Yor language model and slightly extends it by introducing conditioning on auxiliary inputs, possibly at each timestep. The observations are used for incorporating information from a separately trained model, such as LDA. In spite of the title and the abstract the paper has nothing to do with neural language models and very little with representation learning, as the author bizarrely uses the term NPLM to refer to the above n-gram HMM model. The model is evaluated as a part of a machine translation pipeline.\r\n\r\nThis is a very poorly written paper. The quality of writing makes it at times very difficult to understand what exactly has been done. The paper makes no significant  contributions from the machine learning standpoint, as what the author calls the 'n-gram HMM' is not novel, having been introduced by Blunsom & Cohn in 2011. The only material related to representation learning is not new either as it involves running LDA on documents. The rest of the paper is about tweaking a translation pipeline and is far too specialized for ICLR.\r\n\r\nReference:\r\nBlunsom, Phil, and Trevor Cohn. 'A hierarchical Pitman-Yor process HMM for unsupervised part of speech induction.' Proceedings of the 49th Annual Meeting of the ACL, 2011"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"decision": "reject", "title": "Joint Space Neural Probabilistic Language Model for Statistical Machine\r\n      Translation", "abstract": "A neural probabilistic language model (NPLM) provides an idea to achieve the better perplexity than n-gram language model and their smoothed language models. This paper investigates application area in bilingual NLP, specifically Statistical Machine Translation (SMT). We focus on the perspectives that NPLM has potential to open the possibility to complement potentially `huge' monolingual resources into the `resource-constraint' bilingual resources. We introduce an ngram-HMM language model as NPLM using the non-parametric Bayesian construction. In order to facilitate the application to various tasks, we propose the joint space model of ngram-HMM language model. We show an experiment of system combination in the area of SMT. One discovery was that our treatment of noise improved the results 0.20 BLEU points if NPLM is trained in relatively small corpus, in our case 500,000 sentence pairs, which is often the case due to the long training time of NPLM.", "pdf": "https://arxiv.org/abs/1301.3614", "paperhash": "okita|joint_space_neural_probabilistic_language_model_for_statistical_machine_translation", "authors": ["Tsuyoshi Okita"], "authorids": ["tsuyoshi.okita2@mail.dcu.ie"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358835300000, "tcdate": 1358835300000, "number": 44, "id": "GgtWGz7e5_MeB", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "GgtWGz7e5_MeB", "signatures": ["tsuyoshi.okita2@mail.dcu.ie"], "readers": ["everyone"], "content": {"decision": "reject", "title": "Joint Space Neural Probabilistic Language Model for Statistical Machine\r\n      Translation", "abstract": "A neural probabilistic language model (NPLM) provides an idea to achieve the better perplexity than n-gram language model and their smoothed language models. This paper investigates application area in bilingual NLP, specifically Statistical Machine Translation (SMT). We focus on the perspectives that NPLM has potential to open the possibility to complement potentially `huge' monolingual resources into the `resource-constraint' bilingual resources. We introduce an ngram-HMM language model as NPLM using the non-parametric Bayesian construction. In order to facilitate the application to various tasks, we propose the joint space model of ngram-HMM language model. We show an experiment of system combination in the area of SMT. One discovery was that our treatment of noise improved the results 0.20 BLEU points if NPLM is trained in relatively small corpus, in our case 500,000 sentence pairs, which is often the case due to the long training time of NPLM.", "pdf": "https://arxiv.org/abs/1301.3614", "paperhash": "okita|joint_space_neural_probabilistic_language_model_for_statistical_machine_translation", "authors": ["Tsuyoshi Okita"], "authorids": ["tsuyoshi.okita2@mail.dcu.ie"], "keywords": [], "conflicts": []}, "writers": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 4}