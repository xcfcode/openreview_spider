{"notes": [{"id": "BkeStsCcKQ", "original": "HJxL_7DKK7", "number": 443, "cdate": 1538087805181, "ddate": null, "tcdate": 1538087805181, "tmdate": 1550883654238, "tddate": null, "forum": "BkeStsCcKQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Critical Learning Periods in Deep Networks", "abstract": "Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training.  To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training.  Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of \"Information Plasticity\".  Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.", "keywords": ["Critical Period", "Deep Learning", "Information Theory", "Artificial Neuroscience", "Information Plasticity"], "authorids": ["achille@cs.ucla.edu", "matrovere@gmail.com", "soatto@cs.ucla.edu"], "authors": ["Alessandro Achille", "Matteo Rovere", "Stefano Soatto"], "TL;DR": "Sensory deficits in early training phases can lead to irreversible performance loss in both artificial and neuronal networks, suggesting information phenomena as the common cause, and point to the importance of the initial transient and forgetting.", "pdf": "/pdf/65e37345d89ab9fe6794070c13dbd5406bdd1c64.pdf", "paperhash": "achille|critical_learning_periods_in_deep_networks", "_bibtex": "@inproceedings{\nachille2018critical,\ntitle={Critical Learning Periods in Deep Networks},\nauthor={Alessandro Achille and Matteo Rovere and Stefano Soatto},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkeStsCcKQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJgKEmuGeV", "original": null, "number": 1, "cdate": 1544876849114, "ddate": null, "tcdate": 1544876849114, "tmdate": 1545354533645, "tddate": null, "forum": "BkeStsCcKQ", "replyto": "BkeStsCcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper443/Meta_Review", "content": {"metareview": "Irrespective of their taste for comparisons of neural networks to biological organisms, all reviewers agree that the empirical observations in this paper are quite interesting and well presented. While some reviewers note that the paper is not making theoretical contributions, the empirical results in themselves are intriguing enough to be of interest to ICLR audiences.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Interesting observations about critical learning periods in deep networks in a well-written paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper443/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper443/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Critical Learning Periods in Deep Networks", "abstract": "Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training.  To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training.  Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of \"Information Plasticity\".  Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.", "keywords": ["Critical Period", "Deep Learning", "Information Theory", "Artificial Neuroscience", "Information Plasticity"], "authorids": ["achille@cs.ucla.edu", "matrovere@gmail.com", "soatto@cs.ucla.edu"], "authors": ["Alessandro Achille", "Matteo Rovere", "Stefano Soatto"], "TL;DR": "Sensory deficits in early training phases can lead to irreversible performance loss in both artificial and neuronal networks, suggesting information phenomena as the common cause, and point to the importance of the initial transient and forgetting.", "pdf": "/pdf/65e37345d89ab9fe6794070c13dbd5406bdd1c64.pdf", "paperhash": "achille|critical_learning_periods_in_deep_networks", "_bibtex": "@inproceedings{\nachille2018critical,\ntitle={Critical Learning Periods in Deep Networks},\nauthor={Alessandro Achille and Matteo Rovere and Stefano Soatto},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkeStsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper443/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353214911, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkeStsCcKQ", "replyto": "BkeStsCcKQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper443/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper443/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper443/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353214911}}}, {"id": "rJxHyW8rRX", "original": null, "number": 5, "cdate": 1542967516747, "ddate": null, "tcdate": 1542967516747, "tmdate": 1542967516747, "tddate": null, "forum": "BkeStsCcKQ", "replyto": "ryguIWkf6Q", "invitation": "ICLR.cc/2019/Conference/-/Paper443/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We are thankful to the reviewer for their positive assessment of our paper. In fact, we share the same sentiment, as we articulate in the Conclusion, that one should resist the temptation to build too much on structural correspondences between such diverse systems. By showing these data we mostly wanted to emphasize how our reasoning is inspired by a reflection on the neurobiology of visual systems, and how such paradigms could be employed to better understand DNNs, since both systems share similar information processing goals."}, "signatures": ["ICLR.cc/2019/Conference/Paper443/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper443/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Critical Learning Periods in Deep Networks", "abstract": "Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training.  To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training.  Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of \"Information Plasticity\".  Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.", "keywords": ["Critical Period", "Deep Learning", "Information Theory", "Artificial Neuroscience", "Information Plasticity"], "authorids": ["achille@cs.ucla.edu", "matrovere@gmail.com", "soatto@cs.ucla.edu"], "authors": ["Alessandro Achille", "Matteo Rovere", "Stefano Soatto"], "TL;DR": "Sensory deficits in early training phases can lead to irreversible performance loss in both artificial and neuronal networks, suggesting information phenomena as the common cause, and point to the importance of the initial transient and forgetting.", "pdf": "/pdf/65e37345d89ab9fe6794070c13dbd5406bdd1c64.pdf", "paperhash": "achille|critical_learning_periods_in_deep_networks", "_bibtex": "@inproceedings{\nachille2018critical,\ntitle={Critical Learning Periods in Deep Networks},\nauthor={Alessandro Achille and Matteo Rovere and Stefano Soatto},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkeStsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper443/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616296, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkeStsCcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference/Paper443/Reviewers", "ICLR.cc/2019/Conference/Paper443/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper443/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper443/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper443/Authors|ICLR.cc/2019/Conference/Paper443/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper443/Reviewers", "ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference/Paper443/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616296}}}, {"id": "rJlxyjwX07", "original": null, "number": 4, "cdate": 1542843096309, "ddate": null, "tcdate": 1542843096309, "tmdate": 1542883210565, "tddate": null, "forum": "BkeStsCcKQ", "replyto": "Bkgba5DmCQ", "invitation": "ICLR.cc/2019/Conference/-/Paper443/Official_Comment", "content": {"title": "Response to Reviewer 2 (continued)", "comment": ">> In neuroscience the opening of the critical period window if thought to be mechanistically mediated by the maturation of inhibition. Is that view compatible with the results presented in this paper? This is sort of complementary to the FIM analysis, since is mostly about net average input to a neuron, i.e. about the information contained in the activations, rather than the weights.\n\nWe thank the reviewer for the insightful comment. Opening and closing of critical periods in neuronal networks have indeed been shown to be regulated by inhibitory (mostly GABAergic, but not exclusively) neuronal populations, which provide the critical balance between competing pathways (such as in ocular dominance) shaping the network in its mature form (Hensch, Curr. Top. Dev. Biol., 2005). While the CNN architectures we have tested in our study do not have direct inhibitory connections between elements of the CNN, we can speculate that \"diffuse\" inhibitory effects could emerge naturally during network optimization in order to make the inference more robust, leading to the effective \"pruning\" of certain connections, as mirrored by the FIM trace (Figure 4). It should also be noted here the connection existing between the decrease of the information in the weights (e.g., by \"pruning\") and the loss of information in the corresponding activations.\n\nIn addition we have the fact that only datasets which provide robust \"stimulation\" of the CNNs exhibit critical-period-like behavior, while being fed noise as the input is a deficit that the network promptly recovers from (Figure 2, left). An analogous phenomenon has been demonstrated in kittens, with dark rearing causing prolonged plasticity and delayed critical period inception (and closure), but, remarkably, this behavioral evidence has been tied to the decreased inhibitory GABAergic tone in the relevant circuits (e.g. Chen et al., Mol. Brain Res. 2001), which, by not providing the necessary competitive balance, lengthens the plastic window of visual development."}, "signatures": ["ICLR.cc/2019/Conference/Paper443/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper443/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Critical Learning Periods in Deep Networks", "abstract": "Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training.  To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training.  Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of \"Information Plasticity\".  Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.", "keywords": ["Critical Period", "Deep Learning", "Information Theory", "Artificial Neuroscience", "Information Plasticity"], "authorids": ["achille@cs.ucla.edu", "matrovere@gmail.com", "soatto@cs.ucla.edu"], "authors": ["Alessandro Achille", "Matteo Rovere", "Stefano Soatto"], "TL;DR": "Sensory deficits in early training phases can lead to irreversible performance loss in both artificial and neuronal networks, suggesting information phenomena as the common cause, and point to the importance of the initial transient and forgetting.", "pdf": "/pdf/65e37345d89ab9fe6794070c13dbd5406bdd1c64.pdf", "paperhash": "achille|critical_learning_periods_in_deep_networks", "_bibtex": "@inproceedings{\nachille2018critical,\ntitle={Critical Learning Periods in Deep Networks},\nauthor={Alessandro Achille and Matteo Rovere and Stefano Soatto},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkeStsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper443/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616296, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkeStsCcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference/Paper443/Reviewers", "ICLR.cc/2019/Conference/Paper443/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper443/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper443/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper443/Authors|ICLR.cc/2019/Conference/Paper443/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper443/Reviewers", "ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference/Paper443/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616296}}}, {"id": "Bkgba5DmCQ", "original": null, "number": 3, "cdate": 1542843064662, "ddate": null, "tcdate": 1542843064662, "tmdate": 1542882196956, "tddate": null, "forum": "BkeStsCcKQ", "replyto": "SJg8cOOka7", "invitation": "ICLR.cc/2019/Conference/-/Paper443/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We are thankful to the reviewer for the feedback and the many insightful suggestions. We have added the suggested experiments to the revised version of the paper (in particular Figure 3, Figure 8), and also discuss some of the points more in detail below.\n\n>> Presumably, early training on blurred images prevents the initial conv filters from learning to discriminate high-frequency components (first of all, is this true?).  The crucial phenomenon pointed out by the authors is that, even after removing the blur, the lower convolutions aren't able to recover and learn the high-frequency components.\n\nWe share the same intuition: We added to Figure 8 in the Appendix a visualization of the first-layer filters for networks with and without a deficit, and with the deficit removed after the end of the critical period. Figure 8 qualitatively shows that if high-resolution stimuli are not available before the critical period, the network does not manage to extract high-frequency features in the first layer. Unfortunately, the filters of the architecture we use are small (3x3), making the analysis more difficult: We are considering alternate experiments to test this hypothesis indirectly using responses to sinusoidal gratings to obtain clearer results.\n\n>> In fact, the high FIM trace in the latest layers could be due to the fact that they're trying to compensate for the lack of appropriate low-level feature extractors by composing low-frequency filters so as \"build\" high-frequency ones. If this makes sense, one would assume that freezing the last layers and only maintaining plasticity in the lower ones could be a way of \"reopening\" the critical period.  Is that indeed the case?\n\nThis is a very interesting hypothesis: We tried to test it as suggested, freezing layers 3-6 of AllCNN while leaving layers 0-3 and the final classifier free to change. We observe that upon deficit removal the network error increases (since the data distribution changes), only to revert to the performance with deficit soon after. This may be because freezing the upper layers makes new information extracted by the lower layers invisible to the classifier, and therefore does not promote learning of new features. Rather, lower layers may adapt to blur them as before, to fit the upper response expected by the upper layers. However, we agree that finding ways to reopen critical periods (either by augmenting the data with more \"stimulating\" experiences, as is sometimes done in neuroscience, see e.g. Knudsen, J. Cogn. Neurosci., 2004, or by changing the training procedure) is an intriguing question.\n\n>> The authors show that their main results are robust to changes in the learning rate annealing schedule. However, it is not clear how changing the optimizer might affect the presence of the critical period. What would happen for instance using Adam or another optimization procedure that relies on the normalization of the gradient?\n\nWe have conducted the experiment suggested and show in Figure 3 (Bottom Right) the result of training a ResNet with Adam, following the same experimental setup as Figure 1, and confirming that Adam also follows a similar trend.\n\n>> On a related note, the authors point out the importance of forgetting, in particular as the main mechanism behind the second learning phase. They also point out that the deficit in learning the task after sensory deprivation is accompanied by large FIM trace in the last layers. What would happen in the presence of a standard regularizer like weight decay?\n\nWe fully agree on the importance of weight decay for critical periods, and in the revised version of the paper we have added new experiments that corroborate it (Figure 3, bottom left). We observe that training in the same setup as Figure 1, but without weight decay, leads to a sharper and sensibly shorter critical period.  Gradually increasing the value of weight decay leads to more prolonged critical periods, up to the point where the network eventually stops training properly altogether."}, "signatures": ["ICLR.cc/2019/Conference/Paper443/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper443/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Critical Learning Periods in Deep Networks", "abstract": "Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training.  To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training.  Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of \"Information Plasticity\".  Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.", "keywords": ["Critical Period", "Deep Learning", "Information Theory", "Artificial Neuroscience", "Information Plasticity"], "authorids": ["achille@cs.ucla.edu", "matrovere@gmail.com", "soatto@cs.ucla.edu"], "authors": ["Alessandro Achille", "Matteo Rovere", "Stefano Soatto"], "TL;DR": "Sensory deficits in early training phases can lead to irreversible performance loss in both artificial and neuronal networks, suggesting information phenomena as the common cause, and point to the importance of the initial transient and forgetting.", "pdf": "/pdf/65e37345d89ab9fe6794070c13dbd5406bdd1c64.pdf", "paperhash": "achille|critical_learning_periods_in_deep_networks", "_bibtex": "@inproceedings{\nachille2018critical,\ntitle={Critical Learning Periods in Deep Networks},\nauthor={Alessandro Achille and Matteo Rovere and Stefano Soatto},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkeStsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper443/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616296, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkeStsCcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference/Paper443/Reviewers", "ICLR.cc/2019/Conference/Paper443/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper443/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper443/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper443/Authors|ICLR.cc/2019/Conference/Paper443/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper443/Reviewers", "ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference/Paper443/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616296}}}, {"id": "H1ebz8D7Cm", "original": null, "number": 2, "cdate": 1542841865094, "ddate": null, "tcdate": 1542841865094, "tmdate": 1542881185146, "tddate": null, "forum": "BkeStsCcKQ", "replyto": "HJgH4ifFi7", "invitation": "ICLR.cc/2019/Conference/-/Paper443/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for their feedback and suggestions. We have updated the paper accordingly, and address some of the points more in detail below:\n\n>> I was disappointed to see Tishby's result (2017) only remotely discussed, an earlier work than the one by Tishby is by Montavon et al 2011 in JMLR. Also in this work properties of successive compression and dimensionality reduction are discussed, perhaps the starting point of quantitative analysis of various DNNs.\n\nWe preferred not to elaborate at length on the connections with Schwartz and Tishby's results, since the relationship between the FIM of the weights (which we use in our paper) and the Shannon information of the activations, used by Tishby, is non-trivial and has already been discussed in more detail by other authors. However, given how important this aspect is and since it has also been discussed by the other reviewers, we have included in the revised version an extended discussion on it, which hopefully will also make the manuscript more self-contained.\n\nConcerning Montavon's paper, that is indeed our miss; we have added the paper to the revised discussion, and thank the reviewer for pointing it out.\n\n>> To this point the paper presents no theoretical contribution, rather empirical findings only, that may or may not be ubiquitous in DNN learning systems. The latter point may be worthwhile to discuss and analyse. \nOverall, the paper is interesting with its nice empirical studies but stays somewhat superficial. \n\nThe empirical findings are observed across the most commonly used architectures and optimization algorithms, as we also confirm with the new experiments in Fig. 3. But it is true that it will take much more experimentation to assess whether they are truly ubiquitous and how they may affect different kinds of data. On the theoretical side, the analysis of the transient, irreversible, properties of the learning process using the Fisher information in the weights is not only novel, but also different from other theoretical analyses, such as the study of flat minima, which focuses on the asymptotic behavior of the optimization (see the last paragraph of the Discussion). In particular, our analysis suggests that crossing bottlenecks in the loss landscape, as opposed to convergence to critical points, may play a fundamental role in characterizing the final behavior of the network. This aspect has, until now, been largely ignored and we are hopeful it may be fruitfully integrated in the current understanding of deep networks using, for example, tools from non-equilibrium dynamics, where such studies are common.\n\nAlthough we agree on the need for an analytical model, we tried to avoid the pitfalls of prematurely settling on a particular abstraction of the problem in order to paint a clearer picture, both through empirical experiments and by establishing connections with the most recent theories in deep learning, and yet providing a novel approach where the Fisher Information becomes one of the central quantities to consider.\n\n>> To learn more a simpler toy model may be worthwhile to study. \n\nWe fully agree. In this paper, we focused on testing our hypotheses on current state-of-the-art models and relatively complex datasets, in order to understand what are the key aspects that need to be captured by any simplified model. Now that this is established, and shown to be of practical relevance, given the widespread practice of fine-tuning, we can and will focus on simpler models that perhaps are also tractable analytically."}, "signatures": ["ICLR.cc/2019/Conference/Paper443/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper443/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Critical Learning Periods in Deep Networks", "abstract": "Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training.  To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training.  Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of \"Information Plasticity\".  Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.", "keywords": ["Critical Period", "Deep Learning", "Information Theory", "Artificial Neuroscience", "Information Plasticity"], "authorids": ["achille@cs.ucla.edu", "matrovere@gmail.com", "soatto@cs.ucla.edu"], "authors": ["Alessandro Achille", "Matteo Rovere", "Stefano Soatto"], "TL;DR": "Sensory deficits in early training phases can lead to irreversible performance loss in both artificial and neuronal networks, suggesting information phenomena as the common cause, and point to the importance of the initial transient and forgetting.", "pdf": "/pdf/65e37345d89ab9fe6794070c13dbd5406bdd1c64.pdf", "paperhash": "achille|critical_learning_periods_in_deep_networks", "_bibtex": "@inproceedings{\nachille2018critical,\ntitle={Critical Learning Periods in Deep Networks},\nauthor={Alessandro Achille and Matteo Rovere and Stefano Soatto},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkeStsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper443/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616296, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkeStsCcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference/Paper443/Reviewers", "ICLR.cc/2019/Conference/Paper443/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper443/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper443/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper443/Authors|ICLR.cc/2019/Conference/Paper443/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper443/Reviewers", "ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference/Paper443/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616296}}}, {"id": "ryguIWkf6Q", "original": null, "number": 3, "cdate": 1541693775560, "ddate": null, "tcdate": 1541693775560, "tmdate": 1541693775560, "tddate": null, "forum": "BkeStsCcKQ", "replyto": "BkeStsCcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper443/Official_Review", "content": {"title": "Interesting experiments casting some light on surprising properties of artificial neural networks", "review": "Let's be frank: I have never been a fan of comparing real brains with back-prop trained multilayer neural networks that have little to do with real neurons.  For instance, I am unmoved when Figure 1 compares multilayer network simulations with experimental data on actual kitten. More precisely, I see such comparisons as cheap shots.\n\nHowever, after forgetting about the kitten,  I can see lots of good things in this paper.  The artificial neural network experiments designed by the authors show interesting phenomena in a manner that is amenable to replication. The experiments about the varied effects of different kinds of deficits are particularly interesting and could inspire other researchers in creating mathematical models for these striking differences.  The authors also correlate these effects with the two phases they observe in the variations of the trace of the Fisher information matrix.  This is reminiscent of Tishby's bottleneck view on neural networks, but different in interesting ways. To start with, the trace of the Fisher information matrix is much easier to estimate than Tishby's mutual information between patterns, labels, and layer activation. It also might represent something of a different nature, in ways that I do not understand at this point.\n\nIn addition the paper is very well written, the comments are well though, and the experiments seem easy to replicate.\n\nGiven all these qualities, I'll gladly take the kitten as well..\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper443/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Critical Learning Periods in Deep Networks", "abstract": "Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training.  To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training.  Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of \"Information Plasticity\".  Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.", "keywords": ["Critical Period", "Deep Learning", "Information Theory", "Artificial Neuroscience", "Information Plasticity"], "authorids": ["achille@cs.ucla.edu", "matrovere@gmail.com", "soatto@cs.ucla.edu"], "authors": ["Alessandro Achille", "Matteo Rovere", "Stefano Soatto"], "TL;DR": "Sensory deficits in early training phases can lead to irreversible performance loss in both artificial and neuronal networks, suggesting information phenomena as the common cause, and point to the importance of the initial transient and forgetting.", "pdf": "/pdf/65e37345d89ab9fe6794070c13dbd5406bdd1c64.pdf", "paperhash": "achille|critical_learning_periods_in_deep_networks", "_bibtex": "@inproceedings{\nachille2018critical,\ntitle={Critical Learning Periods in Deep Networks},\nauthor={Alessandro Achille and Matteo Rovere and Stefano Soatto},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkeStsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper443/Official_Review", "cdate": 1542234460377, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkeStsCcKQ", "replyto": "BkeStsCcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper443/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335725319, "tmdate": 1552335725319, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper443/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJg8cOOka7", "original": null, "number": 2, "cdate": 1541535886147, "ddate": null, "tcdate": 1541535886147, "tmdate": 1541535886147, "tddate": null, "forum": "BkeStsCcKQ", "replyto": "BkeStsCcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper443/Official_Review", "content": {"title": "Very good paper identifying a novel phenomenology in training of deep neural networks: the presence of \"critical periods\" (reminiscent of the same phenomenon in many biological brain circuits) where perturbations in training can permanently affect the final performance of the model.", "review": "The authors analyze the learning dynamics in deep neural networks and identify an intriguing phenomenon that reflects what in biological learning is known as critical period: a relatively short time window early in post-natal development where organisms become particularly sensitive to particular changes in experience. The importance of critical periods in biology is due to the fact that specific types of perturbations to the input statistic can cause deficits in performance which can be permanent in the sense that later training cannot rescue them.\n\nThe authors did a great job illustrating the parallelism between critical periods in biological neural systems and the analogous phenomenon in artificial deep neural networks. Essentially, they showed that blurring the input samples of the cifar10 dataset during the initial phase of training had an effect that is very reminiscent of the result of sensory deprivation during the critical periods of visual learning in mammals, resulting in a long-term impairments in visual object recognition that persists even if blurring is removed later in training. The authors go as far as characterizing the effects of the length of the \"sensory deprivation\" window and its onset during training, and comparing the results to classic neuroscience monocular deprivation experiments in kittens, pointing out very striking phenomenological similarities.\n\nNext, the authors establish a connection between critical periods in deep neural networks and the amount of information that the weights of the trained model contain about the task by looking at the Fisher Information Matrix (FIM). With this method they obtain a host of interesting insights. One insight is that there are two phases in learning: an initial one where the trace of the FIM grows together with a rapid increase in classification accuracy, and a second one where accuracy keeps slightly increasing, but Fisher Information trace globally decreases. They then go into detail and look at how this quantity evolves within individual layers of the deep learning architecture, revealing that the deficit caused by the blurring perturbation during the early epochs training is accompanied by larger FIM trace in the last layers of the architecture at the expense of the intermediate layers.\nBesides the fact that deep neural network exhibit critical periods, another important result of this work is the demonstration that pretraining, if done inappropriately can actually be deleterious to the performance of the network.\n\nThis paper is insightful, and interesting. The conceptual and experimental part of the paper is very clearly presented, and the methodology is very appropriate to tease apart some of the mechanisms underlying the basic phenomenological observations. Here are some detailed questions meant to elucidate some points that are still unclear.\n\n- Presumably, early training on blurred images prevents the initial conv filters from learning to discriminate high-frequency components (first of all, is this true?). The crucial phenomenon pointed out by the authors is that, even after removing the blur, the lower convolutions aren't able to recover and learn the high-frequency components. In fact, the high FIM trace in the latest layers could be due to the fact that they're trying to compensate for the lack of appropriate low-level feature extractors by composing low-frequency filters so as \"build\" high-frequency ones. If this makes sense, one would assume that freezing the last layers and only maintaining plasticity in the lower ones could be a way of \"reopening\" the critical period. Is that indeed the case?\n- The authors show that their main results are robust to changes in the learning rate annealing schedule. However, it is not clear how changing the optimizer might affect the presence of the critical period. What would happen for instance using Adam or another optimization procedure that relies on the normalization of the gradient?\n- On a related note, the authors point out the importance of forgetting, in particular as the main mechanism behind the second learning phase. They also point out that the deficit in learning the task after sensory deprivation is accompanied by large FIM trace in the last layers. What would happen in the presence of a standard regularizer like weight decay? Assuming that large FIM trace in the last layers is correlated with large weighs, that might mitigate the negative effect of early sensory deprivation.\n- In neuroscience the opening of the critical period window if thought to be mechanistically mediated by the maturation of inhibition. Is that view compatible with the results presented in this paper? This is sort of complementary to the FIM analysis, since is mostly about net average input to a neuron, i.e. about the information contained in the activations, rather than the weights.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper443/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Critical Learning Periods in Deep Networks", "abstract": "Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training.  To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training.  Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of \"Information Plasticity\".  Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.", "keywords": ["Critical Period", "Deep Learning", "Information Theory", "Artificial Neuroscience", "Information Plasticity"], "authorids": ["achille@cs.ucla.edu", "matrovere@gmail.com", "soatto@cs.ucla.edu"], "authors": ["Alessandro Achille", "Matteo Rovere", "Stefano Soatto"], "TL;DR": "Sensory deficits in early training phases can lead to irreversible performance loss in both artificial and neuronal networks, suggesting information phenomena as the common cause, and point to the importance of the initial transient and forgetting.", "pdf": "/pdf/65e37345d89ab9fe6794070c13dbd5406bdd1c64.pdf", "paperhash": "achille|critical_learning_periods_in_deep_networks", "_bibtex": "@inproceedings{\nachille2018critical,\ntitle={Critical Learning Periods in Deep Networks},\nauthor={Alessandro Achille and Matteo Rovere and Stefano Soatto},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkeStsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper443/Official_Review", "cdate": 1542234460377, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkeStsCcKQ", "replyto": "BkeStsCcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper443/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335725319, "tmdate": 1552335725319, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper443/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJgH4ifFi7", "original": null, "number": 1, "cdate": 1540070188933, "ddate": null, "tcdate": 1540070188933, "tmdate": 1541533991295, "tddate": null, "forum": "BkeStsCcKQ", "replyto": "BkeStsCcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper443/Official_Review", "content": {"title": "Learning Phases in DNNs", "review": "The paper is interesting and I like it. I draws parallels from biological learning and the well known critical learning phases in biological systems to artificial neural network learning. \nA series of empirical simulation experiments that all aim to disturb the learning process of the DNN and to artificially create criticality are presented. They are providing food for thought, in order to introduce some quantitative results, the authors use well known Fisher Information to measure the changes. So far so good and interesting.\nI was disappointed to see Tishby's result (2017) only remotely discussed, an earlier work than the one by Tishby is by Montavon et al 2011 in JMLR. Also in this work properties of successive compression and dimensionality reduction are discussed, perhaps the starting point of quantitative analysis of various DNNs. \n\nTo this point the paper presents no theoretical contribution, rather empirical findings only, that may or may not be ubiquitous in DNN learning systems. The latter point may be worthwhile to discuss and analyse. \nOverall, the paper is interesting with its nice empirical studies but stays somewhat superficial. To learn more a simpler toy model may be worthwhile to study. \n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper443/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Critical Learning Periods in Deep Networks", "abstract": "Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training.  To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training.  Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of \"Information Plasticity\".  Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.", "keywords": ["Critical Period", "Deep Learning", "Information Theory", "Artificial Neuroscience", "Information Plasticity"], "authorids": ["achille@cs.ucla.edu", "matrovere@gmail.com", "soatto@cs.ucla.edu"], "authors": ["Alessandro Achille", "Matteo Rovere", "Stefano Soatto"], "TL;DR": "Sensory deficits in early training phases can lead to irreversible performance loss in both artificial and neuronal networks, suggesting information phenomena as the common cause, and point to the importance of the initial transient and forgetting.", "pdf": "/pdf/65e37345d89ab9fe6794070c13dbd5406bdd1c64.pdf", "paperhash": "achille|critical_learning_periods_in_deep_networks", "_bibtex": "@inproceedings{\nachille2018critical,\ntitle={Critical Learning Periods in Deep Networks},\nauthor={Alessandro Achille and Matteo Rovere and Stefano Soatto},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkeStsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper443/Official_Review", "cdate": 1542234460377, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkeStsCcKQ", "replyto": "BkeStsCcKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper443/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335725319, "tmdate": 1552335725319, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper443/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJlyF5rTnQ", "original": null, "number": 1, "cdate": 1541393014828, "ddate": null, "tcdate": 1541393014828, "tmdate": 1541393014828, "tddate": null, "forum": "BkeStsCcKQ", "replyto": "BJeHWCM6hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper443/Official_Comment", "content": {"title": "The observed phenomena and their connection to irreversible phases of learning have not been studied and cannot be derived from the referenced paper", "comment": "The phenomena observed in our study of the Fisher Information of the weights, and especially their connections with irreversible changes in the connectivity of Deep Networks during the early phases of training, cannot be derived from the results of Shwartz-Ziv and Tishby concerning the Shannon mutual information of the activations. In fact, to the best of our knowledge, we are the first to show the relationship between changes of Fisher Information and irreversible effects of optimizing a Deep Network. It is however true that some of the results by Shwartz-Ziv and Tishby are related, in fact implied, by our observations, which therefore provide further and independent corroborations of their claims. \n\nIn particular, Shwartz-Ziv & Tishby report changes in the Shannon mutual information of the activations (not of the weights) during training. This are however not observed to be associated with any kind of irreversible changes. In fact, the existence of critical (irreversible) phases of learning have not been observed, let alone studied, by https://arxiv.org/abs/1703.00810 nor by anyone else to our knowledge. Furthermore, note that changes in mutual information of the activations are always observed during training, however critical learning periods are present only for some specific types of deficits. Therefore, Shwartz-Ziv & Tishby's framework and results on the different phases of information in the activations during training cannot explain the existence and the observed phenomenology of critical periods.\n\nOn the other hand, thanks to the introducing of the Fisher Information of the weights, and by exploiting its relationship to the network connectivity, we can empirically characterize critical-period-inducing deficits as precisely those those that severely alter the connectivity of the network, and suggest a theoretical explanation for these phenomena (see end of Section 3). To the best of our knowledge, we are the first to compute and track these changes of the Fisher information of the weights during training of a state of the art, modern deep network, and in particular, nobody has shown plots like those in Figure 1, Figure 2, Figure 3, Figure 5.\n\nHowever, even if discussing different quantities, Shwartz-Ziv and Tishby\u2019s results are indeed related to the plots we show in Figure 4, as we also discuss in Section 4 (pag. 7, second paragraph). The non-trivial connection between the two can be derived from the bound on information introduced by Achille and Soatto (https://arxiv.org/abs/1706.01350, JMLR 2018): As we describe on Page 7, they show that reduction of the information in the weights implies information reduction in the activation (but not vice-versa). In this sense, our results are can also serve to corroborate and expand, using an independent framework, the experimental evidence on the existence of multiple phases of learning shown by Shwartz-Ziv and Tishby."}, "signatures": ["ICLR.cc/2019/Conference/Paper443/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper443/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Critical Learning Periods in Deep Networks", "abstract": "Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training.  To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training.  Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of \"Information Plasticity\".  Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.", "keywords": ["Critical Period", "Deep Learning", "Information Theory", "Artificial Neuroscience", "Information Plasticity"], "authorids": ["achille@cs.ucla.edu", "matrovere@gmail.com", "soatto@cs.ucla.edu"], "authors": ["Alessandro Achille", "Matteo Rovere", "Stefano Soatto"], "TL;DR": "Sensory deficits in early training phases can lead to irreversible performance loss in both artificial and neuronal networks, suggesting information phenomena as the common cause, and point to the importance of the initial transient and forgetting.", "pdf": "/pdf/65e37345d89ab9fe6794070c13dbd5406bdd1c64.pdf", "paperhash": "achille|critical_learning_periods_in_deep_networks", "_bibtex": "@inproceedings{\nachille2018critical,\ntitle={Critical Learning Periods in Deep Networks},\nauthor={Alessandro Achille and Matteo Rovere and Stefano Soatto},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkeStsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper443/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621616296, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkeStsCcKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference/Paper443/Reviewers", "ICLR.cc/2019/Conference/Paper443/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper443/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper443/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper443/Authors|ICLR.cc/2019/Conference/Paper443/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper443/Reviewers", "ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference/Paper443/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621616296}}}, {"id": "BJeHWCM6hQ", "original": null, "number": 1, "cdate": 1541381629296, "ddate": null, "tcdate": 1541381629296, "tmdate": 1541381674466, "tddate": null, "forum": "BkeStsCcKQ", "replyto": "BkeStsCcKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper443/Public_Comment", "content": {"comment": "I wonder if there is any novelty in your experiment about Fisher Information. Many of the phenomenon in your experiments have been studied in https://arxiv.org/abs/1703.00810 ", "title": "Questions About Novelty"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper443/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Critical Learning Periods in Deep Networks", "abstract": "Similar to humans and animals, deep artificial neural networks exhibit critical periods during which a temporary stimulus deficit can impair the development of a skill. The extent of the impairment depends on the onset and length of the deficit window, as in animal models, and on the size of the neural network. Deficits that do not affect low-level statistics, such as vertical flipping of the images, have no lasting effect on performance and can be overcome with further training.  To better understand this phenomenon, we use the Fisher Information of the weights to measure the effective connectivity between layers of a network during training.  Counterintuitively, information rises rapidly in the early phases of training, and then decreases, preventing redistribution of information resources in a phenomenon we refer to as a loss of \"Information Plasticity\".  Our analysis suggests that the first few epochs are critical for the creation of strong connections that are optimal relative to the input data distribution. Once such strong connections are created, they do not appear to change during additional training. These findings suggest that the initial learning transient, under-scrutinized compared to asymptotic behavior, plays a key role in determining the outcome of the training process. Our findings, combined with recent theoretical results in the literature, also suggest that forgetting (decrease of information in the weights) is critical to achieving invariance and disentanglement in representation learning. Finally, critical periods are not restricted to biological systems, but can emerge naturally in learning systems, whether biological or artificial, due to fundamental constrains arising from learning dynamics and information processing.", "keywords": ["Critical Period", "Deep Learning", "Information Theory", "Artificial Neuroscience", "Information Plasticity"], "authorids": ["achille@cs.ucla.edu", "matrovere@gmail.com", "soatto@cs.ucla.edu"], "authors": ["Alessandro Achille", "Matteo Rovere", "Stefano Soatto"], "TL;DR": "Sensory deficits in early training phases can lead to irreversible performance loss in both artificial and neuronal networks, suggesting information phenomena as the common cause, and point to the importance of the initial transient and forgetting.", "pdf": "/pdf/65e37345d89ab9fe6794070c13dbd5406bdd1c64.pdf", "paperhash": "achille|critical_learning_periods_in_deep_networks", "_bibtex": "@inproceedings{\nachille2018critical,\ntitle={Critical Learning Periods in Deep Networks},\nauthor={Alessandro Achille and Matteo Rovere and Stefano Soatto},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=BkeStsCcKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper443/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311839160, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BkeStsCcKQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference/Paper443/Reviewers", "ICLR.cc/2019/Conference/Paper443/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper443/Authors", "ICLR.cc/2019/Conference/Paper443/Reviewers", "ICLR.cc/2019/Conference/Paper443/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311839160}}}], "count": 11}