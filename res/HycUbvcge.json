{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396539637, "tcdate": 1486396539637, "number": 1, "id": "rkVKhzIdl", "invitation": "ICLR.cc/2017/conference/-/paper361/acceptance", "forum": "HycUbvcge", "replyto": "HycUbvcge", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This is largely a clearly written paper that proposes a nonlinear generalization of a generalized CCA approach for multi-view learning. In terms of technical novelty, the generalization follows rather straightforwardly. Reviewers have expressed the need to clarify relationship and provide comparisons to existing proposals for combining deep learning with CCA. As such the paper has been evaluated to be borderline. The proposed method appears to yield significant gains on a speech dataset, though comparisons on other datasets appear to be less conclusive. Some basic baselines as missing, e.g., concatenating views and running a deep model, or using much older nonlinear extensions of CCA such as kernel CCA (e.g. accelerated via random features, and combined with deep representations underneath)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generalized Canonical Correlation Analysis", "abstract": "We present Deep Generalized Canonical Correlation Analysis (DGCCA) \u2013 a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.", "pdf": "/pdf/6a69ee127e5fb0baa8d1630b128a5f8820e30cbf.pdf", "TL;DR": "A multiview representation learning technique that can learn nonlinear mappings from arbitrarily many views to a shared semantic space -- Deep Generalized Canonical Correlation Analysis.", "paperhash": "benton|deep_generalized_canonical_correlation_analysis", "keywords": ["Unsupervised Learning", "Deep learning", "Multi-modal learning"], "conflicts": ["cs.jhu.edu", "berkeley.edu"], "authors": ["Adrian Benton", "Huda Khayrallah", "Biman Gujral", "Drew Reisinger", "Sheng Zhang", "Raman Arora"], "authorids": ["adrian@cs.jhu.edu", "huda@jhu.edu", "bgujral1@jhu.edu", "reisinger@cogsci.jhu.edu", "zsheng2@jhu.edu", "arora@cs.jhu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396540170, "id": "ICLR.cc/2017/conference/-/paper361/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HycUbvcge", "replyto": "HycUbvcge", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396540170}}}, {"tddate": null, "tmdate": 1484271502543, "tcdate": 1484271502543, "number": 5, "id": "HkLqk2SUg", "invitation": "ICLR.cc/2017/conference/-/paper361/public/comment", "forum": "HycUbvcge", "replyto": "HycUbvcge", "signatures": ["~Adrian_Benton1"], "readers": ["everyone"], "writers": ["~Adrian_Benton1"], "content": {"title": "Draft revised, added prior work section", "comment": "Thank you for your helpful comments.  We just uploaded a revised draft, incorporating the reviewers' suggestions, and hopefully addressing many of their concerns.  Below are the things to note:\n\n- The linear GCCA solution for G and U is included in Appendix A, along with a full gradient derivation: \"... the rows of G are the top r (orthonormal) eigenvectors of M, and $U_j = C_{jj}^{\u22121} Y_j G^T$\" (reviewer 2)\n\n- In the last paragraph of the Optimization subsection (page 4), we include big-Oh notation for the gradient update time complexity.  We leverage the GCCA solution presented in [R1] to scale DGCCA to large datasets. (reviewer 2)\n\n- We qualify the pronouncement of being \"the first nonlinear multiview learning technique\" with the adjective \"CCA-style\".  Although our work focuses on extending CCA-based multiview methods, we recognize that others have attempted to learn embeddings by merging information from multiple views. (reviewer 5)\n\n- In Section 5, \"Other Multiview Work\", we include a discussion of a non-CCA-based techniques for nonlinear representation learning from multiple views, and how they differ from DGCCA.  As reviewers 2 and 5 mention, the multiview learning literature is vast, and we are not able to address all models proposed that make learned representations from more than one view.  However, we do hope that this section will clarify how our proposed model differs from other representation learning techniques exploiting multiple views. (reviewers 2 and 5)\n\n- Appendix C includes a short discussion of how the DGCCA objective reconstruction error relates to downstream task performance for Twitter hashtag recommendation.  In short, we found that high reconstruction error is a strong signal for poor downstream performance, but there is significant variation in downstream performance between embeddings learned by models with low reconstruction error. (reviewer 4)\n\nWe also trained Bridge Correlational Neural Network embeddings for Twitter hashtag and friend recommendation in a series of preliminary experiments.  We swept over hidden layer width in the same range as the DGCCA experiments, $\\lambda \\in \\{0.0, 0.1, 1.0, 10.0\\}$ (the strength of the correlation term in the DGCCA objective), and used either the Twitter user ego text view or their friend network view as the pivot view, since these were the solely most effective views for hashtag and friend recommendtion.  Other learning parameters were left at the defaults and networks were trained for 50 epochs.  However, the performance of these embeddings was much worse than the CCA-style models we compare to (R@1000=0.06 for hashtag recommendation.)  We grant that downstream performance would be improved by tuning learning parameters, but these preliminary experiments underscore the fact that this class of models is not a panacea, and may not be appropriate for these recommendation tasks.  We explicitly note this in the text, since these models assume that all views should be correlated with a pivot view representation.  Entraining all embeddings to a single view is, thus, probably not as effective at hashtag recommendation as learning a CCA-style joint representation for all views. (reviewer 5)\n\nPlease let us know if any of these revisions are confusing, or if you have any additional suggestions.\n\n[R1] Pushpendre Rastogi, Benamin Van Durme, and Raman Arora. Multiview LSA: Representation Learning via Generalized CCA. Proceedings of NAACL. 2015."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generalized Canonical Correlation Analysis", "abstract": "We present Deep Generalized Canonical Correlation Analysis (DGCCA) \u2013 a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.", "pdf": "/pdf/6a69ee127e5fb0baa8d1630b128a5f8820e30cbf.pdf", "TL;DR": "A multiview representation learning technique that can learn nonlinear mappings from arbitrarily many views to a shared semantic space -- Deep Generalized Canonical Correlation Analysis.", "paperhash": "benton|deep_generalized_canonical_correlation_analysis", "keywords": ["Unsupervised Learning", "Deep learning", "Multi-modal learning"], "conflicts": ["cs.jhu.edu", "berkeley.edu"], "authors": ["Adrian Benton", "Huda Khayrallah", "Biman Gujral", "Drew Reisinger", "Sheng Zhang", "Raman Arora"], "authorids": ["adrian@cs.jhu.edu", "huda@jhu.edu", "bgujral1@jhu.edu", "reisinger@cogsci.jhu.edu", "zsheng2@jhu.edu", "arora@cs.jhu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608344, "id": "ICLR.cc/2017/conference/-/paper361/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HycUbvcge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper361/reviewers", "ICLR.cc/2017/conference/paper361/areachairs"], "cdate": 1485287608344}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484271327471, "tcdate": 1478287698219, "number": 361, "id": "HycUbvcge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HycUbvcge", "signatures": ["~Adrian_Benton1"], "readers": ["everyone"], "content": {"title": "Deep Generalized Canonical Correlation Analysis", "abstract": "We present Deep Generalized Canonical Correlation Analysis (DGCCA) \u2013 a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.", "pdf": "/pdf/6a69ee127e5fb0baa8d1630b128a5f8820e30cbf.pdf", "TL;DR": "A multiview representation learning technique that can learn nonlinear mappings from arbitrarily many views to a shared semantic space -- Deep Generalized Canonical Correlation Analysis.", "paperhash": "benton|deep_generalized_canonical_correlation_analysis", "keywords": ["Unsupervised Learning", "Deep learning", "Multi-modal learning"], "conflicts": ["cs.jhu.edu", "berkeley.edu"], "authors": ["Adrian Benton", "Huda Khayrallah", "Biman Gujral", "Drew Reisinger", "Sheng Zhang", "Raman Arora"], "authorids": ["adrian@cs.jhu.edu", "huda@jhu.edu", "bgujral1@jhu.edu", "reisinger@cogsci.jhu.edu", "zsheng2@jhu.edu", "arora@cs.jhu.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1482591376678, "tcdate": 1482591251475, "number": 4, "id": "rysznW2Eg", "invitation": "ICLR.cc/2017/conference/-/paper361/public/comment", "forum": "HycUbvcge", "replyto": "H1GCKjB4l", "signatures": ["~Adrian_Benton1"], "readers": ["everyone"], "writers": ["~Adrian_Benton1"], "content": {"title": "AnonReviewer4 Review Response", "comment": "\n- The detailed algorithm and updates for U, G are given in Algorithm 1 in Appendix B on page 13. We will highlight the key updates in the main text as well. \n\n- Figures 3(b) and Figure 4(a) show different things. Figure 3(b) shows the shared representation G whereas Figure 4(a) shows the projection of View 1 (in Figure 2(a)) projected onto its corresponding subspace, in other words it shows U\u2019X_1. They do look similar in that the two components are well separated. \n\n- We can easily add view-specific reconstruction errors and rainbow plots. Note that from Figure 4 it appears that in terms of reconstruction error, View 3 is worse than View 2 which is worse than View 1. \n\n- We found that reconstruction error was a reasonable proxy in choosing regularization parameters, but not the best model for a downstream task.  For example, if the L2 regularization penalty was too low, the reconstruction error would be orders of magnitude larger than a correctly regularized model, and downstream task performance would be very poor.  However, we did not purely rely on reconstruction error to select models, since networks with narrower output layers will necessarily have lower reconstruction error.  Ultimately we selected models that perform best on the downstream task.  We can include a figure containing reconstruction error vs. downstream task performance to illustrate this."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generalized Canonical Correlation Analysis", "abstract": "We present Deep Generalized Canonical Correlation Analysis (DGCCA) \u2013 a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.", "pdf": "/pdf/6a69ee127e5fb0baa8d1630b128a5f8820e30cbf.pdf", "TL;DR": "A multiview representation learning technique that can learn nonlinear mappings from arbitrarily many views to a shared semantic space -- Deep Generalized Canonical Correlation Analysis.", "paperhash": "benton|deep_generalized_canonical_correlation_analysis", "keywords": ["Unsupervised Learning", "Deep learning", "Multi-modal learning"], "conflicts": ["cs.jhu.edu", "berkeley.edu"], "authors": ["Adrian Benton", "Huda Khayrallah", "Biman Gujral", "Drew Reisinger", "Sheng Zhang", "Raman Arora"], "authorids": ["adrian@cs.jhu.edu", "huda@jhu.edu", "bgujral1@jhu.edu", "reisinger@cogsci.jhu.edu", "zsheng2@jhu.edu", "arora@cs.jhu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608344, "id": "ICLR.cc/2017/conference/-/paper361/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HycUbvcge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper361/reviewers", "ICLR.cc/2017/conference/paper361/areachairs"], "cdate": 1485287608344}}}, {"tddate": null, "tmdate": 1482591365322, "tcdate": 1482591156431, "number": 3, "id": "rJ23ob3Nl", "invitation": "ICLR.cc/2017/conference/-/paper361/public/comment", "forum": "HycUbvcge", "replyto": "B1atHaUEg", "signatures": ["~Adrian_Benton1"], "readers": ["everyone"], "writers": ["~Adrian_Benton1"], "content": {"title": "AnonReviewer5 Review Response", "comment": "\n- Thanks for the additional reference, we will discuss it in the revision as it is related work. However, the \u201cBridge Correlational Neural Networks\u201d paper considers a setting where instead of several parallel views, there is a bridge view that provides a parallel view between other views.  Therefore the problem reduces to two-view CCA and their nonlinear/deep extensions. This is clearly very different from the setting we consider here as DGCCA has no such notion of a bridge view. We emphasize again that extending correlative learning to many views, say p views, using (two-view) CCA would require solving p-choose-2 CCA-like problems and various heuristics to combine them. We will add a discussion to that effect; future work/extensions will compare with bridge Corr-nets.\n\n- We discuss and compare with the most relevant related work in two view learning here, especially CCA and Deep CCA. For extensive literature, we refer the readers to our prior work including the DCCA and DCCAE papers available online. We emphasize again that this is a non-trivial extension of DCCA and various modifications/extensions of DCCA can be considered for the proposed method here as well, but that is not the focus here."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generalized Canonical Correlation Analysis", "abstract": "We present Deep Generalized Canonical Correlation Analysis (DGCCA) \u2013 a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.", "pdf": "/pdf/6a69ee127e5fb0baa8d1630b128a5f8820e30cbf.pdf", "TL;DR": "A multiview representation learning technique that can learn nonlinear mappings from arbitrarily many views to a shared semantic space -- Deep Generalized Canonical Correlation Analysis.", "paperhash": "benton|deep_generalized_canonical_correlation_analysis", "keywords": ["Unsupervised Learning", "Deep learning", "Multi-modal learning"], "conflicts": ["cs.jhu.edu", "berkeley.edu"], "authors": ["Adrian Benton", "Huda Khayrallah", "Biman Gujral", "Drew Reisinger", "Sheng Zhang", "Raman Arora"], "authorids": ["adrian@cs.jhu.edu", "huda@jhu.edu", "bgujral1@jhu.edu", "reisinger@cogsci.jhu.edu", "zsheng2@jhu.edu", "arora@cs.jhu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608344, "id": "ICLR.cc/2017/conference/-/paper361/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HycUbvcge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper361/reviewers", "ICLR.cc/2017/conference/paper361/areachairs"], "cdate": 1485287608344}}}, {"tddate": null, "tmdate": 1482591350914, "tcdate": 1482590965536, "number": 2, "id": "BJpliW3Nl", "invitation": "ICLR.cc/2017/conference/-/paper361/public/comment", "forum": "HycUbvcge", "replyto": "BkPCVXwEl", "signatures": ["~Adrian_Benton1"], "readers": ["everyone"], "writers": ["~Adrian_Benton1"], "content": {"title": "AnonReviewer2 Review Response", "comment": "\n- The code and other resources are already posted online (see the footnote on page 2).  The synthetic data are included in the linked repository.\n\n- Yes, the key idea in extending correlative learning to many views, say p views, is to compute correlation between every pair of views, store them in a pxp symmetric matrix and maximize some norm of that matrix. The norm that we consider in this paper is the spectral norm, as it leads to tractable solutions.  Na\u00efve approaches solve p-choose-2 CCA-like problems and come up with heuristics to combine them. We will add a discussion to this effect.\n\n- We greatly appreciate pointers to additional references; there has been immense activity in this area and it has been hard to catch up with the related work. We have done extensive review of related work in machine learning conferences as well as speech and language processing, but we do realize that there have been several remarkable contributions in computer vision and information retrieval. We will rectify this in the revision. \n\n- It is hard to characterize the computational complexity of the proposed methodology as is the case with any deep learning technique (it being a highly non-convex optimization problem). We can, and will, indeed discuss the computational cost per iteration and memory requirements of the proposed method (which we had included in a previous draft.)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generalized Canonical Correlation Analysis", "abstract": "We present Deep Generalized Canonical Correlation Analysis (DGCCA) \u2013 a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.", "pdf": "/pdf/6a69ee127e5fb0baa8d1630b128a5f8820e30cbf.pdf", "TL;DR": "A multiview representation learning technique that can learn nonlinear mappings from arbitrarily many views to a shared semantic space -- Deep Generalized Canonical Correlation Analysis.", "paperhash": "benton|deep_generalized_canonical_correlation_analysis", "keywords": ["Unsupervised Learning", "Deep learning", "Multi-modal learning"], "conflicts": ["cs.jhu.edu", "berkeley.edu"], "authors": ["Adrian Benton", "Huda Khayrallah", "Biman Gujral", "Drew Reisinger", "Sheng Zhang", "Raman Arora"], "authorids": ["adrian@cs.jhu.edu", "huda@jhu.edu", "bgujral1@jhu.edu", "reisinger@cogsci.jhu.edu", "zsheng2@jhu.edu", "arora@cs.jhu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287608344, "id": "ICLR.cc/2017/conference/-/paper361/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HycUbvcge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper361/reviewers", "ICLR.cc/2017/conference/paper361/areachairs"], "cdate": 1485287608344}}}, {"tddate": null, "tmdate": 1482269902570, "tcdate": 1482269902570, "number": 3, "id": "BkPCVXwEl", "invitation": "ICLR.cc/2017/conference/-/paper361/official/review", "forum": "HycUbvcge", "replyto": "HycUbvcge", "signatures": ["ICLR.cc/2017/conference/paper361/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper361/AnonReviewer2"], "content": {"title": "Deep Generalised Canonical Correlation Analysis", "rating": "6: Marginally above acceptance threshold", "review": "The authors propose a method that extends the non-linear two-view representation learning methods, and the linear multiview techniques, and combines information from multiple sources into a new non-linear representation learning techniques. \n\nIn general, the method is well described and seems to lead to benefits in different experiments of phonetic transcription of hashtag recommendation. Even if the method is mostly a extension of classical tools (the scheme learns a (deep) network for each view essentially), the combination of the different sources of information seems to be effective for the studied datasets. \n\nIt would be interesting to add or discuss the following issues:\n\n- what is the complexity of the proposed method, esp. the representation learning part?\n- would there by any alternative solution to combine the different networks/views? That could make the proposed solution more novel.\n- the experimental settings, especially in the synthetic experiments, should be more detailed. If possible, the datasets should be made available to encourage reproducibility. \n- the related work is far from complete unfortunately, especially from the perspective of the numerous multiview/multi-modal/multi-layer algorithms that have been proposed in the literature, in different applications domaines like image retrieval or classification, or bibliographic data for example (authors like A. Kumar, X. Dong, Ping-Yu Chen, M. Bronstein, and many others have proposed works in that direction in the last 5 years). No need to compare to all these works obviously, but a more complete description of the related could help appreciating better the true benefits of DGCCA.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generalized Canonical Correlation Analysis", "abstract": "We present Deep Generalized Canonical Correlation Analysis (DGCCA) \u2013 a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.", "pdf": "/pdf/6a69ee127e5fb0baa8d1630b128a5f8820e30cbf.pdf", "TL;DR": "A multiview representation learning technique that can learn nonlinear mappings from arbitrarily many views to a shared semantic space -- Deep Generalized Canonical Correlation Analysis.", "paperhash": "benton|deep_generalized_canonical_correlation_analysis", "keywords": ["Unsupervised Learning", "Deep learning", "Multi-modal learning"], "conflicts": ["cs.jhu.edu", "berkeley.edu"], "authors": ["Adrian Benton", "Huda Khayrallah", "Biman Gujral", "Drew Reisinger", "Sheng Zhang", "Raman Arora"], "authorids": ["adrian@cs.jhu.edu", "huda@jhu.edu", "bgujral1@jhu.edu", "reisinger@cogsci.jhu.edu", "zsheng2@jhu.edu", "arora@cs.jhu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512612303, "id": "ICLR.cc/2017/conference/-/paper361/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper361/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper361/AnonReviewer4", "ICLR.cc/2017/conference/paper361/AnonReviewer5", "ICLR.cc/2017/conference/paper361/AnonReviewer2"], "reply": {"forum": "HycUbvcge", "replyto": "HycUbvcge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper361/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper361/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512612303}}}, {"tddate": null, "tmdate": 1482245509039, "tcdate": 1482245509039, "number": 2, "id": "B1atHaUEg", "invitation": "ICLR.cc/2017/conference/-/paper361/official/review", "forum": "HycUbvcge", "replyto": "HycUbvcge", "signatures": ["ICLR.cc/2017/conference/paper361/AnonReviewer5"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper361/AnonReviewer5"], "content": {"title": "Review", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes a deep extension of generalized CCA. The main contribution of the paper is deriving the gradient update for the GCCA objective.\n\nI disagree with the claim that \u201cthis is the first Multiview representation learning technique that combines the flexibility of nonlinear representation learning with the statistical power of incorporating information from many independent resources or views\u201d.  [R1] proposes a Multiview representation learning method which is both non-linear and capable of handling more than 2 views. This is very much relevant to what authors are proposing. The objective function proposed in [R1] maximizes the correlation between views and minimizes the self and cross reconstruction errors. This is intuitively similar to nonlinear version of PCA+CCA for multiple views. Comparing these 2 methods is crucial to prove the usefulness of DGCCA and the paper is incomplete without this comparison. Authors should also change their strong claim.\n\nRelated work section is minimal. There are significant advances in 2-view non-linear representation learning which are worth mentioning. \n\nReferences:\n[R1] Janarthanan Rajendran, Mitesh M. Khapra, Sarath Chandar, Balaraman Ravindran: Bridge Correlational Neural Networks for Multilingual Multimodal Representation Learning. HLT-NAACL 2016: 171-181\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generalized Canonical Correlation Analysis", "abstract": "We present Deep Generalized Canonical Correlation Analysis (DGCCA) \u2013 a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.", "pdf": "/pdf/6a69ee127e5fb0baa8d1630b128a5f8820e30cbf.pdf", "TL;DR": "A multiview representation learning technique that can learn nonlinear mappings from arbitrarily many views to a shared semantic space -- Deep Generalized Canonical Correlation Analysis.", "paperhash": "benton|deep_generalized_canonical_correlation_analysis", "keywords": ["Unsupervised Learning", "Deep learning", "Multi-modal learning"], "conflicts": ["cs.jhu.edu", "berkeley.edu"], "authors": ["Adrian Benton", "Huda Khayrallah", "Biman Gujral", "Drew Reisinger", "Sheng Zhang", "Raman Arora"], "authorids": ["adrian@cs.jhu.edu", "huda@jhu.edu", "bgujral1@jhu.edu", "reisinger@cogsci.jhu.edu", "zsheng2@jhu.edu", "arora@cs.jhu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512612303, "id": "ICLR.cc/2017/conference/-/paper361/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper361/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper361/AnonReviewer4", "ICLR.cc/2017/conference/paper361/AnonReviewer5", "ICLR.cc/2017/conference/paper361/AnonReviewer2"], "reply": {"forum": "HycUbvcge", "replyto": "HycUbvcge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper361/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper361/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512612303}}}, {"tddate": null, "tmdate": 1482172873871, "tcdate": 1482172873871, "number": 1, "id": "H1GCKjB4l", "invitation": "ICLR.cc/2017/conference/-/paper361/official/review", "forum": "HycUbvcge", "replyto": "HycUbvcge", "signatures": ["ICLR.cc/2017/conference/paper361/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper361/AnonReviewer4"], "content": {"title": "Good paper", "rating": "7: Good paper, accept", "review": "The proposed method is simple and elegant; it builds upon the huge success of gradient based optimization for deep non-linear function approximators and combines it with established (linear) many-view CCA methods. A major contribution of this paper is the derivation of the gradients with respect to the non-linear encoding networks which project the different views into a common space. The derivation seems correct. In general this approach seems very interesting and I could imagine that it might be applicable to many other similarly structured problems.\nThe paper is well written; but it could be enhanced with an explicit description of the complete algorithm which also highlights how the joint embeddings G and U are updated. \n \nI don\u2019t have prior experience with CCA-style many-view techniques and it is therefore hard for me to judge the practical/empirical progress presented here. But the experiments seem reasonable convincing; although generally only performed on small and medium sized datasets.\n  \nDetailed comments: \n\nThe colours or the sign of the x-axis in figure 3b seem to be flipped compared to figure 4.\n  \nIt would be nice to additionally see a continuous (rainbow-coloured) version for Figures 2, 3 and 4 to better identify neighbouring datapoints; but more importantly: I\u2019d like to see how the average reconstruction error between the individual network outputs and the learned representation develop during training.  Is the mismatch between different views on a validation/test-set a useful metric for cross validation? In general, it seems the method is sensitive to regularization and hyperparameter selection  (because it has many more parameters compared to GCCA and different regularization parameters have been chosen for different views) and I wonder if there is a clear metric to optimize these.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Generalized Canonical Correlation Analysis", "abstract": "We present Deep Generalized Canonical Correlation Analysis (DGCCA) \u2013 a method for learning nonlinear transformations of arbitrarily many views of data, such that the resulting transformations are maximally informative of each other. While methods for nonlinear two-view representation learning (Deep CCA, (Andrew et al., 2013)) and linear many-view representation learning (Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview representation learning technique that combines the flexibility of nonlinear (deep) representation learning with the statistical power of incorporating information from many independent sources, or views. We present the DGCCA formulation as well as an efficient stochastic optimization algorithm for solving it. We learn DGCCA representations on two distinct datasets for three downstream tasks: phonetic transcription from acoustic and articulatory measurements, and recommending hashtags and friends on a dataset of Twitter users. We find that DGCCA representations soundly beat existing methods at phonetic transcription and hashtag recommendation, and in general perform no worse than standard linear many-view techniques.", "pdf": "/pdf/6a69ee127e5fb0baa8d1630b128a5f8820e30cbf.pdf", "TL;DR": "A multiview representation learning technique that can learn nonlinear mappings from arbitrarily many views to a shared semantic space -- Deep Generalized Canonical Correlation Analysis.", "paperhash": "benton|deep_generalized_canonical_correlation_analysis", "keywords": ["Unsupervised Learning", "Deep learning", "Multi-modal learning"], "conflicts": ["cs.jhu.edu", "berkeley.edu"], "authors": ["Adrian Benton", "Huda Khayrallah", "Biman Gujral", "Drew Reisinger", "Sheng Zhang", "Raman Arora"], "authorids": ["adrian@cs.jhu.edu", "huda@jhu.edu", "bgujral1@jhu.edu", "reisinger@cogsci.jhu.edu", "zsheng2@jhu.edu", "arora@cs.jhu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512612303, "id": "ICLR.cc/2017/conference/-/paper361/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper361/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper361/AnonReviewer4", "ICLR.cc/2017/conference/paper361/AnonReviewer5", "ICLR.cc/2017/conference/paper361/AnonReviewer2"], "reply": {"forum": "HycUbvcge", "replyto": "HycUbvcge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper361/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper361/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512612303}}}], "count": 9}