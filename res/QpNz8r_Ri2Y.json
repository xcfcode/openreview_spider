{"notes": [{"id": "QpNz8r_Ri2Y", "original": "z7WJDIzWKl", "number": 1498, "cdate": 1601308166710, "ddate": null, "tcdate": 1601308166710, "tmdate": 1616064224953, "tddate": null, "forum": "QpNz8r_Ri2Y", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Representation Balancing Offline Model-based Reinforcement Learning", "authorids": ["~Byung-Jun_Lee1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Byung-Jun Lee", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Off-policy policy evaluation"], "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.", "one-sentence_summary": "We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|representation_balancing_offline_modelbased_reinforcement_learning", "pdf": "/pdf/bb8bd2e330b957759e7d56f7715bdb54a256caab.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021representation,\ntitle={Representation Balancing Offline Model-based Reinforcement Learning},\nauthor={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QpNz8r_Ri2Y}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "LWoAKVjXWaw", "original": null, "number": 2, "cdate": 1615382006762, "ddate": null, "tcdate": 1615382006762, "tmdate": 1615382006762, "tddate": null, "forum": "QpNz8r_Ri2Y", "replyto": "EvOwOre2x5L", "invitation": "ICLR.cc/2021/Conference/Paper1498/-/Comment", "content": {"title": "About the codebase", "comment": "Hi Yue,\n\nThank you for your interest. The code to reproduce the proposed algorithm is now made public,\n\nand can be found in: (https://github.com/dlqudwns/repb-sde).\n\nThanks."}, "signatures": ["~Byung-Jun_Lee2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Byung-Jun_Lee2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Balancing Offline Model-based Reinforcement Learning", "authorids": ["~Byung-Jun_Lee1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Byung-Jun Lee", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Off-policy policy evaluation"], "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.", "one-sentence_summary": "We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|representation_balancing_offline_modelbased_reinforcement_learning", "pdf": "/pdf/bb8bd2e330b957759e7d56f7715bdb54a256caab.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021representation,\ntitle={Representation Balancing Offline Model-based Reinforcement Learning},\nauthor={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QpNz8r_Ri2Y}\n}"}, "tags": [], "invitation": {"reply": {"forum": "QpNz8r_Ri2Y", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper1498/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1498/Authors|ICLR.cc/2021/Conference/Paper1498/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649465405, "tmdate": 1610649465405, "id": "ICLR.cc/2021/Conference/Paper1498/-/Comment"}}}, {"id": "EvOwOre2x5L", "original": null, "number": 1, "cdate": 1615184188227, "ddate": null, "tcdate": 1615184188227, "tmdate": 1615184188227, "tddate": null, "forum": "QpNz8r_Ri2Y", "replyto": "QpNz8r_Ri2Y", "invitation": "ICLR.cc/2021/Conference/Paper1498/-/Comment", "content": {"title": "Open source code", "comment": "Hi, all,\n\nThank your for your great work. I am interested in the proposed method.\n\nWould you please share your code that can reproduce the proposed algorithm?\n\nThank you very much!\n\n\n\n "}, "signatures": ["~Yue_Wang15"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "~Yue_Wang15"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Balancing Offline Model-based Reinforcement Learning", "authorids": ["~Byung-Jun_Lee1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Byung-Jun Lee", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Off-policy policy evaluation"], "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.", "one-sentence_summary": "We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|representation_balancing_offline_modelbased_reinforcement_learning", "pdf": "/pdf/bb8bd2e330b957759e7d56f7715bdb54a256caab.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021representation,\ntitle={Representation Balancing Offline Model-based Reinforcement Learning},\nauthor={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QpNz8r_Ri2Y}\n}"}, "tags": [], "invitation": {"reply": {"forum": "QpNz8r_Ri2Y", "readers": {"values": ["everyone"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2021/Conference/Paper1498/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1498/Authors|ICLR.cc/2021/Conference/Paper1498/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs"}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}}, "multiReply": true, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["everyone"], "tcdate": 1610649465405, "tmdate": 1610649465405, "id": "ICLR.cc/2021/Conference/Paper1498/-/Comment"}}}, {"id": "Nd6RFAf3oli", "original": null, "number": 1, "cdate": 1610040493526, "ddate": null, "tcdate": 1610040493526, "tmdate": 1610474099625, "tddate": null, "forum": "QpNz8r_Ri2Y", "replyto": "QpNz8r_Ri2Y", "invitation": "ICLR.cc/2021/Conference/Paper1498/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper studies offline RL, which is an important topic in high risk domains. Compared with the existing works, this paper gives a tractable method to explicitly learn the model representation w.r.t the stationary distributions of two policies. This method is pretty general and could be paired with other pessimistic model-based RL methods.\n\nThe experiments are limited to simpler domains, and could be extended to include harder tasks from other continuous control domains. Some examples could be domains such as in Robosuite (http://robosuite.ai/) or Robogym (https://github.com/openai/robogym). These environments have higher dimensional systems with clearer implications of representation learning. \n\nThere are concerns on writing style and comprehension. \n- The work is on the one hand very specialized, on the other hand just an incremental modification of existing methods. \n- The presentation is very dense and quite hard to grasp, even with the Appendix.\n- The formalism, while important, can be very loose in terms of bounds. While that does open questions in RL theory, it would be useful for authors to be more candid about this fact in the paper. \nI would recommend including the response to R1 in the paper.\n\nOther relevant and concurrent papers to potentially take note of:\n- Fine-Tuning Offline Reinforcement Learning with Model-Based Policy Optimization (https://openreview.net/forum?id=wiSgdeJ29ee) \n- Robust Offline Reinforcement Learning from Low-Quality Data (https://openreview.net/forum?id=uOjm_xqKEoX)\n\nGiven the overall positive reviews, I would recommend acceptance. However, the method would benefit from additional pass on re-writing to make the manuscript more accessible, which in turn to increase impact of this work. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Balancing Offline Model-based Reinforcement Learning", "authorids": ["~Byung-Jun_Lee1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Byung-Jun Lee", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Off-policy policy evaluation"], "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.", "one-sentence_summary": "We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|representation_balancing_offline_modelbased_reinforcement_learning", "pdf": "/pdf/bb8bd2e330b957759e7d56f7715bdb54a256caab.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021representation,\ntitle={Representation Balancing Offline Model-based Reinforcement Learning},\nauthor={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QpNz8r_Ri2Y}\n}"}, "tags": [], "invitation": {"reply": {"forum": "QpNz8r_Ri2Y", "replyto": "QpNz8r_Ri2Y", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040493511, "tmdate": 1610474099607, "id": "ICLR.cc/2021/Conference/Paper1498/-/Decision"}}}, {"id": "qbc92xrLMg4", "original": null, "number": 4, "cdate": 1604644718751, "ddate": null, "tcdate": 1604644718751, "tmdate": 1606283434347, "tddate": null, "forum": "QpNz8r_Ri2Y", "replyto": "QpNz8r_Ri2Y", "invitation": "ICLR.cc/2021/Conference/Paper1498/-/Official_Review", "content": {"title": "This paper studies offline model-based RL, proposes a new framework to learn model representation that is robust under distribution shift.", "review": "- Clarity and Originality:\nThis paper is well-written and easy to read. The motivation is clearly stated: the original paper [Liu, et al., 2018] highly relies on the marginal action probability ratios to calculate the IPM metric, which suffers the curse of horizon issue. This paper addresses this by utilizing the discounted stationary distribution, which could be estimated using a similar DualDice trick. Theoretically the paper shows an upper bound for the policy evaluation error, which is a balance of model fitting error and the distance between the stationary distributions under behavior policy and target policy. This serves as a reasonable loss function to train the model. Furthermore, the learned model is also being used in learning task through iterative procedures. Empirically, the proposed method compared with the original Rep-BM in OPE setting, and various offline learning method in learning setting.\n\n- Significance:\nThe paper studies offline RL, which is an important topic in high risk domains. Compared with the existing works, this paper gives a tractable method to explicitly learn the model representation w.r.t the stationary distributions of two policies. This method is pretty general and could be paired with other pessimistic model-based RL methods.\n\n-  Questions:\n1. For the evaluation task, it seems the only baseline is RepBM, how's the method compared with other off-policy evaluation methods, such as IS, DR? \n2. Typically how do you choose the hyper-parameter $\\alpha$, which controls the balance of model fitting and invariance in model representation learning?\n3. Compared with Rep-BM, this work seems to upper bound $|R(\\pi)-\\hat{R}(\\pi)|$, instead of MSE, which also results in different loss functions in training M, one is L2 loss, one is log-likelihood, how does this affect the empirical results?\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1498/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Balancing Offline Model-based Reinforcement Learning", "authorids": ["~Byung-Jun_Lee1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Byung-Jun Lee", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Off-policy policy evaluation"], "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.", "one-sentence_summary": "We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|representation_balancing_offline_modelbased_reinforcement_learning", "pdf": "/pdf/bb8bd2e330b957759e7d56f7715bdb54a256caab.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021representation,\ntitle={Representation Balancing Offline Model-based Reinforcement Learning},\nauthor={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QpNz8r_Ri2Y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QpNz8r_Ri2Y", "replyto": "QpNz8r_Ri2Y", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1498/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538117205, "tmdate": 1606915798390, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1498/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1498/-/Official_Review"}}}, {"id": "alZI3EEHCOQ", "original": null, "number": 14, "cdate": 1606253742085, "ddate": null, "tcdate": 1606253742085, "tmdate": 1606253742085, "tddate": null, "forum": "QpNz8r_Ri2Y", "replyto": "k77ZuwzNoBW", "invitation": "ICLR.cc/2021/Conference/Paper1498/-/Official_Comment", "content": {"title": "Response to Authors", "comment": "I am satisfied with the feedback and the changes to the paper. I raised my rating."}, "signatures": ["ICLR.cc/2021/Conference/Paper1498/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Balancing Offline Model-based Reinforcement Learning", "authorids": ["~Byung-Jun_Lee1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Byung-Jun Lee", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Off-policy policy evaluation"], "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.", "one-sentence_summary": "We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|representation_balancing_offline_modelbased_reinforcement_learning", "pdf": "/pdf/bb8bd2e330b957759e7d56f7715bdb54a256caab.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021representation,\ntitle={Representation Balancing Offline Model-based Reinforcement Learning},\nauthor={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QpNz8r_Ri2Y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QpNz8r_Ri2Y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1498/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1498/Authors|ICLR.cc/2021/Conference/Paper1498/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858983, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1498/-/Official_Comment"}}}, {"id": "nA3L4OZ8WnX", "original": null, "number": 2, "cdate": 1603493162849, "ddate": null, "tcdate": 1603493162849, "tmdate": 1606253531004, "tddate": null, "forum": "QpNz8r_Ri2Y", "replyto": "QpNz8r_Ri2Y", "invitation": "ICLR.cc/2021/Conference/Paper1498/-/Official_Review", "content": {"title": "dense presentation gives the reader a hard time", "review": "Summary:\nThe paper deals with batch RL (aka offline RL). It builds on \"Representation Balancing MDP (RepBM)\" and tries to improve the procedure by \"stationary DIstribution Correction Estimation (DICE)\". DualDICE\", a further development of DICE, is used for this purpose. \nThese procedures are known in each case.\nMain claims:\n\u201eWe empirically show that the model trained by the RepB-SDE objective is robust to the distribution shift for the OPE task, particularly when the difference between the target and the behavior is large.\u201e\nThe introduced \u201emodel-based offline RL algorithm based on RepB-SDE\u201c has \u201estate-of-the-art performance in a representative set of tasks\u201c from D4RL.\n\nStrong points:\nThe paper makes no exaggerated claims.\nThe treatment of the newer, related works is very good.\nThe experiments are extensive.\nI like the formulation \u201ebehavior-agnostic setting where we do not have any knowledge of the data collection\nprocess.\u201c  This expresses the, in my opinion, correct view of the real situation well, while the assumption that there is a \"behavior policy\" that generated the data is not true in general. It may have been different people at different times who performed the actions while the data set was recorded.\n\nWeak points:\nThe work is on the one hand very specialized, on the other hand just an incremental modification of existing methods.\nThe presentation is very dense and quite hard to grasp, even with the Appendix.\n\nRecommendation:\nBorderline accept.\n\nQuestions:\nIn the Appendix a potential limitation to deterministic environments is mentioned. Is this just a special case, or is this a true limitation? If it is a true limitation, then this should be mentioned not only in the Appendix, but in the main text.\nWhy are the uncertainties (standard error) of the other methods not also given in Table 1?\n\nAdditional feedback with the aim to improve the paper:\nPlease be more explicit to make the text more understandable. It should be clarified early that the procedure is applicable to continuous state spaces and continuous action spaces---the initial consideration of the stationary distribution uses discrete state and discrete action spaces. Accordingly the spaces S, A, Z should be defined.\n\nPlease correct missing capital letters in the bibliography, e.g. mdps, Algaedice, gaussian\nAnd fix \"\\phi-divergences\" -> \"$\\phi$-divergences\"\n\n\"16GB\" -> \"16 GB\"\n\nIn Figure 1 the measurement points are connected by lines. Actually lines in such a plot are reserved for a fit to the points or a theoretical curve. The plot would therefore look more scientific if the points were not connected.\n\n\n===================\n\n(Nov 24)  I increased my score to 7.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1498/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Balancing Offline Model-based Reinforcement Learning", "authorids": ["~Byung-Jun_Lee1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Byung-Jun Lee", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Off-policy policy evaluation"], "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.", "one-sentence_summary": "We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|representation_balancing_offline_modelbased_reinforcement_learning", "pdf": "/pdf/bb8bd2e330b957759e7d56f7715bdb54a256caab.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021representation,\ntitle={Representation Balancing Offline Model-based Reinforcement Learning},\nauthor={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QpNz8r_Ri2Y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QpNz8r_Ri2Y", "replyto": "QpNz8r_Ri2Y", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1498/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538117205, "tmdate": 1606915798390, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1498/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1498/-/Official_Review"}}}, {"id": "OmX82___iHN", "original": null, "number": 13, "cdate": 1606236171882, "ddate": null, "tcdate": 1606236171882, "tmdate": 1606236171882, "tddate": null, "forum": "QpNz8r_Ri2Y", "replyto": "QrVEScFtBtW", "invitation": "ICLR.cc/2021/Conference/Paper1498/-/Official_Comment", "content": {"title": "Response to Authors", "comment": "Thanks for the updates. I appreciate the clarification about the assumption and raised my score accordingly. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1498/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Balancing Offline Model-based Reinforcement Learning", "authorids": ["~Byung-Jun_Lee1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Byung-Jun Lee", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Off-policy policy evaluation"], "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.", "one-sentence_summary": "We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|representation_balancing_offline_modelbased_reinforcement_learning", "pdf": "/pdf/bb8bd2e330b957759e7d56f7715bdb54a256caab.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021representation,\ntitle={Representation Balancing Offline Model-based Reinforcement Learning},\nauthor={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QpNz8r_Ri2Y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QpNz8r_Ri2Y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1498/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1498/Authors|ICLR.cc/2021/Conference/Paper1498/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858983, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1498/-/Official_Comment"}}}, {"id": "bCip_qLEjK2", "original": null, "number": 1, "cdate": 1603061297431, "ddate": null, "tcdate": 1603061297431, "tmdate": 1606234393247, "tddate": null, "forum": "QpNz8r_Ri2Y", "replyto": "QpNz8r_Ri2Y", "invitation": "ICLR.cc/2021/Conference/Paper1498/-/Official_Review", "content": {"title": "Assumptions need better motivation", "review": "Strength: \nModel learning is an important component for offline RL, which is usually done independently from policy evaluation / optimization. The authors propose a new model learning method for offline RL that takes policy evaluation error into consideration / as regularization. \nAn upper bound is derived to guarantee the worst case performance. In terms of policy evaluation, the authors show empirical advantages over previous model-based offline OPE algorithms. In terms of control, the authors show empirical advantages over existing model-based and model-free algorithms in the challenging D4RL dataset.\n\nWeakness & Points to be clarified:\nMy major concern is the assumption used in the paper. \nThe assumption about B_\\phi in Theorem 4.3 looks not well motivated. When should we expect there exists such a B_\\phi? How large are B_\\phi and \\bar{k}? If B_\\phi and \\bar{k} are very large, I feel the bound in theorem 4.3 can be very loose. I think the paper may benefit from clarifying more on this assumption.\n\nMinor comments:\nThe authors compare with both model-based and model-free approaches in the control setting.\nIn OPE, however, only model-based approach is compared. I would suggest to add more model-free baselines, e.g., Fitted-Q-Evaluation [1] and DICEs, to motivate the necessity for learning a model.\n\nOverall I think the empirical results are convincing and I am happy to increase my score if the assumptions are further clarified.\n\n[1] Voloshin, Cameron, et al. \"Empirical Study of Off-Policy Policy Evaluation for Reinforcement Learning.\" arXiv preprint arXiv:1911.06854 (2019).\n\n===================\n\n(Nov 24) The authors addressed my concerns in the reply so I increased my score to 6.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1498/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Balancing Offline Model-based Reinforcement Learning", "authorids": ["~Byung-Jun_Lee1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Byung-Jun Lee", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Off-policy policy evaluation"], "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.", "one-sentence_summary": "We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|representation_balancing_offline_modelbased_reinforcement_learning", "pdf": "/pdf/bb8bd2e330b957759e7d56f7715bdb54a256caab.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021representation,\ntitle={Representation Balancing Offline Model-based Reinforcement Learning},\nauthor={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QpNz8r_Ri2Y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QpNz8r_Ri2Y", "replyto": "QpNz8r_Ri2Y", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1498/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538117205, "tmdate": 1606915798390, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1498/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1498/-/Official_Review"}}}, {"id": "8EMgrdQfzYx", "original": null, "number": 8, "cdate": 1606225921604, "ddate": null, "tcdate": 1606225921604, "tmdate": 1606225921604, "tddate": null, "forum": "QpNz8r_Ri2Y", "replyto": "QpNz8r_Ri2Y", "invitation": "ICLR.cc/2021/Conference/Paper1498/-/Official_Comment", "content": {"title": "Summary of the responses and revisions", "comment": "We thank all the reviewers for their time, comments and for providing constructive suggestions. Our responses to the questions and the improvements in the revised paper are summarized below.\n\n**Responses to the questions:**\n\n1. **[R1] Assumptions in Theorem 4.3 about $B _ { \\phi } , \\bar { k }$ :**  in our case, $B _ { \\phi }$ can be seen as an RKHS norm of discounted sum of model errors, and $\\bar { k }$ can be subsumed into $B _ \\phi$.\n2. **[R4, R5] Relation to the loss function of RepBM:** L2 distance minimization of RepBM is a special case of log-likelihood maximization we used. We used L2 distance minimization in OPE experiments for the sake of comparison with RepBM.\n3. **[R2] Concern about the motivation:** we referred to BCQ in the introduction because simple Q-learning on a pre-collected dataset converges to the Q-function of MLE MDP in tabular case. \n4. **[R5] Hyperparameter selection:** similar to the other concurrent works on offline RL, we reported the result of hyperparameters with the best mean performance.\n\n**Major paper improvements:**\n1. We added OPE results with more baselines (IS, DR, FQE, DualDICE) in Appendix B (Figure 2), as suggested by Reviewer 1 and Reviewer 5.\n2. We added D4RL benchmark results with standard errors fully specified in Appendix B (Table 4), as suggested by Reviewer 4.\n3. We added state-of-the-art baseline (CQL) to the D4RL experiment section, as suggested by Reviewer 2.\n\n**Minor paper improvements:**\n1. We corrected typos in the bibliography and the Appendix as suggested by Reviewer 4.\n2. We clarified the reason why we used L2 distance minimization in the OPE experiments in Appendix B to address the concern of Reviewer 4.\n3. In Section 3, we clarified that we use continuous state spaces and both (discrete and continuous) action spaces throughout the paper, as suggested by Reviewer 4.\n4. We removed the line connecting the measurement points in Figure 1, as suggested by Reviewer 4.\n5. We clarified the motivation part in the introduction (3rd paragraph) to address the concern of Reviewer 2.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1498/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Balancing Offline Model-based Reinforcement Learning", "authorids": ["~Byung-Jun_Lee1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Byung-Jun Lee", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Off-policy policy evaluation"], "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.", "one-sentence_summary": "We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|representation_balancing_offline_modelbased_reinforcement_learning", "pdf": "/pdf/bb8bd2e330b957759e7d56f7715bdb54a256caab.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021representation,\ntitle={Representation Balancing Offline Model-based Reinforcement Learning},\nauthor={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QpNz8r_Ri2Y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QpNz8r_Ri2Y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1498/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1498/Authors|ICLR.cc/2021/Conference/Paper1498/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858983, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1498/-/Official_Comment"}}}, {"id": "TaWq5n13hfr", "original": null, "number": 5, "cdate": 1605624648230, "ddate": null, "tcdate": 1605624648230, "tmdate": 1605626803496, "tddate": null, "forum": "QpNz8r_Ri2Y", "replyto": "FFVGONJo6I6", "invitation": "ICLR.cc/2021/Conference/Paper1498/-/Official_Comment", "content": {"title": "About the motivation we made", "comment": "Thank you for your constructive comments and feedback.\n\n**Missing baseline:** We included the suggested missing baseline (CQL) in the comparisons. CQL performs the best among model-free algorithms in more than half of the datasets, and by including CQL we found that the best of Model-free methods (MF in Table 1) becomes better than our RepB-SDE in two more datasets. Still, RepB-SDE shows the best average performance in 4 of 12 datasets (same with CQL) and performed competitively in many other datasets. This offline RL performance could be further improved by replacing the simple SAC-based optimization using penalized reward with a more sophisticated model-based RL algorithm.\n\n**Concern about motivation:** The point we aim to address in the introduction was that, in finite MDPs, the fixed point admitted by Q-learning algorithm based on a pre-collected dataset is equivalent to the Q-function of MLE MDP constructed from the corresponding dataset (e.g. see Theorem 3 of SPIBB [1]). Even in the model-free cases, it might be helpful to learn a Q-function that is equivalent to that of some other MDP which is more robust in OPE than simple MLE MDP, unlike simple MSTDE minimization in BCQ. We revised the paper to clarify our point.\n\n[1] Laroche, Romain, Paul Trichelair, and Remi Tachet Des Combes. \"Safe policy improvement with baseline bootstrapping.\" International Conference on Machine Learning. PMLR, 2019.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1498/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Balancing Offline Model-based Reinforcement Learning", "authorids": ["~Byung-Jun_Lee1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Byung-Jun Lee", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Off-policy policy evaluation"], "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.", "one-sentence_summary": "We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|representation_balancing_offline_modelbased_reinforcement_learning", "pdf": "/pdf/bb8bd2e330b957759e7d56f7715bdb54a256caab.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021representation,\ntitle={Representation Balancing Offline Model-based Reinforcement Learning},\nauthor={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QpNz8r_Ri2Y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QpNz8r_Ri2Y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1498/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1498/Authors|ICLR.cc/2021/Conference/Paper1498/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858983, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1498/-/Official_Comment"}}}, {"id": "KUaMuxbuPKZ", "original": null, "number": 7, "cdate": 1605624894849, "ddate": null, "tcdate": 1605624894849, "tmdate": 1605624954574, "tddate": null, "forum": "QpNz8r_Ri2Y", "replyto": "qbc92xrLMg4", "invitation": "ICLR.cc/2021/Conference/Paper1498/-/Official_Comment", "content": {"title": "We appreciate your feedback.", "comment": "Thank you for your helpful comments and feedback.\n\n**Additional baselines of OPE experiment:** We added OPE results with more baselines (IS, DR, FQE, DualDICE) in the Appendix (Figure 2).\n\n**Choosing the hyper-parameter:** Similar to the other concurrent works on offline RL [1], we ran the algorithm with few different hyperparameters until the convergence and reported the result with the best mean performance. The set of hyperparameters we evaluated and the effect of varying hyperparameters are shown in the Appendix.\n\n**Different loss functions:** Empirically L2 loss is a special case of log-likelihood as we can derive L2 loss by assuming a Gaussian transition model with fixed variance. In OPE experiments where we aim to compare our framework against RepBM, we used L2 loss for our loss function for consistency. In more complex domains, e.g. stochastic environments, assuming transition models with more flexible distributions and training with log-likelihood (i.e. learning the variance parameter as well as the mean parameter) can be better in OPE performance than simply using L2 loss with deterministic transition models.\n\n[1] Kidambi, Rahul, et al. \"MOReL: Model-Based Offline Reinforcement Learning.\" Advances in Neural Information Processing Systems. 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1498/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Balancing Offline Model-based Reinforcement Learning", "authorids": ["~Byung-Jun_Lee1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Byung-Jun Lee", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Off-policy policy evaluation"], "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.", "one-sentence_summary": "We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|representation_balancing_offline_modelbased_reinforcement_learning", "pdf": "/pdf/bb8bd2e330b957759e7d56f7715bdb54a256caab.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021representation,\ntitle={Representation Balancing Offline Model-based Reinforcement Learning},\nauthor={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QpNz8r_Ri2Y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QpNz8r_Ri2Y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1498/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1498/Authors|ICLR.cc/2021/Conference/Paper1498/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858983, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1498/-/Official_Comment"}}}, {"id": "QrVEScFtBtW", "original": null, "number": 4, "cdate": 1605624463159, "ddate": null, "tcdate": 1605624463159, "tmdate": 1605624799492, "tddate": null, "forum": "QpNz8r_Ri2Y", "replyto": "bCip_qLEjK2", "invitation": "ICLR.cc/2021/Conference/Paper1498/-/Official_Comment", "content": {"title": "About the assumptions in Theorem 4.3", "comment": "We sincerely appreciate your valuable comments and feedback.\n\n**Assumptions in Theorem 4.3:** For the ease of notation, in the following comment, we define the expected model loss over trajectories starting from the state represented by $z$, \n$f_{\\phi,R,T}(z)=\\mathbb{E} _ {T, \\pi} [\\sum_{t=0}^\\infty \\gamma^t \\mathcal{E}_{\\phi,R,T}(s_t,a_t)|(s_0,a_0)=\\phi^{-1}(z)]$. We would like to clarify that the assumption on $B_\\phi$ in Theorem 4.3 is equivalent to saying that $ f _ { \\phi, R, T } (z) \\in \\mathcal{H} _ k $ with $ B _ \\phi = \\lVert f _ { \\phi, R, T} \\rVert _ { \\mathcal{H}_k } $, which captures the magnitude and the smoothness of $f _ {\\phi, R, T}$ [1]. In general, assuming the underlying dynamics is smooth, we can expect $B_\\phi$ to be small when the model error is small. Also, $\\bar{k}$ depends on the kernel function we use, but w.l.o.g. we can let $ \\bar { k } = 1$and subsume it into $ B _ \\phi $, i.e. using $ \\tilde { B } _ { \\phi } \\triangleq B _ \\phi \\sqrt{ \\bar { k } }$. Note that these assumptions are also frequently made in related literature on learning generalizable representations [2, 3, 4].\n\n**Additional baselines of OPE experiment:** We added OPE results with more baselines (IS, DR, FQE, DualDICE) in the Appendix (Figure 2).\n\n[1] Kanagawa, Motonobu, et al. \"Gaussian processes and kernel methods: A review on connections and equivalences.\" arXiv preprint arXiv:1807.02582 (2018).\n\n[2] Shalit, Uri, Fredrik D. Johansson, and David Sontag. \"Estimating individual treatment effect: generalization bounds and algorithms.\" International Conference on Machine Learning. PMLR, 2017.\n\n[3] Liu, Yao, et al. \"Representation balancing MDPs for off-policy policy evaluation.\" Advances in Neural Information Processing Systems. 2018.\n\n[4] Johansson, Fredrik D., David Sontag, and Rajesh Ranganath. \"Support and Invertibility in Domain-Invariant Representations.\" The 22nd International Conference on Artificial Intelligence and Statistics. 2019.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1498/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Balancing Offline Model-based Reinforcement Learning", "authorids": ["~Byung-Jun_Lee1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Byung-Jun Lee", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Off-policy policy evaluation"], "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.", "one-sentence_summary": "We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|representation_balancing_offline_modelbased_reinforcement_learning", "pdf": "/pdf/bb8bd2e330b957759e7d56f7715bdb54a256caab.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021representation,\ntitle={Representation Balancing Offline Model-based Reinforcement Learning},\nauthor={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QpNz8r_Ri2Y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QpNz8r_Ri2Y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1498/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1498/Authors|ICLR.cc/2021/Conference/Paper1498/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858983, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1498/-/Official_Comment"}}}, {"id": "k77ZuwzNoBW", "original": null, "number": 6, "cdate": 1605624781604, "ddate": null, "tcdate": 1605624781604, "tmdate": 1605624781604, "tddate": null, "forum": "QpNz8r_Ri2Y", "replyto": "nA3L4OZ8WnX", "invitation": "ICLR.cc/2021/Conference/Paper1498/-/Official_Comment", "content": {"title": "Thank you for your constructive comments and feedback.", "comment": "We greatly appreciate your thoughtful comments and feedback.\n\nWhile we build on an existing method (RepBM), we emphasize that the proposed method effectively addresses the significant drawbacks of RepBM, including the curse of horizon and various limitations such as discrete action space and deterministic target policy. Consequently, RepB-SDE can be applied to a much broader set of tasks, e.g. to solve continuous control tasks with deep model-based RL algorithms.\n\n**Deterministic environment:** Our method can be applied to both deterministic/stochastic environments. We used a deterministic environment and L2 distance minimization for the sake of comparison with RepBM, and L2 distance minimization is a special case of our objective. Sorry for the confusion and we have clarified the text in the Appendix.\n\n**Missing standard error:** We could not include the standard errors of other methods in Table 1 due to the space limitation. We included another table (Table 4) with standard errors fully specified in the Appendix.\n\n**Definitions of spaces and the stationary distribution:** We clarified that the framework is applicable to continuous state-action spaces and defined the spaces accordingly. Although the definition of discounted stationary distribution provided is not formal for continuous state-action space, we decided to keep it as is since it is the definition widely adopted in RL literature [1, 2]. \n\n**Additional feedback:** We corrected the bibliography, the Appendix, and Figure 1 as suggested. \n\n[1] Nachum, Ofir, et al. \"DualDICE: Behavior-agnostic estimation of discounted stationary distribution corrections.\" Advances in Neural Information Processing Systems. 2019.\n\n[2] Zhang, Ruiyi, et al. \"GenDICE: Generalized Offline Estimation of Stationary Values.\" International Conference on Learning Representations. 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1498/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Balancing Offline Model-based Reinforcement Learning", "authorids": ["~Byung-Jun_Lee1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Byung-Jun Lee", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Off-policy policy evaluation"], "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.", "one-sentence_summary": "We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|representation_balancing_offline_modelbased_reinforcement_learning", "pdf": "/pdf/bb8bd2e330b957759e7d56f7715bdb54a256caab.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021representation,\ntitle={Representation Balancing Offline Model-based Reinforcement Learning},\nauthor={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QpNz8r_Ri2Y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QpNz8r_Ri2Y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1498/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1498/Authors|ICLR.cc/2021/Conference/Paper1498/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858983, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1498/-/Official_Comment"}}}, {"id": "yD2I3Br4PD", "original": null, "number": 2, "cdate": 1605041263616, "ddate": null, "tcdate": 1605041263616, "tmdate": 1605041263616, "tddate": null, "forum": "QpNz8r_Ri2Y", "replyto": "QpNz8r_Ri2Y", "invitation": "ICLR.cc/2021/Conference/Paper1498/-/Official_Comment", "content": {"title": "D4RL not yet published", "comment": "It should be considered, that D4RL is currently under review at ICLR2021 (see https://openreview.net/forum?id=px0-N3_KjA) and not yet published."}, "signatures": ["ICLR.cc/2021/Conference/Paper1498/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Balancing Offline Model-based Reinforcement Learning", "authorids": ["~Byung-Jun_Lee1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Byung-Jun Lee", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Off-policy policy evaluation"], "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.", "one-sentence_summary": "We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|representation_balancing_offline_modelbased_reinforcement_learning", "pdf": "/pdf/bb8bd2e330b957759e7d56f7715bdb54a256caab.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021representation,\ntitle={Representation Balancing Offline Model-based Reinforcement Learning},\nauthor={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QpNz8r_Ri2Y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QpNz8r_Ri2Y", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1498/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1498/Authors|ICLR.cc/2021/Conference/Paper1498/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858983, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1498/-/Official_Comment"}}}, {"id": "FFVGONJo6I6", "original": null, "number": 3, "cdate": 1603900869395, "ddate": null, "tcdate": 1603900869395, "tmdate": 1605024427869, "tddate": null, "forum": "QpNz8r_Ri2Y", "replyto": "QpNz8r_Ri2Y", "invitation": "ICLR.cc/2021/Conference/Paper1498/-/Official_Review", "content": {"title": "An interesting model-based offline RL", "review": "In this paper, the authors propose a model-based approach with representation balancing (RepB-SDE)to cope with the distribution shift of offline reinforcement learning. RepB-SDE learns a robust representation for the model learning process, which regularizes the distance between the data distribution and the discount stationary distribution of the target policy in the representation space. RepB-SDE adopts the estimation techniques of DualDICE and a novel point is that RepB-SDE plugs this trick into the model-based representation learning and proposes an effective model-based offline RL algorithm.\t\t\t\t\t\n\nThe combination of policy estimation techniques and model representation learningin this paper looks interesting and its theoretical derivation is sound. The experiments demonstrate the effectiveness of RepB-SDE over almost 10 baselines in the popular offline RL benchmark D4RL. But this experiment part can be improved, e.g., including the state-of-the-art offline model-free baseline (Kumar et al., 2020; CQL).\t\t\t\t\t\n\nThis paper is well written, especially its methodology and experiment parts are clear. But I have some concerns about the motivation of RepB-SDE in the introduction part, where this paper says that \"However, recent offline RL studies mainly focus on how to improve the policy conservatively while using a common policy evaluation technique without much considerations for the distribution shift.\"\u00a0BCQ (Fujimoto et al., 2019) considers the Q-learning (value-iteration)framework of abstracted MDP induced by the given dataset and uses an ensemble of action samplings to deal with continuous action space. From the Q-learning perspective, BCQ can realize effective policy evaluation in the tabular case or with rich expressiveness of Q function. The policy evaluation technique of BCQ does not need importance sampling (see (Haarnoja et al, 2018; SAC) similarly) orDualDICE-like techniques. In order to better presentation, the introduction part needs to be well-motivated and justify recent offline RL algorithms fairly.\u00a0", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1498/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1498/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Representation Balancing Offline Model-based Reinforcement Learning", "authorids": ["~Byung-Jun_Lee1", "~Jongmin_Lee1", "~Kee-Eung_Kim4"], "authors": ["Byung-Jun Lee", "Jongmin Lee", "Kee-Eung Kim"], "keywords": ["Reinforcement Learning", "Model-based Reinforcement Learning", "Offline Reinforcement Learning", "Batch Reinforcement Learning", "Off-policy policy evaluation"], "abstract": "One of the main challenges in offline and off-policy reinforcement learning is to cope with the distribution shift that arises from the mismatch between the target policy and the data collection policy. In this paper, we focus on a model-based approach, particularly on learning the representation for a robust model of the environment under the distribution shift, which has been first studied by Representation Balancing MDP (RepBM). Although this prior work has shown promising results, there are a number of shortcomings that still hinder its applicability to practical tasks. In particular, we address the curse of horizon exhibited by RepBM, rejecting most of the pre-collected data in long-term tasks. We present a new objective for model learning motivated by recent advances in the estimation of stationary distribution corrections. This effectively overcomes the aforementioned limitation of RepBM, as well as naturally extending to continuous action spaces and stochastic policies. We also present an offline model-based policy optimization using this new objective, yielding the state-of-the-art performance in a representative set of benchmark offline RL tasks.", "one-sentence_summary": "We present RepB-SDE, a framework for balancing the model representation with stationary distribution estimation, aiming at obtaining a model robust to the distribution shift that arises in off-policy and offline RL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|representation_balancing_offline_modelbased_reinforcement_learning", "pdf": "/pdf/bb8bd2e330b957759e7d56f7715bdb54a256caab.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021representation,\ntitle={Representation Balancing Offline Model-based Reinforcement Learning},\nauthor={Byung-Jun Lee and Jongmin Lee and Kee-Eung Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=QpNz8r_Ri2Y}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QpNz8r_Ri2Y", "replyto": "QpNz8r_Ri2Y", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1498/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538117205, "tmdate": 1606915798390, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1498/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1498/-/Official_Review"}}}], "count": 16}