{"notes": [{"id": "Skx82ySYPH", "original": "HyxyFZytDB", "number": 1953, "cdate": 1569439662179, "ddate": null, "tcdate": 1569439662179, "tmdate": 1583912022365, "tddate": null, "forum": "Skx82ySYPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Neural Outlier Rejection for Self-Supervised Keypoint Learning", "authors": ["Jiexiong Tang", "Hanme Kim", "Vitor Guizilini", "Sudeep Pillai", "Rares Ambrus"], "authorids": ["jiexiong@kth.se", "hanme.kim@tri.global", "vitor.guizilini@tri.global", "sudeep.pillai@tri.global", "rares.ambrus@tri.global"], "keywords": ["Self-Supervised Learning", "Keypoint Detection", "Outlier Rejection", "Deep Learning"], "TL;DR": "Learning to extract distinguishable keypoints from a proxy task, outlier rejection.", "abstract": "Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks.  However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.", "pdf": "/pdf/7e852806d1e285ccb14c305768f8d9ae40bfb2a8.pdf", "paperhash": "tang|neural_outlier_rejection_for_selfsupervised_keypoint_learning", "code": "https://github.com/TRI-ML/KP2D", "_bibtex": "@inproceedings{\ntang2020neural,\ntitle={Neural Outlier Rejection for Self-Supervised Keypoint Learning},\nauthor={Jiexiong Tang and Rares Ambrus and Vitor Guizilini and Hanme Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx82ySYPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cbedd0a3f9c84eacc4049cbcc3aa8e086d9527.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "rsg7Mx14yE", "original": null, "number": 1, "cdate": 1576798736725, "ddate": null, "tcdate": 1576798736725, "tmdate": 1576800899631, "tddate": null, "forum": "Skx82ySYPH", "replyto": "Skx82ySYPH", "invitation": "ICLR.cc/2020/Conference/Paper1953/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper proposes a solid (if somewhat incremental) improvement on an interesting and well-studied problem. I suggest accepting it.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Outlier Rejection for Self-Supervised Keypoint Learning", "authors": ["Jiexiong Tang", "Hanme Kim", "Vitor Guizilini", "Sudeep Pillai", "Rares Ambrus"], "authorids": ["jiexiong@kth.se", "hanme.kim@tri.global", "vitor.guizilini@tri.global", "sudeep.pillai@tri.global", "rares.ambrus@tri.global"], "keywords": ["Self-Supervised Learning", "Keypoint Detection", "Outlier Rejection", "Deep Learning"], "TL;DR": "Learning to extract distinguishable keypoints from a proxy task, outlier rejection.", "abstract": "Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks.  However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.", "pdf": "/pdf/7e852806d1e285ccb14c305768f8d9ae40bfb2a8.pdf", "paperhash": "tang|neural_outlier_rejection_for_selfsupervised_keypoint_learning", "code": "https://github.com/TRI-ML/KP2D", "_bibtex": "@inproceedings{\ntang2020neural,\ntitle={Neural Outlier Rejection for Self-Supervised Keypoint Learning},\nauthor={Jiexiong Tang and Rares Ambrus and Vitor Guizilini and Hanme Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx82ySYPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cbedd0a3f9c84eacc4049cbcc3aa8e086d9527.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Skx82ySYPH", "replyto": "Skx82ySYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795706002, "tmdate": 1576800253919, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1953/-/Decision"}}}, {"id": "HJeKkwvk5H", "original": null, "number": 3, "cdate": 1571940064841, "ddate": null, "tcdate": 1571940064841, "tmdate": 1574377319632, "tddate": null, "forum": "Skx82ySYPH", "replyto": "Skx82ySYPH", "invitation": "ICLR.cc/2020/Conference/Paper1953/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "The following work proposes several improvements over prior works in unsupervised/self-supervised keypoint-descriptor learning such as Christiansen et al. One improvement is the relaxation of the cell-boundaries for keypoint prediction -- specifically allowing keypoints anchored at the cell's center to be offset into neighboring cells. Another change was the introduction of an inlier-outlier classifier network to be used as a proxy loss for the keypoint position and descriptors. They found the inlier-outlier loss to improve homography accuracy at 1 and 3 pixel thresholds.\n\nStrengths:\n-The ablation study seems complete\n-Clear improvements over state of the art methods\n\nWeaknesses/improvements:\n-The description of the evaluation procedure was a bit vague. Is RANSAC being used to find correspondences? If so, perhaps error bars are necessary to account for variance across multiple runs?\n-Make it more clear in the related works about how the proposed method relates to Unsuperpoint. My understanding is that the proposed work is a somewhat incremental improvement over Unsuperpoint.\n-Section 3.3 (Score learning) was a bit difficult to follow. I find it better to start by stating the high level goal of the loss function before going into the formulation.\n-Captions for Tables 2 and 3 are lacking. At the very least, mention what the numbers being compared are.\n\nOverall, I think the improvements are a bit incremental, but the experiments seem to support the claim that they are beneficial. I had some concerns about the clarity of the paper, and would be willing to raise my rating if addressed.\n\nPost Rebuttal:\nThe authors have adequately addressed my concerns regarding clarity. I have updated my rating to weak accept in agreement with the other reviews.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1953/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1953/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Outlier Rejection for Self-Supervised Keypoint Learning", "authors": ["Jiexiong Tang", "Hanme Kim", "Vitor Guizilini", "Sudeep Pillai", "Rares Ambrus"], "authorids": ["jiexiong@kth.se", "hanme.kim@tri.global", "vitor.guizilini@tri.global", "sudeep.pillai@tri.global", "rares.ambrus@tri.global"], "keywords": ["Self-Supervised Learning", "Keypoint Detection", "Outlier Rejection", "Deep Learning"], "TL;DR": "Learning to extract distinguishable keypoints from a proxy task, outlier rejection.", "abstract": "Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks.  However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.", "pdf": "/pdf/7e852806d1e285ccb14c305768f8d9ae40bfb2a8.pdf", "paperhash": "tang|neural_outlier_rejection_for_selfsupervised_keypoint_learning", "code": "https://github.com/TRI-ML/KP2D", "_bibtex": "@inproceedings{\ntang2020neural,\ntitle={Neural Outlier Rejection for Self-Supervised Keypoint Learning},\nauthor={Jiexiong Tang and Rares Ambrus and Vitor Guizilini and Hanme Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx82ySYPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cbedd0a3f9c84eacc4049cbcc3aa8e086d9527.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skx82ySYPH", "replyto": "Skx82ySYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1953/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1953/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575778255801, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1953/Reviewers"], "noninvitees": [], "tcdate": 1570237729896, "tmdate": 1575778255815, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1953/-/Official_Review"}}}, {"id": "rklyawTEYS", "original": null, "number": 1, "cdate": 1571243958654, "ddate": null, "tcdate": 1571243958654, "tmdate": 1574337140869, "tddate": null, "forum": "Skx82ySYPH", "replyto": "Skx82ySYPH", "invitation": "ICLR.cc/2020/Conference/Paper1953/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "The paper is devoted to self-supervised learning of local features (both detectors and descriptors simultaneously). The problem is old yet not fully solved yet, because handcrafted SIFT is still winning the benchmarks. This work mostly follows and improves upon SuperPoint (DeTone et.al 2017) and the follow-up work UnsuperPoint (Christiansen et.al 2019) architecture and training scheme.\n\nThe claimed contributions are following:\n  - use the recently published Neural Guided RANSAC as additional auxilary loss provider\n  - allowing the \"cells\" to predict keypoint location outside the cell while learning\n  - special procedure for improving descriptors interpolation\n\n\nThe experiments are performed on HSequences dataset (wrongly called \"HPatches\", as HPatches dataset is literally image patches, not full images), showing noticable improvement over the state of the art.\n\n\nStrong points:\n\n - Method is sound, paper is mostly well written and results are good (may be too good, see questions).\n \n \n \n Questions:\n \n  1) Regarding descriptor interpolation, which is claimed as contribution. It is not clear for me, how different it is compared to SuperPoint one, which also do descriptor upsampling, so that network output is H x W x [256], i.e. full resolution. Could you please clarify the differences to it? Also, in Figure 2 it is not clear, how one can do \"feature concatenation\" for blocks with different spatial resolution.\n\n  2) Why the IONet is used only for training?  Wouldn`t it better to actually learn everything end-to-end, which is already done in paper and evaluate? \n  \n  3) How is association in training (e.g. on Fig.3) done, if multiple cells in img2 returns keypoint close to the same keypoint in img1? \n\n  4) HSequences consists of two subsets: Illumination and Viewpoint. Could you please report results per subset instead of per whole dataset? Could you also please specifically report results for the following image sequences: graffity, bark, boat, especially for 1-6 pairs and visualize matches (same way as in Figure 4-6)?\n  The reason that I am asking these, is results looks like too well and I suspect overfitting to a points, which are suitable for estimation (small) homography, not general-purpose points.\n  \n  5) Could you please explain in more details, how did you do homography estimation precision benchmark? Specifically, was Lowe`s second nearest neighbor ratio used for filtering out wrong matches? If not, could you please repeat this experiments with it, at least for SIFT matches?\n  \n  \nSmall comments:\n    - list of contributions in abstract is inconsistent with 3rd paragraph in Introduction, which also lists contributions.\n    \n\n***\nOverall I like the work, but there are unclear moments to me. \n\n\n****\nAfter rebuttal comments. While this paper may appear not \"sexy\" I think it is quite valuable for the local features learning community: both for the main contributions, and small details and tricks evaluated inside. \nI am happy to increase my score to strong accept.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1953/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1953/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Outlier Rejection for Self-Supervised Keypoint Learning", "authors": ["Jiexiong Tang", "Hanme Kim", "Vitor Guizilini", "Sudeep Pillai", "Rares Ambrus"], "authorids": ["jiexiong@kth.se", "hanme.kim@tri.global", "vitor.guizilini@tri.global", "sudeep.pillai@tri.global", "rares.ambrus@tri.global"], "keywords": ["Self-Supervised Learning", "Keypoint Detection", "Outlier Rejection", "Deep Learning"], "TL;DR": "Learning to extract distinguishable keypoints from a proxy task, outlier rejection.", "abstract": "Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks.  However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.", "pdf": "/pdf/7e852806d1e285ccb14c305768f8d9ae40bfb2a8.pdf", "paperhash": "tang|neural_outlier_rejection_for_selfsupervised_keypoint_learning", "code": "https://github.com/TRI-ML/KP2D", "_bibtex": "@inproceedings{\ntang2020neural,\ntitle={Neural Outlier Rejection for Self-Supervised Keypoint Learning},\nauthor={Jiexiong Tang and Rares Ambrus and Vitor Guizilini and Hanme Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx82ySYPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cbedd0a3f9c84eacc4049cbcc3aa8e086d9527.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skx82ySYPH", "replyto": "Skx82ySYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1953/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1953/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575778255801, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1953/Reviewers"], "noninvitees": [], "tcdate": 1570237729896, "tmdate": 1575778255815, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1953/-/Official_Review"}}}, {"id": "Bkes8ku3sS", "original": null, "number": 9, "cdate": 1573842770946, "ddate": null, "tcdate": 1573842770946, "tmdate": 1573842770946, "tddate": null, "forum": "Skx82ySYPH", "replyto": "SJg5VTHnsH", "invitation": "ICLR.cc/2020/Conference/Paper1953/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We have updated it in the paper. Thank you for pointing this out."}, "signatures": ["ICLR.cc/2020/Conference/Paper1953/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Outlier Rejection for Self-Supervised Keypoint Learning", "authors": ["Jiexiong Tang", "Hanme Kim", "Vitor Guizilini", "Sudeep Pillai", "Rares Ambrus"], "authorids": ["jiexiong@kth.se", "hanme.kim@tri.global", "vitor.guizilini@tri.global", "sudeep.pillai@tri.global", "rares.ambrus@tri.global"], "keywords": ["Self-Supervised Learning", "Keypoint Detection", "Outlier Rejection", "Deep Learning"], "TL;DR": "Learning to extract distinguishable keypoints from a proxy task, outlier rejection.", "abstract": "Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks.  However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.", "pdf": "/pdf/7e852806d1e285ccb14c305768f8d9ae40bfb2a8.pdf", "paperhash": "tang|neural_outlier_rejection_for_selfsupervised_keypoint_learning", "code": "https://github.com/TRI-ML/KP2D", "_bibtex": "@inproceedings{\ntang2020neural,\ntitle={Neural Outlier Rejection for Self-Supervised Keypoint Learning},\nauthor={Jiexiong Tang and Rares Ambrus and Vitor Guizilini and Hanme Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx82ySYPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cbedd0a3f9c84eacc4049cbcc3aa8e086d9527.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skx82ySYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference/Paper1953/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1953/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1953/Reviewers", "ICLR.cc/2020/Conference/Paper1953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1953/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1953/Authors|ICLR.cc/2020/Conference/Paper1953/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148464, "tmdate": 1576860555616, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference/Paper1953/Reviewers", "ICLR.cc/2020/Conference/Paper1953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1953/-/Official_Comment"}}}, {"id": "BkluihCpYr", "original": null, "number": 2, "cdate": 1571839136199, "ddate": null, "tcdate": 1571839136199, "tmdate": 1573834087639, "tddate": null, "forum": "Skx82ySYPH", "replyto": "Skx82ySYPH", "invitation": "ICLR.cc/2020/Conference/Paper1953/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "#\u00a0UPDATE following rebuttal\n\nScore increased to 6 due to architecture details in supplemental.\n\n\n# Contributions\n\nThe paper contributes a self-supervised method of jointly learning 2D keypoint locations, descriptors, and scores given an input RGB image. The paper builds on previous work, adding:\n\n* A more expressive keypoint location regression, which allows each 8x8 pixel region to vote for a keypoint location outside its boundary\n\n* An upsampling step, similar to a U-net, to allow descriptors to be regressed with more detailed information\n\n* An additional proxy task for the total loss, based on outlier rejection.\n\nThe authors train on COCO by manually distorting images to generate pairs with known homography, and show competitive results for keypoint detection and homography estimation tasks.\n\nDecision: Weak reject. I would give this a 5 if the website allowed me to. A more detailed explanation of the neural network architecture, along with minor fixes described below, would make me increase my rating.\n\n\nI feel the additions to existing pipelines are well motivated but insufficiently explained. In particular, the explanation of the neural network architecture along with figures 1 and 2 leaves many details unclear to me. Phrases like \"a 1D CNN ... with 4 default setting residual blocks\" is to me insufficient - residual networks have many details such as Resnet V1 or V2 style (ie is there a path right through the network which doesn't hit any activation functions), what kind of normalization is applied, number of channels in each block, how to do skips between different spatial resolutions, etc. The upsampling step for the descriptor head, which is claimed as a novel contribution, is not fully explained - \"fast upsampling\" implies (correctly) there are many variants of upsampling with different tradeoffs, but from the text I am unsure whether this is nearest neighbour upsampling, a ConvTranspose, etc. Similarly, \"VGG style block\" leaves some details unclear - whether the resolution downsampling is with a strided convolution / pooling / etc. Lots of the details are implied to be in other previous papers, but I feel that the paper would be hugely improved by exact architectural details.\n\nThere are various minor notational discrepancies in the paper - for example the outlier rejection is various defined as \"InlierOuterNet (IONet)\" and \"The Inlier-Outlier model \\emph{IO}\", which also seems to be the same as the function $C$ defined a paragraph above. Perhaps it is common in this part of the literature, but to me an encoder decoder network is more likely to either be an autoencoder, or for the decoder to output something in the same modality (eg in machine translation). To say that some VGG blocks are an encoder, and the heads which produce keypoint locations / score / descriptor is a decoder, implies all neural networks could be described as an encoder/decoder.\n\nThe two figures showing the architecture are very different in design, which is not in itself a problem but the relationship between them could be clearer. I feel that the 'matching' box in figure 1 is misleading because it implies that matching only happens for the IONet, but the loss function for location described in Eq 1 also requires matching keypoints between the image pair. I'm also unclear on the division between direct and indirect supervisory signal - all the 4 loss components have a clear purpose, but it's not obvious what this partitioning means. \"Indirect\" only appears in this figure and the caption - perhaps.\n\nThe term \"Anchor\" appears only once with no reference, below equation 3 - I appreciate this is an existing term in this subfield, but given that the start of section 3 goes as far as explicitly defining what it means to produce 2D keypoints for an image, I feel defining this term would make the descriptor loss much clearer. \n\nOne of the main contributions, that of allowing locations to regress outside their 8x8 area, sounds like a good idea but I feel that Figure 3 does not adequately show the benefit. In both a) and b), the blue estimates appear to be roughly as good as each other - clearly from the ablation a large benefit is gained from this innovation but perhaps a better illustrative example could be made here?\n\nOn a more positive note, I feel the components of the loss function are in general very clearly motivated and defined, and the description of training & data augmentation hyperparameters appears complete. If the description of the architecture could be improved that would result in a paper very amenable to reproduction.\n\nThe experiments are well explained, and the ablation of the various proposed  components is good. I feel table 1 would be improved with error bars - given that the bold best score is not exclusively next to V4, but in many cases the difference between V4 and the best is ~1%, error bars from different training runs might make clearer that V4 is overall the best configuration.\n\nIn the conclusion - \"even without an explicit loss\" - what is the difference between the loss functions used in this work, and an explicit loss?\n\n\nMinor corrections:\n\nThe euclidean distance between descriptors is various notated as $d$ (section 3), $x$ (above equation 5) and $d$ again (below equation 5).\n\nTypos: \"normalzation\" -> \"normalization\", \"funcion\" -> \"function\", \"tripled\" -> \"triplet\".\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1953/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1953/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Outlier Rejection for Self-Supervised Keypoint Learning", "authors": ["Jiexiong Tang", "Hanme Kim", "Vitor Guizilini", "Sudeep Pillai", "Rares Ambrus"], "authorids": ["jiexiong@kth.se", "hanme.kim@tri.global", "vitor.guizilini@tri.global", "sudeep.pillai@tri.global", "rares.ambrus@tri.global"], "keywords": ["Self-Supervised Learning", "Keypoint Detection", "Outlier Rejection", "Deep Learning"], "TL;DR": "Learning to extract distinguishable keypoints from a proxy task, outlier rejection.", "abstract": "Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks.  However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.", "pdf": "/pdf/7e852806d1e285ccb14c305768f8d9ae40bfb2a8.pdf", "paperhash": "tang|neural_outlier_rejection_for_selfsupervised_keypoint_learning", "code": "https://github.com/TRI-ML/KP2D", "_bibtex": "@inproceedings{\ntang2020neural,\ntitle={Neural Outlier Rejection for Self-Supervised Keypoint Learning},\nauthor={Jiexiong Tang and Rares Ambrus and Vitor Guizilini and Hanme Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx82ySYPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cbedd0a3f9c84eacc4049cbcc3aa8e086d9527.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Skx82ySYPH", "replyto": "Skx82ySYPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1953/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1953/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575778255801, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1953/Reviewers"], "noninvitees": [], "tcdate": 1570237729896, "tmdate": 1575778255815, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1953/-/Official_Review"}}}, {"id": "SJg5VTHnsH", "original": null, "number": 8, "cdate": 1573834033715, "ddate": null, "tcdate": 1573834033715, "tmdate": 1573834033715, "tddate": null, "forum": "Skx82ySYPH", "replyto": "Bylh6imcsr", "invitation": "ICLR.cc/2020/Conference/Paper1953/-/Official_Comment", "content": {"title": "Response to rebuttal", "comment": "I thank the authors for their detailed response and paper revision. The new version of figure 3 and architecture details in Tables 6 and 7 in particular are good, and address most of my concerns. I am raising my score to a 6.\n\nCorrection in Table 6 - \"Tan Harmonic\" -> \"Tan Hyperbolic\" presumably?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1953/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1953/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Outlier Rejection for Self-Supervised Keypoint Learning", "authors": ["Jiexiong Tang", "Hanme Kim", "Vitor Guizilini", "Sudeep Pillai", "Rares Ambrus"], "authorids": ["jiexiong@kth.se", "hanme.kim@tri.global", "vitor.guizilini@tri.global", "sudeep.pillai@tri.global", "rares.ambrus@tri.global"], "keywords": ["Self-Supervised Learning", "Keypoint Detection", "Outlier Rejection", "Deep Learning"], "TL;DR": "Learning to extract distinguishable keypoints from a proxy task, outlier rejection.", "abstract": "Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks.  However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.", "pdf": "/pdf/7e852806d1e285ccb14c305768f8d9ae40bfb2a8.pdf", "paperhash": "tang|neural_outlier_rejection_for_selfsupervised_keypoint_learning", "code": "https://github.com/TRI-ML/KP2D", "_bibtex": "@inproceedings{\ntang2020neural,\ntitle={Neural Outlier Rejection for Self-Supervised Keypoint Learning},\nauthor={Jiexiong Tang and Rares Ambrus and Vitor Guizilini and Hanme Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx82ySYPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cbedd0a3f9c84eacc4049cbcc3aa8e086d9527.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skx82ySYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference/Paper1953/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1953/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1953/Reviewers", "ICLR.cc/2020/Conference/Paper1953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1953/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1953/Authors|ICLR.cc/2020/Conference/Paper1953/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148464, "tmdate": 1576860555616, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference/Paper1953/Reviewers", "ICLR.cc/2020/Conference/Paper1953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1953/-/Official_Comment"}}}, {"id": "rJgDnhXcjH", "original": null, "number": 7, "cdate": 1573694639359, "ddate": null, "tcdate": 1573694639359, "tmdate": 1573694639359, "tddate": null, "forum": "Skx82ySYPH", "replyto": "rklyawTEYS", "invitation": "ICLR.cc/2020/Conference/Paper1953/-/Official_Comment", "content": {"title": "Response to Reviewer #3 (part 1/2)", "comment": "R3: The experiments are performed on HSequences dataset (wrongly called \"HPatches\", as HPatches dataset is literally image patches, not full images), showing noticable improvement over the state of the art.\n\nWe thank the reviewer for this clarification. We updated the text to emphasize the fact that we are evaluating on image sequences from the HPatches dataset. \n\nR3: Regarding descriptor interpolation, which is claimed as contribution. It is not clear for me, how different it is compared to SuperPoint one, which also do descriptor upsampling, so that network output is H x W x [256], i.e. full resolution. Could you please clarify the differences to it? \n\nBoth our method and SuperPoint perform interpolation to the original image resolution H x W x [256]. The key difference is that our network includes a learneable upsampling step before the interpolation, which goes from H/8 x W/8 x [256] to H/4 x W/4 x [256] based on the work of [1]. \n\nR3: Also, in Figure 2 it is not clear, how one can do \"feature concatenation\" for blocks with different spatial resolution.\n\nWe thank the reviewer for pointing out this mistake, we have fixed it and now the feature concatenation process should be correctly illustrated in Figure 2. Diagrams detailing our two networks (KeyPointNet and IO-Net)  were also added to the Appendix.\n\nR3: Why the IONet is used only for training?  Wouldn\u2019t it better to actually learn everything end-to-end, which is already done in paper and evaluate? \n\nThe reasons why we only use IO-Net for training are two-fold:\n(1) Our major contribution in this paper is showing that KeyPointNet can learn from a proxy task (IO-Net) to either train the descriptor directly or to improve it (when trained with the descriptor loss). We focus on improving the actual performance of KeyPointNet at training time, rather than using another network to improve the final matching during inference. Moreover, for a fair comparison with other methods we aimed to keep the homography component estimation the same (i.e. using OpenCV and RANSAC), thus showing that our superior results are due to our improved keypoints and descriptors.\n(2) Inspired by Negative Sample mining, which is commonly deployed for metric learning, we perform a similar strategy by feeding the lowest-k score (non-border) keypoints to the IO-Net during training. We found that using the top-k score keypoints performs worse. However, at test time, it is not meaningful to take the keypoints with lowest score to estimate the homography, hence we do not use IO-Net for the evaluation. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1953/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Outlier Rejection for Self-Supervised Keypoint Learning", "authors": ["Jiexiong Tang", "Hanme Kim", "Vitor Guizilini", "Sudeep Pillai", "Rares Ambrus"], "authorids": ["jiexiong@kth.se", "hanme.kim@tri.global", "vitor.guizilini@tri.global", "sudeep.pillai@tri.global", "rares.ambrus@tri.global"], "keywords": ["Self-Supervised Learning", "Keypoint Detection", "Outlier Rejection", "Deep Learning"], "TL;DR": "Learning to extract distinguishable keypoints from a proxy task, outlier rejection.", "abstract": "Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks.  However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.", "pdf": "/pdf/7e852806d1e285ccb14c305768f8d9ae40bfb2a8.pdf", "paperhash": "tang|neural_outlier_rejection_for_selfsupervised_keypoint_learning", "code": "https://github.com/TRI-ML/KP2D", "_bibtex": "@inproceedings{\ntang2020neural,\ntitle={Neural Outlier Rejection for Self-Supervised Keypoint Learning},\nauthor={Jiexiong Tang and Rares Ambrus and Vitor Guizilini and Hanme Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx82ySYPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cbedd0a3f9c84eacc4049cbcc3aa8e086d9527.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skx82ySYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference/Paper1953/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1953/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1953/Reviewers", "ICLR.cc/2020/Conference/Paper1953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1953/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1953/Authors|ICLR.cc/2020/Conference/Paper1953/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148464, "tmdate": 1576860555616, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference/Paper1953/Reviewers", "ICLR.cc/2020/Conference/Paper1953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1953/-/Official_Comment"}}}, {"id": "Hkx7DhXqoH", "original": null, "number": 6, "cdate": 1573694555216, "ddate": null, "tcdate": 1573694555216, "tmdate": 1573694555216, "tddate": null, "forum": "Skx82ySYPH", "replyto": "rklyawTEYS", "invitation": "ICLR.cc/2020/Conference/Paper1953/-/Official_Comment", "content": {"title": "Response to Reviewer #3 (part 2/2)", "comment": "R3: How is association in training (e.g. on Fig.3) done, if multiple cells in img2 returns keypoint close to the same keypoint in img1? \n\nWe have updated the caption in Figure 3 to properly explain the differences between each scenario. In particular, the UnsuperPoint method in (a) forces keypoint predictions to be in the same cell. Our method in (b) predicts locations from the cell-center and allow keypoints to  cross cell borders, which promotes better matching and aggregation. This implies that multiple keypoints from one image (e.g. blue keypoints) may have the same corresponding keypoint in the second image (e.g. red keypoints) which is the desirable behavior we aim for. \n\nR3: HSequences consists of two subsets: Illumination and Viewpoint. Could you please report results per subset instead of per whole dataset? Could you also please specifically report results for the following image sequences: graffiti, bark, boat, especially for 1-6 pairs and visualize matches (same way as in Figure 4-6)?\n\nWe thank the reviewer for this comment - it allows us to further explore the performance of our method. We added Table 4 in the appendix, where we evaluate the performance of our method on the two subsets of HPatches. We mention that each entry reports the mean and standard deviation across 10 runs with varying RANSAC seeds, to also evaluate any randomness induced during the evaluation. We notice better homography performance on the illumination subset than on the viewpoint subset. This is to be expected as the viewpoint subset contains image pairs with extreme rotation which are problematic for our method which is fully convolutional. \nWe evaluate the Graffiti, Bark and Boat sequences of the HPatches dataset, and report the results of our method as well as SIFT and ORB in Table 5 in the appendix, again reporting averaged results over 10 runs. We note that our method consistently outperforms ORB. Our method performs worse than SIFT (which is more robust to extreme rotations) on the bark and boat sequences, but we obtain better results on the graffiti sequence. \nFinally, we added Figures 5, 6 and 7 to qualitatively show our matches on pairs of images from the graffiti, bark and boat sequences. Specifically, the top row in each figure shows our results, while the bottom row shows SIFT. The left subfigure on each row shows images (1,2) of each sequence, while the right subfigure shows images (1,6). We note that on images (1,2) our results are comparable to SIFT, while on images (1,6) we get fewer matches. Despite the extreme perspective change, we report that our method is able to successfully match features on images (1,6) of the boat sequence. \n\nR3: Could you please explain in more details, how did you do homography estimation precision benchmark? Specifically, was Lowe`s second nearest neighbor ratio used for filtering out wrong matches? If not, could you please repeat this experiments with it, at least for SIFT matches?\n\nTo estimate the homography, we performed reciprocal descriptor matching, and we only kept matches that have each other as nearest neighbors, e.g. if the nearest neighbor of a descriptor X from image A is descriptor Y in image B, then the nearest neighbor of descriptor Y is descriptor X in image A. We did not use Lowe\u2019s second nearest neighbor ratio which we found to yield worse results. To compute the homography, we used OpenCV\u2019s findHomography method with RANSAC, error threshold 3 and a maximum of 5000 iterations. \nPlease see below the results of our evaluation of SIFT on 320x240 and 640x480 on HPatches with and without Lowe\u2019s second nearest neighbor ratio: \n\n320x240: \nMethod                 |    Repeat.  |    Loc.   |  Cor-1  | Cor-3 |  Cor-5 |   M.Score\nSIFT+ 0.7 ratio     |      0.474   |  0.993   |  0.497   | 0.752 | 0.798  |   0.287\nSIFT+ 0.8 ratio     |      0.474   |  0.993   |  0.529   | 0.788 | 0.852  |   0.287\nSIFT+ 0.9 ratio     |      0.474   |  0.993   |  0.564   | 0.836 | 0.879  |   0.287\nSIFT + reciprocal |      0.474    |  0.993   |  0.605  | 0.859 | 0.895  |    0.287\n\n640x480: \nMethod                 |    Repeat.  |    Loc.   |  Cor-1  | Cor-3 |  Cor-5 |   M.Score\nSIFT+ 0.7 ratio     |     0.498    |  1.062   |  0.507   | 0.790 | 0.862  |   0.290\nSIFT+ 0.8 ratio     |     0.498    |  1.062   |  0.548   | 0.826 | 0.905  |   0.290\nSIFT+ 0.9 ratio     |     0.498    |  1.062   |  0.571   | 0.862 | 0.910  |   0.290\nSIFT + reciprocal |      0.498    |  1.062   |  0.584  | 0.864 | 0.917  |    0.290\n\nR3: List of contributions in abstract is inconsistent with 3rd paragraph in Introduction, which also lists contributions.\nWe appreciate that the reviewer identified this inconsistency. We have updated the abstract and introduction to clarify the contributions of the paper in a consistent manner.\n\n[1] Shi, Wenzhe, et al. \"Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network.\" Proceedings of the IEEE CVPR. 2016."}, "signatures": ["ICLR.cc/2020/Conference/Paper1953/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Outlier Rejection for Self-Supervised Keypoint Learning", "authors": ["Jiexiong Tang", "Hanme Kim", "Vitor Guizilini", "Sudeep Pillai", "Rares Ambrus"], "authorids": ["jiexiong@kth.se", "hanme.kim@tri.global", "vitor.guizilini@tri.global", "sudeep.pillai@tri.global", "rares.ambrus@tri.global"], "keywords": ["Self-Supervised Learning", "Keypoint Detection", "Outlier Rejection", "Deep Learning"], "TL;DR": "Learning to extract distinguishable keypoints from a proxy task, outlier rejection.", "abstract": "Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks.  However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.", "pdf": "/pdf/7e852806d1e285ccb14c305768f8d9ae40bfb2a8.pdf", "paperhash": "tang|neural_outlier_rejection_for_selfsupervised_keypoint_learning", "code": "https://github.com/TRI-ML/KP2D", "_bibtex": "@inproceedings{\ntang2020neural,\ntitle={Neural Outlier Rejection for Self-Supervised Keypoint Learning},\nauthor={Jiexiong Tang and Rares Ambrus and Vitor Guizilini and Hanme Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx82ySYPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cbedd0a3f9c84eacc4049cbcc3aa8e086d9527.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skx82ySYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference/Paper1953/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1953/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1953/Reviewers", "ICLR.cc/2020/Conference/Paper1953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1953/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1953/Authors|ICLR.cc/2020/Conference/Paper1953/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148464, "tmdate": 1576860555616, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference/Paper1953/Reviewers", "ICLR.cc/2020/Conference/Paper1953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1953/-/Official_Comment"}}}, {"id": "Bylh6imcsr", "original": null, "number": 5, "cdate": 1573694404377, "ddate": null, "tcdate": 1573694404377, "tmdate": 1573694404377, "tddate": null, "forum": "Skx82ySYPH", "replyto": "BkluihCpYr", "invitation": "ICLR.cc/2020/Conference/Paper1953/-/Official_Comment", "content": {"title": "Response to Reviewer #1  (part 1/2)", "comment": "R1: I feel the additions to existing pipelines are well motivated but insufficiently explained. In particular, the explanation of the neural network architecture along with figures 1 and 2 leaves many details unclear to me. Phrases like \"a 1D CNN ... with 4 default setting residual blocks\" is to me insufficient - residual networks have many details such as Resnet V1 or V2 style (ie is there a path right through the network which doesn't hit any activation functions), what kind of normalization is applied, number of channels in each block, how to do skips between different spatial resolutions, etc. \n\nWe agree that many details involved in the description and implementation of our networks have been left out of the original paper because of the page limitation. To address this, we have added these details in the Appendix diagrams describing both KeyPointNet and IO-Net with sufficient network architecture description. (Section D, Tables 6 and 7).\n\nR1: There are various minor notational discrepancies in the paper - for example the outlier rejection is various defined as \"InlierOuterNet (IONet)\" and \"The Inlier-Outlier model \\emph{IO}\", which also seems to be the same as the function  defined a paragraph above. Perhaps it is common in this part of the literature, but to me an encoder decoder network is more likely to either be an autoencoder, or for the decoder to output something in the same modality (eg in machine translation). To say that some VGG blocks are an encoder, and the heads which produce keypoint locations / score / descriptor is a decoder, implies all neural networks could be described as an encoder/decoder.\n\nThank you for pointing out the notational discrepancy. We have focused on unifying the terminology in the revised paper, and we refer to the outlier rejection network as IO-Net. Regarding the encoder-decoder terminology, we aimed to be consistent with the terminology used in SuperPoint [1], where the detector and descriptor heads are referred to as \u201cdecoders\u201d using the same shared \u201cencoded\u201d features as input.\n\nR1: The two figures showing the architecture are very different in design, which is not in itself a problem but the relationship between them could be clearer. I feel that the 'matching' box in figure 1 is misleading because it implies that matching only happens for the IONet, but the loss function for location described in Eq 1 also requires matching keypoints between the image pair. I'm also unclear on the division between direct and indirect supervisory signal - all the 4 loss components have a clear purpose, but it's not obvious what this partitioning means. \"Indirect\" only appears in this figure and the caption - perhaps.\n\nWe thank the reviewer for pointing this out - we have updated the caption of Figures 1 and 2 to better explain the relationship between them as well as the combination of the explicit loss applied directly on the KeyPointNet outputs (score, location and descriptor) and indirect loss derived from IO-Net via the outlier rejection classification task. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1953/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Outlier Rejection for Self-Supervised Keypoint Learning", "authors": ["Jiexiong Tang", "Hanme Kim", "Vitor Guizilini", "Sudeep Pillai", "Rares Ambrus"], "authorids": ["jiexiong@kth.se", "hanme.kim@tri.global", "vitor.guizilini@tri.global", "sudeep.pillai@tri.global", "rares.ambrus@tri.global"], "keywords": ["Self-Supervised Learning", "Keypoint Detection", "Outlier Rejection", "Deep Learning"], "TL;DR": "Learning to extract distinguishable keypoints from a proxy task, outlier rejection.", "abstract": "Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks.  However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.", "pdf": "/pdf/7e852806d1e285ccb14c305768f8d9ae40bfb2a8.pdf", "paperhash": "tang|neural_outlier_rejection_for_selfsupervised_keypoint_learning", "code": "https://github.com/TRI-ML/KP2D", "_bibtex": "@inproceedings{\ntang2020neural,\ntitle={Neural Outlier Rejection for Self-Supervised Keypoint Learning},\nauthor={Jiexiong Tang and Rares Ambrus and Vitor Guizilini and Hanme Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx82ySYPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cbedd0a3f9c84eacc4049cbcc3aa8e086d9527.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skx82ySYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference/Paper1953/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1953/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1953/Reviewers", "ICLR.cc/2020/Conference/Paper1953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1953/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1953/Authors|ICLR.cc/2020/Conference/Paper1953/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148464, "tmdate": 1576860555616, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference/Paper1953/Reviewers", "ICLR.cc/2020/Conference/Paper1953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1953/-/Official_Comment"}}}, {"id": "S1gX9jm5sr", "original": null, "number": 4, "cdate": 1573694347271, "ddate": null, "tcdate": 1573694347271, "tmdate": 1573694347271, "tddate": null, "forum": "Skx82ySYPH", "replyto": "BkluihCpYr", "invitation": "ICLR.cc/2020/Conference/Paper1953/-/Official_Comment", "content": {"title": "Response to Reviewer #1  (part 2/2)", "comment": "\nR1: The term \"Anchor\" appears only once with no reference, below equation 3 - I appreciate this is an existing term in this subfield, but given that the start of section 3 goes as far as explicitly defining what it means to produce 2D keypoints for an image, I feel defining this term would make the descriptor loss much clearer. \nWe agree that this term should be explained, and have added in Section 3.3 a proper description of what an anchor is within the context of the paper.\n\nR1: One of the main contributions, that of allowing locations to regress outside their 8x8 area, sounds like a good idea but I feel that Figure 3 does not adequately show the benefit. In both a) and b), the blue estimates appear to be roughly as good as each other - clearly from the ablation a large benefit is gained from this innovation but perhaps a better illustrative example could be made here?\nWe have updated the caption in Figure 3 to properly explain the differences between each scenario. In particular, the UnsuperPoint method in (a) forces keypoint predictions to be in the same cell. While our method predicts localization from the cell-center, so keypoints can be cross-border, which promotes better matching and aggregation. In addition, we have also updated the figure to better illustrate the behavior of both methods.\n\nR1: On a more positive note, I feel the components of the loss function are in general very clearly motivated and defined, and the description of training & data augmentation hyperparameters appears complete. If the description of the architecture could be improved that would result in a paper very amenable to reproduction.\nWe thank the reviewer for the positive feedback. We have added in the appendix diagrams with implementation details for our networks, to facilitate reproduction (Section D, Tables 6 and 7). We also plan to open-source our code upon publication.\n\nR1: The experiments are well explained, and the ablation of the various proposed  components is good. I feel table 1 would be improved with error bars - given that the bold best score is not exclusively next to V4, but in many cases the difference between V4 and the best is ~1%, error bars from different training runs might make clearer that V4 is overall the best configuration.\nUnfortunately due to time constraints we were not able to re-train all the models to generate the error bars. However, we have evaluated the best model variant (V4) with different seed configurations to capture the variance induced in the sampling and consensus, and report the results of this experiment in the Appendix in Table 4, where we perform 10 evaluation runs with different random seeds and report the mean and standard deviation.\n\n\nR1: In the conclusion - \"even without an explicit loss\" - what is the difference between the loss functions used in this work, and an explicit loss?\nWe refer to an explicit loss as one that is defined over each of the 3 target outputs (score, location, descriptor) as in Equations 1, 3, and 4. The inlier-outlier loss (Equation 5) however, does not penalize the KeyPointNet outputs directly but acts as an indirect supervisory signal that is able to generate distinguishable keypoint descriptors during matching.\n\n[1] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interestpoint detection and description. InProceedings of the IEEE Conference on Computer Vision andPattern Recognition Workshops, pp. 224\u2013236, 2018b.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1953/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Outlier Rejection for Self-Supervised Keypoint Learning", "authors": ["Jiexiong Tang", "Hanme Kim", "Vitor Guizilini", "Sudeep Pillai", "Rares Ambrus"], "authorids": ["jiexiong@kth.se", "hanme.kim@tri.global", "vitor.guizilini@tri.global", "sudeep.pillai@tri.global", "rares.ambrus@tri.global"], "keywords": ["Self-Supervised Learning", "Keypoint Detection", "Outlier Rejection", "Deep Learning"], "TL;DR": "Learning to extract distinguishable keypoints from a proxy task, outlier rejection.", "abstract": "Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks.  However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.", "pdf": "/pdf/7e852806d1e285ccb14c305768f8d9ae40bfb2a8.pdf", "paperhash": "tang|neural_outlier_rejection_for_selfsupervised_keypoint_learning", "code": "https://github.com/TRI-ML/KP2D", "_bibtex": "@inproceedings{\ntang2020neural,\ntitle={Neural Outlier Rejection for Self-Supervised Keypoint Learning},\nauthor={Jiexiong Tang and Rares Ambrus and Vitor Guizilini and Hanme Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx82ySYPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cbedd0a3f9c84eacc4049cbcc3aa8e086d9527.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skx82ySYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference/Paper1953/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1953/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1953/Reviewers", "ICLR.cc/2020/Conference/Paper1953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1953/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1953/Authors|ICLR.cc/2020/Conference/Paper1953/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148464, "tmdate": 1576860555616, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference/Paper1953/Reviewers", "ICLR.cc/2020/Conference/Paper1953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1953/-/Official_Comment"}}}, {"id": "BygUyjQ5iH", "original": null, "number": 3, "cdate": 1573694173822, "ddate": null, "tcdate": 1573694173822, "tmdate": 1573694173822, "tddate": null, "forum": "Skx82ySYPH", "replyto": "HJeKkwvk5H", "invitation": "ICLR.cc/2020/Conference/Paper1953/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "R2: The description of the evaluation procedure was a bit vague. Is RANSAC being used to find correspondences? If so, perhaps error bars are necessary to account for variance across multiple runs?\n\nWe thank the reviewer for this suggestion. To address this we added more details about how we compute the homography in the Appendix. Specifically, to estimate the homography, we performed reciprocal descriptor matching and we used OpenCV\u2019s findHomography method with RANSAC, error threshold 3 and a maximum of 5000 iterations. \n\nTo capture the variance induced by the RANSAC component during evaluation, we added Table 4 in the Appendix, where we perform 10 evaluation runs with different random seeds and report the mean and standard deviation.\n\nR2: Make it more clear in the related works about how the proposed method relates to Unsuperpoint. My understanding is that the proposed work is a somewhat incremental improvement over Unsuperpoint.\n\nWe thank the reviewer for pointing this out. We updated sections pertaining to UnsuperPoint in the related work, and emphasized how our work differs from previous work. We emphasized in the paper that our main contribution is IO-Net, a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that we are able to simultaneously self-supervise keypoint description and improve keypoint matching. \nIn addition, with respect to the UnsuperPoint keypoint-network architecture, we propose two modifications that make our approach especially amenable to robust and fine-grained keypoint detection and description. We modify the keypoint location head to detect keypoints outside their corresponding cell-boundaries, allowing for improved keypoint matching especially at cell-boundaries. We also further improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. We have updated the text to clarify these details further.\n\n\nR2: Section 3.3 (Score learning) was a bit difficult to follow. I find it better to start by stating the high level goal of the loss function before going into the formulation.\n\nWe have updated Section 3.3 to include a high-level description of the loss function, before moving on to the formulation. Our main goal is two-fold: (i) we want to ensure that feature-pairs have consistent scores, and (ii) the network should learn that good keypoints are the ones with low feature point distance.\n\nR2: Captions for Tables 2 and 3 are lacking. At the very least, mention what the numbers being compared are.\n\nWe have updated these captions with clear explanations of what is being discussed in each table. More specifically, in Table 2 we show that our proposed method outperforms all other listed traditional and learned feature methods in repeatability, and for localization we outperform UnsuperPoint for higher resolution images. In Table 3, we show that our proposed method outperforms all listed methods for different pixel distance thresholds, except for SIFT in one single instance (the Correctness-1 metric).\n\nR2: Overall, I think the improvements are a bit incremental, but the experiments seem to support the claim that they are beneficial. I had some concerns about the clarity of the paper, and would be willing to raise my rating if addressed.\n\nWe thank the reviewer for the feedback, and hope that the improvements to the paper's clarity and contributions sufficiently addresses these concerns. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1953/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Outlier Rejection for Self-Supervised Keypoint Learning", "authors": ["Jiexiong Tang", "Hanme Kim", "Vitor Guizilini", "Sudeep Pillai", "Rares Ambrus"], "authorids": ["jiexiong@kth.se", "hanme.kim@tri.global", "vitor.guizilini@tri.global", "sudeep.pillai@tri.global", "rares.ambrus@tri.global"], "keywords": ["Self-Supervised Learning", "Keypoint Detection", "Outlier Rejection", "Deep Learning"], "TL;DR": "Learning to extract distinguishable keypoints from a proxy task, outlier rejection.", "abstract": "Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks.  However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.", "pdf": "/pdf/7e852806d1e285ccb14c305768f8d9ae40bfb2a8.pdf", "paperhash": "tang|neural_outlier_rejection_for_selfsupervised_keypoint_learning", "code": "https://github.com/TRI-ML/KP2D", "_bibtex": "@inproceedings{\ntang2020neural,\ntitle={Neural Outlier Rejection for Self-Supervised Keypoint Learning},\nauthor={Jiexiong Tang and Rares Ambrus and Vitor Guizilini and Hanme Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx82ySYPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cbedd0a3f9c84eacc4049cbcc3aa8e086d9527.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skx82ySYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference/Paper1953/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1953/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1953/Reviewers", "ICLR.cc/2020/Conference/Paper1953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1953/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1953/Authors|ICLR.cc/2020/Conference/Paper1953/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148464, "tmdate": 1576860555616, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference/Paper1953/Reviewers", "ICLR.cc/2020/Conference/Paper1953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1953/-/Official_Comment"}}}, {"id": "HklcBcmcsr", "original": null, "number": 2, "cdate": 1573694017528, "ddate": null, "tcdate": 1573694017528, "tmdate": 1573694017528, "tddate": null, "forum": "Skx82ySYPH", "replyto": "Skx82ySYPH", "invitation": "ICLR.cc/2020/Conference/Paper1953/-/Official_Comment", "content": {"title": "Response to Reviewers", "comment": "First of all, we would like to thank all the reviewers for their feedback and useful suggestions, and we are glad to see the recognition of the novelty of our work, relevance, and state-of-the-art results. As the reviewers have pointed out, we acknowledge that our paper would certainly benefit from a clearer and more detailed technical presentation. To this end, we have updated the manuscript in the following ways:\n\n* Clarified the description of our contributions in the related work, particularly in regards to UnsuperPoint. \n* Added a detailed description of our neural network architecture in the Appendix.\n* Improved the figures/tables and associated captions.\n* As requested by the reviewers, we conducted new experiments to quantify (i) the standard deviation of our homography estimation (ii) additional qualitative and quantitative experiments on HPatches (viewpoint, illumination and specific sequences).\n* Fixed minor corrections and typos that were accurately pointed out by the reviewers.\n\nTo facilitate this rebuttal process, we would like to suggest referring to our updated manuscript and detailed comments on each of the reviewers\u2019 points, that were addressed individually.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1953/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Outlier Rejection for Self-Supervised Keypoint Learning", "authors": ["Jiexiong Tang", "Hanme Kim", "Vitor Guizilini", "Sudeep Pillai", "Rares Ambrus"], "authorids": ["jiexiong@kth.se", "hanme.kim@tri.global", "vitor.guizilini@tri.global", "sudeep.pillai@tri.global", "rares.ambrus@tri.global"], "keywords": ["Self-Supervised Learning", "Keypoint Detection", "Outlier Rejection", "Deep Learning"], "TL;DR": "Learning to extract distinguishable keypoints from a proxy task, outlier rejection.", "abstract": "Identifying salient points in images is a crucial component for visual odometry, Structure-from-Motion or SLAM algorithms. Recently, several learned keypoint methods have demonstrated compelling performance on challenging benchmarks.  However, generating consistent and accurate training data for interest-point detection in natural images still remains challenging, especially for human annotators. We introduce IO-Net (i.e. InlierOutlierNet), a novel proxy task for the self-supervision of keypoint detection, description and matching. By making the sampling of inlier-outlier sets from point-pair correspondences fully differentiable within the keypoint learning framework, we show that are able to simultaneously self-supervise keypoint description and improve keypoint matching. Second, we introduce KeyPointNet, a keypoint-network architecture that is especially amenable to robust keypoint detection and description. We design the network to allow local keypoint aggregation to avoid artifacts due to spatial discretizations commonly used for this task, and we improve fine-grained keypoint descriptor performance by taking advantage of efficient sub-pixel convolutions to upsample the descriptor feature-maps to a higher operating resolution. Through extensive experiments and ablative analysis, we show that the proposed self-supervised keypoint learning method greatly improves the quality of feature matching and homography estimation on challenging benchmarks over the state-of-the-art.", "pdf": "/pdf/7e852806d1e285ccb14c305768f8d9ae40bfb2a8.pdf", "paperhash": "tang|neural_outlier_rejection_for_selfsupervised_keypoint_learning", "code": "https://github.com/TRI-ML/KP2D", "_bibtex": "@inproceedings{\ntang2020neural,\ntitle={Neural Outlier Rejection for Self-Supervised Keypoint Learning},\nauthor={Jiexiong Tang and Rares Ambrus and Vitor Guizilini and Hanme Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Skx82ySYPH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/17cbedd0a3f9c84eacc4049cbcc3aa8e086d9527.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Skx82ySYPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference/Paper1953/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1953/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1953/Reviewers", "ICLR.cc/2020/Conference/Paper1953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1953/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1953/Authors|ICLR.cc/2020/Conference/Paper1953/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148464, "tmdate": 1576860555616, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1953/Authors", "ICLR.cc/2020/Conference/Paper1953/Reviewers", "ICLR.cc/2020/Conference/Paper1953/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1953/-/Official_Comment"}}}], "count": 13}