{"notes": [{"id": "rkeXrIIt_4", "original": "SkgR9VJYu4", "number": 48, "cdate": 1553716795026, "ddate": null, "tcdate": 1553716795026, "tmdate": 1562083049431, "tddate": null, "forum": "rkeXrIIt_4", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "content": {"title": "Understanding the Relation Between Maximum-Entropy Inverse Reinforcement Learning and Behaviour Cloning", "authors": ["Seyed Kamyar Seyed Ghasemipour", "Shane Gu", "Richard Zemel"], "authorids": ["kamyar@cs.toronto.edu", "shanegu@google.com", "zemel@cs.toronto.edu"], "keywords": ["Inverse Reinforcement Learning", "Behaviour Cloning", "f-divergence", "distribution matching"], "TL;DR": "Distribution matching through divergence minimization provides a common ground for comparing adversarial Maximum-Entropy Inverse Reinforcement Learning methods to Behaviour Cloning.", "abstract": "In many settings, it is desirable to learn decision-making and control policies through learning or from expert demonstrations. The most common approaches under this framework are Behaviour Cloning (BC), and Inverse Reinforcement Learning (IRL). Recent methods for IRL have demonstrated the capacity to learn effective policies with access to a very limited set of demonstrations, a scenario in which BC methods often fail. Unfortunately, directly comparing the algorithms for these methods does not provide adequate intuition for understanding this difference in performance. This is the motivating factor for our work. We begin by presenting $f$-MAX, a generalization of AIRL (Fu et al., 2018), a state-of-the-art IRL method. $f$-MAX provides grounds for more directly comparing the objectives for LfD. We demonstrate that $f$-MAX, and by inheritance AIRL, is a subset of the cost-regularized IRL framework laid out by Ho & Ermon (2016). We conclude by empirically evaluating the factors of difference between various LfD objectives in the continuous control domain.", "pdf": "/pdf/a5863b1eaf31664443499be4311dbdb08549e444.pdf", "paperhash": "ghasemipour|understanding_the_relation_between_maximumentropy_inverse_reinforcement_learning_and_behaviour_cloning"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Blind_Submission", "cdate": 1547567085825, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": [".*"]}, "writers": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1547567085825, "tmdate": 1555704438520, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"]}}, "tauthor": "OpenReview.net"}, {"id": "ByeU-VZN9V", "original": null, "number": 2, "cdate": 1555465214064, "ddate": null, "tcdate": 1555465214064, "tmdate": 1556906152757, "tddate": null, "forum": "rkeXrIIt_4", "replyto": "rkeXrIIt_4", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper48/Official_Review", "content": {"title": "Review", "review": "This paper discusses several adversarial imitation learning algorithms and connects them through f-divergence. A variant of the algorithms is proposed that minimizes forward KLD b/w the expert policy and the student policy. \n\nThe f-divergence formulations of adversarial IL in the paper are directly derived from the f-GAN work, which is thus pretty straightforward.\n\nThe two hypotheses that matching state marginals (instead of action marginals) is better and that forward KL is better than reverse KL look reasonable.\n", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper48/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper48/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the Relation Between Maximum-Entropy Inverse Reinforcement Learning and Behaviour Cloning", "authors": ["Seyed Kamyar Seyed Ghasemipour", "Shane Gu", "Richard Zemel"], "authorids": ["kamyar@cs.toronto.edu", "shanegu@google.com", "zemel@cs.toronto.edu"], "keywords": ["Inverse Reinforcement Learning", "Behaviour Cloning", "f-divergence", "distribution matching"], "TL;DR": "Distribution matching through divergence minimization provides a common ground for comparing adversarial Maximum-Entropy Inverse Reinforcement Learning methods to Behaviour Cloning.", "abstract": "In many settings, it is desirable to learn decision-making and control policies through learning or from expert demonstrations. The most common approaches under this framework are Behaviour Cloning (BC), and Inverse Reinforcement Learning (IRL). Recent methods for IRL have demonstrated the capacity to learn effective policies with access to a very limited set of demonstrations, a scenario in which BC methods often fail. Unfortunately, directly comparing the algorithms for these methods does not provide adequate intuition for understanding this difference in performance. This is the motivating factor for our work. We begin by presenting $f$-MAX, a generalization of AIRL (Fu et al., 2018), a state-of-the-art IRL method. $f$-MAX provides grounds for more directly comparing the objectives for LfD. We demonstrate that $f$-MAX, and by inheritance AIRL, is a subset of the cost-regularized IRL framework laid out by Ho & Ermon (2016). We conclude by empirically evaluating the factors of difference between various LfD objectives in the continuous control domain.", "pdf": "/pdf/a5863b1eaf31664443499be4311dbdb08549e444.pdf", "paperhash": "ghasemipour|understanding_the_relation_between_maximumentropy_inverse_reinforcement_learning_and_behaviour_cloning"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper48/Official_Review", "cdate": 1554234170451, "reply": {"forum": "rkeXrIIt_4", "replyto": "rkeXrIIt_4", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper48/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper48/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234170451, "tmdate": 1556906093799, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper48/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "Sygj1wxNq4", "original": null, "number": 1, "cdate": 1555461858873, "ddate": null, "tcdate": 1555461858873, "tmdate": 1556906152538, "tddate": null, "forum": "rkeXrIIt_4", "replyto": "rkeXrIIt_4", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper48/Official_Review", "content": {"title": "Maximum-entropy inverse RL methods and comparison with Behaviour Cloning", "review": "The paper presents a framework to unify different Maximum-entropy inverse RL methods and to compare them with Behaviour Cloning (BC). Specifically, the paper is motivated by the question of why BC performs significantly worse than Maximum-Entropy IRL in the small data regime. The authors hypothesise that the reasons are (i) that BC tries to model conditional policies while other approaches try to match also state marginals and (ii) due to the moment matching properties of BC as opposed to  mode seeking divergences that can better match expert policies to learning policies.  \n\nI believe that the paper presents an interesting unified framework based on f-divergences together with some novel modifications of previous methods. The experiments on continuous optimal control support to some extend the theoretical analysis of the paper. However, a more extensive experimental comparison is needed to draw more clear conclusions.   \n", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper48/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper48/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the Relation Between Maximum-Entropy Inverse Reinforcement Learning and Behaviour Cloning", "authors": ["Seyed Kamyar Seyed Ghasemipour", "Shane Gu", "Richard Zemel"], "authorids": ["kamyar@cs.toronto.edu", "shanegu@google.com", "zemel@cs.toronto.edu"], "keywords": ["Inverse Reinforcement Learning", "Behaviour Cloning", "f-divergence", "distribution matching"], "TL;DR": "Distribution matching through divergence minimization provides a common ground for comparing adversarial Maximum-Entropy Inverse Reinforcement Learning methods to Behaviour Cloning.", "abstract": "In many settings, it is desirable to learn decision-making and control policies through learning or from expert demonstrations. The most common approaches under this framework are Behaviour Cloning (BC), and Inverse Reinforcement Learning (IRL). Recent methods for IRL have demonstrated the capacity to learn effective policies with access to a very limited set of demonstrations, a scenario in which BC methods often fail. Unfortunately, directly comparing the algorithms for these methods does not provide adequate intuition for understanding this difference in performance. This is the motivating factor for our work. We begin by presenting $f$-MAX, a generalization of AIRL (Fu et al., 2018), a state-of-the-art IRL method. $f$-MAX provides grounds for more directly comparing the objectives for LfD. We demonstrate that $f$-MAX, and by inheritance AIRL, is a subset of the cost-regularized IRL framework laid out by Ho & Ermon (2016). We conclude by empirically evaluating the factors of difference between various LfD objectives in the continuous control domain.", "pdf": "/pdf/a5863b1eaf31664443499be4311dbdb08549e444.pdf", "paperhash": "ghasemipour|understanding_the_relation_between_maximumentropy_inverse_reinforcement_learning_and_behaviour_cloning"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper48/Official_Review", "cdate": 1554234170451, "reply": {"forum": "rkeXrIIt_4", "replyto": "rkeXrIIt_4", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper48/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/DeepGenStruct/Paper48/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1554234170451, "tmdate": 1556906093799, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Paper48/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}, {"id": "Hyg8BQuw5N", "original": null, "number": 1, "cdate": 1555690302472, "ddate": null, "tcdate": 1555690302472, "tmdate": 1556906152320, "tddate": null, "forum": "rkeXrIIt_4", "replyto": "rkeXrIIt_4", "invitation": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper48/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Understanding the Relation Between Maximum-Entropy Inverse Reinforcement Learning and Behaviour Cloning", "authors": ["Seyed Kamyar Seyed Ghasemipour", "Shane Gu", "Richard Zemel"], "authorids": ["kamyar@cs.toronto.edu", "shanegu@google.com", "zemel@cs.toronto.edu"], "keywords": ["Inverse Reinforcement Learning", "Behaviour Cloning", "f-divergence", "distribution matching"], "TL;DR": "Distribution matching through divergence minimization provides a common ground for comparing adversarial Maximum-Entropy Inverse Reinforcement Learning methods to Behaviour Cloning.", "abstract": "In many settings, it is desirable to learn decision-making and control policies through learning or from expert demonstrations. The most common approaches under this framework are Behaviour Cloning (BC), and Inverse Reinforcement Learning (IRL). Recent methods for IRL have demonstrated the capacity to learn effective policies with access to a very limited set of demonstrations, a scenario in which BC methods often fail. Unfortunately, directly comparing the algorithms for these methods does not provide adequate intuition for understanding this difference in performance. This is the motivating factor for our work. We begin by presenting $f$-MAX, a generalization of AIRL (Fu et al., 2018), a state-of-the-art IRL method. $f$-MAX provides grounds for more directly comparing the objectives for LfD. We demonstrate that $f$-MAX, and by inheritance AIRL, is a subset of the cost-regularized IRL framework laid out by Ho & Ermon (2016). We conclude by empirically evaluating the factors of difference between various LfD objectives in the continuous control domain.", "pdf": "/pdf/a5863b1eaf31664443499be4311dbdb08549e444.pdf", "paperhash": "ghasemipour|understanding_the_relation_between_maximumentropy_inverse_reinforcement_learning_and_behaviour_cloning"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/DeepGenStruct/-/Paper48/Decision", "cdate": 1554814601620, "reply": {"forum": "rkeXrIIt_4", "replyto": "rkeXrIIt_4", "readers": [".*"], "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554814601620, "tmdate": 1556906103239, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "invitees": ["ICLR.cc/2019/Workshop/DeepGenStruct/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/DeepGenStruct"], "details": {"writable": true}}}}], "count": 4}