{"notes": [{"id": "B1xmOgrFPS", "original": "Hygoo2xFvS", "number": 2392, "cdate": 1569439851138, "ddate": null, "tcdate": 1569439851138, "tmdate": 1577168259625, "tddate": null, "forum": "B1xmOgrFPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["xwwu.2015@smu.edu.sg", "dsahoo@salesforce.com", "shoi@salesforce.com"], "title": "Meta-RCNN: Meta Learning for Few-Shot Object Detection", "authors": ["Xiongwei Wu", "Doyen Sahoo", "Steven C. H. Hoi"], "pdf": "/pdf/0b83df79e8e6fc0f217140732bf46707516e9e99.pdf", "TL;DR": "We develop Meta-RCNN which learns both the object classifier and the region proposal network via meta-learning in order to do few-shot detection", "abstract": "Despite significant advances in object detection in recent years, training effective detectors in a small data regime remains an open challenge. Labelling training data for object detection is extremely expensive, and there is a need to develop techniques that can generalize well from small amounts of labelled data. We investigate this problem of few-shot object detection, where a detector has access to only limited amounts of annotated data. Based on the recently evolving meta-learning principle, we propose a novel meta-learning framework for object detection named ``Meta-RCNN\", which learns the ability to perform few-shot detection via meta-learning. Specifically, Meta-RCNN learns an object detector in an episodic learning paradigm on the (meta) training data. This learning scheme helps acquire a prior which enables Meta-RCNN to do few-shot detection on novel tasks. Built on top of the Faster RCNN model, in Meta-RCNN, both the Region Proposal Network (RPN) and the object classification branch are meta-learned. The meta-trained RPN learns to provide class-specific proposals, while the object classifier learns to do few-shot classification. The novel loss objectives and learning strategy of Meta-RCNN can be trained in an end-to-end manner. We demonstrate the effectiveness of Meta-RCNN in addressing few-shot detection on Pascal VOC dataset and achieve promising results. ", "keywords": ["Few-shot detection", "Meta-Learning", "Object Detection"], "paperhash": "wu|metarcnn_meta_learning_for_fewshot_object_detection", "original_pdf": "/attachment/8f6381f7788288d1077a1f91f017a8997f4506bc.pdf", "_bibtex": "@misc{\nwu2020metarcnn,\ntitle={Meta-{\\{}RCNN{\\}}: Meta Learning for Few-Shot Object Detection},\nauthor={Xiongwei Wu and Doyen Sahoo and Steven C. H. Hoi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xmOgrFPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "F8FXr3itAs", "original": null, "number": 1, "cdate": 1576798747963, "ddate": null, "tcdate": 1576798747963, "tmdate": 1576800888080, "tddate": null, "forum": "B1xmOgrFPS", "replyto": "B1xmOgrFPS", "invitation": "ICLR.cc/2020/Conference/Paper2392/-/Decision", "content": {"decision": "Reject", "comment": "This paper develops a meta-learning approach for few-shot object detection. This paper is borderline and the reviewers are split. The problem is important, albeit somewhat specific to computer vision applications. The main concerns were that it was lacking a head-to-head comparison to RepMet and that it was missing important details (e.g. the image resolution was not clarified, nor was the paper updated to include the details). The authors suggested that the RepMet code was not available, but I was able to find the official code for RepMet via a simple Google search:\nhttps://github.com/jshtok/RepMet\nReviewers also brought up concerns about an ICCV 2019 paper, though this should be considered as concurrent work, as it was not publicly available at the time of submission.\nOverall, I think the paper is borderline. Given that many meta-learning papers compare on rather synthetic benchmarks, the study of a more realistic problem setting is refreshing. That said, it's unclear if the insights from this paper would transfer to other machine learning problem settings of interest to the ICLR community.\nWith all of this in mind, the paper is slightly below the bar for acceptance at ICLR.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xwwu.2015@smu.edu.sg", "dsahoo@salesforce.com", "shoi@salesforce.com"], "title": "Meta-RCNN: Meta Learning for Few-Shot Object Detection", "authors": ["Xiongwei Wu", "Doyen Sahoo", "Steven C. H. Hoi"], "pdf": "/pdf/0b83df79e8e6fc0f217140732bf46707516e9e99.pdf", "TL;DR": "We develop Meta-RCNN which learns both the object classifier and the region proposal network via meta-learning in order to do few-shot detection", "abstract": "Despite significant advances in object detection in recent years, training effective detectors in a small data regime remains an open challenge. Labelling training data for object detection is extremely expensive, and there is a need to develop techniques that can generalize well from small amounts of labelled data. We investigate this problem of few-shot object detection, where a detector has access to only limited amounts of annotated data. Based on the recently evolving meta-learning principle, we propose a novel meta-learning framework for object detection named ``Meta-RCNN\", which learns the ability to perform few-shot detection via meta-learning. Specifically, Meta-RCNN learns an object detector in an episodic learning paradigm on the (meta) training data. This learning scheme helps acquire a prior which enables Meta-RCNN to do few-shot detection on novel tasks. Built on top of the Faster RCNN model, in Meta-RCNN, both the Region Proposal Network (RPN) and the object classification branch are meta-learned. The meta-trained RPN learns to provide class-specific proposals, while the object classifier learns to do few-shot classification. The novel loss objectives and learning strategy of Meta-RCNN can be trained in an end-to-end manner. We demonstrate the effectiveness of Meta-RCNN in addressing few-shot detection on Pascal VOC dataset and achieve promising results. ", "keywords": ["Few-shot detection", "Meta-Learning", "Object Detection"], "paperhash": "wu|metarcnn_meta_learning_for_fewshot_object_detection", "original_pdf": "/attachment/8f6381f7788288d1077a1f91f017a8997f4506bc.pdf", "_bibtex": "@misc{\nwu2020metarcnn,\ntitle={Meta-{\\{}RCNN{\\}}: Meta Learning for Few-Shot Object Detection},\nauthor={Xiongwei Wu and Doyen Sahoo and Steven C. H. Hoi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xmOgrFPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1xmOgrFPS", "replyto": "B1xmOgrFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711593, "tmdate": 1576800260824, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2392/-/Decision"}}}, {"id": "rJe_tNnijS", "original": null, "number": 4, "cdate": 1573794943771, "ddate": null, "tcdate": 1573794943771, "tmdate": 1573799309663, "tddate": null, "forum": "B1xmOgrFPS", "replyto": "BygTZ2FdYH", "invitation": "ICLR.cc/2020/Conference/Paper2392/-/Official_Comment", "content": {"title": "We offer clarifications on why the experiment setting is fair", "comment": "Thanks for the comments! We agree with your concerns, and would like to offer clarifications for a clearer understanding. \n\nTo do a novel few-shot detection task, a prior needs to be acquired from some base data (e.g. meta train data in our case). To acquire this prior, we can follow two approaches: 1) Train a traditional model (e.g. a detector or classifier), and then fine tune on the novel few-shot task; OR 2) Acquire a prior via meta-learning on the base data, and learn a model that is trained to do few-shot learning. \n\nLSTD follows the first paradigm, while our proposed Meta-RCNN follows the second paradigm. Note that both methods have access to the exact same base data, i.e., they have access to the same information. They differ only in the learning algorithm. Then, a novel few-shot task is given to the algorithm, and the algorithm makes the prediction.\n\nSince both models have access to the same information, and make predictions on the same few-shot test task, the comparison is fair.\n\nData Split Difference\nMeta-learning literature (Vinyals et al. 2016, Finn et al. 2017, Snell et al. 2017, etc.) evaluates few-shot performance over multiple tasks drawn from a test task distribution, i.e., the few-shot performance is measured and averaged over multiple tasks. This is a more reliable metric than evaluating performance on only one few-shot task. LSTD data split considers evaluation on only one few-shot task in their data split. We train LSTD on appropriate base data, and then evaluate its performance over multiple tasks, and compare this performance with our method. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2392/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2392/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xwwu.2015@smu.edu.sg", "dsahoo@salesforce.com", "shoi@salesforce.com"], "title": "Meta-RCNN: Meta Learning for Few-Shot Object Detection", "authors": ["Xiongwei Wu", "Doyen Sahoo", "Steven C. H. Hoi"], "pdf": "/pdf/0b83df79e8e6fc0f217140732bf46707516e9e99.pdf", "TL;DR": "We develop Meta-RCNN which learns both the object classifier and the region proposal network via meta-learning in order to do few-shot detection", "abstract": "Despite significant advances in object detection in recent years, training effective detectors in a small data regime remains an open challenge. Labelling training data for object detection is extremely expensive, and there is a need to develop techniques that can generalize well from small amounts of labelled data. We investigate this problem of few-shot object detection, where a detector has access to only limited amounts of annotated data. Based on the recently evolving meta-learning principle, we propose a novel meta-learning framework for object detection named ``Meta-RCNN\", which learns the ability to perform few-shot detection via meta-learning. Specifically, Meta-RCNN learns an object detector in an episodic learning paradigm on the (meta) training data. This learning scheme helps acquire a prior which enables Meta-RCNN to do few-shot detection on novel tasks. Built on top of the Faster RCNN model, in Meta-RCNN, both the Region Proposal Network (RPN) and the object classification branch are meta-learned. The meta-trained RPN learns to provide class-specific proposals, while the object classifier learns to do few-shot classification. The novel loss objectives and learning strategy of Meta-RCNN can be trained in an end-to-end manner. We demonstrate the effectiveness of Meta-RCNN in addressing few-shot detection on Pascal VOC dataset and achieve promising results. ", "keywords": ["Few-shot detection", "Meta-Learning", "Object Detection"], "paperhash": "wu|metarcnn_meta_learning_for_fewshot_object_detection", "original_pdf": "/attachment/8f6381f7788288d1077a1f91f017a8997f4506bc.pdf", "_bibtex": "@misc{\nwu2020metarcnn,\ntitle={Meta-{\\{}RCNN{\\}}: Meta Learning for Few-Shot Object Detection},\nauthor={Xiongwei Wu and Doyen Sahoo and Steven C. H. Hoi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xmOgrFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xmOgrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2392/Authors", "ICLR.cc/2020/Conference/Paper2392/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2392/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2392/Reviewers", "ICLR.cc/2020/Conference/Paper2392/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2392/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2392/Authors|ICLR.cc/2020/Conference/Paper2392/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142048, "tmdate": 1576860543494, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2392/Authors", "ICLR.cc/2020/Conference/Paper2392/Reviewers", "ICLR.cc/2020/Conference/Paper2392/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2392/-/Official_Comment"}}}, {"id": "Bkeg5r3sir", "original": null, "number": 5, "cdate": 1573795207975, "ddate": null, "tcdate": 1573795207975, "tmdate": 1573796364293, "tddate": null, "forum": "B1xmOgrFPS", "replyto": "HJeEHAb6tS", "invitation": "ICLR.cc/2020/Conference/Paper2392/-/Official_Comment", "content": {"title": "Thanks for highlighting potential issues! We think they can be addressed. ", "comment": "Thanks for the comments! We do agree with some of your concerns, but do think that most of the suggested issues are addressable. \n\n1. Implementation details with high-res images\n\nThanks for the suggestions, and we apologise for lack details.\n\nDuring meta-training, we train the model using 5-way-1shot tasks, and only 5 query images (1 query image per class). This results in a total of 10 images for one task. With this, implementing the meta-training is not too difficult. Using this trained model, we evaluate performance on various settings (e.g. 5-way 1-shot, and 5-way 5-shot) meta test tasks. We apologize for the lack of clarity in the first RPN - this is not a critical component, and just a minor trick we use to make the prototype more robust (instead of constructing prototypes of the support objects by directly using ground truth bounding boxes and labels, we also use the proposals generated by the RPN, and if it has sufficient overlap with the ground truth, it is used for constructing the prototype). The main contribution in RPN is the one that is trained to generate support(class)-specific proposals.\n\nWe will definitely release the code.\n\n\n2 & 3. Regarding RepMet\n\nThanks for these comments regarding RepMet. \ni) We have improved the presentation to not call it RepMet, but to call it FRCN-PN(baseline), and have changed the written section describing the relation of FRCN-PN with RepMet.\nii) FRCN-PN shares a similar principle as RepMet (traditional detector training + replacing the object classifier with a meta-learner), and thus is a baseline we considered for our work.\niii) We would have liked to reproduce RepMet and compare directly with the original method, however, we were not able to find the code for it. As a result, we decided to implement the method based on this principle ourselves as a baseline. \niv) The code for RepMet: The arxiv version and the published version do not have a working link for the code being available online. We found a not well tested/incomplete version of the code  ( https://github.com/HaydenFaulkner/pytorch.repmet ) done by a third party, which has not yet reproduced the results. \n\n\n4. Comparison with Meta-RCNN in ICCV2019\n\nThanks for suggesting the reference. We believe this work was done in parallel with our work. We would like to highlight that this work was made available on arxiv (28th September) a few days after the ICLR submission deadline (25th September). Moreover, it appeared in ICCV even more recently (27th October).\n\nThis work does share some similarities as our work (principle of class-attentive module), however, there is a fundamental difference in the training approach, specifically for the RPN. In contrast to the reference paper, our RPN is meta-trained and is tailored to generate proposals for the few-shot setting. \n\nWe train the RPN in the meta-learning paradigm (meta-RPN), whereas the RPN training in the ICCV paper is trained using the traditional setting. This difference is extremely crucial for few-shot detection. Traditional RPNs will detect all objects in the image (including objects not of interest, i.e., they will even detect objects that are not available in given support set). Our meta-trained RPN generates proposals for an object from classes only belonging to the support set (i.e., it generates class-specific proposals). \n\nFinally, we would also like to highlight that following the meta-learning literature, we have evaluated the performance of the object detector on \u201cmultiple\u201d few-shot detection tasks. Our reported few-shot performance is average performance over these tasks, in contrast to the existing reference which evaluates result on exactly one few-shot task.\n\nAs regards empirical comparisons, it would be slightly time consuming to do this given different settings (e.g. hyperparameters, backbone, data splits, different approach for using the meta-train dataset, etc.). We do aim to do this in the future. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2392/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2392/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xwwu.2015@smu.edu.sg", "dsahoo@salesforce.com", "shoi@salesforce.com"], "title": "Meta-RCNN: Meta Learning for Few-Shot Object Detection", "authors": ["Xiongwei Wu", "Doyen Sahoo", "Steven C. H. Hoi"], "pdf": "/pdf/0b83df79e8e6fc0f217140732bf46707516e9e99.pdf", "TL;DR": "We develop Meta-RCNN which learns both the object classifier and the region proposal network via meta-learning in order to do few-shot detection", "abstract": "Despite significant advances in object detection in recent years, training effective detectors in a small data regime remains an open challenge. Labelling training data for object detection is extremely expensive, and there is a need to develop techniques that can generalize well from small amounts of labelled data. We investigate this problem of few-shot object detection, where a detector has access to only limited amounts of annotated data. Based on the recently evolving meta-learning principle, we propose a novel meta-learning framework for object detection named ``Meta-RCNN\", which learns the ability to perform few-shot detection via meta-learning. Specifically, Meta-RCNN learns an object detector in an episodic learning paradigm on the (meta) training data. This learning scheme helps acquire a prior which enables Meta-RCNN to do few-shot detection on novel tasks. Built on top of the Faster RCNN model, in Meta-RCNN, both the Region Proposal Network (RPN) and the object classification branch are meta-learned. The meta-trained RPN learns to provide class-specific proposals, while the object classifier learns to do few-shot classification. The novel loss objectives and learning strategy of Meta-RCNN can be trained in an end-to-end manner. We demonstrate the effectiveness of Meta-RCNN in addressing few-shot detection on Pascal VOC dataset and achieve promising results. ", "keywords": ["Few-shot detection", "Meta-Learning", "Object Detection"], "paperhash": "wu|metarcnn_meta_learning_for_fewshot_object_detection", "original_pdf": "/attachment/8f6381f7788288d1077a1f91f017a8997f4506bc.pdf", "_bibtex": "@misc{\nwu2020metarcnn,\ntitle={Meta-{\\{}RCNN{\\}}: Meta Learning for Few-Shot Object Detection},\nauthor={Xiongwei Wu and Doyen Sahoo and Steven C. H. Hoi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xmOgrFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xmOgrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2392/Authors", "ICLR.cc/2020/Conference/Paper2392/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2392/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2392/Reviewers", "ICLR.cc/2020/Conference/Paper2392/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2392/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2392/Authors|ICLR.cc/2020/Conference/Paper2392/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142048, "tmdate": 1576860543494, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2392/Authors", "ICLR.cc/2020/Conference/Paper2392/Reviewers", "ICLR.cc/2020/Conference/Paper2392/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2392/-/Official_Comment"}}}, {"id": "rygUTX2isH", "original": null, "number": 3, "cdate": 1573794749536, "ddate": null, "tcdate": 1573794749536, "tmdate": 1573794749536, "tddate": null, "forum": "B1xmOgrFPS", "replyto": "ryg6pPtQ5r", "invitation": "ICLR.cc/2020/Conference/Paper2392/-/Official_Comment", "content": {"title": "Thanks for the positive comments and the concerns!", "comment": "Thank you for your review! We were delighted with your comments! \n\nAs regards novelty, we would like to highlight that it is not trivial to adapt meta-learning for object detection, and to the best of our knowledge, ours is the first work that trains both the object classifier and the RPN in a meta-learning paradigm, making all the components tailored for  few-shot detection.\n\nThanks for identifying the writing issues, we have fixed them in the current version. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2392/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2392/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xwwu.2015@smu.edu.sg", "dsahoo@salesforce.com", "shoi@salesforce.com"], "title": "Meta-RCNN: Meta Learning for Few-Shot Object Detection", "authors": ["Xiongwei Wu", "Doyen Sahoo", "Steven C. H. Hoi"], "pdf": "/pdf/0b83df79e8e6fc0f217140732bf46707516e9e99.pdf", "TL;DR": "We develop Meta-RCNN which learns both the object classifier and the region proposal network via meta-learning in order to do few-shot detection", "abstract": "Despite significant advances in object detection in recent years, training effective detectors in a small data regime remains an open challenge. Labelling training data for object detection is extremely expensive, and there is a need to develop techniques that can generalize well from small amounts of labelled data. We investigate this problem of few-shot object detection, where a detector has access to only limited amounts of annotated data. Based on the recently evolving meta-learning principle, we propose a novel meta-learning framework for object detection named ``Meta-RCNN\", which learns the ability to perform few-shot detection via meta-learning. Specifically, Meta-RCNN learns an object detector in an episodic learning paradigm on the (meta) training data. This learning scheme helps acquire a prior which enables Meta-RCNN to do few-shot detection on novel tasks. Built on top of the Faster RCNN model, in Meta-RCNN, both the Region Proposal Network (RPN) and the object classification branch are meta-learned. The meta-trained RPN learns to provide class-specific proposals, while the object classifier learns to do few-shot classification. The novel loss objectives and learning strategy of Meta-RCNN can be trained in an end-to-end manner. We demonstrate the effectiveness of Meta-RCNN in addressing few-shot detection on Pascal VOC dataset and achieve promising results. ", "keywords": ["Few-shot detection", "Meta-Learning", "Object Detection"], "paperhash": "wu|metarcnn_meta_learning_for_fewshot_object_detection", "original_pdf": "/attachment/8f6381f7788288d1077a1f91f017a8997f4506bc.pdf", "_bibtex": "@misc{\nwu2020metarcnn,\ntitle={Meta-{\\{}RCNN{\\}}: Meta Learning for Few-Shot Object Detection},\nauthor={Xiongwei Wu and Doyen Sahoo and Steven C. H. Hoi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xmOgrFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xmOgrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2392/Authors", "ICLR.cc/2020/Conference/Paper2392/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2392/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2392/Reviewers", "ICLR.cc/2020/Conference/Paper2392/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2392/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2392/Authors|ICLR.cc/2020/Conference/Paper2392/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142048, "tmdate": 1576860543494, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2392/Authors", "ICLR.cc/2020/Conference/Paper2392/Reviewers", "ICLR.cc/2020/Conference/Paper2392/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2392/-/Official_Comment"}}}, {"id": "BygTZ2FdYH", "original": null, "number": 1, "cdate": 1571490820891, "ddate": null, "tcdate": 1571490820891, "tmdate": 1572972344245, "tddate": null, "forum": "B1xmOgrFPS", "replyto": "B1xmOgrFPS", "invitation": "ICLR.cc/2020/Conference/Paper2392/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, authors propose a meta-learning based approach for low-shot object detection. Specifically, they use prototype in the support set as attention guidance, and learn the category-specific representation for each query image. Subsequently, they use the style of Faster RCNN for object detection.\n\nIt is an OK paper with good structure. The idea is somewhat novel, in terms of meta-learning based low-shot detection framework. My main concern is about experiment. First, the data setting is branch new. Why not use the data setting in the literature, e.g., COCO to VOC in LSTD (Chen et al., 2018)? As a result, how to make a fair comparison bothers me a little. Furthermore, LSTD is a non-episodic approach. How to make it in a meta-learning way? Please clarify the implementation details for all other related works in the comparison.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper2392/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2392/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xwwu.2015@smu.edu.sg", "dsahoo@salesforce.com", "shoi@salesforce.com"], "title": "Meta-RCNN: Meta Learning for Few-Shot Object Detection", "authors": ["Xiongwei Wu", "Doyen Sahoo", "Steven C. H. Hoi"], "pdf": "/pdf/0b83df79e8e6fc0f217140732bf46707516e9e99.pdf", "TL;DR": "We develop Meta-RCNN which learns both the object classifier and the region proposal network via meta-learning in order to do few-shot detection", "abstract": "Despite significant advances in object detection in recent years, training effective detectors in a small data regime remains an open challenge. Labelling training data for object detection is extremely expensive, and there is a need to develop techniques that can generalize well from small amounts of labelled data. We investigate this problem of few-shot object detection, where a detector has access to only limited amounts of annotated data. Based on the recently evolving meta-learning principle, we propose a novel meta-learning framework for object detection named ``Meta-RCNN\", which learns the ability to perform few-shot detection via meta-learning. Specifically, Meta-RCNN learns an object detector in an episodic learning paradigm on the (meta) training data. This learning scheme helps acquire a prior which enables Meta-RCNN to do few-shot detection on novel tasks. Built on top of the Faster RCNN model, in Meta-RCNN, both the Region Proposal Network (RPN) and the object classification branch are meta-learned. The meta-trained RPN learns to provide class-specific proposals, while the object classifier learns to do few-shot classification. The novel loss objectives and learning strategy of Meta-RCNN can be trained in an end-to-end manner. We demonstrate the effectiveness of Meta-RCNN in addressing few-shot detection on Pascal VOC dataset and achieve promising results. ", "keywords": ["Few-shot detection", "Meta-Learning", "Object Detection"], "paperhash": "wu|metarcnn_meta_learning_for_fewshot_object_detection", "original_pdf": "/attachment/8f6381f7788288d1077a1f91f017a8997f4506bc.pdf", "_bibtex": "@misc{\nwu2020metarcnn,\ntitle={Meta-{\\{}RCNN{\\}}: Meta Learning for Few-Shot Object Detection},\nauthor={Xiongwei Wu and Doyen Sahoo and Steven C. H. Hoi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xmOgrFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xmOgrFPS", "replyto": "B1xmOgrFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2392/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2392/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574970333145, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2392/Reviewers"], "noninvitees": [], "tcdate": 1570237723473, "tmdate": 1574970333160, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2392/-/Official_Review"}}}, {"id": "HJeEHAb6tS", "original": null, "number": 2, "cdate": 1571786299617, "ddate": null, "tcdate": 1571786299617, "tmdate": 1572972344208, "tddate": null, "forum": "B1xmOgrFPS", "replyto": "B1xmOgrFPS", "invitation": "ICLR.cc/2020/Conference/Paper2392/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a method for few-shot object detection (FSOD), a variant of few-shot learning (FSL) where using a support set of few training images for novel categories (usually 1 or 5) not only the correct category labels are predicted on the query images, but also the object instances from the novel categories are localized and their bounding boxes are predicted. The method proposes a network architecture where the sliding window features that enter the RPN are first attenuated using support classes prototypes discovered using (a different?) RPN and found as matching to the few provided box annotations on the support images. The attenuation is by channel wise multiplication of the feature map and concatenation of the resulting feature maps (one per support class). After the RPN, ROI-pooling is applied on the concatenated feature map that is reduced using 1x1 convolution and original feature map (before attenuation) being added to the result. Following this a two FC layer classifier is fine-tuned on the support data to form the final \nRCNN head of the few-shot detector. The whole network is claimed to be meta-trained end to end following COCO or ImageNet (LOC? DET?) pre-training. The method is tested on a split of PASCAL VOC07 into two sets of 10 categories, one for meta-training and the other for meta-testing. In addition, experiments are carried out on ImageNet-LOC animals subset. In both cases, the result are compared to some baselines, and some prior work.\n\nAlthough FSOD is an important emerging problem, and advances on it are very important, I believe there are still certain gaps in the current paper that need to be fixed before it is accepted. Specifically:\n\n1. Some important details are missing from the description. For example, detectors are usually trained on high resolution images (e.g. 1000 x 1000) and hence are problematic to train with large batches, yet in the proposed approach it is claimed that the proposed model is meta-trained with batch size 5 on 5 way tasks with 10 queries each, so even in 1-shot case, does it mean that 5 x 15 = 75 high resolution images enter the GPU at each batch? I doubt that even in parallel mode with 5 GPUs and 15 high res image per GPU it is possible for claimed backbone architectures (ResNet-50 and VGG16).\nAs another example, the details of fine-tuning during meta-training seem to be left out, is the model optimized with an inner loop? Details of the RPN that is used to select the support categories prototypes are not specified, where it comes from and how is it trained (clearly as the \"main\" RPN relies on attenuated features, it cannot be it)? Some additional technical details are not very clear and hinder the reproducibility of the paper (no code seem to be promised?), in general I suggest the authors to improve the writing and clarity of the paper.\n\n2. In VOC07 experiment, FRCN-PN is very vaguely described and being claimed that it stands for RepMet (Karlinksy et al., CVPR 2019). It is not clear what it is and its training procedure on VOC07 is not clearly described.\nIt is also claimed in ImageNet experiment that the real RepMet is \"more carefully designed then FRCN-PN\" and has a better backbone, hence it is not clear why FRCN-PN should stand for it.\nI suggest the authors to either do a direct comparison or remove their claim of comparison.\n\n3. RepMet paper has proposed an additional benchmark on ImageNet-LOC with 5-way 1/5/10-shot episodes, and afaik it is reproducible as its code is released, so I am wondering as to why it was not used for \nevaluation given that the authors made the effort of reproducing another ImageNet-LOC test on the same categories? It should be evaluated for a fair comparison.\n\n4. Although they don't strictly have to compare to it, I am wondering if the authors would be willing to relate to a similar approach that was proposed for the upcoming ICCV 19: \n\"Meta R-CNN : Towards General Solver for Instance-level Low-shot Learning\", by Yan et al. Their approach is more similar to RepMet in a sense that the meta-learning is done in the classifier head,\nand better results are reported on VOC07 benchmark (and except for 1-shot, higher results are reported for the 3 and 5 shot FRCNN fine-tuning)."}, "signatures": ["ICLR.cc/2020/Conference/Paper2392/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2392/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xwwu.2015@smu.edu.sg", "dsahoo@salesforce.com", "shoi@salesforce.com"], "title": "Meta-RCNN: Meta Learning for Few-Shot Object Detection", "authors": ["Xiongwei Wu", "Doyen Sahoo", "Steven C. H. Hoi"], "pdf": "/pdf/0b83df79e8e6fc0f217140732bf46707516e9e99.pdf", "TL;DR": "We develop Meta-RCNN which learns both the object classifier and the region proposal network via meta-learning in order to do few-shot detection", "abstract": "Despite significant advances in object detection in recent years, training effective detectors in a small data regime remains an open challenge. Labelling training data for object detection is extremely expensive, and there is a need to develop techniques that can generalize well from small amounts of labelled data. We investigate this problem of few-shot object detection, where a detector has access to only limited amounts of annotated data. Based on the recently evolving meta-learning principle, we propose a novel meta-learning framework for object detection named ``Meta-RCNN\", which learns the ability to perform few-shot detection via meta-learning. Specifically, Meta-RCNN learns an object detector in an episodic learning paradigm on the (meta) training data. This learning scheme helps acquire a prior which enables Meta-RCNN to do few-shot detection on novel tasks. Built on top of the Faster RCNN model, in Meta-RCNN, both the Region Proposal Network (RPN) and the object classification branch are meta-learned. The meta-trained RPN learns to provide class-specific proposals, while the object classifier learns to do few-shot classification. The novel loss objectives and learning strategy of Meta-RCNN can be trained in an end-to-end manner. We demonstrate the effectiveness of Meta-RCNN in addressing few-shot detection on Pascal VOC dataset and achieve promising results. ", "keywords": ["Few-shot detection", "Meta-Learning", "Object Detection"], "paperhash": "wu|metarcnn_meta_learning_for_fewshot_object_detection", "original_pdf": "/attachment/8f6381f7788288d1077a1f91f017a8997f4506bc.pdf", "_bibtex": "@misc{\nwu2020metarcnn,\ntitle={Meta-{\\{}RCNN{\\}}: Meta Learning for Few-Shot Object Detection},\nauthor={Xiongwei Wu and Doyen Sahoo and Steven C. H. Hoi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xmOgrFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xmOgrFPS", "replyto": "B1xmOgrFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2392/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2392/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574970333145, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2392/Reviewers"], "noninvitees": [], "tcdate": 1570237723473, "tmdate": 1574970333160, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2392/-/Official_Review"}}}, {"id": "ryg6pPtQ5r", "original": null, "number": 3, "cdate": 1572210628559, "ddate": null, "tcdate": 1572210628559, "tmdate": 1572972344164, "tddate": null, "forum": "B1xmOgrFPS", "replyto": "B1xmOgrFPS", "invitation": "ICLR.cc/2020/Conference/Paper2392/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper is about the task of object detection in the setting of few-shots dataset. The problem is addressed in the learning scheme of meta-learning paradigm: the proposed meta-rcnn trains the popular faster-rcnn on several tasks of few shots object detection while the RPN and the object classification networks are meta-learned among the tasks. Compared to previous work the paper introduces the meta learning framework and several changes to the faster rcnn detector. A prototype representation is derived from the standard RPN network and its proposed bounding box. An attention mechanism choose the object of interest and is used to train the final RPN and classification network. Experiments on the popular Pascal Voc 2007 and ImageNet-FSOD show that the proposed system have state of the art performance.\n\nThe paper is very well written, easy to read and of excellent presentation. The introduction of the meta learning paradigm and its use to learn the RPN and classification networks are incremental in novelty but interesting. The experiments are solid and show state of the art performance. As a result I recommend this paper to be accepted.\n\nMinor issues:\n- in caption of Fig1: avialable -> available\n- in 4.1: \u201cCompared to other variants...\u201d please add a reference to the specific methods you are comparing to."}, "signatures": ["ICLR.cc/2020/Conference/Paper2392/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2392/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xwwu.2015@smu.edu.sg", "dsahoo@salesforce.com", "shoi@salesforce.com"], "title": "Meta-RCNN: Meta Learning for Few-Shot Object Detection", "authors": ["Xiongwei Wu", "Doyen Sahoo", "Steven C. H. Hoi"], "pdf": "/pdf/0b83df79e8e6fc0f217140732bf46707516e9e99.pdf", "TL;DR": "We develop Meta-RCNN which learns both the object classifier and the region proposal network via meta-learning in order to do few-shot detection", "abstract": "Despite significant advances in object detection in recent years, training effective detectors in a small data regime remains an open challenge. Labelling training data for object detection is extremely expensive, and there is a need to develop techniques that can generalize well from small amounts of labelled data. We investigate this problem of few-shot object detection, where a detector has access to only limited amounts of annotated data. Based on the recently evolving meta-learning principle, we propose a novel meta-learning framework for object detection named ``Meta-RCNN\", which learns the ability to perform few-shot detection via meta-learning. Specifically, Meta-RCNN learns an object detector in an episodic learning paradigm on the (meta) training data. This learning scheme helps acquire a prior which enables Meta-RCNN to do few-shot detection on novel tasks. Built on top of the Faster RCNN model, in Meta-RCNN, both the Region Proposal Network (RPN) and the object classification branch are meta-learned. The meta-trained RPN learns to provide class-specific proposals, while the object classifier learns to do few-shot classification. The novel loss objectives and learning strategy of Meta-RCNN can be trained in an end-to-end manner. We demonstrate the effectiveness of Meta-RCNN in addressing few-shot detection on Pascal VOC dataset and achieve promising results. ", "keywords": ["Few-shot detection", "Meta-Learning", "Object Detection"], "paperhash": "wu|metarcnn_meta_learning_for_fewshot_object_detection", "original_pdf": "/attachment/8f6381f7788288d1077a1f91f017a8997f4506bc.pdf", "_bibtex": "@misc{\nwu2020metarcnn,\ntitle={Meta-{\\{}RCNN{\\}}: Meta Learning for Few-Shot Object Detection},\nauthor={Xiongwei Wu and Doyen Sahoo and Steven C. H. Hoi},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xmOgrFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xmOgrFPS", "replyto": "B1xmOgrFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2392/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2392/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574970333145, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2392/Reviewers"], "noninvitees": [], "tcdate": 1570237723473, "tmdate": 1574970333160, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2392/-/Official_Review"}}}], "count": 8}