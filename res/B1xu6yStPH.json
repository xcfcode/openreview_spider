{"notes": [{"id": "B1xu6yStPH", "original": "H1eJ-ByYPr", "number": 1995, "cdate": 1569439680145, "ddate": null, "tcdate": 1569439680145, "tmdate": 1577168265348, "tddate": null, "forum": "B1xu6yStPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["amosy3@gmail.com", "gal.chechik@gmail.com"], "title": "Using Explainabilty to Detect Adversarial Attacks", "authors": ["Ohad Amosy and Gal Chechik"], "pdf": "/pdf/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "TL;DR": "A novel adversarial detection approach, which uses explainability methods to identify images whose explanations are inconsistent with the predicted class.  ", "abstract": "Deep learning models are often sensitive to adversarial attacks, where carefully-designed input samples can cause the system to produce incorrect decisions. Here we focus on the problem of detecting attacks, rather than robust classification, since detecting that an attack occurs may be even more important than avoiding misclassification. We build on advances in explainability, where activity-map-like explanations are used to justify and validate decisions, by highlighting features that are involved with a classification decision. The key observation is that it is hard to create explanations for incorrect decisions.  We propose EXAID, a novel attack-detection approach, which uses model explainability to identify images whose explanations are inconsistent with the predicted class. Specifically, we use SHAP, which uses Shapley values in the space of the input image, to identify which input features contribute to a class decision. Interestingly, this approach does not require to modify the attacked model, and it can be applied without modelling a specific attack. It can therefore be applied successfully to detect unfamiliar attacks, that were unknown at the time the detection model was designed. We evaluate EXAID on two benchmark datasets CIFAR-10 and SVHN, and against three leading attack techniques, FGSM, PGD and C&W. We find that EXAID improves over the SoTA detection methods by a large margin across a wide range of noise levels, improving detection from 70% to over 90% for small perturbations.", "keywords": ["adversarial", "detection", "explainability"], "paperhash": "chechik|using_explainabilty_to_detect_adversarial_attacks", "original_pdf": "/attachment/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "_bibtex": "@misc{\nchechik2020using,\ntitle={Using Explainabilty to Detect Adversarial Attacks},\nauthor={Ohad Amosy and Gal Chechik},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xu6yStPH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Rj0ZE8tyv", "original": null, "number": 1, "cdate": 1576798737842, "ddate": null, "tcdate": 1576798737842, "tmdate": 1576800898515, "tddate": null, "forum": "B1xu6yStPH", "replyto": "B1xu6yStPH", "invitation": "ICLR.cc/2020/Conference/Paper1995/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes EXAID, a method to detect adversarial attacks by building on the advances in explainability (particularly SHAP), where activity-map-like explanations are used to justify and validate decisions. Though it may have some valuable ideas, the execution is not satisfying, with various issues raised in comments. No rebuttal was provided.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["amosy3@gmail.com", "gal.chechik@gmail.com"], "title": "Using Explainabilty to Detect Adversarial Attacks", "authors": ["Ohad Amosy and Gal Chechik"], "pdf": "/pdf/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "TL;DR": "A novel adversarial detection approach, which uses explainability methods to identify images whose explanations are inconsistent with the predicted class.  ", "abstract": "Deep learning models are often sensitive to adversarial attacks, where carefully-designed input samples can cause the system to produce incorrect decisions. Here we focus on the problem of detecting attacks, rather than robust classification, since detecting that an attack occurs may be even more important than avoiding misclassification. We build on advances in explainability, where activity-map-like explanations are used to justify and validate decisions, by highlighting features that are involved with a classification decision. The key observation is that it is hard to create explanations for incorrect decisions.  We propose EXAID, a novel attack-detection approach, which uses model explainability to identify images whose explanations are inconsistent with the predicted class. Specifically, we use SHAP, which uses Shapley values in the space of the input image, to identify which input features contribute to a class decision. Interestingly, this approach does not require to modify the attacked model, and it can be applied without modelling a specific attack. It can therefore be applied successfully to detect unfamiliar attacks, that were unknown at the time the detection model was designed. We evaluate EXAID on two benchmark datasets CIFAR-10 and SVHN, and against three leading attack techniques, FGSM, PGD and C&W. We find that EXAID improves over the SoTA detection methods by a large margin across a wide range of noise levels, improving detection from 70% to over 90% for small perturbations.", "keywords": ["adversarial", "detection", "explainability"], "paperhash": "chechik|using_explainabilty_to_detect_adversarial_attacks", "original_pdf": "/attachment/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "_bibtex": "@misc{\nchechik2020using,\ntitle={Using Explainabilty to Detect Adversarial Attacks},\nauthor={Ohad Amosy and Gal Chechik},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xu6yStPH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1xu6yStPH", "replyto": "B1xu6yStPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725802, "tmdate": 1576800277779, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1995/-/Decision"}}}, {"id": "r1xUKwB0FH", "original": null, "number": 1, "cdate": 1571866494363, "ddate": null, "tcdate": 1571866494363, "tmdate": 1572972396894, "tddate": null, "forum": "B1xu6yStPH", "replyto": "B1xu6yStPH", "invitation": "ICLR.cc/2020/Conference/Paper1995/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper suggests a method for detecting adversarial attacks known as EXAID, which leverages deep learning explainability techniques to detect adversarial examples. The method works by looking at the prediction made by the classifier as well as the output of the explainability method, and labelling the input as an adversarial example if the predicted class is inconsistent with the model explanation. EXAID uses Shapley values as the explanation technique, and is shown to successfully detect many standard first-order attacks.\n\nThough method is well-presented and the evaluation is substantial, the threat model of the oblivious adversary is unconvincing. The paper makes the argument that oblivious adversaries are more prevalent in the real world, but several works [1,2,3,etc.] have shown that with only query access to input-label pairs from a deep learning-based system, it is possible to construct black-box adversarial attacks. Thus, it is unclear why an attacker cannot just treat the detection mechanism as part of this black box, and mount a successful query-based attack. \n\nThough I recognize that the task of detection is separate from the task of robust classification, in both cases the defender should at least operate in the case where the attacker has input-output access to the end-to-end system (including whatever detection mechanisms are present). In particular, it seems impossible to \"hide\" a detector from an end user (when the method detects an adversarial example, it will alert the user somehow that the input was rejected), and so the user will be able to use this information to fool the system. The authors should investigate the white-box accuracy of their detection system, or at the very least try black-box attacks against the detector. For this reason I do not recommend acceptance for the paper at this time.\n\n[1] https://arxiv.org/abs/1804.08598\n[2] https://arxiv.org/abs/1807.04457\n[3] https://arxiv.org/abs/1712.04248"}, "signatures": ["ICLR.cc/2020/Conference/Paper1995/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1995/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["amosy3@gmail.com", "gal.chechik@gmail.com"], "title": "Using Explainabilty to Detect Adversarial Attacks", "authors": ["Ohad Amosy and Gal Chechik"], "pdf": "/pdf/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "TL;DR": "A novel adversarial detection approach, which uses explainability methods to identify images whose explanations are inconsistent with the predicted class.  ", "abstract": "Deep learning models are often sensitive to adversarial attacks, where carefully-designed input samples can cause the system to produce incorrect decisions. Here we focus on the problem of detecting attacks, rather than robust classification, since detecting that an attack occurs may be even more important than avoiding misclassification. We build on advances in explainability, where activity-map-like explanations are used to justify and validate decisions, by highlighting features that are involved with a classification decision. The key observation is that it is hard to create explanations for incorrect decisions.  We propose EXAID, a novel attack-detection approach, which uses model explainability to identify images whose explanations are inconsistent with the predicted class. Specifically, we use SHAP, which uses Shapley values in the space of the input image, to identify which input features contribute to a class decision. Interestingly, this approach does not require to modify the attacked model, and it can be applied without modelling a specific attack. It can therefore be applied successfully to detect unfamiliar attacks, that were unknown at the time the detection model was designed. We evaluate EXAID on two benchmark datasets CIFAR-10 and SVHN, and against three leading attack techniques, FGSM, PGD and C&W. We find that EXAID improves over the SoTA detection methods by a large margin across a wide range of noise levels, improving detection from 70% to over 90% for small perturbations.", "keywords": ["adversarial", "detection", "explainability"], "paperhash": "chechik|using_explainabilty_to_detect_adversarial_attacks", "original_pdf": "/attachment/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "_bibtex": "@misc{\nchechik2020using,\ntitle={Using Explainabilty to Detect Adversarial Attacks},\nauthor={Ohad Amosy and Gal Chechik},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xu6yStPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xu6yStPH", "replyto": "B1xu6yStPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1995/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1995/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575442650209, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1995/Reviewers"], "noninvitees": [], "tcdate": 1570237729282, "tmdate": 1575442650221, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1995/-/Official_Review"}}}, {"id": "Bke1qCDAtB", "original": null, "number": 2, "cdate": 1571876486961, "ddate": null, "tcdate": 1571876486961, "tmdate": 1572972396846, "tddate": null, "forum": "B1xu6yStPH", "replyto": "B1xu6yStPH", "invitation": "ICLR.cc/2020/Conference/Paper1995/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "A Simple method to detect adversarial examples, but needs more work.\n\n#Summary:\nThe paper proposed a method that utilizes the model\u2019s explainability to detect adversarial images whose explanations that are not consistent with the predicted class.  The explainability is generated by SHAP, which uses Shapley values to identify relative contributions of each input to a class decision. It designs two detection methods: EXAID familiar, which is aimed to detect the known attacks and EXAID unknown, which is against unknown attacks. Both of the two methods are evaluated on perturbed test data which are generated by FGSM, PGD and CW attack with perturbations of different magnitudes. Qualitative results also show that the proposed method can effectively detect adversaries, especially when the perturbation is relatively small.\n\n#Strength\nThe method is easy to implement and using the idea of interpretation for detecting adversarial examples seems interesting.\n\nGood results are demonstrated compared with other comparators.\n\n#Weakness\nThe idea of this paper is based on the interpretation method of DNN. However, it has been shown that these interpretation methods are not reliable and easy to be manipulated [1][2]. Therefore, although the method is simple to design, it also brings other security concerns.\nUnfortunately, the paper does not address these issues. In addition, the comparators listed in the experiments are not state-of-art or common baselines. It is either not clear why authors modified the existing method and develop their own \u201cunsupervised\u201d version. \nIn the experiments, many details are omitted. For example, how is the \u201cnoise level\u201d defined? Are they based on L1, L2 or L-inf perturbation? For PGD attack, how many iterations does the generation run and what is the step size? How many effective adversarial examples are generated for training and testing? And all the experiments are conducted in a relatively small dataset, it is also suggested to do experiments on large datasets, e.g. Imagenet.\nIn the evaluation part, it looks strange to me why the EXAID familiar performs worse than EXAID unknown in evaluating FGSM attack on SVHN since the EXAID familiar is trained using FGSM attack.\n\n#Presentation\nI think the authors used a wrong template to generate the article. The font looks strange and the headnote indicates it is prepared for ICLR2020. The paper contains many typos and even the title contains a misspelling. Poor coverage of citations. There are more works for detecting adversarial examples that are published, e.g. [3][4][5]. On the other hand, the paper does not have the literature review for work related to the model interpretation.\n\nOverall, I think the paper is not good enough for publication at ICLR.\n[1] Dombrowski, Ann-Kathrin, et al. \"Explanations can be manipulated and geometry is to blame.\" arXiv preprint arXiv:1906.07983 (2019).\n[2] Ghorbani, Amirata, Abubakar Abid, and James Zou. \"Interpretation of neural networks is fragile.\" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 33. 2019.\n[3] Meng, Dongyu, and Hao Chen. \"Magnet: a two-pronged defense against adversarial examples.\" In Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, pp. 135-147. ACM, 2017.\n[4] Liao, Fangzhou, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. \"Defense against adversarial attacks using high-level representation guided denoiser.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1778-1787. 2018.\n[5] Ma, Shiqing, Yingqi Liu, Guanhong Tao, Wen-Chuan Lee, and Xiangyu Zhang. \"NIC: Detecting Adversarial Samples with Neural Network Invariant Checking.\" In NDSS. 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1995/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1995/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["amosy3@gmail.com", "gal.chechik@gmail.com"], "title": "Using Explainabilty to Detect Adversarial Attacks", "authors": ["Ohad Amosy and Gal Chechik"], "pdf": "/pdf/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "TL;DR": "A novel adversarial detection approach, which uses explainability methods to identify images whose explanations are inconsistent with the predicted class.  ", "abstract": "Deep learning models are often sensitive to adversarial attacks, where carefully-designed input samples can cause the system to produce incorrect decisions. Here we focus on the problem of detecting attacks, rather than robust classification, since detecting that an attack occurs may be even more important than avoiding misclassification. We build on advances in explainability, where activity-map-like explanations are used to justify and validate decisions, by highlighting features that are involved with a classification decision. The key observation is that it is hard to create explanations for incorrect decisions.  We propose EXAID, a novel attack-detection approach, which uses model explainability to identify images whose explanations are inconsistent with the predicted class. Specifically, we use SHAP, which uses Shapley values in the space of the input image, to identify which input features contribute to a class decision. Interestingly, this approach does not require to modify the attacked model, and it can be applied without modelling a specific attack. It can therefore be applied successfully to detect unfamiliar attacks, that were unknown at the time the detection model was designed. We evaluate EXAID on two benchmark datasets CIFAR-10 and SVHN, and against three leading attack techniques, FGSM, PGD and C&W. We find that EXAID improves over the SoTA detection methods by a large margin across a wide range of noise levels, improving detection from 70% to over 90% for small perturbations.", "keywords": ["adversarial", "detection", "explainability"], "paperhash": "chechik|using_explainabilty_to_detect_adversarial_attacks", "original_pdf": "/attachment/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "_bibtex": "@misc{\nchechik2020using,\ntitle={Using Explainabilty to Detect Adversarial Attacks},\nauthor={Ohad Amosy and Gal Chechik},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xu6yStPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xu6yStPH", "replyto": "B1xu6yStPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1995/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1995/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575442650209, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1995/Reviewers"], "noninvitees": [], "tcdate": 1570237729282, "tmdate": 1575442650221, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1995/-/Official_Review"}}}, {"id": "Skxtu9afqB", "original": null, "number": 3, "cdate": 1572162160553, "ddate": null, "tcdate": 1572162160553, "tmdate": 1572972396799, "tddate": null, "forum": "B1xu6yStPH", "replyto": "B1xu6yStPH", "invitation": "ICLR.cc/2020/Conference/Paper1995/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary: The authors propose an explanation-based adversarial example detection algorithm. The main idea is to train a discriminator to detect whether the explanatory saliency map is consistent with the input. Experiments have been conducted on CIFAR10 and SVHN to validate the method.\n\nComments:\n+ The idea is straightforward and easy to follow.\n\n- The use of SHAP as the only explanation method is not well explained. There are a plenty of works on visual explanation methods, such as guided-backprop[1], excitation-backprop[2], integrated gradient[3], Grad-CAM[4], real-time saliency[5] and so on. And based on my expertise, SHAP cannot generate the most accurate saliency among these methods.  If the proposed framework is general, why not to conduct ablation study on the different choice of explainer?\n\n- Doubts on the effectiveness of the proposed method. According to former works[6, 7], explanatory saliency methods  are vulnerable and unreliable with respect to input perturbations. But in this paper, the authors assume that the explanation saliency map for normal examples are perfectly correct and used as positive instances for training the discriminator. I think they only focus on target attack, in which the attacking target label is semantically distinct from the original label, and the resulting saliency map distribution is very different from the correct one. However, considering a tabby cat image is perturbed to become tiger cat, since two classes are very close, the resulting saliency maps should be similar and the detector may fail to detect the adversarial example. Therefore, I encourage the authors to provide more results on this challenging scenario (for example, conduct un-target attack on imagenet dataset).\n\n- The reported results in Figure 2(e) is abnormal. First, the blue line (authors' method) is very close to AUC=1.0 across different noise levels, which means that the detector can perfectly classify all the adversarial examples in all the situation. Second, the reported values of other methods are not correct. For example, the black line (original Mahalanobis) is below AUC=0.5 across all the noise level. However, in the Table3 ResNet-CIFAR10 row of its original paper[8], the reported AUC under C&W attack is 95.84, which is much larger than those shown in the figure. Therefore, I think the comparison is invalid. Similar problems also appear in Figure 2(f).\n\n[1] J. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. (2015). Striving for simplicity: The all convolutional net. In ICLR (workshop track).\n[2] J. Zhang, Z. Lin, J. Brandt, X. Shen, and S. Sclaroff. (2016). Top-down neural attention by excitation backprop. In ECCV.\n[3] Sundararajan, M., Taly, A., & Yan, Q. (2017). Axiomatic attribution for deep networks. In ICML. \n[4] Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., & Batra, D. (2017). Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV.\n[5] Dabkowski, P., & Gal, Y. (2017). Real time image saliency for black box classifiers. In NeurIPS.\n[6] Kindermans, P. J., Hooker, S., Adebayo, J., Alber, M., Sch\u00fctt, K. T., D\u00e4hne, S., ... & Kim, B. (2019). The (un) reliability of saliency methods. In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning (pp. 267-280). Springer, Cham.\n[7] Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I., Hardt, M., & Kim, B. (2018). Sanity checks for saliency maps. In NeurIPS\n[8] Lee, K., Lee, K., Lee, H., & Shin, J. (2018). A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In NeurIPS"}, "signatures": ["ICLR.cc/2020/Conference/Paper1995/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1995/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["amosy3@gmail.com", "gal.chechik@gmail.com"], "title": "Using Explainabilty to Detect Adversarial Attacks", "authors": ["Ohad Amosy and Gal Chechik"], "pdf": "/pdf/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "TL;DR": "A novel adversarial detection approach, which uses explainability methods to identify images whose explanations are inconsistent with the predicted class.  ", "abstract": "Deep learning models are often sensitive to adversarial attacks, where carefully-designed input samples can cause the system to produce incorrect decisions. Here we focus on the problem of detecting attacks, rather than robust classification, since detecting that an attack occurs may be even more important than avoiding misclassification. We build on advances in explainability, where activity-map-like explanations are used to justify and validate decisions, by highlighting features that are involved with a classification decision. The key observation is that it is hard to create explanations for incorrect decisions.  We propose EXAID, a novel attack-detection approach, which uses model explainability to identify images whose explanations are inconsistent with the predicted class. Specifically, we use SHAP, which uses Shapley values in the space of the input image, to identify which input features contribute to a class decision. Interestingly, this approach does not require to modify the attacked model, and it can be applied without modelling a specific attack. It can therefore be applied successfully to detect unfamiliar attacks, that were unknown at the time the detection model was designed. We evaluate EXAID on two benchmark datasets CIFAR-10 and SVHN, and against three leading attack techniques, FGSM, PGD and C&W. We find that EXAID improves over the SoTA detection methods by a large margin across a wide range of noise levels, improving detection from 70% to over 90% for small perturbations.", "keywords": ["adversarial", "detection", "explainability"], "paperhash": "chechik|using_explainabilty_to_detect_adversarial_attacks", "original_pdf": "/attachment/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "_bibtex": "@misc{\nchechik2020using,\ntitle={Using Explainabilty to Detect Adversarial Attacks},\nauthor={Ohad Amosy and Gal Chechik},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xu6yStPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xu6yStPH", "replyto": "B1xu6yStPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1995/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1995/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575442650209, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1995/Reviewers"], "noninvitees": [], "tcdate": 1570237729282, "tmdate": 1575442650221, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1995/-/Official_Review"}}}, {"id": "HJeuJ1NX9S", "original": null, "number": 4, "cdate": 1572187871767, "ddate": null, "tcdate": 1572187871767, "tmdate": 1572972396753, "tddate": null, "forum": "B1xu6yStPH", "replyto": "B1xu6yStPH", "invitation": "ICLR.cc/2020/Conference/Paper1995/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1995", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper proposes a method to check whether a model is under attack by using state of the art explainability model, SHAP. They evaluated their technique using CIFAR-10 and SVHN w.r.t. 5 baseline techniques. They showed their method outperforms all the other baselines with a significant margin.\n\n+ Overall I think the paper made a valuable contribution to the adversarial ML literature. Using explainability to detect the presence of adversarial attacks seems like a nice intuitive idea and the results show that it indeed works. \n\n-\tHowever, the contribution of the paper is rather incremental. They just used SHAPE to adversarial and negative examples. I do not see any insight while explaining the results. \n\n-\tWhy under PGD and FGSM attack under higher noise, the proposed technique is similar or slightly worse than Lid and Mohalonabis baselines?\n\n-\tI would also like to see how these results hold good for a complicated dataset like ImageNet\n\n\t\t\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1995/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1995/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["amosy3@gmail.com", "gal.chechik@gmail.com"], "title": "Using Explainabilty to Detect Adversarial Attacks", "authors": ["Ohad Amosy and Gal Chechik"], "pdf": "/pdf/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "TL;DR": "A novel adversarial detection approach, which uses explainability methods to identify images whose explanations are inconsistent with the predicted class.  ", "abstract": "Deep learning models are often sensitive to adversarial attacks, where carefully-designed input samples can cause the system to produce incorrect decisions. Here we focus on the problem of detecting attacks, rather than robust classification, since detecting that an attack occurs may be even more important than avoiding misclassification. We build on advances in explainability, where activity-map-like explanations are used to justify and validate decisions, by highlighting features that are involved with a classification decision. The key observation is that it is hard to create explanations for incorrect decisions.  We propose EXAID, a novel attack-detection approach, which uses model explainability to identify images whose explanations are inconsistent with the predicted class. Specifically, we use SHAP, which uses Shapley values in the space of the input image, to identify which input features contribute to a class decision. Interestingly, this approach does not require to modify the attacked model, and it can be applied without modelling a specific attack. It can therefore be applied successfully to detect unfamiliar attacks, that were unknown at the time the detection model was designed. We evaluate EXAID on two benchmark datasets CIFAR-10 and SVHN, and against three leading attack techniques, FGSM, PGD and C&W. We find that EXAID improves over the SoTA detection methods by a large margin across a wide range of noise levels, improving detection from 70% to over 90% for small perturbations.", "keywords": ["adversarial", "detection", "explainability"], "paperhash": "chechik|using_explainabilty_to_detect_adversarial_attacks", "original_pdf": "/attachment/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "_bibtex": "@misc{\nchechik2020using,\ntitle={Using Explainabilty to Detect Adversarial Attacks},\nauthor={Ohad Amosy and Gal Chechik},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xu6yStPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xu6yStPH", "replyto": "B1xu6yStPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1995/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1995/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575442650209, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1995/Reviewers"], "noninvitees": [], "tcdate": 1570237729282, "tmdate": 1575442650221, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1995/-/Official_Review"}}}, {"id": "SylGovyRFS", "original": null, "number": 2, "cdate": 1571841946212, "ddate": null, "tcdate": 1571841946212, "tmdate": 1571841946212, "tddate": null, "forum": "B1xu6yStPH", "replyto": "rJeIFIAcYS", "invitation": "ICLR.cc/2020/Conference/Paper1995/-/Official_Comment", "content": {"comment": "Thank you for your comment. \n\nThere is a fundamental distinction that should be stressed between two different tasks:  (A) Build robustness against an attack, and (B) detect that an attack was made. While the tasks are related, they are fundamentally different. \n\nThe current paper discusses attack detection. The question points out that the results differ from a model-robustness paper, which is expected.\n\nMore specifically: Consistent with previous papers, we do find that running Nattack on our model, the success rate of the attack was 100% on CIFAR-10. However, the current paper aims to *detect* successful adversarial examples rather than make a model more robust.  Also, we did not use Nattack to attack the robust LID model that was used in the Nattack paper (which has accuracy of 66.9%), but to attack our base model, which is unprotected, and has accuracy of 87%. We used our own model instead of the robust model to maintain consistency across the rest of the experiments. In this setup, which was justified in the paper, the attack was not aimed to evade LID detection, so it isn't surprising Nattack didn't completely evade LID detector.  \n", "title": "Attack detection and Model robustness are different tasks"}, "signatures": ["ICLR.cc/2020/Conference/Paper1995/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1995/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["amosy3@gmail.com", "gal.chechik@gmail.com"], "title": "Using Explainabilty to Detect Adversarial Attacks", "authors": ["Ohad Amosy and Gal Chechik"], "pdf": "/pdf/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "TL;DR": "A novel adversarial detection approach, which uses explainability methods to identify images whose explanations are inconsistent with the predicted class.  ", "abstract": "Deep learning models are often sensitive to adversarial attacks, where carefully-designed input samples can cause the system to produce incorrect decisions. Here we focus on the problem of detecting attacks, rather than robust classification, since detecting that an attack occurs may be even more important than avoiding misclassification. We build on advances in explainability, where activity-map-like explanations are used to justify and validate decisions, by highlighting features that are involved with a classification decision. The key observation is that it is hard to create explanations for incorrect decisions.  We propose EXAID, a novel attack-detection approach, which uses model explainability to identify images whose explanations are inconsistent with the predicted class. Specifically, we use SHAP, which uses Shapley values in the space of the input image, to identify which input features contribute to a class decision. Interestingly, this approach does not require to modify the attacked model, and it can be applied without modelling a specific attack. It can therefore be applied successfully to detect unfamiliar attacks, that were unknown at the time the detection model was designed. We evaluate EXAID on two benchmark datasets CIFAR-10 and SVHN, and against three leading attack techniques, FGSM, PGD and C&W. We find that EXAID improves over the SoTA detection methods by a large margin across a wide range of noise levels, improving detection from 70% to over 90% for small perturbations.", "keywords": ["adversarial", "detection", "explainability"], "paperhash": "chechik|using_explainabilty_to_detect_adversarial_attacks", "original_pdf": "/attachment/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "_bibtex": "@misc{\nchechik2020using,\ntitle={Using Explainabilty to Detect Adversarial Attacks},\nauthor={Ohad Amosy and Gal Chechik},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xu6yStPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xu6yStPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1995/Authors", "ICLR.cc/2020/Conference/Paper1995/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1995/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1995/Reviewers", "ICLR.cc/2020/Conference/Paper1995/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1995/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1995/Authors|ICLR.cc/2020/Conference/Paper1995/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147853, "tmdate": 1576860540966, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1995/Authors", "ICLR.cc/2020/Conference/Paper1995/Reviewers", "ICLR.cc/2020/Conference/Paper1995/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1995/-/Official_Comment"}}}, {"id": "rJeIFIAcYS", "original": null, "number": 2, "cdate": 1571640957925, "ddate": null, "tcdate": 1571640957925, "tmdate": 1571641045034, "tddate": null, "forum": "B1xu6yStPH", "replyto": "Hyxa0GMcFH", "invitation": "ICLR.cc/2020/Conference/Paper1995/-/Public_Comment", "content": {"comment": "Sorry, I find the result about Nattack in terms of LID is strange and unconvincing.\n\nAs the reported result by the work of Nattack , Nattck has broken the detection of LID with the attack success rate of 100%. That is, the result of LID  on Nattack is 0%. \n\nHowever, as the reply shown, the result of LID  on Nattack reported by the authors is 67%, which is close with the clean accuracy (66.9%) reported by the work of Nattack and has a big gap with the previous result (0%). Maybe the minor adjustments make something wrong for Nattack.\n\n", "title": "Strange results"}, "signatures": ["~Anthony_Wittmer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anthony_Wittmer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["amosy3@gmail.com", "gal.chechik@gmail.com"], "title": "Using Explainabilty to Detect Adversarial Attacks", "authors": ["Ohad Amosy and Gal Chechik"], "pdf": "/pdf/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "TL;DR": "A novel adversarial detection approach, which uses explainability methods to identify images whose explanations are inconsistent with the predicted class.  ", "abstract": "Deep learning models are often sensitive to adversarial attacks, where carefully-designed input samples can cause the system to produce incorrect decisions. Here we focus on the problem of detecting attacks, rather than robust classification, since detecting that an attack occurs may be even more important than avoiding misclassification. We build on advances in explainability, where activity-map-like explanations are used to justify and validate decisions, by highlighting features that are involved with a classification decision. The key observation is that it is hard to create explanations for incorrect decisions.  We propose EXAID, a novel attack-detection approach, which uses model explainability to identify images whose explanations are inconsistent with the predicted class. Specifically, we use SHAP, which uses Shapley values in the space of the input image, to identify which input features contribute to a class decision. Interestingly, this approach does not require to modify the attacked model, and it can be applied without modelling a specific attack. It can therefore be applied successfully to detect unfamiliar attacks, that were unknown at the time the detection model was designed. We evaluate EXAID on two benchmark datasets CIFAR-10 and SVHN, and against three leading attack techniques, FGSM, PGD and C&W. We find that EXAID improves over the SoTA detection methods by a large margin across a wide range of noise levels, improving detection from 70% to over 90% for small perturbations.", "keywords": ["adversarial", "detection", "explainability"], "paperhash": "chechik|using_explainabilty_to_detect_adversarial_attacks", "original_pdf": "/attachment/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "_bibtex": "@misc{\nchechik2020using,\ntitle={Using Explainabilty to Detect Adversarial Attacks},\nauthor={Ohad Amosy and Gal Chechik},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xu6yStPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xu6yStPH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504186614, "tmdate": 1576860574472, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1995/Authors", "ICLR.cc/2020/Conference/Paper1995/Reviewers", "ICLR.cc/2020/Conference/Paper1995/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1995/-/Public_Comment"}}}, {"id": "Hyxa0GMcFH", "original": null, "number": 1, "cdate": 1571590868945, "ddate": null, "tcdate": 1571590868945, "tmdate": 1571590868945, "tddate": null, "forum": "B1xu6yStPH", "replyto": "r1l5cbcsPB", "invitation": "ICLR.cc/2020/Conference/Paper1995/-/Official_Comment", "content": {"comment": "Thank you for your important feedback and helpful suggestion! \n\nWe originally discussed Nattack [1] when explaining the attack scenario, but did not compare with it directly to keep the focus on attack detection, rather than model robustness.\n\nFollowing your comment, we further evaluated our detection approach with the [1] attacks. Specifically, we used the implementation provided by the authors for attacking LID (github.com/Cold-Winter/Nattack/tree/master/lid) using their best published hyper parameters, and made minor adjustments to fit our pytorch model. We also reduced the population size from 300 to 200 so the attack model fit our K40 GPU RAM. We evaluated our defense using the successful adversarial images.\n\nFor Nattack, \u00a0EXAID (our approach) again consistently outperforms other detection baselines, on both CIFAR and SVHN, while keeping detection rates at the same ball park as with other attacks. Specifically, on CIFAR, EXAID improves detection AUC over the baselines, from 0.70 (ANR), 0.68 (unsupervised LID), 0.67 (original LID), 0.43 (unsupervised Mahalanobis) and 0.46 (original Mahalanobis) to *0.96* (EXAID familiar) and 0.89 (EXAID unknown).\n\nSimilarly, on SVHN, EXAID improves from 0.53 (ANR), 0.63 (expand LID), 0.49 (original LID), 0.35 (expand Mahalanobis) and 0.56 (original Mahalanobis), to *0.95* (EXAID unknown) and 0.78 (EXAID familiar).\n\nWe will add detailed results to the next version of the paper.", "title": "EXAID also detects Nattack, outperforms baselines. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1995/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1995/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["amosy3@gmail.com", "gal.chechik@gmail.com"], "title": "Using Explainabilty to Detect Adversarial Attacks", "authors": ["Ohad Amosy and Gal Chechik"], "pdf": "/pdf/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "TL;DR": "A novel adversarial detection approach, which uses explainability methods to identify images whose explanations are inconsistent with the predicted class.  ", "abstract": "Deep learning models are often sensitive to adversarial attacks, where carefully-designed input samples can cause the system to produce incorrect decisions. Here we focus on the problem of detecting attacks, rather than robust classification, since detecting that an attack occurs may be even more important than avoiding misclassification. We build on advances in explainability, where activity-map-like explanations are used to justify and validate decisions, by highlighting features that are involved with a classification decision. The key observation is that it is hard to create explanations for incorrect decisions.  We propose EXAID, a novel attack-detection approach, which uses model explainability to identify images whose explanations are inconsistent with the predicted class. Specifically, we use SHAP, which uses Shapley values in the space of the input image, to identify which input features contribute to a class decision. Interestingly, this approach does not require to modify the attacked model, and it can be applied without modelling a specific attack. It can therefore be applied successfully to detect unfamiliar attacks, that were unknown at the time the detection model was designed. We evaluate EXAID on two benchmark datasets CIFAR-10 and SVHN, and against three leading attack techniques, FGSM, PGD and C&W. We find that EXAID improves over the SoTA detection methods by a large margin across a wide range of noise levels, improving detection from 70% to over 90% for small perturbations.", "keywords": ["adversarial", "detection", "explainability"], "paperhash": "chechik|using_explainabilty_to_detect_adversarial_attacks", "original_pdf": "/attachment/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "_bibtex": "@misc{\nchechik2020using,\ntitle={Using Explainabilty to Detect Adversarial Attacks},\nauthor={Ohad Amosy and Gal Chechik},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xu6yStPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xu6yStPH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1995/Authors", "ICLR.cc/2020/Conference/Paper1995/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1995/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1995/Reviewers", "ICLR.cc/2020/Conference/Paper1995/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1995/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1995/Authors|ICLR.cc/2020/Conference/Paper1995/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147853, "tmdate": 1576860540966, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1995/Authors", "ICLR.cc/2020/Conference/Paper1995/Reviewers", "ICLR.cc/2020/Conference/Paper1995/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1995/-/Official_Comment"}}}, {"id": "r1l5cbcsPB", "original": null, "number": 1, "cdate": 1569591698436, "ddate": null, "tcdate": 1569591698436, "tmdate": 1569682447336, "tddate": null, "forum": "B1xu6yStPH", "replyto": "B1xu6yStPH", "invitation": "ICLR.cc/2020/Conference/Paper1995/-/Public_Comment", "content": {"comment": "Hi, this paper is an interesting work. However, I have some questions about the evalation.\n\nI think a stronger attack is missing in the evalation, i.e., Nattack[1]. Since the baseline LID (Ma et al., 2018). has been broken by Nattck with the attack success rate of 100%, I have the question whether the proposed method provides the true robustness against the adversarial examples.  That it, I am wondering does the proposed method suffer from the same attack?\n\nIt would be solid to include further experiments on the robustness against Nattack in the paper. \n\n[1] NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks. ICML 2019\n", "title": "Interesting work, how about evalating on Nattack?"}, "signatures": ["~Anthony_Wittmer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anthony_Wittmer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["amosy3@gmail.com", "gal.chechik@gmail.com"], "title": "Using Explainabilty to Detect Adversarial Attacks", "authors": ["Ohad Amosy and Gal Chechik"], "pdf": "/pdf/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "TL;DR": "A novel adversarial detection approach, which uses explainability methods to identify images whose explanations are inconsistent with the predicted class.  ", "abstract": "Deep learning models are often sensitive to adversarial attacks, where carefully-designed input samples can cause the system to produce incorrect decisions. Here we focus on the problem of detecting attacks, rather than robust classification, since detecting that an attack occurs may be even more important than avoiding misclassification. We build on advances in explainability, where activity-map-like explanations are used to justify and validate decisions, by highlighting features that are involved with a classification decision. The key observation is that it is hard to create explanations for incorrect decisions.  We propose EXAID, a novel attack-detection approach, which uses model explainability to identify images whose explanations are inconsistent with the predicted class. Specifically, we use SHAP, which uses Shapley values in the space of the input image, to identify which input features contribute to a class decision. Interestingly, this approach does not require to modify the attacked model, and it can be applied without modelling a specific attack. It can therefore be applied successfully to detect unfamiliar attacks, that were unknown at the time the detection model was designed. We evaluate EXAID on two benchmark datasets CIFAR-10 and SVHN, and against three leading attack techniques, FGSM, PGD and C&W. We find that EXAID improves over the SoTA detection methods by a large margin across a wide range of noise levels, improving detection from 70% to over 90% for small perturbations.", "keywords": ["adversarial", "detection", "explainability"], "paperhash": "chechik|using_explainabilty_to_detect_adversarial_attacks", "original_pdf": "/attachment/8cee59947d1d42acff775ded49e79e9c52f396f5.pdf", "_bibtex": "@misc{\nchechik2020using,\ntitle={Using Explainabilty to Detect Adversarial Attacks},\nauthor={Ohad Amosy and Gal Chechik},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xu6yStPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xu6yStPH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504186614, "tmdate": 1576860574472, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1995/Authors", "ICLR.cc/2020/Conference/Paper1995/Reviewers", "ICLR.cc/2020/Conference/Paper1995/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1995/-/Public_Comment"}}}], "count": 10}