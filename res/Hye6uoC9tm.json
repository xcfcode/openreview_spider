{"notes": [{"id": "Hye6uoC9tm", "original": "HJgNi45qt7", "number": 394, "cdate": 1538087796650, "ddate": null, "tcdate": 1538087796650, "tmdate": 1545355383621, "tddate": null, "forum": "Hye6uoC9tm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Incremental Hierarchical Reinforcement Learning with Multitask LMDPs", "abstract": "Exploration is a well known challenge in Reinforcement Learning. One principled way of overcoming this challenge is to find a hierarchical abstraction of the base problem and explore at these higher levels, rather than in the space of primitives. However, discovering a deep abstraction autonomously remains a largely unsolved problem, with practitioners typically hand-crafting these hierarchical control architectures. Recent work with multitask linear Markov decision processes, allows for the autonomous discovery of deep hierarchical abstractions, but operates exclusively in the offline setting. By extending this work, we develop an agent that is capable of incrementally growing a hierarchical representation, and using its experience to date to improve exploration.", "keywords": ["Reinforcement learning", "hierarchy", "linear markov decision process", "lmdl", "subtask discovery", "incremental"], "authorids": ["adamchristopherearle@gmail.com", "andrew.saxe@psy.ox.ac.uk", "benjros@gmail.com"], "authors": ["Adam C Earle", "Andrew M Saxe", "Benjamin Rosman"], "TL;DR": "We develop an agent capable of incrementally growing a hierarchical representation, and using its experience to date to improve exploration.", "pdf": "/pdf/9705b6ec7657158f7d433b7b6df194ba813c0028.pdf", "paperhash": "earle|incremental_hierarchical_reinforcement_learning_with_multitask_lmdps", "_bibtex": "@misc{\nearle2019incremental,\ntitle={Incremental Hierarchical Reinforcement Learning with Multitask {LMDP}s},\nauthor={Adam C Earle and Andrew M Saxe and Benjamin Rosman},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye6uoC9tm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1e83lUWlN", "original": null, "number": 1, "cdate": 1544802478270, "ddate": null, "tcdate": 1544802478270, "tmdate": 1545354526393, "tddate": null, "forum": "Hye6uoC9tm", "replyto": "Hye6uoC9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper394/Meta_Review", "content": {"metareview": "The paper studies an interesting problem with a reasonable solution.  However, reviewers feel that the technical contributions are somewhat incremental.  Furthermore, the empirical study would have been stronger with more proper baselines (simple adaptation to the multitask setting), and on problems beyond the simple grid worlds.  In addition, reviewers also find the presentation should be improved substantially.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Interesting work, but contribution  seems incremental"}, "signatures": ["ICLR.cc/2019/Conference/Paper394/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper394/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental Hierarchical Reinforcement Learning with Multitask LMDPs", "abstract": "Exploration is a well known challenge in Reinforcement Learning. One principled way of overcoming this challenge is to find a hierarchical abstraction of the base problem and explore at these higher levels, rather than in the space of primitives. However, discovering a deep abstraction autonomously remains a largely unsolved problem, with practitioners typically hand-crafting these hierarchical control architectures. Recent work with multitask linear Markov decision processes, allows for the autonomous discovery of deep hierarchical abstractions, but operates exclusively in the offline setting. By extending this work, we develop an agent that is capable of incrementally growing a hierarchical representation, and using its experience to date to improve exploration.", "keywords": ["Reinforcement learning", "hierarchy", "linear markov decision process", "lmdl", "subtask discovery", "incremental"], "authorids": ["adamchristopherearle@gmail.com", "andrew.saxe@psy.ox.ac.uk", "benjros@gmail.com"], "authors": ["Adam C Earle", "Andrew M Saxe", "Benjamin Rosman"], "TL;DR": "We develop an agent capable of incrementally growing a hierarchical representation, and using its experience to date to improve exploration.", "pdf": "/pdf/9705b6ec7657158f7d433b7b6df194ba813c0028.pdf", "paperhash": "earle|incremental_hierarchical_reinforcement_learning_with_multitask_lmdps", "_bibtex": "@misc{\nearle2019incremental,\ntitle={Incremental Hierarchical Reinforcement Learning with Multitask {LMDP}s},\nauthor={Adam C Earle and Andrew M Saxe and Benjamin Rosman},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye6uoC9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper394/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353232542, "tddate": null, "super": null, "final": null, "reply": {"forum": "Hye6uoC9tm", "replyto": "Hye6uoC9tm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper394/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper394/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper394/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353232542}}}, {"id": "r1gHwX8ZTX", "original": null, "number": 3, "cdate": 1541657437014, "ddate": null, "tcdate": 1541657437014, "tmdate": 1541657437014, "tddate": null, "forum": "Hye6uoC9tm", "replyto": "Hye6uoC9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper394/Official_Review", "content": {"title": "Interesting ideas; contribution is marginal over previous work; lots of questions about hierarchy construction and utility", "review": "Major comments:\n\nThis paper builds on previous work in hierarchical LMDPs and extends the core ideas to an online setting.  Essentially, we incrementally construct a hierarchy by adding new states to upper-level MDPs every once in a while; these are loosely initialized and the parameters are then refined with additional experience.\n\nOverall, I felt that this paper lacked a substantial enough contribution. \n\n* The key contributions over previous work seems to be entirely contained in Sec. 3.1 and 3.2: (1) when we have visited k new states, add a new state to the hierarchy, and (2) initialize its parameters with intuitive values.\n\n* To me, this level of contribution is below the bar for ICLR.  The ideas seem simplistic and likely to work only in the simplest of domains.\n\n* The improvement over previous work is marginal.\n\n* I think the paper is lacking in clarity.  I do not think I could re-implement the paper, given the level of detail presented.\n\n* I was very disappointed in the experiments.  Not only were they on gridworld-like domains (see below), but it was not clear if the improvements were significant in any way.\n\n* While I thought the discussion in Sec. 3.3 was interesting, it didn't seem to be a \"contribution\"; it felt like some adhoc thoughts.\n\n* Keeping per-state counts is only workable in small state space domains.\n\nMinor comments:\n\nPlease fix the formatting of your citations.\n\nThere are numerous typos and spelling errors.  Please correct them.\n\nI am strongly opposed to the use of gridworlds, or anything like them, in modern RL research.  While many ideas work fine in small, toy domains, they simply do not scale.  As a field, we need to move past them and focus more on algorithms and ideas that have more practical relevance.\n\nPros:\n+ Core ideas seem promising\n+ Leveraging mathematical structure is a great strategy for constructing algorithms with desirable properties\n\nCons:\n- Very limited experimental results\n- Not clear if improvements are significant\n- Hierarchy construction seems to be too limited to work in any reasonably sized problem\n- No evidence, theoretical or otherwise, is given to suggest that this particular hierarchy construction method is any better than any other method\n\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper394/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental Hierarchical Reinforcement Learning with Multitask LMDPs", "abstract": "Exploration is a well known challenge in Reinforcement Learning. One principled way of overcoming this challenge is to find a hierarchical abstraction of the base problem and explore at these higher levels, rather than in the space of primitives. However, discovering a deep abstraction autonomously remains a largely unsolved problem, with practitioners typically hand-crafting these hierarchical control architectures. Recent work with multitask linear Markov decision processes, allows for the autonomous discovery of deep hierarchical abstractions, but operates exclusively in the offline setting. By extending this work, we develop an agent that is capable of incrementally growing a hierarchical representation, and using its experience to date to improve exploration.", "keywords": ["Reinforcement learning", "hierarchy", "linear markov decision process", "lmdl", "subtask discovery", "incremental"], "authorids": ["adamchristopherearle@gmail.com", "andrew.saxe@psy.ox.ac.uk", "benjros@gmail.com"], "authors": ["Adam C Earle", "Andrew M Saxe", "Benjamin Rosman"], "TL;DR": "We develop an agent capable of incrementally growing a hierarchical representation, and using its experience to date to improve exploration.", "pdf": "/pdf/9705b6ec7657158f7d433b7b6df194ba813c0028.pdf", "paperhash": "earle|incremental_hierarchical_reinforcement_learning_with_multitask_lmdps", "_bibtex": "@misc{\nearle2019incremental,\ntitle={Incremental Hierarchical Reinforcement Learning with Multitask {LMDP}s},\nauthor={Adam C Earle and Andrew M Saxe and Benjamin Rosman},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye6uoC9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper394/Official_Review", "cdate": 1542234471488, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hye6uoC9tm", "replyto": "Hye6uoC9tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper394/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335714198, "tmdate": 1552335714198, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper394/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJxYDLd52m", "original": null, "number": 2, "cdate": 1541207649437, "ddate": null, "tcdate": 1541207649437, "tmdate": 1541534032550, "tddate": null, "forum": "Hye6uoC9tm", "replyto": "Hye6uoC9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper394/Official_Review", "content": {"title": "This work extend previous work with multitask linear MDP for discovery of deep hierarchical abstractions from offline setting to a online setting. It lacks clarify in presentation and has limited experiments and anlaysis. ", "review": "pros:\nThe experiment results demonstrate better performance of the proposed method.\nCons: \n1. It does not compare with any existing HRL (with simple adaptation to multitask settings), except for Q learning under different exploration strategies. \n\n2. Also the method is tested only on one type of domain-gridworld, which seems very limited, especially for supporting the claims, including \u201cthe proposed method can uncover a deep hierarchical abstraction of task ensemble in a complete online fashion\u201d and \u201cthe process of learning the hierarchical structure online does not significantly slow learning\u201d\n\n3. There is no implementation details of the proposed in a algorithmic form.\n\n4. There is also no ablation test or discussion of parameters that affect the performance of the experiment results, for example, learning rate, size of the domain. \n\n5. There are a lot of notations used without definition. The paper should be more self-contained by providing accurate definition and citations.\n\nFor example, what are are Interior state,  boundary state, finite exit problem\nIs V(s) computed with based on R_i or R, or both?\nWhat is the control cost refer to? Is it related to R_i or R somehow?\n\n6. Some references are cited wrongly. \nMachado et al (2018) cited is not about HRL. Please double check\nThe references should be updated to reflect the latest information, for example,\nIt might be better to cite the AAAI17 version of the option-critic architectures \nBetween MDPs and Semi-MDP: A framework for Temporal Abstraction in Reinforcement learning should use the 1999 version \n\n \n7. Minor issues:\ntask it is -> task is\nZ^l \\in R^{N\\times N}\n Howver -> however\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper394/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental Hierarchical Reinforcement Learning with Multitask LMDPs", "abstract": "Exploration is a well known challenge in Reinforcement Learning. One principled way of overcoming this challenge is to find a hierarchical abstraction of the base problem and explore at these higher levels, rather than in the space of primitives. However, discovering a deep abstraction autonomously remains a largely unsolved problem, with practitioners typically hand-crafting these hierarchical control architectures. Recent work with multitask linear Markov decision processes, allows for the autonomous discovery of deep hierarchical abstractions, but operates exclusively in the offline setting. By extending this work, we develop an agent that is capable of incrementally growing a hierarchical representation, and using its experience to date to improve exploration.", "keywords": ["Reinforcement learning", "hierarchy", "linear markov decision process", "lmdl", "subtask discovery", "incremental"], "authorids": ["adamchristopherearle@gmail.com", "andrew.saxe@psy.ox.ac.uk", "benjros@gmail.com"], "authors": ["Adam C Earle", "Andrew M Saxe", "Benjamin Rosman"], "TL;DR": "We develop an agent capable of incrementally growing a hierarchical representation, and using its experience to date to improve exploration.", "pdf": "/pdf/9705b6ec7657158f7d433b7b6df194ba813c0028.pdf", "paperhash": "earle|incremental_hierarchical_reinforcement_learning_with_multitask_lmdps", "_bibtex": "@misc{\nearle2019incremental,\ntitle={Incremental Hierarchical Reinforcement Learning with Multitask {LMDP}s},\nauthor={Adam C Earle and Andrew M Saxe and Benjamin Rosman},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye6uoC9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper394/Official_Review", "cdate": 1542234471488, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hye6uoC9tm", "replyto": "Hye6uoC9tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper394/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335714198, "tmdate": 1552335714198, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper394/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rylvQpg5n7", "original": null, "number": 1, "cdate": 1541176606730, "ddate": null, "tcdate": 1541176606730, "tmdate": 1541534032346, "tddate": null, "forum": "Hye6uoC9tm", "replyto": "Hye6uoC9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper394/Official_Review", "content": {"title": "Nice idea and combination of methods. Difficult to assess significance", "review": "This work builds on the ICML paper from Saxe et al (2017) in which the compositionality property of LMDPs was exploited to solve multi-task hierarchies. The paper extends this work by proposing a method that learns incrementally such hierarchies instead of pre-defining them by design. Some experimental results illustrate the method on two toy problems, a 1D corridor and a corridor of rooms.\n\nThe paper deals with an interesting and hard problem. Learning hierarchies while solving an MDP is a much harder problem than solving the flat MDP or solving the hierarchical MDP. The authors leverage the compositionality of optimal controls of the LMDP framework to learn incrementally the hierarchies. Surprisingly, the proposed method not only learns those hierarchies, but also is more effective in terms of exploration.\n\nOn the positive side, the main idea is very interesting and has a lot of potential. The authors combine many techniques under the powerful framework of LMDPs such as hierarchical RL, low-rank factorization, and count-based exploration. The authors do a good job describing their approach (at a higher level).\n\nOn the negative side, the paper looks a bit incremental, given the prior existing work. I also found the paper unclear in many aspects lacking some relevant technical details (see below). The narrative is sometimes superficial or focused mainly on intuitions and analogies. Overall, it is difficult to assess the significance of this work and the results give the impression of limited applicability, beyond the described toy problem.\n\n1- First, in order to combine optimal controls, the LMDPs need to be solved for each different boundary state, i.e., do you require to solve as many LMDPs as possible states? If that is the case, I don't think it makes sens to talk about exploration/exploration trade-off, since you really need to visit all states a priori.\n\n2- I cannot understand what is learned and what is required a priori. the authors state that if \"the multi-scale structure of the domain is known a priori, the decomposition (...) explicitly specified\". What does exactly that mean? If what the method does is an incremental version of the low-rank factorization proposed in Earle at al (2018), I think the presentation can be better described in those terms.\n\n2- Regarding exploration/exploitation tradeoff. From section 3, it seems that authors focus on a particular spatial problem and define already some exploration choices. But this means that the choice about when a state is integrated in the current MDP is already done, so no real trade-off exists?\n\n3- The narrative in Section 3.2 is not very rigorous. The authors just mention the computational problem to keep consistency between layers and then just argue that \"in practice\" using count-based exploration everything works. I think a more principle approach is necessary.\n\n4- Experiments I: what do the authors means by \"exploration\"? Is it just Boltzmann exploration? I can think of an exploration strategy that would choose an unseen state with probability 1 and would bring the agent to the goal in one shot.\n\n5- Experiments II: I like the benchmark but, how does the result depend on the structure of the problem? What happens if I the rooms have very different sizes?\n\n6- I miss some references that are very relevant to this work:\n\n- the ICAPS paper \"Hierarchical Linearly-Solvable Markov Decision Problems\" by Jonson et al. seems to be the first proposing a hierarchical embedding of LMDPs.\n\n- other factorization techniques exist, e.g., \"Incremental Stochastic Factorization for Online Reinforcement Learning\", Barreto et al (AAAI' 16), to uncover an MDP structure.\n\nThere are also some minor grammar mistakes:\n\n\"passive dynamics then become\" -> \"passive dynamics then becomes\"\n\"reward function r\" -> \"reward function R\"\n\"Howver\" -> \"However\"\n\"the room contains\" -> \"the room that contains\"\n\"have simply add\" -> \"have simply added\"\n\"spacial and temporal\" -> \"spatial and temporal\"\n...", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper394/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Incremental Hierarchical Reinforcement Learning with Multitask LMDPs", "abstract": "Exploration is a well known challenge in Reinforcement Learning. One principled way of overcoming this challenge is to find a hierarchical abstraction of the base problem and explore at these higher levels, rather than in the space of primitives. However, discovering a deep abstraction autonomously remains a largely unsolved problem, with practitioners typically hand-crafting these hierarchical control architectures. Recent work with multitask linear Markov decision processes, allows for the autonomous discovery of deep hierarchical abstractions, but operates exclusively in the offline setting. By extending this work, we develop an agent that is capable of incrementally growing a hierarchical representation, and using its experience to date to improve exploration.", "keywords": ["Reinforcement learning", "hierarchy", "linear markov decision process", "lmdl", "subtask discovery", "incremental"], "authorids": ["adamchristopherearle@gmail.com", "andrew.saxe@psy.ox.ac.uk", "benjros@gmail.com"], "authors": ["Adam C Earle", "Andrew M Saxe", "Benjamin Rosman"], "TL;DR": "We develop an agent capable of incrementally growing a hierarchical representation, and using its experience to date to improve exploration.", "pdf": "/pdf/9705b6ec7657158f7d433b7b6df194ba813c0028.pdf", "paperhash": "earle|incremental_hierarchical_reinforcement_learning_with_multitask_lmdps", "_bibtex": "@misc{\nearle2019incremental,\ntitle={Incremental Hierarchical Reinforcement Learning with Multitask {LMDP}s},\nauthor={Adam C Earle and Andrew M Saxe and Benjamin Rosman},\nyear={2019},\nurl={https://openreview.net/forum?id=Hye6uoC9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper394/Official_Review", "cdate": 1542234471488, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Hye6uoC9tm", "replyto": "Hye6uoC9tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper394/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335714198, "tmdate": 1552335714198, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper394/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}