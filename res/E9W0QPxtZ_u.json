{"notes": [{"id": "E9W0QPxtZ_u", "original": "CA3Jnk1XvJE", "number": 2695, "cdate": 1601308298693, "ddate": null, "tcdate": 1601308298693, "tmdate": 1614985719490, "tddate": null, "forum": "E9W0QPxtZ_u", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "not-so-big-GAN: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution", "authorids": ["~Seungwook_Han1", "~Akash_Srivastava1", "~Cole_Lincoln_Hurwitz1", "~Prasanna_Sattigeri1", "~David_Daniel_Cox1"], "authors": ["Seungwook Han", "Akash Srivastava", "Cole Lincoln Hurwitz", "Prasanna Sattigeri", "David Daniel Cox"], "keywords": ["deep generative modeling", "GAN", "super-resolution", "wavelet transformation", "energy efficient"], "abstract": "State-of-the-art models for high-resolution image generation, such as BigGAN and VQVAE-2, require an incredible amount of compute resources and/or time (512 TPU-v3 cores) to train, putting them out of reach for the larger research community. On the other hand, GAN-based image super-resolution models, such as ESRGAN, can not only upscale images to high dimensions, but also are efficient to train. In this paper, we present not-so-big-GAN (nsb-GAN), a simple yet cost-effective two-step training framework for deep generative models (DGMs) of high-dimensional natural images. First, we generate images in low-frequency bands by training a sampler in the wavelet domain. Then, we super-resolve these images from the wavelet domain back to the pixel-space with our novel wavelet super-resolution decoder network. Wavelet-based down-sampling method preserves more structural information than pixel-based methods, leading to significantly better generative quality of the low-resolution sampler (e.g., 64\u00d764). Since the sampler and decoder can be trained in parallel and operate on much lower dimensional spaces than end-to-end models, the training cost is substantially reduced. On ImageNet 512\u00d7512, our model achieves a Fr\u00e9chet Inception Distance (FID) of 10.59 \u2013 beating the baseline BigGAN model \u2013 at half the compute (256 TPU-v3 cores).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|notsobiggan_generating_highfidelity_images_on_small_compute_with_waveletbased_superresolution", "one-sentence_summary": "Energy-efficient framework for generating high-fidelity, high-resolution images using wavelet-based super-resolution", "pdf": "/pdf/9e76109e3f9567090b881db9e556c5915994d8b1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=s-bN49Yq2", "_bibtex": "@misc{\nhan2021notsobiggan,\ntitle={not-so-big-{\\{}GAN{\\}}: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution},\nauthor={Seungwook Han and Akash Srivastava and Cole Lincoln Hurwitz and Prasanna Sattigeri and David Daniel Cox},\nyear={2021},\nurl={https://openreview.net/forum?id=E9W0QPxtZ_u}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "m1ayDtZnYUB", "original": null, "number": 1, "cdate": 1610040422426, "ddate": null, "tcdate": 1610040422426, "tmdate": 1610474021258, "tddate": null, "forum": "E9W0QPxtZ_u", "replyto": "E9W0QPxtZ_u", "invitation": "ICLR.cc/2021/Conference/Paper2695/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The reviewers brought up significant concerns that were not resolved by the authors' responses. The concerns are too significant for the paper to be accepted at this time."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "not-so-big-GAN: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution", "authorids": ["~Seungwook_Han1", "~Akash_Srivastava1", "~Cole_Lincoln_Hurwitz1", "~Prasanna_Sattigeri1", "~David_Daniel_Cox1"], "authors": ["Seungwook Han", "Akash Srivastava", "Cole Lincoln Hurwitz", "Prasanna Sattigeri", "David Daniel Cox"], "keywords": ["deep generative modeling", "GAN", "super-resolution", "wavelet transformation", "energy efficient"], "abstract": "State-of-the-art models for high-resolution image generation, such as BigGAN and VQVAE-2, require an incredible amount of compute resources and/or time (512 TPU-v3 cores) to train, putting them out of reach for the larger research community. On the other hand, GAN-based image super-resolution models, such as ESRGAN, can not only upscale images to high dimensions, but also are efficient to train. In this paper, we present not-so-big-GAN (nsb-GAN), a simple yet cost-effective two-step training framework for deep generative models (DGMs) of high-dimensional natural images. First, we generate images in low-frequency bands by training a sampler in the wavelet domain. Then, we super-resolve these images from the wavelet domain back to the pixel-space with our novel wavelet super-resolution decoder network. Wavelet-based down-sampling method preserves more structural information than pixel-based methods, leading to significantly better generative quality of the low-resolution sampler (e.g., 64\u00d764). Since the sampler and decoder can be trained in parallel and operate on much lower dimensional spaces than end-to-end models, the training cost is substantially reduced. On ImageNet 512\u00d7512, our model achieves a Fr\u00e9chet Inception Distance (FID) of 10.59 \u2013 beating the baseline BigGAN model \u2013 at half the compute (256 TPU-v3 cores).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|notsobiggan_generating_highfidelity_images_on_small_compute_with_waveletbased_superresolution", "one-sentence_summary": "Energy-efficient framework for generating high-fidelity, high-resolution images using wavelet-based super-resolution", "pdf": "/pdf/9e76109e3f9567090b881db9e556c5915994d8b1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=s-bN49Yq2", "_bibtex": "@misc{\nhan2021notsobiggan,\ntitle={not-so-big-{\\{}GAN{\\}}: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution},\nauthor={Seungwook Han and Akash Srivastava and Cole Lincoln Hurwitz and Prasanna Sattigeri and David Daniel Cox},\nyear={2021},\nurl={https://openreview.net/forum?id=E9W0QPxtZ_u}\n}"}, "tags": [], "invitation": {"reply": {"forum": "E9W0QPxtZ_u", "replyto": "E9W0QPxtZ_u", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040422412, "tmdate": 1610474021242, "id": "ICLR.cc/2021/Conference/Paper2695/-/Decision"}}}, {"id": "rVG0ALVHmb1", "original": null, "number": 1, "cdate": 1603865586572, "ddate": null, "tcdate": 1603865586572, "tmdate": 1606803251731, "tddate": null, "forum": "E9W0QPxtZ_u", "replyto": "E9W0QPxtZ_u", "invitation": "ICLR.cc/2021/Conference/Paper2695/-/Official_Review", "content": {"title": "A cost-effective two-step GAN framework", "review": "**Summary**\n\nThis paper proposes a cost-effective two-step training GAN framework (NSB-GAN). NSB-GAN contains a learned sampler in the wavelet domain, and a decoder to super-resolve images from the wavelet domain to the RGB space. Compared with the baseline BigGAN model their method reduces the cost of training and generates higher quality images at 512x512 resolution.\n\n**Strengths**\n- Contributions clearly stated and validated and the paper is clearly written and easy to understand.\n- Comprehensive experiments to show the effectiveness of their method. \n- Cost-effective than BigGAN and performance is good.\n\n**Weaknesses**\n- The idea of this work is not so novel. It seems low-resolution sampler + super-resolution decoder which is a straightforward idea and lacks novelty. And the two parts are trained independently. Why the authors didn't finetune the sampler and decoder networks together in the end so that to get a better sampler from a more correct sampling space.\n- For (x4) super-resolution networks, e.g., EDSRGAN, they often fail to generate a high-quality HR image with rich details when the input LR image is low quality. Therefore, what if the LR images from the learned sampler contain do not contains any desired patterns/textures?\n- For the NSB-GAN sampler, the authors use a batch size of 512, which is much smaller than that of BigGAN (2048). The authors should give an ablation study to analyze the effect of batch size on the proposed framework.\n- From Figure 2 in Appendix A, it seems that wavelet-based downsampling loses more structural information than pixel-based downsampling, which is not consistent with the description in the paper and violates the motivation of wavelet-based super-resolution as well. Please check it.\n- I can not understand the claim that \"NSB-GAN models reduce the training compute budget by up to four times\" in the caption of Table 2. Because Table 2 shows the compute of NSB-GAN is half of BigGAN, not 1/4.\n- I notice that NSB-GAN outperforms BigGAN at 512 resolution in terms of min FID, but it shows worse performance at 256 resolution. Which leads to these results? The authors are suggested to give some analysis on this problem.\n\n**Post rebuttal**\nI appreciate that the authors answer my questions. After reading through the rebuttal and other reviews,  I partly agree with R1's comments and I would like to downgrade my score by 1. My main concern is the novelty of this method. I disagree that the decoupling of generation from upsampling is interesting, it seems more like an engineering problem. Besides, I found that the authors changed the images in Figure 2 in Appendix A, not only changed the order, which is not consistent with their explanation in rebuttal. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2695/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2695/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "not-so-big-GAN: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution", "authorids": ["~Seungwook_Han1", "~Akash_Srivastava1", "~Cole_Lincoln_Hurwitz1", "~Prasanna_Sattigeri1", "~David_Daniel_Cox1"], "authors": ["Seungwook Han", "Akash Srivastava", "Cole Lincoln Hurwitz", "Prasanna Sattigeri", "David Daniel Cox"], "keywords": ["deep generative modeling", "GAN", "super-resolution", "wavelet transformation", "energy efficient"], "abstract": "State-of-the-art models for high-resolution image generation, such as BigGAN and VQVAE-2, require an incredible amount of compute resources and/or time (512 TPU-v3 cores) to train, putting them out of reach for the larger research community. On the other hand, GAN-based image super-resolution models, such as ESRGAN, can not only upscale images to high dimensions, but also are efficient to train. In this paper, we present not-so-big-GAN (nsb-GAN), a simple yet cost-effective two-step training framework for deep generative models (DGMs) of high-dimensional natural images. First, we generate images in low-frequency bands by training a sampler in the wavelet domain. Then, we super-resolve these images from the wavelet domain back to the pixel-space with our novel wavelet super-resolution decoder network. Wavelet-based down-sampling method preserves more structural information than pixel-based methods, leading to significantly better generative quality of the low-resolution sampler (e.g., 64\u00d764). Since the sampler and decoder can be trained in parallel and operate on much lower dimensional spaces than end-to-end models, the training cost is substantially reduced. On ImageNet 512\u00d7512, our model achieves a Fr\u00e9chet Inception Distance (FID) of 10.59 \u2013 beating the baseline BigGAN model \u2013 at half the compute (256 TPU-v3 cores).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|notsobiggan_generating_highfidelity_images_on_small_compute_with_waveletbased_superresolution", "one-sentence_summary": "Energy-efficient framework for generating high-fidelity, high-resolution images using wavelet-based super-resolution", "pdf": "/pdf/9e76109e3f9567090b881db9e556c5915994d8b1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=s-bN49Yq2", "_bibtex": "@misc{\nhan2021notsobiggan,\ntitle={not-so-big-{\\{}GAN{\\}}: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution},\nauthor={Seungwook Han and Akash Srivastava and Cole Lincoln Hurwitz and Prasanna Sattigeri and David Daniel Cox},\nyear={2021},\nurl={https://openreview.net/forum?id=E9W0QPxtZ_u}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "E9W0QPxtZ_u", "replyto": "E9W0QPxtZ_u", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2695/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090524, "tmdate": 1606915778800, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2695/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2695/-/Official_Review"}}}, {"id": "7gLqdeeDMub", "original": null, "number": 6, "cdate": 1606181088605, "ddate": null, "tcdate": 1606181088605, "tmdate": 1606181088605, "tddate": null, "forum": "E9W0QPxtZ_u", "replyto": "E9W0QPxtZ_u", "invitation": "ICLR.cc/2021/Conference/Paper2695/-/Official_Comment", "content": {"title": "Summary", "comment": "Here is a summary of all the new experiments and changes that we made last week in response to the reviewers\u2019 feedback and suggestions. Before the rebuttal period ends tomorrow, it will be very helpful to know if there are any new questions about our work or the new experiments that we ran on your suggestions.\n\n1. We corrected the typo in the caption of Figure 2 in the appendix (that led to the doubts of R2 and R3 about the results).\n2. We have added a new experiment to Appendix I as per the suggestion of R4. Here we use a pre-trained BigGAN with a pre-trained ESRGAN and show how it results in sub-optimal performance compared to NSB-GAN.\n3. We have added a new experiment to Appendix J as per the suggestion of R4 to compare the inference times for our models that are marginally slower than BigGAN.\n4. We added a new experiment to Appendix K as per the suggestion of R3 and R4 to show the impact of end-to-end fine tuning of our model. We do not find it to improve the performance.\n5. We have added a clarification to the results in Table 2 as per R3\u2019s question to explain the *up to 4x claim.*\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2695/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2695/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "not-so-big-GAN: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution", "authorids": ["~Seungwook_Han1", "~Akash_Srivastava1", "~Cole_Lincoln_Hurwitz1", "~Prasanna_Sattigeri1", "~David_Daniel_Cox1"], "authors": ["Seungwook Han", "Akash Srivastava", "Cole Lincoln Hurwitz", "Prasanna Sattigeri", "David Daniel Cox"], "keywords": ["deep generative modeling", "GAN", "super-resolution", "wavelet transformation", "energy efficient"], "abstract": "State-of-the-art models for high-resolution image generation, such as BigGAN and VQVAE-2, require an incredible amount of compute resources and/or time (512 TPU-v3 cores) to train, putting them out of reach for the larger research community. On the other hand, GAN-based image super-resolution models, such as ESRGAN, can not only upscale images to high dimensions, but also are efficient to train. In this paper, we present not-so-big-GAN (nsb-GAN), a simple yet cost-effective two-step training framework for deep generative models (DGMs) of high-dimensional natural images. First, we generate images in low-frequency bands by training a sampler in the wavelet domain. Then, we super-resolve these images from the wavelet domain back to the pixel-space with our novel wavelet super-resolution decoder network. Wavelet-based down-sampling method preserves more structural information than pixel-based methods, leading to significantly better generative quality of the low-resolution sampler (e.g., 64\u00d764). Since the sampler and decoder can be trained in parallel and operate on much lower dimensional spaces than end-to-end models, the training cost is substantially reduced. On ImageNet 512\u00d7512, our model achieves a Fr\u00e9chet Inception Distance (FID) of 10.59 \u2013 beating the baseline BigGAN model \u2013 at half the compute (256 TPU-v3 cores).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|notsobiggan_generating_highfidelity_images_on_small_compute_with_waveletbased_superresolution", "one-sentence_summary": "Energy-efficient framework for generating high-fidelity, high-resolution images using wavelet-based super-resolution", "pdf": "/pdf/9e76109e3f9567090b881db9e556c5915994d8b1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=s-bN49Yq2", "_bibtex": "@misc{\nhan2021notsobiggan,\ntitle={not-so-big-{\\{}GAN{\\}}: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution},\nauthor={Seungwook Han and Akash Srivastava and Cole Lincoln Hurwitz and Prasanna Sattigeri and David Daniel Cox},\nyear={2021},\nurl={https://openreview.net/forum?id=E9W0QPxtZ_u}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "E9W0QPxtZ_u", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2695/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2695/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2695/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2695/Authors|ICLR.cc/2021/Conference/Paper2695/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2695/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845414, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2695/-/Official_Comment"}}}, {"id": "CP27_Vs7--", "original": null, "number": 5, "cdate": 1605467366182, "ddate": null, "tcdate": 1605467366182, "tmdate": 1605468846152, "tddate": null, "forum": "E9W0QPxtZ_u", "replyto": "rVG0ALVHmb1", "invitation": "ICLR.cc/2021/Conference/Paper2695/-/Official_Comment", "content": {"title": "Response", "comment": "We thank R3 for the feedback. Here are our responses to their questions in order:\n>The idea of this work is not so novel. It seems low-resolution sampler + super-resolution decoder which is a straightforward idea and lacks novelty.\n\nWe respectfully disagree with R3. To the best of our knowledge, the decoupling of generation from upsampling in deep generative models is novel and has not been studied before. Furthermore, our method is the first successful attempt at wavelet space training of DGMs and SISRs. Most importantly, our method is the most energy efficient and the fastest way of training the BigGAN model without any modelling changes. In fact, our method even outperforms the BigGAN model when generating samples beyond the original resolution of the dataset (at 512x512). \n\nThe simplicity and straightforwardness of our method is in its implementation. But, as R4 points out, this is in fact a strength; our two-step training framework is very flexible and can be realized using a number of different existing models. \n\n>Why the authors didn't finetune the sampler and decoder networks together in the end...\n\nFinetuning the two models together requires ground truth samples from an already trained BigGAN at the target resolution. This defeats the purpose of our approach, which tries to avoid training a BigGAN at the target resolution to begin with. Following your suggestion, we ran new experiments where we used samples from a pretrained BigGAN at 512x512 to refine our end-to-end model. We found that this leads to high-frequency artifacts in the generated images and worse results on FID/IS (FID 13.73, IS 44.02). We have added these results to Appendix K. \n\n>...what if the LR images from the learned sampler contain do not contains any desired patterns/textures\n\nR3 is correct. We explain this in Section 4.1.1 using Figure 2 from the Appendix.  When the LR resolution falls below 64, the FID suffers an increase due to loss in structural information. We are a bit unclear exactly what weakness this pertains to. Could R3 please clarify? \n\n>For the NSB-GAN sampler, the authors use a batch size of 512, which is much smaller than that of BigGAN (2048). The authors should give an ablation study to analyze the effect of batch size...\n\nBased on the original BigGAN paper, where they do study the impact of batch size, we do think our 64 x 64 model will benefit from a higher batch size. Unfortunately, running this experiment is not possible for two reasons.\n1. The public implementation of BigGAN does not implement Sync BatchNorm (as we mentioned in the manuscript) which is crucial to see any benefit from higher batch size training. \n2. We do not have access to a 128-core TPU v3 Pod or equivalently large GPU clusters to run the 2048 mini-batch setting. This is actually one of the main motivations behind our work.\n\n\n>From Figure 2 in Appendix A, it seems that wavelet-based downsampling loses more structural information than pixel-based downsampling...\n\nWe sincerely apologize for the confusion. The caption of Figure 2 is incorrect. The rightmost image is indeed wavelet-based downsampling and not the middle column. With the correction, it is evident from the figure that pixel-space downsampling suffers from more information loss and blurriness than wavelet-based downsampling. We have updated the manuscript with the corrected version.\n\n>I cannot understand the claim that \"NSB-GAN models reduce the training compute budget by up to four times\"\n\nWe are referring to the last row of Table 2, where we directly generate samples from the 128x128 pretrained BigGAN sampler and run it through our decoder to generate 512x512 images. We will clarify this in the manuscript.\n\n>I notice that NSB-GAN outperforms BigGAN at 512 resolution in terms of min FID, but it shows worse performance at 256 resolution... The authors are suggested to give some analysis on this problem.\n\nAt 512x512, our model outperforms the BigGAN model because of how the 512x512 training data is generated. The ImageNet dataset is natively at 256x256 (approximately). When training the BigGAN model to generate 512x512, an interpolation-based method is used to upsample ImageNet images to 512x512, resulting in gaussian noise in the upsampled images. In comparison, our method takes samples at 128x128 and upsamples them using a learned SISR model. This leads to substantially sharper images and therefore better FID scores. In fact, based on our study, it is better to use our model over an end-to-end DGM when learning to generate samples beyond the native resolution of the dataset. To clearly demonstrate this difference, we have added illustrative samples from the bilinearly interpolated 512x512 ImageNet data and super-resolved version of the same samples with our decoder in Appendix K (Figure 31). Clearly, interpolated images are blurrier than super-resolved images. Since BigGAN is trained to generate this blurry data, compared to our approach, it performs sub-optimally.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2695/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2695/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "not-so-big-GAN: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution", "authorids": ["~Seungwook_Han1", "~Akash_Srivastava1", "~Cole_Lincoln_Hurwitz1", "~Prasanna_Sattigeri1", "~David_Daniel_Cox1"], "authors": ["Seungwook Han", "Akash Srivastava", "Cole Lincoln Hurwitz", "Prasanna Sattigeri", "David Daniel Cox"], "keywords": ["deep generative modeling", "GAN", "super-resolution", "wavelet transformation", "energy efficient"], "abstract": "State-of-the-art models for high-resolution image generation, such as BigGAN and VQVAE-2, require an incredible amount of compute resources and/or time (512 TPU-v3 cores) to train, putting them out of reach for the larger research community. On the other hand, GAN-based image super-resolution models, such as ESRGAN, can not only upscale images to high dimensions, but also are efficient to train. In this paper, we present not-so-big-GAN (nsb-GAN), a simple yet cost-effective two-step training framework for deep generative models (DGMs) of high-dimensional natural images. First, we generate images in low-frequency bands by training a sampler in the wavelet domain. Then, we super-resolve these images from the wavelet domain back to the pixel-space with our novel wavelet super-resolution decoder network. Wavelet-based down-sampling method preserves more structural information than pixel-based methods, leading to significantly better generative quality of the low-resolution sampler (e.g., 64\u00d764). Since the sampler and decoder can be trained in parallel and operate on much lower dimensional spaces than end-to-end models, the training cost is substantially reduced. On ImageNet 512\u00d7512, our model achieves a Fr\u00e9chet Inception Distance (FID) of 10.59 \u2013 beating the baseline BigGAN model \u2013 at half the compute (256 TPU-v3 cores).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|notsobiggan_generating_highfidelity_images_on_small_compute_with_waveletbased_superresolution", "one-sentence_summary": "Energy-efficient framework for generating high-fidelity, high-resolution images using wavelet-based super-resolution", "pdf": "/pdf/9e76109e3f9567090b881db9e556c5915994d8b1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=s-bN49Yq2", "_bibtex": "@misc{\nhan2021notsobiggan,\ntitle={not-so-big-{\\{}GAN{\\}}: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution},\nauthor={Seungwook Han and Akash Srivastava and Cole Lincoln Hurwitz and Prasanna Sattigeri and David Daniel Cox},\nyear={2021},\nurl={https://openreview.net/forum?id=E9W0QPxtZ_u}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "E9W0QPxtZ_u", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2695/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2695/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2695/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2695/Authors|ICLR.cc/2021/Conference/Paper2695/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2695/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845414, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2695/-/Official_Comment"}}}, {"id": "wsF53x2FeBd", "original": null, "number": 4, "cdate": 1605467122032, "ddate": null, "tcdate": 1605467122032, "tmdate": 1605467122032, "tddate": null, "forum": "E9W0QPxtZ_u", "replyto": "JeStZAqlEsY", "invitation": "ICLR.cc/2021/Conference/Paper2695/-/Official_Comment", "content": {"title": "Response", "comment": "We thank R4 for their feedback and recognising our core contribution. We have run all the three suggested experiments and added the results to the manuscript. We now describe them here.\n\n>Does the proposed method also outperform the approach that first generates using a pre-trained BigGAN-256 then upsamples using an officially pre-trained ESRGAN?\n\nThat is correct. Applying an officially pre-trained ESRGAN does not perform as well. Specifically, at 512x512 resolution, it suffers a slight increase in FID (10.73) and a substantial decrease (>100) in IS (52.25). Following your suggestion, we have added this experiment to Appendix I.\n\n>What about the inference time (or complexity) of NSB-GAN compared to BigGAN?\n\nNSB-GAN inference takes 0.039 s / image, whereas BigGAN inference takes 0.029 s / image, at 512x512. Therefore, the overhead of upsampling (ESRGAN) is relatively small (0.010 s / image). We have added these numbers to the Appendix J.\n\n>If the decoders are trained with real samples only (drawn from the imagenet dataset), the upsampled outputs may have visual artifacts due to the mismatched distribution between the train (real) and test (fake gen from sampler) images. For example, the images of the cabinet (Fig3, 4, 5th row) have overly sharpened artifacts that BigGAN does not suffer.\n\nFollowing R4\u2019s suggestion, we tested finetuning our models, and we found that it increases high-frequency artifacts in the generated images. We have included examples in the manuscript in Appendix K.\nPlease note, finetuning the two models together requires ground truth samples from an already trained BigGAN at the target resolution. This somewhat defeats the purpose of our approach, which is trying to avoid training a BigGAN at the target resolution to begin with. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2695/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2695/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "not-so-big-GAN: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution", "authorids": ["~Seungwook_Han1", "~Akash_Srivastava1", "~Cole_Lincoln_Hurwitz1", "~Prasanna_Sattigeri1", "~David_Daniel_Cox1"], "authors": ["Seungwook Han", "Akash Srivastava", "Cole Lincoln Hurwitz", "Prasanna Sattigeri", "David Daniel Cox"], "keywords": ["deep generative modeling", "GAN", "super-resolution", "wavelet transformation", "energy efficient"], "abstract": "State-of-the-art models for high-resolution image generation, such as BigGAN and VQVAE-2, require an incredible amount of compute resources and/or time (512 TPU-v3 cores) to train, putting them out of reach for the larger research community. On the other hand, GAN-based image super-resolution models, such as ESRGAN, can not only upscale images to high dimensions, but also are efficient to train. In this paper, we present not-so-big-GAN (nsb-GAN), a simple yet cost-effective two-step training framework for deep generative models (DGMs) of high-dimensional natural images. First, we generate images in low-frequency bands by training a sampler in the wavelet domain. Then, we super-resolve these images from the wavelet domain back to the pixel-space with our novel wavelet super-resolution decoder network. Wavelet-based down-sampling method preserves more structural information than pixel-based methods, leading to significantly better generative quality of the low-resolution sampler (e.g., 64\u00d764). Since the sampler and decoder can be trained in parallel and operate on much lower dimensional spaces than end-to-end models, the training cost is substantially reduced. On ImageNet 512\u00d7512, our model achieves a Fr\u00e9chet Inception Distance (FID) of 10.59 \u2013 beating the baseline BigGAN model \u2013 at half the compute (256 TPU-v3 cores).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|notsobiggan_generating_highfidelity_images_on_small_compute_with_waveletbased_superresolution", "one-sentence_summary": "Energy-efficient framework for generating high-fidelity, high-resolution images using wavelet-based super-resolution", "pdf": "/pdf/9e76109e3f9567090b881db9e556c5915994d8b1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=s-bN49Yq2", "_bibtex": "@misc{\nhan2021notsobiggan,\ntitle={not-so-big-{\\{}GAN{\\}}: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution},\nauthor={Seungwook Han and Akash Srivastava and Cole Lincoln Hurwitz and Prasanna Sattigeri and David Daniel Cox},\nyear={2021},\nurl={https://openreview.net/forum?id=E9W0QPxtZ_u}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "E9W0QPxtZ_u", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2695/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2695/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2695/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2695/Authors|ICLR.cc/2021/Conference/Paper2695/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2695/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845414, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2695/-/Official_Comment"}}}, {"id": "vQ5dXa7ZuiO", "original": null, "number": 3, "cdate": 1605466900854, "ddate": null, "tcdate": 1605466900854, "tmdate": 1605467011402, "tddate": null, "forum": "E9W0QPxtZ_u", "replyto": "WYdzxYS1_Tt", "invitation": "ICLR.cc/2021/Conference/Paper2695/-/Official_Comment", "content": {"title": "Response", "comment": "We thank R2 for their in-depth review and would like to address each of the four issues they brought up next.\n\n> I believe the bilinear downsampling, the basis of the -P variant, is implemented incorrectly...\n\nWe want to assure R2 that bilinear interpolation (BI) in our model is implemented correctly. Unfortunately, there is a typo in the caption of Figure 2 (Appendix). The image in the rightmost column is created using wavelet-based downsampling and NOT using BI (as incorrectly stated in the caption). So the aliasing you found was in the wavelet-based downsampled image (which is to be expected; your point #3), and not in the bilinearly interpolated image. We sincerely apologize for the confusion and have updated the draft with corrections.\n\n>... teaching a GAN to generate aliased images... seems like a task that is fundamentally harder than if the aliasing wasn\u2019t there. Hence the worse results are not unexpected.\n\nSince the pixel-space downsampling was done using PIL\u2019s implementation of BI, therefore, it does not suffer from aliasing. But please note, even with aliasing, the -W model substantially (by 10 points) outperforms the -P model with our learned decoder, as illustrated in Table 1. In light of this, R2\u2019s claim that the -P model is worse due to aliasing does not hold and our results are not biased.\n\nWe hope that these clarifications address your primary concern, re-establishing the validity of our results. \n\n>visual inspection ... cannot be skipped...I do not approve of pushing them to the appendix.\n\nWe agree with R2 about FID/IS, and would have loved to include the samples in the main text. However, given the 256x256 and 512x512 sizes of the samples, it would have easily taken several pages of the main text only to show an unsatisfactory number of samples. Therefore, in the appendix, we provide an abundance of qualitative proof -- class-wise samples and full-resolution samples -- comparing the -W and -P models, alongside pretrained BigGAN samples. We request R2 not to penalize us for this, as this is somewhat beyond our control given the resolution of the images.\n\n>The functional prior ... highly structured representation space... What does this mean, precisely?\n\nHere we are not comparing the -W model with the -P model. Instead, we emphasize that, compared to a VAE or VQ-VAE, our latent spaces are very structured (in fact, they are downsampled images) and therefore we are able to use a convolution-based GAN to sample from them. \n\n>the kinds of critically sampled wavelets employed here are known for their aliasing issues\n\nWe empirically tested a large number of bases before settling for bior-2.2 as it led to the best result albeit with aliasing. If R2 has any particular suggestions, we would be more than happy to try it. \n\n>What precisely is going on with Equation 2? ... how is this actually different from the pixel-space version..?\n\nEq 2 illustrates that the decoder first applies an IWT operation to the image (unlike pixel space upsampling) and then uses the learned function f to fill in the missing information. This differs from the pixel-space version in both the up-sampling method and the amount/type of information the function f needs to learn. For example, in Figure 2 (Appendix), structural details are clearly preserved better using WT downsampling as the two downsampling methods (WT and Pixel) result in different amounts and types of information in the LR image.\n\n>I find it difficult to find the big picture from this paper even after hours of trying.\n\nWe apologize if R2 found it difficult to gather the big picture of our work. As mentioned in the contributions subsection, NSB-GAN is an energy-efficient method of training large, deep generative models of high-dimensional natural images with orders of magnitude lower training cost. In this framework, we propose two types of models:\n\n1. -P, that operates in pixel space and works better in higher dimensional setting (because it loses too much information for the SR method to reconstruct in lower dimensional settings)\n2. -W that operates in the wavelet space and works relatively well in lower dimensional setting than the -P model as WT downsampling results in relatively smaller content loss (Figure 2). \n\n>I do not understand the paragraph 3.1.2 \u201cU-Net decoder\u201d. Why does [it] not take full advantage ...?\n\nUnlike ESRGAN-W, the U-Net-based decoder breaks down an image (e.g., 256x256) into a collection of 32x32 patches by repeatedly applying WT to all TL, TR, BL, and BR quadrants. By doing so, the decoder learns to fill in the details by reconstructing only 32x32 patches and deterministically combines them using IWT. In effect, the decoder skips to ever operate in the full dimensionality of the image. This paradigm can be best understood with Figure 30 (Appendix) and further explanation in Appendix C.3. In contrast, ESRGAN operates on the full dimensionality of the image and does not benefit from the wavelet decomposition as much.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2695/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2695/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "not-so-big-GAN: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution", "authorids": ["~Seungwook_Han1", "~Akash_Srivastava1", "~Cole_Lincoln_Hurwitz1", "~Prasanna_Sattigeri1", "~David_Daniel_Cox1"], "authors": ["Seungwook Han", "Akash Srivastava", "Cole Lincoln Hurwitz", "Prasanna Sattigeri", "David Daniel Cox"], "keywords": ["deep generative modeling", "GAN", "super-resolution", "wavelet transformation", "energy efficient"], "abstract": "State-of-the-art models for high-resolution image generation, such as BigGAN and VQVAE-2, require an incredible amount of compute resources and/or time (512 TPU-v3 cores) to train, putting them out of reach for the larger research community. On the other hand, GAN-based image super-resolution models, such as ESRGAN, can not only upscale images to high dimensions, but also are efficient to train. In this paper, we present not-so-big-GAN (nsb-GAN), a simple yet cost-effective two-step training framework for deep generative models (DGMs) of high-dimensional natural images. First, we generate images in low-frequency bands by training a sampler in the wavelet domain. Then, we super-resolve these images from the wavelet domain back to the pixel-space with our novel wavelet super-resolution decoder network. Wavelet-based down-sampling method preserves more structural information than pixel-based methods, leading to significantly better generative quality of the low-resolution sampler (e.g., 64\u00d764). Since the sampler and decoder can be trained in parallel and operate on much lower dimensional spaces than end-to-end models, the training cost is substantially reduced. On ImageNet 512\u00d7512, our model achieves a Fr\u00e9chet Inception Distance (FID) of 10.59 \u2013 beating the baseline BigGAN model \u2013 at half the compute (256 TPU-v3 cores).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|notsobiggan_generating_highfidelity_images_on_small_compute_with_waveletbased_superresolution", "one-sentence_summary": "Energy-efficient framework for generating high-fidelity, high-resolution images using wavelet-based super-resolution", "pdf": "/pdf/9e76109e3f9567090b881db9e556c5915994d8b1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=s-bN49Yq2", "_bibtex": "@misc{\nhan2021notsobiggan,\ntitle={not-so-big-{\\{}GAN{\\}}: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution},\nauthor={Seungwook Han and Akash Srivastava and Cole Lincoln Hurwitz and Prasanna Sattigeri and David Daniel Cox},\nyear={2021},\nurl={https://openreview.net/forum?id=E9W0QPxtZ_u}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "E9W0QPxtZ_u", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2695/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2695/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2695/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2695/Authors|ICLR.cc/2021/Conference/Paper2695/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2695/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923845414, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2695/-/Official_Comment"}}}, {"id": "JeStZAqlEsY", "original": null, "number": 2, "cdate": 1603967905846, "ddate": null, "tcdate": 1603967905846, "tmdate": 1605024151663, "tddate": null, "forum": "E9W0QPxtZ_u", "replyto": "E9W0QPxtZ_u", "invitation": "ICLR.cc/2021/Conference/Paper2695/-/Official_Review", "content": {"title": "compute-efficient deep generative models for large images", "review": "This paper proposes a new framework NSB-GAN for high-resolution natural image generation with a low computation budget. They first utilize the BigGAN (or StyleGANv2) to generate a low-resolution image, and the wavelet- or pixel-based SR network decodes the intermediate image to the desire resolution. With this simple two-step approach, they successfully train the model on the limited resources where vanilla BigGAN cannot be trained. In addition, NSB-GAN outperforms the BigGAN on high-resolution (>512x512) maybe due to the instability of the BigGAN at extremely high resolution.\n\nStrengths:\n+ The motivation of compute-efficient deep generative models for large images is a very important issue, but not many methods have taken this into account. The proposed method tackles this issue in a simple but effective manner.\n+ Since the proposed approach does not modify the network architecture, it is flexible, allowing any model compression techniques can be adapted accordingly.\n\nWeaknesses:\n- Lack of some critical comparisons: 1) Does the proposed method also outperform the approach that first generates using a pre-trained BigGAN-256 then upsamples using an officially pre-trained ESRGAN? 2) What about the inference time (or complexity) of NSB-GAN compared to BigGAN?\n- If the decoders are trained with real samples only (drawn from the imagenet dataset), the upsampled outputs may have visual artifacts due to the mismatched distribution between the train (real) and test (fake gen from sampler) images. For example, the images of the cabinet (Fig3, 4, 5th row) have overly sharpened artifacts that BigGAN does not suffer.\n\nOverall, the suggested work effectively decreases the training time of the BigGAN using a simple idea. However, there are missing comparisons and analyses such as 1) comparison with pre-trained BigGAN -> ESRGAN 2) How the capacity of the SR model affects the FID. And lastly, since the proposed method is pipelining, there are some unexpected artifacts.\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2695/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2695/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "not-so-big-GAN: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution", "authorids": ["~Seungwook_Han1", "~Akash_Srivastava1", "~Cole_Lincoln_Hurwitz1", "~Prasanna_Sattigeri1", "~David_Daniel_Cox1"], "authors": ["Seungwook Han", "Akash Srivastava", "Cole Lincoln Hurwitz", "Prasanna Sattigeri", "David Daniel Cox"], "keywords": ["deep generative modeling", "GAN", "super-resolution", "wavelet transformation", "energy efficient"], "abstract": "State-of-the-art models for high-resolution image generation, such as BigGAN and VQVAE-2, require an incredible amount of compute resources and/or time (512 TPU-v3 cores) to train, putting them out of reach for the larger research community. On the other hand, GAN-based image super-resolution models, such as ESRGAN, can not only upscale images to high dimensions, but also are efficient to train. In this paper, we present not-so-big-GAN (nsb-GAN), a simple yet cost-effective two-step training framework for deep generative models (DGMs) of high-dimensional natural images. First, we generate images in low-frequency bands by training a sampler in the wavelet domain. Then, we super-resolve these images from the wavelet domain back to the pixel-space with our novel wavelet super-resolution decoder network. Wavelet-based down-sampling method preserves more structural information than pixel-based methods, leading to significantly better generative quality of the low-resolution sampler (e.g., 64\u00d764). Since the sampler and decoder can be trained in parallel and operate on much lower dimensional spaces than end-to-end models, the training cost is substantially reduced. On ImageNet 512\u00d7512, our model achieves a Fr\u00e9chet Inception Distance (FID) of 10.59 \u2013 beating the baseline BigGAN model \u2013 at half the compute (256 TPU-v3 cores).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|notsobiggan_generating_highfidelity_images_on_small_compute_with_waveletbased_superresolution", "one-sentence_summary": "Energy-efficient framework for generating high-fidelity, high-resolution images using wavelet-based super-resolution", "pdf": "/pdf/9e76109e3f9567090b881db9e556c5915994d8b1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=s-bN49Yq2", "_bibtex": "@misc{\nhan2021notsobiggan,\ntitle={not-so-big-{\\{}GAN{\\}}: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution},\nauthor={Seungwook Han and Akash Srivastava and Cole Lincoln Hurwitz and Prasanna Sattigeri and David Daniel Cox},\nyear={2021},\nurl={https://openreview.net/forum?id=E9W0QPxtZ_u}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "E9W0QPxtZ_u", "replyto": "E9W0QPxtZ_u", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2695/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090524, "tmdate": 1606915778800, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2695/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2695/-/Official_Review"}}}, {"id": "WYdzxYS1_Tt", "original": null, "number": 3, "cdate": 1603996016745, "ddate": null, "tcdate": 1603996016745, "tmdate": 1605024151596, "tddate": null, "forum": "E9W0QPxtZ_u", "replyto": "E9W0QPxtZ_u", "invitation": "ICLR.cc/2021/Conference/Paper2695/-/Official_Review", "content": {"title": "Neat idea, but technical flaws cast doubt on conclusions", "review": "The paper describes a two-stage generative image model: first, a GAN is trained to output low-resolution images, and another model to then perform single-image superresolution on the results of the first model. The claim is that the resulting model is slightly better than BigGAN-512 using half the compute requirements, in terms of FID. Two variants are described: one that generates in the wavelet decomposition domain (-W), and another that operates in pixel space (-P).\n\nThe idea of applying SISR to a GAN output seems potentially novel and useful, but as it stands, I find the paper convoluted and lacking a clear message. I cannot recommend acceptance at this point.\n\nThe main issues I see are the following:\n\n1. I suspect much of the image processing is performed incorrectly, casting doubt on the validity of the -P model and rendering the comparison to the wavelet case moot.\n2. Results are only reported in numeric form as FID and IS.\n3. The argumentation about the properties of the wavelet decomposition seem vague and without clear technical counterparts in the (well-developed) literature on wavelets and image processing.\n4. Simultaneous lack of details and overall verbosity; I find it difficult to find the big picture from this paper even after hours of trying.\n\n\n1. I believe the bilinear downsampling, the basis of the -P variant, is implemented incorrectly. This is visible as clear aliasing in the supplemental Figure 2, rightmost \u201cpixel-space\u201d column. To verify, I extracted the dog and sailboat images from the PDF and applied bilinear downsampling in Matlab \u2013 which uses proper pre-filtering before downsampling to remove aliases, unlike for instance the bilinear grid_sample operation in PyTorch \u2013 and get a significantly different result; one that does not have the signature aliasing artifacts that remain in the images shown. If, on the other hand, I explicitly turn off antialiasing, the result quite closely matches the right-hand column. To be clear, this a rudimentary mistake in image processing (which is surprisingly prevalent in the ML and vision literature).\n\nThis makes me suspect all results of the -P variants are not to be trusted: teaching a GAN to generate aliased images, and then another model to up-res those aliased images, seems like a task that is fundamentally harder than if the aliasing wasn\u2019t there. Hence the worse results are not unexpected.\n\nIn particular: I find the conclusions drawn from the results in Table 1 all potentially invalid.  On the other hand, using pretrained samplers, the pixel space versions appear to actually do a little *better* (in terms of FID) than the wavelet ones in Table 2. Comparing the results in the appendix does not appear to reveal large differences.\n\n\n2. It is well known that metrics like FID and IS do not capture the notion of the quality of a GAN well. They are useful in drawing a picture of how the model performs. While some metrics that better correlate with result quality are known, a satisfactory one hardly exists, so visual inspection and analysis of the results cannot be skipped. I do not approve of pushing them to the appendix.\n\n3. Example: \"The functional prior imposed by our deterministic encoder leads to a highly structured representation space made up of low frequency TL patches of images.\u201d What does this mean, precisely? The repeated application of the wavelet approximation coefficient filter followed by decimation by 2 is equivalent to a particular linear downsampling operation applied to the original image; a poor one at that, because the kinds of critically sampled wavelets employed here are known for their aliasing issues (which has long ago led to a preference of using overcomplete bases). Similar language about the \u201cstructuredness\u201d of the wavelet representation can be found near Figure 2, where the pixel-space comparison is, I believe, incorrect, as I detail above.\n\n4. What precisely is going on with Equation 2? IWT(\u2026) would appear to be a reconstruction operation that combines the approximation and detail coefficients into an image of 2x the size, in pixel space; then addition of f(W^l_1,1) seems to add hallucinated detail on top. Does f do anything random or is it deterministic? And more pressingly, how is this actually different from the pixel-space version..?\n\nI do not understand the paragraph 3.1.2 \u201cU-Net decoder\u201d. Why \u201cdoes [it] not take full advantage of the compression that wavelet space modeling brings about\u201d? \n\n", "rating": "2: Strong rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2695/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2695/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "not-so-big-GAN: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution", "authorids": ["~Seungwook_Han1", "~Akash_Srivastava1", "~Cole_Lincoln_Hurwitz1", "~Prasanna_Sattigeri1", "~David_Daniel_Cox1"], "authors": ["Seungwook Han", "Akash Srivastava", "Cole Lincoln Hurwitz", "Prasanna Sattigeri", "David Daniel Cox"], "keywords": ["deep generative modeling", "GAN", "super-resolution", "wavelet transformation", "energy efficient"], "abstract": "State-of-the-art models for high-resolution image generation, such as BigGAN and VQVAE-2, require an incredible amount of compute resources and/or time (512 TPU-v3 cores) to train, putting them out of reach for the larger research community. On the other hand, GAN-based image super-resolution models, such as ESRGAN, can not only upscale images to high dimensions, but also are efficient to train. In this paper, we present not-so-big-GAN (nsb-GAN), a simple yet cost-effective two-step training framework for deep generative models (DGMs) of high-dimensional natural images. First, we generate images in low-frequency bands by training a sampler in the wavelet domain. Then, we super-resolve these images from the wavelet domain back to the pixel-space with our novel wavelet super-resolution decoder network. Wavelet-based down-sampling method preserves more structural information than pixel-based methods, leading to significantly better generative quality of the low-resolution sampler (e.g., 64\u00d764). Since the sampler and decoder can be trained in parallel and operate on much lower dimensional spaces than end-to-end models, the training cost is substantially reduced. On ImageNet 512\u00d7512, our model achieves a Fr\u00e9chet Inception Distance (FID) of 10.59 \u2013 beating the baseline BigGAN model \u2013 at half the compute (256 TPU-v3 cores).", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "han|notsobiggan_generating_highfidelity_images_on_small_compute_with_waveletbased_superresolution", "one-sentence_summary": "Energy-efficient framework for generating high-fidelity, high-resolution images using wavelet-based super-resolution", "pdf": "/pdf/9e76109e3f9567090b881db9e556c5915994d8b1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=s-bN49Yq2", "_bibtex": "@misc{\nhan2021notsobiggan,\ntitle={not-so-big-{\\{}GAN{\\}}: Generating High-Fidelity Images on Small Compute with Wavelet-based Super-Resolution},\nauthor={Seungwook Han and Akash Srivastava and Cole Lincoln Hurwitz and Prasanna Sattigeri and David Daniel Cox},\nyear={2021},\nurl={https://openreview.net/forum?id=E9W0QPxtZ_u}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "E9W0QPxtZ_u", "replyto": "E9W0QPxtZ_u", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2695/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538090524, "tmdate": 1606915778800, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2695/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2695/-/Official_Review"}}}], "count": 9}