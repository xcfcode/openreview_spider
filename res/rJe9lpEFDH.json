{"notes": [{"id": "rJe9lpEFDH", "original": "r1gc6omUvH", "number": 348, "cdate": 1569438961797, "ddate": null, "tcdate": 1569438961797, "tmdate": 1577168259878, "tddate": null, "forum": "rJe9lpEFDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["lukas.balles@tuebingen.mpg.de", "f@bianp.net", "nicolas@le-roux.name"], "title": "The Geometry of Sign Gradient Descent", "authors": ["Lukas Balles", "Fabian Pedregosa", "Nicolas Le Roux"], "pdf": "/pdf/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "TL;DR": "We investigate which properties of an objective function favor sign gradient descent.", "abstract": "Sign gradient descent has become popular in machine learning due to its favorable communication cost in distributed optimization and its good performance in neural network training. However, we currently do not have a good understanding of which geometrical properties of the objective function determine the relative speed of sign gradient descent compared to standard gradient descent. In this work, we frame sign gradient descent as steepest descent with respect to the maximum norm. We review the steepest descent framework and the related concept of smoothness with respect to arbitrary norms.\nBy studying the smoothness constant resulting from the $L^\\infty$-geometry, we isolate properties of the objective which favor sign gradient descent relative to gradient descent. In short, we find two requirements on its Hessian: (i) some degree of ``diagonal dominance'' and (ii) the maximal eigenvalue being much larger than the average eigenvalue. We also clarify the meaning of a certain separable smoothness assumption used in previous analyses of sign gradient descent.\nExperiments verify the developed theory.", "keywords": ["Sign gradient descent", "signSGD", "steepest descent", "Adam"], "paperhash": "balles|the_geometry_of_sign_gradient_descent", "original_pdf": "/attachment/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "_bibtex": "@misc{\nballes2020the,\ntitle={The Geometry of Sign Gradient Descent},\nauthor={Lukas Balles and Fabian Pedregosa and Nicolas Le Roux},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe9lpEFDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "A-GEi32Mm5", "original": null, "number": 1, "cdate": 1576798693874, "ddate": null, "tcdate": 1576798693874, "tmdate": 1576800941618, "tddate": null, "forum": "rJe9lpEFDH", "replyto": "rJe9lpEFDH", "invitation": "ICLR.cc/2020/Conference/Paper348/-/Decision", "content": {"decision": "Reject", "comment": "The paper is rejected based on unanimous reviews.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lukas.balles@tuebingen.mpg.de", "f@bianp.net", "nicolas@le-roux.name"], "title": "The Geometry of Sign Gradient Descent", "authors": ["Lukas Balles", "Fabian Pedregosa", "Nicolas Le Roux"], "pdf": "/pdf/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "TL;DR": "We investigate which properties of an objective function favor sign gradient descent.", "abstract": "Sign gradient descent has become popular in machine learning due to its favorable communication cost in distributed optimization and its good performance in neural network training. However, we currently do not have a good understanding of which geometrical properties of the objective function determine the relative speed of sign gradient descent compared to standard gradient descent. In this work, we frame sign gradient descent as steepest descent with respect to the maximum norm. We review the steepest descent framework and the related concept of smoothness with respect to arbitrary norms.\nBy studying the smoothness constant resulting from the $L^\\infty$-geometry, we isolate properties of the objective which favor sign gradient descent relative to gradient descent. In short, we find two requirements on its Hessian: (i) some degree of ``diagonal dominance'' and (ii) the maximal eigenvalue being much larger than the average eigenvalue. We also clarify the meaning of a certain separable smoothness assumption used in previous analyses of sign gradient descent.\nExperiments verify the developed theory.", "keywords": ["Sign gradient descent", "signSGD", "steepest descent", "Adam"], "paperhash": "balles|the_geometry_of_sign_gradient_descent", "original_pdf": "/attachment/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "_bibtex": "@misc{\nballes2020the,\ntitle={The Geometry of Sign Gradient Descent},\nauthor={Lukas Balles and Fabian Pedregosa and Nicolas Le Roux},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe9lpEFDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rJe9lpEFDH", "replyto": "rJe9lpEFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729792, "tmdate": 1576800282456, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper348/-/Decision"}}}, {"id": "HJxz7YuTYH", "original": null, "number": 2, "cdate": 1571813657553, "ddate": null, "tcdate": 1571813657553, "tmdate": 1574261748797, "tddate": null, "forum": "rJe9lpEFDH", "replyto": "rJe9lpEFDH", "invitation": "ICLR.cc/2020/Conference/Paper348/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper studies sign gradient descent in the framework of steepest descent with respect to a general norm. The authors show the connection to a smoothness with respect to a general norm, which is further related to a bound on the Hessian. Based on the interpretation of steepest descent, the authors compare the behavior of sign gradient descent with the gradient descent, and show that the relative behavior depends on both the percentage of diagonal ? and the spread of eigenvalues. Some experimental results are reported to verify the theory.\n\nThe paper is very well written and easy to follow. The theoretical analysis is sound and intuitive. However, the paper considers a variant of sign gradient descent where the l_1 norm of the gradients are introduced. The introduction of this l_1 norm is required for the interpretation of a steep descent but is not the one used often in practice.\n\nMost analysis seem follows from standard arguments. The extension from gradient descent to the steep descent with a general norm is a bit standard. In this sense, the paper is a bit incremental.\n\n----------------------\nAfter rebuttal:\n\nI have read the authors' response. I think the contribution may not be sufficient. I would like to keep my original score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper348/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper348/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lukas.balles@tuebingen.mpg.de", "f@bianp.net", "nicolas@le-roux.name"], "title": "The Geometry of Sign Gradient Descent", "authors": ["Lukas Balles", "Fabian Pedregosa", "Nicolas Le Roux"], "pdf": "/pdf/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "TL;DR": "We investigate which properties of an objective function favor sign gradient descent.", "abstract": "Sign gradient descent has become popular in machine learning due to its favorable communication cost in distributed optimization and its good performance in neural network training. However, we currently do not have a good understanding of which geometrical properties of the objective function determine the relative speed of sign gradient descent compared to standard gradient descent. In this work, we frame sign gradient descent as steepest descent with respect to the maximum norm. We review the steepest descent framework and the related concept of smoothness with respect to arbitrary norms.\nBy studying the smoothness constant resulting from the $L^\\infty$-geometry, we isolate properties of the objective which favor sign gradient descent relative to gradient descent. In short, we find two requirements on its Hessian: (i) some degree of ``diagonal dominance'' and (ii) the maximal eigenvalue being much larger than the average eigenvalue. We also clarify the meaning of a certain separable smoothness assumption used in previous analyses of sign gradient descent.\nExperiments verify the developed theory.", "keywords": ["Sign gradient descent", "signSGD", "steepest descent", "Adam"], "paperhash": "balles|the_geometry_of_sign_gradient_descent", "original_pdf": "/attachment/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "_bibtex": "@misc{\nballes2020the,\ntitle={The Geometry of Sign Gradient Descent},\nauthor={Lukas Balles and Fabian Pedregosa and Nicolas Le Roux},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe9lpEFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJe9lpEFDH", "replyto": "rJe9lpEFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper348/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper348/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575633153656, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper348/Reviewers"], "noninvitees": [], "tcdate": 1570237753444, "tmdate": 1575633153675, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper348/-/Official_Review"}}}, {"id": "SJg6bETUir", "original": null, "number": 5, "cdate": 1573471236667, "ddate": null, "tcdate": 1573471236667, "tmdate": 1573471236667, "tddate": null, "forum": "rJe9lpEFDH", "replyto": "r1lGs7a8ir", "invitation": "ICLR.cc/2020/Conference/Paper348/-/Official_Comment", "content": {"title": "Response to R1 ctd.", "comment": "(3) Analyzing the stochastic setting\n\nWe would like to note again that the main point of the paper is *not* any specific convergence statement, but rather to understand the properties that determine the L-inf smoothness constant, which will critically affect the performance of sign gradient methods in the deterministic as well the stochastic setting. As we point out in Section 6.2, the (weaker) max-norm smoothness can replace the separable smoothness assumption used in previous analyses of sign-based methods in the stochastic setting. I.e., we relax the assumption of existing works while also clarifying the properties of the objective encoded in the assumption.\n\nPorting a general convergence analysis of steepest descent to the stochastic setting would be an interesting endeavor, but poses many technical difficulties and is beyond the scope of this paper. Your comment regarding the gradient variance actually pertains to one of the main difficulties: In general it will not be possible to obtain an *unbiased* estimate of the steepest descent direction. If $g$ is an unbiased stochastic estimate of $\\nabla f$, then $\\Vert g\\Vert_1 \\text{sign}(g)$ will not be an unbiased estimate of $\\Vert\\nabla f\\Vert_1\\text{sign}(\\nabla f)$. The error due to stochasticity thus has a bias and a variance component.\n \n \n(4) Experiments.\n\nAs we say in the paper, the empirical results on neural nets have to be taken with a grain of salt. Nevertheless, we think that they are convincing in that between the two models there is a considerable gap between in the observed improvement ratio, R(x_t), which is in line with the actual relative performance of GD/signGD on the two models.\n\n\n(5) Generalization\n\nThis is certainly an interesting direction but we think that it is best left for future work. Understanding sign(S)GD purely as an optimization method is an important step and adequate for scope of a single conference paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper348/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper348/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lukas.balles@tuebingen.mpg.de", "f@bianp.net", "nicolas@le-roux.name"], "title": "The Geometry of Sign Gradient Descent", "authors": ["Lukas Balles", "Fabian Pedregosa", "Nicolas Le Roux"], "pdf": "/pdf/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "TL;DR": "We investigate which properties of an objective function favor sign gradient descent.", "abstract": "Sign gradient descent has become popular in machine learning due to its favorable communication cost in distributed optimization and its good performance in neural network training. However, we currently do not have a good understanding of which geometrical properties of the objective function determine the relative speed of sign gradient descent compared to standard gradient descent. In this work, we frame sign gradient descent as steepest descent with respect to the maximum norm. We review the steepest descent framework and the related concept of smoothness with respect to arbitrary norms.\nBy studying the smoothness constant resulting from the $L^\\infty$-geometry, we isolate properties of the objective which favor sign gradient descent relative to gradient descent. In short, we find two requirements on its Hessian: (i) some degree of ``diagonal dominance'' and (ii) the maximal eigenvalue being much larger than the average eigenvalue. We also clarify the meaning of a certain separable smoothness assumption used in previous analyses of sign gradient descent.\nExperiments verify the developed theory.", "keywords": ["Sign gradient descent", "signSGD", "steepest descent", "Adam"], "paperhash": "balles|the_geometry_of_sign_gradient_descent", "original_pdf": "/attachment/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "_bibtex": "@misc{\nballes2020the,\ntitle={The Geometry of Sign Gradient Descent},\nauthor={Lukas Balles and Fabian Pedregosa and Nicolas Le Roux},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe9lpEFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJe9lpEFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper348/Authors", "ICLR.cc/2020/Conference/Paper348/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper348/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper348/Reviewers", "ICLR.cc/2020/Conference/Paper348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper348/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper348/Authors|ICLR.cc/2020/Conference/Paper348/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172763, "tmdate": 1576860543393, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper348/Authors", "ICLR.cc/2020/Conference/Paper348/Reviewers", "ICLR.cc/2020/Conference/Paper348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper348/-/Official_Comment"}}}, {"id": "r1lGs7a8ir", "original": null, "number": 4, "cdate": 1573471129569, "ddate": null, "tcdate": 1573471129569, "tmdate": 1573471129569, "tddate": null, "forum": "rJe9lpEFDH", "replyto": "r1gsMvgpFH", "invitation": "ICLR.cc/2020/Conference/Paper348/-/Official_Comment", "content": {"title": "Response to R1", "comment": "Thank you for your review. We gather that your main criticism is (1) that you \"do not think the insights claimed in the paper are especially novel\", and (2) that the results are \"rather high-level\" and \"depend on quantities that are unknown such as the norm of the gradient\". We address these two larger points first and then respond to specific questions/comments below.\n\n\n(1) Novelty and discussion of prior work\n\nAfter reading your review, we agree with you that we need to do a better job at contrasting with prior work to make our contributions stand out. We will update the paper shortly. We want to summarize for you what we see as the main contributions of our work. We believe that these are substantial and warrant publication. We hope that you will reconsider your judgement of their novelty in light of our response.\n\nThe main goal of the paper is to understand the properties of the objective function which determine the performance of sign gradient descent relative to gradient descent. We do not achieve a full characterization (which we acknowledge, see also next comment) but we make the following contributions towards this goal which can not be found in any prior work:\n\n- We show that the smoothness constant for any norm arises as a bound on the Hessian in an induced matrix norm, relating it to properties of the Hessian. This is, of course, known for the Euclidean norm but we are not aware of any published works considering the general case.\n\n- For the smoothness constant w.r.t. the maximum norm, which is pertinent to the performance of signGD, we show an upper bound based on axis alignment and the eigenvalues. This sensitivity of signGD to the axis alignment has never been discussed before.\n\n- Based on that, we compare of GD and signGD. This comparison is qualitative since, as you point out, it depends on the unknown gradient norms over the trajectory of the algorithms. We also verify it experimentally for quadratic functions.\n\n- We explain how our insights relate to the \"separable smoothness\" assumption made in prior works on sign(S)GD. We clarify its meaning by showing that the sensitivity to axis alignment is hidden in the l_i values. We further show that this assumption can be replaced with the (weaker) assumption of max-norm smoothness in all existing results on sign(S)GD. I.e., we relax the assumptions of existing works while also clarifying the properties of the objective encoded in the assumption.\n\nWith regards to the specific works you mentioned in your review:\n- Thanks for the pointer to Becker & Le Cun (1988), which supports our point that axis-alignment is a feature of neural network training objectives. We will add a brief discussion to the paper. Their interest in axis alignment stems from diagonal approximations in second order methods, which is an orthogonal direction.\n- Boyd & Vanderberghe indeed discuss how the choice of norm affects the convergence of steepest descent methods. However, the chapter \u201cchoice of norm for steepest descent\u201d exclusively discusses quadratic norms resulting in preconditioned gradient descent methods.\n\n\n(2) Inconclusiveness of the results.\n\nWe concede that the results in Section 4 are qualitative more than quantitative. As you rightly point out, this is because in addition to the smoothness constants, the performance depends on the gradient norms over the trajectory, which we can't get non-trivial lower bounds on. We nevertheless believe that understanding the smoothness constant gives important insights. We also verify this experimentally on quadratic functions."}, "signatures": ["ICLR.cc/2020/Conference/Paper348/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper348/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lukas.balles@tuebingen.mpg.de", "f@bianp.net", "nicolas@le-roux.name"], "title": "The Geometry of Sign Gradient Descent", "authors": ["Lukas Balles", "Fabian Pedregosa", "Nicolas Le Roux"], "pdf": "/pdf/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "TL;DR": "We investigate which properties of an objective function favor sign gradient descent.", "abstract": "Sign gradient descent has become popular in machine learning due to its favorable communication cost in distributed optimization and its good performance in neural network training. However, we currently do not have a good understanding of which geometrical properties of the objective function determine the relative speed of sign gradient descent compared to standard gradient descent. In this work, we frame sign gradient descent as steepest descent with respect to the maximum norm. We review the steepest descent framework and the related concept of smoothness with respect to arbitrary norms.\nBy studying the smoothness constant resulting from the $L^\\infty$-geometry, we isolate properties of the objective which favor sign gradient descent relative to gradient descent. In short, we find two requirements on its Hessian: (i) some degree of ``diagonal dominance'' and (ii) the maximal eigenvalue being much larger than the average eigenvalue. We also clarify the meaning of a certain separable smoothness assumption used in previous analyses of sign gradient descent.\nExperiments verify the developed theory.", "keywords": ["Sign gradient descent", "signSGD", "steepest descent", "Adam"], "paperhash": "balles|the_geometry_of_sign_gradient_descent", "original_pdf": "/attachment/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "_bibtex": "@misc{\nballes2020the,\ntitle={The Geometry of Sign Gradient Descent},\nauthor={Lukas Balles and Fabian Pedregosa and Nicolas Le Roux},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe9lpEFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJe9lpEFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper348/Authors", "ICLR.cc/2020/Conference/Paper348/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper348/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper348/Reviewers", "ICLR.cc/2020/Conference/Paper348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper348/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper348/Authors|ICLR.cc/2020/Conference/Paper348/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172763, "tmdate": 1576860543393, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper348/Authors", "ICLR.cc/2020/Conference/Paper348/Reviewers", "ICLR.cc/2020/Conference/Paper348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper348/-/Official_Comment"}}}, {"id": "Hkl0ZXaIoS", "original": null, "number": 3, "cdate": 1573470982099, "ddate": null, "tcdate": 1573470982099, "tmdate": 1573470982099, "tddate": null, "forum": "rJe9lpEFDH", "replyto": "HJxz7YuTYH", "invitation": "ICLR.cc/2020/Conference/Paper348/-/Official_Comment", "content": {"title": "Response to R3", "comment": "(1) \"However, the paper considers a variant of sign gradient descent where the l_1 norm of the gradients are introduced.\"\n\nThat is correct and the paper is completely open about that limitation; see footnote 2 in the paper. This allows us to leverage the gradient descent framework and also makes signGD converge with a constant step size to begin with. We believe that there's still a lot of insight to be gained from this analysis.\n\nThis rescaling of the update direction can be rolled into the step size. In optimization it is fairly common to analyze a method with a special choice of step size (e.g., with exact line search) even if this deviates from practical choices. In practice, sign(S)GD is used with a decreasing step size schedule which can be seen as emulating the effect of the decreasing L1-norm of the gradient. Thus, although they are different algorithms, we do not expect the behaviour to be drastically different.\n\nIn addition to all of our results,  we explore a possible direction to understand the normalization aspect of sign gradient descent via a relaxed smoothness condition in the appendix.\n\n\n(2) \"The extension to [...] a general norm is a bit standard. In this sense, the paper is a bit incremental.\"\n\nWe are astonished by this comment. Yes, this is absolutely standard, but this is *entirely* besides the point of the paper. The steepest descent framework is not the subject matter of the paper, it is merely a tool that we use to analyze sign gradient descent. The novelty lies in understanding the properties determining the performance of sign gradient descent, which we do by analyzing the non-Euclidean smoothness constant arising from the steepest descent framework."}, "signatures": ["ICLR.cc/2020/Conference/Paper348/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper348/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lukas.balles@tuebingen.mpg.de", "f@bianp.net", "nicolas@le-roux.name"], "title": "The Geometry of Sign Gradient Descent", "authors": ["Lukas Balles", "Fabian Pedregosa", "Nicolas Le Roux"], "pdf": "/pdf/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "TL;DR": "We investigate which properties of an objective function favor sign gradient descent.", "abstract": "Sign gradient descent has become popular in machine learning due to its favorable communication cost in distributed optimization and its good performance in neural network training. However, we currently do not have a good understanding of which geometrical properties of the objective function determine the relative speed of sign gradient descent compared to standard gradient descent. In this work, we frame sign gradient descent as steepest descent with respect to the maximum norm. We review the steepest descent framework and the related concept of smoothness with respect to arbitrary norms.\nBy studying the smoothness constant resulting from the $L^\\infty$-geometry, we isolate properties of the objective which favor sign gradient descent relative to gradient descent. In short, we find two requirements on its Hessian: (i) some degree of ``diagonal dominance'' and (ii) the maximal eigenvalue being much larger than the average eigenvalue. We also clarify the meaning of a certain separable smoothness assumption used in previous analyses of sign gradient descent.\nExperiments verify the developed theory.", "keywords": ["Sign gradient descent", "signSGD", "steepest descent", "Adam"], "paperhash": "balles|the_geometry_of_sign_gradient_descent", "original_pdf": "/attachment/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "_bibtex": "@misc{\nballes2020the,\ntitle={The Geometry of Sign Gradient Descent},\nauthor={Lukas Balles and Fabian Pedregosa and Nicolas Le Roux},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe9lpEFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJe9lpEFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper348/Authors", "ICLR.cc/2020/Conference/Paper348/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper348/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper348/Reviewers", "ICLR.cc/2020/Conference/Paper348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper348/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper348/Authors|ICLR.cc/2020/Conference/Paper348/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172763, "tmdate": 1576860543393, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper348/Authors", "ICLR.cc/2020/Conference/Paper348/Reviewers", "ICLR.cc/2020/Conference/Paper348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper348/-/Official_Comment"}}}, {"id": "HJgSymaLjH", "original": null, "number": 2, "cdate": 1573470940576, "ddate": null, "tcdate": 1573470940576, "tmdate": 1573470940576, "tddate": null, "forum": "rJe9lpEFDH", "replyto": "BylbgAdatr", "invitation": "ICLR.cc/2020/Conference/Paper348/-/Official_Comment", "content": {"title": "Response to R2", "comment": "(1) \"They study the sign gradient times its norm, not the classical sign gradient. \"\n\nThat is correct and the paper is completely open about that limitation; see footnote 2 in the paper. This allows us to leverage the gradient descent framework and also makes signGD converge with a constant step size to begin with. We are thus baffled by your claim that we \u201cchange the problem\u201d as there's still a lot of insight to be gained from this analysis.\n\nThis rescaling of the update direction can be rolled into the step size. In optimization it is fairly common to analyze a method with a special choice of step size (e.g., with exact line search) even if this deviates from practical choices. In practice, sign(S)GD is used with a decreasing step size schedule which can be seen as emulating the effect of the decreasing L1-norm of the gradient. Thus, although they are different algorithms, we do not expect the behaviour to be drastically different.\n\nIn addition to all of our results,  we explore a possible direction to understand the normalization aspect of sign gradient descent via a relaxed smoothness condition in the appendix.\n\n\n(2) \"The results are too general and do not focus on ML.\"\n\nWe analyze a method that is popular in ML. We view the fact that this analysis is kept as general as possible (and as specific as necessary) as a quality, not a drawback."}, "signatures": ["ICLR.cc/2020/Conference/Paper348/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper348/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lukas.balles@tuebingen.mpg.de", "f@bianp.net", "nicolas@le-roux.name"], "title": "The Geometry of Sign Gradient Descent", "authors": ["Lukas Balles", "Fabian Pedregosa", "Nicolas Le Roux"], "pdf": "/pdf/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "TL;DR": "We investigate which properties of an objective function favor sign gradient descent.", "abstract": "Sign gradient descent has become popular in machine learning due to its favorable communication cost in distributed optimization and its good performance in neural network training. However, we currently do not have a good understanding of which geometrical properties of the objective function determine the relative speed of sign gradient descent compared to standard gradient descent. In this work, we frame sign gradient descent as steepest descent with respect to the maximum norm. We review the steepest descent framework and the related concept of smoothness with respect to arbitrary norms.\nBy studying the smoothness constant resulting from the $L^\\infty$-geometry, we isolate properties of the objective which favor sign gradient descent relative to gradient descent. In short, we find two requirements on its Hessian: (i) some degree of ``diagonal dominance'' and (ii) the maximal eigenvalue being much larger than the average eigenvalue. We also clarify the meaning of a certain separable smoothness assumption used in previous analyses of sign gradient descent.\nExperiments verify the developed theory.", "keywords": ["Sign gradient descent", "signSGD", "steepest descent", "Adam"], "paperhash": "balles|the_geometry_of_sign_gradient_descent", "original_pdf": "/attachment/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "_bibtex": "@misc{\nballes2020the,\ntitle={The Geometry of Sign Gradient Descent},\nauthor={Lukas Balles and Fabian Pedregosa and Nicolas Le Roux},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe9lpEFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rJe9lpEFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper348/Authors", "ICLR.cc/2020/Conference/Paper348/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper348/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper348/Reviewers", "ICLR.cc/2020/Conference/Paper348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper348/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper348/Authors|ICLR.cc/2020/Conference/Paper348/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172763, "tmdate": 1576860543393, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper348/Authors", "ICLR.cc/2020/Conference/Paper348/Reviewers", "ICLR.cc/2020/Conference/Paper348/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper348/-/Official_Comment"}}}, {"id": "BylbgAdatr", "original": null, "number": 3, "cdate": 1571814888839, "ddate": null, "tcdate": 1571814888839, "tmdate": 1572972606559, "tddate": null, "forum": "rJe9lpEFDH", "replyto": "rJe9lpEFDH", "invitation": "ICLR.cc/2020/Conference/Paper348/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper tries to study the sign gradient descent by the squared maximum norm.  The authors clarify the meaning of a certain separable smoothness assumption using previous studies for signed gradient descent methods.\n\n1. The authors change the problem. They study the sign gradient times its norm, not the classical sign gradient. \nIn fact, this change dramatically changes the flow in continuous time. \n\n2. The results are too general, which does not focus on machine learning problems. \n\n3. The paper is clearly not written well with many typos. The organization of the paper also needs to be improved a lot for publication. \n\nFor these reasons, I clearly reject this paper for publications.  "}, "signatures": ["ICLR.cc/2020/Conference/Paper348/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper348/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lukas.balles@tuebingen.mpg.de", "f@bianp.net", "nicolas@le-roux.name"], "title": "The Geometry of Sign Gradient Descent", "authors": ["Lukas Balles", "Fabian Pedregosa", "Nicolas Le Roux"], "pdf": "/pdf/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "TL;DR": "We investigate which properties of an objective function favor sign gradient descent.", "abstract": "Sign gradient descent has become popular in machine learning due to its favorable communication cost in distributed optimization and its good performance in neural network training. However, we currently do not have a good understanding of which geometrical properties of the objective function determine the relative speed of sign gradient descent compared to standard gradient descent. In this work, we frame sign gradient descent as steepest descent with respect to the maximum norm. We review the steepest descent framework and the related concept of smoothness with respect to arbitrary norms.\nBy studying the smoothness constant resulting from the $L^\\infty$-geometry, we isolate properties of the objective which favor sign gradient descent relative to gradient descent. In short, we find two requirements on its Hessian: (i) some degree of ``diagonal dominance'' and (ii) the maximal eigenvalue being much larger than the average eigenvalue. We also clarify the meaning of a certain separable smoothness assumption used in previous analyses of sign gradient descent.\nExperiments verify the developed theory.", "keywords": ["Sign gradient descent", "signSGD", "steepest descent", "Adam"], "paperhash": "balles|the_geometry_of_sign_gradient_descent", "original_pdf": "/attachment/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "_bibtex": "@misc{\nballes2020the,\ntitle={The Geometry of Sign Gradient Descent},\nauthor={Lukas Balles and Fabian Pedregosa and Nicolas Le Roux},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe9lpEFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJe9lpEFDH", "replyto": "rJe9lpEFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper348/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper348/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575633153656, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper348/Reviewers"], "noninvitees": [], "tcdate": 1570237753444, "tmdate": 1575633153675, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper348/-/Official_Review"}}}, {"id": "r1gsMvgpFH", "original": null, "number": 1, "cdate": 1571780370771, "ddate": null, "tcdate": 1571780370771, "tmdate": 1572972606515, "tddate": null, "forum": "rJe9lpEFDH", "replyto": "rJe9lpEFDH", "invitation": "ICLR.cc/2020/Conference/Paper348/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper analyzes the performance of sign gradient descent as a function of the \u201cgeometry\u201d of the objective. They contrast the performance of sign GD to gradient descent. Both algorithms are steepest descent methods with respect to different norms. Overall, I do not think the insights claimed in the paper are especially novel. It is already known that the choice of a norm can have a significant impact on the speed of steepest descent but practically, the performance depends on quantities that are unknown such as the norm of the gradient over the specific trajectory of the algorithm. I do not find the arguments in section 4 especially convincing as they seem rather high-level to me. Instead I would find it more valuable if one could make a statement about the performance of signGD as a function of known quantities. One could perhaps start by analyzing a specific class of functions (quadratics, Polyak-Lojasiewicz functions, \u2026).\n\nPrior work\nIt seems to me that most results discussed in the paper are already known in the optimization community. The book by Boyd & Vandenberghe (cited by the authors) has an entire section on the \u201cChoice of norm for steepest descent\u201d where they indeed explain that the choice of the norm used for steepest descent can potentially have a dramatic effect on the convergence rate. Can the authors explain how they see their contribution compared to prior work? I could concede that the result of Proposition 3 is somewhat novel, although I think the authors should discuss prior work on axis-alignment, which dates back to \nBecker, Sue, and Yann Le Cun. \"Improving the convergence of back-propagation learning with second order methods.\" Proceedings of the 1988 connectionist models summer school. 1988.\n\nSection 4\nThe discussion in section 4 is high-level and I don\u2019t see any particular insight one gains over what\u2019s already known (see again book by Boyd & Vandenberghe). As explained by the authors, the comparison between signGD and GD will depend on the trajectory being followed.\n\nStochastic setting\nCan the authors comment on generalizing their results to a stochastic setting? Since the gradients have a larger norm when using the infinity norm, I would expect that the variance is also larger.\n\nExperimental results\nThe empirical results on the neural networks do not seem especially convincing, and the authors do seem to acknowledge this. What aspect of the analysis or the experimental setup do you expect to be responsible for this? Have you considered optimizing smaller models first? (something in between the quadratic problem in section 5.1 and the neural net in section 5.2. Some sort of ablation study where one strips away various components (batch-norm, residual connections, \u2026.) of a neural network would also be valuable.\n\nGeneralization\nOne aspect that might be worth investigating is the generalization ability of steepest descent for different norms. There is already some prior work for adaptive methods that could perhaps be generalized, see e.g.\nWilson, Ashia C., et al. \"The marginal value of adaptive gradient methods in machine learning.\" Advances in Neural Information Processing Systems. 2017.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper348/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper348/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lukas.balles@tuebingen.mpg.de", "f@bianp.net", "nicolas@le-roux.name"], "title": "The Geometry of Sign Gradient Descent", "authors": ["Lukas Balles", "Fabian Pedregosa", "Nicolas Le Roux"], "pdf": "/pdf/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "TL;DR": "We investigate which properties of an objective function favor sign gradient descent.", "abstract": "Sign gradient descent has become popular in machine learning due to its favorable communication cost in distributed optimization and its good performance in neural network training. However, we currently do not have a good understanding of which geometrical properties of the objective function determine the relative speed of sign gradient descent compared to standard gradient descent. In this work, we frame sign gradient descent as steepest descent with respect to the maximum norm. We review the steepest descent framework and the related concept of smoothness with respect to arbitrary norms.\nBy studying the smoothness constant resulting from the $L^\\infty$-geometry, we isolate properties of the objective which favor sign gradient descent relative to gradient descent. In short, we find two requirements on its Hessian: (i) some degree of ``diagonal dominance'' and (ii) the maximal eigenvalue being much larger than the average eigenvalue. We also clarify the meaning of a certain separable smoothness assumption used in previous analyses of sign gradient descent.\nExperiments verify the developed theory.", "keywords": ["Sign gradient descent", "signSGD", "steepest descent", "Adam"], "paperhash": "balles|the_geometry_of_sign_gradient_descent", "original_pdf": "/attachment/74fba520b221a41520e6a07f135be2395f9e4bca.pdf", "_bibtex": "@misc{\nballes2020the,\ntitle={The Geometry of Sign Gradient Descent},\nauthor={Lukas Balles and Fabian Pedregosa and Nicolas Le Roux},\nyear={2020},\nurl={https://openreview.net/forum?id=rJe9lpEFDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rJe9lpEFDH", "replyto": "rJe9lpEFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper348/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper348/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575633153656, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper348/Reviewers"], "noninvitees": [], "tcdate": 1570237753444, "tmdate": 1575633153675, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper348/-/Official_Review"}}}], "count": 9}