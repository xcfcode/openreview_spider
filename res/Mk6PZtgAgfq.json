{"notes": [{"id": "Mk6PZtgAgfq", "original": "FlGjcqNlMOW", "number": 1166, "cdate": 1601308130899, "ddate": null, "tcdate": 1601308130899, "tmdate": 1615996150420, "tddate": null, "forum": "Mk6PZtgAgfq", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator", "authorids": ["~Max_B_Paulus1", "~Chris_J._Maddison1", "~Andreas_Krause1"], "authors": ["Max B Paulus", "Chris J. Maddison", "Andreas Krause"], "keywords": ["gumbel", "softmax", "gumbel-softmax", "straight-through", "straightthrough", "rao", "rao-blackwell"], "abstract": "Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.", "one-sentence_summary": "We reduce the variance of the straight-through Gumbel-Softmax estimator to improve its performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paulus|raoblackwellizing_the_straightthrough_gumbelsoftmax_gradient_estimator", "supplementary_material": "/attachment/360b5c736614b92f048e24e213e058263c7a3290.zip", "pdf": "/pdf/99dded8f5b416c9596f112afe789c337af877339.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaulus2021raoblackwellizing,\ntitle={Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator},\nauthor={Max B Paulus and Chris J. Maddison and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Mk6PZtgAgfq}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "lPZz0Cbwxyn", "original": null, "number": 1, "cdate": 1610040483245, "ddate": null, "tcdate": 1610040483245, "tmdate": 1610474088436, "tddate": null, "forum": "Mk6PZtgAgfq", "replyto": "Mk6PZtgAgfq", "invitation": "ICLR.cc/2021/Conference/Paper1166/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Oral)", "comment": "The paper presents a variance reduction technique to the Straight-Through version of the Gumbel-Softmax estimator. The technique is relying on the truncated Gumbel of Maddison et al. I share the excitement of the reviewers about this work and I expect this technique to further influence the field. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator", "authorids": ["~Max_B_Paulus1", "~Chris_J._Maddison1", "~Andreas_Krause1"], "authors": ["Max B Paulus", "Chris J. Maddison", "Andreas Krause"], "keywords": ["gumbel", "softmax", "gumbel-softmax", "straight-through", "straightthrough", "rao", "rao-blackwell"], "abstract": "Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.", "one-sentence_summary": "We reduce the variance of the straight-through Gumbel-Softmax estimator to improve its performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paulus|raoblackwellizing_the_straightthrough_gumbelsoftmax_gradient_estimator", "supplementary_material": "/attachment/360b5c736614b92f048e24e213e058263c7a3290.zip", "pdf": "/pdf/99dded8f5b416c9596f112afe789c337af877339.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaulus2021raoblackwellizing,\ntitle={Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator},\nauthor={Max B Paulus and Chris J. Maddison and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Mk6PZtgAgfq}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Mk6PZtgAgfq", "replyto": "Mk6PZtgAgfq", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040483232, "tmdate": 1610474088421, "id": "ICLR.cc/2021/Conference/Paper1166/-/Decision"}}}, {"id": "DVdOEf4LS7G", "original": null, "number": 2, "cdate": 1603893623167, "ddate": null, "tcdate": 1603893623167, "tmdate": 1606761689440, "tddate": null, "forum": "Mk6PZtgAgfq", "replyto": "Mk6PZtgAgfq", "invitation": "ICLR.cc/2021/Conference/Paper1166/-/Official_Review", "content": {"title": "cheap and effective variance reduction trick to improve a widely adopted gradient estimator (for discrete distributions)", "review": "\n\nThis paper introduces the Rao-blackwellization technique to reduce the variance of the straight-through gumbel-softmax gradient (STGS) estimator wrt the parameters of discrete distributions. The proposed method introduces almost trivial computational costs (relative to function evaluations) and is empirically and theoretically shown to systematically improve STGS. \n\nI don\u2019t have a lot of nitpicking to make for this paper, as it is quite well executed. The proposed method is very clean and the improvement over the STGS baseline is very consistent, and makes it even competitive with concrete-relaxation in the discrete latent variable model experiment. \n\nDetails:\n\nWhy not show the curve of ELBO during training, but the arrival-time-at-certain-thresholds in Fig 2-c? \nLast paragraph of sec 5.4: The larger batch size here also reduces the variance of minibatch SGD, not just the variance of $\\nabla_{GR}$ in (13). In fact each instance is a different approximate posterior, which has different base variance. This makes the discussion in 3.3 a bit misleading. \n\nSuggestions:\nFor figure 1, perhaps visualize the variance of both separately. An improvement by 2 is not that meaningful if the variances of both are >> 2. \n\n-- After rebuttal\n\nThank you for revising the paper. I've read the revised section, and stand by my original evaluation. ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1166/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator", "authorids": ["~Max_B_Paulus1", "~Chris_J._Maddison1", "~Andreas_Krause1"], "authors": ["Max B Paulus", "Chris J. Maddison", "Andreas Krause"], "keywords": ["gumbel", "softmax", "gumbel-softmax", "straight-through", "straightthrough", "rao", "rao-blackwell"], "abstract": "Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.", "one-sentence_summary": "We reduce the variance of the straight-through Gumbel-Softmax estimator to improve its performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paulus|raoblackwellizing_the_straightthrough_gumbelsoftmax_gradient_estimator", "supplementary_material": "/attachment/360b5c736614b92f048e24e213e058263c7a3290.zip", "pdf": "/pdf/99dded8f5b416c9596f112afe789c337af877339.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaulus2021raoblackwellizing,\ntitle={Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator},\nauthor={Max B Paulus and Chris J. Maddison and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Mk6PZtgAgfq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Mk6PZtgAgfq", "replyto": "Mk6PZtgAgfq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1166/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538125214, "tmdate": 1606915795646, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1166/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1166/-/Official_Review"}}}, {"id": "c621Ks-ECY", "original": null, "number": 3, "cdate": 1603937357193, "ddate": null, "tcdate": 1603937357193, "tmdate": 1605714461762, "tddate": null, "forum": "Mk6PZtgAgfq", "replyto": "Mk6PZtgAgfq", "invitation": "ICLR.cc/2021/Conference/Paper1166/-/Official_Review", "content": {"title": "Method allows training of gumbel straight-through at lower temperatures, but lower temperature gains seem small", "review": "Summary:\n* This paper proposes a Rao-Blackwellized version of the straight-through gumbel-softmax gradient (STGS) estimator.\n* The Gumbel-Rao estimator remains single-evaluation (but multiple sample), does not have higher variance than the original straight-through estimator.\n* The estimator exhibits lower variance at lower temperatures in the experiments.\n\nContributions:\n* Proposes a single-evaluation estimator that cannot have higher variance than the STGS gradient estimator.\n* Demonstrates effectiveness of proposed estimator in terms of the variance of the gradient estimator and the ELBO on a toy task, a simple parsing task (ListOps), and a mixture model for MNIST.\n\nStrengths:\n* The method is simple and the computational overhead is very small compared to the original STGS estimator.\n* The empirical results support lower variance claims and effectiveness at lower temperature.\n\nWeaknesses:\n* I am not convinced that the relative gains from training at lower temperatures are significant.\n* The overall gains over ST-GS seem to be modest on MNIST as well as the L <= 50 setting in ListOps.\n* In the ListOps experiments, lower temperatures barely achieved better accuracy.\n\nDecision: Marginally below acceptance threshold\n* Improving gradient estimators for discrete latent variable models is an important problem.\n* The method is straightforward and the claims of performing better at lower temperatures are supported by empirical evidence.\n* However, the overall performance on the ListOps dataset is lower than related work [1], and there does not appear to be a large gain from low temperatures.\n\nQuestions:\n* The main argument of this paper hinges on the claim that lower temperatures result in lower bias of the gradient estimator. This claim seems reasonable, and is supported by figure 2b. Is there a proof or citation for it, and do we know more? It would be nice to know how variance and bias are traded off, as that would tell us how much (or how little) we could gain from training at lower temperatures.\n* Is there an explanation for the difference in performance between the 99% accuracy obtained in Havrylov et. al. 2019 [1] and the performance obtained at low temperatures in this paper?\n* How does this method perform versus the estimator proposed in Pervez et. al. [2], which is also single-evaluation?\n\nSuggestions:\n* The GR estimator is not guaranteed to have lower variance than ST-GS, just not higher.\n* Is there an application where lower temperatures are necessary for training? That would strengthen the argument.\n\n[1] Serhii Havrylov, German Kruszewski, and Armand Joulin. Cooperative Learning of Disjoint Syntax and Semantics. In Proceedings of NAACL 2019.\n\n[2] Pervez, A., Cohen, T., & Gavves, E. 2020. Low Bias Low Variance Gradient Estimates for Hierarchical Boolean Stochastic Networks. ICML 2020.\n\nEdited score after author comments.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1166/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator", "authorids": ["~Max_B_Paulus1", "~Chris_J._Maddison1", "~Andreas_Krause1"], "authors": ["Max B Paulus", "Chris J. Maddison", "Andreas Krause"], "keywords": ["gumbel", "softmax", "gumbel-softmax", "straight-through", "straightthrough", "rao", "rao-blackwell"], "abstract": "Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.", "one-sentence_summary": "We reduce the variance of the straight-through Gumbel-Softmax estimator to improve its performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paulus|raoblackwellizing_the_straightthrough_gumbelsoftmax_gradient_estimator", "supplementary_material": "/attachment/360b5c736614b92f048e24e213e058263c7a3290.zip", "pdf": "/pdf/99dded8f5b416c9596f112afe789c337af877339.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaulus2021raoblackwellizing,\ntitle={Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator},\nauthor={Max B Paulus and Chris J. Maddison and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Mk6PZtgAgfq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Mk6PZtgAgfq", "replyto": "Mk6PZtgAgfq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1166/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538125214, "tmdate": 1606915795646, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1166/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1166/-/Official_Review"}}}, {"id": "udGDPfEkKXb", "original": null, "number": 10, "cdate": 1605714357212, "ddate": null, "tcdate": 1605714357212, "tmdate": 1605714357212, "tddate": null, "forum": "Mk6PZtgAgfq", "replyto": "m9Y06a9MerE", "invitation": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment", "content": {"title": "Great response", "comment": "Thank you for the thorough response, and correcting my interpretation of the argument. All of my concerns have been addressed, although I do have some lingering concerns about the performance on ListOps in comparison to Havrylov et. al. I am still worried that there is a large gap between the upper bound performance by training via the proposed gradient estimator vs an unbiased one, but that does not detract from the contribution of the paper which demonstrates improvements over the ST estimator baseline. There are enough confounding factors, in particular model differences, that it is unclear whether the results in Havrylov et. al. are comparable. I will update my score accordingly."}, "signatures": ["ICLR.cc/2021/Conference/Paper1166/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator", "authorids": ["~Max_B_Paulus1", "~Chris_J._Maddison1", "~Andreas_Krause1"], "authors": ["Max B Paulus", "Chris J. Maddison", "Andreas Krause"], "keywords": ["gumbel", "softmax", "gumbel-softmax", "straight-through", "straightthrough", "rao", "rao-blackwell"], "abstract": "Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.", "one-sentence_summary": "We reduce the variance of the straight-through Gumbel-Softmax estimator to improve its performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paulus|raoblackwellizing_the_straightthrough_gumbelsoftmax_gradient_estimator", "supplementary_material": "/attachment/360b5c736614b92f048e24e213e058263c7a3290.zip", "pdf": "/pdf/99dded8f5b416c9596f112afe789c337af877339.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaulus2021raoblackwellizing,\ntitle={Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator},\nauthor={Max B Paulus and Chris J. Maddison and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Mk6PZtgAgfq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mk6PZtgAgfq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1166/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1166/Authors|ICLR.cc/2021/Conference/Paper1166/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862893, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment"}}}, {"id": "gA-brMwKEl3", "original": null, "number": 3, "cdate": 1605711991484, "ddate": null, "tcdate": 1605711991484, "tmdate": 1605713863735, "tddate": null, "forum": "Mk6PZtgAgfq", "replyto": "DVdOEf4LS7G", "invitation": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment", "content": {"title": "We clarified Section 3.3.", "comment": "**Thank you very much for your review and the positive reception of our work.**\n\nWe apologize for any confusion the discussion in 3.3 may have caused. We have addressed your concern and clarified this point in the revision by adding a footnote: Indeed, each instance has a different approximate posterior. GR estimates \u201cjoint parameters\u201d that parameterize this approximate posterior distribution. The dataset is assumed i.i.d and the expectation over the data is omitted to improve readability.  \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1166/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator", "authorids": ["~Max_B_Paulus1", "~Chris_J._Maddison1", "~Andreas_Krause1"], "authors": ["Max B Paulus", "Chris J. Maddison", "Andreas Krause"], "keywords": ["gumbel", "softmax", "gumbel-softmax", "straight-through", "straightthrough", "rao", "rao-blackwell"], "abstract": "Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.", "one-sentence_summary": "We reduce the variance of the straight-through Gumbel-Softmax estimator to improve its performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paulus|raoblackwellizing_the_straightthrough_gumbelsoftmax_gradient_estimator", "supplementary_material": "/attachment/360b5c736614b92f048e24e213e058263c7a3290.zip", "pdf": "/pdf/99dded8f5b416c9596f112afe789c337af877339.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaulus2021raoblackwellizing,\ntitle={Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator},\nauthor={Max B Paulus and Chris J. Maddison and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Mk6PZtgAgfq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mk6PZtgAgfq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1166/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1166/Authors|ICLR.cc/2021/Conference/Paper1166/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862893, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment"}}}, {"id": "LrpEg-vmpw", "original": null, "number": 2, "cdate": 1605711756314, "ddate": null, "tcdate": 1605711756314, "tmdate": 1605713852870, "tddate": null, "forum": "Mk6PZtgAgfq", "replyto": "df7KKqW0LRD", "invitation": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment", "content": {"title": "We implemented an additional baseline.", "comment": "**Thank you very much for your review and the positive reception of our work.**\n\nWe have addressed your concern and incorporated an additional single-evaluation baseline, i.e., Pervez et al. (2020), based on the suggestion of R1. The reason we do not compare to multi-evaluation estimators (e.g. VIMCO) is that is very difficult to provide a level playing field: Each training iteration is significantly more expensive and even when basing comparisons on training time instead, these estimators may require significantly more memory, possibly training on multiple GPUs, which confounds comparisons.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1166/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator", "authorids": ["~Max_B_Paulus1", "~Chris_J._Maddison1", "~Andreas_Krause1"], "authors": ["Max B Paulus", "Chris J. Maddison", "Andreas Krause"], "keywords": ["gumbel", "softmax", "gumbel-softmax", "straight-through", "straightthrough", "rao", "rao-blackwell"], "abstract": "Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.", "one-sentence_summary": "We reduce the variance of the straight-through Gumbel-Softmax estimator to improve its performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paulus|raoblackwellizing_the_straightthrough_gumbelsoftmax_gradient_estimator", "supplementary_material": "/attachment/360b5c736614b92f048e24e213e058263c7a3290.zip", "pdf": "/pdf/99dded8f5b416c9596f112afe789c337af877339.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaulus2021raoblackwellizing,\ntitle={Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator},\nauthor={Max B Paulus and Chris J. Maddison and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Mk6PZtgAgfq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mk6PZtgAgfq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1166/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1166/Authors|ICLR.cc/2021/Conference/Paper1166/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862893, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment"}}}, {"id": "BTxE28VJgT", "original": null, "number": 4, "cdate": 1605712250947, "ddate": null, "tcdate": 1605712250947, "tmdate": 1605713837169, "tddate": null, "forum": "Mk6PZtgAgfq", "replyto": "c621Ks-ECY", "invitation": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment", "content": {"title": "MNIST example is a great example of benefits of training at other (lower) temperatures.", "comment": "*> \u201cRelative gains from training at lower temperatures are [not] significant\u201d/ \u201cOverall gains over ST-GS seem to be modest on MNIST\u201d/ \u201cThere does not appear to be a large gain from low temperatures.\u201d/ \u201cIs there an application where lower temperatures are necessary for training?\u201d*\n\nWe politely disagree. We believe our VAE example on MNIST is a great example to demonstrate the benefits of lower temperature training and the improvements are significant: \n\nWe achieve up to two nats improvement. To put this into perspective, this is comparable to the improvements of other important innovations in VAE training (e.g., IWAE by Burda et al., 2016 [3]). As R2 points out the improvements are also \u201cvery consistent\u201d, and even \u201ccompetitive with the concrete-relaxation\u201d. Indeed, our results indicate that these improvements are mainly caused by the ability of our estimator to facilitate lower temperature training, because \u201cthe best models using ST-GS trained at an average temperature of 0.65, while the best models using GR-MC1000 trained at an average temperature of 0.35\u201d.\n\nAdditional References:\n[3] Burda, Y., Grosse, R., & Salakhutdinov, R. 2016. Importance Weighted Autoencoders. ICLR 2016.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1166/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator", "authorids": ["~Max_B_Paulus1", "~Chris_J._Maddison1", "~Andreas_Krause1"], "authors": ["Max B Paulus", "Chris J. Maddison", "Andreas Krause"], "keywords": ["gumbel", "softmax", "gumbel-softmax", "straight-through", "straightthrough", "rao", "rao-blackwell"], "abstract": "Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.", "one-sentence_summary": "We reduce the variance of the straight-through Gumbel-Softmax estimator to improve its performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paulus|raoblackwellizing_the_straightthrough_gumbelsoftmax_gradient_estimator", "supplementary_material": "/attachment/360b5c736614b92f048e24e213e058263c7a3290.zip", "pdf": "/pdf/99dded8f5b416c9596f112afe789c337af877339.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaulus2021raoblackwellizing,\ntitle={Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator},\nauthor={Max B Paulus and Chris J. Maddison and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Mk6PZtgAgfq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mk6PZtgAgfq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1166/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1166/Authors|ICLR.cc/2021/Conference/Paper1166/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862893, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment"}}}, {"id": "b2Ept5Vt6qx", "original": null, "number": 5, "cdate": 1605712313781, "ddate": null, "tcdate": 1605712313781, "tmdate": 1605713826550, "tddate": null, "forum": "Mk6PZtgAgfq", "replyto": "c621Ks-ECY", "invitation": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment", "content": {"title": "We extend the space of trainable temperatures.", "comment": "*> [Lower temperatures result in lower bias of the gradient estimator. [...] Proof or citation?]*\n\nWe would like to clarify that the contribution of our paper does not hinge on this argument. Still, you make a great point and it has made us reconsider the messaging. We have clarified the following two points throughout the paper.\n\n* The key advantage of our estimator is that it produces variance improvements for all (!) temperatures, thus extending the region of suitable temperatures over which one can tune. This allows a practitioner to explore an expanded set when trading-off of bias and variance.\n* All empirical evidence (including our own) suggests that lower temperatures result in lower bias for the gradient estimator. This has turned into a widely held belief in the community, but as far as we know, it has not been proven. What is known is that the bias in the forward pass of relaxed estimators is reduced: As the temperature is reduced, the relaxed loss converges (see for example Paulus et al. (2020) and references therein) to the true loss. While we are not aware of any work that addresses the convergence of the derivative, it may be possible to use the result in Rudin [1976, Theorem 7.7]. This is an interesting avenue for future work! \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1166/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator", "authorids": ["~Max_B_Paulus1", "~Chris_J._Maddison1", "~Andreas_Krause1"], "authors": ["Max B Paulus", "Chris J. Maddison", "Andreas Krause"], "keywords": ["gumbel", "softmax", "gumbel-softmax", "straight-through", "straightthrough", "rao", "rao-blackwell"], "abstract": "Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.", "one-sentence_summary": "We reduce the variance of the straight-through Gumbel-Softmax estimator to improve its performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paulus|raoblackwellizing_the_straightthrough_gumbelsoftmax_gradient_estimator", "supplementary_material": "/attachment/360b5c736614b92f048e24e213e058263c7a3290.zip", "pdf": "/pdf/99dded8f5b416c9596f112afe789c337af877339.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaulus2021raoblackwellizing,\ntitle={Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator},\nauthor={Max B Paulus and Chris J. Maddison and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Mk6PZtgAgfq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mk6PZtgAgfq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1166/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1166/Authors|ICLR.cc/2021/Conference/Paper1166/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862893, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment"}}}, {"id": "Pn7sAiLk-9N", "original": null, "number": 7, "cdate": 1605712702600, "ddate": null, "tcdate": 1605712702600, "tmdate": 1605713816107, "tddate": null, "forum": "Mk6PZtgAgfq", "replyto": "c621Ks-ECY", "invitation": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment", "content": {"title": "There are significant experimental differences between [1] and us, which we now clearly highlight.", "comment": "*> \u201cThe overall performance on the ListOps dataset is lower than related work. [...] Is there an explanation for the difference?\u201d* \n\nThank you for raising this point. Yes, there is an explanation. We were interested in a controlled setting to investigate the influence of K and \\tau, while [1] focus on cracking ListOps. These are the most important experimental differences:\n* [1] does not use single-evaluation estimators, we do: They report near perfect accuracy only when using the self-critical baseline. This baseline requires an additional forward pass. All their single-evaluation results are in a similar ballpark as ours accounting for the additional differences below.\n* [1] uses extensive hyperparameter tuning, we do not: They tune learning rate, learning rate schedule, weight decay, entropy regularisation, variance reduction hyperparameters, optimizer (Adadelta), number of updates for PPO, leaf transformations and train for 300 epochs. In contrast, we only tune the (constant) learning rate via gridsearch and use SGD (Appendix D1) for each temperature and train for ten epochs.\n* [1] uses customized training procedures, we are simply plug-in: They use PPO, gradient normalization, different control variates and entropy regularization. We simply plug our estimator into the model from Choi et al. (2018). \n* [1] uses a model with more parameters than us: We do not use any leaf LSTM. It improves performance [Choi et al. (2018)], but may also confound tree learning (e.g. leaf-LSTM may learn to solve the task, making tree obsolete), so we do not use it. \n* [1] uses more training data than us: We reserved 10% of the training set for validation, while they use less than 2%. \n\nWe think this is an important discussion, so in our revision, we added a reference to [1] in the main body and highlighted the most important differences in the appendix.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1166/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator", "authorids": ["~Max_B_Paulus1", "~Chris_J._Maddison1", "~Andreas_Krause1"], "authors": ["Max B Paulus", "Chris J. Maddison", "Andreas Krause"], "keywords": ["gumbel", "softmax", "gumbel-softmax", "straight-through", "straightthrough", "rao", "rao-blackwell"], "abstract": "Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.", "one-sentence_summary": "We reduce the variance of the straight-through Gumbel-Softmax estimator to improve its performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paulus|raoblackwellizing_the_straightthrough_gumbelsoftmax_gradient_estimator", "supplementary_material": "/attachment/360b5c736614b92f048e24e213e058263c7a3290.zip", "pdf": "/pdf/99dded8f5b416c9596f112afe789c337af877339.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaulus2021raoblackwellizing,\ntitle={Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator},\nauthor={Max B Paulus and Chris J. Maddison and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Mk6PZtgAgfq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mk6PZtgAgfq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1166/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1166/Authors|ICLR.cc/2021/Conference/Paper1166/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862893, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment"}}}, {"id": "sFYHnImdnFD", "original": null, "number": 9, "cdate": 1605713161260, "ddate": null, "tcdate": 1605713161260, "tmdate": 1605713183591, "tddate": null, "forum": "Mk6PZtgAgfq", "replyto": "Mk6PZtgAgfq", "invitation": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment", "content": {"title": "We made revisions based on the feedback of the reviewers. ", "comment": "**We have incorporated the feedback of the reviewers and summarize here the changes in our revision (new upload!):**\n\n* Additional baselines (R1, R4): We ran experiments for FouST (Pervez et al., 2020, as kindly suggested by R1) where applicable (binary VAE): FouST improved clearly over ST, but is outperformed by our method. \n* Low temperatures and low bias (R1): We have clarified throughout the paper that our method extends the space of trainable temperatures to those that have lower bias, but higher variance. Empirically, these tend to be low temperatures. \n* ListOps experimental set-up (R1): We added a reference to Havrylov et al. (2019) in the main body and highlight the differences between their and our experimental set-up in the appendix to put results into perspective. \n* We clarified the discussion in 3.3 as suggested by R2 and added a footnote. \n\nPlease see individual responses below for more details. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1166/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator", "authorids": ["~Max_B_Paulus1", "~Chris_J._Maddison1", "~Andreas_Krause1"], "authors": ["Max B Paulus", "Chris J. Maddison", "Andreas Krause"], "keywords": ["gumbel", "softmax", "gumbel-softmax", "straight-through", "straightthrough", "rao", "rao-blackwell"], "abstract": "Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.", "one-sentence_summary": "We reduce the variance of the straight-through Gumbel-Softmax estimator to improve its performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paulus|raoblackwellizing_the_straightthrough_gumbelsoftmax_gradient_estimator", "supplementary_material": "/attachment/360b5c736614b92f048e24e213e058263c7a3290.zip", "pdf": "/pdf/99dded8f5b416c9596f112afe789c337af877339.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaulus2021raoblackwellizing,\ntitle={Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator},\nauthor={Max B Paulus and Chris J. Maddison and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Mk6PZtgAgfq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mk6PZtgAgfq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1166/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1166/Authors|ICLR.cc/2021/Conference/Paper1166/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862893, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment"}}}, {"id": "m9Y06a9MerE", "original": null, "number": 8, "cdate": 1605712782673, "ddate": null, "tcdate": 1605712782673, "tmdate": 1605712801469, "tddate": null, "forum": "Mk6PZtgAgfq", "replyto": "c621Ks-ECY", "invitation": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment", "content": {"title": "Thank you! We made revisions based on your feedback. ", "comment": "**Thank you very much for your thoughtful review. We have made revisions to our paper based on your feedback and address your concerns in more detail in the comments below.**\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1166/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator", "authorids": ["~Max_B_Paulus1", "~Chris_J._Maddison1", "~Andreas_Krause1"], "authors": ["Max B Paulus", "Chris J. Maddison", "Andreas Krause"], "keywords": ["gumbel", "softmax", "gumbel-softmax", "straight-through", "straightthrough", "rao", "rao-blackwell"], "abstract": "Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.", "one-sentence_summary": "We reduce the variance of the straight-through Gumbel-Softmax estimator to improve its performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paulus|raoblackwellizing_the_straightthrough_gumbelsoftmax_gradient_estimator", "supplementary_material": "/attachment/360b5c736614b92f048e24e213e058263c7a3290.zip", "pdf": "/pdf/99dded8f5b416c9596f112afe789c337af877339.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaulus2021raoblackwellizing,\ntitle={Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator},\nauthor={Max B Paulus and Chris J. Maddison and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Mk6PZtgAgfq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mk6PZtgAgfq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1166/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1166/Authors|ICLR.cc/2021/Conference/Paper1166/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862893, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment"}}}, {"id": "iZrhRtLK7xP", "original": null, "number": 6, "cdate": 1605712591286, "ddate": null, "tcdate": 1605712591286, "tmdate": 1605712591286, "tddate": null, "forum": "Mk6PZtgAgfq", "replyto": "c621Ks-ECY", "invitation": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment", "content": {"title": "We implemented the estimator in [2] and it is outperformed by our estimator.", "comment": "*> \u201cHow does this method perform [against] the estimator in [2]?*\n\nThanks, this is a cool paper! Unfortunately, it only works for binary variables, while ours works for all arities. Still, we implemented it as a baseline for our binary VAE and have included the results in a revision: The estimator from [2] improved over ST, but was outperformed by our estimator. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1166/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator", "authorids": ["~Max_B_Paulus1", "~Chris_J._Maddison1", "~Andreas_Krause1"], "authors": ["Max B Paulus", "Chris J. Maddison", "Andreas Krause"], "keywords": ["gumbel", "softmax", "gumbel-softmax", "straight-through", "straightthrough", "rao", "rao-blackwell"], "abstract": "Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.", "one-sentence_summary": "We reduce the variance of the straight-through Gumbel-Softmax estimator to improve its performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paulus|raoblackwellizing_the_straightthrough_gumbelsoftmax_gradient_estimator", "supplementary_material": "/attachment/360b5c736614b92f048e24e213e058263c7a3290.zip", "pdf": "/pdf/99dded8f5b416c9596f112afe789c337af877339.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaulus2021raoblackwellizing,\ntitle={Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator},\nauthor={Max B Paulus and Chris J. Maddison and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Mk6PZtgAgfq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mk6PZtgAgfq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1166/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1166/Authors|ICLR.cc/2021/Conference/Paper1166/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923862893, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1166/-/Official_Comment"}}}, {"id": "df7KKqW0LRD", "original": null, "number": 1, "cdate": 1603730249985, "ddate": null, "tcdate": 1603730249985, "tmdate": 1605024513796, "tddate": null, "forum": "Mk6PZtgAgfq", "replyto": "Mk6PZtgAgfq", "invitation": "ICLR.cc/2021/Conference/Paper1166/-/Official_Review", "content": {"title": "Good work", "review": "Summary: The paper presents a new way algorithm to compute the straight-through variant of the Gumbel Softmax gradient estimator. The method does not change the estimator's bias, but provably reduces its variance (with a small overhead, using Rao-blackwellization). The new estimator shows good performance on different tasks, and appears to lead to more efficient optimization for lower temperatures (lower bias).\n\nClarity: The paper is well written.\n\nOriginality: The use of Rao-blackwellization in the proposed way is, up to the best of my knowledge, novel.\n\nPros of the paper and significance: \n- Relaxation-based gradient estimators are widely used, and the proposed method may their variance quite significantly.\n- The proposed algorithm has a clear justification from a theoretical perspective, and admits a simple implementation.\n- The proposed algorithm does not require additional model evaluations, and thus may lead to large reductions in variance without incurring a high computational cost.\n- The proposed method leads to more efficient optimization at lower temperatures (lower temperature translates to lower bias, but often higher variances).\n\nCons: I'd say one thing that could be included are additional baselines in the experimental section. There are other estimators that may be use. For instance, you could compare against VIMCO. While this is a different type of estimator (non single evaluation, not based on relaxations), it could be interesting to see how the results compare using this estimator too.\n\nRecommendation: Accept (reasons in the \"pros\" list above).", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1166/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1166/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator", "authorids": ["~Max_B_Paulus1", "~Chris_J._Maddison1", "~Andreas_Krause1"], "authors": ["Max B Paulus", "Chris J. Maddison", "Andreas Krause"], "keywords": ["gumbel", "softmax", "gumbel-softmax", "straight-through", "straightthrough", "rao", "rao-blackwell"], "abstract": "Gradient estimation in models with discrete latent variables is a challenging problem, because the simplest unbiased estimators tend to have high variance. To counteract this, modern estimators either introduce bias, rely on multiple function evaluations, or use learned, input-dependent baselines. Thus, there is a need for estimators that require minimal tuning, are computationally cheap, and have low mean squared error. In this paper, we show that the variance of the straight-through variant of the popular Gumbel-Softmax estimator can be reduced through Rao-Blackwellization without increasing the number of function evaluations. This provably reduces the mean squared error. We empirically demonstrate that this leads to variance reduction, faster convergence, and generally improved performance in two unsupervised latent variable models.", "one-sentence_summary": "We reduce the variance of the straight-through Gumbel-Softmax estimator to improve its performance. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "paulus|raoblackwellizing_the_straightthrough_gumbelsoftmax_gradient_estimator", "supplementary_material": "/attachment/360b5c736614b92f048e24e213e058263c7a3290.zip", "pdf": "/pdf/99dded8f5b416c9596f112afe789c337af877339.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\npaulus2021raoblackwellizing,\ntitle={Rao-Blackwellizing the Straight-Through Gumbel-Softmax Gradient Estimator},\nauthor={Max B Paulus and Chris J. Maddison and Andreas Krause},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Mk6PZtgAgfq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Mk6PZtgAgfq", "replyto": "Mk6PZtgAgfq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1166/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538125214, "tmdate": 1606915795646, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1166/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1166/-/Official_Review"}}}], "count": 14}