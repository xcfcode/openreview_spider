{"notes": [{"id": "HklOo0VFDH", "original": "SJehCfY_Dr", "number": 1325, "cdate": 1569439392101, "ddate": null, "tcdate": 1569439392101, "tmdate": 1583912024883, "tddate": null, "forum": "HklOo0VFDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["syed.zaidi1@monash.edu", "t.cohn@unimelb.edu.au", "reza.haffari@gmail.com"], "title": "Decoding As Dynamic Programming For Recurrent Autoregressive Models", "authors": ["Najam Zaidi", "Trevor Cohn", "Gholamreza Haffari"], "pdf": "/pdf/cbec9ce6e7b1e593eac93abfff0fd90fd2ee6ed8.pdf", "TL;DR": "Approximate inference using dynamic programming for Autoregressive models.", "abstract": "Decoding in autoregressive models (ARMs) consists of searching for a high scoring output sequence under the trained model.  Standard decoding methods, based on unidirectional greedy algorithm or beam search, are suboptimal due to error propagation and myopic decisions which do not account for future steps in the generation process. In this paper we present a novel decoding approach based on the method of auxiliary coordinates (Carreira-Perpinan & Wang, 2014) to address the aforementioned shortcomings.  Our method introduces discrete variables for output tokens,  and auxiliary continuous variables representing the states of the underlying ARM. The auxiliary variables lead to a factor graph approximation of the ARM, whose maximum a posteriori (MAP) inference is found exactly using dynamic programming. The MAP inference is then used to recreate an improved factor graph approximation of the ARM via updated auxiliary variables. We then extend our approach to decode in an ensemble of ARMs, possibly with different generation orders,  which is out of reach for the standard unidirectional decoding algorithms. Experiments on the text infilling task over SWAG and Daily Dialogue datasets show that our decoding method is superior to strong unidirectional decoding baselines.", "keywords": ["Decoding"], "paperhash": "zaidi|decoding_as_dynamic_programming_for_recurrent_autoregressive_models", "_bibtex": "@inproceedings{\nZaidi2020Decoding,\ntitle={Decoding As Dynamic Programming For Recurrent Autoregressive Models},\nauthor={Najam Zaidi and Trevor Cohn and Gholamreza Haffari},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklOo0VFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9be0a3759ff319c7a025837c36bf329cc33586bf.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "W5MOblSDJ", "original": null, "number": 1, "cdate": 1576798720564, "ddate": null, "tcdate": 1576798720564, "tmdate": 1576800916018, "tddate": null, "forum": "HklOo0VFDH", "replyto": "HklOo0VFDH", "invitation": "ICLR.cc/2020/Conference/Paper1325/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper proposes an approximate inference approach for decoding in autoregressive models, based on the method of auxiliary coordinates,  which uses iterative factor graph approximations of the model.  The approach leads to nice improvements in performance on a text infilling task.  The reviewers were generally positive about this paper, though there was a concern that more baselines are needed and discussion was very limited following the author responses.  I tend to agree with the authors that their results are convincing on the infilling task.  The impact of the paper is a bit limited by the lack of experiments on more standard decoding tasks, which, as the authors point out, would be challenging as their approach is computationally demanding.  Overall I believe this would be an interesting contribution to the ICLR community.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["syed.zaidi1@monash.edu", "t.cohn@unimelb.edu.au", "reza.haffari@gmail.com"], "title": "Decoding As Dynamic Programming For Recurrent Autoregressive Models", "authors": ["Najam Zaidi", "Trevor Cohn", "Gholamreza Haffari"], "pdf": "/pdf/cbec9ce6e7b1e593eac93abfff0fd90fd2ee6ed8.pdf", "TL;DR": "Approximate inference using dynamic programming for Autoregressive models.", "abstract": "Decoding in autoregressive models (ARMs) consists of searching for a high scoring output sequence under the trained model.  Standard decoding methods, based on unidirectional greedy algorithm or beam search, are suboptimal due to error propagation and myopic decisions which do not account for future steps in the generation process. In this paper we present a novel decoding approach based on the method of auxiliary coordinates (Carreira-Perpinan & Wang, 2014) to address the aforementioned shortcomings.  Our method introduces discrete variables for output tokens,  and auxiliary continuous variables representing the states of the underlying ARM. The auxiliary variables lead to a factor graph approximation of the ARM, whose maximum a posteriori (MAP) inference is found exactly using dynamic programming. The MAP inference is then used to recreate an improved factor graph approximation of the ARM via updated auxiliary variables. We then extend our approach to decode in an ensemble of ARMs, possibly with different generation orders,  which is out of reach for the standard unidirectional decoding algorithms. Experiments on the text infilling task over SWAG and Daily Dialogue datasets show that our decoding method is superior to strong unidirectional decoding baselines.", "keywords": ["Decoding"], "paperhash": "zaidi|decoding_as_dynamic_programming_for_recurrent_autoregressive_models", "_bibtex": "@inproceedings{\nZaidi2020Decoding,\ntitle={Decoding As Dynamic Programming For Recurrent Autoregressive Models},\nauthor={Najam Zaidi and Trevor Cohn and Gholamreza Haffari},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklOo0VFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9be0a3759ff319c7a025837c36bf329cc33586bf.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HklOo0VFDH", "replyto": "HklOo0VFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795709853, "tmdate": 1576800258705, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1325/-/Decision"}}}, {"id": "H1gEW1rhor", "original": null, "number": 4, "cdate": 1573830395689, "ddate": null, "tcdate": 1573830395689, "tmdate": 1573830395689, "tddate": null, "forum": "HklOo0VFDH", "replyto": "HklOo0VFDH", "invitation": "ICLR.cc/2020/Conference/Paper1325/-/Official_Comment", "content": {"title": "Reviewers, any comments on author response?", "comment": "Dear Reviewers, thanks for your thoughtful input on this submission!  The authors have now responded to your comments.  Please be sure to go through their replies and revisions.  If you have additional feedback or questions, it would be great to know.  The authors still have one more day to respond/revise further.  Thanks!\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1325/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1325/Area_Chair1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["syed.zaidi1@monash.edu", "t.cohn@unimelb.edu.au", "reza.haffari@gmail.com"], "title": "Decoding As Dynamic Programming For Recurrent Autoregressive Models", "authors": ["Najam Zaidi", "Trevor Cohn", "Gholamreza Haffari"], "pdf": "/pdf/cbec9ce6e7b1e593eac93abfff0fd90fd2ee6ed8.pdf", "TL;DR": "Approximate inference using dynamic programming for Autoregressive models.", "abstract": "Decoding in autoregressive models (ARMs) consists of searching for a high scoring output sequence under the trained model.  Standard decoding methods, based on unidirectional greedy algorithm or beam search, are suboptimal due to error propagation and myopic decisions which do not account for future steps in the generation process. In this paper we present a novel decoding approach based on the method of auxiliary coordinates (Carreira-Perpinan & Wang, 2014) to address the aforementioned shortcomings.  Our method introduces discrete variables for output tokens,  and auxiliary continuous variables representing the states of the underlying ARM. The auxiliary variables lead to a factor graph approximation of the ARM, whose maximum a posteriori (MAP) inference is found exactly using dynamic programming. The MAP inference is then used to recreate an improved factor graph approximation of the ARM via updated auxiliary variables. We then extend our approach to decode in an ensemble of ARMs, possibly with different generation orders,  which is out of reach for the standard unidirectional decoding algorithms. Experiments on the text infilling task over SWAG and Daily Dialogue datasets show that our decoding method is superior to strong unidirectional decoding baselines.", "keywords": ["Decoding"], "paperhash": "zaidi|decoding_as_dynamic_programming_for_recurrent_autoregressive_models", "_bibtex": "@inproceedings{\nZaidi2020Decoding,\ntitle={Decoding As Dynamic Programming For Recurrent Autoregressive Models},\nauthor={Najam Zaidi and Trevor Cohn and Gholamreza Haffari},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklOo0VFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9be0a3759ff319c7a025837c36bf329cc33586bf.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklOo0VFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1325/Authors", "ICLR.cc/2020/Conference/Paper1325/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1325/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1325/Reviewers", "ICLR.cc/2020/Conference/Paper1325/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1325/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1325/Authors|ICLR.cc/2020/Conference/Paper1325/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157727, "tmdate": 1576860547693, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1325/Authors", "ICLR.cc/2020/Conference/Paper1325/Reviewers", "ICLR.cc/2020/Conference/Paper1325/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1325/-/Official_Comment"}}}, {"id": "BkxjgFZ3oH", "original": null, "number": 3, "cdate": 1573816563053, "ddate": null, "tcdate": 1573816563053, "tmdate": 1573816563053, "tddate": null, "forum": "HklOo0VFDH", "replyto": "BygmgjVnYr", "invitation": "ICLR.cc/2020/Conference/Paper1325/-/Official_Comment", "content": {"title": "Reply to reviewer #3", "comment": "We appreciate the reviewer\u2019s constructive comments. \n\nQ. Novelty of the proposed solution.\n\nA. To the best of our knowledge, this is the first work to use the method of auxiliary variables in order to solve the decoding problem in ARMs. As discussed in our paper, the current approaches either use greedy/beam search or rely on techniques that involve relaxation of the discrete output space to continuous, followed by a projection step to convert the \u2018fractional\u2019 solution to an \u2018integral\u2019 solution . In contrast, our method does not require such a relaxation, hence does not need any projection step which may harm the solution. The experimental results show that our method outperforms not only popular decoding algorithm (i.e. greedy/beam search), but also TIGS which is a state-of-the-art inference method for the text infilling task. TIGS is a strong decoding method representing the inference approach of relaxing the discrete output space followed by projection. \n\nQ. What is the thought behind the choice of the optimizer? Why Nesterov\u2019s instead of plain GD? \n\nA. The optimiser was chosen based upon experiments. We tried different optimisers including Adam, simple SGD and found the Nesterov optimiser worked the best. This momentum-based optimizer stabilizes update directions and seems to better escape from poor local optima during the decoding iterations. \n\nQ. Is it essential to use the scheduled adjustment of \\mu?\n\nA. From the theoretical point of view, the theorems on the convergence proofs of the quadratic penalty for solving constrained optimisation problems (such as those resulted from our decoding approach) rely on increasing sequences of \\mu (eg theorems 17.1 and 17.2 in [1]). Empirically, we found that increasing \\mu by multiplying it with 1.2 every 5 iterations works well. Increasing \\mu\u2019s is also done in [2], which proposes the method of auxiliary coordinates for gradient-free training of feed forward neural networks. \n\n[1] J. Nocedal and S. J. Wright. Numerical Optimization. Springer Series in Operations Research and Financial Engineering. Springer-Verlag, New York, second edition, 2006.\n[2] Miguel Carreira-Perpinan and Weiran Wang. Distributed optimization of deeply nested systems. In Artificial Intelligence and Statistics (AISTATS), pp. 10\u201319, 2014\n\t\t\t\t\n\nQ. It states \u201cAll \\mu\u2019s\u201d but there seems only a single \\mu in Eq.(5). \n\nA.  Thanks, we meant different \\mu\u2019s for the penalty terms corresponding to different constraints. However, we have used one \\mu for all constraints in this work, so we have updated the text. \n\nQ. Regarding initialisation of the g variables.\n\nA. We observe that beam initialisation is better than greedy, while random initialisation requires more steps then greedy or beam to reach convergence. Compared to beam, random initialisation requires an average of 12x more iterations. Sometimes the solution found using random initialisation is poor, and the solution oscillates without convergence.   \n \nQ. There is a plot without a figure caption in p.8...Why are their values at \u2018epoch\u2019=0 lower than those at the subsequent epochs? \n\nA. We spotted a plotting error, which is now fixed. The new plot has been added together with the axis label and figure caption.\n\nAdditional comments:\nWe have put a paragraph on the notation in the updated paper.\nWe have replaced the word \u2018epoch\u2019 with \u2018iteration\u2019 in the updated version of the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1325/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1325/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["syed.zaidi1@monash.edu", "t.cohn@unimelb.edu.au", "reza.haffari@gmail.com"], "title": "Decoding As Dynamic Programming For Recurrent Autoregressive Models", "authors": ["Najam Zaidi", "Trevor Cohn", "Gholamreza Haffari"], "pdf": "/pdf/cbec9ce6e7b1e593eac93abfff0fd90fd2ee6ed8.pdf", "TL;DR": "Approximate inference using dynamic programming for Autoregressive models.", "abstract": "Decoding in autoregressive models (ARMs) consists of searching for a high scoring output sequence under the trained model.  Standard decoding methods, based on unidirectional greedy algorithm or beam search, are suboptimal due to error propagation and myopic decisions which do not account for future steps in the generation process. In this paper we present a novel decoding approach based on the method of auxiliary coordinates (Carreira-Perpinan & Wang, 2014) to address the aforementioned shortcomings.  Our method introduces discrete variables for output tokens,  and auxiliary continuous variables representing the states of the underlying ARM. The auxiliary variables lead to a factor graph approximation of the ARM, whose maximum a posteriori (MAP) inference is found exactly using dynamic programming. The MAP inference is then used to recreate an improved factor graph approximation of the ARM via updated auxiliary variables. We then extend our approach to decode in an ensemble of ARMs, possibly with different generation orders,  which is out of reach for the standard unidirectional decoding algorithms. Experiments on the text infilling task over SWAG and Daily Dialogue datasets show that our decoding method is superior to strong unidirectional decoding baselines.", "keywords": ["Decoding"], "paperhash": "zaidi|decoding_as_dynamic_programming_for_recurrent_autoregressive_models", "_bibtex": "@inproceedings{\nZaidi2020Decoding,\ntitle={Decoding As Dynamic Programming For Recurrent Autoregressive Models},\nauthor={Najam Zaidi and Trevor Cohn and Gholamreza Haffari},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklOo0VFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9be0a3759ff319c7a025837c36bf329cc33586bf.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklOo0VFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1325/Authors", "ICLR.cc/2020/Conference/Paper1325/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1325/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1325/Reviewers", "ICLR.cc/2020/Conference/Paper1325/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1325/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1325/Authors|ICLR.cc/2020/Conference/Paper1325/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157727, "tmdate": 1576860547693, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1325/Authors", "ICLR.cc/2020/Conference/Paper1325/Reviewers", "ICLR.cc/2020/Conference/Paper1325/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1325/-/Official_Comment"}}}, {"id": "Skg3_uWniS", "original": null, "number": 2, "cdate": 1573816435942, "ddate": null, "tcdate": 1573816435942, "tmdate": 1573816435942, "tddate": null, "forum": "HklOo0VFDH", "replyto": "SJlQ7OR3FB", "invitation": "ICLR.cc/2020/Conference/Paper1325/-/Official_Comment", "content": {"title": "Reply to reviewer #1 ", "comment": "We thank the reviewer for the insightful comments. \n\nQ. Don't say ARMs are non-Markov model and/or with unbounded Markov order\u2026. RNNs are just a nonlinear version of HMM/hidden Kalman filter.\nA.  We agree that an RNN decoder can be considered as a non-linear HMM/hidden Kalman filter. However, we note that RNNs are a subclass of the more general class of ARMs which involve other architectures as well, such as Transformer decoders. Please note that we have mentioned the unbounded Markov property for the general class of ARMs. \n\nTo elaborate further, one of the crucial differences between the Transformer and RNN decoders is the dependency of the next state to the previous states. In Transformer decoders, the next state is an immediate function of \u2018all\u2019 previous states, i.e. S_t = f(S_{t-1},..,S_1). However in vanilla RNN decoders, the next state is an immediate function of only the previous state, i.e. S_t = f(S_{t-1}). Therefore, Transformer decoders (as ARMs) do not have a bounded Markov order, as having only the previous state is not enough for computing the next state. \n\nQ. I am surprised (if the authors' claim is correct) the RNN community still relies on greedy/beam search for decoding.\nA. Greedy/Beam search are the most popular decoding algorithms in the RNN community (eg [1]). The NLP community, for example, use beam/greedy search algorithms prominently for various sequence-to-sequence tasks including machine translation (eg [2]) and summarisation (eg [3]), to name a few. Designing better decoding algorithms is an active research area, which includes this paper and the papers discussed in section 2.\n\n[1] E Cohen and J Beck, \u201cEmpirical Analysis of Beam Search Performance Degradation in Neural Sequence Models\u201d, ICML, 2019.\n[2]  Gu, J., Wang, Y., Cho, K., & Li, V.O. \u201cImproved Zero-shot Neural Machine Translation via Ignoring Spurious Correlations\u201d, ACL, 2019.\n[3] Liu, Y., & Lapata, \u201cM. Hierarchical Transformers for Multi-Document Summarization\u201d, ACL, 2019.\n\nQ. The idea, however, is quite straightforward to people who are familiar with probabilistic graphical models and inference. \nA. One of the key contributions of the paper is to propose a decoding method based on the method of auxiliary variables and link the resulting approach to probabilistic graphical models (PGMs). Once the connection is made clear, the rest of the approach becomes clearer for researchers with a background in PGMs (although there are important details to figure out for alternate optimization of the hidden states and the output variables to make them efficient and effective). \n\nNote that our approach considers (i) high-order factors capturing the dependency of an output variable with its \u2018several\u2019 previous variables (similarly, high-order state dependencies can be considered), and (ii) an extension to inference for ensemble of ARMs (in our application, left-to-right and right-to-left models).   \n\nTo the best of our knowledge, our paper is the first to formulate the inference problem in ARMs by the method of auxiliary variables, and linking it to the factor graphs with high-order dependency factors as a way of approximating the underlying model. Furthermore, our paper presents solid experimental results showing that the resulting decoding method outperforms not only the most popular decoding algorithms (i.e. greedy and beam search), but also the state-of-the-art TIGS algorithm on the text infilling task. \n\nQ. The error bars. \n\nA.  Our test set contains 5000 sentence pairs which is quite large, and accordingly the error bars will be small. However, we will add the error bars to the tables and figures in the new version of the paper.  \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1325/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1325/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["syed.zaidi1@monash.edu", "t.cohn@unimelb.edu.au", "reza.haffari@gmail.com"], "title": "Decoding As Dynamic Programming For Recurrent Autoregressive Models", "authors": ["Najam Zaidi", "Trevor Cohn", "Gholamreza Haffari"], "pdf": "/pdf/cbec9ce6e7b1e593eac93abfff0fd90fd2ee6ed8.pdf", "TL;DR": "Approximate inference using dynamic programming for Autoregressive models.", "abstract": "Decoding in autoregressive models (ARMs) consists of searching for a high scoring output sequence under the trained model.  Standard decoding methods, based on unidirectional greedy algorithm or beam search, are suboptimal due to error propagation and myopic decisions which do not account for future steps in the generation process. In this paper we present a novel decoding approach based on the method of auxiliary coordinates (Carreira-Perpinan & Wang, 2014) to address the aforementioned shortcomings.  Our method introduces discrete variables for output tokens,  and auxiliary continuous variables representing the states of the underlying ARM. The auxiliary variables lead to a factor graph approximation of the ARM, whose maximum a posteriori (MAP) inference is found exactly using dynamic programming. The MAP inference is then used to recreate an improved factor graph approximation of the ARM via updated auxiliary variables. We then extend our approach to decode in an ensemble of ARMs, possibly with different generation orders,  which is out of reach for the standard unidirectional decoding algorithms. Experiments on the text infilling task over SWAG and Daily Dialogue datasets show that our decoding method is superior to strong unidirectional decoding baselines.", "keywords": ["Decoding"], "paperhash": "zaidi|decoding_as_dynamic_programming_for_recurrent_autoregressive_models", "_bibtex": "@inproceedings{\nZaidi2020Decoding,\ntitle={Decoding As Dynamic Programming For Recurrent Autoregressive Models},\nauthor={Najam Zaidi and Trevor Cohn and Gholamreza Haffari},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklOo0VFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9be0a3759ff319c7a025837c36bf329cc33586bf.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklOo0VFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1325/Authors", "ICLR.cc/2020/Conference/Paper1325/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1325/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1325/Reviewers", "ICLR.cc/2020/Conference/Paper1325/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1325/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1325/Authors|ICLR.cc/2020/Conference/Paper1325/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157727, "tmdate": 1576860547693, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1325/Authors", "ICLR.cc/2020/Conference/Paper1325/Reviewers", "ICLR.cc/2020/Conference/Paper1325/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1325/-/Official_Comment"}}}, {"id": "ByeEgdZhjr", "original": null, "number": 1, "cdate": 1573816300319, "ddate": null, "tcdate": 1573816300319, "tmdate": 1573816357517, "tddate": null, "forum": "HklOo0VFDH", "replyto": "rJgHLapy9r", "invitation": "ICLR.cc/2020/Conference/Paper1325/-/Official_Comment", "content": {"title": "Reply to reviewer #2", "comment": "We thank the reviewer for the feedback.  We would like to make the following clarifications. \n\nQ. Comparison with other approximate inference methods for ARMs.\nA. In addition to comparison with classical inference methods, i.e. greedy and beam search, we have also compared our inference technique with TIGS, which is introduced in ACL 2019 [1]. TIGS is state-of-the-art inference method for the text infilling task. It is a strong and sophisticated inference method representative of the body of the literature which consider a continuous relaxation of the discrete optimisation involved in the decoding process. These methods then use algorithms for continuous optimisation to solve the relaxed problem, and then project back the \u2018fractional\u2019 solution to the discrete space (c.f. please see the Related Work section in the paper for more details).\n\nGiven that our method outperforms TIGS in comparable experimental conditions to [1], it implies that our method will outperform the other inference methods which are compared with TIGS as well [1], including bidirectional beam search and inference algorithms that can be applied directly to generative models including [2,3].\n \n[1]  Dayiheng Liu, Jie Fu, Pengfei Liu, and Jiancheng Lv. Tigs: An inference algorithm for text infilling with gradient search. ACL, 2019\n[2] Berglund, M., Raiko, T., Honkala, M., K\u00e4rkk\u00e4inen, L., Vetek, A., & Karhunen, J. T. (2015). Bidirectional recurrent neural networks as generative models. In Advances in Neural Information Processing Systems (pp. 856-864).\n[3] Sun, Q., Lee, S., & Batra, D. (2017). Bidirectional beam search: Forward-backward inference in neural sequence models for fill-in-the-blank image captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 6961-6969).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1325/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1325/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["syed.zaidi1@monash.edu", "t.cohn@unimelb.edu.au", "reza.haffari@gmail.com"], "title": "Decoding As Dynamic Programming For Recurrent Autoregressive Models", "authors": ["Najam Zaidi", "Trevor Cohn", "Gholamreza Haffari"], "pdf": "/pdf/cbec9ce6e7b1e593eac93abfff0fd90fd2ee6ed8.pdf", "TL;DR": "Approximate inference using dynamic programming for Autoregressive models.", "abstract": "Decoding in autoregressive models (ARMs) consists of searching for a high scoring output sequence under the trained model.  Standard decoding methods, based on unidirectional greedy algorithm or beam search, are suboptimal due to error propagation and myopic decisions which do not account for future steps in the generation process. In this paper we present a novel decoding approach based on the method of auxiliary coordinates (Carreira-Perpinan & Wang, 2014) to address the aforementioned shortcomings.  Our method introduces discrete variables for output tokens,  and auxiliary continuous variables representing the states of the underlying ARM. The auxiliary variables lead to a factor graph approximation of the ARM, whose maximum a posteriori (MAP) inference is found exactly using dynamic programming. The MAP inference is then used to recreate an improved factor graph approximation of the ARM via updated auxiliary variables. We then extend our approach to decode in an ensemble of ARMs, possibly with different generation orders,  which is out of reach for the standard unidirectional decoding algorithms. Experiments on the text infilling task over SWAG and Daily Dialogue datasets show that our decoding method is superior to strong unidirectional decoding baselines.", "keywords": ["Decoding"], "paperhash": "zaidi|decoding_as_dynamic_programming_for_recurrent_autoregressive_models", "_bibtex": "@inproceedings{\nZaidi2020Decoding,\ntitle={Decoding As Dynamic Programming For Recurrent Autoregressive Models},\nauthor={Najam Zaidi and Trevor Cohn and Gholamreza Haffari},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklOo0VFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9be0a3759ff319c7a025837c36bf329cc33586bf.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HklOo0VFDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1325/Authors", "ICLR.cc/2020/Conference/Paper1325/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1325/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1325/Reviewers", "ICLR.cc/2020/Conference/Paper1325/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1325/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1325/Authors|ICLR.cc/2020/Conference/Paper1325/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157727, "tmdate": 1576860547693, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1325/Authors", "ICLR.cc/2020/Conference/Paper1325/Reviewers", "ICLR.cc/2020/Conference/Paper1325/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1325/-/Official_Comment"}}}, {"id": "BygmgjVnYr", "original": null, "number": 1, "cdate": 1571732203493, "ddate": null, "tcdate": 1571732203493, "tmdate": 1572972483469, "tddate": null, "forum": "HklOo0VFDH", "replyto": "HklOo0VFDH", "invitation": "ICLR.cc/2020/Conference/Paper1325/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "I think this is a good study, unless I miss something. It proposes a new solution to one of the fundamental problems in the field, which significantly improves previous solutions in terms of accuracy. I will recommend it for acceptance unless I miss something. (I\u2019m not an expert in the field and the problem seems to be so fundamental, making me cautious about judging the novelty of the proposed solution.)\n\nThere seems to be some unclarity about the optimization algorithm. In short, I suspect that the proposed optimization has some difficulty with its convergence. \n-\tIn Sec.4, the authors suggest \u201ca two-step block coordinate descent algorithm to alternate between (i) optimizing y\u2019s while g\u2019s are fixed, and (ii) optimizing g\u2019s while y\u2019s are fixed\u201d. They further suggest `a variant of the Viterbi algorithm\u2019 for (i) and gradient-based algorithms for (ii). These seem to make sense to me so far. \n-\tThen, in Sec.6, they state that \u201cWe use the Nesterov optimizer with a learning rate of 0.1. All \\mu\u2019s are initialised with 0.5 and are multiplied by 1.2 after 5 epochs\u201d. I think this needs some explanation. \n-\tFor instance, what is the thought behind the choice of the optimizer? Why was Nesterov\u2019s acceleration necessary instead of plain GD? Is it essential to use the scheduled adjustment of \\mu?\n-\tIt states \u201cAll \\mu\u2019s\u201d but there seems only a single \\mu in Eq.(5). Am I missing something?\n-\t Some explanation on what \u201cNesterov optimizer\u201d and \\mu would also be necessary for a wide range of readers. \n-\tThere is a statement on the initialization of the optimization in p.7: \u201ch corresponding to the beam search solution to initialize the g variables\u201d. How sensitive is the optimization to the initial values? For instance, will the results change if g\u2019s are initialized by the solution of the greedy method. \n-\tThere is a plot without a figure caption in p.8, which shows how the objective cost decreases with parameter updates. Why are their values at \u2018epoch\u2019=0 lower than those at the subsequent epochs? Does this mean the initial values give more optimal parameters?\n\nAdditional comments:\n-\tIt would be more friendly to the readers to show the definition of the notations like y^n_1. \n-\tThere is a statement like \u201cDecoding was run for 10 epochs.\u201d I suppose epochs here mean iteration count of the alternated parameter updates in the proposed algorithm. Why do the authors call it `epoch\u2019? \n-\tNo figure caption for the plot in p.8. It also uses `epochs\u2019 for the horizontal axis title. No axis title for the vertical axis. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1325/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1325/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["syed.zaidi1@monash.edu", "t.cohn@unimelb.edu.au", "reza.haffari@gmail.com"], "title": "Decoding As Dynamic Programming For Recurrent Autoregressive Models", "authors": ["Najam Zaidi", "Trevor Cohn", "Gholamreza Haffari"], "pdf": "/pdf/cbec9ce6e7b1e593eac93abfff0fd90fd2ee6ed8.pdf", "TL;DR": "Approximate inference using dynamic programming for Autoregressive models.", "abstract": "Decoding in autoregressive models (ARMs) consists of searching for a high scoring output sequence under the trained model.  Standard decoding methods, based on unidirectional greedy algorithm or beam search, are suboptimal due to error propagation and myopic decisions which do not account for future steps in the generation process. In this paper we present a novel decoding approach based on the method of auxiliary coordinates (Carreira-Perpinan & Wang, 2014) to address the aforementioned shortcomings.  Our method introduces discrete variables for output tokens,  and auxiliary continuous variables representing the states of the underlying ARM. The auxiliary variables lead to a factor graph approximation of the ARM, whose maximum a posteriori (MAP) inference is found exactly using dynamic programming. The MAP inference is then used to recreate an improved factor graph approximation of the ARM via updated auxiliary variables. We then extend our approach to decode in an ensemble of ARMs, possibly with different generation orders,  which is out of reach for the standard unidirectional decoding algorithms. Experiments on the text infilling task over SWAG and Daily Dialogue datasets show that our decoding method is superior to strong unidirectional decoding baselines.", "keywords": ["Decoding"], "paperhash": "zaidi|decoding_as_dynamic_programming_for_recurrent_autoregressive_models", "_bibtex": "@inproceedings{\nZaidi2020Decoding,\ntitle={Decoding As Dynamic Programming For Recurrent Autoregressive Models},\nauthor={Najam Zaidi and Trevor Cohn and Gholamreza Haffari},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklOo0VFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9be0a3759ff319c7a025837c36bf329cc33586bf.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklOo0VFDH", "replyto": "HklOo0VFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1325/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1325/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575529573350, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1325/Reviewers"], "noninvitees": [], "tcdate": 1570237739021, "tmdate": 1575529573365, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1325/-/Official_Review"}}}, {"id": "SJlQ7OR3FB", "original": null, "number": 2, "cdate": 1571772443063, "ddate": null, "tcdate": 1571772443063, "tmdate": 1572972483426, "tddate": null, "forum": "HklOo0VFDH", "replyto": "HklOo0VFDH", "invitation": "ICLR.cc/2020/Conference/Paper1325/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper proposes a decoding algorithm for auto-regressive models (ARMs) to improve the decoding accuracy. The key idea is to introduce auxiliary continuous states variables, and alternatively optimize the discrete output variables (i.e., the tokens) and the continuous states. Given the hidden states, the decoding can be efficiently done by veterbi-like algorithms. Then the constraint of the auxiliary variables can be imposed by penalty based continuous based optimization. The paper also extends the idea to the ensemble of ARMs.  On two NLP tasks, the paper shows improvement upon the existing greedy or beach search based approaches. \n\nOverall, the proposed method can be very useful in RNN decoding and structure prediction. The idea, however, is quite straightforward to people who are familiar with probabilistic graphic models and inference. I am surprised (if the authors' claim is correct) the RNN community still relies on greedy/beam search for decoding. In graphical model language, the hidden states (h_i or g_i in the paper) are typically viewed as continuous latent random variables. The function f that links consecutive hidden states are factors or potential functions. For inference/prediction, you can jointly optimize the hidden states and discrete outputs, where the alternative updates are a natural choice. Similar ideas were used long time ago when (hierarhical) conditional random fields were popular. Honestly, I don't see anything new here.  Here are a few comments:\n\n1. Don't say ARMs are non-Markov model and/or with unbounded Markov order. This is very misleading and over bragging. RNNs are just a nonlinear version of HMM/hidden Kalman filter. The only difference is that the states are continuous (Kalman filter uses continuous states as well), and the state transition kernel is nonlinear, constructed in a very black-box way. People like to make some analogy with brains --- unfortunately, these explanations are at most an analogy.  Given hidden states, RNNs are just first order Markov-chains, nothing special. If you integrate out hidden states, of course, every output is dependent on all the previous outputs. But the same argument applies to all the hidden markov models/dynamic systems. This is not something unique to RNNs, and shouldn't be hyped everywhere.\n\n2. Is there a way to show the standard deviations/error bars in the test results, say, Table 1 & 2 and the figure? One single number is usually not representative for the performance, unless you have a large test set. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1325/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1325/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["syed.zaidi1@monash.edu", "t.cohn@unimelb.edu.au", "reza.haffari@gmail.com"], "title": "Decoding As Dynamic Programming For Recurrent Autoregressive Models", "authors": ["Najam Zaidi", "Trevor Cohn", "Gholamreza Haffari"], "pdf": "/pdf/cbec9ce6e7b1e593eac93abfff0fd90fd2ee6ed8.pdf", "TL;DR": "Approximate inference using dynamic programming for Autoregressive models.", "abstract": "Decoding in autoregressive models (ARMs) consists of searching for a high scoring output sequence under the trained model.  Standard decoding methods, based on unidirectional greedy algorithm or beam search, are suboptimal due to error propagation and myopic decisions which do not account for future steps in the generation process. In this paper we present a novel decoding approach based on the method of auxiliary coordinates (Carreira-Perpinan & Wang, 2014) to address the aforementioned shortcomings.  Our method introduces discrete variables for output tokens,  and auxiliary continuous variables representing the states of the underlying ARM. The auxiliary variables lead to a factor graph approximation of the ARM, whose maximum a posteriori (MAP) inference is found exactly using dynamic programming. The MAP inference is then used to recreate an improved factor graph approximation of the ARM via updated auxiliary variables. We then extend our approach to decode in an ensemble of ARMs, possibly with different generation orders,  which is out of reach for the standard unidirectional decoding algorithms. Experiments on the text infilling task over SWAG and Daily Dialogue datasets show that our decoding method is superior to strong unidirectional decoding baselines.", "keywords": ["Decoding"], "paperhash": "zaidi|decoding_as_dynamic_programming_for_recurrent_autoregressive_models", "_bibtex": "@inproceedings{\nZaidi2020Decoding,\ntitle={Decoding As Dynamic Programming For Recurrent Autoregressive Models},\nauthor={Najam Zaidi and Trevor Cohn and Gholamreza Haffari},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklOo0VFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9be0a3759ff319c7a025837c36bf329cc33586bf.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklOo0VFDH", "replyto": "HklOo0VFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1325/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1325/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575529573350, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1325/Reviewers"], "noninvitees": [], "tcdate": 1570237739021, "tmdate": 1575529573365, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1325/-/Official_Review"}}}, {"id": "rJgHLapy9r", "original": null, "number": 3, "cdate": 1571966285483, "ddate": null, "tcdate": 1571966285483, "tmdate": 1572972483382, "tddate": null, "forum": "HklOo0VFDH", "replyto": "HklOo0VFDH", "invitation": "ICLR.cc/2020/Conference/Paper1325/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This work introduces a new algorithm to improve decoding in discrete autoregressive models (ARMs). Because exact decoding of ARMs is computationally difficult, the contribution of this paper is to introduce a new approximate solution. The authors show experimentally that this new framework is very effective and outperforms multiple baselines such as greedy and bean search and another text filing algorithm called TIGC on a text-filling benchmark. This work is not in my domain of expertise so I am not able to have a very accurate evaluation. However, based on the references cited in this paper, there are other approximate solution for ARMs and I believe the authors need to use those as baselines to show that the proposed approximate solution is useful."}, "signatures": ["ICLR.cc/2020/Conference/Paper1325/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1325/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["syed.zaidi1@monash.edu", "t.cohn@unimelb.edu.au", "reza.haffari@gmail.com"], "title": "Decoding As Dynamic Programming For Recurrent Autoregressive Models", "authors": ["Najam Zaidi", "Trevor Cohn", "Gholamreza Haffari"], "pdf": "/pdf/cbec9ce6e7b1e593eac93abfff0fd90fd2ee6ed8.pdf", "TL;DR": "Approximate inference using dynamic programming for Autoregressive models.", "abstract": "Decoding in autoregressive models (ARMs) consists of searching for a high scoring output sequence under the trained model.  Standard decoding methods, based on unidirectional greedy algorithm or beam search, are suboptimal due to error propagation and myopic decisions which do not account for future steps in the generation process. In this paper we present a novel decoding approach based on the method of auxiliary coordinates (Carreira-Perpinan & Wang, 2014) to address the aforementioned shortcomings.  Our method introduces discrete variables for output tokens,  and auxiliary continuous variables representing the states of the underlying ARM. The auxiliary variables lead to a factor graph approximation of the ARM, whose maximum a posteriori (MAP) inference is found exactly using dynamic programming. The MAP inference is then used to recreate an improved factor graph approximation of the ARM via updated auxiliary variables. We then extend our approach to decode in an ensemble of ARMs, possibly with different generation orders,  which is out of reach for the standard unidirectional decoding algorithms. Experiments on the text infilling task over SWAG and Daily Dialogue datasets show that our decoding method is superior to strong unidirectional decoding baselines.", "keywords": ["Decoding"], "paperhash": "zaidi|decoding_as_dynamic_programming_for_recurrent_autoregressive_models", "_bibtex": "@inproceedings{\nZaidi2020Decoding,\ntitle={Decoding As Dynamic Programming For Recurrent Autoregressive Models},\nauthor={Najam Zaidi and Trevor Cohn and Gholamreza Haffari},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HklOo0VFDH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/9be0a3759ff319c7a025837c36bf329cc33586bf.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HklOo0VFDH", "replyto": "HklOo0VFDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1325/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1325/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575529573350, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1325/Reviewers"], "noninvitees": [], "tcdate": 1570237739021, "tmdate": 1575529573365, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1325/-/Official_Review"}}}], "count": 9}