{"notes": [{"id": "SyezvsC5tX", "original": "SygACXw9tQ", "number": 241, "cdate": 1538087769535, "ddate": null, "tcdate": 1538087769535, "tmdate": 1545355418662, "tddate": null, "forum": "SyezvsC5tX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 20, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1gClIytkV", "original": null, "number": 1, "cdate": 1544250869665, "ddate": null, "tcdate": 1544250869665, "tmdate": 1545354496447, "tddate": null, "forum": "SyezvsC5tX", "replyto": "SyezvsC5tX", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Meta_Review", "content": {"metareview": "The paper proves that the locus of the global minima of an over-parameterized neural nets objective forms a low-dimensional manifold. The reviewers and AC note the following potential weaknesses:  \n--- it's not clear why the proved result is significant: it neither implies the SGD can find a global minimum, nor that the found solution can generalize. (Very likely, most of the global minima on the manifold cannot generalize.) \n--- the results seem very intuitive and are a straightforward application of certain topological theorem.\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper241/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper241/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353284508, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyezvsC5tX", "replyto": "SyezvsC5tX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper241/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353284508}}}, {"id": "SygNerAUJ4", "original": null, "number": 17, "cdate": 1544115436068, "ddate": null, "tcdate": 1544115436068, "tmdate": 1544115436068, "tddate": null, "forum": "SyezvsC5tX", "replyto": "SJlgKxXq0Q", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "content": {"title": "Response", "comment": "Great.  Glad we reached a common understanding about the special properties of global minima that were used in the argument.  Thanks for helping us clarify that.\n\nWe wanted to make a brief clarification about your first concern.  It is true that given a set of data points {(x_i,y_i)}, x_i in R^a, y_i in R^b, $M$ may not be a smooth n-d dimensional manifold.  However, the set of data points in R^{a X b X d} for which this fails has measure 0.  So the probability that your data points have this property is 0.  \n\nFurthermore, if your data set is one for which $M$ is not a smooth n-d dimensional manifold, you can choose any arbitrarily small \\epsilon > 0 and there exists a perturbation of the data set so the distance between each new (x_i', y_i') and (x_i, y_i) is smaller than \\epsilon.  So in real world applications, this should not be an issue."}, "signatures": ["ICLR.cc/2019/Conference/Paper241/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622600, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyezvsC5tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper241/Authors|ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622600}}}, {"id": "rklQPG0IkE", "original": null, "number": 16, "cdate": 1544114778614, "ddate": null, "tcdate": 1544114778614, "tmdate": 1544114778614, "tddate": null, "forum": "SyezvsC5tX", "replyto": "HJgdPoB8yE", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "content": {"title": "Response", "comment": "That is an excellent question, thanks for bringing it up.  No, they do not.  In fact, under the assumptions used in this paper, you can find examples where $M$ is not connected.  \n\nThat being said, we would like to know under what assumptions $M$ is connected.  We don't currently have a theorem about that."}, "signatures": ["ICLR.cc/2019/Conference/Paper241/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622600, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyezvsC5tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper241/Authors|ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622600}}}, {"id": "HJgdPoB8yE", "original": null, "number": 15, "cdate": 1544080224030, "ddate": null, "tcdate": 1544080224030, "tmdate": 1544080224030, "tddate": null, "forum": "SyezvsC5tX", "replyto": "SyezvsC5tX", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "content": {"title": "quick question", "comment": "I am wondering whether the results imply that all the global minima form a *connected* n-d dimensional manifold? "}, "signatures": ["ICLR.cc/2019/Conference/Paper241/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper241/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622600, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyezvsC5tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper241/Authors|ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622600}}}, {"id": "HkgWAw29hm", "original": null, "number": 3, "cdate": 1541224393217, "ddate": null, "tcdate": 1541224393217, "tmdate": 1543282849789, "tddate": null, "forum": "SyezvsC5tX", "replyto": "SyezvsC5tX", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Review", "content": {"title": "Interesting but contribution is minor and doesn't meet the standards. Would recommend rejection.", "review": "This paper considers over-parametrized neural networks (n > d), where n is the number of parameters and d is the number of data, in the regression setting. It consists of three main results:\n\na) (Theorem 2.1) If the output of the network is a smooth function of its parameters and global minimum with zero squared loss is attainable, then the locus of global minima is a smooth $n - d$ dimensional submanifold of R^n.\nb) (Lemma 3.3) A neural network with a hidden layer as wide as the number of data points can attain global minima with zero loss.\nc) (Theorem 4.1) If a neural network has a hidden layer as wide as the number of data points and is a smooth function of its parameters, then the locus of global minima with loss zero is a smooth $n - d$ dimensional submanifold. This is just a combination of a) and b).\n\nI think the only contribution of this paper is Theorem 2.1. It is already well-known (e.g., Zhang et al. 16\u2019) that a network with a hidden layer as wide as the number of data can easily attain zero empirical error. The authors claim that it is an extension over ReLU, but the extension is almost trivial. Theorem 4.1 is just a corollary from Theorem 2.1 and Lemma 3.3.\n\nTheorem 2.1 is an interesting observation, and its implication that the Hessian at minima has only d positive eigenvalues is interesting and explains empirical observations. \n\nFrom my understanding, however, the n - d dimensional submanifold argument can be applied to any regular values of H(w,b) (not necessarily global minima), so the theorem is basically saying there are a lot of equivalence classes in the parameter space that outputs exactly the same outcome. If my understanding is correct, this result does not give us any insight into why SGD can find global minima easily, because we can apply the same proof to spurious local minima and say, \u201clook, the locus of this spurious local minimum is a high-dimensional submanifold, so it is easy to get stuck at poor local minima.\u201d\n\nMoreover, the notation used in this paper is not commonly used in deep learning community, thus might confuse the readers. $n$ is more commonly used for the number of data points and $d$ is used for some dimension. Also, there are overloading uses of $p$: \n* In Section 2.1 it is used for the dimension of the input space\n* In the proof of Prop 2.3 it is now a vector in R^n\n* In section 3 it is now a parameter vector of a neural network\n* Back to the input dimension in Lemma 3.3 and Section 4\n\nAlso, the formulation of neural networks in Section 3 is very non-standard. If you are to end up showing some result on standard fully-connected feed-forward networks, why not just use, for example, $W_2 \\sigma(W_1 x_i + b_1) + b_2$? To me, this graph theoretical formulation only adds confusion. Also at the end of page 4, there is a typo: the activation function (\\sigma) must be applied after the summation.\n\nOverall, I believe this paper does not meet the standards of ICLR at the moment. I would recommend rejection.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper241/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Review", "cdate": 1542234506860, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyezvsC5tX", "replyto": "SyezvsC5tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335680112, "tmdate": 1552335680112, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SJlgKxXq0Q", "original": null, "number": 13, "cdate": 1543282808188, "ddate": null, "tcdate": 1543282808188, "tmdate": 1543282808188, "tddate": null, "forum": "SyezvsC5tX", "replyto": "rJetN0YBR7", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "content": {"title": "Reply", "comment": "I appreciate the authors for the clarification. I get your point; the argument can be applied to any regular value (r_1, ..., r_d) of H, but the points in M are not necessarily critical points of L. In contrast, for (0, ..., 0), the points in M are all global minima of L. I admit that I made a false statement about \"local minima\" in the review.\n\nHowever, even with this clarification, I think the contribution of this paper is rather limited, in the sense that:\n1) The result doesn't always work; it requires additional noise injection when (0, ..., 0) is not a regular value of H.\n2) As pointed out by Reviewer2, the theorem doesn't provide comparison between global minima and other critical points of L.\n3) After Section 2, the remaining theorems do not make significant contribution, in my opinion.\n\nI'll adjust my rating accordingly."}, "signatures": ["ICLR.cc/2019/Conference/Paper241/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper241/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622600, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyezvsC5tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper241/Authors|ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622600}}}, {"id": "Byl5UnmICQ", "original": null, "number": 12, "cdate": 1543023697672, "ddate": null, "tcdate": 1543023697672, "tmdate": 1543023697672, "tddate": null, "forum": "SyezvsC5tX", "replyto": "rygVs1sSCm", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "content": {"title": "Reply", "comment": "Yes, you are exactly correct.  The example of an n-dimensional space of polynomials is a great example of a setting in which Theorem 2.1 holds.  If one uses L2 loss, then it is correct that the locus M = L^{-1}(0) is a nonempty manifold of dimension $n-d$ as long as there exists a polynomial in the chosen space that can perfectly fit the data.\n\nAs you say, the first half of this paper applies in a much more general setting than only neural networks.  In Theorem 2.1, we state precisely the assumptions under which the locus of global minima of the loss function L is a smooth manifold of dimension $n-d$.  All those assumptions hold in the case of an n-dimensional space of polynomials with L2 loss.  \n\nOne take home message of this paper is that several recent empirical observations can be explained by some properties of the geometry of the loss function which are very general, and hold in a more general setting than only neural networks."}, "signatures": ["ICLR.cc/2019/Conference/Paper241/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622600, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyezvsC5tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper241/Authors|ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622600}}}, {"id": "rygVs1sSCm", "original": null, "number": 11, "cdate": 1542987676393, "ddate": null, "tcdate": 1542987676393, "tmdate": 1542987676393, "tddate": null, "forum": "SyezvsC5tX", "replyto": "BkgQtf5S0X", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "content": {"title": "Clarification", "comment": "Let me rephrase the question with a concrete example in the following way: now instead of using neural networks,  replace the function f_{w,b} by a polynomial, where w, b are the coefficients or the polynomial. The number of coefficients is n. (overparametrization somehow equivalent to using a high order polynomial)\n\nAs far as I understand, the following statement is still valid: \nAssume the polynomial f can perfectly fit the data, then the dimension of $M$ is $n-d$.\n\nWhat I am trying to say is such theorem does not involve any property of neural network, it only depends on \n1) smoothness, which allows using differential topology\n2) being universal approximator \nIf assuming the function has the capacity to perfectly fit the training data, then 2) is no longer needed so the result only relies on the property of smoothness. "}, "signatures": ["ICLR.cc/2019/Conference/Paper241/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper241/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622600, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyezvsC5tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper241/Authors|ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622600}}}, {"id": "BkgQtf5S0X", "original": null, "number": 10, "cdate": 1542984314855, "ddate": null, "tcdate": 1542984314855, "tmdate": 1542984314855, "tddate": null, "forum": "SyezvsC5tX", "replyto": "SyxQoGBER7", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "content": {"title": "Reply", "comment": "We thank the reviewer for their question.\n\nCan you clarify what you mean by an n-dimensional smooth function f?  We are not able to answer your question as stated, but would like to address your question.  \n\nWe agree that understanding why solving regression problems using neural nets works better than, for example, using polynomials or Fourier series would be very interesting.  We don't aim to address that question here.  For now our aim is to uncover some aspects of the geometry of neural networks, and in doing so, explain some empirical observations about the loss landscape of neural networks that have been discussed in several recent papers."}, "signatures": ["ICLR.cc/2019/Conference/Paper241/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622600, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyezvsC5tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper241/Authors|ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622600}}}, {"id": "rJetN0YBR7", "original": null, "number": 9, "cdate": 1542983216974, "ddate": null, "tcdate": 1542983216974, "tmdate": 1542983216974, "tddate": null, "forum": "SyezvsC5tX", "replyto": "HklQuKk-0m", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "content": {"title": "Additional comment", "comment": "We include a more extensive explanation, in case it helps to clarify things.  In this paper, our aim is to understand the locus of global minima of L, a subset of the critical locus of the function L.  A priori our goal has nothing to do with the regular values of any function.  In the course of our proof, we construct a function H (or if necessary, \\tilde{H}) with the nice property that (0,...,0) is a regular value of H and that H^{-1}(0,...,0) is equal to L^{-1}(0).  So we replace the more difficult problem of understanding a critical locus of $L$ with the more tractable problem of understanding the preimage of a regular value of H, aka a level set of H.\n\nIn general studying the level sets of L is a very different problem.  For example, for most (a subset of R of full measure) values a, a is a regular value of L and L^{-1}(a) is a smooth n-1 dimensional manifold.  And when a is not a regular value of L, it is possible that some points of L^{-1}(a) are critical points of L and that at other points of L^{-1}(a), the map L is smooth.  \n\nSo the locus M=L^{-1}(0) we study in this paper has many special properties, one of which is that every point in M is a critical point of L.  We do not know of any reason to expect that for any other value a, L^{-1}(a) also has that property."}, "signatures": ["ICLR.cc/2019/Conference/Paper241/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622600, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyezvsC5tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper241/Authors|ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622600}}}, {"id": "SyxQoGBER7", "original": null, "number": 8, "cdate": 1542898331032, "ddate": null, "tcdate": 1542898331032, "tmdate": 1542898500882, "tddate": null, "forum": "SyezvsC5tX", "replyto": "rkgAlW71AQ", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "content": {"title": "Does the result on landscape hold for any smooth function? ", "comment": "I thank the authors for the clarification.\n\nI totally agree with the authors that the result of the paper holds as long as the network can perfectly fit the data. However, this is somehow a tautology in my opinion. Since given any n-dimensional smooth function f, assuming f can perfectly fit the data, we are able to obtain the same landscape result. (please correct me know if I am wrong)\n\nThen the core question becomes how to achieve this perfect fitting property in an efficient way. Naively, we can use any family of functions which satisfies the universal approximation property (like polynomials or Fourier series). With the polynomial interpolation, a high-dimensional polynomial can also fit perfectly the data points, so it enjoys the same (n-d) dimensional manifold of global minima. However, we might not be able to benefit from this fact to efficiently find the coefficients of the polynomials. \n\nIn my opinion, the success of deep learning implicitly suggests there is something spectacular in the structure of neural network, which is not true for any universal approximation family. I think a more interesting/important question will be how to take account of the structure/architecture of the network into the landscape.  "}, "signatures": ["ICLR.cc/2019/Conference/Paper241/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper241/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622600, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyezvsC5tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper241/Authors|ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622600}}}, {"id": "HklQuKk-0m", "original": null, "number": 7, "cdate": 1542678891042, "ddate": null, "tcdate": 1542678891042, "tmdate": 1542678891042, "tddate": null, "forum": "SyezvsC5tX", "replyto": "rkgw4upgAX", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for the reply.  We are glad to have the opportunity to clear up this point.  There are two functions - the loss function L we are interested in, and an auxiliary function H we construct to help us understand the geometry of L.  L is a function from R^n to R, while H is a function from R^n to R^d.  \n\nYou are entirely correct that if one takes any regular value (r_1, ..., r_d) of H, the preimage of that point is a smooth codimension $n-d$ dimensional submanifold.  However, in general, H^{-1}(r_1, ... r_d) is not L^{-1} of anything.  The special property of 0 that we are using is that L^{-1}(0) exactly equals H^{-1}(0,...0).  "}, "signatures": ["ICLR.cc/2019/Conference/Paper241/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622600, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyezvsC5tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper241/Authors|ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622600}}}, {"id": "rkgw4upgAX", "original": null, "number": 6, "cdate": 1542670382958, "ddate": null, "tcdate": 1542670382958, "tmdate": 1542670382958, "tddate": null, "forum": "SyezvsC5tX", "replyto": "rJehBLmkCm", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "content": {"title": "Reply to the response", "comment": "I appreciate the authors for the response. \n\nI still do not understand why \u201cglobal minimum with zero\u201d is so important in the proof of Theorem 2.1. It just says that (0,0,...,0) is a regular value of H (or \\tilde H), so M is a smooth n-d dim submanifold (if not empty). What if I pick another regular value (let's say, (1,0,...,0))? What prevents me from applying the same argument to these regular values?"}, "signatures": ["ICLR.cc/2019/Conference/Paper241/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper241/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622600, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyezvsC5tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper241/Authors|ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622600}}}, {"id": "rkxd587yRm", "original": null, "number": 5, "cdate": 1542563472362, "ddate": null, "tcdate": 1542563472362, "tmdate": 1542563472362, "tddate": null, "forum": "SyezvsC5tX", "replyto": "r1lAj8TRhm", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "content": {"title": "Author response", "comment": "We thank the commenter for this comment.\n\nWe'd like to clarify that after the statement of Theorem 2.1, we give a heuristic for why one might expect this theorem to be true.  We then give a formal proof of the theorem.  We do not claim that 0 is a regular value of $L$.  Rather, the proof establishes that after perhaps an arbitrarily small perturbation of the data set, 0 is a regular value of each $f_i$.  \n\nThank you for bringing our attention to the paper mentioned.  We did not see it stated in that paper that \"global minimum zero is not a regular value of the loss function with over-parameterized neural networks\", but in any case, this paper does not contradict that statement.  Namely, in this paper, we do not prove that 0 is a regular value of the loss function $L$.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper241/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622600, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyezvsC5tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper241/Authors|ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622600}}}, {"id": "rJehBLmkCm", "original": null, "number": 4, "cdate": 1542563395541, "ddate": null, "tcdate": 1542563395541, "tmdate": 1542563395541, "tddate": null, "forum": "SyezvsC5tX", "replyto": "HkgWAw29hm", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "content": {"title": "Author response", "comment": "We thank the reviewer for the helpful feedback.  \n\nThanks for suggesting some modifications of the letters used and catching a typo.  As to the definition of neural networks, we state a fairly general definition because that is the setting of Theorem 2.1.  We only restrict to the special case of a fully connected feedforward network in Theorem 2.3.\n\nNow we turn to the reviewer's central concern.  The proof that $M = L^{-1}(0)$ is a manifold of dimension $n-d$ depends in an important way on the fact that we are studying the pre-image of 0, and this argument would not apply to any other minima - not even global minima of $L$ if the value of $L$ at the global minimum is not 0.  It is essential to the proof that 0 has the property that the sum of $d$ nonnegative numbers is 0 if and only if each summand is 0.  \n\nWe would not characterize Theorem 2.1 as \"saying there are a lot of equivalence classes in the parameter space that outputs exactly the same outcome\".  It is true that there may be multiple parameters that name the same function, e.g. there are many symmetries if ReLU is used as the activation function.  However, the parameters in $M$ generally will not all be names for the same function - rather, they encode many different functions, all of which share the property that they pass through the given $d$ data points.  \n\nAs a simplest example, consider a parameter space of lines in $R^2$, y=ax+b.  If we ask for all the lines that pass through the point $(1,3)$, we can find many distinct lines (the graphs of distinct functions) that pass through the point.  They are different lines - what they have in common is that they all pass through the point $(1,3)$.  "}, "signatures": ["ICLR.cc/2019/Conference/Paper241/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622600, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyezvsC5tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper241/Authors|ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622600}}}, {"id": "rJe40SXkRm", "original": null, "number": 3, "cdate": 1542563276032, "ddate": null, "tcdate": 1542563276032, "tmdate": 1542563276032, "tddate": null, "forum": "SyezvsC5tX", "replyto": "BkeYca75nX", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "content": {"title": "Author response", "comment": "We thank the reviewer for the helpful feedback.  \n\nWe'd be happy to provide more details in the proof of Theorem 2.1.  Would the following clarification help?  The statement of Sard's theorem on wikipedia is that under certain conditions, the critical values of a function $f$ are measure 0.  An equivalent and alternative formulation which we use in this paper is that under those conditions, the regular values of $f$ are full measure.  \n\nThe reviewer suggests several further directions, which we think are valuable.  We have also wondered about many of these, but don't at the time have answers to them.  We hope this paper will serve as a first step for exploring the more difficult questions the reviewer brings up.  "}, "signatures": ["ICLR.cc/2019/Conference/Paper241/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622600, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyezvsC5tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper241/Authors|ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622600}}}, {"id": "rkgAlW71AQ", "original": null, "number": 2, "cdate": 1542562037664, "ddate": null, "tcdate": 1542562037664, "tmdate": 1542562037664, "tddate": null, "forum": "SyezvsC5tX", "replyto": "SkgSSMfBnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "content": {"title": "Author response", "comment": "We thank the reviewer for the helpful feedback.  This review brings up three main points, which we address here.\n\n1.  We prove two theorems in this paper.  The first theorem holds very generally, and requires almost no assumptions about the architecture of the neural net.  Informally, one can think of this theorem as saying that one should generically expect the locus $M=L^{-1}(0)$ to be a manifold of dimension $n-d$.  The second theorem invokes several assumptions.  The purpose of these assumptions is simply to prove that $M$ is nonempty.  We expect that $M$ is nonempty in many cases beyond what we prove here, and leave it to future work.  \n\nBut one way to frame it is that as soon as your neural network, whatever it is, has the capacity to perfectly fit the training data, then the dimension of $M$ is $n-d$.  So if no layer has width $d$ but you know that your network has the capacity to perfectly fit the training data, the conclusion of the second theorem still holds. \n\n2.  It is true that a small perturbation of the data may be necessary, but this perturbation can be chosen as small as you like.  To be precise, if you fix any $\\epsilon > 0$, a perturbation such that each new label is within $\\epsilon$ of the original label can be found for which the theorem holds.  \n\n3.  This is a good point.  We are not currently able to compute the dimension of the locus of local but non-global minima.  However, while it is logically possible for loci of local non-global minima to have large dimensions, even larger than the dimension of the locus of global minima $M$, we would not expect this to be the case.  \n\nA generic function has isolated critical points.  For a positive dimensional critical manifold to appear requires something special to happen.  In this paper we show that something special indeed happens at the locus of global minima $M = L^{-1}$, and we prove that its dimension is positive.  However, our argument relies on the fact that we are looking at the preimage of 0.  It would not apply to any other minima.  We agree that it would be interesting to study the dimension of the loci of local non-global minima.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper241/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622600, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyezvsC5tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper241/Authors|ICLR.cc/2019/Conference/Paper241/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622600}}}, {"id": "BkeYca75nX", "original": null, "number": 2, "cdate": 1541189008519, "ddate": null, "tcdate": 1541189008519, "tmdate": 1541534164712, "tddate": null, "forum": "SyezvsC5tX", "replyto": "SyezvsC5tX", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Review", "content": {"title": "The results from the paper are sort of known in previous literature, yet the proof in the paper was still smart and innovative. ", "review": "This paper gave an interesting theoretical result that the global minima of an overparameterized neural network is a high-dimensional sub-manifold. This result is particularly meaningful as it connected several previous observations about neural networks and a indirect evidence for why overparameterization for deep learning has been so helpful empirically.\n\nThe proof in the paper was smart and the rough logic was quite easy to follow. The minor issue is that the proof in the paper was too sketchy to be strict. For example, in proof for Thm 2.1, the original statement about Sard\u2019s theorem was about the critical values, but the usage of this theorem in the proof was a little indirect. I can roughly see how the logic can go through, but I still hope the author can give more detailed explaining about this part to make the proof more readable and strict.\n\nOverall, I think the result in this paper should be enough to justify a publication. However, there\u2019re still limitations of the result here. For example, the result only explained about the fitting on training data but cannot explain at all why overfitting is not a concern here. It also didn\u2019t explain why stochastic gradient descent can find these minima empirically. In particular, even though the minima manifold is n-d dimensional, it\u2019s still a zero-measure set which will almost never get hit with a random initialization. Of course, these are harder questions to explore, but maybe worthy some discussion in the final revision.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper241/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Review", "cdate": 1542234506860, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyezvsC5tX", "replyto": "SyezvsC5tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335680112, "tmdate": 1552335680112, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkgSSMfBnQ", "original": null, "number": 1, "cdate": 1540854332590, "ddate": null, "tcdate": 1540854332590, "tmdate": 1541534164501, "tddate": null, "forum": "SyezvsC5tX", "replyto": "SyezvsC5tX", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Official_Review", "content": {"title": "A short and concise theoretical paper, but I find the contribution limited", "review": "The paper shows that the set of global minimums of an overparametrized network with smooth activation function is almost surely a high dimensional manifold. In particular, the dimension of this manifold is exactly n-d, where n is the number of parameters and d is the number of datas. To the best of my knowledge, this is the first proper proof of such non-surprising result.  \n\nThe theoretical analysis is a straightforward combination of neural network's expressiveness result and some classical theorems in the field of differential topology. However, the assumption on the overparametrized neural network is somehow unrealistic since it requires that at least one layer has no less than d neurons, which is as many as the number of datas. This is usually not the case. Moreover, the result is to some extent \"asymptotic\", in the sense that a small perturbation in terms of data may be required in order to make the statement holds.  \n\nMore importantly, the result does not provide any useful characterization distinguishing stationary point/local minimum versus global minimum. It might be possible that the set of stationary points is also a manifold with very high dimension, which is indeed supported by the argument listed in the bottom of page 7: \"the Hessian of the loss function tended to have many zero eigenvalues, some positive eigenvalues, and a small number of negative eigenvalues\". It is possibly the case that the set of stationary points has even higher dimension. This suggests that the dimensionality itself is not right indicator, but the difference in terms of dimension between different type of stationary points that matters. \n\nOverall, the paper is short and concise but I find the contribution a bit limited.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper241/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Official_Review", "cdate": 1542234506860, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyezvsC5tX", "replyto": "SyezvsC5tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper241/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335680112, "tmdate": 1552335680112, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper241/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1lAj8TRhm", "original": null, "number": 1, "cdate": 1541490342223, "ddate": null, "tcdate": 1541490342223, "tmdate": 1541490342223, "tddate": null, "forum": "SyezvsC5tX", "replyto": "SyezvsC5tX", "invitation": "ICLR.cc/2019/Conference/-/Paper241/Public_Comment", "content": {"comment": "The paper aims to characterize the loss landscape of over-parameterized neural networks, which fit a fixed training dataset exactly. The authors try to use the regular value theorem to show the set of global minima is a smooth $n - d$ dimensional submanifold of $R^n$, where $n$ is the number of parameters of a neural network and $d$ is the number of data. The proof of this claim is only heuristic, namely, by Sard's theorem, the set of critical values of a smooth function between two smooth manifolds has Lebesgue measure zero. Unfortunately, this is insufficient to assert that global minimum zero, if exists, is a regular value. There is a similar attempt to study this problem at CVPR 2018 (check the link below), which shows that global minimum zero is not a regular value of the loss function with over-parameterized neural networks.\n\nhttp://openaccess.thecvf.com/content_cvpr_2018/html/Shen_Towards_a_Mathematical_CVPR_2018_paper.html", "title": "The result is based on heuristic, hence an unrigorous work."}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper241/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The loss landscape of overparameterized neural networks", "abstract": "We explore some mathematical features of the loss landscape of overparameterized neural networks.  A priori one might imagine that the loss function looks like a typical function from $\\mathbb{R}^n$ to $\\mathbb{R}$ - in particular, nonconvex, with discrete global minima.  In this paper, we prove that in at least one important way, the loss function of an overparameterized neural network does not look like a typical function.  If a neural net has $n$ parameters and is trained on $d$ data points, with $n>d$, we show that the locus $M$ of global minima of $L$ is usually not discrete, but rather an $n-d$ dimensional submanifold of $\\mathbb{R}^n$.  In practice, neural nets commonly have orders of magnitude more parameters than data points, so this observation implies that $M$ is typically a very high-dimensional subset of $\\mathbb{R}^n$. ", "keywords": [], "authorids": ["yaim@math.ias.edu"], "authors": ["Y. Cooper"], "pdf": "/pdf/4c6b42b9a39a8986ce7723ce2673cd42afd55fe9.pdf", "paperhash": "cooper|the_loss_landscape_of_overparameterized_neural_networks", "_bibtex": "@misc{\ncooper2019the,\ntitle={The loss landscape of overparameterized neural networks},\nauthor={Y. Cooper},\nyear={2019},\nurl={https://openreview.net/forum?id=SyezvsC5tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper241/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311886193, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyezvsC5tX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper241/Authors", "ICLR.cc/2019/Conference/Paper241/Reviewers", "ICLR.cc/2019/Conference/Paper241/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311886193}}}], "count": 21}