{"notes": [{"id": "H6ATjJ0TKdf", "original": "kt2arENuPXq", "number": 2494, "cdate": 1601308275759, "ddate": null, "tcdate": 1601308275759, "tmdate": 1616053678296, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "U2rH0RvFFAg", "original": null, "number": 1, "cdate": 1610040413584, "ddate": null, "tcdate": 1610040413584, "tmdate": 1610474011318, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "H6ATjJ0TKdf", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper proposes a layer-wise magnitude-based tuning method through the adoption of LAMP score, motivated by minimizing the model output distortion. The new importance score differs from vanilla magnitude-based score in that it incorporate more layer-wise information. Extensive experiments are conducted on image and language models to show the improved accuracy upon prior arts under same model compression ratio. Ablation study is also provided to further explain the intuition and comparison of LAMP with other pruning methods. \n\nThough the experiments are extensive, some reviewers raised questions that only image datasets are tested. In the rebuttal, the authors addressed more on Appendix D which provides non-image results, and also modified the abstract to highlight the efficacy on image data. In all, given the extensive empirical evaluation on various datasets and model architectures, the improvement of LAMB over prior methods seems convincing. Nevertheless, we urge the authors to include more experimental results, for example ResNet-18 on ImageNet as promised to Reviewer 1, to make the results more solid. It is also suggested to include and discuss some relevant papers mentioned by the reviewers in the final version.  "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"forum": "H6ATjJ0TKdf", "replyto": "H6ATjJ0TKdf", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040413571, "tmdate": 1610474011299, "id": "ICLR.cc/2021/Conference/Paper2494/-/Decision"}}}, {"id": "lF5LHgMrP-v", "original": null, "number": 4, "cdate": 1604285089629, "ddate": null, "tcdate": 1604285089629, "tmdate": 1606800460641, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "H6ATjJ0TKdf", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Official_Review", "content": {"title": "Great Results; Section 3 Needs to be Rewritten", "review": "# After Rebuttal: Score Lowered from 7 to 6\n\n## Concerns Addressed\n\nI appreciate the effort the reviewers put into revising the paper to include the settings I suggested.\n\nI am generally pleased with the revisions the authors made to the paper (especially Section 3), and I appreciate their attention to these details.\n\n## Remaining Concern: Settings in the Main Body are Poorly-Tuned and May Overstate Results\n\nI am concerned by one aspect of Figure 2: the unpruned accuracies for VGG-16 and ResNet-20 are much lower than they should be. VGG-16 should get 94% accuracy on CIFAR-10 (vs. 91% in the plot), and ResNet-20 should get 92% accuracy (vs. 86% accuracy in the plot).  This is because the paper uses Adam to train all networks without any learning rate drops, whereas the typical learning rate schedules for VGG-16 and ResNet-20 use SGD with momentum with learning rate drops.\n\nThis important difference raises the concern of whether the results shown in the paper will translate into fully-tuned, large-scale settings. As evidence of this concern, Appendix G does show a fully-tuned VGG-16 getting standard accuracy. In this setting, LAMP is no better than global magnitude pruning until very extreme sparsities.\n\n**I have lowered my score on the basis that the results in Figure 2 may overstate the value of LAMP in well-tuned settings. I no longer have unequivocal confidence that LAMP is an improvement that should be adopted in general. I implore the authors to replace the experiments in Figure 2 with well-tuned versions of these networks that achieve SOTA accuracies.**\n\n## Overall: Score Lowered from 7 to 6\n\nI am less confident in the method's significance in well-tuned settings, and I can no longer unequivocally trust the empirical evaluation in the paper. I still support acceptance, but only tentatively.\n\n# Overall\n\nI think the technical results are excellent. The authors should be commended for making a productive contribution to the pruning literature. My biggest concerns are :\n(1) The mathematical derivations are unreadable and impossible for future researchers to build on, so the authors *must* expand this section and make it crystal clear if I am to continue to recommend acceptance (they should use the extra page for this)\n(2) The authors should include a network that isn't severely overparameterized for CIFAR-10. I recommend ResNet-20 or ResNet-56.\n(3) The authors need to verify that their statements about prior papers are correct. I have noted a few things that need to be addressed below. In many cases, prior work was unclear about how it handled global/layerwise pruning decisions, and the authors should mention when there is ambiguity.\n\nIn addition, other smaller (but important) changes I would like to see include:\n(4) The authors should ideally include results on all of ImageNet, since subsets of ImageNet tend to show very different results from the full task. However, I understand that this is very expensive and that not everyone has the resources to do so, and I understand if the authors are not in a position to accomplish this.\n(5) The authors should use weight-rewinding as described below in the manner of \"Linear Mode Connectivity and the Lottery Ticket Hypothesis\" (Frankle et al.). Conv-6 is not a setting whose results correspond to larger-scale settings.\n(6) I think the authors should revise the title to clarify that they are proposing a better way to select the layerwise rates for magnitude pruning. This paper makes a valuable contribution, and that should come through from the title so readers can determine this easily.\n\n# Score\n\nI have issued the paper a 7 on the assumption that the authors will make substantial revisions to Section 3 and take advantage of the extra page of space to do so. If they do not address my major concerns (and ideally my minor concerns), I will lower my score.\n\n# Questions\n\nWhy is \"minimizing the l2 distortion for the worst-case input signal\" a reasonable design choice to make? The authors assert that it is what they aim to do, but they never explain why this is a good idea. I see why this makes sense at a high level, but adding a couple of sentences to this effect would be valuable, and I would like to hear from the authors directly.\n\n\"Minimizing l2 distortion for the worst-case input signal is equivalent to minimizing the spectral norm distortion.\" Please justify this. It's not immediate. I assume that the norm on the right side of (4) is the spectral norm, but please clarify.\n\n\"The optimization (4) can be relaxed to a Frobenius distortion minimization.\" Please justify this. It is not immediate. In general, this math is dense and the paper goes through it too quickly, and I wasn't able to follow it.\n\nWhere does the sparsity constraint k come from? \n\nIf I understand \"MP: layerwise distortion minimization\" correctly (which I don't think I do), it's meant to say that pruning the lowest-magnitude weights is a way of minimizing the l2 distortion for the worst-case input signal for a single layer in isolation? If this is the statement, I'm concerned that there's a <= in the derivation, since this suggests these two are not necessarily equivalent. And I don't understand why k is in there if we're pruning to a specific target sparsity. In general, I'm quite confused by (1) what you're trying to show here and (2) the actual details of this derivation.\n\nThe section on the LAMP score is completely uninformative. You need to include more of the derivation in the main body of the paper, especially explaining why it's appropriate to use the relaxation from the generalization theory literature (that again includes a <= suggesting that the two sides are not equivalent), what these quantities mean, what a \"damage score\" is, and how this leads to the LAMP scores. Right now, this paper suffers from - at best - obfuscation via math; worse, I highly doubt it would be possible for a reader (or someone working on follow-up work) to understand or reproduce your derivations/rationale. This is a crucial flaw in the paper.\n\nThe models you use are quite overparameterized in general. I would prefer to see a model like ResNet-20 or ResNet-56 on CIFAR-10, which is much less overparameterized for the specific task. \n\nIn general, the results look very impressive.\n\nI am concerned that these graphs zoom in on the lowest sparsities. Importantly, it is difficult to tell what the unpruned accuracy looks like for these models.\n\nFor weight-rewinding, do not use Conv-6. It is a toy network with little correspondence to real behavior. Use a larger-scale network like VGG-1`6 or ResNet-20 and rewind to iteration 1000 as Frankle et al. do in \"Linear Mode Connectivity and the Lottery Ticket Hypothesis.\"\n\n# Things to Address\n\nVerify that the details of the way that prior work has pruned networks is correct.\n\nEnsure the purpose and derivations in Section 3.1 are explained in a way that is crystal clear to a reader. I'm a mathematically well-informed reader who is an expert on pruning, and I can't follow the derivations or what you're trying to say with those derivations. This is due to unclear writing; I'm not a poorly-informed reader.\n\n# Other Notes\n\n\"The iterative pruning scheme of Han et al. uses global magnitude pruning.\" I believe this is incorrect. Han et al say that \"the pruning threshold is chosen as a quality parameter multiplied by the standard deviation of a layer's weights.\" \n\n\"Frankle & Carbin (2019)...employ uniform layerwise sparsity.\" I believe this is incorrect. Frankle & Carbin use uniform sparsity for MNIST but use global pruning for larger-scale networks (ResNet-18 and VGG-19).", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2494/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "H6ATjJ0TKdf", "replyto": "H6ATjJ0TKdf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2494/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095113, "tmdate": 1606915776398, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2494/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2494/-/Official_Review"}}}, {"id": "UC_pxmKtHsj", "original": null, "number": 1, "cdate": 1603835804866, "ddate": null, "tcdate": 1603835804866, "tmdate": 1606779124192, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "H6ATjJ0TKdf", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Official_Review", "content": {"title": "Well-motivated paper proposing hyperparameter-free layerwise pruning rate heuristic, though theoretical justification and evaluation could both be strengthened.", "review": "# Summary\n\nThe paper proposes LAMP, an importance score for unstructured pruning that incorporates layerwise statistics such that the resultant scores for each connection can be compared globally, cutting down on the hyperparameter space for magnitude pruning from the relatively standard practice of requiring hand-specified layerwise pruning rates. LAMP is motivated with a distortion analysis: LAMP is shown to be equivalent to minimizing an upper bound on the supremum of the change in model predictions of unit vectors. LAMP is compared against layerwise pruning rates obtained by standard uniform layerwise and global pruning, along with the less standard Erdos-Renyi kernel method, showing that LAMP can achieve higher accuracy for equivalent pruning rates in a specific experimental setup across several different networks.\n\n# Strengths\n\n- The problem the paper addresses is well-motivated: magnitude pruning rests on a poorly motivated and poorly understood set of heuristics.\n- The approach proposed by the paper has several desirable properties: it is hyperparameter-free, relatively cheap to compute, and handles some corner cases of global magnitude pruning (e.g., LAMP is invariant to rescaling of a layer in a net with BatchNorm).\n- Within the scope of the empirical evaluation in the paper, LAMP is demonstrated to outperform uniform and global magnitude pruning, the two most common approaches in the literature.\n\n# Weaknesses\n\n- Theoretical justification of LAMP: it's unclear how useful or strong the bound in Equation 7 is, especially given that the network is re-trained after pruning. These types of bounds often seem to be weak or vacuous in practice; further, there's no explicit connection between the bound and the loss, let alone the loss after re-training.\n- Empirical evaluation: the training and re-training regimes used by the paper are very non-standard, raising questions about the generality of the technique and the comparison to prior work. Specifically, the choice of AdamW with a fixed learning rate and the seemingly arbitrarily chosen durations of re-training are out of line with standard practice for many of these networks; these choices mean that it is impossible to compare the accuracy/sparsity tradeoffs achieved by LAMP to the results of various other papers that claim state-of-the-art pruning results with local/global magnitude pruning.\n\n# Overall recommendation\n\n6: Weak accept\n\n# Questions / suggestions for authors\n\nI would be willing to raise my score to a 7 with a more thorough comparison against prior work with more standard hyperparameters. Specifically, if the authors reported the results a comparison of using LAMP with the same networks and training/re-training schedule as any of [1,2,3,4], or any other specific setting that claims to be state-of-the-art, I would raise my score.\n\n# Other comments and suggestions\n\n- The notation of Equation 1 is a little bit confusing: I'm assuming that $W[i]$ is the $i$th element in some unrolled version of $W$, and that $W^2$ denotes element-wise squaring, but it'd be good to clarify this non-standard notation.\n- It would be good to have some more details on the networks, especially since many are being used in non-standard settings (e.g., VGG and ResNet-18 on CIFAR). Specifically: how are these networks adapted to these datasets, and what are the base accuracies for each networks on each dataset?\n- A deeper comparison to the layerwise sparsities (and overall pereformance) of [2] would be appreciated. For example: Section 5 of this paper notes that \"LAMP tends to keep the number of nonzero weights relatively uniform throughout the layers at extreme sparsity level.\" Section 4.2 of [2] notes \"The peaks and crests show that the RL agent automatically learns to prune $3 \\times 3$ convolutional layers with larger sparsity.\" Is there a reason for this discrepancy between the findings of each paper?\n- More details on the experimental methodologies of Figures 4(b,c), along with motivations for these experiments, would be appreciated. I am particularly confused by 4(c) -- I do not see standard SNIP scores on this plot, unless \"Global\" here refers to SNIP scores and not global magnitude pruning (in which case, how does standard global magnitude pruning perform here?). It would be worthwhile to include full methodological details for each of these experiments, including the training and re-training (where applicable) schemes, the specific point in training at which the importance scores are calculated, and the specific importance score used by each line on the plot.\n- Minor point: I don't believe that [1] used global magnitude pruning; I believe a better citation would be [5] (though there are also other prior/contemporary examples of global heuristics, like [6] and [7])\n\n# References used in review:\n\n[1] Song Han, Jeff Pool, John Tran, William Dally. \"Learning both weights and connections for efficient neural networks\".\n[2] Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, Song Han. \"AMC: AutoML for Model Compression and Acceleration on Mobile Devices\"\n[3] Trevor Gale, Erich Elsen, Sara Hooker. \"The State of Sparsity in Deep Neural Networks\"\n[4] Alex Renda, Jonathan Frankle, Michael Carbin. \"Comparing Rewinding and Fine-tuning in Neural Network Pruning\"\n[5] Jonathan Frankle, Michael Carbin. \"The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\"\n[6] Yann Le Cun, John S. Denker, Sara A. Solla. \"Optimal Brain Damage\"\n[7] Namhoon Lee, Thalaiyasingam Ajanthan, Philip H. S. Torr. \"SNIP: Single-shot Network Pruning based on Connection Sensitivity\"\n\n\n# Update post author response\n\nThanks to the authors for the detailed response. The authors have satisfactorily responded to my main criticisms of the paper (primarily about the non-standard evaluation regime, and secondarily about the motivation and strength of the theoretical results), so I\u2019m raising my score to a 7, though I do think the paper would be further improved with references to Appendix G in the main body of the paper.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2494/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "H6ATjJ0TKdf", "replyto": "H6ATjJ0TKdf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2494/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095113, "tmdate": 1606915776398, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2494/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2494/-/Official_Review"}}}, {"id": "sdcBAF9F0lp", "original": null, "number": 15, "cdate": 1606235454290, "ddate": null, "tcdate": 1606235454290, "tmdate": 1606235454290, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "03ysLm7C4ql", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment", "content": {"title": "Thanks!", "comment": "Dear reviewer,\n\nWe are glad to hear that the revision we made was satisfactory to you. We really appreciate your additional effort to check our response and the revised manuscript.\n\nBest regards,  \nAuthors."}, "signatures": ["ICLR.cc/2021/Conference/Paper2494/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ATjJ0TKdf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2494/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2494/Authors|ICLR.cc/2021/Conference/Paper2494/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847749, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment"}}}, {"id": "03ysLm7C4ql", "original": null, "number": 14, "cdate": 1606230164290, "ddate": null, "tcdate": 1606230164290, "tmdate": 1606230164290, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "7GIZcBwsq53", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment", "content": {"title": "I like the changes", "comment": "I like the change to the abstract which narrows the claim of superiority to apply only to image datasets. Much appreciated !\n\nI must confess I had missed Appendix D- that's great that it is included. It gives a sense for how the method performs on a non-image problem. "}, "signatures": ["ICLR.cc/2021/Conference/Paper2494/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ATjJ0TKdf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2494/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2494/Authors|ICLR.cc/2021/Conference/Paper2494/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847749, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment"}}}, {"id": "qw0ba3jlE5I", "original": null, "number": 12, "cdate": 1606205464181, "ddate": null, "tcdate": 1606205464181, "tmdate": 1606205464181, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "H6ATjJ0TKdf", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment", "content": {"title": "Title change.", "comment": "Dear reviewers and AC,\n\nFollowing R1's suggestion, we just changed the title of our manuscript from \"A deeper look at the layerwise sparsity of magnitude-based pruning\" to \"Layer-adaptive sparsity for the magnitude-based pruning.\" We believe that this change would make it easier for the readers to grasp the main contribution of our paper. Any other comments on the title would be greatly appreciated.\n\nBest,\nAuthors."}, "signatures": ["ICLR.cc/2021/Conference/Paper2494/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ATjJ0TKdf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2494/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2494/Authors|ICLR.cc/2021/Conference/Paper2494/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847749, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment"}}}, {"id": "zfQbqq6njHr", "original": null, "number": 6, "cdate": 1606063911343, "ddate": null, "tcdate": 1606063911343, "tmdate": 1606064722054, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "lF5LHgMrP-v", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment", "content": {"title": "Response to R1: Part 2. Other concerns. ", "comment": "__Weight rewinding experiments__  \nFollowing the reviewer\u2019s suggestion, we conducted the weight rewinding experiment on VGG-16 following the settings stated in the \u201cLinear Mode Connectivity and the Lottery Ticket Hypothesis\u201d paper. The result is summarized in **Figure 4-b** of the revised manuscript. Similar to the Conv-6 case, we observe that LAMP performs the best. Unlike the Conv-6 experiment, however, ERK underperforms the global MP only in the low-to-medium sparsity regime, and outperforms global MP with higher sparsities. We have updated the discussions accordingly.\n\n---\n__ImageNet experiments__  \nTo best respond to the reviewer\u2019s question, we conducted _one-shot pruning_ on ResNet-18 trained on the ImageNet dataset, to the sparsity level of 10.74%. We decided to go with the one-shot pruning, as each pruning-retraining iteration took $~1.8$ days on our experimental settings; it was physically impossible for us to report any meaningful _iterative pruning_ result until the end of the response phase. Nevertheless, we will try to have the iterative pruning result ready for the final version.\n\nThe experimental result is as follows: (figures denote the error rate.)  \nunpruned [test@1 30.46] [test@5 10.84]  \n  \nlamp [test@1 41.40] [test@5 18.09]  \nerk   [test@1 41.64] [test@5 18.41]  \nglob [test@1 42.34] [test@5 18.98]  \nunif [test@1 45.58] [test@5 21.46]  \n\n---\n__Zooming on lowest sparsities__  \nThe \u201czoom ratios\u201d of the figures in the original manuscript were selected to capture both the sparsity levels without any performance loss and the sparsity levels where the performance differences are more visible. On the other hand, we agree that there is a room for further clarification.\n\nTo address your concern, we have added **two dotted horizontal lines** to denote the (1) peak accuracy among all sparsity levels and (2) the accuracy at zero sparsity. If you have any more suggestions on how to visualize even better, we would be more than happy to incorporate.\n\n---\nPlease let us know if you have any further concerns.\n\n\nSincerely,  \nAuthors."}, "signatures": ["ICLR.cc/2021/Conference/Paper2494/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ATjJ0TKdf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2494/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2494/Authors|ICLR.cc/2021/Conference/Paper2494/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847749, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment"}}}, {"id": "kQpiIhwUo6J", "original": null, "number": 5, "cdate": 1606063755254, "ddate": null, "tcdate": 1606063755254, "tmdate": 1606064701707, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "lF5LHgMrP-v", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment", "content": {"title": "Response to R1: Part 1. Major concerns.", "comment": "Dear R1,\n\nThank you for taking your time and effort to take a careful look at our manuscript.\n\n---\n__Clarity of Section 3__  \nThank you for this feedback. We revised Section 3 to make the following points clearer.  \n(1) _Why distortion minimization:_ We speculate that the unexpected effectiveness of the MP comes from the fact that the MP solves a relaxed version of the layerwise output distortion minimization. Then, we use this speculation as a guiding principle to design a layerwise sparsity for MP (which might work unexpectedly well as if our speculation is true). The spirit is somewhat similar to the Optimal Brain Damage.  \n(2) _Inequality in derivation:_ We now clearly state that MP solves a \u201crelaxed\u201d version of output distortion minimization.  \n(3) _Sparsity constraint k:_ In the revised version, we state that the sparsity constraint is assumed to be coming from an operational constraint (e.g., required model size).  \n(4) _Why is the used relaxation appropriate:_ The relaxation we used may not be the only relaxed form one can find. However, the relaxation provides several advantages, e.g. efficient computability.  \nPlease let us know if you believe that the revised section 3 can be clarified further. We would be more than happy to make any further revisions if necessary.\n\n---\n__Smaller models for CIFAR-10.__  \nFollowing the reviewer\u2019s suggestion, we added an experimental result on ResNet-20 trained on CIFAR-10 (see **Figure 2-b**); the result replaces the previous result on ResNet-18. Similar to the ResNet-18 result, we observe that LAMP achieves the best performance. In addition, we observe that the accuracy of the ERK baseline closely matches the LAMP accuracy in this smaller model.\n---\n__Descriptions of prior works__\nWe thank the reviewer for pointing this out. We have corrected our descriptions on the algorithms of Han et al. (2015) and Frankle & Carbin (2019)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2494/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ATjJ0TKdf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2494/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2494/Authors|ICLR.cc/2021/Conference/Paper2494/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847749, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment"}}}, {"id": "ztBtu3WrLS", "original": null, "number": 10, "cdate": 1606064314705, "ddate": null, "tcdate": 1606064314705, "tmdate": 1606064641646, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "UC_pxmKtHsj", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment", "content": {"title": "Response to R2: Part 2.", "comment": "__Details on Figs. 4b-c.__  \nThank you for pointing out the need for extra details. To clarify, \u201cGlobal\u201d on Figure 4c indeed refers to \u201cglobal thresholding with SNIP scores.\u201d In fact, we simply plugged in SNIP scores in the place of magnitude scores. We revised our manuscript to provide more details to the readers in **Section 4.2**.\n\nAbout the motivation of Figure 4b-c: As noted in the introduction, our primary goal was to provide a layerwise sparsity scheme that could be plugged in to diverse algorithms which utilizes magnitude heuristics for pruning, regardless of auxiliary modules that constitute a full algorithm. For instance, the algorithm of [Zhu&Gupta 2017](https://arxiv.org/abs/1710.01878) can be viewed as \u201cMP + uniform sparsity + reconnection + cubic pruning schedule.\u201d Here, the re-connectability and cubic pruning schedule may be viewed as modules that are auxiliary to the layerwise sparsity module. In Figure 4b, we test if the LAMP sparsity works well when used jointly with the \u201cweight rewinding module,\u201d which seems to be a promising new component for MP algorithms as [Renda et al. (2020)](https://arxiv.org/abs/2003.02389) showed. For Figure 4c, we asked whether the LAMP sparsity can be combined with pruning scores other than the weight magnitude. While this application is not exactly theory-motivated, we interestingly observed that LAMP sparsity performs competitively with \u201cGlobal SNIP,\u201d which is more theoretically justified. Based on this observation, we would (mildly) suggest using LAMP-like sparsity determination schemes for any \u201cnot-designed-to-be-global scores\u201d for pruning (e.g. [movement pruning](https://arxiv.org/pdf/2005.07683.pdf)).\n\n---\n__Details on the networks.__  \nThank you for this suggestion. We have added the details to **Appendix A**, along with references from which such setting and architecture has been taken from. Also, the base accuracies are now explicitly visualized on each figure, for the ease of comparison.\n\n---\n__Notations.__  \nFollowing your suggestion, we revised **Section 3** to provide more detailed explanations on the notations being used.\n\n---\n__Note on Han et al.__  \nThank you for pointing this out. We revised accordingly.\n\n---\nPlease let us know if you have any further concerns.\n\nSincerely,  \nAuthors."}, "signatures": ["ICLR.cc/2021/Conference/Paper2494/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ATjJ0TKdf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2494/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2494/Authors|ICLR.cc/2021/Conference/Paper2494/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847749, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment"}}}, {"id": "HIl3AT5T6te", "original": null, "number": 9, "cdate": 1606064251360, "ddate": null, "tcdate": 1606064251360, "tmdate": 1606064596588, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "UC_pxmKtHsj", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment", "content": {"title": "Response to R2: Part 1", "comment": "Response to R2.\n\nDear R2,\n\nThank you for a (very detailed) review on our manuscript, and providing various suggestions. \n\n---\n__Non-standard training recipe.__  \nFollowing your suggestion, we ran the VGG-16 + CIFAR-10 again, using the exact experimental setup of [Frankle et al. (2020)](https://arxiv.org/abs/1912.05671), which in turn inherits the settings of [ICLR 2019 paper by Liu et al.](https://arxiv.org/abs/1810.05270); we selected this setup as it seems to be the state-of-the-art for VGG-16+CIFAR-10 setup (with full hyperparameters available), which is the base experimental setup for our paper but has not been discussed in [1,2,3,4].\n\nThe experimental result is added to the **Appendix G**. We averaged over five independent trials for reproducibility, similarly to other experimental results in the paper.\n\n_Observations_: Similarly to the original setup, we observe that (1) LAMP outperforms/matches the performance of the strongest baselines, which are global MP (in the low-sparsity regime) and ERK (in the high-sparsity regime), and (2) ERK > Uniform+ > Uniform holds. One remark is that the performance of global MP tends to be very sensitive to the training recipe, compared to other baselines. LAMP, on the other hand, performs more robustly well under different training setups and achieves a better performance, especially in the high-sparsity regime.\n\n\n\n\n__Theoretical justification of LAMP.__  \nAs the reviewer pointed out, the theoretically motivated design of the LAMP score does not lead to a theoretical performance guarantee of the pruned-retrained model. We would like to clarify, however, that the primary role of the provided inequality (eq. 7) is to motivate the LAMP score and deliver the overall insight. In a sense, this is similar to how [OBD](https://papers.nips.cc/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf) is designed; LeCun et al. motivates the score from the Taylor series decomposition of loss, but the OBD score itself cannot be directly related to the actual loss (even before the retraining), as (1) multiple connections are pruned simultaneously and (2) there should exist higher-order terms as well. The difference from OBD is that the LAMP score is designed having the *output distortion* in mind, which can be thought of as an \u201c$\\ell_2$ loss with respect to the worst-case input.\u201d\n\nWe clarified these points in **Section 3.1** of the revised draft.\n\n---\n__Comparison with AMC.__  \nWe deeply appreciate the pointer to the [AMC paper.](https://arxiv.org/abs/1802.03494), and the suggestion for more discussions. We dug a little deeper into the apparent discrepancy between the AMC-discovered layerwise sparsity and LAMP-discovered sparsity.\n\nWe believe that the discrepancy is due to the discrepancy of model architectures under consideration. According to the AMC paper, \u201cpeaks and crests\u201d take place for ResNet-50 because \u201cthe reinforcement learning agent automatically learns that 3x3 convolution has more redundancy than 1x1 convolutions.\u201d The AMC sparsities of Plain-20 do not exhibit such clearly visible peaks-and-crests. Similar to Plain-20, we reported the layerwise sparsities for VGG-16 composed of 3x3 convolutions.\n\nCan LAMP recover \u201cpeaks and crests\u201d for the mixture-of-1x1-and-3x3 models like ResNet-50? To answer this question, we took a PyTorch pretrained ResNet-50 and performed one-shot pruning with LAMP (without an inference/training step). We report the results on the newly added Appendix F. In Figure 6, we observe that the layerwise sparsity of LAMP has peaks at the 1x1 sparsity. The difference from AMC-discovered sparsity is that for LAMP sparsities such behavior is clearer in the layer residual blocks, while in AMC sparsities the crests and peaks take place at earlier blocks as well.\n\nWe added the discussion to **Appendix F**, which we believe that the readers will enjoy. Thanks again!"}, "signatures": ["ICLR.cc/2021/Conference/Paper2494/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ATjJ0TKdf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2494/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2494/Authors|ICLR.cc/2021/Conference/Paper2494/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847749, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment"}}}, {"id": "oFFSAj_S43i", "original": null, "number": 11, "cdate": 1606064394973, "ddate": null, "tcdate": 1606064394973, "tmdate": 1606064394973, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "H6ATjJ0TKdf", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment", "content": {"title": "Summary of revisions", "comment": "Dear Reviewers and AC,\n\nThank you again for your continuing time and effort to provide detailed feedback on our manuscript. To best appreciate your comments, we conducted several supplementary experiments and revised the manuscript; here is a short list of updates.\n\n- Added experiment on ResNet-20 (less overparameterized model) (R1).\n- Added weight-rewinding experiment on VGG-16 (R1).\n- Fixed the misleading descriptions on the prior work (R1, R2).\n- Revised Section 3 with more detailed explanations (R1, R2).\n- Updated figures with additional visual cues for unpruned/peak accuracies (R1)\n- Added experiments with state-of-the-art training recipe of VGG-16 (R2)\n- Added comparisons to the layerwise sparsity discovered by AMC (R2)\n- \u2026 and other editorial revisions and added explanations to enhance the clarity of the manuscript (R1,2,3,4).\n\nThe updated texts--except for editorial changes--are marked in **purple.**\n\nVia individual comments, we give detailed responses and clarifications to the concerns raised by the reviewers, with pointers to corresponding revisions of the manuscript.\n\nBest regards,  \nAuthors.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2494/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ATjJ0TKdf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2494/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2494/Authors|ICLR.cc/2021/Conference/Paper2494/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847749, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment"}}}, {"id": "wYK2YNXp4XQ", "original": null, "number": 8, "cdate": 1606064153574, "ddate": null, "tcdate": 1606064153574, "tmdate": 1606064153574, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "c3LTo2vo3So", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment", "content": {"title": "Additional response to R4", "comment": "Dear R4,  \n\nThank you again for the thoughtful comments and suggestions on our manuscript. In addition to our initial response regarding the first two concerns you raised, we now respond to your third concern; thank you for your patience!\n\n__ImageNet experiments__  \nAs reviewer#1 also noted, ImageNet experiments are quite computationally expensive; each pruning-retraining iteration takes $~1.8$ days with the computational resource that we have access to. Thus, to best respond to the reviewer\u2019s question, we conducted _one-shot pruning_ on ResNet-18 + ImageNet setup, instead of the iterative pruning: we used the sparsity 10.74%. We will try to have the iterative pruning result ready for the final version.\n\nThe experimental result is as follows: (figures denote the error rate.)  \nunpruned [test@1 30.46] [test@5 10.84]  \n\nlamp [test@1 41.40] [test@5 18.09]  \nerk [test@1 41.64] [test@5 18.41]  \nglob [test@1 42.34] [test@5 18.98]  \nunif [test@1 45.58] [test@5 21.46]  \n\nFinally, we note that we already report the results on the [Restricted ImageNet](https://arxiv.org/abs/1805.12152) dataset, which contains ~268k images of ImageNet resolution.\n\n---\nPlease let us know if you have any further feedback!\n\nSincerely,  \nAuthors.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2494/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ATjJ0TKdf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2494/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2494/Authors|ICLR.cc/2021/Conference/Paper2494/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847749, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment"}}}, {"id": "7GIZcBwsq53", "original": null, "number": 7, "cdate": 1606064005421, "ddate": null, "tcdate": 1606064005421, "tmdate": 1606064005421, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "NQqKMDRXNgB", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment", "content": {"title": "Response to R3", "comment": "Dear R3,\n\nWe appreciate your positive comments on our manuscript (and a nice summary).\n\n__Non-image datasets.__  \nAs the reviewer pointed out, many of the algorithms that work well on the models for image datasets do not work particularly well on non-image models, due to the different training practices and/or model architectures. For this reason, we have reported additional experimental results on the language modeling task on Transformers in Appendix D of our original manuscript; we confirmed that LAMP continues to perform competitively, although the gain is smaller than in the image classification case. Regrettably, this point has not been emphasized enough in the original manuscript.\n\nIn the revised draft, we make the following changes:\n- More pointers to Appendix D is given in the main text.\n- We slightly down-toned the abstract to remove exaggeration.\n- Enhanced Appendix D to have more discussions on the relevant work.\n\n---\n__Typos.__  \nThank you for catching a few of these. They are now fixed!\n\n---\n\nPlease let us know if there are any more comments; we would be more than happy to continue the discussion!\n\nSincerely,  \nAuthors.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2494/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ATjJ0TKdf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2494/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2494/Authors|ICLR.cc/2021/Conference/Paper2494/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847749, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment"}}}, {"id": "0cRFGYb5KwW", "original": null, "number": 4, "cdate": 1605185804217, "ddate": null, "tcdate": 1605185804217, "tmdate": 1605185804217, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "c3LTo2vo3So", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment", "content": {"title": "Clarifications on the Role/Scope of LAMP", "comment": "Dear R4,\n\nThank you for your feedback on our manuscript.\n\nWe would like to briefly answer the first two concerns you have, and straighten out some potential misunderstandings on the proposed method, LAMP.\n\n__Difference from squared magnitude.__\nAs the title of our paper suggests, LAMP aims to provide a method to select the *layerwise sparsity* for the magnitude-based pruning. In other words, our goal is to do the magnitude-based pruning at each layer (to meet the global sparsity constraint), but we hope to select the layerwise sparsity level carefully to maximize the performance of the pruned models.\n\nHere, _global_ pruning (i.e., using a global threshold on the score) using the LAMP score differs from the global pruning using the square weight score. In each layer, they both do the magnitude-based pruning, but globally, they lead to different layerwise sparsity. Indeed, we show that \u201cglobal pruning based on lamp\u201d gives a better performing subnetworks (Section 4) with very different layerwise sparsity levels (Section 5) than the \u201cglobal pruning based on squared magnitude.\u201d We will clarify this in the revised manuscript.\n\n__Comparisons to channel pruning methods.__\nAs we briefly mentioned above, LAMP is designed as a layerwise sparsity selection scheme for MP (which is an important subject, as Gale et al. (2019) demonstrated), that can be combined with various other algorithmic modules (gradual pruning schedule, weight rewinding) to constitute a full pruning-retraining pipeline. In this sense, we do not view that the works R4 mentioned (Network-Slimming, Soft filter pruning) should be the baseline of our method; instead, we make comparisons to other *layerwise sparsity* selection schemes for magnitude-based pruning algorithms. We will revise our introduction to emphasize this point.\n\nWe hope this response will resolve some of your concerns. Please let us know if there are any unclear points remaining.\n\nSincerely,  \nAuthors.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2494/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ATjJ0TKdf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2494/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2494/Authors|ICLR.cc/2021/Conference/Paper2494/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847749, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment"}}}, {"id": "bmutg2xptS", "original": null, "number": 3, "cdate": 1605185728391, "ddate": null, "tcdate": 1605185728391, "tmdate": 1605185744734, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "H6ATjJ0TKdf", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment", "content": {"title": "An early response", "comment": "Dear reviewers and AC,\n\nWe wholeheartedly appreciate your insightful comments and constructive suggestions provided to help us improve our manuscript. Embracing the feedback, we are making several revisions and conducting supplementary experiments, which we hope to deliver soon.\n\nBefore the upcoming revision, however, we would like to briefly address some of the concerns which might have come from a slight misunderstanding; we would provide detailed clarifications via individual replies to each reviews.\n\nBest regards,  \nAuthors.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2494/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ATjJ0TKdf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2494/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2494/Authors|ICLR.cc/2021/Conference/Paper2494/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847749, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2494/-/Official_Comment"}}}, {"id": "c3LTo2vo3So", "original": null, "number": 2, "cdate": 1603857348713, "ddate": null, "tcdate": 1603857348713, "tmdate": 1605024198701, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "H6ATjJ0TKdf", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Official_Review", "content": {"title": "Reviews", "review": "Summary\n- The authors propose LAMP, a layerwise adaptive magnitude-based pruning method.  The authors conduct extensive experiments on CIFAR10/CIFAR100/SVHN and Penn Treebank to validate the method.\n\nPros\n- somewhat novel pruning method, based on new weight score\n- extensive experiments on image and language datasets\n\nCons\n- In Equation (2), the authors point out that LAMP score is align with the order of weight squares. Therefore, one can directly prune the network based on weight squares. Why is it necessary to prune the network based on LAMP?\n- The comparisons are not sufficient. The authors should compare other \"pruning-retraining\" methods, like network slimming [1], soft filter pruning [2], etc. Though they focus on structured pruning, the core idea can be borrowed and adapted for unstructured pruning.\n- Lacking of experiments on large-scale datasets and large models, for example, on ImageNet. The performance of pruning methods  can be very sensitive and versatile when only evaluating on small datasets and models. And usually, a pruning method can be invalid when testing on ImageNet models.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2494/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "H6ATjJ0TKdf", "replyto": "H6ATjJ0TKdf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2494/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095113, "tmdate": 1606915776398, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2494/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2494/-/Official_Review"}}}, {"id": "NQqKMDRXNgB", "original": null, "number": 3, "cdate": 1603918704279, "ddate": null, "tcdate": 1603918704279, "tmdate": 1605024198544, "tddate": null, "forum": "H6ATjJ0TKdf", "replyto": "H6ATjJ0TKdf", "invitation": "ICLR.cc/2021/Conference/Paper2494/-/Official_Review", "content": {"title": "Strong experimental results and an elegant technique- more experimental diversity would be beneficial", "review": "This paper presents a novel technique  (layer-adaptive magnitude based pruning, or LAMP) for pruning neural network weights (pruning can be beneficial in terms of overfitting prevention as well as other practical considerations).  LAMP evaluates weights in each layer in terms of the ratio of the magnitude of the weight to the sum of magnitudes of all surviving weights in the layer. The weight which evaluates as least important across all layers is pruned and then the process is repeated until the desired sparsity is achieved. The method is motivated theoretically as minimizing the distortion in the input/output mapping implemented by the weights of the layer. Experimental results on several benchmarks are presented.\n\nPros:\n\nThe experimental results are strong,  with LAMP consistently winning vs. competing techniques on 4 benchmarks problems. \n\nThe method is elegant, requiring no hyperparameter tuning and minimal computation. \n\nThe theoretical justification (mapping-distortion-minimization) makes a lot of sense. \n\nCons:\n\nMy only objection is that all of the experiments seem to be done on image datasets. It seems possible that deep learning networks applied to non-image data might not do as well under LAMP as they do for images. 'Under diverse datasets' in the abstract seems like an exaggeration. \n\nFurther comments:\n\nI found a couple of typos.\n\nP3 while such unstrutured pruning -> while such unstructured pruning\nP5 Global \u2018on every layers\u2019 -> on every layer\n\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2494/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2494/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Layer-adaptive Sparsity for the Magnitude-based Pruning", "authorids": ["~Jaeho_Lee3", "~Sejun_Park1", "~Sangwoo_Mo1", "~Sungsoo_Ahn1", "~Jinwoo_Shin1"], "authors": ["Jaeho Lee", "Sejun Park", "Sangwoo Mo", "Sungsoo Ahn", "Jinwoo Shin"], "keywords": ["network pruning", "layerwise sparsity", "magnitude-based pruning"], "abstract": "Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level $\\ell_2$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation.\nUnder various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection.\nFurthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity", "one-sentence_summary": "We propose LAMP, a general-purpose layerwise sparsity selection scheme for magnitude pruning.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|layeradaptive_sparsity_for_the_magnitudebased_pruning", "supplementary_material": "/attachment/d58ab526ea48bf9d41dc018a7eed88dcd369e06b.zip", "pdf": "/pdf/8f3f5f35c8fab8132c04a19d4211d5329db20042.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nlee2021layeradaptive,\ntitle={Layer-adaptive Sparsity for the Magnitude-based Pruning},\nauthor={Jaeho Lee and Sejun Park and Sangwoo Mo and Sungsoo Ahn and Jinwoo Shin},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ATjJ0TKdf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "H6ATjJ0TKdf", "replyto": "H6ATjJ0TKdf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2494/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095113, "tmdate": 1606915776398, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2494/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2494/-/Official_Review"}}}], "count": 18}