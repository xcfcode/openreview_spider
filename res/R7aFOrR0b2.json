{"notes": [{"id": "R7aFOrR0b2", "original": "E6TgOtAgvoA", "number": 682, "cdate": 1601308080759, "ddate": null, "tcdate": 1601308080759, "tmdate": 1614985625308, "tddate": null, "forum": "R7aFOrR0b2", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Dataset Curation Beyond Accuracy", "authorids": ["~Johan_Bjorck2", "~Carla_P_Gomes1"], "authors": ["Johan Bjorck", "Carla P Gomes"], "keywords": ["crowd-sourcing", "calibration", "dataset", "uncertainty"], "abstract": "Neural networks are known to be data-hungry, and collecting large labeled datasets is often a crucial step in deep learning deployment. Researchers have studied dataset aspects such as distributional shift and labeling cost, primarily using downstream prediction accuracy for evaluation. In sensitive real-world applications such as medicine and self-driving cars, not only is the accuracy important, but also the calibration -- the extent that model uncertainty reflects the actual correctness likelihood. It has recently been shown that modern neural networks are ill-calibrated. In this work, we take a complementary approach -- studying how dataset properties, rather than architecture, affects calibration. For the common issue of dataset imbalance, we show that calibration varies significantly among classes, even when common strategies to mitigate class imbalance are employed. We also study the effects of label quality, showing how label noise dramatically increases calibration error. Furthermore, poor calibration can come from small dataset sizes, which we motive via results on network expressivity. Our experiments demonstrate that dataset properties can significantly affect calibration and suggest that calibration should be measured during dataset curation.", "one-sentence_summary": "We demonstrate that dataset properties such as imbalanced datasets and noisy labels influence not only accuracy, but also calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bjorck|dataset_curation_beyond_accuracy", "pdf": "/pdf/19547af72952e4c1e55f808961f95c872390e5fb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GautIMZOjd", "_bibtex": "@misc{\nbjorck2021dataset,\ntitle={Dataset Curation Beyond Accuracy},\nauthor={Johan Bjorck and Carla P Gomes},\nyear={2021},\nurl={https://openreview.net/forum?id=R7aFOrR0b2}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "aXqnbSq4nCN", "original": null, "number": 1, "cdate": 1610040534504, "ddate": null, "tcdate": 1610040534504, "tmdate": 1610474144403, "tddate": null, "forum": "R7aFOrR0b2", "replyto": "R7aFOrR0b2", "invitation": "ICLR.cc/2021/Conference/Paper682/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The authors empirically analyse the properties of datasets which lead to poor calibration. In particular, they show that high class imbalance, high degree of label noise, and small dataset size are all likely to lead to poor overall calibration or poor per-class calibration. While there are some interesting insights in this work, the reviewers argued that the contribution is not substantial enough for ICLR. To improve the manuscript the authors should consider accuracy and calibration jointly and extend the results pertaining to label noise which were appreciated by the reviewers. For the former, the same conclusions hold for accuracy, instead of calibration, which raises the question of their relationship -- is there a tradeoff? For the latter, the reviewers pointed to a concrete extension with structured label noise. Finally, the theoretical analysis is a step in the right direction, but the assumption on the width of the network required to fit the training set is too restrictive in practice. Therefore, I will recommend rejection."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Curation Beyond Accuracy", "authorids": ["~Johan_Bjorck2", "~Carla_P_Gomes1"], "authors": ["Johan Bjorck", "Carla P Gomes"], "keywords": ["crowd-sourcing", "calibration", "dataset", "uncertainty"], "abstract": "Neural networks are known to be data-hungry, and collecting large labeled datasets is often a crucial step in deep learning deployment. Researchers have studied dataset aspects such as distributional shift and labeling cost, primarily using downstream prediction accuracy for evaluation. In sensitive real-world applications such as medicine and self-driving cars, not only is the accuracy important, but also the calibration -- the extent that model uncertainty reflects the actual correctness likelihood. It has recently been shown that modern neural networks are ill-calibrated. In this work, we take a complementary approach -- studying how dataset properties, rather than architecture, affects calibration. For the common issue of dataset imbalance, we show that calibration varies significantly among classes, even when common strategies to mitigate class imbalance are employed. We also study the effects of label quality, showing how label noise dramatically increases calibration error. Furthermore, poor calibration can come from small dataset sizes, which we motive via results on network expressivity. Our experiments demonstrate that dataset properties can significantly affect calibration and suggest that calibration should be measured during dataset curation.", "one-sentence_summary": "We demonstrate that dataset properties such as imbalanced datasets and noisy labels influence not only accuracy, but also calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bjorck|dataset_curation_beyond_accuracy", "pdf": "/pdf/19547af72952e4c1e55f808961f95c872390e5fb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GautIMZOjd", "_bibtex": "@misc{\nbjorck2021dataset,\ntitle={Dataset Curation Beyond Accuracy},\nauthor={Johan Bjorck and Carla P Gomes},\nyear={2021},\nurl={https://openreview.net/forum?id=R7aFOrR0b2}\n}"}, "tags": [], "invitation": {"reply": {"forum": "R7aFOrR0b2", "replyto": "R7aFOrR0b2", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040534490, "tmdate": 1610474144387, "id": "ICLR.cc/2021/Conference/Paper682/-/Decision"}}}, {"id": "TTtiwm_TVRC", "original": null, "number": 2, "cdate": 1606175983308, "ddate": null, "tcdate": 1606175983308, "tmdate": 1606175983308, "tddate": null, "forum": "R7aFOrR0b2", "replyto": "R7aFOrR0b2", "invitation": "ICLR.cc/2021/Conference/Paper682/-/Official_Comment", "content": {"title": "Rebuttal", "comment": "\nWe thank the reviewers for their thoughtful comments and ideas to improve the paper. We address major questions from reviewers (R1, R2, R3, R4) below.\n\n\n\n# R1. \n\n----------------\n\nHow do these experiments relate to accuracy? \n\n- There is no simple relationship between calibration and accuracy. It has previously been shown that models with more parameters have worse calibration despite being more accurate. We show how e.g. smaller datasets result in worse calibration in addition to worse accuracy. Generally, noisy classes have worse accuracy and smaller datasets result in worse accuracy. There is plenty of work on what factors influence accuracy, and we have specifically focused on calibration to complement previous studies and highlight another way in which datasets can influence training.\n\n\n\n\n# R2. \n----------------\n\nIt would be helpful to consider non-synthetic noise\n\n- This is a great point, we\u2019d love to do that but in order to measure the noise, it\u2019s preferable to have \u201ccorrect\u201d ground truth labels to be able to assess the noise rate. We\u2019re happy for any pointers towards specific datasets that could be used for this.\n\n\n\n\n# R4. \n----------------\n\nThe organization can be improved by putting experiments last \n\n- This is a good point, the suggested layout of stating the main observations and the theoretical motivation first, and thereafter going through the experiments would likely improve the clarity of the paper. Thanks!\n\n\nThe  practicability is limited\n\n- We believe that the first step towards addressing an issue is being aware of its existence. To the best of our knowledge, the role that datasets can play in calibration has not been observed before, thus making the community aware of this issue is a first step towards developing methods to address it. Furthermore, the results of this paper might inform practitioners on the importance of dataset curation even when target accuracy is met. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper682/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper682/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Curation Beyond Accuracy", "authorids": ["~Johan_Bjorck2", "~Carla_P_Gomes1"], "authors": ["Johan Bjorck", "Carla P Gomes"], "keywords": ["crowd-sourcing", "calibration", "dataset", "uncertainty"], "abstract": "Neural networks are known to be data-hungry, and collecting large labeled datasets is often a crucial step in deep learning deployment. Researchers have studied dataset aspects such as distributional shift and labeling cost, primarily using downstream prediction accuracy for evaluation. In sensitive real-world applications such as medicine and self-driving cars, not only is the accuracy important, but also the calibration -- the extent that model uncertainty reflects the actual correctness likelihood. It has recently been shown that modern neural networks are ill-calibrated. In this work, we take a complementary approach -- studying how dataset properties, rather than architecture, affects calibration. For the common issue of dataset imbalance, we show that calibration varies significantly among classes, even when common strategies to mitigate class imbalance are employed. We also study the effects of label quality, showing how label noise dramatically increases calibration error. Furthermore, poor calibration can come from small dataset sizes, which we motive via results on network expressivity. Our experiments demonstrate that dataset properties can significantly affect calibration and suggest that calibration should be measured during dataset curation.", "one-sentence_summary": "We demonstrate that dataset properties such as imbalanced datasets and noisy labels influence not only accuracy, but also calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bjorck|dataset_curation_beyond_accuracy", "pdf": "/pdf/19547af72952e4c1e55f808961f95c872390e5fb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GautIMZOjd", "_bibtex": "@misc{\nbjorck2021dataset,\ntitle={Dataset Curation Beyond Accuracy},\nauthor={Johan Bjorck and Carla P Gomes},\nyear={2021},\nurl={https://openreview.net/forum?id=R7aFOrR0b2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "R7aFOrR0b2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper682/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper682/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper682/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper682/Authors|ICLR.cc/2021/Conference/Paper682/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper682/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868330, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper682/-/Official_Comment"}}}, {"id": "aVVxVfb3gvn", "original": null, "number": 1, "cdate": 1603685749695, "ddate": null, "tcdate": 1603685749695, "tmdate": 1605024631091, "tddate": null, "forum": "R7aFOrR0b2", "replyto": "R7aFOrR0b2", "invitation": "ICLR.cc/2021/Conference/Paper682/-/Official_Review", "content": {"title": "This paper studied how dataset properties, included label quality and data size, affects calibration. The author  analyzed the affection of different dataset properties by testing on varying computer vision datasets  qualitatively. The experimental results show that poor calibration error accompanies with large noisy label rate, large imbalance ratio and small datasets. The author also provided the theoretical proof of why small data size results in high calibration error. ", "review": "\tThis paper discussed how data properties (e.g., label noise, label imbalance, data size) affects calibration error. The author designed experiments on varying computer vision datasets (i.e., cifar10, cifar100, eurosat and iNaturalist) qualitatively: 1) calibration error for various individual classes under class-imbalance situation; 2) calibration error for different scale of label noise; 3) calibration error under non-uniform noise; 4) calibration error under various scale of dataset size; 5) Calibration error under different combinations of data augmentations. The experimental results show that poor calibration performance accompanies with large noisy label rate, large imbalance ratio and small dataset size. For the reason of small dataset size causing poor calibration error, this paper provided the theoretical proof. \n\t\n\tAdvantages:\n\t\t\u25cb The idea of considering a softmax-cross entropy logit loss to help explain how data size affect the calibration error is interesting.\n\t\n\tMajor concerns:\n\t\t\u25cb Organization should be improved. In particular, the factors that affect the calibration error should be listed and well described in a separate section (e.g., Intro -> Background -> (affected data properties) -> experiments), and the theoretical motivation could also be integrated in such section rather than put it after experiments. \n\t\t\u25cb The novelty and practicability of this paper is limited, since this paper only tells people that low label quality and small data size would arise calibration error, the paper analyzed the factors qualitatively but not quantitatively. In the future research, the researcher still hard to justify how much calibration error the current dataset whould bring or can't tell whether the current the current classifier whould be robust enough to defense the calibration bring by the current set. An example is: \n\t\t[1] \"Robustness of classifiers: from adversarial to random noise.\" Fawz et al. NIPS2016. This paper analyzed the robustness of classifiers quantitatively with considering adversarial and random noise. \n\tMinor comments: \n\t\t\u25cb Table 1, \"exp-inbalance\" -> \"exp-imbalance\"\n\t\t\u25cb Should the captions of Figure 2 and Figure 3 be changed?\n\t\t\u25cb Assumption 1, \"x_i != x_i\" -> \"x_i != x_j\"\n\t\t\u25cb Equation 2, \"-sum_i(A+B)\" -> \"sum_i(A-B)\".\n\t\t\u25cb There are many typos in this paper, should go over the paper again and correct these small mistakes.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper682/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper682/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Curation Beyond Accuracy", "authorids": ["~Johan_Bjorck2", "~Carla_P_Gomes1"], "authors": ["Johan Bjorck", "Carla P Gomes"], "keywords": ["crowd-sourcing", "calibration", "dataset", "uncertainty"], "abstract": "Neural networks are known to be data-hungry, and collecting large labeled datasets is often a crucial step in deep learning deployment. Researchers have studied dataset aspects such as distributional shift and labeling cost, primarily using downstream prediction accuracy for evaluation. In sensitive real-world applications such as medicine and self-driving cars, not only is the accuracy important, but also the calibration -- the extent that model uncertainty reflects the actual correctness likelihood. It has recently been shown that modern neural networks are ill-calibrated. In this work, we take a complementary approach -- studying how dataset properties, rather than architecture, affects calibration. For the common issue of dataset imbalance, we show that calibration varies significantly among classes, even when common strategies to mitigate class imbalance are employed. We also study the effects of label quality, showing how label noise dramatically increases calibration error. Furthermore, poor calibration can come from small dataset sizes, which we motive via results on network expressivity. Our experiments demonstrate that dataset properties can significantly affect calibration and suggest that calibration should be measured during dataset curation.", "one-sentence_summary": "We demonstrate that dataset properties such as imbalanced datasets and noisy labels influence not only accuracy, but also calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bjorck|dataset_curation_beyond_accuracy", "pdf": "/pdf/19547af72952e4c1e55f808961f95c872390e5fb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GautIMZOjd", "_bibtex": "@misc{\nbjorck2021dataset,\ntitle={Dataset Curation Beyond Accuracy},\nauthor={Johan Bjorck and Carla P Gomes},\nyear={2021},\nurl={https://openreview.net/forum?id=R7aFOrR0b2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "R7aFOrR0b2", "replyto": "R7aFOrR0b2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper682/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538137628, "tmdate": 1606915809664, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper682/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper682/-/Official_Review"}}}, {"id": "LY3yLjXs7-", "original": null, "number": 3, "cdate": 1603916427733, "ddate": null, "tcdate": 1603916427733, "tmdate": 1605024631028, "tddate": null, "forum": "R7aFOrR0b2", "replyto": "R7aFOrR0b2", "invitation": "ICLR.cc/2021/Conference/Paper682/-/Official_Review", "content": {"title": "Interesting work but can be extended", "review": "This work is an empirical survey of the calibration problem with convnets. The authors use several existing benchmark datasets and create synthetic class-imbalance for datasets that are initially balanced. They then extend the well-known results on higher prediction error of minority class, to its calibration error. The work investigates several existing methods that alleviate prediction error in imbalanced datasets and examine their effect on calibration error. At last, the effect of dataset size and data augmentation on calibration error is reported. Later on, the effect of random label noise is also examined. The observations, although not surprising, have not been reported before \nThe work is interesting, the writing is clear, and the experiments are comprehensive. Although the observations are very informative, the overall contribution of the paper is not sufficient for the ICLR venue. The work is mostly focused on reporting an existing issue with no major theoretical analysis of the problem and guidelines for alleviating the mentioned problems. The paper is in an interesting direction but needs to become more mature. \n\nQuestions and suggestoins:\n1- The label noise experiments are interesting. In reality, label noise is rarely random and is structured. It would be more helpful if the authors could extend the experiment to incorporate such scenarios.\n2- There seems to be an interesting difference among various reweighting methods in Table 1. It would be interesting if authors compared their calibration error performance to their prediction error performance to find out if there is a trade-off or the two phenomena are in the same direction.\n3- In a lot of experiments, for instance the dataset size, it's expected to have higher calibration error for smaller data. It would be more informative if the general trend of calibration error is compared with the trend in prediction error side by side.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper682/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper682/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Curation Beyond Accuracy", "authorids": ["~Johan_Bjorck2", "~Carla_P_Gomes1"], "authors": ["Johan Bjorck", "Carla P Gomes"], "keywords": ["crowd-sourcing", "calibration", "dataset", "uncertainty"], "abstract": "Neural networks are known to be data-hungry, and collecting large labeled datasets is often a crucial step in deep learning deployment. Researchers have studied dataset aspects such as distributional shift and labeling cost, primarily using downstream prediction accuracy for evaluation. In sensitive real-world applications such as medicine and self-driving cars, not only is the accuracy important, but also the calibration -- the extent that model uncertainty reflects the actual correctness likelihood. It has recently been shown that modern neural networks are ill-calibrated. In this work, we take a complementary approach -- studying how dataset properties, rather than architecture, affects calibration. For the common issue of dataset imbalance, we show that calibration varies significantly among classes, even when common strategies to mitigate class imbalance are employed. We also study the effects of label quality, showing how label noise dramatically increases calibration error. Furthermore, poor calibration can come from small dataset sizes, which we motive via results on network expressivity. Our experiments demonstrate that dataset properties can significantly affect calibration and suggest that calibration should be measured during dataset curation.", "one-sentence_summary": "We demonstrate that dataset properties such as imbalanced datasets and noisy labels influence not only accuracy, but also calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bjorck|dataset_curation_beyond_accuracy", "pdf": "/pdf/19547af72952e4c1e55f808961f95c872390e5fb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GautIMZOjd", "_bibtex": "@misc{\nbjorck2021dataset,\ntitle={Dataset Curation Beyond Accuracy},\nauthor={Johan Bjorck and Carla P Gomes},\nyear={2021},\nurl={https://openreview.net/forum?id=R7aFOrR0b2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "R7aFOrR0b2", "replyto": "R7aFOrR0b2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper682/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538137628, "tmdate": 1606915809664, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper682/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper682/-/Official_Review"}}}, {"id": "X9QI5g_7BqY", "original": null, "number": 4, "cdate": 1603998052554, "ddate": null, "tcdate": 1603998052554, "tmdate": 1605024630965, "tddate": null, "forum": "R7aFOrR0b2", "replyto": "R7aFOrR0b2", "invitation": "ICLR.cc/2021/Conference/Paper682/-/Official_Review", "content": {"title": "If we were reporting accuracy in the paper experiments, how different would the conclusions be?", "review": "The paper is an empirical study looking at how different dataset properties affect model calibration in the context of vision tasks. All experiments use a specific well-known vision model (ResNet 50). \n\nIn particular, the dataset properties that are investigated are:\n\n- Balanced/Unbalanced classes.\n- Label quality.\n- Dataset size.\n- Augmentations.\n- NLP.\n\nI briefly present the main conclusions below.\n\n- Balance in classes. Often times some classes have way more datapoints than others. The authors look at four datasets (Cifar 10, Cifar 100, Eurosat, iNaturalist). The last one's classes are unbalanced, whereas the first three require some sampling method to (artificially) make them unbalanced (note in this case by design there is no relationship between balance/unbalance and the class properties). Figure 1 shows the results. For Cifar and Eurosat those classes with more examples are better calibrated. The trend is somewhat similar for iNaturalist.\n\nThen, the authors present a number of approaches people have tried in the past to mitigate the consequences of unbalance in data. They repeat the previous experiment (on Cifar 10, Cifar 100, Eurosat) but, this time, using each of those methods while training the model. Table 1 shows the results. The ratio column offers very mixed results depending on the dataset and method. The authors conclude that overall the imbalance in calibration persists in most cases.\n\nQ. How do these results compare to accuracy? One would also expect to do better on classes with more data.\n\n- Label quality. The authors tackle the question of how label noise affects calibration. In order to do that, they artificially inject noise to the \"true\" labels with increasing probability. Figure 2 summarizes the calibration error for a number of datasets and noise level. The pattern is clear: the more noise, the worse the calibration. Importantly, the calibration is measured on a test set that is not perturbed with random noise. Accordingly, results were to be expected: there's a mismatch between training and test distributions, and the further apart they are, the less \"meaningful\" predicted probabilities one should expect. Again, it would be informative to see how the *accuracy* of the model also degrades under this circumstances. Similarly, Figure 3 shows the effect of non-uniform noise across classes. Those classes \"attacked\" with more noise are worse calibrated.\n\n- Dataset size. Another important practical aspect to study is dataset size. The authors subsample uniformly at random a fraction of the data points, and measure ECE. Figure 4 shows how models trained on more data are better calibrated. Again, the accuracy of the model should also be shown for context.\n\n- Augmentations. It is common to use data augmentation to train better models; augmentations make the effective datasize larger. Figure 5 shows how removing augmentation axes leads to worse calibration. The same probably applies to accuracy (that's the reason why people use this!). This result is probably intimately related to the previous point (dataset size).\n\n- NLP. The conclusions regarding dataset size also hold with a Transformer on an NLP dataset.\n\nFinally, Section 4 provides some theoretical explanation. We can summarize this as: the cross-entropy loss wants to have more and more confidence / probability on the right class for a given example, and when the data is small and the model powerful enough, we can basically memorize it to make cross-entropy happy. This, however, leads to overconfidence and poor calibration.\n\nOn one hand, it's recently becoming clear that ECE is not a very robust estimator. Depending on design choices (as number of bins, argmax vs all, adaptive versus fixed bins, etc.) the ranking among models and conclusions can change wildly [1]. On the other, this study fixed a specific model, so one could say that the conclusions are \"shown\" for the (dataset, model) pairs. Still, I believe the conclusions are true in a more general setting, though, and the model is fairly reasonable. However, while the paper is titled \"Dataset Curation Beyond Accuracy\", I do not see how the outcome and conclusions of all these experiments would be different if we were looking at accuracy rather than calibration. The authors should measure, include, and address this, and try to disentangle both aspects, or argue for any correlation / causation relationship among them.\n\n[1] - Measuring Calibration in Deep Learning - https://arxiv.org/abs/1904.01685", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper682/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper682/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Curation Beyond Accuracy", "authorids": ["~Johan_Bjorck2", "~Carla_P_Gomes1"], "authors": ["Johan Bjorck", "Carla P Gomes"], "keywords": ["crowd-sourcing", "calibration", "dataset", "uncertainty"], "abstract": "Neural networks are known to be data-hungry, and collecting large labeled datasets is often a crucial step in deep learning deployment. Researchers have studied dataset aspects such as distributional shift and labeling cost, primarily using downstream prediction accuracy for evaluation. In sensitive real-world applications such as medicine and self-driving cars, not only is the accuracy important, but also the calibration -- the extent that model uncertainty reflects the actual correctness likelihood. It has recently been shown that modern neural networks are ill-calibrated. In this work, we take a complementary approach -- studying how dataset properties, rather than architecture, affects calibration. For the common issue of dataset imbalance, we show that calibration varies significantly among classes, even when common strategies to mitigate class imbalance are employed. We also study the effects of label quality, showing how label noise dramatically increases calibration error. Furthermore, poor calibration can come from small dataset sizes, which we motive via results on network expressivity. Our experiments demonstrate that dataset properties can significantly affect calibration and suggest that calibration should be measured during dataset curation.", "one-sentence_summary": "We demonstrate that dataset properties such as imbalanced datasets and noisy labels influence not only accuracy, but also calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bjorck|dataset_curation_beyond_accuracy", "pdf": "/pdf/19547af72952e4c1e55f808961f95c872390e5fb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GautIMZOjd", "_bibtex": "@misc{\nbjorck2021dataset,\ntitle={Dataset Curation Beyond Accuracy},\nauthor={Johan Bjorck and Carla P Gomes},\nyear={2021},\nurl={https://openreview.net/forum?id=R7aFOrR0b2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "R7aFOrR0b2", "replyto": "R7aFOrR0b2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper682/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538137628, "tmdate": 1606915809664, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper682/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper682/-/Official_Review"}}}, {"id": "Bf5rDzJjmlq", "original": null, "number": 2, "cdate": 1603892616202, "ddate": null, "tcdate": 1603892616202, "tmdate": 1605024630906, "tddate": null, "forum": "R7aFOrR0b2", "replyto": "R7aFOrR0b2", "invitation": "ICLR.cc/2021/Conference/Paper682/-/Official_Review", "content": {"title": "not innovative, yet important simulations to help develop more robust experiments", "review": "In this work, authors demonstrate that dataset properties can significantly affect calibration and suggest that calibration should be measured during dataset curation. In the field of applied AI to real-life problem, we face all the time decision-makings on what is the most effective strategy in the pipeline (eg. sampling, noise, labeling) and this paper present some evidence for those decisions.\n\n\nThis type of work is important to systematically highlight areas or processes to follow in model development.\nThe study is not very novel, but important. Since the conclusions are very important and have key implications, I would suggest to apply this to more datasets, and also some of the existing synthetic datasets. Personally, I would like to see if these observations remain solid with more datasets and more variation of datasets.  \n\nI did not found any inconsistencies.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper682/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper682/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Dataset Curation Beyond Accuracy", "authorids": ["~Johan_Bjorck2", "~Carla_P_Gomes1"], "authors": ["Johan Bjorck", "Carla P Gomes"], "keywords": ["crowd-sourcing", "calibration", "dataset", "uncertainty"], "abstract": "Neural networks are known to be data-hungry, and collecting large labeled datasets is often a crucial step in deep learning deployment. Researchers have studied dataset aspects such as distributional shift and labeling cost, primarily using downstream prediction accuracy for evaluation. In sensitive real-world applications such as medicine and self-driving cars, not only is the accuracy important, but also the calibration -- the extent that model uncertainty reflects the actual correctness likelihood. It has recently been shown that modern neural networks are ill-calibrated. In this work, we take a complementary approach -- studying how dataset properties, rather than architecture, affects calibration. For the common issue of dataset imbalance, we show that calibration varies significantly among classes, even when common strategies to mitigate class imbalance are employed. We also study the effects of label quality, showing how label noise dramatically increases calibration error. Furthermore, poor calibration can come from small dataset sizes, which we motive via results on network expressivity. Our experiments demonstrate that dataset properties can significantly affect calibration and suggest that calibration should be measured during dataset curation.", "one-sentence_summary": "We demonstrate that dataset properties such as imbalanced datasets and noisy labels influence not only accuracy, but also calibration.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "bjorck|dataset_curation_beyond_accuracy", "pdf": "/pdf/19547af72952e4c1e55f808961f95c872390e5fb.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=GautIMZOjd", "_bibtex": "@misc{\nbjorck2021dataset,\ntitle={Dataset Curation Beyond Accuracy},\nauthor={Johan Bjorck and Carla P Gomes},\nyear={2021},\nurl={https://openreview.net/forum?id=R7aFOrR0b2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "R7aFOrR0b2", "replyto": "R7aFOrR0b2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper682/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538137628, "tmdate": 1606915809664, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper682/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper682/-/Official_Review"}}}], "count": 7}