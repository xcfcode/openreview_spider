{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396517356, "tcdate": 1486396517356, "number": 1, "id": "HkpDnfI_g", "invitation": "ICLR.cc/2017/conference/-/paper328/acceptance", "forum": "rkpdnIqlx", "replyto": "rkpdnIqlx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "This paper proposes an algorithm for training undirected probabilistic graphical models. However, there are technical concerns of correctness that haven't been responded to. It also wasn't felt the method was evaluated appropriately."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Walkback Algorithm", "abstract": "A recognized obstacle to training undirected graphical models with latent variables such as Boltzmann machines is that the maximum likelihood training procedure requires sampling from Monte-Carlo Markov chains which may not mix well, in the inner loop of training, for each example.  We first propose the idea that it is sufficient to locally carve the energy function everywhere so that its gradient points in the \"right\" direction (i.e., towards generating the data). Following on previous work on contrastive divergence, denoising autoencoders, generative stochastic networks and unsupervised learning using non-equilibrium dynamics, we propose a variational bound on the marginal log-likelihood of the data which corresponds to a new learning procedure that first walks away from data points by following the model transition operator and then trains that operator to walk backwards for each of these steps, back towards the training example. The tightness of the variational bound relies on gradually increasing temperature as we walk away from the data, at each step providing a gradient on the parameters to maximize the probability that the transition operator returns to its previous state. Interestingly, this algorithm admits a variant where there is no explicit energy function, i.e., the parameters are used to directly define the transition operator. This also eliminates the explicit need for symmetric weights which previous Boltzmann machine or Hopfield net models require, and which makes these models less biologically plausible.", "pdf": "/pdf/94ee3487a640bddefae7254b8764734bce178f40.pdf", "TL;DR": "A new algorithm for training undirected graphical models.", "paperhash": "goyal|the_variational_walkback_algorithm", "keywords": ["Unsupervised Learning"], "conflicts": ["umontreal.ca", "amazon.com"], "authors": ["Anirudh Goyal", "Nan Rosemary Ke", "Alex Lamb", "Yoshua Bengio"], "authorids": ["anirudhgoyal9119@gmail.com", "rosemary.nan.ke@gmail.com", "lambalex@iro.umontreal.ca", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396519710, "id": "ICLR.cc/2017/conference/-/paper328/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rkpdnIqlx", "replyto": "rkpdnIqlx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396519710}}}, {"tddate": null, "tmdate": 1483169180300, "tcdate": 1483169180300, "number": 3, "id": "SJ4ia04Sl", "invitation": "ICLR.cc/2017/conference/-/paper328/public/comment", "forum": "rkpdnIqlx", "replyto": "BJTgCsIEg", "signatures": ["~Yoshua_Bengio1"], "readers": ["everyone"], "writers": ["~Yoshua_Bengio1"], "content": {"title": "Please check added demonstration to justify the approach", "comment": "An appendix was added which clarifies the conditions under which the approximation of the true posterior by the variational estimator Q becomes tight (basically hinging on a slow annealing). However, keep in mind that even though Q does not match the true posterior, like for any other variational method, we are optimizing a valid bound on the log-likelihood."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Walkback Algorithm", "abstract": "A recognized obstacle to training undirected graphical models with latent variables such as Boltzmann machines is that the maximum likelihood training procedure requires sampling from Monte-Carlo Markov chains which may not mix well, in the inner loop of training, for each example.  We first propose the idea that it is sufficient to locally carve the energy function everywhere so that its gradient points in the \"right\" direction (i.e., towards generating the data). Following on previous work on contrastive divergence, denoising autoencoders, generative stochastic networks and unsupervised learning using non-equilibrium dynamics, we propose a variational bound on the marginal log-likelihood of the data which corresponds to a new learning procedure that first walks away from data points by following the model transition operator and then trains that operator to walk backwards for each of these steps, back towards the training example. The tightness of the variational bound relies on gradually increasing temperature as we walk away from the data, at each step providing a gradient on the parameters to maximize the probability that the transition operator returns to its previous state. Interestingly, this algorithm admits a variant where there is no explicit energy function, i.e., the parameters are used to directly define the transition operator. This also eliminates the explicit need for symmetric weights which previous Boltzmann machine or Hopfield net models require, and which makes these models less biologically plausible.", "pdf": "/pdf/94ee3487a640bddefae7254b8764734bce178f40.pdf", "TL;DR": "A new algorithm for training undirected graphical models.", "paperhash": "goyal|the_variational_walkback_algorithm", "keywords": ["Unsupervised Learning"], "conflicts": ["umontreal.ca", "amazon.com"], "authors": ["Anirudh Goyal", "Nan Rosemary Ke", "Alex Lamb", "Yoshua Bengio"], "authorids": ["anirudhgoyal9119@gmail.com", "rosemary.nan.ke@gmail.com", "lambalex@iro.umontreal.ca", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287618959, "id": "ICLR.cc/2017/conference/-/paper328/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkpdnIqlx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper328/reviewers", "ICLR.cc/2017/conference/paper328/areachairs"], "cdate": 1485287618959}}}, {"tddate": null, "tmdate": 1482239476954, "tcdate": 1482239476954, "number": 3, "id": "BJTgCsIEg", "invitation": "ICLR.cc/2017/conference/-/paper328/official/review", "forum": "rkpdnIqlx", "replyto": "rkpdnIqlx", "signatures": ["ICLR.cc/2017/conference/paper328/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper328/AnonReviewer4"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "The authors present a method for training probabilistic models by maximizing a stochastic variational-lower-bound-type objective. Training involves sampling and then learning a transition-based inference to \"walk back\" samples to the data. Because of its focus on transitions, it can be used to learn a raw transition operator rather than purely learning an energy-based model. The objective is intuitively appealing because of its similarity to previous successful but less principled training methods for MRFs like Contrastive Divergence.\n\nThe idea for the algorithm is appealing, and it looks like it could find a nice place in the literature. However, the submission in its current form is not yet ready for publication. Experiments are qualitative and the generated samples are not obviously indicative of a high model quality. As pointed out elsewhere, the mathematical analysis does not currently demonstrate tightness of the variational bound in the case of a learned transition operator. More evaluation using e.g. annealed importance sampling to estimate held-out likelihoods is necessary. Assuming that the analysis can be repaired, the ability to directly parametrize a transition operator, an interesting strength of this method, should be explored in further experiments and contrasted with the more standard energy-based modeling.\n\nThis looks like a promising idea, and other reviews and questions have already raised some important technical points which should help strengthen this paper for future submission.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Walkback Algorithm", "abstract": "A recognized obstacle to training undirected graphical models with latent variables such as Boltzmann machines is that the maximum likelihood training procedure requires sampling from Monte-Carlo Markov chains which may not mix well, in the inner loop of training, for each example.  We first propose the idea that it is sufficient to locally carve the energy function everywhere so that its gradient points in the \"right\" direction (i.e., towards generating the data). Following on previous work on contrastive divergence, denoising autoencoders, generative stochastic networks and unsupervised learning using non-equilibrium dynamics, we propose a variational bound on the marginal log-likelihood of the data which corresponds to a new learning procedure that first walks away from data points by following the model transition operator and then trains that operator to walk backwards for each of these steps, back towards the training example. The tightness of the variational bound relies on gradually increasing temperature as we walk away from the data, at each step providing a gradient on the parameters to maximize the probability that the transition operator returns to its previous state. Interestingly, this algorithm admits a variant where there is no explicit energy function, i.e., the parameters are used to directly define the transition operator. This also eliminates the explicit need for symmetric weights which previous Boltzmann machine or Hopfield net models require, and which makes these models less biologically plausible.", "pdf": "/pdf/94ee3487a640bddefae7254b8764734bce178f40.pdf", "TL;DR": "A new algorithm for training undirected graphical models.", "paperhash": "goyal|the_variational_walkback_algorithm", "keywords": ["Unsupervised Learning"], "conflicts": ["umontreal.ca", "amazon.com"], "authors": ["Anirudh Goyal", "Nan Rosemary Ke", "Alex Lamb", "Yoshua Bengio"], "authorids": ["anirudhgoyal9119@gmail.com", "rosemary.nan.ke@gmail.com", "lambalex@iro.umontreal.ca", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512622190, "id": "ICLR.cc/2017/conference/-/paper328/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper328/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper328/AnonReviewer3", "ICLR.cc/2017/conference/paper328/AnonReviewer1", "ICLR.cc/2017/conference/paper328/AnonReviewer4"], "reply": {"forum": "rkpdnIqlx", "replyto": "rkpdnIqlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper328/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper328/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512622190}}}, {"tddate": null, "tmdate": 1481926650397, "tcdate": 1481926650397, "number": 2, "id": "SkM-d1zVg", "invitation": "ICLR.cc/2017/conference/-/paper328/official/review", "forum": "rkpdnIqlx", "replyto": "rkpdnIqlx", "signatures": ["ICLR.cc/2017/conference/paper328/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper328/AnonReviewer1"], "content": {"title": "Like the underlying idea, not convinced by its current incarnation", "rating": "5: Marginally below acceptance threshold", "review": "I very much like the underlying idea for this paper. I wasn't convinced by the execution in its current state. My primary concern is the one I expressed in my pre-review question below, which I don't think the authors addressed. Specifically, I think the choice of q(s | s') = p(s | s') will make the forward and reverse trajectories almost pathologically mismatched to each other, and will thus make the variational bound extremely loose and high variance. \n\nThe claim about the tightness of the bound in Appendix D relies on the assumption that the transition distribution obeys detailed balance. The learned transition distribution in the paper does not obey detailed balance, and therefore the tightness claim in Appendix D does not hold. (In Section 2.1 you briefly discuss the idea of learning an energy function, rather than directly learning a transition distribution. I think this would be excellent, and in that case you could choose an MCMC transition operator that does obey detailed balance for that energy function.) I did not go through Appendix D beyond this step.\n\nThe experimental results were not visually impressive. I suspect this is primarily driven by the mismatch between generative and inference trajectories. See my concern above and in the pre-review question below.\n\nAlso, see note below for sec. 5. I suspect some terms are being dropped from the training gradient.\n\nThe paper is optimizing a variational bound on log likelihood. You should really, really, really report and compare log likelihoods against competing methods!\n\nDetailed comments below. Some of these were written based on a previous version of the paper.\nsec 1.2 - first paragraph is very difficult to follow\n\"these modes these spurious modes\" -> \"these spurious modes\"\nsec 2.1 - \"s = (v,h)\" -> \"s = {v,h}\"\nsec 2.2 - \"with an MCMC\" -> \"with an MCMC chain\"\n\"(ideally an MCMC)\" -> \"(e.g. via MCMC)\" MCMC is not ideal ... it's just often the best we can do.\nsec 3, last bullet - could make the temperature infinite for the last step, in which case the last step will sample directly from the prior, and the posterior and the prior will be exactly the same.\nsec. 4 -- Using an energy function would be great!! Especially, because many MCMC transition operators obey detailed balance, you would be far less prone to suffer from the forward/backward transition mismatch that is my primary concern about this technique.\neq. 12,13 -- What is alpha? How does it depend on the temperature. It's never specified.\nsec. 5, last paragraph in GSN section -- Note that q also depends on theta, so by not backpropagating through the full q chain you are dropping terms from the gradient.\nsec. 5, non-equilibrium thermodynamics -- Note that the noneq. paper also increases the noise variance as the distance from the data increases.\nFig. 1 -- right/left mislabeled\nFig. 2 -- label panes\nFig. 3 -- After how many walkback steps?", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Walkback Algorithm", "abstract": "A recognized obstacle to training undirected graphical models with latent variables such as Boltzmann machines is that the maximum likelihood training procedure requires sampling from Monte-Carlo Markov chains which may not mix well, in the inner loop of training, for each example.  We first propose the idea that it is sufficient to locally carve the energy function everywhere so that its gradient points in the \"right\" direction (i.e., towards generating the data). Following on previous work on contrastive divergence, denoising autoencoders, generative stochastic networks and unsupervised learning using non-equilibrium dynamics, we propose a variational bound on the marginal log-likelihood of the data which corresponds to a new learning procedure that first walks away from data points by following the model transition operator and then trains that operator to walk backwards for each of these steps, back towards the training example. The tightness of the variational bound relies on gradually increasing temperature as we walk away from the data, at each step providing a gradient on the parameters to maximize the probability that the transition operator returns to its previous state. Interestingly, this algorithm admits a variant where there is no explicit energy function, i.e., the parameters are used to directly define the transition operator. This also eliminates the explicit need for symmetric weights which previous Boltzmann machine or Hopfield net models require, and which makes these models less biologically plausible.", "pdf": "/pdf/94ee3487a640bddefae7254b8764734bce178f40.pdf", "TL;DR": "A new algorithm for training undirected graphical models.", "paperhash": "goyal|the_variational_walkback_algorithm", "keywords": ["Unsupervised Learning"], "conflicts": ["umontreal.ca", "amazon.com"], "authors": ["Anirudh Goyal", "Nan Rosemary Ke", "Alex Lamb", "Yoshua Bengio"], "authorids": ["anirudhgoyal9119@gmail.com", "rosemary.nan.ke@gmail.com", "lambalex@iro.umontreal.ca", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512622190, "id": "ICLR.cc/2017/conference/-/paper328/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper328/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper328/AnonReviewer3", "ICLR.cc/2017/conference/paper328/AnonReviewer1", "ICLR.cc/2017/conference/paper328/AnonReviewer4"], "reply": {"forum": "rkpdnIqlx", "replyto": "rkpdnIqlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper328/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper328/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512622190}}}, {"tddate": null, "tmdate": 1481854656076, "tcdate": 1481854656076, "number": 1, "id": "SJuaA6l4g", "invitation": "ICLR.cc/2017/conference/-/paper328/official/review", "forum": "rkpdnIqlx", "replyto": "rkpdnIqlx", "signatures": ["ICLR.cc/2017/conference/paper328/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper328/AnonReviewer3"], "content": {"title": "clever idea, but needs more quantitative validation and discussion of (closely) related work", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes a new kind of generative model based on an annealing process, where the transition probabilities are learned directly to maximize a variational lower bound on the log-likelihood. Overall, the idea is clever and appealing, but I think the paper needs more quantitative validation and better discussion of the relationship with prior work.\n\nIn terms of prior work, AIS and RAISE are both closely related algorithms, and share much of the mathematical structure with the proposed method. For this reason, it\u2019s not sufficient to mention them in passing in the related work section; those methods and their relationship to variational walkback need to be discussed in detail. If I understand correctly, the proposed method is essentially an extension of RAISE where the transition probabilities are learned rather than fixed based on an existing MRF. I think this is an interesting and worthwhile extension, but the relationship to existing work needs to be clarified.\n\nThe analysis of Appendix D seems incorrect. It derives a formula for the ratios of prior and posterior probabilities, but this formula only holds under the assumption of constant temperature (in which case the ratio is very large). When the temperature is varied, the analysis of Neal (2001) applies, and the answer is different. \n\nOne of the main selling points of the method is that it optimizes a variational lower bound on the log-likelihood; even more accurate estimates can be obtained using importance sampling. It ought to be easy to report log-likelihood estimates for this method, so I wonder why such estimates aren\u2019t reported. There are lots of prior results to compare against on MNIST. (In addition, a natural baseline would be RAISE, so that one can check if the ability to learn the transitions actually helps.)\n\nI think the basic idea here is a sound one, so I would be willing to raise my score if the above issues are addressed in a revised version.\n\n\nMinor comments:\n\n\u201cA recognized obstacle to training undirected graphical models\u2026 is that ML training requires sampling from MCMC chains in the inner loop of training, for each example.\u201d This seems like an unfair characterization, since the standard algorithm is PCD, which usually takes only a single step per mini-batch.\n\nSome of the methods discussed in the related work are missing citations.\n\nThe method is justified in terms of \u201ccarving the energy function in the right direction at each point\u201d, but I\u2019m not sure this is actually what\u2019s happening. Isn\u2019t the point of the method that it can optimize a lower bound on the log-likelihood, and therefore learn a globally correct allocation of probability mass?\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Walkback Algorithm", "abstract": "A recognized obstacle to training undirected graphical models with latent variables such as Boltzmann machines is that the maximum likelihood training procedure requires sampling from Monte-Carlo Markov chains which may not mix well, in the inner loop of training, for each example.  We first propose the idea that it is sufficient to locally carve the energy function everywhere so that its gradient points in the \"right\" direction (i.e., towards generating the data). Following on previous work on contrastive divergence, denoising autoencoders, generative stochastic networks and unsupervised learning using non-equilibrium dynamics, we propose a variational bound on the marginal log-likelihood of the data which corresponds to a new learning procedure that first walks away from data points by following the model transition operator and then trains that operator to walk backwards for each of these steps, back towards the training example. The tightness of the variational bound relies on gradually increasing temperature as we walk away from the data, at each step providing a gradient on the parameters to maximize the probability that the transition operator returns to its previous state. Interestingly, this algorithm admits a variant where there is no explicit energy function, i.e., the parameters are used to directly define the transition operator. This also eliminates the explicit need for symmetric weights which previous Boltzmann machine or Hopfield net models require, and which makes these models less biologically plausible.", "pdf": "/pdf/94ee3487a640bddefae7254b8764734bce178f40.pdf", "TL;DR": "A new algorithm for training undirected graphical models.", "paperhash": "goyal|the_variational_walkback_algorithm", "keywords": ["Unsupervised Learning"], "conflicts": ["umontreal.ca", "amazon.com"], "authors": ["Anirudh Goyal", "Nan Rosemary Ke", "Alex Lamb", "Yoshua Bengio"], "authorids": ["anirudhgoyal9119@gmail.com", "rosemary.nan.ke@gmail.com", "lambalex@iro.umontreal.ca", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512622190, "id": "ICLR.cc/2017/conference/-/paper328/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper328/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper328/AnonReviewer3", "ICLR.cc/2017/conference/paper328/AnonReviewer1", "ICLR.cc/2017/conference/paper328/AnonReviewer4"], "reply": {"forum": "rkpdnIqlx", "replyto": "rkpdnIqlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper328/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper328/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512622190}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1480732289366, "tcdate": 1478286453487, "number": 328, "id": "rkpdnIqlx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rkpdnIqlx", "signatures": ["~Anirudh_Goyal1"], "readers": ["everyone"], "content": {"title": "The Variational Walkback Algorithm", "abstract": "A recognized obstacle to training undirected graphical models with latent variables such as Boltzmann machines is that the maximum likelihood training procedure requires sampling from Monte-Carlo Markov chains which may not mix well, in the inner loop of training, for each example.  We first propose the idea that it is sufficient to locally carve the energy function everywhere so that its gradient points in the \"right\" direction (i.e., towards generating the data). Following on previous work on contrastive divergence, denoising autoencoders, generative stochastic networks and unsupervised learning using non-equilibrium dynamics, we propose a variational bound on the marginal log-likelihood of the data which corresponds to a new learning procedure that first walks away from data points by following the model transition operator and then trains that operator to walk backwards for each of these steps, back towards the training example. The tightness of the variational bound relies on gradually increasing temperature as we walk away from the data, at each step providing a gradient on the parameters to maximize the probability that the transition operator returns to its previous state. Interestingly, this algorithm admits a variant where there is no explicit energy function, i.e., the parameters are used to directly define the transition operator. This also eliminates the explicit need for symmetric weights which previous Boltzmann machine or Hopfield net models require, and which makes these models less biologically plausible.", "pdf": "/pdf/94ee3487a640bddefae7254b8764734bce178f40.pdf", "TL;DR": "A new algorithm for training undirected graphical models.", "paperhash": "goyal|the_variational_walkback_algorithm", "keywords": ["Unsupervised Learning"], "conflicts": ["umontreal.ca", "amazon.com"], "authors": ["Anirudh Goyal", "Nan Rosemary Ke", "Alex Lamb", "Yoshua Bengio"], "authorids": ["anirudhgoyal9119@gmail.com", "rosemary.nan.ke@gmail.com", "lambalex@iro.umontreal.ca", "yoshua.umontreal@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1480732233242, "tcdate": 1480732233237, "number": 2, "id": "S1WUAsk7g", "invitation": "ICLR.cc/2017/conference/-/paper328/public/comment", "forum": "rkpdnIqlx", "replyto": "BJbQ2o2zl", "signatures": ["~Anirudh_Goyal1"], "readers": ["everyone"], "writers": ["~Anirudh_Goyal1"], "content": {"title": "Reviewer 3", "comment": "AIS and RAISE are indeed related methods which were not mentioned in the original submission.  We updated the paper to include additions to the related work section which explain how these methods differ from variational walkback.  What we added is reposted below for clarity of discussion.  \n\nAnnealed  Importance  Sampling (AIS)  is  a  sampling  procedure.   Like  variational  walkback,  it  uses  an annealing schedule corresponding to a range of temperature from infinity to 1. It is used to estimate a partition function. Unlike Annealed Importance Sampling, variational walkback is meant to provide a good variational lower bound for training a transition operator.\n\nRAISE is a reverse AIS, as it starts from a data point and then increases the temperature.  In this way it is similar to the Q-chain in variational walkback.  The advantage of RAISE over AIS is that it yields an estimator of the log-likelihood that tends to be pessimistic rather than optimistic, which makes it better as an evaluation criteria.Like AIS, RAISE estimates the log-likelihood using a form of importance sampling,  based on a product (over the chain) of the ratios of consecutive probabilities (not conditional probabilities from the model). Variational walkback does not work with estimates of the model\u2019s unconditional probability, and instead works directly with a conditional probability defined by the transition operator. It is for this reason that variational walkback does not need to have an explicit energy function)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Walkback Algorithm", "abstract": "A recognized obstacle to training undirected graphical models with latent variables such as Boltzmann machines is that the maximum likelihood training procedure requires sampling from Monte-Carlo Markov chains which may not mix well, in the inner loop of training, for each example.  We first propose the idea that it is sufficient to locally carve the energy function everywhere so that its gradient points in the \"right\" direction (i.e., towards generating the data). Following on previous work on contrastive divergence, denoising autoencoders, generative stochastic networks and unsupervised learning using non-equilibrium dynamics, we propose a variational bound on the marginal log-likelihood of the data which corresponds to a new learning procedure that first walks away from data points by following the model transition operator and then trains that operator to walk backwards for each of these steps, back towards the training example. The tightness of the variational bound relies on gradually increasing temperature as we walk away from the data, at each step providing a gradient on the parameters to maximize the probability that the transition operator returns to its previous state. Interestingly, this algorithm admits a variant where there is no explicit energy function, i.e., the parameters are used to directly define the transition operator. This also eliminates the explicit need for symmetric weights which previous Boltzmann machine or Hopfield net models require, and which makes these models less biologically plausible.", "pdf": "/pdf/94ee3487a640bddefae7254b8764734bce178f40.pdf", "TL;DR": "A new algorithm for training undirected graphical models.", "paperhash": "goyal|the_variational_walkback_algorithm", "keywords": ["Unsupervised Learning"], "conflicts": ["umontreal.ca", "amazon.com"], "authors": ["Anirudh Goyal", "Nan Rosemary Ke", "Alex Lamb", "Yoshua Bengio"], "authorids": ["anirudhgoyal9119@gmail.com", "rosemary.nan.ke@gmail.com", "lambalex@iro.umontreal.ca", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287618959, "id": "ICLR.cc/2017/conference/-/paper328/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkpdnIqlx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper328/reviewers", "ICLR.cc/2017/conference/paper328/areachairs"], "cdate": 1485287618959}}}, {"tddate": null, "tmdate": 1480731874870, "tcdate": 1480731874865, "number": 1, "id": "HJoypj1mg", "invitation": "ICLR.cc/2017/conference/-/paper328/public/comment", "forum": "rkpdnIqlx", "replyto": "HJZW823zx", "signatures": ["~Anirudh_Goyal1"], "readers": ["everyone"], "writers": ["~Anirudh_Goyal1"], "content": {"title": "Tightness  of Variational Bound (Reviewer1)", "comment": "The tightness of the variational bound is an important issue and it was not sufficiently clearly discussed in the paper.  An appendix (D) has been added to the paper which contains a proof detailing the conditions under which the bound becomes tight.  \n\nHere is a brief summary of the argument: \n\n-The bound becomes tight when the stationary walkback distribution matches the noise prior.  \n-A temperature term which draws the parameters of the distribution towards the prior is increased over the course of training.  \n-If we run for enough steps, this causes the walkback distribution to approach the noise prior.  \n\nA limitation in our method is that we do not have a principled way to determine how many steps of walkback are needed or how much the temperature should grow with each step.  If the temperature grows too quickly, it could make training the transition operator too challenging.  If the temperature grows too slowly, we will not approach the noise prior during walkback and sample quality will be poor.  \n\nIntuitively, the longer we run the walkback chain for, the more the injected noise will dominate the effect of the transition operator.  Since the transition operator is trained to generate (by modelling the conditional likelihood on) the result from \"one step back\", we'll eventually learn a transition operator that can walk from a prior noise distribution back to the data manifold.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Walkback Algorithm", "abstract": "A recognized obstacle to training undirected graphical models with latent variables such as Boltzmann machines is that the maximum likelihood training procedure requires sampling from Monte-Carlo Markov chains which may not mix well, in the inner loop of training, for each example.  We first propose the idea that it is sufficient to locally carve the energy function everywhere so that its gradient points in the \"right\" direction (i.e., towards generating the data). Following on previous work on contrastive divergence, denoising autoencoders, generative stochastic networks and unsupervised learning using non-equilibrium dynamics, we propose a variational bound on the marginal log-likelihood of the data which corresponds to a new learning procedure that first walks away from data points by following the model transition operator and then trains that operator to walk backwards for each of these steps, back towards the training example. The tightness of the variational bound relies on gradually increasing temperature as we walk away from the data, at each step providing a gradient on the parameters to maximize the probability that the transition operator returns to its previous state. Interestingly, this algorithm admits a variant where there is no explicit energy function, i.e., the parameters are used to directly define the transition operator. This also eliminates the explicit need for symmetric weights which previous Boltzmann machine or Hopfield net models require, and which makes these models less biologically plausible.", "pdf": "/pdf/94ee3487a640bddefae7254b8764734bce178f40.pdf", "TL;DR": "A new algorithm for training undirected graphical models.", "paperhash": "goyal|the_variational_walkback_algorithm", "keywords": ["Unsupervised Learning"], "conflicts": ["umontreal.ca", "amazon.com"], "authors": ["Anirudh Goyal", "Nan Rosemary Ke", "Alex Lamb", "Yoshua Bengio"], "authorids": ["anirudhgoyal9119@gmail.com", "rosemary.nan.ke@gmail.com", "lambalex@iro.umontreal.ca", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287618959, "id": "ICLR.cc/2017/conference/-/paper328/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkpdnIqlx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper328/reviewers", "ICLR.cc/2017/conference/paper328/areachairs"], "cdate": 1485287618959}}}, {"tddate": null, "tmdate": 1480710399782, "tcdate": 1480710399778, "number": 3, "id": "HkdbYLyml", "invitation": "ICLR.cc/2017/conference/-/paper328/pre-review/question", "forum": "rkpdnIqlx", "replyto": "rkpdnIqlx", "signatures": ["ICLR.cc/2017/conference/paper328/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper328/AnonReviewer4"], "content": {"title": "Question", "question": "Similar to the other question, can any guarantees be given on the tightness of the variational bound as the number of steps goes to infinity? Cd-k is equivalent in expectation to the true likelihood gradient as k goes to infinity. The distinction between P(s|s') and P(s'|s) makes it hard to see whether the bound will be tight. I am also curious as to what likelihoods this method obtains on the data and how it compares to other learning methods like CD-k qualitatively and quantitatively."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Walkback Algorithm", "abstract": "A recognized obstacle to training undirected graphical models with latent variables such as Boltzmann machines is that the maximum likelihood training procedure requires sampling from Monte-Carlo Markov chains which may not mix well, in the inner loop of training, for each example.  We first propose the idea that it is sufficient to locally carve the energy function everywhere so that its gradient points in the \"right\" direction (i.e., towards generating the data). Following on previous work on contrastive divergence, denoising autoencoders, generative stochastic networks and unsupervised learning using non-equilibrium dynamics, we propose a variational bound on the marginal log-likelihood of the data which corresponds to a new learning procedure that first walks away from data points by following the model transition operator and then trains that operator to walk backwards for each of these steps, back towards the training example. The tightness of the variational bound relies on gradually increasing temperature as we walk away from the data, at each step providing a gradient on the parameters to maximize the probability that the transition operator returns to its previous state. Interestingly, this algorithm admits a variant where there is no explicit energy function, i.e., the parameters are used to directly define the transition operator. This also eliminates the explicit need for symmetric weights which previous Boltzmann machine or Hopfield net models require, and which makes these models less biologically plausible.", "pdf": "/pdf/94ee3487a640bddefae7254b8764734bce178f40.pdf", "TL;DR": "A new algorithm for training undirected graphical models.", "paperhash": "goyal|the_variational_walkback_algorithm", "keywords": ["Unsupervised Learning"], "conflicts": ["umontreal.ca", "amazon.com"], "authors": ["Anirudh Goyal", "Nan Rosemary Ke", "Alex Lamb", "Yoshua Bengio"], "authorids": ["anirudhgoyal9119@gmail.com", "rosemary.nan.ke@gmail.com", "lambalex@iro.umontreal.ca", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959337189, "id": "ICLR.cc/2017/conference/-/paper328/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper328/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper328/AnonReviewer3", "ICLR.cc/2017/conference/paper328/AnonReviewer1", "ICLR.cc/2017/conference/paper328/AnonReviewer4"], "reply": {"forum": "rkpdnIqlx", "replyto": "rkpdnIqlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper328/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper328/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959337189}}}, {"tddate": null, "tmdate": 1480549521523, "tcdate": 1480537593405, "number": 2, "id": "HJZW823zx", "invitation": "ICLR.cc/2017/conference/-/paper328/pre-review/question", "forum": "rkpdnIqlx", "replyto": "rkpdnIqlx", "signatures": ["ICLR.cc/2017/conference/paper328/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper328/AnonReviewer1"], "content": {"title": "matching inference and generative chains", "question": "I think the motivation of choosing the reverse process so that it closely matches the forward process is a good one. However, I have a concern about the way in which you try to achieve this.\n\nSpecifically, you set q(s | s') = p(s | s') (eq. 9). The variational bound (eq. 7) though depends on the ratio p(s | s') / q(s' | s). I don't believe the choice of q(s | s') in eq. 9 guarantees anything about the tightness of the variational bound.\n\nEven worse, the intent is that p(s | s') will collapse samples towards a data manifold. It is therefore expected that p(s' | s) will be very small when p(s | s') is large ... since if s' -> s is a highly probable collapse towards the data distribution, then s -> s' is a highly improbable movement away from the data distribution. This seems to almost guarantee that p(s | s') / q(s' | s) will be very high variance, and that the variational bound will thus be very loose.\n\nIs my interpretation of the situation valid? Is there a reason to expect that setting q(s | s') = p(s | s') will make the reverse trajectory similar to the forward trajectory?\n\n(Note that the ideal (variational bound is tight) thing to set q(s | s') to is the *reversal* of p(s' | s):\nq(s | s') = p( s' | s ) * pi_neq(s) / pi_neq(s')\nwhere pi_neq(s) and pi_neq(s') are the marginal distributions over s and s' under the p trajectory.)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Walkback Algorithm", "abstract": "A recognized obstacle to training undirected graphical models with latent variables such as Boltzmann machines is that the maximum likelihood training procedure requires sampling from Monte-Carlo Markov chains which may not mix well, in the inner loop of training, for each example.  We first propose the idea that it is sufficient to locally carve the energy function everywhere so that its gradient points in the \"right\" direction (i.e., towards generating the data). Following on previous work on contrastive divergence, denoising autoencoders, generative stochastic networks and unsupervised learning using non-equilibrium dynamics, we propose a variational bound on the marginal log-likelihood of the data which corresponds to a new learning procedure that first walks away from data points by following the model transition operator and then trains that operator to walk backwards for each of these steps, back towards the training example. The tightness of the variational bound relies on gradually increasing temperature as we walk away from the data, at each step providing a gradient on the parameters to maximize the probability that the transition operator returns to its previous state. Interestingly, this algorithm admits a variant where there is no explicit energy function, i.e., the parameters are used to directly define the transition operator. This also eliminates the explicit need for symmetric weights which previous Boltzmann machine or Hopfield net models require, and which makes these models less biologically plausible.", "pdf": "/pdf/94ee3487a640bddefae7254b8764734bce178f40.pdf", "TL;DR": "A new algorithm for training undirected graphical models.", "paperhash": "goyal|the_variational_walkback_algorithm", "keywords": ["Unsupervised Learning"], "conflicts": ["umontreal.ca", "amazon.com"], "authors": ["Anirudh Goyal", "Nan Rosemary Ke", "Alex Lamb", "Yoshua Bengio"], "authorids": ["anirudhgoyal9119@gmail.com", "rosemary.nan.ke@gmail.com", "lambalex@iro.umontreal.ca", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959337189, "id": "ICLR.cc/2017/conference/-/paper328/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper328/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper328/AnonReviewer3", "ICLR.cc/2017/conference/paper328/AnonReviewer1", "ICLR.cc/2017/conference/paper328/AnonReviewer4"], "reply": {"forum": "rkpdnIqlx", "replyto": "rkpdnIqlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper328/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper328/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959337189}}}, {"tddate": null, "tmdate": 1480535064751, "tcdate": 1480535064747, "number": 1, "id": "BJbQ2o2zl", "invitation": "ICLR.cc/2017/conference/-/paper328/pre-review/question", "forum": "rkpdnIqlx", "replyto": "rkpdnIqlx", "signatures": ["ICLR.cc/2017/conference/paper328/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper328/AnonReviewer3"], "content": {"title": "relationship with prior work?", "question": "Can you clarify the relationship with AIS (Neal, 2001) and the RAISE estimator (Burda et al., 2015)?  Is the proposed lower bound equivalent to RAISE?\n\nWhat log-likelihoods does this method achieve on MNIST, CIFAR, etc.?  This seems important since one of the main contributions is a way of estimating log-likelihood.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Variational Walkback Algorithm", "abstract": "A recognized obstacle to training undirected graphical models with latent variables such as Boltzmann machines is that the maximum likelihood training procedure requires sampling from Monte-Carlo Markov chains which may not mix well, in the inner loop of training, for each example.  We first propose the idea that it is sufficient to locally carve the energy function everywhere so that its gradient points in the \"right\" direction (i.e., towards generating the data). Following on previous work on contrastive divergence, denoising autoencoders, generative stochastic networks and unsupervised learning using non-equilibrium dynamics, we propose a variational bound on the marginal log-likelihood of the data which corresponds to a new learning procedure that first walks away from data points by following the model transition operator and then trains that operator to walk backwards for each of these steps, back towards the training example. The tightness of the variational bound relies on gradually increasing temperature as we walk away from the data, at each step providing a gradient on the parameters to maximize the probability that the transition operator returns to its previous state. Interestingly, this algorithm admits a variant where there is no explicit energy function, i.e., the parameters are used to directly define the transition operator. This also eliminates the explicit need for symmetric weights which previous Boltzmann machine or Hopfield net models require, and which makes these models less biologically plausible.", "pdf": "/pdf/94ee3487a640bddefae7254b8764734bce178f40.pdf", "TL;DR": "A new algorithm for training undirected graphical models.", "paperhash": "goyal|the_variational_walkback_algorithm", "keywords": ["Unsupervised Learning"], "conflicts": ["umontreal.ca", "amazon.com"], "authors": ["Anirudh Goyal", "Nan Rosemary Ke", "Alex Lamb", "Yoshua Bengio"], "authorids": ["anirudhgoyal9119@gmail.com", "rosemary.nan.ke@gmail.com", "lambalex@iro.umontreal.ca", "yoshua.umontreal@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959337189, "id": "ICLR.cc/2017/conference/-/paper328/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper328/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper328/AnonReviewer3", "ICLR.cc/2017/conference/paper328/AnonReviewer1", "ICLR.cc/2017/conference/paper328/AnonReviewer4"], "reply": {"forum": "rkpdnIqlx", "replyto": "rkpdnIqlx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper328/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper328/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959337189}}}], "count": 11}