{"notes": [{"id": "HyeYTgrFPB", "original": "rylEDbZFDr", "number": 2582, "cdate": 1569439937321, "ddate": null, "tcdate": 1569439937321, "tmdate": 1583912046994, "tddate": null, "forum": "HyeYTgrFPB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["berendg@inf.u-szeged.hu"], "title": "Massively Multilingual Sparse Word Representations", "authors": ["G\u00e1bor Berend"], "pdf": "/pdf/9b5d1b517dc6dbea93317053cc40738e3920a6bc.pdf", "TL;DR": "We propose an efficient algorithm for determining multilingually comparable sparse word representations that we release for 27 typologically diverse languages.", "abstract": "In this paper, we introduce Mamus for constructing multilingual sparse word representations. Our algorithm operates by determining a shared set of semantic units which get reutilized across languages, providing it a competitive edge both in terms of speed and evaluation performance. We demonstrate that our proposed algorithm behaves competitively to strong baselines through a series of rigorous experiments performed towards downstream applications spanning over dependency parsing, document classification and natural language inference. Additionally, our experiments relying on the QVEC-CCA evaluation score suggests that the proposed sparse word representations convey an increased interpretability as opposed to alternative approaches. Finally, we are releasing our multilingual sparse word representations for the 27 typologically diverse set of languages that we conducted our various experiments on.", "code": "https://github.com/begab/mamus", "keywords": ["sparse word representations", "multilinguality", "sparse coding"], "paperhash": "berend|massively_multilingual_sparse_word_representations", "_bibtex": "@inproceedings{\nBerend2020Massively,\ntitle={Massively Multilingual Sparse Word Representations},\nauthor={G\u00e1bor Berend},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeYTgrFPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/be8cf4458a7d44474ac09bf8b1563103319c9f1c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "fPjrfR2Q1_", "original": null, "number": 1, "cdate": 1576798752646, "ddate": null, "tcdate": 1576798752646, "tmdate": 1576800882929, "tddate": null, "forum": "HyeYTgrFPB", "replyto": "HyeYTgrFPB", "invitation": "ICLR.cc/2020/Conference/Paper2582/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper describes a new method for creating word embeddings that can operate on corpora from more than one language.  The algorithm is simple, but rivals more complex approaches.  \n\nThe reviewers were happy with this paper.  They were also impressed that the authors ran the requested multi-lingual BERT experiments, even though they did not show positive results. One reviewer did think that non-contextual word embeddings were of less interest to the NLP community, but thought your arguments for the computational efficiency were convincing.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["berendg@inf.u-szeged.hu"], "title": "Massively Multilingual Sparse Word Representations", "authors": ["G\u00e1bor Berend"], "pdf": "/pdf/9b5d1b517dc6dbea93317053cc40738e3920a6bc.pdf", "TL;DR": "We propose an efficient algorithm for determining multilingually comparable sparse word representations that we release for 27 typologically diverse languages.", "abstract": "In this paper, we introduce Mamus for constructing multilingual sparse word representations. Our algorithm operates by determining a shared set of semantic units which get reutilized across languages, providing it a competitive edge both in terms of speed and evaluation performance. We demonstrate that our proposed algorithm behaves competitively to strong baselines through a series of rigorous experiments performed towards downstream applications spanning over dependency parsing, document classification and natural language inference. Additionally, our experiments relying on the QVEC-CCA evaluation score suggests that the proposed sparse word representations convey an increased interpretability as opposed to alternative approaches. Finally, we are releasing our multilingual sparse word representations for the 27 typologically diverse set of languages that we conducted our various experiments on.", "code": "https://github.com/begab/mamus", "keywords": ["sparse word representations", "multilinguality", "sparse coding"], "paperhash": "berend|massively_multilingual_sparse_word_representations", "_bibtex": "@inproceedings{\nBerend2020Massively,\ntitle={Massively Multilingual Sparse Word Representations},\nauthor={G\u00e1bor Berend},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeYTgrFPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/be8cf4458a7d44474ac09bf8b1563103319c9f1c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HyeYTgrFPB", "replyto": "HyeYTgrFPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795719487, "tmdate": 1576800270146, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2582/-/Decision"}}}, {"id": "S1eGTLLnjB", "original": null, "number": 7, "cdate": 1573836474160, "ddate": null, "tcdate": 1573836474160, "tmdate": 1573836474160, "tddate": null, "forum": "HyeYTgrFPB", "replyto": "HJlcVa2doH", "invitation": "ICLR.cc/2020/Conference/Paper2582/-/Official_Comment", "content": {"title": "Thanks!", "comment": "Thanks very much for adding these two experiments: for adding multilingual BERT to the paper and discussing the relation to BiSparse in the comments above (and presumably later adding it to the paper). I think the conclusion that you actually outperform BiSparse really helps strengthen the paper, and I greatly  appreciate your honesty and straightforwardness with the multilingual BERT result. \n\nYou have definitely addressed all of my major questions."}, "signatures": ["ICLR.cc/2020/Conference/Paper2582/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2582/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["berendg@inf.u-szeged.hu"], "title": "Massively Multilingual Sparse Word Representations", "authors": ["G\u00e1bor Berend"], "pdf": "/pdf/9b5d1b517dc6dbea93317053cc40738e3920a6bc.pdf", "TL;DR": "We propose an efficient algorithm for determining multilingually comparable sparse word representations that we release for 27 typologically diverse languages.", "abstract": "In this paper, we introduce Mamus for constructing multilingual sparse word representations. Our algorithm operates by determining a shared set of semantic units which get reutilized across languages, providing it a competitive edge both in terms of speed and evaluation performance. We demonstrate that our proposed algorithm behaves competitively to strong baselines through a series of rigorous experiments performed towards downstream applications spanning over dependency parsing, document classification and natural language inference. Additionally, our experiments relying on the QVEC-CCA evaluation score suggests that the proposed sparse word representations convey an increased interpretability as opposed to alternative approaches. Finally, we are releasing our multilingual sparse word representations for the 27 typologically diverse set of languages that we conducted our various experiments on.", "code": "https://github.com/begab/mamus", "keywords": ["sparse word representations", "multilinguality", "sparse coding"], "paperhash": "berend|massively_multilingual_sparse_word_representations", "_bibtex": "@inproceedings{\nBerend2020Massively,\ntitle={Massively Multilingual Sparse Word Representations},\nauthor={G\u00e1bor Berend},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeYTgrFPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/be8cf4458a7d44474ac09bf8b1563103319c9f1c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeYTgrFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2582/Authors", "ICLR.cc/2020/Conference/Paper2582/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2582/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2582/Reviewers", "ICLR.cc/2020/Conference/Paper2582/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2582/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2582/Authors|ICLR.cc/2020/Conference/Paper2582/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139149, "tmdate": 1576860545652, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2582/Authors", "ICLR.cc/2020/Conference/Paper2582/Reviewers", "ICLR.cc/2020/Conference/Paper2582/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2582/-/Official_Comment"}}}, {"id": "SJeylM2cjS", "original": null, "number": 6, "cdate": 1573728742756, "ddate": null, "tcdate": 1573728742756, "tmdate": 1573728742756, "tddate": null, "forum": "HyeYTgrFPB", "replyto": "SklpIWaPqS", "invitation": "ICLR.cc/2020/Conference/Paper2582/-/Official_Comment", "content": {"title": "Comment related to comparison to BiSparse", "comment": "As we noted in our general answer, we have experienced that using the same hyperparameters for regularization in the case of BiSparse as for MultiSparse resulted in subpar results in terms of the level of sparsity.\n\nAfter increasing the monolingual hyperparameters from $\\lambda_s=\\lambda_t=2$ to $\\lambda_s=\\lambda_t=5$, we managed to obtain representations relying on BiSparse that behave comparably to MultiSparse as well.\nWe created BiSparse representations for the language pairs English--Italian and English--Danish, obtaining approximately 3% of the coefficients becoming nonzero by the end of the optimization.\n\nEvaluating the BiSparse representation by QVEC-CCA, we obtained the scores of 0.602, 0.789/0.786, 0.585 for Danish, English and Italian, respectively.\nWe now have two scores for English this time (0.789/0.786) since BiSparse created separate representations for English when jointly trained with Danish and Italian as well.\nMultiSparse on the other hand obtained QVEC-CCA scores as follows: 0.612, 0.808 and 0.596 for the same languages.\n\nThis means that MultiSparse performs comparably (or even better) to BiSparse based on their QVEC-CCA scores. We think that this could potentially be explained by the fact that BiSparse tries to optimize a more difficult objective -- as it involves the joint optimization of two non-convex problems -- hence finding a good solution is more difficult in that case."}, "signatures": ["ICLR.cc/2020/Conference/Paper2582/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2582/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["berendg@inf.u-szeged.hu"], "title": "Massively Multilingual Sparse Word Representations", "authors": ["G\u00e1bor Berend"], "pdf": "/pdf/9b5d1b517dc6dbea93317053cc40738e3920a6bc.pdf", "TL;DR": "We propose an efficient algorithm for determining multilingually comparable sparse word representations that we release for 27 typologically diverse languages.", "abstract": "In this paper, we introduce Mamus for constructing multilingual sparse word representations. Our algorithm operates by determining a shared set of semantic units which get reutilized across languages, providing it a competitive edge both in terms of speed and evaluation performance. We demonstrate that our proposed algorithm behaves competitively to strong baselines through a series of rigorous experiments performed towards downstream applications spanning over dependency parsing, document classification and natural language inference. Additionally, our experiments relying on the QVEC-CCA evaluation score suggests that the proposed sparse word representations convey an increased interpretability as opposed to alternative approaches. Finally, we are releasing our multilingual sparse word representations for the 27 typologically diverse set of languages that we conducted our various experiments on.", "code": "https://github.com/begab/mamus", "keywords": ["sparse word representations", "multilinguality", "sparse coding"], "paperhash": "berend|massively_multilingual_sparse_word_representations", "_bibtex": "@inproceedings{\nBerend2020Massively,\ntitle={Massively Multilingual Sparse Word Representations},\nauthor={G\u00e1bor Berend},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeYTgrFPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/be8cf4458a7d44474ac09bf8b1563103319c9f1c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeYTgrFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2582/Authors", "ICLR.cc/2020/Conference/Paper2582/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2582/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2582/Reviewers", "ICLR.cc/2020/Conference/Paper2582/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2582/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2582/Authors|ICLR.cc/2020/Conference/Paper2582/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139149, "tmdate": 1576860545652, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2582/Authors", "ICLR.cc/2020/Conference/Paper2582/Reviewers", "ICLR.cc/2020/Conference/Paper2582/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2582/-/Official_Comment"}}}, {"id": "HJlcVa2doH", "original": null, "number": 3, "cdate": 1573600562407, "ddate": null, "tcdate": 1573600562407, "tmdate": 1573604620879, "tddate": null, "forum": "HyeYTgrFPB", "replyto": "SklpIWaPqS", "invitation": "ICLR.cc/2020/Conference/Paper2582/-/Official_Comment", "content": {"title": "Answers to Review #4", "comment": "We would like to thank the reviewer for the insightful review and the useful suggestions for improving the quality of the paper.\n\nWe are planning to report evaluation on QVEC-CCA when using BiSparse instead of MultiSparse. There are two difficulties we have encountered regarding this experiment.\nAs BiSparse involves the optimization of nearly twice as much parameters as MultiSparse (since it learns representations for the source and target languages at the same time), training BiSparse representations for a single pair of languages takes more than 50 hours.\nFurthermore, as it turned out, using the same hyperparameters in the BiSparse setting could result in substantially different results.\nFor instance, in the case of Italian embeddings, the BiSparse representations contained more than 25 times as many nonzero coefficients as opposed to the representations obtained by MultiSparse, 1100+ and 44.3, respectively.\nThis sensitivity of BiSparse for the choice of the regularization hyperparameters, together with its slow running time makes the conduction of the proposed experiment rather cumbersome, given that we would like to compare BiSparse representations of similar sparsity level to the previously calculated MutliSparse representations.\n\nWe found the comment on contextual word embeddings a very inspiring one. Hence we conducted further XNLI experiments when relying on multilingual BERT embeddings. We revised the paper with the additional comparisons of multilingual BERT and Mamus. In short, the relative performance of the models that use Mamus instead of multilingual BERT is above 90%. For more details, please refer to the revised version of the paper.\n\nFinally, the revised paper also addresses the further points raised in the review."}, "signatures": ["ICLR.cc/2020/Conference/Paper2582/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2582/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["berendg@inf.u-szeged.hu"], "title": "Massively Multilingual Sparse Word Representations", "authors": ["G\u00e1bor Berend"], "pdf": "/pdf/9b5d1b517dc6dbea93317053cc40738e3920a6bc.pdf", "TL;DR": "We propose an efficient algorithm for determining multilingually comparable sparse word representations that we release for 27 typologically diverse languages.", "abstract": "In this paper, we introduce Mamus for constructing multilingual sparse word representations. Our algorithm operates by determining a shared set of semantic units which get reutilized across languages, providing it a competitive edge both in terms of speed and evaluation performance. We demonstrate that our proposed algorithm behaves competitively to strong baselines through a series of rigorous experiments performed towards downstream applications spanning over dependency parsing, document classification and natural language inference. Additionally, our experiments relying on the QVEC-CCA evaluation score suggests that the proposed sparse word representations convey an increased interpretability as opposed to alternative approaches. Finally, we are releasing our multilingual sparse word representations for the 27 typologically diverse set of languages that we conducted our various experiments on.", "code": "https://github.com/begab/mamus", "keywords": ["sparse word representations", "multilinguality", "sparse coding"], "paperhash": "berend|massively_multilingual_sparse_word_representations", "_bibtex": "@inproceedings{\nBerend2020Massively,\ntitle={Massively Multilingual Sparse Word Representations},\nauthor={G\u00e1bor Berend},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeYTgrFPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/be8cf4458a7d44474ac09bf8b1563103319c9f1c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeYTgrFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2582/Authors", "ICLR.cc/2020/Conference/Paper2582/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2582/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2582/Reviewers", "ICLR.cc/2020/Conference/Paper2582/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2582/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2582/Authors|ICLR.cc/2020/Conference/Paper2582/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139149, "tmdate": 1576860545652, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2582/Authors", "ICLR.cc/2020/Conference/Paper2582/Reviewers", "ICLR.cc/2020/Conference/Paper2582/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2582/-/Official_Comment"}}}, {"id": "B1l1Gwp_ir", "original": null, "number": 5, "cdate": 1573603079450, "ddate": null, "tcdate": 1573603079450, "tmdate": 1573603079450, "tddate": null, "forum": "HyeYTgrFPB", "replyto": "rJe0V3HRKH", "invitation": "ICLR.cc/2020/Conference/Paper2582/-/Official_Comment", "content": {"title": "Answers to Reviewer #2", "comment": "We would like to thank the reviewer for the feedbacks provided.\n\nWe agree with the review that the investigation of the actual interpretability of the word representations besides their QVEC-CCA scores could provide additional useful insights.\nConducting such experiments (e.g. performing Word intrusion detection) was currently beyond the scope of the paper, however, marks an interesting future path.\n\nBased on the suggestions on how to make the paper more easily accessible, we submitted an updated version of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2582/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2582/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["berendg@inf.u-szeged.hu"], "title": "Massively Multilingual Sparse Word Representations", "authors": ["G\u00e1bor Berend"], "pdf": "/pdf/9b5d1b517dc6dbea93317053cc40738e3920a6bc.pdf", "TL;DR": "We propose an efficient algorithm for determining multilingually comparable sparse word representations that we release for 27 typologically diverse languages.", "abstract": "In this paper, we introduce Mamus for constructing multilingual sparse word representations. Our algorithm operates by determining a shared set of semantic units which get reutilized across languages, providing it a competitive edge both in terms of speed and evaluation performance. We demonstrate that our proposed algorithm behaves competitively to strong baselines through a series of rigorous experiments performed towards downstream applications spanning over dependency parsing, document classification and natural language inference. Additionally, our experiments relying on the QVEC-CCA evaluation score suggests that the proposed sparse word representations convey an increased interpretability as opposed to alternative approaches. Finally, we are releasing our multilingual sparse word representations for the 27 typologically diverse set of languages that we conducted our various experiments on.", "code": "https://github.com/begab/mamus", "keywords": ["sparse word representations", "multilinguality", "sparse coding"], "paperhash": "berend|massively_multilingual_sparse_word_representations", "_bibtex": "@inproceedings{\nBerend2020Massively,\ntitle={Massively Multilingual Sparse Word Representations},\nauthor={G\u00e1bor Berend},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeYTgrFPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/be8cf4458a7d44474ac09bf8b1563103319c9f1c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeYTgrFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2582/Authors", "ICLR.cc/2020/Conference/Paper2582/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2582/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2582/Reviewers", "ICLR.cc/2020/Conference/Paper2582/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2582/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2582/Authors|ICLR.cc/2020/Conference/Paper2582/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139149, "tmdate": 1576860545652, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2582/Authors", "ICLR.cc/2020/Conference/Paper2582/Reviewers", "ICLR.cc/2020/Conference/Paper2582/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2582/-/Official_Comment"}}}, {"id": "Byx08zpuiS", "original": null, "number": 4, "cdate": 1573601877731, "ddate": null, "tcdate": 1573601877731, "tmdate": 1573601877731, "tddate": null, "forum": "HyeYTgrFPB", "replyto": "B1lY96o1cH", "invitation": "ICLR.cc/2020/Conference/Paper2582/-/Official_Comment", "content": {"title": "Answers to Review #3", "comment": "We would like to thank the reviewer for the general interest in our work and the constructive feedback provided.\n\nWe regard the stability regarding the number of the nonzero coefficients per word forms characterizing MAMUS a useful property, because it means that one can reliably anticipate the level of sparsity that would be induced by a certain choice of the regularization hyperparameter \\lambda. Inspecting the average number of nonzero coefficients per word forms in the case of MultiSparse (or BiSparse), we found much higher fluctuations in the sparsity levels obtained for the same choice of hyperparameters.\n\nInspired by the question related to the potential usage of multilingual contextual representations, we conducted further XNLI experiments when relying on multilingual BERT representations as inputs. The relative performance of the models trained over MAMUS representations is above 90% to those that are built on top of multilingual BERT embeddings. This suggests that MAMUS representations can serve as a viable alternative to the application of the computationally more demanding contextualized representations.\nFor further details, please  refer to the revised version of the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2582/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2582/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["berendg@inf.u-szeged.hu"], "title": "Massively Multilingual Sparse Word Representations", "authors": ["G\u00e1bor Berend"], "pdf": "/pdf/9b5d1b517dc6dbea93317053cc40738e3920a6bc.pdf", "TL;DR": "We propose an efficient algorithm for determining multilingually comparable sparse word representations that we release for 27 typologically diverse languages.", "abstract": "In this paper, we introduce Mamus for constructing multilingual sparse word representations. Our algorithm operates by determining a shared set of semantic units which get reutilized across languages, providing it a competitive edge both in terms of speed and evaluation performance. We demonstrate that our proposed algorithm behaves competitively to strong baselines through a series of rigorous experiments performed towards downstream applications spanning over dependency parsing, document classification and natural language inference. Additionally, our experiments relying on the QVEC-CCA evaluation score suggests that the proposed sparse word representations convey an increased interpretability as opposed to alternative approaches. Finally, we are releasing our multilingual sparse word representations for the 27 typologically diverse set of languages that we conducted our various experiments on.", "code": "https://github.com/begab/mamus", "keywords": ["sparse word representations", "multilinguality", "sparse coding"], "paperhash": "berend|massively_multilingual_sparse_word_representations", "_bibtex": "@inproceedings{\nBerend2020Massively,\ntitle={Massively Multilingual Sparse Word Representations},\nauthor={G\u00e1bor Berend},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeYTgrFPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/be8cf4458a7d44474ac09bf8b1563103319c9f1c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HyeYTgrFPB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2582/Authors", "ICLR.cc/2020/Conference/Paper2582/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2582/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2582/Reviewers", "ICLR.cc/2020/Conference/Paper2582/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2582/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2582/Authors|ICLR.cc/2020/Conference/Paper2582/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139149, "tmdate": 1576860545652, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2582/Authors", "ICLR.cc/2020/Conference/Paper2582/Reviewers", "ICLR.cc/2020/Conference/Paper2582/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2582/-/Official_Comment"}}}, {"id": "rJe0V3HRKH", "original": null, "number": 1, "cdate": 1571867701519, "ddate": null, "tcdate": 1571867701519, "tmdate": 1572972319416, "tddate": null, "forum": "HyeYTgrFPB", "replyto": "HyeYTgrFPB", "invitation": "ICLR.cc/2020/Conference/Paper2582/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a method to generate sparse multilingual embeddings. The key idea is to build only one set of source basis embeddings, and then represent all multilingual embeddings as a linear combination of these source embeddings. I felt the paper is a nice extension of the Vyas 2016 paper. Compared to existing approaches, their method will be faster to train and will need less data (particularly useful for low resource languages). Since I am less aware of work in this area, I cannot comment on whether the evaluation is complete. Particularly, I wonder if there is a qualitative way to show interpretability of the sparse vectors created by the method. We currently only have QVEC-CCA numbers to judge interpretability.  A few more suggestions:\n\n1. I got confused by the sentence \".. over a reduced number of parameters for each target language as it treats D_s as D_t .\". Things got clear from the equations, but will be good to fix.\n\n2. Figure 1 was very difficult to understand. Ideally your caption should be enough to understand the figure. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2582/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2582/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["berendg@inf.u-szeged.hu"], "title": "Massively Multilingual Sparse Word Representations", "authors": ["G\u00e1bor Berend"], "pdf": "/pdf/9b5d1b517dc6dbea93317053cc40738e3920a6bc.pdf", "TL;DR": "We propose an efficient algorithm for determining multilingually comparable sparse word representations that we release for 27 typologically diverse languages.", "abstract": "In this paper, we introduce Mamus for constructing multilingual sparse word representations. Our algorithm operates by determining a shared set of semantic units which get reutilized across languages, providing it a competitive edge both in terms of speed and evaluation performance. We demonstrate that our proposed algorithm behaves competitively to strong baselines through a series of rigorous experiments performed towards downstream applications spanning over dependency parsing, document classification and natural language inference. Additionally, our experiments relying on the QVEC-CCA evaluation score suggests that the proposed sparse word representations convey an increased interpretability as opposed to alternative approaches. Finally, we are releasing our multilingual sparse word representations for the 27 typologically diverse set of languages that we conducted our various experiments on.", "code": "https://github.com/begab/mamus", "keywords": ["sparse word representations", "multilinguality", "sparse coding"], "paperhash": "berend|massively_multilingual_sparse_word_representations", "_bibtex": "@inproceedings{\nBerend2020Massively,\ntitle={Massively Multilingual Sparse Word Representations},\nauthor={G\u00e1bor Berend},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeYTgrFPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/be8cf4458a7d44474ac09bf8b1563103319c9f1c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyeYTgrFPB", "replyto": "HyeYTgrFPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2582/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2582/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575607395050, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2582/Reviewers"], "noninvitees": [], "tcdate": 1570237720783, "tmdate": 1575607395061, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2582/-/Official_Review"}}}, {"id": "B1lY96o1cH", "original": null, "number": 2, "cdate": 1571958161509, "ddate": null, "tcdate": 1571958161509, "tmdate": 1572972319372, "tddate": null, "forum": "HyeYTgrFPB", "replyto": "HyeYTgrFPB", "invitation": "ICLR.cc/2020/Conference/Paper2582/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new approach for generating multilingual sparse representations. \nFor generating such representations, the proposed approach solves a series of convex optimization problems, serially for each language. Compared to previous work for generating sparse cross-lingual representations which is applicable to a pair of languages, the proposed approach is applicable to an arbitrary number of languages.\n\nThe paper argues that these sparse representations can lead to better performance for downstream tasks and interpretability. This is demonstrated using experiments on QVEC-CCA (for interpretability analysis), NLI, cross-lingual document classification, and dependency parsing (downstream tasks).\n\nOverall, the approach is well-motivated and performs well empirically. The experimental setup is also described in detail. \n\nMinor - I did not understand the benefit of MAMUS being \"stable\" across languages, as argued from Fig 1? The use of the word \"cognitively\" in the statement \"representations determined by MAMUS behave in a cognitively more plausible manner\" also seems a stretch to me.\n \nOne issue that the authors should discuss is whether such representations hold any extra value over contextual representations like multilingual Elmo, BERT etc. For instance, why would someone use MAMUS representations instead?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2582/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2582/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["berendg@inf.u-szeged.hu"], "title": "Massively Multilingual Sparse Word Representations", "authors": ["G\u00e1bor Berend"], "pdf": "/pdf/9b5d1b517dc6dbea93317053cc40738e3920a6bc.pdf", "TL;DR": "We propose an efficient algorithm for determining multilingually comparable sparse word representations that we release for 27 typologically diverse languages.", "abstract": "In this paper, we introduce Mamus for constructing multilingual sparse word representations. Our algorithm operates by determining a shared set of semantic units which get reutilized across languages, providing it a competitive edge both in terms of speed and evaluation performance. We demonstrate that our proposed algorithm behaves competitively to strong baselines through a series of rigorous experiments performed towards downstream applications spanning over dependency parsing, document classification and natural language inference. Additionally, our experiments relying on the QVEC-CCA evaluation score suggests that the proposed sparse word representations convey an increased interpretability as opposed to alternative approaches. Finally, we are releasing our multilingual sparse word representations for the 27 typologically diverse set of languages that we conducted our various experiments on.", "code": "https://github.com/begab/mamus", "keywords": ["sparse word representations", "multilinguality", "sparse coding"], "paperhash": "berend|massively_multilingual_sparse_word_representations", "_bibtex": "@inproceedings{\nBerend2020Massively,\ntitle={Massively Multilingual Sparse Word Representations},\nauthor={G\u00e1bor Berend},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeYTgrFPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/be8cf4458a7d44474ac09bf8b1563103319c9f1c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyeYTgrFPB", "replyto": "HyeYTgrFPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2582/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2582/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575607395050, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2582/Reviewers"], "noninvitees": [], "tcdate": 1570237720783, "tmdate": 1575607395061, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2582/-/Official_Review"}}}, {"id": "SklpIWaPqS", "original": null, "number": 3, "cdate": 1572487509395, "ddate": null, "tcdate": 1572487509395, "tmdate": 1572972319328, "tddate": null, "forum": "HyeYTgrFPB", "replyto": "HyeYTgrFPB", "invitation": "ICLR.cc/2020/Conference/Paper2582/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper describes a method to build sparse multilingual word vectors that is designed to scale easily to many languages. The key idea is to pick one language to be the source language, and then to build word embeddings and then a sparse dictionary + sparse coefficients  for that source language monolingually. Other target languages then first align their embedding to the source using a seed list of translations and standard techniques, and then determine their sparse coefficients based on the fixed source sparse dictionary. This latter process is a convex optimization, which improves efficiency and stability. The method is tested with an established correlation-based intrinsic metric (QVEC-CCA) as well as by using the multilingual embeddings to project systems for cross-lingual document classification, dependency parsing and natural language inference.\n\nThe core idea of this paper is substantially simpler than the method it compares to (BiSparse), which jointly optimizes dictionaries, coefficients and cross-lingual coefficient alignment. So, the question that immediately comes to mind for me is, how much accuracy am I giving up for improved scalability to multiple languages? This could be easily tested for the two language case, where BiSparse could be directly compared without modification to their proposed method. Instead, BiSparse is modified to fit scale to the multilingual setting (becoming MultiSparse), but since it shares the constraint that the source dictionary and coefficients are fixed, it has already lost a lot of the power of joint optimization. I think the paper would be stronger with a two-language experiment where we would expect the proposed method to lose to BiSparse, but we could begin to understand what has been given up for scalability.\n\nI also wonder how relevant bilingual word embeddings are in a world of multilingual BERT and similar approaches. It would be interesting to know how cross-lingual embedding-in-context methods would do on the extrinsic evaluations in this paper, though I also acknowledge that this could be considered beyond the scope of the paper.\n\nOtherwise, this is a fine paper. It is well written and easy to follow. The experiments look sane, and the inclusion of both intrinsic and extrinsic tasks makes them fairly convincing. I have only a few remaining nitpicks:\n\n(1) As someone relatively unfamiliar with multilingual (as opposed to bilingual) word embedding research, it wasn\u2019t clear to me how the experiments described here tested the multilinguality (as opposed to bilinguality) of the embeddings. It would be nice to provide an explanation for why (beyond the obvious efficiency gains) one couldn\u2019t just do the necessary language pairs with bilingual methods for these experiments. And if one could perform the tests with bilingual methods, they should be included as baselines. \n\n(2) The discussion of MultiSpare hyper-parameter tuning appears in the Monolingual Experiments section, leading me to wonder what target languages were used for this tuning process.\n\n(3) In the second-last paragraph of 3.2.2, there is a sentence fragment that ends in \u201cnonetheless their multiCluster and multiCCA embedding spaces contain no embeddings for\u201d\n\n(4) The last paragraph before the Conclusion also feels like a fragment, or like two sentences have been spliced together: \u201cWe have detailed the differences to Upadhyay et al. (2018) extends the previous work by incorporating dependency relations into sparse coding.\u201d\n\nThere are also several places where periods seem to have been left out (such as immediately before the above sentence)."}, "signatures": ["ICLR.cc/2020/Conference/Paper2582/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2582/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["berendg@inf.u-szeged.hu"], "title": "Massively Multilingual Sparse Word Representations", "authors": ["G\u00e1bor Berend"], "pdf": "/pdf/9b5d1b517dc6dbea93317053cc40738e3920a6bc.pdf", "TL;DR": "We propose an efficient algorithm for determining multilingually comparable sparse word representations that we release for 27 typologically diverse languages.", "abstract": "In this paper, we introduce Mamus for constructing multilingual sparse word representations. Our algorithm operates by determining a shared set of semantic units which get reutilized across languages, providing it a competitive edge both in terms of speed and evaluation performance. We demonstrate that our proposed algorithm behaves competitively to strong baselines through a series of rigorous experiments performed towards downstream applications spanning over dependency parsing, document classification and natural language inference. Additionally, our experiments relying on the QVEC-CCA evaluation score suggests that the proposed sparse word representations convey an increased interpretability as opposed to alternative approaches. Finally, we are releasing our multilingual sparse word representations for the 27 typologically diverse set of languages that we conducted our various experiments on.", "code": "https://github.com/begab/mamus", "keywords": ["sparse word representations", "multilinguality", "sparse coding"], "paperhash": "berend|massively_multilingual_sparse_word_representations", "_bibtex": "@inproceedings{\nBerend2020Massively,\ntitle={Massively Multilingual Sparse Word Representations},\nauthor={G\u00e1bor Berend},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HyeYTgrFPB}\n}", "full_presentation_video": "", "original_pdf": "/attachment/be8cf4458a7d44474ac09bf8b1563103319c9f1c.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HyeYTgrFPB", "replyto": "HyeYTgrFPB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2582/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2582/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575607395050, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2582/Reviewers"], "noninvitees": [], "tcdate": 1570237720783, "tmdate": 1575607395061, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2582/-/Official_Review"}}}], "count": 10}