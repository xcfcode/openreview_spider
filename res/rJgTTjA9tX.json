{"notes": [{"id": "rJgTTjA9tX", "original": "r1ejl6AKYQ", "number": 845, "cdate": 1538087876930, "ddate": null, "tcdate": 1538087876930, "tmdate": 1551721905196, "tddate": null, "forum": "rJgTTjA9tX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "The Comparative Power of ReLU Networks and Polynomial Kernels in the Presence of Sparse Latent Structure", "abstract": "There has been a large amount of interest, both in the past and particularly recently, into the relative advantage of different families of universal function approximators, for instance neural networks, polynomials, rational functions, etc. However, current research has focused almost exclusively on understanding this problem in a worst case setting: e.g. characterizing the best L1 or L_{infty} approximation in a box (or sometimes, even under an adversarially constructed data distribution.) In this setting many classical tools from approximation theory can be effectively used.\n\nHowever, in typical applications we expect data to be high dimensional, but structured -- so, it would only be important to approximate the desired function well on the relevant part of its domain, e.g. a small manifold on which real input data actually lies. Moreover, even within this domain the desired quality of approximation may not be uniform; for instance in classification problems, the approximation needs to be more accurate near the decision boundary. These issues, to the best of our knowledge, have remain unexplored until now.\n\t\nWith this in mind, we analyze the performance of neural networks and polynomial kernels in a natural regression setting where the data enjoys sparse latent structure, and the labels depend in a simple way on the latent variables. We give an almost-tight theoretical analysis of the performance of both neural networks and polynomials for this problem, as well as verify our theory with simulations. Our results both involve new (complex-analytic) techniques, which may be of independent interest, and show substantial qualitative differences with what is known in the worst-case setting.", "keywords": ["theory", "representational power", "universal approximators", "polynomial kernels", "latent sparsity", "beyond worst case", "separation result"], "authorids": ["fkoehler@mit.edu", "risteski@mit.edu"], "authors": ["Frederic Koehler", "Andrej Risteski"], "TL;DR": "Beyond-worst-case analysis of the representational power of  ReLU nets & polynomial kernels  -- in particular in the presence of sparse latent structure.", "pdf": "/pdf/edcdc50db0ad342e6f270889bfa8fe869a1cc1b7.pdf", "paperhash": "koehler|the_comparative_power_of_relu_networks_and_polynomial_kernels_in_the_presence_of_sparse_latent_structure", "_bibtex": "@inproceedings{\nkoehler2018the,\ntitle={The Comparative Power of Re{LU} Networks and Polynomial Kernels in the Presence of Sparse Latent Structure},\nauthor={Frederic Koehler and Andrej Risteski},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTTjA9tX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Bkg1sk6Ng4", "original": null, "number": 1, "cdate": 1545027479111, "ddate": null, "tcdate": 1545027479111, "tmdate": 1545354473171, "tddate": null, "forum": "rJgTTjA9tX", "replyto": "rJgTTjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper845/Meta_Review", "content": {"metareview": "This paper makes a substantial contribution to the understanding of the approximation ability of deep networks in comparison to classical approximation classes, such as polynomials.  Strong results are given that show fundamental advantages for neural network function approximators in the presence of a natural form of latent structure.  The analysis techniques required to achieve these results are novel and worth reporting to the community.  The reviewers are uniformly supportive.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Interesting new analysis of function approximation in the presence of sparse latent structure"}, "signatures": ["ICLR.cc/2019/Conference/Paper845/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper845/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Comparative Power of ReLU Networks and Polynomial Kernels in the Presence of Sparse Latent Structure", "abstract": "There has been a large amount of interest, both in the past and particularly recently, into the relative advantage of different families of universal function approximators, for instance neural networks, polynomials, rational functions, etc. However, current research has focused almost exclusively on understanding this problem in a worst case setting: e.g. characterizing the best L1 or L_{infty} approximation in a box (or sometimes, even under an adversarially constructed data distribution.) In this setting many classical tools from approximation theory can be effectively used.\n\nHowever, in typical applications we expect data to be high dimensional, but structured -- so, it would only be important to approximate the desired function well on the relevant part of its domain, e.g. a small manifold on which real input data actually lies. Moreover, even within this domain the desired quality of approximation may not be uniform; for instance in classification problems, the approximation needs to be more accurate near the decision boundary. These issues, to the best of our knowledge, have remain unexplored until now.\n\t\nWith this in mind, we analyze the performance of neural networks and polynomial kernels in a natural regression setting where the data enjoys sparse latent structure, and the labels depend in a simple way on the latent variables. We give an almost-tight theoretical analysis of the performance of both neural networks and polynomials for this problem, as well as verify our theory with simulations. Our results both involve new (complex-analytic) techniques, which may be of independent interest, and show substantial qualitative differences with what is known in the worst-case setting.", "keywords": ["theory", "representational power", "universal approximators", "polynomial kernels", "latent sparsity", "beyond worst case", "separation result"], "authorids": ["fkoehler@mit.edu", "risteski@mit.edu"], "authors": ["Frederic Koehler", "Andrej Risteski"], "TL;DR": "Beyond-worst-case analysis of the representational power of  ReLU nets & polynomial kernels  -- in particular in the presence of sparse latent structure.", "pdf": "/pdf/edcdc50db0ad342e6f270889bfa8fe869a1cc1b7.pdf", "paperhash": "koehler|the_comparative_power_of_relu_networks_and_polynomial_kernels_in_the_presence_of_sparse_latent_structure", "_bibtex": "@inproceedings{\nkoehler2018the,\ntitle={The Comparative Power of Re{LU} Networks and Polynomial Kernels in the Presence of Sparse Latent Structure},\nauthor={Frederic Koehler and Andrej Risteski},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTTjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper845/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353063725, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgTTjA9tX", "replyto": "rJgTTjA9tX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper845/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper845/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper845/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353063725}}}, {"id": "Hkl1W-vLRm", "original": null, "number": 1, "cdate": 1543037174964, "ddate": null, "tcdate": 1543037174964, "tmdate": 1543037174964, "tddate": null, "forum": "rJgTTjA9tX", "replyto": "rJgTTjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper845/Official_Comment", "content": {"title": "Response to Reviewers ", "comment": "We thank the reviewers for their valuable feedback, which we have incorporated into the new revision of the paper. In particular, in response to AnonReviewer2, we added a discussion of a related work by Zhang et al. on kernel methods simulating neural networks and have added more details to both the proofs and proof sketches.\n\nAnonReviewer2 and AnonReviewer1 asked about more complex models: we agree that sparse regression is a relatively simple model and that it would be nice to study more complex models as well. Since latent sparsity is a common feature in many models, this seemed like the natural place to start -- we hope the analysis of more sophisticated models will follow. \n\nAnonReviewer2 also asked about tightness of the dependence on \\mu: for noisy sparse linear regression, the polynomial dependence on \\mu cannot be significantly improved due to issues of computational complexity. For instance, https://arxiv.org/pdf/1402.1918.pdf show that sparse linear regression with a statistical rate better than polynomial in \\mu is computationally hard.  If, for example, small-degree polynomials existed (with dependency better than polynomial in \\mu), these computational hardness results would be violated. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper845/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper845/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper845/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Comparative Power of ReLU Networks and Polynomial Kernels in the Presence of Sparse Latent Structure", "abstract": "There has been a large amount of interest, both in the past and particularly recently, into the relative advantage of different families of universal function approximators, for instance neural networks, polynomials, rational functions, etc. However, current research has focused almost exclusively on understanding this problem in a worst case setting: e.g. characterizing the best L1 or L_{infty} approximation in a box (or sometimes, even under an adversarially constructed data distribution.) In this setting many classical tools from approximation theory can be effectively used.\n\nHowever, in typical applications we expect data to be high dimensional, but structured -- so, it would only be important to approximate the desired function well on the relevant part of its domain, e.g. a small manifold on which real input data actually lies. Moreover, even within this domain the desired quality of approximation may not be uniform; for instance in classification problems, the approximation needs to be more accurate near the decision boundary. These issues, to the best of our knowledge, have remain unexplored until now.\n\t\nWith this in mind, we analyze the performance of neural networks and polynomial kernels in a natural regression setting where the data enjoys sparse latent structure, and the labels depend in a simple way on the latent variables. We give an almost-tight theoretical analysis of the performance of both neural networks and polynomials for this problem, as well as verify our theory with simulations. Our results both involve new (complex-analytic) techniques, which may be of independent interest, and show substantial qualitative differences with what is known in the worst-case setting.", "keywords": ["theory", "representational power", "universal approximators", "polynomial kernels", "latent sparsity", "beyond worst case", "separation result"], "authorids": ["fkoehler@mit.edu", "risteski@mit.edu"], "authors": ["Frederic Koehler", "Andrej Risteski"], "TL;DR": "Beyond-worst-case analysis of the representational power of  ReLU nets & polynomial kernels  -- in particular in the presence of sparse latent structure.", "pdf": "/pdf/edcdc50db0ad342e6f270889bfa8fe869a1cc1b7.pdf", "paperhash": "koehler|the_comparative_power_of_relu_networks_and_polynomial_kernels_in_the_presence_of_sparse_latent_structure", "_bibtex": "@inproceedings{\nkoehler2018the,\ntitle={The Comparative Power of Re{LU} Networks and Polynomial Kernels in the Presence of Sparse Latent Structure},\nauthor={Frederic Koehler and Andrej Risteski},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTTjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper845/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614346, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgTTjA9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper845/Authors", "ICLR.cc/2019/Conference/Paper845/Reviewers", "ICLR.cc/2019/Conference/Paper845/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper845/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper845/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper845/Authors|ICLR.cc/2019/Conference/Paper845/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper845/Reviewers", "ICLR.cc/2019/Conference/Paper845/Authors", "ICLR.cc/2019/Conference/Paper845/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614346}}}, {"id": "Sylr0yhhhX", "original": null, "number": 3, "cdate": 1541353420918, "ddate": null, "tcdate": 1541353420918, "tmdate": 1541533641673, "tddate": null, "forum": "rJgTTjA9tX", "replyto": "rJgTTjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper845/Official_Review", "content": {"title": "Review", "review": "This paper studies the problem of understanding the representation power of neural nets with Relu activations for representing structured data. In order to formalize this, the authors consider data generated from a sparse generative model as follows: A sparse m-dimensional vector Z is sampled from a distribution over sparse vectors. In input X is formed \nas AZ, where A is an incoherent matrix. The corresponding output is Y= w. X. The goal is to fit the data of the form (X_i, Y_i). The main result of the paper is that a 2-layer ReLU network can fit the data with near optimal error. On the other hand, low degree polynomials~(of degree up to log m) cannot fit the data with non-trivial error. Finally,\nthe authors also show that polynomials of degree polylog(m) can, in fact, fit the data as well as a 2-layer ReLU network. The paper is well written and provides new insights into the representation power of neural nets. It is also nice to know that ReLU networks can be approximated by low degree polynomials in the non-worst case scenario. This\nis a good paper and I recommend acceptance.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper845/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Comparative Power of ReLU Networks and Polynomial Kernels in the Presence of Sparse Latent Structure", "abstract": "There has been a large amount of interest, both in the past and particularly recently, into the relative advantage of different families of universal function approximators, for instance neural networks, polynomials, rational functions, etc. However, current research has focused almost exclusively on understanding this problem in a worst case setting: e.g. characterizing the best L1 or L_{infty} approximation in a box (or sometimes, even under an adversarially constructed data distribution.) In this setting many classical tools from approximation theory can be effectively used.\n\nHowever, in typical applications we expect data to be high dimensional, but structured -- so, it would only be important to approximate the desired function well on the relevant part of its domain, e.g. a small manifold on which real input data actually lies. Moreover, even within this domain the desired quality of approximation may not be uniform; for instance in classification problems, the approximation needs to be more accurate near the decision boundary. These issues, to the best of our knowledge, have remain unexplored until now.\n\t\nWith this in mind, we analyze the performance of neural networks and polynomial kernels in a natural regression setting where the data enjoys sparse latent structure, and the labels depend in a simple way on the latent variables. We give an almost-tight theoretical analysis of the performance of both neural networks and polynomials for this problem, as well as verify our theory with simulations. Our results both involve new (complex-analytic) techniques, which may be of independent interest, and show substantial qualitative differences with what is known in the worst-case setting.", "keywords": ["theory", "representational power", "universal approximators", "polynomial kernels", "latent sparsity", "beyond worst case", "separation result"], "authorids": ["fkoehler@mit.edu", "risteski@mit.edu"], "authors": ["Frederic Koehler", "Andrej Risteski"], "TL;DR": "Beyond-worst-case analysis of the representational power of  ReLU nets & polynomial kernels  -- in particular in the presence of sparse latent structure.", "pdf": "/pdf/edcdc50db0ad342e6f270889bfa8fe869a1cc1b7.pdf", "paperhash": "koehler|the_comparative_power_of_relu_networks_and_polynomial_kernels_in_the_presence_of_sparse_latent_structure", "_bibtex": "@inproceedings{\nkoehler2018the,\ntitle={The Comparative Power of Re{LU} Networks and Polynomial Kernels in the Presence of Sparse Latent Structure},\nauthor={Frederic Koehler and Andrej Risteski},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTTjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper845/Official_Review", "cdate": 1542234363745, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJgTTjA9tX", "replyto": "rJgTTjA9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper845/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335815570, "tmdate": 1552335815570, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper845/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyeekjH9h7", "original": null, "number": 2, "cdate": 1541196504376, "ddate": null, "tcdate": 1541196504376, "tmdate": 1541533641466, "tddate": null, "forum": "rJgTTjA9tX", "replyto": "rJgTTjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper845/Official_Review", "content": {"title": "Interesting \"Relevant domain\" based approximation of ReLU to approximate sparse latent structures", "review": "The paper studies the representational power of two-layer ReLU networks and polynomials for approximating a linear generative model for data with sparsity in the latent vector. They show that ReLU networks achieve optimal rate whereas low degree polynomials get a much worse rate.\n\nOverall, the results are strong, the authors provide a lower bound on the degree of polynomial needed to approximate the model indicating the power of non-linearity. The observation of moving away from uniform approximators is well-motivated. The approximation theorem for ReLU is intriguing and uses new ideas which I have not seen before and are potentially useful in other applications. So far, only rational functions have been able to give such approximation guarantees. However, the motivation for studying sparse linear regression from a representation view-point is not very clear. Ideally, you would like to study representation for more complex models. \n\nQuestions/Comments:\n- Related work is missing prior work at the intersection of kernel methods and neural networks, please update.\n- Define notation before using, for example, \\rho_\\tau^{\u2a02m}\n- Expand proof sketches, they are not very clear, also full proofs are written with not much detail.\n- Is the dependence on \\mu tight? The current dependence sort of suggests that you need the observation matrix to be very close to identity.\n- Proof of Lemma B.1 is unclear, could you explain how you deduce the lemma from the inequality?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper845/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Comparative Power of ReLU Networks and Polynomial Kernels in the Presence of Sparse Latent Structure", "abstract": "There has been a large amount of interest, both in the past and particularly recently, into the relative advantage of different families of universal function approximators, for instance neural networks, polynomials, rational functions, etc. However, current research has focused almost exclusively on understanding this problem in a worst case setting: e.g. characterizing the best L1 or L_{infty} approximation in a box (or sometimes, even under an adversarially constructed data distribution.) In this setting many classical tools from approximation theory can be effectively used.\n\nHowever, in typical applications we expect data to be high dimensional, but structured -- so, it would only be important to approximate the desired function well on the relevant part of its domain, e.g. a small manifold on which real input data actually lies. Moreover, even within this domain the desired quality of approximation may not be uniform; for instance in classification problems, the approximation needs to be more accurate near the decision boundary. These issues, to the best of our knowledge, have remain unexplored until now.\n\t\nWith this in mind, we analyze the performance of neural networks and polynomial kernels in a natural regression setting where the data enjoys sparse latent structure, and the labels depend in a simple way on the latent variables. We give an almost-tight theoretical analysis of the performance of both neural networks and polynomials for this problem, as well as verify our theory with simulations. Our results both involve new (complex-analytic) techniques, which may be of independent interest, and show substantial qualitative differences with what is known in the worst-case setting.", "keywords": ["theory", "representational power", "universal approximators", "polynomial kernels", "latent sparsity", "beyond worst case", "separation result"], "authorids": ["fkoehler@mit.edu", "risteski@mit.edu"], "authors": ["Frederic Koehler", "Andrej Risteski"], "TL;DR": "Beyond-worst-case analysis of the representational power of  ReLU nets & polynomial kernels  -- in particular in the presence of sparse latent structure.", "pdf": "/pdf/edcdc50db0ad342e6f270889bfa8fe869a1cc1b7.pdf", "paperhash": "koehler|the_comparative_power_of_relu_networks_and_polynomial_kernels_in_the_presence_of_sparse_latent_structure", "_bibtex": "@inproceedings{\nkoehler2018the,\ntitle={The Comparative Power of Re{LU} Networks and Polynomial Kernels in the Presence of Sparse Latent Structure},\nauthor={Frederic Koehler and Andrej Risteski},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTTjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper845/Official_Review", "cdate": 1542234363745, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJgTTjA9tX", "replyto": "rJgTTjA9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper845/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335815570, "tmdate": 1552335815570, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper845/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ByeUreMZhX", "original": null, "number": 1, "cdate": 1540591678062, "ddate": null, "tcdate": 1540591678062, "tmdate": 1541533641260, "tddate": null, "forum": "rJgTTjA9tX", "replyto": "rJgTTjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper845/Official_Review", "content": {"title": "The Comparative Power of ReLU Networks and Polynomial Kernels in the Presence of Sparse Latent Structure", "review": "In this paper, authors analyze the performance of neural networks and polynomial kernels in a natural regression setting where the data enjoys sparse latent structure, and the labels depend in a simple way on the latent variables. They give an almost-tight theoretical analysis of the performance and verify them with simulations.\n \nAuthors motivated the theoretical analysis from typical applications, for which the desired function can be only important to be approximated well on the relevant part of domains. Instead of formalizing the above problem, authors tackle a particular simple question. However, it is not easy to understand the relationships between the two problems.\n \nA regression task is studied where the data has a sparser latent structure. Authors measure the performance of estimators via the expected reconstruction error from theoretical perspectives for both two-layer ReLU network and polynomial kernel. Empirical experiments will be even better to show the performance of some applications consistent with the theoretical results.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper845/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Comparative Power of ReLU Networks and Polynomial Kernels in the Presence of Sparse Latent Structure", "abstract": "There has been a large amount of interest, both in the past and particularly recently, into the relative advantage of different families of universal function approximators, for instance neural networks, polynomials, rational functions, etc. However, current research has focused almost exclusively on understanding this problem in a worst case setting: e.g. characterizing the best L1 or L_{infty} approximation in a box (or sometimes, even under an adversarially constructed data distribution.) In this setting many classical tools from approximation theory can be effectively used.\n\nHowever, in typical applications we expect data to be high dimensional, but structured -- so, it would only be important to approximate the desired function well on the relevant part of its domain, e.g. a small manifold on which real input data actually lies. Moreover, even within this domain the desired quality of approximation may not be uniform; for instance in classification problems, the approximation needs to be more accurate near the decision boundary. These issues, to the best of our knowledge, have remain unexplored until now.\n\t\nWith this in mind, we analyze the performance of neural networks and polynomial kernels in a natural regression setting where the data enjoys sparse latent structure, and the labels depend in a simple way on the latent variables. We give an almost-tight theoretical analysis of the performance of both neural networks and polynomials for this problem, as well as verify our theory with simulations. Our results both involve new (complex-analytic) techniques, which may be of independent interest, and show substantial qualitative differences with what is known in the worst-case setting.", "keywords": ["theory", "representational power", "universal approximators", "polynomial kernels", "latent sparsity", "beyond worst case", "separation result"], "authorids": ["fkoehler@mit.edu", "risteski@mit.edu"], "authors": ["Frederic Koehler", "Andrej Risteski"], "TL;DR": "Beyond-worst-case analysis of the representational power of  ReLU nets & polynomial kernels  -- in particular in the presence of sparse latent structure.", "pdf": "/pdf/edcdc50db0ad342e6f270889bfa8fe869a1cc1b7.pdf", "paperhash": "koehler|the_comparative_power_of_relu_networks_and_polynomial_kernels_in_the_presence_of_sparse_latent_structure", "_bibtex": "@inproceedings{\nkoehler2018the,\ntitle={The Comparative Power of Re{LU} Networks and Polynomial Kernels in the Presence of Sparse Latent Structure},\nauthor={Frederic Koehler and Andrej Risteski},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTTjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper845/Official_Review", "cdate": 1542234363745, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJgTTjA9tX", "replyto": "rJgTTjA9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper845/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335815570, "tmdate": 1552335815570, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper845/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 6}