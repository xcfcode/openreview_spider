{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1368188160000, "tcdate": 1368188160000, "number": 1, "id": "3Ms_MCOhFG34r", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "idpCdOWtqXd60", "replyto": "idpCdOWtqXd60", "signatures": ["Pontus Stenetorp"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "In response to the request for references made by the first author for the statement regarding semantic similarity being intransitive, I think the reference should be to 'Features of similarity' by Tversky (1977). Please find what I believe to be the relevant portion below.\r\n\r\n    `We say 'the portrait resembles the person' rather than 'the person resembles the portrait.' We say 'the son resembles the father' rather than 'the father resembles the son.' We say 'an ellipse is like a circle,' not 'a circle is like an ellipse,' and we say 'North Korea is like Red China' rather than 'Red China is like North Korea.''\r\n\r\nLastly, a question that was raised by the reviewers was whether these relationships also hold for LSA or tf-idf vectors to which the first author responded that this has already been discussed in another paper and it turned out not to be the case. I would be very thankful for a reference to this work since I am not familiar with it."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Estimation of Word Representations in Vector Space", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day and one CPU to derive high quality 300-dimensional vectors for one million vocabulary from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring various types of word similarities. We intend to publish this test set to be used by the research community.", "pdf": "https://arxiv.org/abs/1301.3781", "paperhash": "mikolov|efficient_estimation_of_word_representations_in_vector_space", "keywords": [], "conflicts": [], "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "authorids": ["tmikolov@google.com", "kaichen@google.com", "gcorrado@google.com", "jeff@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363602720000, "tcdate": 1363602720000, "number": 6, "id": "6NMO6i-9pXN8q", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "idpCdOWtqXd60", "replyto": "idpCdOWtqXd60", "signatures": ["anonymous reviewer 3c5e"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "It is really unfortunate that the responding author seems to care\r\nsolely about every possible tweak to his model and combinations of his\r\nmodels but shows a strong disregard for a proper scientific comparison\r\nthat would show what's really the underlying reason for the increase\r\nin accuracy on (again) his own new task. For all we know, some of the\r\nword vectors and models that are being compared to in table 4 may have\r\nbeen trained on datasets that didn't even include the terms used in\r\nthe evaluation, or they may have been very rare in that corpus.\r\nThe models compared in table 4 still all have different word vector\r\nsizes and are trained on different datasets, despite the clear\r\nimportance of word vector size and dataset size. Maybe the\r\nhierarchical softmax on any of the existing models trained on the same\r\ndataset would yield the same performance? There's no way of knowing if\r\nthis paper introduced a new model that works better or just a new\r\ntraining dataset (which won't be published) or just a well selected\r\ncombination of existing methods.\r\n\r\nThe authors write that there are many obvious real tasks that their\r\nword vectors should help but don't show or mention any. NER has been\r\nused to compare word vectors and there are standard datasets out there\r\nfor a comparison on which many people train and test. There are human\r\nsimilarity judgments tasks and datasets that several word vectors have\r\nbeen compared on. Again, the author seems to prefer to ignore all but\r\nhis own models, dataset and tasks. It is still not clear to me what\r\npart of the model gives the performance increase. Is it the top layer\r\ntask or is it the averaging of word vectors. Again, averaging word\r\nvectors has already been done as part of the model of Huang et al.. A\r\nlink to a wikipedia article by the author is not as strong as an\r\nargument as showing equations that point to the actual difference.\r\n\r\nAfter a discussion among the reviewers, we unanimously feel that the revised version of paper and the accompanying rebuttal do not resolve many of the issues raised by the reviewers, and many of the reviewers' questions (e.g., on which models include nonlinearities) remain unanswered.\r\nFor instance, they say that the projection layer in a NNLM has no\r\nnonlinearity but that was not the point, the next layer has one and\r\nfrom the fuzzy definitions it seems like the proposed model does not.\r\nDoes that mean we could just get rid of the non-linearity of the\r\nvector averaging part of Huang's model and get the same performance?\r\nLDA might be in fashion now but papers in high quality conferences are\r\nsupposed to be understood in the future as well when some models may\r\nnot be so obviously known anymore.\r\n\r\nThe figure is much less clear in describing the model than the\r\nequations all three reviewers asked for.\r\n\r\nAgain, there is one interesting bit in here which is the new\r\nevaluation metric (which may or may not be introduced in reference\r\n[14] soon) and the fact that any of these models capture these\r\nrelationships linearly. Unfortunately, the entire comparison to\r\nprevious work (table 4 and the writing) is unscientific and sloppy.\r\nFurthermore, the possibly new models are not clearly enough defined by\r\ntheir equations.\r\n\r\nIt is generally unclear where the improvements are coming from.\r\nWe hope the authors will clean up the writing and include proper\r\ncomparisons for a future submission."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Estimation of Word Representations in Vector Space", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day and one CPU to derive high quality 300-dimensional vectors for one million vocabulary from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring various types of word similarities. We intend to publish this test set to be used by the research community.", "pdf": "https://arxiv.org/abs/1301.3781", "paperhash": "mikolov|efficient_estimation_of_word_representations_in_vector_space", "keywords": [], "conflicts": [], "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "authorids": ["tmikolov@google.com", "kaichen@google.com", "gcorrado@google.com", "jeff@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363350360000, "tcdate": 1363350360000, "number": 11, "id": "OOksUbLar_UGE", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "idpCdOWtqXd60", "replyto": "idpCdOWtqXd60", "signatures": ["anonymous reviewer 13e8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "In light of the authors' response I'm changing my score for the paper to Weak Reject."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Estimation of Word Representations in Vector Space", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day and one CPU to derive high quality 300-dimensional vectors for one million vocabulary from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring various types of word similarities. We intend to publish this test set to be used by the research community.", "pdf": "https://arxiv.org/abs/1301.3781", "paperhash": "mikolov|efficient_estimation_of_word_representations_in_vector_space", "keywords": [], "conflicts": [], "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "authorids": ["tmikolov@google.com", "kaichen@google.com", "gcorrado@google.com", "jeff@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363326840000, "tcdate": 1363326840000, "number": 4, "id": "sJxHJpdSKIJNL", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "idpCdOWtqXd60", "replyto": "idpCdOWtqXd60", "signatures": ["anonymous reviewer f5bf"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The revision and rebuttal failed to address the issues raised by the reviewers. I do not think the paper should be accepted in its current form.\r\n\r\nQuality rating: Strong reject \r\nConfidence: Reviewer is knowledgeable"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Estimation of Word Representations in Vector Space", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day and one CPU to derive high quality 300-dimensional vectors for one million vocabulary from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring various types of word similarities. We intend to publish this test set to be used by the research community.", "pdf": "https://arxiv.org/abs/1301.3781", "paperhash": "mikolov|efficient_estimation_of_word_representations_in_vector_space", "keywords": [], "conflicts": [], "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "authorids": ["tmikolov@google.com", "kaichen@google.com", "gcorrado@google.com", "jeff@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363279380000, "tcdate": 1363279380000, "number": 10, "id": "qX8Cq3hI2EXpf", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "idpCdOWtqXd60", "replyto": "idpCdOWtqXd60", "signatures": ["anonymous reviewer f5bf"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The revision and rebuttal failed to address the issues raised by the reviewers. I do not think the paper should be accepted in its current form.\r\n\r\nQuality rating: Strong reject \r\nConfidence: Reviewer is knowledgeable"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Estimation of Word Representations in Vector Space", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day and one CPU to derive high quality 300-dimensional vectors for one million vocabulary from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring various types of word similarities. We intend to publish this test set to be used by the research community.", "pdf": "https://arxiv.org/abs/1301.3781", "paperhash": "mikolov|efficient_estimation_of_word_representations_in_vector_space", "keywords": [], "conflicts": [], "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "authorids": ["tmikolov@google.com", "kaichen@google.com", "gcorrado@google.com", "jeff@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363279380000, "tcdate": 1363279380000, "number": 7, "id": "mmlAm0ZawBraS", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "idpCdOWtqXd60", "replyto": "idpCdOWtqXd60", "signatures": ["anonymous reviewer f5bf"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The revision and rebuttal failed to address the issues raised by the reviewers. I do not think the paper should be accepted in its current form.\r\n\r\nQuality rating: Strong reject \r\nConfidence: Reviewer is knowledgeable"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Estimation of Word Representations in Vector Space", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day and one CPU to derive high quality 300-dimensional vectors for one million vocabulary from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring various types of word similarities. We intend to publish this test set to be used by the research community.", "pdf": "https://arxiv.org/abs/1301.3781", "paperhash": "mikolov|efficient_estimation_of_word_representations_in_vector_space", "keywords": [], "conflicts": [], "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "authorids": ["tmikolov@google.com", "kaichen@google.com", "gcorrado@google.com", "jeff@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363279380000, "tcdate": 1363279380000, "number": 5, "id": "ddu0ScgIDPSxi", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "idpCdOWtqXd60", "replyto": "idpCdOWtqXd60", "signatures": ["anonymous reviewer f5bf"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The revision and rebuttal failed to address the issues raised by the reviewers. I do not think the paper should be accepted in its current form.\r\n\r\nQuality rating: Strong reject \r\nConfidence: Reviewer is knowledgeable"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Estimation of Word Representations in Vector Space", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day and one CPU to derive high quality 300-dimensional vectors for one million vocabulary from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring various types of word similarities. We intend to publish this test set to be used by the research community.", "pdf": "https://arxiv.org/abs/1301.3781", "paperhash": "mikolov|efficient_estimation_of_word_representations_in_vector_space", "keywords": [], "conflicts": [], "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "authorids": ["tmikolov@google.com", "kaichen@google.com", "gcorrado@google.com", "jeff@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362716940000, "tcdate": 1362716940000, "number": 2, "id": "C8Vn84fqSG8qa", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "idpCdOWtqXd60", "replyto": "idpCdOWtqXd60", "signatures": ["Tomas Mikolov"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We have updated the paper (new version will be visible on Monday):\r\n\r\n- added new results with comparison of models trained on the same data with the same dimensionality of the word vectors\r\n\r\n- additional comparison on a task that was used previously for comparison of word vectors\r\n\r\n- added citations, more discussion about the prior work\r\n\r\n- new results with parallel training of the models on many machines\r\n\r\n- new state of the art result on Microsoft Research Sentence Completion Challenge, using combination of RNNLMs and Skip-gram\r\n\r\n- published the test set\r\n\r\n\r\nWe welcome discussion about the paper. The main contribution (that seems to have been missed by some of the reviews) is that we can use very shallow models to compute good vector representation of words. This can be very efficient, compared to currently popular model architectures.\r\n\r\nAs we are very interested in the deep learning, we are also interested in how this term is being used. Unfortunately, there is an increasing amount of work that tries to associate itself with deep learning, although it has nothing to do with it. According to Bengio+LeCun's paper 'Scaling learning algorithms towards AI', deep architectures should be capable of representing and learning complex functions, composed of simpler functions. The complex functions at the same time cannot be efficiently represented and learned by shallow architectures (basically those that have only 1 or 0 non-linearities). Thus, any paper that claims to be about 'deep learning' should first prove that the given performance cannot be achieved with a shallow model. This has been already shown for deep neural networks for speech recognition and vision problems (one hidden layer is not enough to reach the same performance that more hidden layers can achieve). However, when it comes to NLP, the only such result known to me are the Recurrent neural networks, that have been shown to outperform shallow feed-forward networks on some tasks in language modeling.\r\n\r\nWhen it comes to learning continuous representations of words, such thorough comparison is missing. In our current paper, we actually show that there might be nothing deep about the continuous word vectors - one cannot simply add few hidden layers and label some technique 'deep' to gain attraction. Correct comparison with shallow techniques is necessary.\r\n\r\nHopefully, our paper will improve common understanding of what deep learning is about, and will help to keep the track towards the original goals. We did not write our opinion directly in the paper, as we believe it belongs more to the discussion part of the conference, where people can react to our claims.\r\n\r\n\r\nDetailed responses are below:\r\n\r\nReviewer Anonymous 13e8:\r\n\r\nThe log-linear language models proposed are not quite as novel or uniquely scalable as the paper seems to imply though. Models of this type were introduced in [R1] and further developed in [15] and [R2].\r\n\r\n- We have added the citations and some discussion; however, note that we directly follow model architecture proposed earlier, in 'T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno University of Technology, 2007.', plus the hierarchical softmax proposed in 'F. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS, 2005.'; the novelty of our current approach is in the new architectures that work significantly better than the previous ones (we have added this comparison in the new version of the paper), and the Huffman tree based hierarchical softmax.\r\n\r\n\r\nFor example, the training complexity of the log-linear HLBL model from [15] is the same as that of the Continuous Bag-of-Words models\r\n\r\n- Assuming one will use diagonal weight matrices as is mentioned in [15], the computational complexity will be similar. We have added this information to the paper. Our proposed architectures are however easier to implementation than HLBL, and also it does not seem that we would obtain better vectors with HLBL (just by looking at the table with results - HLBL seems to have performance close to NNLM, ie. does not capture the semantic regularities in words as well as the Skip-gram). Moreover, I was confused about computational complexity of the hierarchical log-bilinear model: in [R2], it is reported that training time for model with 100 hidden units on the Penn Treebank setup is 1.5 hours; for our CBOW model it is a few seconds. So I don't know if author uses the diagonal weight matrices always or not.\r\n\r\nAdditionally, the perplexity results in [R2] are rather weak, even worse than simple trigram model. My explanation of HLBL performance is this: the model does not have non-linearities, thus, it cannot model N-grams. An example of such feature is 'if word X and Y occurred after each other, predict word Z'; the linear model can only represent features such as 'X predicts Z, Y predicts Z'. This means that the HLBL language model will probably not scale up well to large data sets, as it can model only patterns such as bigram, skip-1-bigram, skip-2-bigram etc. (and will thus behave slightly as a cache model, and will improve with longer context - which was actually observed in [R1] and [15]). Also note that the comparison in [R1] with NNLM is flawed, as the result from (Bengio, 2003) is from model that was small and not fully trained (due to computational complexity).\r\n\r\nTo conclude, the HLBL is very interesting model by itself, but we have chosen simpler architecture that follows our earlier work that aims to solve simpler problem - we do not try to learn a language model, just the word vectors. Detailed discussion about HLBL is out of scope of our current paper.\r\n\r\n\r\nThe discussion of techniques for speeding up training of neural language models is incomplete, as the authors do not mention sampling-based approaches such as importance sampling [R3] and noise-contrastive estimation [R2].\r\n\r\n- As our paper is already quite long, we do not plan to discuss speedup techniques that we did not use in our work. It can be a topic for future work.\r\n\r\n\r\nThe paper is unclear about the objective used for model selection. Was it a language-modeling objective (e.g. perplexity) or accuracy on the word similarity tasks?\r\n\r\n- The cost function that we try to minimize during training is the usual one (cross-entropy), however we choose the best models based on the performance on the word similarity task.\r\n\r\n\r\nIn the interests of precision, it would be good to include the equations defining the models in the paper.\r\n\r\n- Unfortunately the paper is already too long, so we just refer to prior work where similar models are properly defined. If we will extend the paper in the future, we will add the equations.\r\n\r\n\r\nReviewer Anonymous f5bf:\r\n\r\nConcern (1): We added a table with comparison of models trained on the same data. The results strongly support our previous claims (we had some of these results already before the first version of the paper was submitted, but due to lack of time these did not appear in the paper).\r\n\r\n(2): We added a Figure that illustrates the topology of the models, and kept the equations as we consider them important.\r\n\r\n(3): No, see the equations and Table 4.\r\n\r\n\r\nThe paper contains numerous typos and small errors. Specifically, I noticed a lot of missing articles throughout the paper.\r\n\r\n- We hope that small errors and missing articles are not the most important issue in research papers.\r\n\r\n\r\n'For many tasks, the amount of \u2026 focus on more advanced techniques.'\r\n\r\n- The introduction was updated.\r\n\r\n\r\nWhat about the fact that semantic similarities are intransitive? (Tversky's famous example of the similarity between China and North Korea.)\r\n\r\n- We are not aware of famous example of Tversky. Please provide reference.\r\n\r\n\r\n'Moreover, we discuss hyper-parameter selection \u2026 millions of words in the vocabulary.' -> I fail to see the relation between hyperparameter selection and training speed. Moreover, the paper actually does not say anything about hyperparameter selection! It only states the initial learning rate is 0.025, and that is linearly decreased (but not how fast).\r\n\r\n- Note that structure and size of the model is also hyper-parameter, as well as fraction of used training data; it is not just the learning rate. However, we simplified the text in the paper.\r\n\r\n\r\nTable 2: It appears that the performance of the CBOW model is still improving. How does it perform when D = 1000 or 2000? Why not make a learning curve here (plot performance as a function of D or of training time)?\r\n\r\n- That is an interesting experiment that we actually performed, but it would not fit into the paper.\r\n\r\n\r\nTable 3: Why is 'our NNLM' so much better than the other NNLMs? Just because it was trained on more data? What model is implemented by 'our NNLM' anyway? \r\n\r\n- Because it was trained in parallel using hundreds of CPUs. It is a feedforward NNLM.\r\n\r\n\r\nTables 3 and 4: Why is the NNLM trained on 6 billion examples and the others on just 0.7 or 1.6 billion examples? The others should be faster, so easier to train on more data, right?\r\n\r\n- We did not have these numbers during submission of the paper, but these results were added to the actual version of the paper. The new model architectures are faster for training than NNLM, and provide better results in our word similarity tasks.\r\n\r\n\r\nIt would be interesting if the authors could say something about how these models deal with intransitive semantic similarities, e.g., with the similarities between 'river', 'bank', and 'bailout'. People like Tversky have advocated against the use of semantic-space models like NLMs because they cannot appropriately model intransitive similarities.\r\n\r\n- We are not aware of Tversky's arguments.\r\n\r\n\r\nInstead of looking at binary question-answering performance, it may also be interesting to look whether a hitlist of answers contains the correct answer.\r\n\r\n- We performed this experiment as well; of course, top-5 accuracy is much better than top-1. However, it would be confusing to add these results into the paper (too many numbers).\r\n\r\n\r\nThe number of self-citations seems somewhat excessive.\r\n\r\n- We added more citations.\r\n\r\n\r\nI tried to find reference [14] to see how it differs from the present paper, but I was not able to find it anywhere.\r\n\r\n- This paper should become available on-line soon.\r\n\r\n\r\nReviewer Anonymous 3c5e:\r\n\r\nOne problem that has already been pointed out by the other reviewers is the lack of comparison and proper acknowledgment of previous models. The log-linear models have already been introduced by Mnih et al. and the averaging of context vectors (though of a larger context) has already been introduced by Huang et al. Both are cited but the model similarity is not mentioned.\r\n\r\n- As we explained earlier, we followed our own work that was published before these papers. We aim to learn word vectors, not language models. Note also that log-linear models and the bag-of-words representation are both very general and well known concepts, not unique to neural network language modeling. Also, Mnih introduced log-bilinear language model, not log-linear models - please read: http://en.wikipedia.org/wiki/Log-linear_model\r\n\r\nand http://en.wikipedia.org/wiki/Bag-of-words_model\r\n\r\n\r\nThe other main problem is that the evaluation metric clearly favors linear models since it checks for linear relationships. While it is an interesting finding that this holds for any of the models, this phenomenon does not necessarily need to lead to better performance. Other non-linear models may have encoded all this information too but not on a linear manifold. The whole new evaluation metric is just showing that linear models have more linear relationships. If this was combined with some performance increase on a real task then non-linearity for word vectors would have been convincingly questioned.\r\n\r\n- Note that projection layer in NNLM also does not have any non-linearity; Mnih's HLBL model does not have any non-linearity even in the hidden layer. We added more results in the paper, however can you be more specific what 'real task' means? The tasks we used are perfectly valid for a wide range of applications.\r\n\r\n\r\nDo these relationships hold for even simpler models like LSA or tf-idf vectors?\r\n\r\n- This is discussed in another paper. In general, linear operations do not work well for LSA vectors.\r\n\r\n\r\nMany very broad and general statements are made without any citations to back them up. \r\n\r\n- Please be more specific.\r\n\r\n\r\nThe motivation talks about how simpler bag of words models are not sufficient anymore to make significant progress... and then the rest of the paper introduces a simpler bag of words model and argues that it's better. The intro and the first paragraph of section 3 directly contradict themselves.\r\n\r\n- This part of the paper was rewritten. However, N-gram models are mentioned in the introduction; not bag-of-words models. Also note that the paper is about computationally efficient continuous representations of words. We do not introduce simple bag of words model, but log-linear model with distributed representations of bag-of-words features (in case of CBOW model).\r\n\r\n\r\nThe other motivation that is mentioned is how useful for actual tasks word vectors can be. I agree but this is not shown. This paper would have been significantly stronger if the vectors from the proposed (not so new) model would have been compared on any of the standard evaluation metrics that have been used for these words. For instance: Turian et al used NER, Huang et al used human similarity judgments, the author himself used them for language modeling. Why not show improvements on any of these tasks? \r\n\r\n- We believe that our task is very interesting by itself. The applications are very straightforward.\r\n\r\n\r\nLDA and LSA are missing citations.\r\n\r\n- We are not using LDA nor LSA in our paper. Moreover, these concepts are generally very well known.\r\n\r\n\r\nThe hidden layer in Collobert et al's word vectors is usually around 100, not between 500 to 1000 as the authors write.\r\n\r\n- We do not claim that hidden layer in Collobert et al's word vectors is usually between 500-1000. We actually point out that 50 and 100-dimensional word vectors have insufficient capacity, and the same holds for size of the hidden layer. The 500 - 2000 dimensional hidden layers are mentioned for NNLMs. We also provide reference to our prior paper that shows empirically that you have to use more than 100 neurons in the hidden layer, unless your training data is tiny ('Strategies for training large scale neural network language models').\r\n\r\n\r\nSection 2.2 is impossible to follow for people not familiar with this line of work.\r\n\r\n- This section is not crucial for understanding of the paper. However, if you are interested in this part, we provided several references for that work.\r\n\r\n\r\nWhy cosine distance? A comparison with Euclidean distance would be interesting, or should all word vectors be length-normalized?\r\n\r\n- We use normalized word vectors. Empirically, this works better.\r\n\r\n\r\nThe authors claim that their evaluation metric 'should be positively correlated with' 'certain applications'. That's yet another unsubstantiated claim that could be made much stronger with showing such a correlation on the above mentioned tasks.\r\n\r\n- While we have also results on another tasks, the point of this paper is not to describe all possible applications, but to introduce techniques for efficient estimation of word vectors from large amounts of data.\r\n\r\n\r\nThe comparisons are lacking consistency. All the models are trained on different corpora and have different dimensionality. Looking at the top 3 previous models (Mikolov 2x and Huang) there seems to be a clear correlation between vector size and overall performance. If one wants to make a convincing argument that the presented models are better, it would be important to show that using the same corpus.\r\n\r\n- Such comparison was added to the new version of the paper.\r\n\r\n\r\nGiven that the overall accuracy is around 50%, the examples in table 5 must have been manually selected? If not, it would be great to know how they were selected.\r\n\r\n- Maybe this will sound surprising, but examples in Table 5 have accuracy only about 60%. We did choose several easy examples from our Semantic-Syntactic test set (so that it would be easy to judge correctness for the readers), and some manually by trying out what the vectors can represent. Note that we did not simply hand-pick the best examples; this is the real performance."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Estimation of Word Representations in Vector Space", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day and one CPU to derive high quality 300-dimensional vectors for one million vocabulary from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring various types of word similarities. We intend to publish this test set to be used by the research community.", "pdf": "https://arxiv.org/abs/1301.3781", "paperhash": "mikolov|efficient_estimation_of_word_representations_in_vector_space", "keywords": [], "conflicts": [], "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "authorids": ["tmikolov@google.com", "kaichen@google.com", "gcorrado@google.com", "jeff@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362415140000, "tcdate": 1362415140000, "number": 3, "id": "ELp1azAY4uaYz", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "idpCdOWtqXd60", "replyto": "idpCdOWtqXd60", "signatures": ["anonymous reviewer 3c5e"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Efficient Estimation of Word Representations in Vector Space", "review": "This paper introduces a linear word vector learning model and shows that it performs better on a linear evaluation task than nonlinear models. While the new evaluation experiment is interesting the paper has too many issues in its current form.\r\n\r\nOne problem that has already been pointed out by the other reviewers is the lack of comparison and proper acknowledgment of previous models. The log-linear models have already been introduced by Mnih et al. and the averaging of context vectors (though of a larger context) has already been introduced by Huang et al. Both are cited but the model similarity is not mentioned.\r\n\r\nThe other main problem is that the evaluation metric clearly favors linear models since it checks for linear relationships. While it is an interesting finding that this holds for any of the models, this phenomenon does not necessarily need to lead to better performance. Other non-linear models may have encoded all this information too but not on a linear manifold. The whole new evaluation metric is just showing that linear models have more linear relationships. If this was combined with some performance increase on a real task then non-linearity for word vectors would have been convincingly questioned.\r\nDo these relationships hold for even simpler models like LSA or tf-idf vectors?\r\n\r\nIntroduction:\r\nMany very broad and general statements are made without any citations to back them up. \r\nThe motivation talks about how simpler bag of words models are not sufficient anymore to make significant progress... and then the rest of the paper introduces a simpler bag of words model and argues that it's better. The intro and the first paragraph of section 3 directly contradict themselves.\r\n\r\nThe other motivation that is mentioned is how useful for actual tasks word vectors can be. I agree but this is not shown. This paper would have been significantly stronger if the vectors from the proposed (not so new) model would have been compared on any of the standard evaluation metrics that have been used for these words. For instance: Turian et al used NER, Huang et al used human similarity judgments, the author himself used them for language modeling. Why not show improvements on any of these tasks? \r\n\r\nLDA and LSA are missing citations.\r\n\r\nCitation [14] which is in submission seems an important paper to back up some of the unsubstantiated claims of this paper but is not available.\r\n\r\nThe hidden layer in Collobert et al's word vectors is usually around 100, not between 500 to 1000 as the authors write.\r\n\r\nSection 2.2 is impossible to follow for people not familiar with this line of work.\r\n\r\nSection 4:\r\nWhy cosine distance? A comparison with Euclidean distance would be interesting, or should all word vectors be length-normalized?\r\n\r\nThe problem with synonyms in the evaluation seems somewhat important but is ignored.\r\n\r\nThe authors claim that their evaluation metric 'should be positively correlated with' 'certain applications'. That's yet another unsubstantiated claim that could be made much stronger with showing such a correlation on the above mentioned tasks.\r\n\r\nMnih is misspelled in table 3.\r\n\r\nThe comparisons are lacking consistency. All the models are trained on different corpora and have different dimensionality. Looking at the top 3 previous models (Mikolov 2x and Huang) there seems to be a clear correlation between vector size and overall performance. If one wants to make a convincing argument that the presented models are better, it would be important to show that using the same corpus.\r\n\r\nGiven that the overall accuracy is around 50%, the examples in table 5 must have been manually selected? If not, it would be great to know how they were selected."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Estimation of Word Representations in Vector Space", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day and one CPU to derive high quality 300-dimensional vectors for one million vocabulary from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring various types of word similarities. We intend to publish this test set to be used by the research community.", "pdf": "https://arxiv.org/abs/1301.3781", "paperhash": "mikolov|efficient_estimation_of_word_representations_in_vector_space", "keywords": [], "conflicts": [], "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "authorids": ["tmikolov@google.com", "kaichen@google.com", "gcorrado@google.com", "jeff@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1360865940000, "tcdate": 1360865940000, "number": 8, "id": "bf2Dnm5t9Ubqe", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "idpCdOWtqXd60", "replyto": "idpCdOWtqXd60", "signatures": ["anonymous reviewer f5bf"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Efficient Estimation of Word Representations in Vector Space", "review": "The paper studies the problem of learning vector representations for words based on large text corpora using 'neural language models' (NLMs). These models learn a feature vector for each word in such a way, that the feature vector of the current word in a document can be predicted from the feature vectors of the words that precede (and/or succeed) that word. Whilst a number of studies have developed techniques to make the training of NLMs more efficient, scaling NLMs up to today's multi-billion-words text corpora is still a challenge.\r\n\r\nThe main contribution of the paper comprises two new NLM architectures that facilitate training on massive data sets. The first model, CBOW, is essentially a standard feed-forward NLM without the intermediate projection layer (but with weight sharing + averaging  before applying the non-linearity in the hidden layer). The second model, skip-gram, comprises a collection of simple feed-forward nets that predict the presence of a preceding or succeeding word from the current word. The models are trained on a massive Google News corpus, and tested on a semantic and syntactic question-answering task. The results of these experiments look promising.\r\n\r\nWhilst I think this line of research is interesting and the presented results look promising, I do have three main concerns with this paper:\r\n\r\n(1) The choice for the proposed models (CBOW and skip-gram) are not clearly motivated. The authors' only motivation appears to be for computational reasons. However, the experiments do not convincingly show that this indeed leads to performance improvements on the task at hand. In particular, the 'vanilla' NLM implementation of the authors actually gives the best performance on the syntactic question-answering task. Faster training speed is mainly useful when you can throw more data at the model, and the model can effectively learn from this new data (as the authors argue themselves in the introduction). The experiments do not convincingly show that this happens. In addition, the comparisons with the models by Collobert-Weston, Turian, Mnih, Mikolov, and Huang appear to be unfair: these models were trained on much smaller corpora. A fair experiment would re-train the models on the same data to show that they learn slower (which is the authors' hypothesis), e.g., by showing learning curves or by showing a graph that shows performance as a function of training time.\r\n\r\n(2) The description of the models that are developed is very minimal, making it hard to determine how different they are from, e.g., the models presented in [15]. It would be very helpful if the authors included some graphical representations and/or more mathematical details of their models. Given that the authors still almost have one page left, and that they use a lot of space for the (frankly, somewhat superfluous) equations for the number of parameters of each model, this should not be a problem.\r\n\r\n(3) Throughout the paper, the authors assume that the computational complexity of learning is proportional to the number of parameters in the model. However, their experimental results show that this assumption is incorrect: doubling the number of parameters in the CBOW and skip-gram models only leads to a very modest increase in training time (see Table 4).\r\n\r\n\r\nDetailed comments\r\n===============\r\n- The paper contains numerous typos and small errors. Specifically, I noticed a lot of missing articles throughout the paper.\r\n\r\n- 'For many tasks, the amount of \u2026 focus on more advanced techniques.' -> This appears to be a contradiction. If speech recognition performance is largely governed by the amount of data we have, than simply scaling up the basic techniques should help a lot!\r\n\r\n- 'solutions were proposed for avoiding it' -> For avoiding what? Computation of the full output distribution over words of length V?\r\n\r\n- 'multiple degrees of similarities' -> What is meant by degrees here? Different dimensions of similarity? (For instance, Fiat is like Ferrari because they're both Italian but unlike Ferrari because it's not a sports car.) Or different strengths of the similarity? (For instance, Denmark is more like Germany than like Spain.) What about the fact that semantic similarities are intransitive? (Tversky's famous example of the similarity between China and North Korea.)\r\n\r\n- 'Moreover, we discuss hyper-parameter selection \u2026 millions of words in the vocabulary.' -> I fail to see the relation between hyperparameter selection and training speed. Moreover, the paper actually does not say anything about hyperparameter selection! It only states the initial learning rate is 0.025, and that is linearly decreased (but not how fast).\r\n\r\n- Table 2: It appears that the performance of the CBOW model is still improving. How does it perform when D = 1000 or 2000? Why not make a learning curve here (plot performance as a function of D or of training time)?\r\n\r\n- Table 3: Why is 'our NNLM' so much better than the other NNLMs? Just because it was trained on more data? What model is implemented by 'our NNLM' anyway? \r\n\r\n- Tables 3 and 4: Why is the NNLM trained on 6 billion examples and the others on just 0.7 or 1.6 billion examples? The others should be faster, so easier to train on more data, right?\r\n\r\n- It would be interesting if the authors could say something about how these models deal with intransitive semantic similarities, e.g., with the similarities between 'river', 'bank', and 'bailout'. People like Tversky have advocated against the use of semantic-space models like NLMs because they cannot appropriately model intransitive similarities.\r\n\r\n- Instead of looking at binary question-answering performance, it may also be interesting to look whether a hitlist of answers contains the correct answer.\r\n\r\n- The number of self-citations seems somewhat excessive.\r\n\r\n- I tried to find reference [14] to see how it differs from the present paper, but I was not able to find it anywhere."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Estimation of Word Representations in Vector Space", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day and one CPU to derive high quality 300-dimensional vectors for one million vocabulary from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring various types of word similarities. We intend to publish this test set to be used by the research community.", "pdf": "https://arxiv.org/abs/1301.3781", "paperhash": "mikolov|efficient_estimation_of_word_representations_in_vector_space", "keywords": [], "conflicts": [], "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "authorids": ["tmikolov@google.com", "kaichen@google.com", "gcorrado@google.com", "jeff@google.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1360857420000, "tcdate": 1360857420000, "number": 9, "id": "QDmFD7aPnX1h7", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "idpCdOWtqXd60", "replyto": "idpCdOWtqXd60", "signatures": ["anonymous reviewer 13e8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Efficient Estimation of Word Representations in Vector Space", "review": "The authors propose two log-linear language models for learning real-valued vector representations of words. The models are designed to be simple and fast and are shown to be scalable to very large datasets. The resulting word embeddings are evaluated on a number of novel word similarity tasks, on which they perform at least as well as the embeddings obtained using a much slower neural language model.\r\n\r\nThe paper is mostly clear and well executed. Its main contributions are a demonstration of scalability of the proposed models and a sensible protocol for evaluating word similarity information captured by such embeddings. The experimental section is convincing.\r\n\r\nThe log-linear language models proposed are not quite as novel or uniquely scalable as the paper seems to imply though. Models of this type were introduced in [R1] and further developed in [15] and [R2]. The idea of speeding up such models by eliminating matrix multiplication when combining the representations of context words was already implemented in [15] and [R2]. For example, the training complexity of the log-linear HLBL model from [15] is the same as that of the Continuous Bag-of-Words models. The authors should explain how the proposed log-linear models relate to the existing ones and in what ways they are superior. Note that Table 3 does contain a result obtained by an existing log-bilinear model, HLBL, which according to [18] was the model used to produce the 'Mhih NNLM' embeddings. These embeddings seem to perform considerably better then the 'Turian NNLM' embeddings obtained with a nonlinear NNLM on the same dataset, though of course not as well as the embeddings induced on much larger datasets. This result actually strengthens the authors argument for using log-linear models by suggesting that even if one could train a slow nonlinear model on the same amount of data it might not be worth it as it will not necessarily produce superior word representations.\r\n\r\nThe discussion of techniques for speeding up training of neural language models is incomplete, as the authors do not mention sampling-based approaches such as importance sampling [R3] and noise-contrastive estimation [R2].\r\n\r\nThe paper is unclear about the objective used for model selection. Was it a language-modeling objective (e.g. perplexity) or accuracy on the word similarity tasks?\r\n\r\nIn the interests of precision, it would be good to include the equations defining the models in the paper.\r\n\r\nIn Section 3, it might be clearer to say that the models are trained to 'predict' words, not 'classify' them.\r\n\r\nFinally, in Table 3 'Mhih NNLM' should probably read 'Mnih NNLM'.\r\n\r\nReferences:\r\n[R1] Mnih, A., & Hinton G. (2007). Three new graphical models for statistical language modelling. ICML 2007.\r\n[R2] Mnih, A., & Teh, Y. W. (2012). A fast and simple algorithm for training neural probabilistic language models. ICML 2012.\r\n[R3] Bengio, Y., & Senecal, J. S. (2008). Adaptive importance sampling to accelerate training of a neural probabilistic language model. IEEE Transactions on Neural Networks, 19(4), 713-722."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient Estimation of Word Representations in Vector Space", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day and one CPU to derive high quality 300-dimensional vectors for one million vocabulary from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring various types of word similarities. We intend to publish this test set to be used by the research community.", "pdf": "https://arxiv.org/abs/1301.3781", "paperhash": "mikolov|efficient_estimation_of_word_representations_in_vector_space", "keywords": [], "conflicts": [], "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "authorids": ["tmikolov@google.com", "kaichen@google.com", "gcorrado@google.com", "jeff@google.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358403300000, "tcdate": 1358403300000, "number": 58, "id": "idpCdOWtqXd60", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "idpCdOWtqXd60", "signatures": ["tmikolov@google.com"], "readers": ["everyone"], "content": {"title": "Efficient Estimation of Word Representations in Vector Space", "decision": "conferencePoster-iclr2013-workshop", "abstract": "We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day and one CPU to derive high quality 300-dimensional vectors for one million vocabulary from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring various types of word similarities. We intend to publish this test set to be used by the research community.", "pdf": "https://arxiv.org/abs/1301.3781", "paperhash": "mikolov|efficient_estimation_of_word_representations_in_vector_space", "keywords": [], "conflicts": [], "authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "authorids": ["tmikolov@google.com", "kaichen@google.com", "gcorrado@google.com", "jeff@google.com"]}, "writers": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 12}