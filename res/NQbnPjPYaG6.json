{"notes": [{"id": "NQbnPjPYaG6", "original": "BMd6yYuUZnk", "number": 1833, "cdate": 1601308202040, "ddate": null, "tcdate": 1601308202040, "tmdate": 1611607623409, "tddate": null, "forum": "NQbnPjPYaG6", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "On the Impossibility of Global Convergence in Multi-Loss Optimization", "authorids": ["~Alistair_Letcher1"], "authors": ["Alistair Letcher"], "keywords": ["impossibility", "global", "convergence", "optimization", "multi-loss", "multi-player", "multi-agent", "gradient", "descent"], "abstract": "Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any 'reasonable' algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum.  Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.", "one-sentence_summary": "We prove that a set of desirable convergence properties cannot simultaneously hold for any multi-loss optimization algorithm.", "pdf": "/pdf/baee1df38dd94202c599b2c6090013486763252f.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "letcher|on_the_impossibility_of_global_convergence_in_multiloss_optimization", "supplementary_material": "/attachment/946d9fbabab67c8a629490d201c814ac3a15dc7f.zip", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nletcher2021on,\ntitle={On the Impossibility of Global Convergence in Multi-Loss Optimization},\nauthor={Alistair Letcher},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NQbnPjPYaG6}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "zwkOfj5hyEV", "original": null, "number": 1, "cdate": 1610040471598, "ddate": null, "tcdate": 1610040471598, "tmdate": 1610474075693, "tddate": null, "forum": "NQbnPjPYaG6", "replyto": "NQbnPjPYaG6", "invitation": "ICLR.cc/2021/Conference/Paper1833/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper presents a series of negative results regarding the convergence of deterministic, \"reasonable\" algorithms in min-max games. The defining characteristic of such algorithms is that (a) the algorithm's fixed points are critical points of the game; and (b) they avoid strict maxima from almost any initialization. The authors then construct a range of simple $2$-dimensional \"market games\" in which every reasonable algorithm fails to converge, from almost any initialization.\n\nThe paper received three positive recommendations and one negative, with all reviewers indicating high confidence. After my own reading of the paper, I concur with the majority view that the paper's message is an interesting one for the community and will likely attract interest in ICLR.\n\nIn more detail, I view the authors' result as a cautionary tale, not unlike the NeurIPS 2019 spotlight paper of Vlatakis-Gkaragkounis et al, and a concurrent arxiv preprint by Hsieh et al. (2020). In contrast to the type of cycling/recurrence phenomena that are well-documented in bilinear games (and which can be resolved through the use of extra-gradient methods), the non-convergence phenomena described by the authors of this paper appear to be considerably more resilient, as they apply to all \"reasonable\" algorithms. Determining whether GANs (or other practical applications of min-max optimization) can exhibit such phenomena is an important open question, and one which needs to be informed by a deeper understanding of the theory. I find this paper successful in this regard and I am happy to recommend acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Impossibility of Global Convergence in Multi-Loss Optimization", "authorids": ["~Alistair_Letcher1"], "authors": ["Alistair Letcher"], "keywords": ["impossibility", "global", "convergence", "optimization", "multi-loss", "multi-player", "multi-agent", "gradient", "descent"], "abstract": "Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any 'reasonable' algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum.  Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.", "one-sentence_summary": "We prove that a set of desirable convergence properties cannot simultaneously hold for any multi-loss optimization algorithm.", "pdf": "/pdf/baee1df38dd94202c599b2c6090013486763252f.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "letcher|on_the_impossibility_of_global_convergence_in_multiloss_optimization", "supplementary_material": "/attachment/946d9fbabab67c8a629490d201c814ac3a15dc7f.zip", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nletcher2021on,\ntitle={On the Impossibility of Global Convergence in Multi-Loss Optimization},\nauthor={Alistair Letcher},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NQbnPjPYaG6}\n}"}, "tags": [], "invitation": {"reply": {"forum": "NQbnPjPYaG6", "replyto": "NQbnPjPYaG6", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040471585, "tmdate": 1610474075676, "id": "ICLR.cc/2021/Conference/Paper1833/-/Decision"}}}, {"id": "2Y03NADFE0g", "original": null, "number": 6, "cdate": 1606305544545, "ddate": null, "tcdate": 1606305544545, "tmdate": 1606305544545, "tddate": null, "forum": "NQbnPjPYaG6", "replyto": "O9uYqa4T4rd", "invitation": "ICLR.cc/2021/Conference/Paper1833/-/Official_Comment", "content": {"title": "All the majors results are built upon specifically designed losses and looks like no one are using these losses in reality", "comment": "Thanks for the authors' effort of trying to address my comments. However, my concerns still persist and have not been fully addressed. One of the biggest concerns is the practical use of the results in the paper. All the majors results are built upon specifically designed losses and looks like no one are using these losses in reality. I was looking for either of two aspects to address my comments:\n1. Current---A justification that the specifically designed losses, upon which the authors have derived their results, are (widely) used in practice. Or perhaps they share common properties with the losses that are used in reality. \n2. Future---An analysis of inspiration that even the results are not based on general losses, the results derived can potentially motivate the community for further study. \n\nNevertheless, I was not able to find either of the above two in the paper or from the rebuttal. Essentially, the results have not been justified valuable for current or future use. Therefore, I would like to keep my rating score for this paper at this point."}, "signatures": ["ICLR.cc/2021/Conference/Paper1833/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1833/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Impossibility of Global Convergence in Multi-Loss Optimization", "authorids": ["~Alistair_Letcher1"], "authors": ["Alistair Letcher"], "keywords": ["impossibility", "global", "convergence", "optimization", "multi-loss", "multi-player", "multi-agent", "gradient", "descent"], "abstract": "Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any 'reasonable' algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum.  Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.", "one-sentence_summary": "We prove that a set of desirable convergence properties cannot simultaneously hold for any multi-loss optimization algorithm.", "pdf": "/pdf/baee1df38dd94202c599b2c6090013486763252f.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "letcher|on_the_impossibility_of_global_convergence_in_multiloss_optimization", "supplementary_material": "/attachment/946d9fbabab67c8a629490d201c814ac3a15dc7f.zip", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nletcher2021on,\ntitle={On the Impossibility of Global Convergence in Multi-Loss Optimization},\nauthor={Alistair Letcher},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NQbnPjPYaG6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NQbnPjPYaG6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1833/Authors|ICLR.cc/2021/Conference/Paper1833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855281, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1833/-/Official_Comment"}}}, {"id": "44ORizNn3p", "original": null, "number": 3, "cdate": 1604091163982, "ddate": null, "tcdate": 1604091163982, "tmdate": 1605862903471, "tddate": null, "forum": "NQbnPjPYaG6", "replyto": "NQbnPjPYaG6", "invitation": "ICLR.cc/2021/Conference/Paper1833/-/Official_Review", "content": {"title": "Games are not optimization", "review": "Thank you for the explanation and the update of the writeup.\nI still find the main message interesting but not critical for any real application. \nHence my evaluation remains as weak accept.\nThank you.  \n\n\n\n%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n\n\nThe paper provides an impossibility type of result for convergence of reasonable dynamics in general multi-loss settings. Specifically, the paper constructs a game between two agents whose only simultaneous critical point is a strict maximum. Thus under the definition of a 'reasonable' algorithm as an algorithm that avoids strict maxima, the dynamics cannot converge to equilibria in this case and has to therefore cycle.\n\nThe paper builds on a series of recent papers that show that many reasonable dynamics fail to converge in reasonable multi-loss settings (e.g. GD in zero-sum games). On the positive side, the results accounts for a large class of optimization/gradient driven dynamics. Also, the example of the multi-agent setting is relatively small. Two agents/two degrees of freedom. On the negative side, the example is rather artificial and it does not seem to capture any specific setting of independent interest. Moreover, there is little intuition in the current writeup behind the construction of the example. The appendix just shows that this example satisfies the target properties.\n\nThe main of message of the paper is that multi-loss settings are very different from single loss settings and we cannot quickly and easily apply tools from optimization and hope to succeed at least not in all cases. The predominance of multi-loss architectures in AI settings makes this a reasonable message for ICLR.  On the other hand, it is clear that we are not moving away from these architectures any time soon, so such a message is less effective than explanation/tools of how to understand multi-loss environments or make them work better. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1833/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1833/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Impossibility of Global Convergence in Multi-Loss Optimization", "authorids": ["~Alistair_Letcher1"], "authors": ["Alistair Letcher"], "keywords": ["impossibility", "global", "convergence", "optimization", "multi-loss", "multi-player", "multi-agent", "gradient", "descent"], "abstract": "Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any 'reasonable' algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum.  Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.", "one-sentence_summary": "We prove that a set of desirable convergence properties cannot simultaneously hold for any multi-loss optimization algorithm.", "pdf": "/pdf/baee1df38dd94202c599b2c6090013486763252f.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "letcher|on_the_impossibility_of_global_convergence_in_multiloss_optimization", "supplementary_material": "/attachment/946d9fbabab67c8a629490d201c814ac3a15dc7f.zip", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nletcher2021on,\ntitle={On the Impossibility of Global Convergence in Multi-Loss Optimization},\nauthor={Alistair Letcher},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NQbnPjPYaG6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NQbnPjPYaG6", "replyto": "NQbnPjPYaG6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1833/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538109837, "tmdate": 1606915792459, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1833/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1833/-/Official_Review"}}}, {"id": "O9uYqa4T4rd", "original": null, "number": 4, "cdate": 1605816431309, "ddate": null, "tcdate": 1605816431309, "tmdate": 1605819550573, "tddate": null, "forum": "NQbnPjPYaG6", "replyto": "9vRKMxOqyJn", "invitation": "ICLR.cc/2021/Conference/Paper1833/-/Official_Comment", "content": {"title": "The practical value of our paper is to warn the machine learning community that reasonable optimization algorithms have no global convergence guarantees in multi-loss optimization, contrary to single-loss optimization, even in simple classes of games. Proving or disproving this for specific applications (eg GANs) is an important avenue for further research, but requires that we recognise the impossibility of global guarantees in the first place.", "comment": "Thank you for your review. We address each point below.\n\n1. The obvious desired outcome for a standard minimization problem is to reach a local (if not global) minimum of the loss. Theorem 1 points out that some simple games (two-player, two-parameter markets) do not have such an outcome. None of the four outcomes include converging to a local minimum, because there are none -- despite coercivity, analyticity and nondegeneracy of the losses. This is not intuitive at first sight, or at the very least, was left unnoticed by previous researchers in the field.\n\n2. As with many theoretical results, applied examples of the phenomenon we exhibit may appear further down the road. However, a first step to finding them or disproving their existence is to recognize the possibility of cyclic behaviour *in principle*, as we establish here. To emphasise the last paragraph of our paper: \u201cThe hope for machine learning practitioners is that local minima with large regions of attraction prevent limit cycles from arising in applications of interest, including GANs. Proving or disproving this is an interesting and important avenue for further research.\u201d\n\n3. It is unclear to us why the reviewer \u201cbelieves\u201d but does not justify that \u201ca game with only strict maximum cannot converge to the maximum is not quite interesting to the community\u201d, beyond the fact that it makes no sense to say that \u201ca game cannot converge to the maximum\u201d. Theorem 3 instead states that \u201calgorithms in $\\mathcal{A}$ almost surely have bounded nonconvergent iterates\u201d in a given weakly-coercive zero-sum game, for appropriate hyperparameters. This was previously unknown to the community, and acts as a warning that even simple zero-sum games may lead to cyclic iterates for a wide class of algorithms."}, "signatures": ["ICLR.cc/2021/Conference/Paper1833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Impossibility of Global Convergence in Multi-Loss Optimization", "authorids": ["~Alistair_Letcher1"], "authors": ["Alistair Letcher"], "keywords": ["impossibility", "global", "convergence", "optimization", "multi-loss", "multi-player", "multi-agent", "gradient", "descent"], "abstract": "Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any 'reasonable' algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum.  Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.", "one-sentence_summary": "We prove that a set of desirable convergence properties cannot simultaneously hold for any multi-loss optimization algorithm.", "pdf": "/pdf/baee1df38dd94202c599b2c6090013486763252f.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "letcher|on_the_impossibility_of_global_convergence_in_multiloss_optimization", "supplementary_material": "/attachment/946d9fbabab67c8a629490d201c814ac3a15dc7f.zip", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nletcher2021on,\ntitle={On the Impossibility of Global Convergence in Multi-Loss Optimization},\nauthor={Alistair Letcher},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NQbnPjPYaG6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NQbnPjPYaG6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1833/Authors|ICLR.cc/2021/Conference/Paper1833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855281, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1833/-/Official_Comment"}}}, {"id": "Hp-RusoLmR8", "original": null, "number": 5, "cdate": 1605816662344, "ddate": null, "tcdate": 1605816662344, "tmdate": 1605817398520, "tddate": null, "forum": "NQbnPjPYaG6", "replyto": "DKr1SDLtYi", "invitation": "ICLR.cc/2021/Conference/Paper1833/-/Official_Comment", "content": {"title": "We have updated the paper to incorporate your questions/comments.", "comment": "Thank you for your detailed review. We address each question below.\n\n### Proof of Theorem 2\nThanks for pointing this out: $h$ is upper semi-continuous but not necessarily continuous. We have corrected and clarified the proof in the uploaded revision.\n\nWe thought about defining algorithms to be reasonable if they avoid strict maxima for ALL hyperparameters upon writing the paper, but chose to avoid this stronger requirement. For instance, Consensus Optimization (CO) is reasonable for $\\gamma$ sufficiently small because strict maxima of the game remain unstable with respect to CO dynamics. However, CO may well converge to strict maxima if $\\gamma$ is too large; it seems acceptable for algorithms to behave well only in some hyperparameter range. This is similar to gradient descent behaving well only for appropriately small learning rates: it may otherwise diverge to infinity, even for coercive losses.\n\n### Definition of $F(\\theta)$\nWe can easily reconcile this abuse of notation by defining (R1) as $F(\\theta_k, \\ldots, \\theta_0) = \\theta_k \\implies \\xi(\\theta_k) = 0$. This is consistent with the intuition that a non-zero gradient at current parameters $T_k$ should lead to further decrease of the loss for any reasonable algorithm $F$, regardless of previous parameter dependence.\n\n### About Proposition 2\nThe proposition is rather trivial to prove and can indeed be seen as a collage of existing results, but is included to point clearly (and more visually than can be found elsewhere) to the fact that $H(\\theta) \\prec 0$ is the weakest generalisation of the concept of \u2018strict maxima\u2019 to multi-loss optimization. We do not believe this was established in previous work, though in any case, is not central to our paper. It is simply a further justification that our assumptions on reasonable algorithms are as weak as possible.\n\n### Global convergence in Balduzzi et al. 2018\n(Balduzzi et al., 2018) establish global convergence of Hamiltonian gradient descent in Hamiltonian games in Section 2.5, Theorem 3, under some regularity conditions. They also state without proof the global convergence of simultaneous gradient descent in potential games in Section 2.4: \u201c[potential games] are games where simultaneous gradient descent on the losses is gradient descent on a single function. It follows that descent on $\\xi$ converges to a fixed point that is a local minimum of $\\phi$\u201d. Despite the lack of proof, one can indeed establish that simultaneous gradient descent converges globally in potential games by appealing to global convergence of gradient descent on single losses under the usual regularity conditions, as in Proposition 1 of our paper.\n\nWe differentiated the work of (Hsieh et al., 2020) from other related work by placing it in a separate in-depth paragraph and stating that their focus is similar to ours, but will clarify the difference further.\n\n### Minor comment\nWe have incorporated these helpful comments in the revision."}, "signatures": ["ICLR.cc/2021/Conference/Paper1833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Impossibility of Global Convergence in Multi-Loss Optimization", "authorids": ["~Alistair_Letcher1"], "authors": ["Alistair Letcher"], "keywords": ["impossibility", "global", "convergence", "optimization", "multi-loss", "multi-player", "multi-agent", "gradient", "descent"], "abstract": "Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any 'reasonable' algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum.  Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.", "one-sentence_summary": "We prove that a set of desirable convergence properties cannot simultaneously hold for any multi-loss optimization algorithm.", "pdf": "/pdf/baee1df38dd94202c599b2c6090013486763252f.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "letcher|on_the_impossibility_of_global_convergence_in_multiloss_optimization", "supplementary_material": "/attachment/946d9fbabab67c8a629490d201c814ac3a15dc7f.zip", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nletcher2021on,\ntitle={On the Impossibility of Global Convergence in Multi-Loss Optimization},\nauthor={Alistair Letcher},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NQbnPjPYaG6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NQbnPjPYaG6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1833/Authors|ICLR.cc/2021/Conference/Paper1833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855281, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1833/-/Official_Comment"}}}, {"id": "20yrgHochKG", "original": null, "number": 3, "cdate": 1605815425860, "ddate": null, "tcdate": 1605815425860, "tmdate": 1605817281764, "tddate": null, "forum": "NQbnPjPYaG6", "replyto": "44ORizNn3p", "invitation": "ICLR.cc/2021/Conference/Paper1833/-/Official_Comment", "content": {"title": "We have updated the paper with further explanation/intuition.", "comment": "Thank you for your review. As you point out, the main message of our paper is that we cannot always hope to succeed in the realm of multi-loss optimization, at least in the traditional sense of converging to some kind of \u2018local minimum\u2019. Applied examples of this undesirable phenomenon may appear further down the road. However, a first step to finding them or disproving their existence is to recognize the possibility of cyclic behaviour *in principle*, as established here.\n\nOur example in Theorem 1 was constructed by noticing that there is no necessary reason for the local minima of two coercive losses to coincide, even when these losses are tied by zero-sum interaction terms in only two parameters. Instead, the gradients of each loss may only *simultaneously* vanish at a local maximum in each player\u2019s respective coordinate. Looking more specifically at our construction, the highest-order terms (first and last) provide coercivity in both coordinates while still having zero-sum interactions. The $-x^2, -y^2$ terms yield a strict local maximum *at* the origin, while the $\\pm xy$ terms provide opposite incentives *around* the origin which prevent any other simultaneous critical point to arise. We have added this explanation in the appendix of our revised paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper1833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Impossibility of Global Convergence in Multi-Loss Optimization", "authorids": ["~Alistair_Letcher1"], "authors": ["Alistair Letcher"], "keywords": ["impossibility", "global", "convergence", "optimization", "multi-loss", "multi-player", "multi-agent", "gradient", "descent"], "abstract": "Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any 'reasonable' algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum.  Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.", "one-sentence_summary": "We prove that a set of desirable convergence properties cannot simultaneously hold for any multi-loss optimization algorithm.", "pdf": "/pdf/baee1df38dd94202c599b2c6090013486763252f.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "letcher|on_the_impossibility_of_global_convergence_in_multiloss_optimization", "supplementary_material": "/attachment/946d9fbabab67c8a629490d201c814ac3a15dc7f.zip", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nletcher2021on,\ntitle={On the Impossibility of Global Convergence in Multi-Loss Optimization},\nauthor={Alistair Letcher},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NQbnPjPYaG6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NQbnPjPYaG6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1833/Authors|ICLR.cc/2021/Conference/Paper1833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855281, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1833/-/Official_Comment"}}}, {"id": "7tP8Ix3bbus", "original": null, "number": 2, "cdate": 1605815102344, "ddate": null, "tcdate": 1605815102344, "tmdate": 1605816759972, "tddate": null, "forum": "NQbnPjPYaG6", "replyto": "w01IQE1Upfw", "invitation": "ICLR.cc/2021/Conference/Paper1833/-/Official_Comment", "content": {"title": "Thank you.", "comment": "Thank you for your encouraging review. We have updated the paper to describe $\\alpha$ and $\\gamma$ in Section 3.3."}, "signatures": ["ICLR.cc/2021/Conference/Paper1833/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1833/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Impossibility of Global Convergence in Multi-Loss Optimization", "authorids": ["~Alistair_Letcher1"], "authors": ["Alistair Letcher"], "keywords": ["impossibility", "global", "convergence", "optimization", "multi-loss", "multi-player", "multi-agent", "gradient", "descent"], "abstract": "Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any 'reasonable' algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum.  Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.", "one-sentence_summary": "We prove that a set of desirable convergence properties cannot simultaneously hold for any multi-loss optimization algorithm.", "pdf": "/pdf/baee1df38dd94202c599b2c6090013486763252f.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "letcher|on_the_impossibility_of_global_convergence_in_multiloss_optimization", "supplementary_material": "/attachment/946d9fbabab67c8a629490d201c814ac3a15dc7f.zip", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nletcher2021on,\ntitle={On the Impossibility of Global Convergence in Multi-Loss Optimization},\nauthor={Alistair Letcher},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NQbnPjPYaG6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NQbnPjPYaG6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1833/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1833/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1833/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1833/Authors|ICLR.cc/2021/Conference/Paper1833/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1833/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923855281, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1833/-/Official_Comment"}}}, {"id": "w01IQE1Upfw", "original": null, "number": 1, "cdate": 1603896562027, "ddate": null, "tcdate": 1603896562027, "tmdate": 1605024348534, "tddate": null, "forum": "NQbnPjPYaG6", "replyto": "NQbnPjPYaG6", "invitation": "ICLR.cc/2021/Conference/Paper1833/-/Official_Review", "content": {"title": "Meaningful Result", "review": "This paper shows that the class of two-player markets have no satisfactory outcome in the usual sense. Players should neither escape to infinite losses nor converge to strict maxima or non-critical point. Some concrete examples are analyzed with negative results. \n\nThis paper is a reminder of researchers: we should carefully model the objective functions of multiple interacting intelligent agents and the interactions between them.\n\nWeakness:\nIt is better to describe the \\alpha and \\gamma in Sec. 3.3.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1833/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1833/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Impossibility of Global Convergence in Multi-Loss Optimization", "authorids": ["~Alistair_Letcher1"], "authors": ["Alistair Letcher"], "keywords": ["impossibility", "global", "convergence", "optimization", "multi-loss", "multi-player", "multi-agent", "gradient", "descent"], "abstract": "Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any 'reasonable' algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum.  Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.", "one-sentence_summary": "We prove that a set of desirable convergence properties cannot simultaneously hold for any multi-loss optimization algorithm.", "pdf": "/pdf/baee1df38dd94202c599b2c6090013486763252f.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "letcher|on_the_impossibility_of_global_convergence_in_multiloss_optimization", "supplementary_material": "/attachment/946d9fbabab67c8a629490d201c814ac3a15dc7f.zip", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nletcher2021on,\ntitle={On the Impossibility of Global Convergence in Multi-Loss Optimization},\nauthor={Alistair Letcher},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NQbnPjPYaG6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NQbnPjPYaG6", "replyto": "NQbnPjPYaG6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1833/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538109837, "tmdate": 1606915792459, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1833/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1833/-/Official_Review"}}}, {"id": "DKr1SDLtYi", "original": null, "number": 2, "cdate": 1603986774678, "ddate": null, "tcdate": 1603986774678, "tmdate": 1605024348469, "tddate": null, "forum": "NQbnPjPYaG6", "replyto": "NQbnPjPYaG6", "invitation": "ICLR.cc/2021/Conference/Paper1833/-/Official_Review", "content": {"title": "A well written paper providing an important negative result regarding multi-objective optimization", "review": "## Summary:\n\nThis paper deals with the impossibility of global convergence to stationary points in multi-loss optimization. The authors introduce some problems for which any \u201creasonable\u201c method has an undesired behavior.  \nThe notion of \u201creasonable method\u201d is quite general and makes the result of this paper interesting. \n\n\n### pros: \n- The framework of \u201creasonable methods\u201d is quite general. \n- The paper answers an important question of the game optimization community. \n- The paper is easy to read and well written.\n\n### cons:\n- there are some imprecisions (see my section about questions) \n\n\n## Questions: \nThe questions are asked by decreasing order of importance. \n\n### Proof of Theorem 2\n\nIn the proof of Theorem 2 (page 19) you claim that the function $h(\\theta):= \\inf_k (\\|F_0^k(\\theta)\\|)$ is continuous. I do not know if it is true or false (it is likely to be true), but this claim is not obvious (since it is an infimum of continuous functions). \n\nAlso, the statement in your theorems only considers the reasonable algorithms with the hyperparameters, ensuring that (R2) is valid. An easy way to get rid of this additional assumption may be to remove the mention of hyperparameter (in the sense that an algorithm with hyperparameters that make it locally converge to local maxima is not a reasonable algorithm). \n\n### Definition of $F(\\theta)$ \nAt the end of page 3, you have $F$ that can depend on all the previous iterates $F(\\theta_k,\\ldots,\\theta_0)$ but then in your definition R1 $F$ only depend on a single iterate $\\theta$. \nHow can you reconcile that a method may depend on all the previous iterates and Assumption R1? One simple way is maybe to consider a time-dependent operator $F_k$ such that $F_k(\\theta_0) = \\theta_{k+1}$.  .\n\n### About Proposition 2\n\nIt seems to me that most of the results of this proposition have been already covered by \n [Daskalakis and Panageas, 2018] [Adolphs et al., 2018] [Mazumdar et al. 2020] and[Berard et al. 2020]. What is your relative contribution?\nAlso, note that $H_d$ and $S$ are not defined.\n\n### Global convergence in Balduzzi et al. 2018\nI am not sure what global convergence result you are mentioning. Also, in your related work section, you should differentiate Hsieh et al. 2020 from the other related work. While  Hsieh et al. 2020 is trying to tackle the general nonconvex-nonconcave (which is very related to your work), the other related work mentioned in the first paragraph of Page 2 consider restrictive assumptions that allow them to get a Lyapunov function related to convex optimization (while your work implies that such a function does not exist in general).\n\n\n\n### Minor comment \n- the sentence \u201cLipschitz continuity of $\\nabla f$ is not assumed, which would fail even for cubic polynomials\u201d is a bit confusing since you mention the Lipschitz constant of $\\nabla f$ in Proposition 1. I suggest emphasizing that the *global* (outside of $U_0$) Lipschitz continuity is not assumed. \n- In the contribution section, $\\mathcal A$ is not defined. (I suggest to point to the appendix or specify that $\\mathcal A$ is the restricted class of function you mentioned at the beginning of your sentence)\n- Note that it would be interesting to discuss the Lyapunov function used to show the global convergence in the non-convex minimization setting (the suboptimality f(\\theta) -f^*). The current illustration you have  (page 8) also works in the multi-objective setting (by considering that H has a positive definite symmetric part). Thus, it is not very insightful of the main difference between single and multi-objective minimization.  \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1833/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1833/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Impossibility of Global Convergence in Multi-Loss Optimization", "authorids": ["~Alistair_Letcher1"], "authors": ["Alistair Letcher"], "keywords": ["impossibility", "global", "convergence", "optimization", "multi-loss", "multi-player", "multi-agent", "gradient", "descent"], "abstract": "Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any 'reasonable' algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum.  Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.", "one-sentence_summary": "We prove that a set of desirable convergence properties cannot simultaneously hold for any multi-loss optimization algorithm.", "pdf": "/pdf/baee1df38dd94202c599b2c6090013486763252f.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "letcher|on_the_impossibility_of_global_convergence_in_multiloss_optimization", "supplementary_material": "/attachment/946d9fbabab67c8a629490d201c814ac3a15dc7f.zip", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nletcher2021on,\ntitle={On the Impossibility of Global Convergence in Multi-Loss Optimization},\nauthor={Alistair Letcher},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NQbnPjPYaG6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NQbnPjPYaG6", "replyto": "NQbnPjPYaG6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1833/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538109837, "tmdate": 1606915792459, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1833/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1833/-/Official_Review"}}}, {"id": "9vRKMxOqyJn", "original": null, "number": 4, "cdate": 1604301502705, "ddate": null, "tcdate": 1604301502705, "tmdate": 1605024348348, "tddate": null, "forum": "NQbnPjPYaG6", "replyto": "NQbnPjPYaG6", "invitation": "ICLR.cc/2021/Conference/Paper1833/-/Official_Review", "content": {"title": " The paper studies the convergence problem in a two-player game with zero-sum interactions, whose losses are both coercive and analytic. The paper provides some new insights into the studied problem, but the reviewer questions its practical value for the community. Though as the major contribution of the paper, the three developed theorems, as the reviewer thinks, have their respective shortcomings for practical use.", "review": "\n\n1. For Theorem 1, as the reviewer understands it, for an optimization problem whose only critical point is a strict maxima, it only has four outcomes, which are listed in the theorem. The result seems quite intuitive and provides very limited understanding for the problem. Please list other possible outcomes for the general problem and state in such way that the paper finds some impossible outcomes which can be excluded for consideration.\n2. Theorem 2 states that a two-player game with a specific loss cannot converge to the only strict minimum. As the major result of the paper, this finding however has very limited value in practice. The specifically designed loss is not justified to be used in reality, and the result is built upon this loss. If the loss has nowhere to be found in use, investigating such a loss inspires the community very little.\n3. Similar to Theorem 2, it states in Theorem 3 that a two-player game with a specific loss cannot converge to the only strict maximum. Even more unclear to the reviewer, such a result can provide useful value to the community or not. A game with only strict maximum cannot converge to the maximum is not quite interesting to the community (as the reviewer believes).\nSome minor comments:\nThe paper use the term of \u201csimultaneous\u201d quite a lot in the first part of the paper. This is quite confusing for the readers as this term is not quite acknowledged.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1833/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1833/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Impossibility of Global Convergence in Multi-Loss Optimization", "authorids": ["~Alistair_Letcher1"], "authors": ["Alistair Letcher"], "keywords": ["impossibility", "global", "convergence", "optimization", "multi-loss", "multi-player", "multi-agent", "gradient", "descent"], "abstract": "Under mild regularity conditions, gradient-based methods converge globally to a critical point in the single-loss setting. This is known to break down for vanilla gradient descent when moving to multi-loss optimization, but can we hope to build some algorithm with global guarantees? We negatively resolve this open problem by proving that desirable convergence properties cannot simultaneously hold for any algorithm. Our result has more to do with the existence of games with no satisfactory outcomes, than with algorithms per se. More explicitly we construct a two-player game with zero-sum interactions whose losses are both coercive and analytic, but whose only simultaneous critical point is a strict maximum. Any 'reasonable' algorithm, defined to avoid strict maxima, will therefore fail to converge. This is fundamentally different from single losses, where coercivity implies existence of a global minimum.  Moreover, we prove that a wide range of existing gradient-based methods almost surely have bounded but non-convergent iterates in a constructed zero-sum game for suitably small learning rates. It nonetheless remains an open question whether such behavior can arise in high-dimensional games of interest to ML practitioners, such as GANs or multi-agent RL.", "one-sentence_summary": "We prove that a set of desirable convergence properties cannot simultaneously hold for any multi-loss optimization algorithm.", "pdf": "/pdf/baee1df38dd94202c599b2c6090013486763252f.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "letcher|on_the_impossibility_of_global_convergence_in_multiloss_optimization", "supplementary_material": "/attachment/946d9fbabab67c8a629490d201c814ac3a15dc7f.zip", "venueid": "ICLR.cc/2021/Conference", "venue": "ICLR 2021 Poster", "_bibtex": "@inproceedings{\nletcher2021on,\ntitle={On the Impossibility of Global Convergence in Multi-Loss Optimization},\nauthor={Alistair Letcher},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=NQbnPjPYaG6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NQbnPjPYaG6", "replyto": "NQbnPjPYaG6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1833/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538109837, "tmdate": 1606915792459, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1833/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1833/-/Official_Review"}}}], "count": 11}