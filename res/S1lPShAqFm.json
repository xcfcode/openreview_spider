{"notes": [{"id": "S1lPShAqFm", "original": "SklnT10ct7", "number": 1550, "cdate": 1538087998975, "ddate": null, "tcdate": 1538087998975, "tmdate": 1545355416164, "tddate": null, "forum": "S1lPShAqFm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Empirically Characterizing Overparameterization Impact on Convergence", "abstract": "A long-held conventional wisdom states that larger models train more slowly when using gradient descent. This work challenges this widely-held belief, showing that larger models can potentially train faster despite the increasing computational requirements of each training step. In particular, we study the effect of network structure (depth and width) on halting time and show that larger models---wider models in particular---take fewer training steps to converge.\n\nWe design simple experiments to quantitatively characterize the effect of overparametrization on weight space traversal. Results show that halting time improves when growing model's width for three different applications, and the improvement comes from each factor: The distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows with a power-law-like relationship, and gradient vectors become more aligned with each other during traversal.\n", "keywords": ["gradient descent", "optimization", "convergence time", "halting time", "characterization"], "authorids": ["newsha@baidu.com", "joel@baidu.com", "gregdiamos@baidu.com"], "authors": ["Newsha Ardalani", "Joel Hestness", "Gregory Diamos"], "TL;DR": "Empirically shows that larger models train in fewer training steps, because all factors in weight space traversal improve.", "pdf": "/pdf/e74b6e80a2c948d26e7630664b62caffbec89029.pdf", "paperhash": "ardalani|empirically_characterizing_overparameterization_impact_on_convergence", "_bibtex": "@misc{\nardalani2019empirically,\ntitle={Empirically Characterizing Overparameterization Impact on Convergence},\nauthor={Newsha Ardalani and Joel Hestness and Gregory Diamos},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lPShAqFm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SkxL_p32yV", "original": null, "number": 1, "cdate": 1544502638091, "ddate": null, "tcdate": 1544502638091, "tmdate": 1545354498695, "tddate": null, "forum": "S1lPShAqFm", "replyto": "S1lPShAqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1550/Meta_Review", "content": {"metareview": "This paper studies the  behavior of training of over parametrized models. All the reviewers agree that the questions studied in this paper are important. However the experiments in the paper are fairly preliminary and the paper does not offer any answers to the questions it studies.  Further the writing is very loose and the paper is not ready for publication. I advise authors to take the reviews seriously into account before submitting the paper again. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "ICLR 2019 decision"}, "signatures": ["ICLR.cc/2019/Conference/Paper1550/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1550/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirically Characterizing Overparameterization Impact on Convergence", "abstract": "A long-held conventional wisdom states that larger models train more slowly when using gradient descent. This work challenges this widely-held belief, showing that larger models can potentially train faster despite the increasing computational requirements of each training step. In particular, we study the effect of network structure (depth and width) on halting time and show that larger models---wider models in particular---take fewer training steps to converge.\n\nWe design simple experiments to quantitatively characterize the effect of overparametrization on weight space traversal. Results show that halting time improves when growing model's width for three different applications, and the improvement comes from each factor: The distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows with a power-law-like relationship, and gradient vectors become more aligned with each other during traversal.\n", "keywords": ["gradient descent", "optimization", "convergence time", "halting time", "characterization"], "authorids": ["newsha@baidu.com", "joel@baidu.com", "gregdiamos@baidu.com"], "authors": ["Newsha Ardalani", "Joel Hestness", "Gregory Diamos"], "TL;DR": "Empirically shows that larger models train in fewer training steps, because all factors in weight space traversal improve.", "pdf": "/pdf/e74b6e80a2c948d26e7630664b62caffbec89029.pdf", "paperhash": "ardalani|empirically_characterizing_overparameterization_impact_on_convergence", "_bibtex": "@misc{\nardalani2019empirically,\ntitle={Empirically Characterizing Overparameterization Impact on Convergence},\nauthor={Newsha Ardalani and Joel Hestness and Gregory Diamos},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lPShAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1550/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352798134, "tddate": null, "super": null, "final": null, "reply": {"forum": "S1lPShAqFm", "replyto": "S1lPShAqFm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1550/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1550/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1550/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352798134}}}, {"id": "BylWTKda27", "original": null, "number": 3, "cdate": 1541405112595, "ddate": null, "tcdate": 1541405112595, "tmdate": 1541533042760, "tddate": null, "forum": "S1lPShAqFm", "replyto": "S1lPShAqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1550/Official_Review", "content": {"title": "Interesting and inspiring observations, but need some further enhancement", "review": "This paper discusses the effect of increasing the widths in deep neural networks on the convergence of optimization. To this end, the paper focuses on RNNs and applications to NLP and speech recognition, and designs several groups of experiments/measurements to show that wider RNNs improve the convergence speed in three different aspects: 1) the number of steps taken to converge to the minimum validation loss is smaller; 2) the distance from initialization to final weights is shorter; 3) the step sizes (gradient norms) are larger. This in some sense complements the theoretical result in Arora et al. (2018) for linear neural networks (LNN), which states that deeper LNNs accelerates convergence of optimization, but the hidden layers widths are irrelevant. This also shows some essential difference between LNNs and (practical) nonlinear neural networks. \n\n### comments about writing ###\nThe findings are in general interesting and inspiring, but the explanations need some further improvement. In particular, the writing lacks some consistency and clarity in the wordings. For example, it is unclear to me what \"weight space traversal\" means, \"training size\" is mixed with \"dataset size\", and \"we will show that convergence ... to final weights\" seems to be a trivial comment (unless there is some special meaning of \"convergence rate\"), etc. It also lacks some clarity and organization in the results -- some more summarizing comments and sections (and in particular, a separate and clearer conclusion section), as well as less repetitions of the qualitative comments, should largely improve the readability of the paper.\n\n### comments about results ###\nThe observations included in the work may kick off some interesting follow-up work, but it is still a bit preliminary in the following sense:\n1. It lacks some discussions with its connection to some relevant literature about \"wider\" networks (e.g., Wide residual networks, Wider or deeper: revisiting the ResNet model for visual recognition, etc.).\n2. It lacks some discussions about the practical implication of the improvement in optimization convergence with respect to the widening of the hidden layers. In particular, what is the trade-off between the validation loss increase and the optimization convergence speed-up resulted from widening hidden layers? A heuristic discussion/approach should largely improve the impact of this work.\n3. The simplified theory about LNNs in the appendix seems a bit too far from the explanation of the difference between the observations in this paper and Arora et al. (2018).\n\n### typos and small suggestions ###\n1. It is suggested that the full name of LNN is provided at the beginning, and the font size should be larger in Figure 1.\n2. There are some mis-spellings that the authors should check (e.g., gradeint -> gradient).\n3. In formula (4), the authors should mention that the third line holds for all $t$ is a sufficient condition for the previous two equivalent lines.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1550/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirically Characterizing Overparameterization Impact on Convergence", "abstract": "A long-held conventional wisdom states that larger models train more slowly when using gradient descent. This work challenges this widely-held belief, showing that larger models can potentially train faster despite the increasing computational requirements of each training step. In particular, we study the effect of network structure (depth and width) on halting time and show that larger models---wider models in particular---take fewer training steps to converge.\n\nWe design simple experiments to quantitatively characterize the effect of overparametrization on weight space traversal. Results show that halting time improves when growing model's width for three different applications, and the improvement comes from each factor: The distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows with a power-law-like relationship, and gradient vectors become more aligned with each other during traversal.\n", "keywords": ["gradient descent", "optimization", "convergence time", "halting time", "characterization"], "authorids": ["newsha@baidu.com", "joel@baidu.com", "gregdiamos@baidu.com"], "authors": ["Newsha Ardalani", "Joel Hestness", "Gregory Diamos"], "TL;DR": "Empirically shows that larger models train in fewer training steps, because all factors in weight space traversal improve.", "pdf": "/pdf/e74b6e80a2c948d26e7630664b62caffbec89029.pdf", "paperhash": "ardalani|empirically_characterizing_overparameterization_impact_on_convergence", "_bibtex": "@misc{\nardalani2019empirically,\ntitle={Empirically Characterizing Overparameterization Impact on Convergence},\nauthor={Newsha Ardalani and Joel Hestness and Gregory Diamos},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lPShAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1550/Official_Review", "cdate": 1542234205981, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lPShAqFm", "replyto": "S1lPShAqFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1550/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335970226, "tmdate": 1552335970226, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1550/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1l-_TA237", "original": null, "number": 2, "cdate": 1541365096843, "ddate": null, "tcdate": 1541365096843, "tmdate": 1541533042552, "tddate": null, "forum": "S1lPShAqFm", "replyto": "S1lPShAqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1550/Official_Review", "content": {"title": "Interesting observations that are not backed up by a rigorous (empirical or otherwise) study ", "review": "Understanding the effects of over-parametrization in neural network training has been a major challenge, albeit a lot of progress has been made in the past few years. The present paper is another attempt in this direction, with a slightly different point of view: the work characterizes the impact of over-parametrization in the number of iterations it takes an algorithm to converge. Along the way, it also presents further empirical observations such as the distance between the initial point and the final point and the angle between the gradients and the line that connects the initial and final points. Even though the observations presented are very interesting, unfortunately, the paper doesn't have the level of rigor required that would make it a solid reference. \n\nThe work presents its results somewhat clearly in the sense that one can simply reconstruct to probe in order to replicate the observations. This clarity is mainly due to the simplicity of the questions posed. There is nothing inherently wrong with simple questions, in fact, the kind of questions posed in the present paper are quite valuable, however, it lacks detailed study and rigor of a strong empirical work. Furthermore, the style of the exposition (anecdotal) and several obvious typos make the work look quite unfinished. \n\nHere are some flaws and suggestions that would improve the work substantially:\n- A deeper literature review would help guide the reader put the paper in a better context. Especially, the related work section is quite poor, how exactly do those papers appear related to the present work? Do they support similar ideas or do they propose different perspectives?  \n- The exposition should be made more to the point and concise (for instance 3rd paragraph of section 4.3 where it starts with Figure 5(a) What's meant by over-fitting regime, is it worse gen error, is it merely fitting tr data?.. How do we \"know\" from Figure 2, what's a strong evidence? Some concepts such as the capacity do not have precise and commonly agreed upon definitions, the paper uses those quite a bit and sometimes only later on the reader understands what it actually refers to... The misalignment section is also quite unclear.)\n- The observations can be formalized and the curve fitting should be explained in further detail, the appendix touches upon simple cases but there is a strong literature behind those simple cases that could be quite useful for the purposes of the paper. \n- The authors have a lot of data available at no point the power law decay and exponent fitting are discussed. For a paper whose main point is this precise scaling, this looks like a major omission unless there is a specific reason for it (other than the hardness of fitting exponents to power laws). Merely showing the observables in a log-log plot weakens the support of the main claims.\n- The theoretical argument provided is just an elementary observation whose assumptions and conditions are not discussed. It is not a straightforward task, for instance, a suggestion for a theoretical result on the distance between the initial and final weights is presented here: Lemma 1 A.3 https://arxiv.org/abs/1806.07572 (distance shrink as the number of parameters increase consistent with the observations of the present paper) (note that this is in addition to the several early-2018 mean field approximations to NNs whose solutions are found in the limit where the number of  parameters tend to infinity)\n- All the figures from 5 to 8 are presented very quantitatively such as looking at different layers and observing the percentage reductions. The message one can gain from such presentations are extremely limited and not systematic. I encourage the authors to formulate solid observables that can and should be tested in further detail. \n\nEven though the paper is touching upon very interesting questions, at its current stage, it is not a good fit to be presented in a conference as it only presents anecdotal evidence. There is a lot of room to improve, but the good news is that most of the improvement should be straightforward.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1550/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirically Characterizing Overparameterization Impact on Convergence", "abstract": "A long-held conventional wisdom states that larger models train more slowly when using gradient descent. This work challenges this widely-held belief, showing that larger models can potentially train faster despite the increasing computational requirements of each training step. In particular, we study the effect of network structure (depth and width) on halting time and show that larger models---wider models in particular---take fewer training steps to converge.\n\nWe design simple experiments to quantitatively characterize the effect of overparametrization on weight space traversal. Results show that halting time improves when growing model's width for three different applications, and the improvement comes from each factor: The distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows with a power-law-like relationship, and gradient vectors become more aligned with each other during traversal.\n", "keywords": ["gradient descent", "optimization", "convergence time", "halting time", "characterization"], "authorids": ["newsha@baidu.com", "joel@baidu.com", "gregdiamos@baidu.com"], "authors": ["Newsha Ardalani", "Joel Hestness", "Gregory Diamos"], "TL;DR": "Empirically shows that larger models train in fewer training steps, because all factors in weight space traversal improve.", "pdf": "/pdf/e74b6e80a2c948d26e7630664b62caffbec89029.pdf", "paperhash": "ardalani|empirically_characterizing_overparameterization_impact_on_convergence", "_bibtex": "@misc{\nardalani2019empirically,\ntitle={Empirically Characterizing Overparameterization Impact on Convergence},\nauthor={Newsha Ardalani and Joel Hestness and Gregory Diamos},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lPShAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1550/Official_Review", "cdate": 1542234205981, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lPShAqFm", "replyto": "S1lPShAqFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1550/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335970226, "tmdate": 1552335970226, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1550/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Sye-0-Kt37", "original": null, "number": 1, "cdate": 1541145033397, "ddate": null, "tcdate": 1541145033397, "tmdate": 1541533042303, "tddate": null, "forum": "S1lPShAqFm", "replyto": "S1lPShAqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper1550/Official_Review", "content": {"title": "Mostly descriptive experimental analysis", "review": "This paper presents an empirical analysis of the convergence of deep NN training (in particular in language models and speech).\n\nStudying the effect of various hyperparameters on the convergence is certainly of great interest. However, the issue with this paper is that its analyses are mostly *descriptive*, rather than conclusive or even suggestive. For example, in Figure 2, it is shown that the convergence slope of Adam is steeper than that of SGD, when the x-axis is the model size. Very naturally I would be interested in a hypothesis like \u201cAdam converges quicker than SGD as we increase the model size\u201d, but there is no discussion like that. Throughout the paper there are many experimental results, but results are presented one after another, without many conclusions or suggestions made for practice. I don\u2019t have a good take-away after reading it.\n\nThe writing of this paper also needs to be improved significantly. In particular, lots of statements are made casually without justification. For example,\n\n\u201cIf hidden dimension is wide enough to absorb all the information within the input data, increasing width obviously would not affect convergence\u201d -- Not so obvious to me, any reference? \n\n\u201cFigure 4 shows a sketch of a model\u2019s convergence curve ...\u201d -- it\u2019s not a fact but only a hypothesis. For example, what if for super large models the convergence gets slow and the curve gets back up again?\n\nIn general, I think the paper is asking an interesting, important question, but more developments are needed from these initial experimental results.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1550/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirically Characterizing Overparameterization Impact on Convergence", "abstract": "A long-held conventional wisdom states that larger models train more slowly when using gradient descent. This work challenges this widely-held belief, showing that larger models can potentially train faster despite the increasing computational requirements of each training step. In particular, we study the effect of network structure (depth and width) on halting time and show that larger models---wider models in particular---take fewer training steps to converge.\n\nWe design simple experiments to quantitatively characterize the effect of overparametrization on weight space traversal. Results show that halting time improves when growing model's width for three different applications, and the improvement comes from each factor: The distance from initialized weights to converged weights shrinks with a power-law-like relationship, the average step size grows with a power-law-like relationship, and gradient vectors become more aligned with each other during traversal.\n", "keywords": ["gradient descent", "optimization", "convergence time", "halting time", "characterization"], "authorids": ["newsha@baidu.com", "joel@baidu.com", "gregdiamos@baidu.com"], "authors": ["Newsha Ardalani", "Joel Hestness", "Gregory Diamos"], "TL;DR": "Empirically shows that larger models train in fewer training steps, because all factors in weight space traversal improve.", "pdf": "/pdf/e74b6e80a2c948d26e7630664b62caffbec89029.pdf", "paperhash": "ardalani|empirically_characterizing_overparameterization_impact_on_convergence", "_bibtex": "@misc{\nardalani2019empirically,\ntitle={Empirically Characterizing Overparameterization Impact on Convergence},\nauthor={Newsha Ardalani and Joel Hestness and Gregory Diamos},\nyear={2019},\nurl={https://openreview.net/forum?id=S1lPShAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1550/Official_Review", "cdate": 1542234205981, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "S1lPShAqFm", "replyto": "S1lPShAqFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1550/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335970226, "tmdate": 1552335970226, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1550/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}