{"notes": [{"id": "SJG6G2RqtX", "original": "SJeBFnFcK7", "number": 1308, "cdate": 1538087956938, "ddate": null, "tcdate": 1538087956938, "tmdate": 1550831212530, "tddate": null, "forum": "SJG6G2RqtX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Value Propagation Networks", "abstract": "We present Value Propagation (VProp), a set of parameter-efficient differentiable planning modules built on Value Iteration which can successfully be trained using reinforcement learning to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We show that the modules enable learning to plan when the environment also includes stochastic elements, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems. We evaluate on static and dynamic configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes, and on a StarCraft navigation scenario, with more complex dynamics, and pixels as input.", "keywords": ["Reinforcement Learning", "Value Iteration", "Navigation", "Convolutional Neural Networks", "Learning to plan"], "authorids": ["nantas@robots.ox.ac.uk", "gab@fb.com", "zlin@fb.com", "pushmeet@google.com", "philip.torr@eng.ox.ac.uk", "usunier@fb.com"], "authors": ["Nantas Nardelli", "Gabriel Synnaeve", "Zeming Lin", "Pushmeet Kohli", "Philip H. S. Torr", "Nicolas Usunier"], "TL;DR": "We present planners based on convnets that are sample-efficient and that generalize to larger instances of navigation and pathfinding problems.", "pdf": "/pdf/a2a91e186fbe4fa5222aea2c81ac72e074af3efa.pdf", "paperhash": "nardelli|value_propagation_networks", "_bibtex": "@inproceedings{\nnardelli2018value,\ntitle={Value Propagation Networks},\nauthor={Nantas Nardelli and Gabriel Synnaeve and Zeming Lin and Pushmeet Kohli and Philip H. S. Torr and Nicolas Usunier},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG6G2RqtX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1lSzKybxN", "original": null, "number": 1, "cdate": 1544775948703, "ddate": null, "tcdate": 1544775948703, "tmdate": 1545354522782, "tddate": null, "forum": "SJG6G2RqtX", "replyto": "SJG6G2RqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper1308/Meta_Review", "content": {"metareview": "\nInteresting idea, reviewers were positive and indicated presentation should be improved.\n", "confidence": "3: The area chair is somewhat confident", "recommendation": "Accept (Poster)", "title": " Interesting idea, reviewers were positive and indicated presentation should be improved."}, "signatures": ["ICLR.cc/2019/Conference/Paper1308/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1308/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Value Propagation Networks", "abstract": "We present Value Propagation (VProp), a set of parameter-efficient differentiable planning modules built on Value Iteration which can successfully be trained using reinforcement learning to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We show that the modules enable learning to plan when the environment also includes stochastic elements, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems. We evaluate on static and dynamic configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes, and on a StarCraft navigation scenario, with more complex dynamics, and pixels as input.", "keywords": ["Reinforcement Learning", "Value Iteration", "Navigation", "Convolutional Neural Networks", "Learning to plan"], "authorids": ["nantas@robots.ox.ac.uk", "gab@fb.com", "zlin@fb.com", "pushmeet@google.com", "philip.torr@eng.ox.ac.uk", "usunier@fb.com"], "authors": ["Nantas Nardelli", "Gabriel Synnaeve", "Zeming Lin", "Pushmeet Kohli", "Philip H. S. Torr", "Nicolas Usunier"], "TL;DR": "We present planners based on convnets that are sample-efficient and that generalize to larger instances of navigation and pathfinding problems.", "pdf": "/pdf/a2a91e186fbe4fa5222aea2c81ac72e074af3efa.pdf", "paperhash": "nardelli|value_propagation_networks", "_bibtex": "@inproceedings{\nnardelli2018value,\ntitle={Value Propagation Networks},\nauthor={Nantas Nardelli and Gabriel Synnaeve and Zeming Lin and Pushmeet Kohli and Philip H. S. Torr and Nicolas Usunier},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG6G2RqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1308/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352885005, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJG6G2RqtX", "replyto": "SJG6G2RqtX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1308/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1308/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1308/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352885005}}}, {"id": "rkexJb4i2X", "original": null, "number": 2, "cdate": 1541255384211, "ddate": null, "tcdate": 1541255384211, "tmdate": 1543458183819, "tddate": null, "forum": "SJG6G2RqtX", "replyto": "SJG6G2RqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper1308/Official_Review", "content": {"title": "Missing information in the exposition", "review": "Update:\nI thank the authors for their clarifications. I have raised my rating, however I believe the exposition of the paper should be improved and some of their responses should be integrated to the main text.\n\nThe paper proposes two new modules to overcome some limitations of VIN, but the additional or alternative hypotheses used compared to VIN are not clearly stated and explained in my opinion. \n\n    Pros :\n    - experiments are numerous and advanced\n    - transition probabilities are not transition-invariant compared to VIN\n    - do not need pretraining trajectories\n\n    Cons :\n    - limitation and hypotheses are not very explicit\n\n    Questions/remarks :\n    - d_{rew} is not defined \n    - the shared weights should be explained in more details\n    - sometimes \\psi(s) is written as parametrized by \\theta, sometime not\n    - is it normal that the \\gamma never appears in your formula to update the \\theta and w? yet reading the background part I feel that you optimize the discounted sum of the rewards, is it the case?\n    - I think there is a mistake in the definition of 1_{s' \\neq \\emptyset }, it is 1 if s' is NOT terminal and 0 otherwise, am I wrong?\n    - why do you need the parameters w to represent the value function V, if you already have v^k_{i,j} available? is it just to say that your NN is updated with two distinct cost functions? \n    - I did not understand the assumptions made by VProp, do you consider that the transition function T is known? this seems to be the case when you explain that transitions are deterministic and that there is a mapping between the actions and the positions, but is never really said\n    - Compared to VIN, VProp uses an extra maximum to compute v^k_{i, j}, why? In this case, the approximation of the value function can never decrease.\n    - How is R_{a, i, j, i ', j'} broken into r^{in}_{i ', j'} - r^{out}_{i, j} in VProp? Is the reward function known to the agent at all points?\n    - In MVProp, can r_{i, j} be negative?\n    - In MVProp, how does the rewriting in p * v + r * (1-p) shows that only positive rewards are propagated? Does not it come only from the max?\n    - In the experiments, S is not fully described, \\phi(s) neither\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1308/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Value Propagation Networks", "abstract": "We present Value Propagation (VProp), a set of parameter-efficient differentiable planning modules built on Value Iteration which can successfully be trained using reinforcement learning to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We show that the modules enable learning to plan when the environment also includes stochastic elements, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems. We evaluate on static and dynamic configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes, and on a StarCraft navigation scenario, with more complex dynamics, and pixels as input.", "keywords": ["Reinforcement Learning", "Value Iteration", "Navigation", "Convolutional Neural Networks", "Learning to plan"], "authorids": ["nantas@robots.ox.ac.uk", "gab@fb.com", "zlin@fb.com", "pushmeet@google.com", "philip.torr@eng.ox.ac.uk", "usunier@fb.com"], "authors": ["Nantas Nardelli", "Gabriel Synnaeve", "Zeming Lin", "Pushmeet Kohli", "Philip H. S. Torr", "Nicolas Usunier"], "TL;DR": "We present planners based on convnets that are sample-efficient and that generalize to larger instances of navigation and pathfinding problems.", "pdf": "/pdf/a2a91e186fbe4fa5222aea2c81ac72e074af3efa.pdf", "paperhash": "nardelli|value_propagation_networks", "_bibtex": "@inproceedings{\nnardelli2018value,\ntitle={Value Propagation Networks},\nauthor={Nantas Nardelli and Gabriel Synnaeve and Zeming Lin and Pushmeet Kohli and Philip H. S. Torr and Nicolas Usunier},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG6G2RqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1308/Official_Review", "cdate": 1542234258469, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJG6G2RqtX", "replyto": "SJG6G2RqtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1308/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335918446, "tmdate": 1552335918446, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1308/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1lJJiUc27", "original": null, "number": 1, "cdate": 1541200598936, "ddate": null, "tcdate": 1541200598936, "tmdate": 1543323590825, "tddate": null, "forum": "SJG6G2RqtX", "replyto": "SJG6G2RqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper1308/Official_Review", "content": {"title": "Official review", "review": "Update:\n\nI thank the authors for the response. Unfortunately, the response does not mention modifications made to the paper according to the comments. According to pdfdiff, modifications to the paper are very minor, and none of my comments are addressed in the paper. I think the paper shows good results, but it could very much benefit from improved presentation and evaluation. I do recommend acceptance, but if the authors put more work in improving the paper, it could have a larger impact.\n\n------\n\nThe paper proposes a learnable planning model based on value iteration. The proposed methods can be seen as modifications of Value Iteration Networks (VIN), with some improvements aimed at improving sample efficiency and generalization to large environment sizes. The method is validated on gridworld-type environments, as well as on a more complex StarCraft-based domain with raw pixel input.\n\nPros:\n1) The topic of the paper is interesting: combining the advantages of learning and planning seems like a promising direction to achieving adaptive and generalizable systems.\n2) The presentation is quite good, although some details are missing.\n3) The proposed method can be effectively trained with reinforcement learning and generalizes well to much larger environments than trained on. It beats vanilla VIN by a large margin. The MVProp variant of the method is especially successful.\n\nCons:\n1) I would like to see a more complete discussion of the MVProp method. Propagation of only positive rewards seems like somewhat of a hack. Is this a general solution or is it only applicable to gridworld navigation-type tasks? Why? If not, is the area of applicability of MVProp different from VProp? Also, is the area of applicability of VProp different from VIN? It\u2019s important to discuss this in detail.\n2) I wonder how would the method behave in more realistic gridworld environments, for instance similar in layout to those used in RL navigation literature (DMLab, ViZDoom, MINOS, etc). The presented environments are quite artificial and seem to basically only require \u201cobstacle avoidance\u201d, not so much deliberate long-distance planning.\n3) Some details are missing. For instance, I was not able to find the exact network architectures used in different tasks. \nRelated to this, I was confused by the phrase \u201cAs these new environments are not static, the agent needs to re-plan at every step, forcing us to train on 8x8 maps to reduce the time spent rolling-out the recurrent modules.\u201d I might be misunderstanding something, but is there any recurrent network in VProp? Isn\u2019t it just predicting the parameters once and then rolling our value iteration forward without any learning? Is this so time-consuming?\n4) Why does the performance even of the best method not reach 100% even in the simpler environments in Figure 2? Why is the performance plateauing far from 100% in the more difficult case? It would be interesting to see more analysis of how the method works, when it fails, and which parts still need improvement. On a related topic, it would be good to see more qualitative results both in MazeBaze and StarCraft - in the form of images or videos.\n5) Novelty is somewhat limited: the method is conceptually similar to VIN. \n\nTo conclude, I think the paper is interesting and the proposed method seems to perform well in the tested environments. I am quite positive about the paper, and I will gladly raise the rating if my questions are addressed satisfactorily.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1308/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Value Propagation Networks", "abstract": "We present Value Propagation (VProp), a set of parameter-efficient differentiable planning modules built on Value Iteration which can successfully be trained using reinforcement learning to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We show that the modules enable learning to plan when the environment also includes stochastic elements, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems. We evaluate on static and dynamic configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes, and on a StarCraft navigation scenario, with more complex dynamics, and pixels as input.", "keywords": ["Reinforcement Learning", "Value Iteration", "Navigation", "Convolutional Neural Networks", "Learning to plan"], "authorids": ["nantas@robots.ox.ac.uk", "gab@fb.com", "zlin@fb.com", "pushmeet@google.com", "philip.torr@eng.ox.ac.uk", "usunier@fb.com"], "authors": ["Nantas Nardelli", "Gabriel Synnaeve", "Zeming Lin", "Pushmeet Kohli", "Philip H. S. Torr", "Nicolas Usunier"], "TL;DR": "We present planners based on convnets that are sample-efficient and that generalize to larger instances of navigation and pathfinding problems.", "pdf": "/pdf/a2a91e186fbe4fa5222aea2c81ac72e074af3efa.pdf", "paperhash": "nardelli|value_propagation_networks", "_bibtex": "@inproceedings{\nnardelli2018value,\ntitle={Value Propagation Networks},\nauthor={Nantas Nardelli and Gabriel Synnaeve and Zeming Lin and Pushmeet Kohli and Philip H. S. Torr and Nicolas Usunier},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG6G2RqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1308/Official_Review", "cdate": 1542234258469, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJG6G2RqtX", "replyto": "SJG6G2RqtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1308/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335918446, "tmdate": 1552335918446, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1308/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1gEGt6KC7", "original": null, "number": 5, "cdate": 1543260428009, "ddate": null, "tcdate": 1543260428009, "tmdate": 1543260428009, "tddate": null, "forum": "SJG6G2RqtX", "replyto": "B1lJJiUc27", "invitation": "ICLR.cc/2019/Conference/-/Paper1308/Official_Comment", "content": {"title": "Rebuttal to AnonReviewer3", "comment": "We thank the reviewer for the suggestions and positive comments. We would like to answer some of their questions:\n\n1) This is an interesting question, thank you for asking. In general MVProp is applicable to any path-planning task where the dynamics are deterministic, the agent is generally interested in finding the path with lowest cost (or, in this case, highest reward), and the reward function associated to the task does correspond to simply in one based on path-planning (i.e. some cost is associated to moving and getting into invalid states, and some reward is given for reaching goals). This applies to a variety of path-planning problems, but if the environment has a more nuanced reward function that would provide quicker feedback from minimising negative cost, VProp might learn faster. Our experiments show that our models work outside of the deterministic assumption (see experiments on stochastic environments in Section 4.2).\n\n2) We are not certain it is clear we would gain further intuition from looking at DMLab, VizDoom, or other maze environments, since a birds-eye version of the tasks available in these environments would be relatively similar to either our static grid-world setup or the dynamic ones, but much slower and resource intensive. That said, while we might indeed explore in the future such tasks, we feel that VProp and MVProp might be best used as planning modules within a larger and more complex planning architecture that we mentioned in Section 1.1, such as Niu et al. (2017), and Gupta et al. (2017). Also note that these environments provide a standard setup (i.e. first person view, high degree of partial observability, some stochasticity) that is definitely beyond the scope of all our models and baselines.\n\n3) At each agent step, our model (and the baseline) needs to convolve for K steps, where K is equal to roughly the distance to the goal (step-wise) for each step. If the environment is deterministic and \u201cstatic\u201d, i.e. goal state and unreachable states do not change, the agent only needs to do this process once, however if the environment changes stochastically the model assumptions are broken, thus we need to at least allow to replan after some amount of steps (and we chose 1 to give the VIN baseline a fair chance). There are ways to improve upon this, such as building a hierarchical planner directly inside the VProp modules, but such methods require significant changes and are pretty much future work.\n\n4) Figure 1 shows the (average) final reward obtained in the static settings, where the optimal reward is $goal_reward - optimal_steps * step_cost$, where reward_goal = 1 and cost_step = 0.01. MVProp very quickly reaches optimality, which on average gives reward slightly below 1 in all test environments, while VProp is more unstable when generalising to larger instances. Please see Table 1 in the Appendix for more precise numbers. In terms of videos, we will show a demo of the models working at the conference.\n\n5) We don\u2019t think of our work as necessary just an extension of VIN, but more of a principled way to learn planning modules that are fully convolutional and that can generalize across wildly different planning horizons and input sizes while interacting with the environment (thus via RL). The great majority of agents / planners based on deep neural networks tends to either ignore the problem altogether, or use fixed transformations on the input, leading to resolution and/or information loss. We look forward to seeing more work tackling this problem, and we hope VProp and MVProp will provide a good step in that direction.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1308/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1308/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1308/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Value Propagation Networks", "abstract": "We present Value Propagation (VProp), a set of parameter-efficient differentiable planning modules built on Value Iteration which can successfully be trained using reinforcement learning to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We show that the modules enable learning to plan when the environment also includes stochastic elements, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems. We evaluate on static and dynamic configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes, and on a StarCraft navigation scenario, with more complex dynamics, and pixels as input.", "keywords": ["Reinforcement Learning", "Value Iteration", "Navigation", "Convolutional Neural Networks", "Learning to plan"], "authorids": ["nantas@robots.ox.ac.uk", "gab@fb.com", "zlin@fb.com", "pushmeet@google.com", "philip.torr@eng.ox.ac.uk", "usunier@fb.com"], "authors": ["Nantas Nardelli", "Gabriel Synnaeve", "Zeming Lin", "Pushmeet Kohli", "Philip H. S. Torr", "Nicolas Usunier"], "TL;DR": "We present planners based on convnets that are sample-efficient and that generalize to larger instances of navigation and pathfinding problems.", "pdf": "/pdf/a2a91e186fbe4fa5222aea2c81ac72e074af3efa.pdf", "paperhash": "nardelli|value_propagation_networks", "_bibtex": "@inproceedings{\nnardelli2018value,\ntitle={Value Propagation Networks},\nauthor={Nantas Nardelli and Gabriel Synnaeve and Zeming Lin and Pushmeet Kohli and Philip H. S. Torr and Nicolas Usunier},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG6G2RqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1308/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610952, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJG6G2RqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1308/Authors", "ICLR.cc/2019/Conference/Paper1308/Reviewers", "ICLR.cc/2019/Conference/Paper1308/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1308/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1308/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1308/Authors|ICLR.cc/2019/Conference/Paper1308/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1308/Reviewers", "ICLR.cc/2019/Conference/Paper1308/Authors", "ICLR.cc/2019/Conference/Paper1308/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610952}}}, {"id": "rklgnOpKA7", "original": null, "number": 4, "cdate": 1543260327719, "ddate": null, "tcdate": 1543260327719, "tmdate": 1543260327719, "tddate": null, "forum": "SJG6G2RqtX", "replyto": "rkexJb4i2X", "invitation": "ICLR.cc/2019/Conference/-/Paper1308/Official_Comment", "content": {"title": "Rebuttal to AnonReviewer2", "comment": "We thank the reviewer for the positive comments. Here\u2019s some answers which will hopefully clarify some of the questions posed:\n\n>d_{rew} is not defined \n\nRegarding \\drew, in the case of VIN we do implicitly refer to it with \u201coutput channels\u201d, as it really is just some variable defining the number of channels used for the embedding function. We have adjusted Section 2.2 to make this more evident. Note also that in the case of VProp, it is 3 (r^in, r^out, p), and 2 (r, p) for MVProp.\n\n>the shared weights should be explained in more details\n\nWhen we say \u201cshared weights\u201d we mean it literally: the recurrence step is done by the same network layers, as opposed to convolving at each step using different parameters. That (trivially) reduces the amount of parameters needed when the network is fully unrolled, and it allows us to generalise to larger environments by unrolling more.\n\n>Inconsistent use of theta in \\psi [sic.], missing gamma, and definition of 1_{s' \\neq \\emptyset }\n\nWe are not using \\psi anywhere. If instead you are referring to \\Phi, for consistency we have removed the theta from previous equations prior to Section 4 in the new revision.\n\nIndeed, there should be a \\gamma behind V_{w^t} for both updates, and there's a missing \"not\" in the definition of 1_{s' \\neq \\emptyset }, thank you for noticing both. We have fixed them in the new revision.\n\n>why do you need the parameters w to represent the value function V, if you already have v^k_{i,j} available? is it just to say that your NN is updated with two distinct cost functions? \n\nExactly, the loss in off-policy actor-critic for the value head is indeed different from the one applied to the policy head, and the parameters updated are not the same, so we felt that it was clearer to split the two. Furthermore, to be completely clear, V^k{i, j} is the value function inside the planning module, while V is the overall value function of the final \u201cpolicy\u201d layer used within actor-critic.\n\n>I did not understand the assumptions made by VProp, do you consider that the transition function T is known? this seems to be the case when you explain that transitions are deterministic and that there is a mapping between the actions and the positions, but is never really said\n\nWe do not assume we know the transition function T, and in fact we do learn some parameters of it. However we do assume that the function is constrained in certain ways, i.e. at most one state may be accessible from another state, given an action.\n\n>Compared to VIN, VProp uses an extra maximum to compute v^k_{i, j}, why? In this case, the approximation of the value function can never decrease.\n\nThe extra \u201cmax\u201d is equivalent to adding an extra action of staying in the same state with zero immediate reward. We would not lose any generality if immediate rewards are always positive. In our case, it is a convenient way to represent absorbing states (i.e., goal states).\n\n>How is R_{a, i, j, i ', j'} broken into r^{in}_{i ', j'} - r^{out}_{i, j} in VProp? Is the reward function known to the agent at all points?\n\nThe reward broken down in two values is a choice of parametrization. This is an assumption, which drastically reduces the number of parameters to learn (because there are only two values per state instead of R(i,j,a,i\u2019,j\u2019) ). Our point here is to say that this parametrization is sufficient to represent cases of interest, but in general this is an assumption that does not always hold.\n\n>In MVProp, can r_{i, j} be negative?\n\nIt could (in which case it would decrease the values of nearby states that are propagated to the current state). In practice, stopping the propagation can be carried out by near-0 values of the propagation gate, and rewards are constrained to be positive by a sigmoid activation function on the reward channel.\n\n>In MVProp, how does the rewriting in p * v + r * (1-p) shows that only positive rewards are propagated? Does not it come only from the max?\n\nYes, the max with the current value is what makes negative values of the reward in a state not propagate to nearby states (because a high value of r_{i,j} propagates to nearby states by first increasing v_{i,j}, which itself is propagated to the neighboring v_{i\u2019,j\u2019} at further iterations. Negative values of r_{i,j} would not update v_{i,j} and thus wouldn\u2019t be propagated to nearby states. We believed the rewriting as pv + r(1-p) helps understanding that negative values of r_{i,j} will not be used to update v_{i,j} in the first iteration. We will clarify this statement.\n\n>In the experiments, S is not fully described, \\phi(s) neither\n\nFor the grid-world experiments, the state is described in Section 2.2, and \\phi(s) is a fixed function that splits each feature into its own channel; for the StarCraft experiments, the grid-world featurization is similarly done (and based on TorchCraft\u2019s API), while in pixel space \\phi(s) is added to the network as two extra convolutional -> max pooling layers, as described in Appendix D.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1308/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1308/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1308/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Value Propagation Networks", "abstract": "We present Value Propagation (VProp), a set of parameter-efficient differentiable planning modules built on Value Iteration which can successfully be trained using reinforcement learning to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We show that the modules enable learning to plan when the environment also includes stochastic elements, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems. We evaluate on static and dynamic configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes, and on a StarCraft navigation scenario, with more complex dynamics, and pixels as input.", "keywords": ["Reinforcement Learning", "Value Iteration", "Navigation", "Convolutional Neural Networks", "Learning to plan"], "authorids": ["nantas@robots.ox.ac.uk", "gab@fb.com", "zlin@fb.com", "pushmeet@google.com", "philip.torr@eng.ox.ac.uk", "usunier@fb.com"], "authors": ["Nantas Nardelli", "Gabriel Synnaeve", "Zeming Lin", "Pushmeet Kohli", "Philip H. S. Torr", "Nicolas Usunier"], "TL;DR": "We present planners based on convnets that are sample-efficient and that generalize to larger instances of navigation and pathfinding problems.", "pdf": "/pdf/a2a91e186fbe4fa5222aea2c81ac72e074af3efa.pdf", "paperhash": "nardelli|value_propagation_networks", "_bibtex": "@inproceedings{\nnardelli2018value,\ntitle={Value Propagation Networks},\nauthor={Nantas Nardelli and Gabriel Synnaeve and Zeming Lin and Pushmeet Kohli and Philip H. S. Torr and Nicolas Usunier},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG6G2RqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1308/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610952, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJG6G2RqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1308/Authors", "ICLR.cc/2019/Conference/Paper1308/Reviewers", "ICLR.cc/2019/Conference/Paper1308/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1308/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1308/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1308/Authors|ICLR.cc/2019/Conference/Paper1308/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1308/Reviewers", "ICLR.cc/2019/Conference/Paper1308/Authors", "ICLR.cc/2019/Conference/Paper1308/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610952}}}, {"id": "BJxWBITYAX", "original": null, "number": 3, "cdate": 1543259704547, "ddate": null, "tcdate": 1543259704547, "tmdate": 1543259704547, "tddate": null, "forum": "SJG6G2RqtX", "replyto": "B1lNYtd03m", "invitation": "ICLR.cc/2019/Conference/-/Paper1308/Official_Comment", "content": {"title": "Rebuttal to AnonReviewer1", "comment": "We thank the reviewer for the very positive comments. Regarding their point about comparison against standard model-free algorithms, we didn\u2019t compare against them because these agents can\u2019t be applied well to the experimental setup. More precisely:\n\n- typical models used with these algorithms cannot deal with varying input sizes, unless you engineer a function that would downsample / upsample the observation to a particular shape. One could learn in principle such a function, but that would require an entirely different experimental and evaluation loop, e.g. one that would keep the \u201ccore\u201d agent model frozen while training only an embedding function (which would however likely result in unstable learning). Making standard model size-invariant w.r.t. the observation size is a problem that we have decided to tackle by employing fully convolutional models, but this differs from most DRL work.\n\n- the models would need to be much bigger in terms of hyperparameters, and would most likely need a better curriculum to deal with the sparse positive signals, thus also making comparison trickier.\n\nTamar et al. (2016) (whose experimental setup is similar to ours) show these points pretty clearly. \n\nPlease also note that we could replace our actor-critic update rule (and agent setup) with PPO (and DDPG if we were to use a continuous action space), but the focus of our paper was the planning module, so all our experiments share the same agent setup.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1308/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1308/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1308/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Value Propagation Networks", "abstract": "We present Value Propagation (VProp), a set of parameter-efficient differentiable planning modules built on Value Iteration which can successfully be trained using reinforcement learning to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We show that the modules enable learning to plan when the environment also includes stochastic elements, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems. We evaluate on static and dynamic configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes, and on a StarCraft navigation scenario, with more complex dynamics, and pixels as input.", "keywords": ["Reinforcement Learning", "Value Iteration", "Navigation", "Convolutional Neural Networks", "Learning to plan"], "authorids": ["nantas@robots.ox.ac.uk", "gab@fb.com", "zlin@fb.com", "pushmeet@google.com", "philip.torr@eng.ox.ac.uk", "usunier@fb.com"], "authors": ["Nantas Nardelli", "Gabriel Synnaeve", "Zeming Lin", "Pushmeet Kohli", "Philip H. S. Torr", "Nicolas Usunier"], "TL;DR": "We present planners based on convnets that are sample-efficient and that generalize to larger instances of navigation and pathfinding problems.", "pdf": "/pdf/a2a91e186fbe4fa5222aea2c81ac72e074af3efa.pdf", "paperhash": "nardelli|value_propagation_networks", "_bibtex": "@inproceedings{\nnardelli2018value,\ntitle={Value Propagation Networks},\nauthor={Nantas Nardelli and Gabriel Synnaeve and Zeming Lin and Pushmeet Kohli and Philip H. S. Torr and Nicolas Usunier},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG6G2RqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1308/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610952, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJG6G2RqtX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1308/Authors", "ICLR.cc/2019/Conference/Paper1308/Reviewers", "ICLR.cc/2019/Conference/Paper1308/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1308/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1308/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1308/Authors|ICLR.cc/2019/Conference/Paper1308/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1308/Reviewers", "ICLR.cc/2019/Conference/Paper1308/Authors", "ICLR.cc/2019/Conference/Paper1308/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610952}}}, {"id": "B1lNYtd03m", "original": null, "number": 3, "cdate": 1541470588037, "ddate": null, "tcdate": 1541470588037, "tmdate": 1541533247626, "tddate": null, "forum": "SJG6G2RqtX", "replyto": "SJG6G2RqtX", "invitation": "ICLR.cc/2019/Conference/-/Paper1308/Official_Review", "content": {"title": "Interesting extension of the original value iteration networks (VIN), promising work", "review": "The paper presents an extension of the original value iteration networks (VIN) by considering state-dependent transition function, which alleviates the limitation of VIN to translation-invariant transition functions and further constraining the reward function parametrization to improve sample efficiency of learning to plan algorithms. The first problem is addressed  by interpreting transition probabilities as state-dependent discount factors, given by a sigmoid function that takes as input state features. The second problem is addressed by defining the reward function as the difference between an input reward and an output cost. Obstacle states are given a high cost. The proposed method is evaluated on random grids of different sizes, of the same type as the grids considered in the VIN paper. Comparaisons with VIN show that the proposed MVProp approach outperforms VIN by several orders of magnitude and can learn optimal plans in less than a thousand episodes, compared to VIN that doesn't seem here to learn much even after 30 thousands episodes. \nThe paper is well-written in general. Certain aspects of value iteration networks were explained too briefly and the reviewer had to re-read the original VIN paper to grasp certain details of the proposed approach. This work is an interesting improvement of VIN, but somehow incremental in nature as the improvement is limited to slightly changing the reward and transition representations. However, the resulting performance seems very impressive, especially for larger grids. One question that needs to be clarified is: how is this work situated with respect to the body of work on RL? How does this method compare empirically to model-free algorithms such as DDPG and PPO?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1308/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Value Propagation Networks", "abstract": "We present Value Propagation (VProp), a set of parameter-efficient differentiable planning modules built on Value Iteration which can successfully be trained using reinforcement learning to solve unseen tasks, has the capability to generalize to larger map sizes, and can learn to navigate in dynamic environments. We show that the modules enable learning to plan when the environment also includes stochastic elements, providing a cost-efficient learning system to build low-level size-invariant planners for a variety of interactive navigation problems. We evaluate on static and dynamic configurations of MazeBase grid-worlds, with randomly generated environments of several different sizes, and on a StarCraft navigation scenario, with more complex dynamics, and pixels as input.", "keywords": ["Reinforcement Learning", "Value Iteration", "Navigation", "Convolutional Neural Networks", "Learning to plan"], "authorids": ["nantas@robots.ox.ac.uk", "gab@fb.com", "zlin@fb.com", "pushmeet@google.com", "philip.torr@eng.ox.ac.uk", "usunier@fb.com"], "authors": ["Nantas Nardelli", "Gabriel Synnaeve", "Zeming Lin", "Pushmeet Kohli", "Philip H. S. Torr", "Nicolas Usunier"], "TL;DR": "We present planners based on convnets that are sample-efficient and that generalize to larger instances of navigation and pathfinding problems.", "pdf": "/pdf/a2a91e186fbe4fa5222aea2c81ac72e074af3efa.pdf", "paperhash": "nardelli|value_propagation_networks", "_bibtex": "@inproceedings{\nnardelli2018value,\ntitle={Value Propagation Networks},\nauthor={Nantas Nardelli and Gabriel Synnaeve and Zeming Lin and Pushmeet Kohli and Philip H. S. Torr and Nicolas Usunier},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SJG6G2RqtX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1308/Official_Review", "cdate": 1542234258469, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJG6G2RqtX", "replyto": "SJG6G2RqtX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1308/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335918446, "tmdate": 1552335918446, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1308/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}