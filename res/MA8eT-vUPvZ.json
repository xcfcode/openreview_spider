{"notes": [{"id": "MA8eT-vUPvZ", "original": "DDj3KkPeEVq", "number": 2089, "cdate": 1601308230161, "ddate": null, "tcdate": 1601308230161, "tmdate": 1614985662588, "tddate": null, "forum": "MA8eT-vUPvZ", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift", "authorids": ["~Marvin_Mengxin_Zhang2", "marklund@cs.stanford.edu", "nikitadhawan@berkeley.edu", "~Abhishek_Gupta1", "~Sergey_Levine1", "~Chelsea_Finn1"], "authors": ["Marvin Mengxin Zhang", "Henrik Marklund", "Nikita Dhawan", "Abhishek Gupta", "Sergey Levine", "Chelsea Finn"], "keywords": ["meta-learning", "distribution shift", "distributional robustness", "test time adaptation"], "abstract": "A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this assumption is violated in almost all practical applications: machine learning systems are regularly tested under distribution shift, due to temporal correlations, particular end users, or other factors. In this work, we consider the setting where the training data are structured into groups and test time shifts correspond to changes in the group distribution. Prior work has approached this problem by attempting to be robust to all possible test time distributions, which may degrade average performance. In contrast, we propose to use ideas from meta-learning to learn models that are adaptable, such that they can adapt to shift at test time using a batch of unlabeled test points. We acquire such models by learning to adapt to training batches sampled according to different distributions, which simulate structural shifts that may occur at test time. Our primary contribution is to introduce the framework of adaptive risk minimization (ARM), a formalization of this setting that lends itself to meta-learning. We develop meta-learning methods for solving the ARM problem, and compared to a variety of prior methods, these methods provide substantial gains on image classification problems in the presence of shift.", "one-sentence_summary": "We meta-learn models that can use unlabeled test data to adapt to group distribution shift.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|adaptive_risk_minimization_a_metalearning_approach_for_tackling_group_shift", "pdf": "/pdf/748974539df5ba40e33b6a4cf8eaa64d773cb12d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QCjTAFdCt2", "_bibtex": "@misc{\nzhang2021adaptive,\ntitle={Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift},\nauthor={Marvin Mengxin Zhang and Henrik Marklund and Nikita Dhawan and Abhishek Gupta and Sergey Levine and Chelsea Finn},\nyear={2021},\nurl={https://openreview.net/forum?id=MA8eT-vUPvZ}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "JeamqJxOEeI", "original": null, "number": 1, "cdate": 1610040499419, "ddate": null, "tcdate": 1610040499419, "tmdate": 1610474105993, "tddate": null, "forum": "MA8eT-vUPvZ", "replyto": "MA8eT-vUPvZ", "invitation": "ICLR.cc/2021/Conference/Paper2089/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Dear Authors,\n\nThank you very much for your detailed feedback to the reviewers in the rebuttal phase. The feedback certainly clarified some of the concerns raised by the reviewers and improved their understanding of your work. Indeed, some of the reviewers have increased their scores.\n\nHowever, overall, we think this paper has rather marginal novelty and there are still several conceptual and technical issues to be further discussed, such as the definition of the grouping concept and the distributional shift assumption.\n\nFor these reasons, I suggest rejection of this paper, in comparison with many other strong submissions. I hope that the detailed feedback and additional comments from the reviewers help you improve this work for future publication.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift", "authorids": ["~Marvin_Mengxin_Zhang2", "marklund@cs.stanford.edu", "nikitadhawan@berkeley.edu", "~Abhishek_Gupta1", "~Sergey_Levine1", "~Chelsea_Finn1"], "authors": ["Marvin Mengxin Zhang", "Henrik Marklund", "Nikita Dhawan", "Abhishek Gupta", "Sergey Levine", "Chelsea Finn"], "keywords": ["meta-learning", "distribution shift", "distributional robustness", "test time adaptation"], "abstract": "A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this assumption is violated in almost all practical applications: machine learning systems are regularly tested under distribution shift, due to temporal correlations, particular end users, or other factors. In this work, we consider the setting where the training data are structured into groups and test time shifts correspond to changes in the group distribution. Prior work has approached this problem by attempting to be robust to all possible test time distributions, which may degrade average performance. In contrast, we propose to use ideas from meta-learning to learn models that are adaptable, such that they can adapt to shift at test time using a batch of unlabeled test points. We acquire such models by learning to adapt to training batches sampled according to different distributions, which simulate structural shifts that may occur at test time. Our primary contribution is to introduce the framework of adaptive risk minimization (ARM), a formalization of this setting that lends itself to meta-learning. We develop meta-learning methods for solving the ARM problem, and compared to a variety of prior methods, these methods provide substantial gains on image classification problems in the presence of shift.", "one-sentence_summary": "We meta-learn models that can use unlabeled test data to adapt to group distribution shift.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|adaptive_risk_minimization_a_metalearning_approach_for_tackling_group_shift", "pdf": "/pdf/748974539df5ba40e33b6a4cf8eaa64d773cb12d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QCjTAFdCt2", "_bibtex": "@misc{\nzhang2021adaptive,\ntitle={Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift},\nauthor={Marvin Mengxin Zhang and Henrik Marklund and Nikita Dhawan and Abhishek Gupta and Sergey Levine and Chelsea Finn},\nyear={2021},\nurl={https://openreview.net/forum?id=MA8eT-vUPvZ}\n}"}, "tags": [], "invitation": {"reply": {"forum": "MA8eT-vUPvZ", "replyto": "MA8eT-vUPvZ", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040499406, "tmdate": 1610474105977, "id": "ICLR.cc/2021/Conference/Paper2089/-/Decision"}}}, {"id": "v4I0PsRBRJk", "original": null, "number": 3, "cdate": 1604740167186, "ddate": null, "tcdate": 1604740167186, "tmdate": 1606295846488, "tddate": null, "forum": "MA8eT-vUPvZ", "replyto": "MA8eT-vUPvZ", "invitation": "ICLR.cc/2021/Conference/Paper2089/-/Official_Review", "content": {"title": "Official Blind Review #2", "review": "This paper studies domain adaptation under the assumption that only unlabeled target data is available in training and the domain shift follows a special group shift. The main idea for the proposed method is having an adaptation model that takes only the unlabeled data in and output updated parameters. The proposed method also involves the test-time training, which means the adaptation model takes in unlabeled training data in training but takes in unlabeled target data in the adaptation phase. The method is called adaptive risk minimization and there are two meta-learning approaches provided, contextual and gradient-based, in the paper. In the experiment, the proposed method outperforms a limited set of baselines. The paper also discusses a few cases when the assumption is violated like the group indicators are unknown. \n \nStrong points\n1. This paper studies an important setting in domain adaptation, similar to unsupervised multisource domain adaptation.\n2. The idea of using an adaptation component trained in a meta-learning fashion with test-time adaptation is novel.\n \nWeak points (maybe due to my confusion)\n \n \n1. Group shift definition is not very intuitive to me, especially how it would affect the proposed method. It seems to be just a different name for \u201cdomain\u201d. In the DRO setting [Hu 2018], the group information is used in optimization. However, in this paper, the group information is only used in sampling multiple group data for training and sampling single group data to train ARM-BN. Therefore, the so-called group shift in this paper seems to be a domain shift when there are multiple source domains. \n\n2. It is not clear to me if the test data is not one of the group, or is not the same distribution with one of the training distribution, how the model can be \u201cadaptive\u201d. Before the adaptation step, the model only sees unlabeled training data. In the experiment, especially for rotation mnist, it is not clear whether the test is covered in the training. I think a more principled way to do multiple random separations of rotations/corruptions -- making sure there is no overlap, and then evaluate on the test data.\n\n3. The general setting is very similar to unsupervised multi-source domain adaptation. The difference between domain adaptation to a single fixed target domain with only unlabeled data available with this so-called adaptation to shift setting is very subtle to me. The way to use unlabeled target data for adaptation in the former perspective is very rich, including MMD, importance weighting, adversarial training. More should be discussed in the paper. And many should serve as baselines.\n \nGiven the weak points, I recommend weak rejection for this paper.\n \nAdditional questions and suggestions:\n1. Instead of the group shift assumption, I think a more interesting question is: what is the assumption that will make the proposed method to work? When the model claims to adapt to a new test group that is not in the training, usually you assume the group/domain does not change the label. This seems to be the case from the data used in the paper. Another question is whether y|x is the same between training and testing, this seems to be also the case. So for meta-learning and test time training methods, clarifying their assumption is better than just casting a group shift setting to it.\n\n2. The two meta-learning methods, contextual and gradient-based, are not new. I feel they should be discussed more carefully. The novelty of this method is the incorporation of meta-learning-style training and test time adaptation for unlabeled data.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2089/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift", "authorids": ["~Marvin_Mengxin_Zhang2", "marklund@cs.stanford.edu", "nikitadhawan@berkeley.edu", "~Abhishek_Gupta1", "~Sergey_Levine1", "~Chelsea_Finn1"], "authors": ["Marvin Mengxin Zhang", "Henrik Marklund", "Nikita Dhawan", "Abhishek Gupta", "Sergey Levine", "Chelsea Finn"], "keywords": ["meta-learning", "distribution shift", "distributional robustness", "test time adaptation"], "abstract": "A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this assumption is violated in almost all practical applications: machine learning systems are regularly tested under distribution shift, due to temporal correlations, particular end users, or other factors. In this work, we consider the setting where the training data are structured into groups and test time shifts correspond to changes in the group distribution. Prior work has approached this problem by attempting to be robust to all possible test time distributions, which may degrade average performance. In contrast, we propose to use ideas from meta-learning to learn models that are adaptable, such that they can adapt to shift at test time using a batch of unlabeled test points. We acquire such models by learning to adapt to training batches sampled according to different distributions, which simulate structural shifts that may occur at test time. Our primary contribution is to introduce the framework of adaptive risk minimization (ARM), a formalization of this setting that lends itself to meta-learning. We develop meta-learning methods for solving the ARM problem, and compared to a variety of prior methods, these methods provide substantial gains on image classification problems in the presence of shift.", "one-sentence_summary": "We meta-learn models that can use unlabeled test data to adapt to group distribution shift.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|adaptive_risk_minimization_a_metalearning_approach_for_tackling_group_shift", "pdf": "/pdf/748974539df5ba40e33b6a4cf8eaa64d773cb12d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QCjTAFdCt2", "_bibtex": "@misc{\nzhang2021adaptive,\ntitle={Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift},\nauthor={Marvin Mengxin Zhang and Henrik Marklund and Nikita Dhawan and Abhishek Gupta and Sergey Levine and Chelsea Finn},\nyear={2021},\nurl={https://openreview.net/forum?id=MA8eT-vUPvZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MA8eT-vUPvZ", "replyto": "MA8eT-vUPvZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2089/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538104312, "tmdate": 1606915799957, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2089/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2089/-/Official_Review"}}}, {"id": "m-FVv9ijktx", "original": null, "number": 12, "cdate": 1606295809624, "ddate": null, "tcdate": 1606295809624, "tmdate": 1606295809624, "tddate": null, "forum": "MA8eT-vUPvZ", "replyto": "UYjcBorLjEq", "invitation": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment", "content": {"title": "Thanks for the response", "comment": "I read the response and appreciate the additional baselines. I will update to 6. \n\nHowever, two things that are still unresolved for me: (sorry about my late response, but perhaps these issues deserve more thoughts in general):\n\n1. Group shift concept: in the original group DRO paper, group information is used for providing more structure for the distributionally robust constraint sets, by \"grouping\" data together in the training data. However, in the data/domain/distribution shift field, given the terminology is already messy, I'd prefer a more careful usage of the terms, for example with more rigorous definition on the assumptions. (What should be the same and what could be different?)\n\n2. Related to the end of 1, the requirement that the distribution of batches of X and Y|X should be the same between source and the target is still vague (even maybe wrong, or ill-defined). Do you mean you regard batches of X as a big vector concatenated by all the X? And when this would be satisfied?\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2089/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift", "authorids": ["~Marvin_Mengxin_Zhang2", "marklund@cs.stanford.edu", "nikitadhawan@berkeley.edu", "~Abhishek_Gupta1", "~Sergey_Levine1", "~Chelsea_Finn1"], "authors": ["Marvin Mengxin Zhang", "Henrik Marklund", "Nikita Dhawan", "Abhishek Gupta", "Sergey Levine", "Chelsea Finn"], "keywords": ["meta-learning", "distribution shift", "distributional robustness", "test time adaptation"], "abstract": "A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this assumption is violated in almost all practical applications: machine learning systems are regularly tested under distribution shift, due to temporal correlations, particular end users, or other factors. In this work, we consider the setting where the training data are structured into groups and test time shifts correspond to changes in the group distribution. Prior work has approached this problem by attempting to be robust to all possible test time distributions, which may degrade average performance. In contrast, we propose to use ideas from meta-learning to learn models that are adaptable, such that they can adapt to shift at test time using a batch of unlabeled test points. We acquire such models by learning to adapt to training batches sampled according to different distributions, which simulate structural shifts that may occur at test time. Our primary contribution is to introduce the framework of adaptive risk minimization (ARM), a formalization of this setting that lends itself to meta-learning. We develop meta-learning methods for solving the ARM problem, and compared to a variety of prior methods, these methods provide substantial gains on image classification problems in the presence of shift.", "one-sentence_summary": "We meta-learn models that can use unlabeled test data to adapt to group distribution shift.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|adaptive_risk_minimization_a_metalearning_approach_for_tackling_group_shift", "pdf": "/pdf/748974539df5ba40e33b6a4cf8eaa64d773cb12d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QCjTAFdCt2", "_bibtex": "@misc{\nzhang2021adaptive,\ntitle={Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift},\nauthor={Marvin Mengxin Zhang and Henrik Marklund and Nikita Dhawan and Abhishek Gupta and Sergey Levine and Chelsea Finn},\nyear={2021},\nurl={https://openreview.net/forum?id=MA8eT-vUPvZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MA8eT-vUPvZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2089/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2089/Authors|ICLR.cc/2021/Conference/Paper2089/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852400, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment"}}}, {"id": "3Ua6V7IuHFY", "original": null, "number": 11, "cdate": 1606240939419, "ddate": null, "tcdate": 1606240939419, "tmdate": 1606240939419, "tddate": null, "forum": "MA8eT-vUPvZ", "replyto": "xq8BvyPBAbx", "invitation": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment", "content": {"title": "Thanks!", "comment": "Thanks very much for your response! We appreciate your concise summary of the contributions of this work."}, "signatures": ["ICLR.cc/2021/Conference/Paper2089/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift", "authorids": ["~Marvin_Mengxin_Zhang2", "marklund@cs.stanford.edu", "nikitadhawan@berkeley.edu", "~Abhishek_Gupta1", "~Sergey_Levine1", "~Chelsea_Finn1"], "authors": ["Marvin Mengxin Zhang", "Henrik Marklund", "Nikita Dhawan", "Abhishek Gupta", "Sergey Levine", "Chelsea Finn"], "keywords": ["meta-learning", "distribution shift", "distributional robustness", "test time adaptation"], "abstract": "A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this assumption is violated in almost all practical applications: machine learning systems are regularly tested under distribution shift, due to temporal correlations, particular end users, or other factors. In this work, we consider the setting where the training data are structured into groups and test time shifts correspond to changes in the group distribution. Prior work has approached this problem by attempting to be robust to all possible test time distributions, which may degrade average performance. In contrast, we propose to use ideas from meta-learning to learn models that are adaptable, such that they can adapt to shift at test time using a batch of unlabeled test points. We acquire such models by learning to adapt to training batches sampled according to different distributions, which simulate structural shifts that may occur at test time. Our primary contribution is to introduce the framework of adaptive risk minimization (ARM), a formalization of this setting that lends itself to meta-learning. We develop meta-learning methods for solving the ARM problem, and compared to a variety of prior methods, these methods provide substantial gains on image classification problems in the presence of shift.", "one-sentence_summary": "We meta-learn models that can use unlabeled test data to adapt to group distribution shift.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|adaptive_risk_minimization_a_metalearning_approach_for_tackling_group_shift", "pdf": "/pdf/748974539df5ba40e33b6a4cf8eaa64d773cb12d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QCjTAFdCt2", "_bibtex": "@misc{\nzhang2021adaptive,\ntitle={Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift},\nauthor={Marvin Mengxin Zhang and Henrik Marklund and Nikita Dhawan and Abhishek Gupta and Sergey Levine and Chelsea Finn},\nyear={2021},\nurl={https://openreview.net/forum?id=MA8eT-vUPvZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MA8eT-vUPvZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2089/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2089/Authors|ICLR.cc/2021/Conference/Paper2089/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852400, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment"}}}, {"id": "xq8BvyPBAbx", "original": null, "number": 9, "cdate": 1606163170021, "ddate": null, "tcdate": 1606163170021, "tmdate": 1606163170021, "tddate": null, "forum": "MA8eT-vUPvZ", "replyto": "23ICXkzkB-n", "invitation": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment", "content": {"title": "Though \"generalizing\" meta-learning from labeled to unlabeled adaptation is natural and simple, ARM (proposed in this work) is crucial to highlight this and present the results to the community.", "comment": "Thanks! The authors' update was well-received. I like the way how ARM formulates the unlabeled adaptation. Though \"generalizing\" meta-learning from labeled to unlabeled adaptation is natural and simple, ARM (proposed in this work) is crucial to highlight this and present the results to the community. I increased the rating to 7."}, "signatures": ["ICLR.cc/2021/Conference/Paper2089/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift", "authorids": ["~Marvin_Mengxin_Zhang2", "marklund@cs.stanford.edu", "nikitadhawan@berkeley.edu", "~Abhishek_Gupta1", "~Sergey_Levine1", "~Chelsea_Finn1"], "authors": ["Marvin Mengxin Zhang", "Henrik Marklund", "Nikita Dhawan", "Abhishek Gupta", "Sergey Levine", "Chelsea Finn"], "keywords": ["meta-learning", "distribution shift", "distributional robustness", "test time adaptation"], "abstract": "A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this assumption is violated in almost all practical applications: machine learning systems are regularly tested under distribution shift, due to temporal correlations, particular end users, or other factors. In this work, we consider the setting where the training data are structured into groups and test time shifts correspond to changes in the group distribution. Prior work has approached this problem by attempting to be robust to all possible test time distributions, which may degrade average performance. In contrast, we propose to use ideas from meta-learning to learn models that are adaptable, such that they can adapt to shift at test time using a batch of unlabeled test points. We acquire such models by learning to adapt to training batches sampled according to different distributions, which simulate structural shifts that may occur at test time. Our primary contribution is to introduce the framework of adaptive risk minimization (ARM), a formalization of this setting that lends itself to meta-learning. We develop meta-learning methods for solving the ARM problem, and compared to a variety of prior methods, these methods provide substantial gains on image classification problems in the presence of shift.", "one-sentence_summary": "We meta-learn models that can use unlabeled test data to adapt to group distribution shift.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|adaptive_risk_minimization_a_metalearning_approach_for_tackling_group_shift", "pdf": "/pdf/748974539df5ba40e33b6a4cf8eaa64d773cb12d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QCjTAFdCt2", "_bibtex": "@misc{\nzhang2021adaptive,\ntitle={Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift},\nauthor={Marvin Mengxin Zhang and Henrik Marklund and Nikita Dhawan and Abhishek Gupta and Sergey Levine and Chelsea Finn},\nyear={2021},\nurl={https://openreview.net/forum?id=MA8eT-vUPvZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MA8eT-vUPvZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2089/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2089/Authors|ICLR.cc/2021/Conference/Paper2089/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852400, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment"}}}, {"id": "0WWPGpLQaJe", "original": null, "number": 2, "cdate": 1603932291970, "ddate": null, "tcdate": 1603932291970, "tmdate": 1606163040903, "tddate": null, "forum": "MA8eT-vUPvZ", "replyto": "MA8eT-vUPvZ", "invitation": "ICLR.cc/2021/Conference/Paper2089/-/Official_Review", "content": {"title": "The authors propose adaptive risk minimization (ARM) framework to address the problem of distribution. It is a nice work. A key concern is that the proposed ARM sounds incremental to Meta-Learning approaches.", "review": "The authors propose adaptive risk minimization (ARM) framework to address the problem of distribution shift using meta-learning approaches. \n\nThe main contributions of this work are as follows:\n\n1.\tThe authors propose ARM by employing meta-learning approaches to solve distribution shift problem.\n\n2.\tThe authors have done extensive experiments to evaluate their proposed ARM algorithms. The results show that ARM outperforms multiple STOA baselines, including ERM, UW, DRNN, BN, TTT.\n\nI still have a few key concerns below that prevent me from giving an acceptance.\n\nFirstly, the overall contribution seems incremental to meta-learning approaches. The authors highlighted the difference between ARM and ERM as the constraint in eq.(1). However, eq.(1) is pretty much the same as meta-learning objective, with (i) the (task) batch loss is written as a sampling average rather than an expectation; and (ii) the adaptation is done using only x, without labels (y\u2019s). As a result, this works is applying meta-learning paradigm to solve distribution shift problem. The proposed implementations ARM-CML and ARM-LL are black-box and optimization-based meta-learning, respectively. It would be great if the authors could explain in more details of the technical contributions, if I missed anything here. \n\nSecond, in Figure 3, the results show that ARM outperforms ERM with different data samples/shots. However, the results were obtained on rotated MNIST, which is a fairly simple task, with the accuracy roughly 96.7\\%. It would be nice to show results on more complex tasks, such as Tiny ImageNet-C. Would it be the case that more shots/samples lead to higher accuracy?\n\n________________________\n\nThanks! The authors' update was well-received. I like the way how ARM formulates the unlabeled adaptation. Though \"generalizing\" meta-learning from labeled to unlabeled adaptation is natural and simple, ARM (proposed in this work) is crucial to highlight this and present the results to the community. I increased the rating to 7.\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper2089/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift", "authorids": ["~Marvin_Mengxin_Zhang2", "marklund@cs.stanford.edu", "nikitadhawan@berkeley.edu", "~Abhishek_Gupta1", "~Sergey_Levine1", "~Chelsea_Finn1"], "authors": ["Marvin Mengxin Zhang", "Henrik Marklund", "Nikita Dhawan", "Abhishek Gupta", "Sergey Levine", "Chelsea Finn"], "keywords": ["meta-learning", "distribution shift", "distributional robustness", "test time adaptation"], "abstract": "A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this assumption is violated in almost all practical applications: machine learning systems are regularly tested under distribution shift, due to temporal correlations, particular end users, or other factors. In this work, we consider the setting where the training data are structured into groups and test time shifts correspond to changes in the group distribution. Prior work has approached this problem by attempting to be robust to all possible test time distributions, which may degrade average performance. In contrast, we propose to use ideas from meta-learning to learn models that are adaptable, such that they can adapt to shift at test time using a batch of unlabeled test points. We acquire such models by learning to adapt to training batches sampled according to different distributions, which simulate structural shifts that may occur at test time. Our primary contribution is to introduce the framework of adaptive risk minimization (ARM), a formalization of this setting that lends itself to meta-learning. We develop meta-learning methods for solving the ARM problem, and compared to a variety of prior methods, these methods provide substantial gains on image classification problems in the presence of shift.", "one-sentence_summary": "We meta-learn models that can use unlabeled test data to adapt to group distribution shift.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|adaptive_risk_minimization_a_metalearning_approach_for_tackling_group_shift", "pdf": "/pdf/748974539df5ba40e33b6a4cf8eaa64d773cb12d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QCjTAFdCt2", "_bibtex": "@misc{\nzhang2021adaptive,\ntitle={Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift},\nauthor={Marvin Mengxin Zhang and Henrik Marklund and Nikita Dhawan and Abhishek Gupta and Sergey Levine and Chelsea Finn},\nyear={2021},\nurl={https://openreview.net/forum?id=MA8eT-vUPvZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MA8eT-vUPvZ", "replyto": "MA8eT-vUPvZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2089/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538104312, "tmdate": 1606915799957, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2089/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2089/-/Official_Review"}}}, {"id": "xpJGYL_RPhK", "original": null, "number": 8, "cdate": 1606083345629, "ddate": null, "tcdate": 1606083345629, "tmdate": 1606083345629, "tddate": null, "forum": "MA8eT-vUPvZ", "replyto": "MA8eT-vUPvZ", "invitation": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment", "content": {"title": "Hoping to receive responses and updated reviews", "comment": "We are still hoping that the reviewers will acknowledge our responses to your reviews and update your scores accordingly. There are only two days left in the rebuttal period, so hopefully the reviewers are satisfied by the experiments we have already provided in response to the points you have brought up. But we are still able to answer any outstanding questions and provide revisions to the paper, so we would appreciate some dialogue as to whether you have any additional concerns."}, "signatures": ["ICLR.cc/2021/Conference/Paper2089/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift", "authorids": ["~Marvin_Mengxin_Zhang2", "marklund@cs.stanford.edu", "nikitadhawan@berkeley.edu", "~Abhishek_Gupta1", "~Sergey_Levine1", "~Chelsea_Finn1"], "authors": ["Marvin Mengxin Zhang", "Henrik Marklund", "Nikita Dhawan", "Abhishek Gupta", "Sergey Levine", "Chelsea Finn"], "keywords": ["meta-learning", "distribution shift", "distributional robustness", "test time adaptation"], "abstract": "A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this assumption is violated in almost all practical applications: machine learning systems are regularly tested under distribution shift, due to temporal correlations, particular end users, or other factors. In this work, we consider the setting where the training data are structured into groups and test time shifts correspond to changes in the group distribution. Prior work has approached this problem by attempting to be robust to all possible test time distributions, which may degrade average performance. In contrast, we propose to use ideas from meta-learning to learn models that are adaptable, such that they can adapt to shift at test time using a batch of unlabeled test points. We acquire such models by learning to adapt to training batches sampled according to different distributions, which simulate structural shifts that may occur at test time. Our primary contribution is to introduce the framework of adaptive risk minimization (ARM), a formalization of this setting that lends itself to meta-learning. We develop meta-learning methods for solving the ARM problem, and compared to a variety of prior methods, these methods provide substantial gains on image classification problems in the presence of shift.", "one-sentence_summary": "We meta-learn models that can use unlabeled test data to adapt to group distribution shift.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|adaptive_risk_minimization_a_metalearning_approach_for_tackling_group_shift", "pdf": "/pdf/748974539df5ba40e33b6a4cf8eaa64d773cb12d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QCjTAFdCt2", "_bibtex": "@misc{\nzhang2021adaptive,\ntitle={Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift},\nauthor={Marvin Mengxin Zhang and Henrik Marklund and Nikita Dhawan and Abhishek Gupta and Sergey Levine and Chelsea Finn},\nyear={2021},\nurl={https://openreview.net/forum?id=MA8eT-vUPvZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MA8eT-vUPvZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2089/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2089/Authors|ICLR.cc/2021/Conference/Paper2089/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852400, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment"}}}, {"id": "kg3VG7ibH5x", "original": null, "number": 7, "cdate": 1605893589702, "ddate": null, "tcdate": 1605893589702, "tmdate": 1605893589702, "tddate": null, "forum": "MA8eT-vUPvZ", "replyto": "MA8eT-vUPvZ", "invitation": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment", "content": {"title": "Following up with all reviewers", "comment": "To all reviewers, we believe that our previous responses fully addressed your concerns, but we are happy to provide further revisions and experiments should you have any additional concerns. As the window for discussion is closing soon, we are hoping to engage with you in case there are questions and comments we can address before the end of the rebuttal period."}, "signatures": ["ICLR.cc/2021/Conference/Paper2089/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift", "authorids": ["~Marvin_Mengxin_Zhang2", "marklund@cs.stanford.edu", "nikitadhawan@berkeley.edu", "~Abhishek_Gupta1", "~Sergey_Levine1", "~Chelsea_Finn1"], "authors": ["Marvin Mengxin Zhang", "Henrik Marklund", "Nikita Dhawan", "Abhishek Gupta", "Sergey Levine", "Chelsea Finn"], "keywords": ["meta-learning", "distribution shift", "distributional robustness", "test time adaptation"], "abstract": "A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this assumption is violated in almost all practical applications: machine learning systems are regularly tested under distribution shift, due to temporal correlations, particular end users, or other factors. In this work, we consider the setting where the training data are structured into groups and test time shifts correspond to changes in the group distribution. Prior work has approached this problem by attempting to be robust to all possible test time distributions, which may degrade average performance. In contrast, we propose to use ideas from meta-learning to learn models that are adaptable, such that they can adapt to shift at test time using a batch of unlabeled test points. We acquire such models by learning to adapt to training batches sampled according to different distributions, which simulate structural shifts that may occur at test time. Our primary contribution is to introduce the framework of adaptive risk minimization (ARM), a formalization of this setting that lends itself to meta-learning. We develop meta-learning methods for solving the ARM problem, and compared to a variety of prior methods, these methods provide substantial gains on image classification problems in the presence of shift.", "one-sentence_summary": "We meta-learn models that can use unlabeled test data to adapt to group distribution shift.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|adaptive_risk_minimization_a_metalearning_approach_for_tackling_group_shift", "pdf": "/pdf/748974539df5ba40e33b6a4cf8eaa64d773cb12d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QCjTAFdCt2", "_bibtex": "@misc{\nzhang2021adaptive,\ntitle={Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift},\nauthor={Marvin Mengxin Zhang and Henrik Marklund and Nikita Dhawan and Abhishek Gupta and Sergey Levine and Chelsea Finn},\nyear={2021},\nurl={https://openreview.net/forum?id=MA8eT-vUPvZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MA8eT-vUPvZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2089/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2089/Authors|ICLR.cc/2021/Conference/Paper2089/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852400, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment"}}}, {"id": "yOzdJIS2KY-", "original": null, "number": 6, "cdate": 1605775628422, "ddate": null, "tcdate": 1605775628422, "tmdate": 1605775628422, "tddate": null, "forum": "MA8eT-vUPvZ", "replyto": "joiuCE8XslM", "invitation": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment", "content": {"title": "Following up on your score update", "comment": "Thank you for increasing your overall score! As it is still borderline, we are wondering whether our previous response fully addressed your concerns and if you have any additional concerns we can address before the end of the rebuttal period. We would appreciate a dialogue in order to both improve the paper and reach a more satisfying consensus.\n\nWe have revised the paper according to the comments from the other two reviewers, and we believe that this happened after your score update. One revision that is relevant to your comments from earlier is that we have added comparisons to MMD [Li et al, CVPR \u201818] and DANN [Ganin et al, JMLR \u201816] in Section 4, which are both benchmark methods in DomainBed. Our implementations are based heavily off of the DomainBed implementations, and we tune the same hyperparameters including learning rates and adversarial classifier strength. However, we still observe generally worse performance on our testbeds from these methods compared to ARM methods. We believe that this lends additional evidence to the notion that the ARM setting is different from the typical domain generalization setting -- in practice, different methods will perform well in these settings, which are both realistic and worthy of further study.\n\nWe look forward to your response and hope we can engage in a meaningful discussion."}, "signatures": ["ICLR.cc/2021/Conference/Paper2089/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift", "authorids": ["~Marvin_Mengxin_Zhang2", "marklund@cs.stanford.edu", "nikitadhawan@berkeley.edu", "~Abhishek_Gupta1", "~Sergey_Levine1", "~Chelsea_Finn1"], "authors": ["Marvin Mengxin Zhang", "Henrik Marklund", "Nikita Dhawan", "Abhishek Gupta", "Sergey Levine", "Chelsea Finn"], "keywords": ["meta-learning", "distribution shift", "distributional robustness", "test time adaptation"], "abstract": "A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this assumption is violated in almost all practical applications: machine learning systems are regularly tested under distribution shift, due to temporal correlations, particular end users, or other factors. In this work, we consider the setting where the training data are structured into groups and test time shifts correspond to changes in the group distribution. Prior work has approached this problem by attempting to be robust to all possible test time distributions, which may degrade average performance. In contrast, we propose to use ideas from meta-learning to learn models that are adaptable, such that they can adapt to shift at test time using a batch of unlabeled test points. We acquire such models by learning to adapt to training batches sampled according to different distributions, which simulate structural shifts that may occur at test time. Our primary contribution is to introduce the framework of adaptive risk minimization (ARM), a formalization of this setting that lends itself to meta-learning. We develop meta-learning methods for solving the ARM problem, and compared to a variety of prior methods, these methods provide substantial gains on image classification problems in the presence of shift.", "one-sentence_summary": "We meta-learn models that can use unlabeled test data to adapt to group distribution shift.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|adaptive_risk_minimization_a_metalearning_approach_for_tackling_group_shift", "pdf": "/pdf/748974539df5ba40e33b6a4cf8eaa64d773cb12d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QCjTAFdCt2", "_bibtex": "@misc{\nzhang2021adaptive,\ntitle={Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift},\nauthor={Marvin Mengxin Zhang and Henrik Marklund and Nikita Dhawan and Abhishek Gupta and Sergey Levine and Chelsea Finn},\nyear={2021},\nurl={https://openreview.net/forum?id=MA8eT-vUPvZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MA8eT-vUPvZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2089/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2089/Authors|ICLR.cc/2021/Conference/Paper2089/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852400, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment"}}}, {"id": "joiuCE8XslM", "original": null, "number": 1, "cdate": 1603287986605, "ddate": null, "tcdate": 1603287986605, "tmdate": 1605589810177, "tddate": null, "forum": "MA8eT-vUPvZ", "replyto": "MA8eT-vUPvZ", "invitation": "ICLR.cc/2021/Conference/Paper2089/-/Official_Review", "content": {"title": "Two main concerns to be addressed.", "review": "Paper summary:\n\nThe authors try to tackle the *distribution shift* problem with a meta learning approach. The algorithm, namely ARM, is proposed. Following regular meta learning regime, ARM uses an updated version of parameter $\\theta'$ to calculate the loss for back propagation. Several specific implementations are put forward, i.e. the contextual / gradient-based methods. Experiments are performed in small and large scale datasets to demonstrate the effectiveness of the proposed algorithm. Detailed ablation study and qualitative analysis are also conducted.\n\nComments:\n\nThis paper has clear writing and well established methodology. However, the main drawback to me is the lack of novelty. Out-of-distribution generalization and meta learning are both popular areas in the current machine learning community, but a simple combination might not be able to convince me.\n\nA second concern about ARM method is the reliability of the empirical experiments. The [DomainBed](https://github.com/facebookresearch/DomainBed) is a suite which could conduct systematical evaluation for domain shift algorithms. In its [public results](https://github.com/facebookresearch/DomainBed/blob/master/domainbed/results/2020_10_06_7df6f06/results.tex), ARM's performance is consistently worse than ERM (like many other algorithms who claim to improve on OoD setting), in all three validation method (training-domain validation, leave one out, oracle). I am familiar with DomainBed's implementation, so for me this difference indicates that the authors might pick the hyper-parameters and datasets based on testsets, as DomainBed is different from regular evaluation for its random parameter picking and extensive datasets searching. If authors could resolve this concern properly, I will consider raising my score.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2089/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift", "authorids": ["~Marvin_Mengxin_Zhang2", "marklund@cs.stanford.edu", "nikitadhawan@berkeley.edu", "~Abhishek_Gupta1", "~Sergey_Levine1", "~Chelsea_Finn1"], "authors": ["Marvin Mengxin Zhang", "Henrik Marklund", "Nikita Dhawan", "Abhishek Gupta", "Sergey Levine", "Chelsea Finn"], "keywords": ["meta-learning", "distribution shift", "distributional robustness", "test time adaptation"], "abstract": "A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this assumption is violated in almost all practical applications: machine learning systems are regularly tested under distribution shift, due to temporal correlations, particular end users, or other factors. In this work, we consider the setting where the training data are structured into groups and test time shifts correspond to changes in the group distribution. Prior work has approached this problem by attempting to be robust to all possible test time distributions, which may degrade average performance. In contrast, we propose to use ideas from meta-learning to learn models that are adaptable, such that they can adapt to shift at test time using a batch of unlabeled test points. We acquire such models by learning to adapt to training batches sampled according to different distributions, which simulate structural shifts that may occur at test time. Our primary contribution is to introduce the framework of adaptive risk minimization (ARM), a formalization of this setting that lends itself to meta-learning. We develop meta-learning methods for solving the ARM problem, and compared to a variety of prior methods, these methods provide substantial gains on image classification problems in the presence of shift.", "one-sentence_summary": "We meta-learn models that can use unlabeled test data to adapt to group distribution shift.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|adaptive_risk_minimization_a_metalearning_approach_for_tackling_group_shift", "pdf": "/pdf/748974539df5ba40e33b6a4cf8eaa64d773cb12d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QCjTAFdCt2", "_bibtex": "@misc{\nzhang2021adaptive,\ntitle={Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift},\nauthor={Marvin Mengxin Zhang and Henrik Marklund and Nikita Dhawan and Abhishek Gupta and Sergey Levine and Chelsea Finn},\nyear={2021},\nurl={https://openreview.net/forum?id=MA8eT-vUPvZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MA8eT-vUPvZ", "replyto": "MA8eT-vUPvZ", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2089/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538104312, "tmdate": 1606915799957, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2089/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2089/-/Official_Review"}}}, {"id": "23ICXkzkB-n", "original": null, "number": 5, "cdate": 1605565313593, "ddate": null, "tcdate": 1605565313593, "tmdate": 1605565313593, "tddate": null, "forum": "MA8eT-vUPvZ", "replyto": "0WWPGpLQaJe", "invitation": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment", "content": {"title": "Author response to R1", "comment": "Thank you for your positive comments and detailed inquiries! We have revised the paper to highlight the main contributions and added the requested experiment in Section 4.4. We believe that these changes and the responses below address your key concerns, please let us know if otherwise.\n\n- \u201cFirstly, the overall contribution seems incremental to meta-learning approaches\u2026 It would be great if the authors could explain in more details of the technical contributions, if I missed anything here.\u201d\n\nYou are correct in many of the points in this paragraph. However, we do not view this work as being either incremental or not incremental in terms of meta-learning approaches themselves, as the goal is not to propose new meta-learning approaches. The main novel contribution, rather, is the formalization of how the *framework* of meta-learning can already provide better solutions to distribution shift problems than current SOTA methods, with the goal of improving the SOTA for tackling distribution shift problems. We have clarified the exposition in Sections 1-3 to better reflect this core contribution.\n\nMeta-learning has primarily focused on the labeled adaptation setting. Thus the ARM problem formulation *is* a technical contribution, as it provides a framework for extending meta-learning tools to unlabeled adaptation. We demonstrate this by developing ARM-BN, which is a novel meta-learning method for this setting that is simple and effective in practice. Through these contributions, we are able to achieve substantially better empirical results compared to SOTA methods (~1-4% average, ~4-7% worst case) on a range of distribution shift problems.\n\n- \u201cIt would be nice to show results on more complex tasks, such as Tiny ImageNet-C. Would it be the case that more shots/samples lead to higher accuracy?\u201d\n\nWe now include results in Section 4.4 that test ARM-CML and ARM-BN in this setting on Tiny ImageNet-C. We observe the same trends as for the rotated MNIST experiments, affirming that, in the absence of test batches, ARM methods are still able to adapt quickly to new test settings, often with as few as 25 unlabeled test examples for this domain."}, "signatures": ["ICLR.cc/2021/Conference/Paper2089/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift", "authorids": ["~Marvin_Mengxin_Zhang2", "marklund@cs.stanford.edu", "nikitadhawan@berkeley.edu", "~Abhishek_Gupta1", "~Sergey_Levine1", "~Chelsea_Finn1"], "authors": ["Marvin Mengxin Zhang", "Henrik Marklund", "Nikita Dhawan", "Abhishek Gupta", "Sergey Levine", "Chelsea Finn"], "keywords": ["meta-learning", "distribution shift", "distributional robustness", "test time adaptation"], "abstract": "A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this assumption is violated in almost all practical applications: machine learning systems are regularly tested under distribution shift, due to temporal correlations, particular end users, or other factors. In this work, we consider the setting where the training data are structured into groups and test time shifts correspond to changes in the group distribution. Prior work has approached this problem by attempting to be robust to all possible test time distributions, which may degrade average performance. In contrast, we propose to use ideas from meta-learning to learn models that are adaptable, such that they can adapt to shift at test time using a batch of unlabeled test points. We acquire such models by learning to adapt to training batches sampled according to different distributions, which simulate structural shifts that may occur at test time. Our primary contribution is to introduce the framework of adaptive risk minimization (ARM), a formalization of this setting that lends itself to meta-learning. We develop meta-learning methods for solving the ARM problem, and compared to a variety of prior methods, these methods provide substantial gains on image classification problems in the presence of shift.", "one-sentence_summary": "We meta-learn models that can use unlabeled test data to adapt to group distribution shift.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|adaptive_risk_minimization_a_metalearning_approach_for_tackling_group_shift", "pdf": "/pdf/748974539df5ba40e33b6a4cf8eaa64d773cb12d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QCjTAFdCt2", "_bibtex": "@misc{\nzhang2021adaptive,\ntitle={Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift},\nauthor={Marvin Mengxin Zhang and Henrik Marklund and Nikita Dhawan and Abhishek Gupta and Sergey Levine and Chelsea Finn},\nyear={2021},\nurl={https://openreview.net/forum?id=MA8eT-vUPvZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MA8eT-vUPvZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2089/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2089/Authors|ICLR.cc/2021/Conference/Paper2089/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852400, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment"}}}, {"id": "UYjcBorLjEq", "original": null, "number": 4, "cdate": 1605565200287, "ddate": null, "tcdate": 1605565200287, "tmdate": 1605565200287, "tddate": null, "forum": "MA8eT-vUPvZ", "replyto": "v4I0PsRBRJk", "invitation": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment", "content": {"title": "Author response to R2", "comment": "Thank you for your insightful comments and feedback! We have revised the paper according to your feedback and added the requested comparisons in Section 4. We believe that these changes and the responses below address your key concerns, please let us know if otherwise.\n\n- \u201cGroup shift definition is not very intuitive to me, especially how it would affect the proposed method. It seems to be just a different name for \u2018domain\u2019.\u201d\n\nYou are correct. Groups can correspond to users, domains, subpopulations etc. We adopt the term following prior work [Sagawa et al, ICLR \u201820], as we believe it is an appropriate term for generally encompassing users, corruptions, and related concepts.\n\n- \u201cIt is not clear to me if the test data is not one of the group, or is not the same distribution with one of the training distribution, how the model can be \u2018adaptive\u2019.\u201d\n- \u201cInstead of the group shift assumption, I think a more interesting question is: what is the assumption that will make the proposed method to work?\u201d\n\nThis is a good point, and we added some discussion for this in Section 3.2. The assumption is similar to what you have suggested, though slightly more general -- we require that the distributions over *batches* **X** and **y|X** match between train and test. Note that the distributions over individual data points **x** and y|**x** can still shift, illustrating the benefit of meta-learning to adapt to multiple unlabeled data points. In practice, possibly due to the generalization capability of neural networks, we find that this assumption only needs to hold loosely. Similar to how typical meta-learning for few shot classification have found that meta-learned models can generalize to new meta-test classes, we find that models trained with ARM methods can generalize to new test groups.\n\n- \u201cIn the experiment, especially for rotation mnist, it is not clear whether the test is covered in the training. I think a more principled way to do multiple random separations of rotations/corruptions -- making sure there is no overlap, and then evaluate on the test data.\u201d\n\nRotated MNIST is set up slightly differently from the other experiments, in that the training and test groups are the same (but not the datapoints in those groups). The challenge is that some groups are heavily underrepresented in the training data, thus models trained with ERM will have poor test accuracy on those groups. The point of this experiment differs from the other three experiments, which as you have suggested, evaluates the situation in which there is no overlap between train and test groups.\n\n- \u201cThe general setting is very similar to unsupervised multi-source domain adaptation.\u201d\n\nIn the three experiments with no overlap between train and test groups, the primary difference between the ARM setting and domain adaptation is that we do not assume access to any examples, labeled or unlabeled, from the target domain(s) at training time. This complicates the usage of typical domain adaptation methods such as importance weighting.\n\nHowever, the ARM setting does bear some resemblance to domain generalization, which is perhaps what you are referring to as \u201cmulti-source domain adaptation\u201d. Among the methods you pointed out, MMD and adversarial training are also applicable in this setting. Thus, we have added comparisons to MMD [Li et al, CVPR \u201818] and DANN [Ganin et al, JMLR \u201816] in Section 4, and we added some discussion of these methods to Section 2. ARM methods also consistently outperform these methods on all testbeds. Thank you for pointing out this important set of comparisons.\n\n- \u201cThe two meta-learning methods, contextual and gradient-based, are not new. I feel they should be discussed more carefully.\u201d\n\nThank you, we revised Section 3.3 with this in mind. We appreciate your clear summary of the novelty of this work."}, "signatures": ["ICLR.cc/2021/Conference/Paper2089/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift", "authorids": ["~Marvin_Mengxin_Zhang2", "marklund@cs.stanford.edu", "nikitadhawan@berkeley.edu", "~Abhishek_Gupta1", "~Sergey_Levine1", "~Chelsea_Finn1"], "authors": ["Marvin Mengxin Zhang", "Henrik Marklund", "Nikita Dhawan", "Abhishek Gupta", "Sergey Levine", "Chelsea Finn"], "keywords": ["meta-learning", "distribution shift", "distributional robustness", "test time adaptation"], "abstract": "A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this assumption is violated in almost all practical applications: machine learning systems are regularly tested under distribution shift, due to temporal correlations, particular end users, or other factors. In this work, we consider the setting where the training data are structured into groups and test time shifts correspond to changes in the group distribution. Prior work has approached this problem by attempting to be robust to all possible test time distributions, which may degrade average performance. In contrast, we propose to use ideas from meta-learning to learn models that are adaptable, such that they can adapt to shift at test time using a batch of unlabeled test points. We acquire such models by learning to adapt to training batches sampled according to different distributions, which simulate structural shifts that may occur at test time. Our primary contribution is to introduce the framework of adaptive risk minimization (ARM), a formalization of this setting that lends itself to meta-learning. We develop meta-learning methods for solving the ARM problem, and compared to a variety of prior methods, these methods provide substantial gains on image classification problems in the presence of shift.", "one-sentence_summary": "We meta-learn models that can use unlabeled test data to adapt to group distribution shift.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|adaptive_risk_minimization_a_metalearning_approach_for_tackling_group_shift", "pdf": "/pdf/748974539df5ba40e33b6a4cf8eaa64d773cb12d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QCjTAFdCt2", "_bibtex": "@misc{\nzhang2021adaptive,\ntitle={Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift},\nauthor={Marvin Mengxin Zhang and Henrik Marklund and Nikita Dhawan and Abhishek Gupta and Sergey Levine and Chelsea Finn},\nyear={2021},\nurl={https://openreview.net/forum?id=MA8eT-vUPvZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MA8eT-vUPvZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2089/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2089/Authors|ICLR.cc/2021/Conference/Paper2089/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852400, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment"}}}, {"id": "pa4vb0DGb45", "original": null, "number": 3, "cdate": 1605385466185, "ddate": null, "tcdate": 1605385466185, "tmdate": 1605385466185, "tddate": null, "forum": "MA8eT-vUPvZ", "replyto": "joiuCE8XslM", "invitation": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment", "content": {"title": "Author response to R3", "comment": "Thank you for your helpful feedback and comments! We have revised the paper to highlight the main contributions and clarify our experimental protocol. We believe that these changes and the responses below address your key concerns, and please let us know if otherwise.\n\n- \u201cHowever, the main drawback to me is the lack of novelty.\u201d\n\nThe core novel contribution of this work is to formalize how the framework of meta-learning can already provide better solutions to distribution shift problems than current state of the art (SOTA) methods, with the goal of improving the SOTA for tackling distribution shift problems. We have clarified the exposition in Sections 1-3 to better reflect this core contribution. We agree that this contribution seems simple, however, we view the simplicity as a strength, as it provides a framework for extending meta-learning tools to unlabeled adaptation. We demonstrate this by developing ARM-BN, which is a novel meta-learning method for this setting that is simple and effective in practice. Through these contributions, we are able to achieve substantially better empirical results compared to SOTA methods (~1-4% average, ~4-7% worst case) on a range of distribution shift problems.\n\n- \u201cA second concern about ARM method is the reliability of the empirical experiments\u2026 If authors could resolve this concern properly, I will consider raising my score.\u201d\n\nThank you for the opportunity to address this valid concern. First, we note that we definitely do *not*, in any of our experiments, choose hyperparameters, datasets, or anything else based on test results. We now include additional details of our protocol at the top of Appendix B. To summarize briefly, we picked the dataset splits once before running any experiments, and we used only validation performance for hyperparameter selection.\n\nAs for the DomainBed results, we believe that, although the domain generalization setting is similar to the ARM setting, there are two critical differences. First, the testbeds in DomainBed are primarily focused on complete OOD generalization, e.g., using photos and sketches to generalize to cartoons. As evidenced by the papers that first introduced these datasets, these problems were often designed to test methods for invariance. This leads to the second key difference, which is that our testbeds feature an order of magnitude more source domains, with less data per domain and more overlap between domains. We believe that this setting presents a different type of distribution shift problem that offers algorithms greater leverage for tackling the shift, while still pertaining to relevant problems (e.g., federated learning). Further, we believe that it is precisely this setting where multiple data points may provide crucial information about the domain (e.g., the user), thus leading to greater adaptation performance. We see evidence of this in the DomainBed results themselves, as ARM is clearly superior in the colored MNIST setting from IRM, where adaptation provides a significant advantage in determining the underlying color correlations. Thus, we believe that both of these types of problems are worth further study, as different types of methods may perform strongly for these problems.\n\nWe believe that this should resolve your concern about our empirical results, and again, please let us know if otherwise."}, "signatures": ["ICLR.cc/2021/Conference/Paper2089/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift", "authorids": ["~Marvin_Mengxin_Zhang2", "marklund@cs.stanford.edu", "nikitadhawan@berkeley.edu", "~Abhishek_Gupta1", "~Sergey_Levine1", "~Chelsea_Finn1"], "authors": ["Marvin Mengxin Zhang", "Henrik Marklund", "Nikita Dhawan", "Abhishek Gupta", "Sergey Levine", "Chelsea Finn"], "keywords": ["meta-learning", "distribution shift", "distributional robustness", "test time adaptation"], "abstract": "A fundamental assumption of most machine learning algorithms is that the training and test data are drawn from the same underlying distribution. However, this assumption is violated in almost all practical applications: machine learning systems are regularly tested under distribution shift, due to temporal correlations, particular end users, or other factors. In this work, we consider the setting where the training data are structured into groups and test time shifts correspond to changes in the group distribution. Prior work has approached this problem by attempting to be robust to all possible test time distributions, which may degrade average performance. In contrast, we propose to use ideas from meta-learning to learn models that are adaptable, such that they can adapt to shift at test time using a batch of unlabeled test points. We acquire such models by learning to adapt to training batches sampled according to different distributions, which simulate structural shifts that may occur at test time. Our primary contribution is to introduce the framework of adaptive risk minimization (ARM), a formalization of this setting that lends itself to meta-learning. We develop meta-learning methods for solving the ARM problem, and compared to a variety of prior methods, these methods provide substantial gains on image classification problems in the presence of shift.", "one-sentence_summary": "We meta-learn models that can use unlabeled test data to adapt to group distribution shift.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhang|adaptive_risk_minimization_a_metalearning_approach_for_tackling_group_shift", "pdf": "/pdf/748974539df5ba40e33b6a4cf8eaa64d773cb12d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=QCjTAFdCt2", "_bibtex": "@misc{\nzhang2021adaptive,\ntitle={Adaptive Risk Minimization: A Meta-Learning Approach for Tackling Group Shift},\nauthor={Marvin Mengxin Zhang and Henrik Marklund and Nikita Dhawan and Abhishek Gupta and Sergey Levine and Chelsea Finn},\nyear={2021},\nurl={https://openreview.net/forum?id=MA8eT-vUPvZ}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MA8eT-vUPvZ", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2089/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2089/Authors|ICLR.cc/2021/Conference/Paper2089/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2089/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852400, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2089/-/Official_Comment"}}}], "count": 14}