{"notes": [{"id": "B1EjKsRqtQ", "original": "SJgmSSocK7", "number": 478, "cdate": 1538087811336, "ddate": null, "tcdate": 1538087811336, "tmdate": 1545355395281, "tddate": null, "forum": "B1EjKsRqtQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Hierarchical Attention: What Really Counts in Various NLP Tasks", "abstract": "Attention mechanisms in sequence to sequence models have shown great ability and wonderful performance in various natural language processing  (NLP)  tasks, such as sentence embedding, text generation, machine translation, machine reading comprehension, etc. Unfortunately, existing attention mechanisms only learn either high-level or low-level features. In this paper, we think that the lack of hierarchical mechanisms is a bottleneck in improving the performance of the attention mechanisms, and propose a novel Hierarchical Attention Mechanism (Ham) based on the weighted sum of different layers of a multi-level attention. \nHam achieves a state-of-the-art BLEU score of 0.26 on Chinese poem generation task and a nearly 6.5% averaged improvement compared with the existing machine reading comprehension models such as BIDAF and Match-LSTM. Furthermore, our experiments and theorems reveal that Ham has greater generalization and representation ability than existing attention mechanisms. ", "keywords": ["attention", "hierarchical", "machine reading comprehension", "poem generation"], "authorids": ["zehaodou@pku.edu.cn", "zhzhang@math.pku.edu.cn"], "authors": ["Zehao Dou", "Zhihua Zhang"], "TL;DR": "The paper proposed a novel hierarchical model to replace the original attention model in various NLP tasks.", "pdf": "/pdf/2de1a2d23536899248aea15fe0c19a5a9ea7cd65.pdf", "paperhash": "dou|hierarchical_attention_what_really_counts_in_various_nlp_tasks", "_bibtex": "@misc{\ndou2019hierarchical,\ntitle={Hierarchical Attention: What Really Counts in Various {NLP} Tasks},\nauthor={Zehao Dou and Zhihua Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=B1EjKsRqtQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "S1lQ-WYexV", "original": null, "number": 1, "cdate": 1544749307264, "ddate": null, "tcdate": 1544749307264, "tmdate": 1545354516191, "tddate": null, "forum": "B1EjKsRqtQ", "replyto": "B1EjKsRqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper478/Meta_Review", "content": {"metareview": "The authors propose a hierarchical attention layer which combines intermediate layers of multi-level attention. While this is a simple idea, and the authors show some improvements over the baselines, the authors raised a number of concerns about the validity of the chosen baselines, and the lack of more detailed evaluations on additional tasks and analysis of the results. Given the incremental nature of the work, and the significant concerns raised by the reviewers, the AC is recommending that this paper be rejected.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Incremental work, with limited experimental validation"}, "signatures": ["ICLR.cc/2019/Conference/Paper478/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper478/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Attention: What Really Counts in Various NLP Tasks", "abstract": "Attention mechanisms in sequence to sequence models have shown great ability and wonderful performance in various natural language processing  (NLP)  tasks, such as sentence embedding, text generation, machine translation, machine reading comprehension, etc. Unfortunately, existing attention mechanisms only learn either high-level or low-level features. In this paper, we think that the lack of hierarchical mechanisms is a bottleneck in improving the performance of the attention mechanisms, and propose a novel Hierarchical Attention Mechanism (Ham) based on the weighted sum of different layers of a multi-level attention. \nHam achieves a state-of-the-art BLEU score of 0.26 on Chinese poem generation task and a nearly 6.5% averaged improvement compared with the existing machine reading comprehension models such as BIDAF and Match-LSTM. Furthermore, our experiments and theorems reveal that Ham has greater generalization and representation ability than existing attention mechanisms. ", "keywords": ["attention", "hierarchical", "machine reading comprehension", "poem generation"], "authorids": ["zehaodou@pku.edu.cn", "zhzhang@math.pku.edu.cn"], "authors": ["Zehao Dou", "Zhihua Zhang"], "TL;DR": "The paper proposed a novel hierarchical model to replace the original attention model in various NLP tasks.", "pdf": "/pdf/2de1a2d23536899248aea15fe0c19a5a9ea7cd65.pdf", "paperhash": "dou|hierarchical_attention_what_really_counts_in_various_nlp_tasks", "_bibtex": "@misc{\ndou2019hierarchical,\ntitle={Hierarchical Attention: What Really Counts in Various {NLP} Tasks},\nauthor={Zehao Dou and Zhihua Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=B1EjKsRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper478/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353203141, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1EjKsRqtQ", "replyto": "B1EjKsRqtQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper478/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper478/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper478/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353203141}}}, {"id": "Sylhf4LAh7", "original": null, "number": 3, "cdate": 1541461012147, "ddate": null, "tcdate": 1541461012147, "tmdate": 1541533962318, "tddate": null, "forum": "B1EjKsRqtQ", "replyto": "B1EjKsRqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper478/Official_Review", "content": {"title": "Lacks Novelty , incomplete results", "review": "Overall, this is an incremental paper.\nThe authors propose a hierarchical attention layer, which computes an aggregation of self attention layer outputs in the multi level attention model. This seems like a small improvement.\n\nThere are results using this hierarchical attention layer instead of the vanilla attention layers on Machine Reading Comprehension and Chinese Poem Generation. The authors should have also included results on more tasks to show the clear improvement of the proposed method.\n\nThe issues with this paper are:\n- Aggregating weights of different layers has been an idea explored before (Elmo, Cove, etc.). So the model improvement itself seems small.\n- Lack of strong experimental evidence. In my regard, the experiments are somewhat incomplete. In both the tasks, the authors compare only the vanilla model (BIDAF, MatchLSTM, R-NET) and the model with HAM layers. It is not clear where the improvement is coming from. It would have made sense to compare the number of parameters and also, using the same number of vanilla attention layers  which outputs the last layer and compare it to the one proposed by the authors.\n- Since the argument is towards using weighted average rather than the last layer, there should have been a more detailed analysis on what was the weight distribution and on how important were representations from different layers.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper478/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Attention: What Really Counts in Various NLP Tasks", "abstract": "Attention mechanisms in sequence to sequence models have shown great ability and wonderful performance in various natural language processing  (NLP)  tasks, such as sentence embedding, text generation, machine translation, machine reading comprehension, etc. Unfortunately, existing attention mechanisms only learn either high-level or low-level features. In this paper, we think that the lack of hierarchical mechanisms is a bottleneck in improving the performance of the attention mechanisms, and propose a novel Hierarchical Attention Mechanism (Ham) based on the weighted sum of different layers of a multi-level attention. \nHam achieves a state-of-the-art BLEU score of 0.26 on Chinese poem generation task and a nearly 6.5% averaged improvement compared with the existing machine reading comprehension models such as BIDAF and Match-LSTM. Furthermore, our experiments and theorems reveal that Ham has greater generalization and representation ability than existing attention mechanisms. ", "keywords": ["attention", "hierarchical", "machine reading comprehension", "poem generation"], "authorids": ["zehaodou@pku.edu.cn", "zhzhang@math.pku.edu.cn"], "authors": ["Zehao Dou", "Zhihua Zhang"], "TL;DR": "The paper proposed a novel hierarchical model to replace the original attention model in various NLP tasks.", "pdf": "/pdf/2de1a2d23536899248aea15fe0c19a5a9ea7cd65.pdf", "paperhash": "dou|hierarchical_attention_what_really_counts_in_various_nlp_tasks", "_bibtex": "@misc{\ndou2019hierarchical,\ntitle={Hierarchical Attention: What Really Counts in Various {NLP} Tasks},\nauthor={Zehao Dou and Zhihua Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=B1EjKsRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper478/Official_Review", "cdate": 1542234452264, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1EjKsRqtQ", "replyto": "B1EjKsRqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper478/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335733344, "tmdate": 1552335733344, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper478/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1eHy_bahX", "original": null, "number": 2, "cdate": 1541375965247, "ddate": null, "tcdate": 1541375965247, "tmdate": 1541533962102, "tddate": null, "forum": "B1EjKsRqtQ", "replyto": "B1EjKsRqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper478/Official_Review", "content": {"title": "A few major issues", "review": "The paper proposes to enhance existing multi-level attention (self-attention) mechanism by obtaining query and key vectors (= value vectors) from all levels after weighted-averaging them. The paper claims that this is also theoretically beneficial because the loss function will converge to zero as the number of layers increase. It claims that the proposed architecture outperforms existing attention-based models in English MRC test (SQuAD), Chinese MRC test, and Chinese poem generation task.\n\nI find three major issues in the paper.\n\n1.  I think the proposed hypothesis lacks the novelty that ICLR audience seeks for. Through many existing architectures (ResNet, ELMo), we already know that skip connection between CNN layers or weighted average of multiple LSTM layers could improve model significantly. Perhaps this could be an application paper that brings existing methods to a slightly different (attention) domain, but not only such paper is less suitable for ICLR, but also it would require strong experimental results. But as I will detail in the second point, I also have some worries about the experiments. \n\n2. The experimental results have problems. For English MRC experiment (SQuAD), the reproduced match-LSTM score is ~10% below the reported number in its original paper. Furthermore, it is not clear whether the improvement comes from having multiple attention layers (which is not novel) or weighted-averaging the attention layers (the proposed method). BiDAF and match-LSTM have single attention layers, so it is not fair to compare them with multi-layer attention. \n\n3. Lastly, I am not sure I understood the theoretical section correctly, but it is not much interesting that having multiple layers allow one to approach closer to zero loss. In fact, any sufficiently large model can obtain close-to-zero loss on the training data. This is not a sufficient condition for a good model. We cannot guarantee if the model has generalized well; it might have just overfit to the training data.\n\nA few minor issues and typos  on the paper:\n- First para second sentence: In -> in\n- First para second sentence: sequence to sequence -> sequence-to-sequence\n- Second last para of intro: sentence fragment\n- Figure 3: would be good to have English translation. \n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper478/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Attention: What Really Counts in Various NLP Tasks", "abstract": "Attention mechanisms in sequence to sequence models have shown great ability and wonderful performance in various natural language processing  (NLP)  tasks, such as sentence embedding, text generation, machine translation, machine reading comprehension, etc. Unfortunately, existing attention mechanisms only learn either high-level or low-level features. In this paper, we think that the lack of hierarchical mechanisms is a bottleneck in improving the performance of the attention mechanisms, and propose a novel Hierarchical Attention Mechanism (Ham) based on the weighted sum of different layers of a multi-level attention. \nHam achieves a state-of-the-art BLEU score of 0.26 on Chinese poem generation task and a nearly 6.5% averaged improvement compared with the existing machine reading comprehension models such as BIDAF and Match-LSTM. Furthermore, our experiments and theorems reveal that Ham has greater generalization and representation ability than existing attention mechanisms. ", "keywords": ["attention", "hierarchical", "machine reading comprehension", "poem generation"], "authorids": ["zehaodou@pku.edu.cn", "zhzhang@math.pku.edu.cn"], "authors": ["Zehao Dou", "Zhihua Zhang"], "TL;DR": "The paper proposed a novel hierarchical model to replace the original attention model in various NLP tasks.", "pdf": "/pdf/2de1a2d23536899248aea15fe0c19a5a9ea7cd65.pdf", "paperhash": "dou|hierarchical_attention_what_really_counts_in_various_nlp_tasks", "_bibtex": "@misc{\ndou2019hierarchical,\ntitle={Hierarchical Attention: What Really Counts in Various {NLP} Tasks},\nauthor={Zehao Dou and Zhihua Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=B1EjKsRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper478/Official_Review", "cdate": 1542234452264, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1EjKsRqtQ", "replyto": "B1EjKsRqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper478/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335733344, "tmdate": 1552335733344, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper478/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkeUUGo937", "original": null, "number": 1, "cdate": 1541218893949, "ddate": null, "tcdate": 1541218893949, "tmdate": 1541533961884, "tddate": null, "forum": "B1EjKsRqtQ", "replyto": "B1EjKsRqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper478/Official_Review", "content": {"title": "A simple extension of multi-level attention, but needs more extensive comparison to existing methods", "review": "The paper introduces hierarchical attention, where they propose to weighted combine all the intermediate layers of multi-level attention. The idea is simple and seems to be promising, however originality seems incremental.\n\nIn order to fully demonstrate the significance of the proposed algorithm, the authors should conduct more comparisons, for example, to multi-level attention. Just comparing with one-level attention seems unfair given the significant increase of computation. Another aspect of comparison may be to consider computation and performance improvements together and discuss the best trade-off. The authors should also include some standard benchmark datasets for comparisons. The current ones are good but it is not so clear what is the best state-of-the-arts results on them when compared with all other methods.\n\nThe analysis on the network's representation and convergence is nice but it does not bring much insights. The argument for decreasing global minimal of the loss function in terms of increasing parameter size can be made for nearly all models but it is of little practical use since there is no guarantee one can reach the global optimal of these models.\n\nI recommend the authors to analyze/demonstrate how effective this weighted combination is. For example, the paper can benefit from some clear examples that show the learned weights across the layers and which ones are more important.\n\nThe presentation of the paper needs some polishing. For example, there are numerous typos, grammatical errors everywhere.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper478/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Attention: What Really Counts in Various NLP Tasks", "abstract": "Attention mechanisms in sequence to sequence models have shown great ability and wonderful performance in various natural language processing  (NLP)  tasks, such as sentence embedding, text generation, machine translation, machine reading comprehension, etc. Unfortunately, existing attention mechanisms only learn either high-level or low-level features. In this paper, we think that the lack of hierarchical mechanisms is a bottleneck in improving the performance of the attention mechanisms, and propose a novel Hierarchical Attention Mechanism (Ham) based on the weighted sum of different layers of a multi-level attention. \nHam achieves a state-of-the-art BLEU score of 0.26 on Chinese poem generation task and a nearly 6.5% averaged improvement compared with the existing machine reading comprehension models such as BIDAF and Match-LSTM. Furthermore, our experiments and theorems reveal that Ham has greater generalization and representation ability than existing attention mechanisms. ", "keywords": ["attention", "hierarchical", "machine reading comprehension", "poem generation"], "authorids": ["zehaodou@pku.edu.cn", "zhzhang@math.pku.edu.cn"], "authors": ["Zehao Dou", "Zhihua Zhang"], "TL;DR": "The paper proposed a novel hierarchical model to replace the original attention model in various NLP tasks.", "pdf": "/pdf/2de1a2d23536899248aea15fe0c19a5a9ea7cd65.pdf", "paperhash": "dou|hierarchical_attention_what_really_counts_in_various_nlp_tasks", "_bibtex": "@misc{\ndou2019hierarchical,\ntitle={Hierarchical Attention: What Really Counts in Various {NLP} Tasks},\nauthor={Zehao Dou and Zhihua Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=B1EjKsRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper478/Official_Review", "cdate": 1542234452264, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1EjKsRqtQ", "replyto": "B1EjKsRqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper478/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335733344, "tmdate": 1552335733344, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper478/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ByeRPZke5Q", "original": null, "number": 1, "cdate": 1538416998031, "ddate": null, "tcdate": 1538416998031, "tmdate": 1538417321538, "tddate": null, "forum": "B1EjKsRqtQ", "replyto": "B1EjKsRqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper478/Public_Comment", "content": {"comment": "The papers presents a fairly simple addition to the prevailing attention mechanism in form of an attention layer along the depth. The authors test the new architecture on a couple of important NLP tasks and beat the existing state of the art approaches. The paper is clearly written and easy to follow, though the formatting and grammar could be improved.    ", "title": "Simple yet intuitive modification to Attention mechanism gives impressive results"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper478/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Attention: What Really Counts in Various NLP Tasks", "abstract": "Attention mechanisms in sequence to sequence models have shown great ability and wonderful performance in various natural language processing  (NLP)  tasks, such as sentence embedding, text generation, machine translation, machine reading comprehension, etc. Unfortunately, existing attention mechanisms only learn either high-level or low-level features. In this paper, we think that the lack of hierarchical mechanisms is a bottleneck in improving the performance of the attention mechanisms, and propose a novel Hierarchical Attention Mechanism (Ham) based on the weighted sum of different layers of a multi-level attention. \nHam achieves a state-of-the-art BLEU score of 0.26 on Chinese poem generation task and a nearly 6.5% averaged improvement compared with the existing machine reading comprehension models such as BIDAF and Match-LSTM. Furthermore, our experiments and theorems reveal that Ham has greater generalization and representation ability than existing attention mechanisms. ", "keywords": ["attention", "hierarchical", "machine reading comprehension", "poem generation"], "authorids": ["zehaodou@pku.edu.cn", "zhzhang@math.pku.edu.cn"], "authors": ["Zehao Dou", "Zhihua Zhang"], "TL;DR": "The paper proposed a novel hierarchical model to replace the original attention model in various NLP tasks.", "pdf": "/pdf/2de1a2d23536899248aea15fe0c19a5a9ea7cd65.pdf", "paperhash": "dou|hierarchical_attention_what_really_counts_in_various_nlp_tasks", "_bibtex": "@misc{\ndou2019hierarchical,\ntitle={Hierarchical Attention: What Really Counts in Various {NLP} Tasks},\nauthor={Zehao Dou and Zhihua Zhang},\nyear={2019},\nurl={https://openreview.net/forum?id=B1EjKsRqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper478/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311830934, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "B1EjKsRqtQ", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper478/Authors", "ICLR.cc/2019/Conference/Paper478/Reviewers", "ICLR.cc/2019/Conference/Paper478/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper478/Authors", "ICLR.cc/2019/Conference/Paper478/Reviewers", "ICLR.cc/2019/Conference/Paper478/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311830934}}}], "count": 6}