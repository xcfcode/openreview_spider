{"notes": [{"id": "BkxFi2VYvS", "original": "SygtMd-Zvr", "number": 159, "cdate": 1569438880688, "ddate": null, "tcdate": 1569438880688, "tmdate": 1577168212743, "tddate": null, "forum": "BkxFi2VYvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authors": ["Wei-Hsu Chen", "Hsueh-Ming Hang"], "title": "Semi-supervised Semantic Segmentation using Auxiliary Network", "abstract": "Recently, the convolutional neural networks (CNNs) have shown great success on semantic segmentation task. However, for practical applications such as autonomous driving, the popular supervised learning method faces two challenges: the demand of low computational complexity and the need of huge training dataset accompanied by ground truth. Our focus in this paper is semi-supervised learning. We wish to use both labeled and unlabeled data in the training process. A highly efficient semantic segmentation network is our platform, which achieves high segmentation accuracy at low model size and high inference speed. We propose a semi-supervised learning approach to improve segmentation accuracy by including extra images without labels. While most existing semi-supervised learning methods are designed based on the adversarial learning techniques, we present a new and different approach, which trains an auxiliary CNN network that validates labels (ground-truth) on the unlabeled images. Therefore, in the supervised training phase, both the segmentation network and the auxiliary network are trained using labeled images. Then, in the unsupervised training phase, the unlabeled images are segmented and a subset of image pixels are picked up by the auxiliary network; and then they are used as ground truth to train the segmentation network. Thus, at the end, all dataset images can be used for retraining the segmentation network to improve the segmentation results. We use Cityscapes and CamVid datasets to verify the effectiveness of our semi-supervised scheme, and our experimental results show that it can improve the mean IoU for about 1.2% to 2.9% on the challenging Cityscapes dataset.", "authorids": ["qoososola520.ee06g@nctu.edu.tw", "hmhang@nctu.edu.tw"], "keywords": ["deep learning", "semi-supervised segmentation", "semantic segmentation", "CNN"], "TL;DR": "We design a two-branch semi-supervised segmentation system consisting of a segmentation network and an auxiliary CNN network that validates labels (ground-truth) on the unlabeled images", "pdf": "/pdf/6328ed3363a7714598aae2b277d6f994a793239a.pdf", "paperhash": "chen|semisupervised_semantic_segmentation_using_auxiliary_network", "original_pdf": "/attachment/b933e0770fe299566db15ef611b2af7537718037.pdf", "_bibtex": "@misc{\nchen2020semisupervised,\ntitle={Semi-supervised Semantic Segmentation using Auxiliary Network},\nauthor={Wei-Hsu Chen and Hsueh-Ming Hang},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxFi2VYvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "R7GqKNBYNi", "original": null, "number": 1, "cdate": 1576798688956, "ddate": null, "tcdate": 1576798688956, "tmdate": 1576800946157, "tddate": null, "forum": "BkxFi2VYvS", "replyto": "BkxFi2VYvS", "invitation": "ICLR.cc/2020/Conference/Paper159/-/Decision", "content": {"decision": "Reject", "comment": "The paper presents a semi-supervised learning approach to handle semantic classification (pixel-level classification). The approach extends Hung et al. 18, using a confidence map generated by an auxiliary network, aimed to improve the identification of small objects.\n\nThe reviews state that the paper novelty is limited compared to the state of the art; the reviewers made several suggestions to improve the processing pipeline (including all images, including the confidence weights). \nThe reviews also state that the paper needs be carefully polished. \n\nThe area chair hopes that the suggestions about the contents and writing of the paper will help to prepare an improved version of the paper. \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authors": ["Wei-Hsu Chen", "Hsueh-Ming Hang"], "title": "Semi-supervised Semantic Segmentation using Auxiliary Network", "abstract": "Recently, the convolutional neural networks (CNNs) have shown great success on semantic segmentation task. However, for practical applications such as autonomous driving, the popular supervised learning method faces two challenges: the demand of low computational complexity and the need of huge training dataset accompanied by ground truth. Our focus in this paper is semi-supervised learning. We wish to use both labeled and unlabeled data in the training process. A highly efficient semantic segmentation network is our platform, which achieves high segmentation accuracy at low model size and high inference speed. We propose a semi-supervised learning approach to improve segmentation accuracy by including extra images without labels. While most existing semi-supervised learning methods are designed based on the adversarial learning techniques, we present a new and different approach, which trains an auxiliary CNN network that validates labels (ground-truth) on the unlabeled images. Therefore, in the supervised training phase, both the segmentation network and the auxiliary network are trained using labeled images. Then, in the unsupervised training phase, the unlabeled images are segmented and a subset of image pixels are picked up by the auxiliary network; and then they are used as ground truth to train the segmentation network. Thus, at the end, all dataset images can be used for retraining the segmentation network to improve the segmentation results. We use Cityscapes and CamVid datasets to verify the effectiveness of our semi-supervised scheme, and our experimental results show that it can improve the mean IoU for about 1.2% to 2.9% on the challenging Cityscapes dataset.", "authorids": ["qoososola520.ee06g@nctu.edu.tw", "hmhang@nctu.edu.tw"], "keywords": ["deep learning", "semi-supervised segmentation", "semantic segmentation", "CNN"], "TL;DR": "We design a two-branch semi-supervised segmentation system consisting of a segmentation network and an auxiliary CNN network that validates labels (ground-truth) on the unlabeled images", "pdf": "/pdf/6328ed3363a7714598aae2b277d6f994a793239a.pdf", "paperhash": "chen|semisupervised_semantic_segmentation_using_auxiliary_network", "original_pdf": "/attachment/b933e0770fe299566db15ef611b2af7537718037.pdf", "_bibtex": "@misc{\nchen2020semisupervised,\ntitle={Semi-supervised Semantic Segmentation using Auxiliary Network},\nauthor={Wei-Hsu Chen and Hsueh-Ming Hang},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxFi2VYvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BkxFi2VYvS", "replyto": "BkxFi2VYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795715394, "tmdate": 1576800265297, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper159/-/Decision"}}}, {"id": "Skx3g4UIKS", "original": null, "number": 1, "cdate": 1571345395846, "ddate": null, "tcdate": 1571345395846, "tmdate": 1572972631336, "tddate": null, "forum": "BkxFi2VYvS", "replyto": "BkxFi2VYvS", "invitation": "ICLR.cc/2020/Conference/Paper159/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This submission introduces a semi-supervised method using auxiliary network for improved semantic segmentation. The authors modify a previous work as their main network architecture and use another small network as auxiliary branch. The framework can work in a semi-supervised setting since they can use confidence map to annotate unlabeled images to train the network. \n\nI give an initial rating of weak reject because (1) novelty in architecture design is trivial (2) the way of using unlabeled images is not new (3) experiments are not supportive (3) performance is not comparative to state-of-the-art. I will illustrate more as below.\n\n1. Architecture is not novel. As I mentioned, the authors adopt DSNet-fast as their main branch with minor modifications. And they use a simplified DeepLab architecture as their auxiliary branch. There is no ablation study or strong motivation to design this network. \n\n2. Using an auxiliary branch to measure confidence and try to get more labeled data is not new. There are many previous literature exploring similar ideas. Please clarify the differences between the literature and this submission in either introduction or related work section. \n    (1) Universal Semi-Supervised Semantic Segmentation, ICCV 2019\n    (2) Improving Semantic Segmentation via Video Propagation and Label Relaxation, CVPR 2019\n    (3) Semi-Supervised Semantic Segmentation with High- and Low-level Consistency, Arxiv 2019\nEspecially for (2) and (3), (2) use unlabeled images with confidence map, (3) has two branches as well. Both of them has results on Cityscapes. \n\n3. In table2 , speed comparison is not intuitive. The methods are evaluated on too many kinds of hardware and can not be directly compared. It is good to re-evaluate all algorithms on a single hardware, or remove this column. \n\n4. The experiment setting is too simple. Divide existing training data into half will make it hard to compare with other approaches. All the experiments in this submission can only show you are better than the baselines, but can't convince me your approach actually works. Because the training distribution is similar, it is easy for the auxiliary network to generalize. I would suggest the authors to use all the training data, and bring additional unlabeled images into picture to see what will happen. In addition, if you use all training data, you can make fair comparisons to many literatures and demonstrate the effectiveness of your approach.\n\n5. What is \"ours\" and \"ours with proposed network\" in table 5, please clarify."}, "signatures": ["ICLR.cc/2020/Conference/Paper159/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper159/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authors": ["Wei-Hsu Chen", "Hsueh-Ming Hang"], "title": "Semi-supervised Semantic Segmentation using Auxiliary Network", "abstract": "Recently, the convolutional neural networks (CNNs) have shown great success on semantic segmentation task. However, for practical applications such as autonomous driving, the popular supervised learning method faces two challenges: the demand of low computational complexity and the need of huge training dataset accompanied by ground truth. Our focus in this paper is semi-supervised learning. We wish to use both labeled and unlabeled data in the training process. A highly efficient semantic segmentation network is our platform, which achieves high segmentation accuracy at low model size and high inference speed. We propose a semi-supervised learning approach to improve segmentation accuracy by including extra images without labels. While most existing semi-supervised learning methods are designed based on the adversarial learning techniques, we present a new and different approach, which trains an auxiliary CNN network that validates labels (ground-truth) on the unlabeled images. Therefore, in the supervised training phase, both the segmentation network and the auxiliary network are trained using labeled images. Then, in the unsupervised training phase, the unlabeled images are segmented and a subset of image pixels are picked up by the auxiliary network; and then they are used as ground truth to train the segmentation network. Thus, at the end, all dataset images can be used for retraining the segmentation network to improve the segmentation results. We use Cityscapes and CamVid datasets to verify the effectiveness of our semi-supervised scheme, and our experimental results show that it can improve the mean IoU for about 1.2% to 2.9% on the challenging Cityscapes dataset.", "authorids": ["qoososola520.ee06g@nctu.edu.tw", "hmhang@nctu.edu.tw"], "keywords": ["deep learning", "semi-supervised segmentation", "semantic segmentation", "CNN"], "TL;DR": "We design a two-branch semi-supervised segmentation system consisting of a segmentation network and an auxiliary CNN network that validates labels (ground-truth) on the unlabeled images", "pdf": "/pdf/6328ed3363a7714598aae2b277d6f994a793239a.pdf", "paperhash": "chen|semisupervised_semantic_segmentation_using_auxiliary_network", "original_pdf": "/attachment/b933e0770fe299566db15ef611b2af7537718037.pdf", "_bibtex": "@misc{\nchen2020semisupervised,\ntitle={Semi-supervised Semantic Segmentation using Auxiliary Network},\nauthor={Wei-Hsu Chen and Hsueh-Ming Hang},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxFi2VYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkxFi2VYvS", "replyto": "BkxFi2VYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper159/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper159/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575393643110, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper159/Reviewers"], "noninvitees": [], "tcdate": 1570237756156, "tmdate": 1575393643121, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper159/-/Official_Review"}}}, {"id": "H1ejMTzTtB", "original": null, "number": 2, "cdate": 1571790098841, "ddate": null, "tcdate": 1571790098841, "tmdate": 1572972631301, "tddate": null, "forum": "BkxFi2VYvS", "replyto": "BkxFi2VYvS", "invitation": "ICLR.cc/2020/Conference/Paper159/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "- This paper proposes a semi-supervised learning strategy for semantic segmentation of road scenes. Specifically, authors propose to include an auxiliary network that will predict the confidence (at pixel-level) of the predictions on unlabeled images. These confidence values will be used to  generate a new auxiliary ground-truth to retrain the network using the unlabeled images.\n- Even though the idea is somehow interesting and results seem to improve with respect to the baselines, this paper is very similar to standard semi-supervised learning approaches for natural images that employ image proposals (i.e., EM-based methods). In those works, unlabeled images are segmented with the network trained on labeled images, generating some proposals. These proposals are later employed as a fake ground truth to re-train the network employing both labeled and unlabeled images. The only difference in this work is to employ the virtual confidence map to mask-out some pixels (those with lowest confidence values).  \n- Related work section is extremely weak. Authors merely mention few papers (some other relevant papers are missing), and throw a sentence for each one, without making connections between works. This makes difficult to place their work among the literature (e.g., which limitations of previous approaches the current method intend to address?). Authors should significantly improve this section.\n- I am not sure about the fact that employing only the probability maps as input to generate a confidence map is reliable, if no other information is employed. These predictions (e.g., confidence map) will be based only on the probabilities obtained by the first network. While this is already a good indicator of the confidence of the network to make those predictions, I believe that input images should also be included. The intuition behind this is that there may exist some regions with similar probabilities (from first network), which are incorrectly classified (leading to 0-masked pixels on the auxiliary ground-truth) in some cases, while correctly classified in other situations (leading to 1-masked pixels on the auxiliary ground-truth).  \n- Eq.1) is basically the standard cross-entropy loss, with the difference of the weighting terms.\n- a_{h,w} is the softmax of the auxiliary network, isn\u2019t it?\n- Further, authors threshold the values of the confidence map to generate the new auxiliary ground truth. Why not to use the raw values so that each pixel is weighted differently according to its importance? \n- Authors make some claims which were never demonstrated. For example, they mention that the proposed approach performs better on small targets than previous approaches. Nevertheless, only mean results (over all the classes) are shown. To this end authors should report per-class performances, instead of the mean.\n- Furthermore, authors make several over claims, misleading information. For example, they mentioned that they proposed a highly efficient segmentation method. Nevertheless, from Table 2 it can be observed that the proposed method ranks in the middle in terms of both speed and parameters, compared to other state-of-the-art models. Similarly, authors mention that their model is equipped with a carefully designed auxiliary loss function during training, while they basically employ a standard cross-entropy weighted by some values to account for imbalance between classes and between positive and negative pixels within the same class.   \n- The paper contains many grammatical errors.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper159/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper159/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authors": ["Wei-Hsu Chen", "Hsueh-Ming Hang"], "title": "Semi-supervised Semantic Segmentation using Auxiliary Network", "abstract": "Recently, the convolutional neural networks (CNNs) have shown great success on semantic segmentation task. However, for practical applications such as autonomous driving, the popular supervised learning method faces two challenges: the demand of low computational complexity and the need of huge training dataset accompanied by ground truth. Our focus in this paper is semi-supervised learning. We wish to use both labeled and unlabeled data in the training process. A highly efficient semantic segmentation network is our platform, which achieves high segmentation accuracy at low model size and high inference speed. We propose a semi-supervised learning approach to improve segmentation accuracy by including extra images without labels. While most existing semi-supervised learning methods are designed based on the adversarial learning techniques, we present a new and different approach, which trains an auxiliary CNN network that validates labels (ground-truth) on the unlabeled images. Therefore, in the supervised training phase, both the segmentation network and the auxiliary network are trained using labeled images. Then, in the unsupervised training phase, the unlabeled images are segmented and a subset of image pixels are picked up by the auxiliary network; and then they are used as ground truth to train the segmentation network. Thus, at the end, all dataset images can be used for retraining the segmentation network to improve the segmentation results. We use Cityscapes and CamVid datasets to verify the effectiveness of our semi-supervised scheme, and our experimental results show that it can improve the mean IoU for about 1.2% to 2.9% on the challenging Cityscapes dataset.", "authorids": ["qoososola520.ee06g@nctu.edu.tw", "hmhang@nctu.edu.tw"], "keywords": ["deep learning", "semi-supervised segmentation", "semantic segmentation", "CNN"], "TL;DR": "We design a two-branch semi-supervised segmentation system consisting of a segmentation network and an auxiliary CNN network that validates labels (ground-truth) on the unlabeled images", "pdf": "/pdf/6328ed3363a7714598aae2b277d6f994a793239a.pdf", "paperhash": "chen|semisupervised_semantic_segmentation_using_auxiliary_network", "original_pdf": "/attachment/b933e0770fe299566db15ef611b2af7537718037.pdf", "_bibtex": "@misc{\nchen2020semisupervised,\ntitle={Semi-supervised Semantic Segmentation using Auxiliary Network},\nauthor={Wei-Hsu Chen and Hsueh-Ming Hang},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxFi2VYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkxFi2VYvS", "replyto": "BkxFi2VYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper159/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper159/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575393643110, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper159/Reviewers"], "noninvitees": [], "tcdate": 1570237756156, "tmdate": 1575393643121, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper159/-/Official_Review"}}}, {"id": "BJlYwvx1qH", "original": null, "number": 3, "cdate": 1571911521362, "ddate": null, "tcdate": 1571911521362, "tmdate": 1572972631258, "tddate": null, "forum": "BkxFi2VYvS", "replyto": "BkxFi2VYvS", "invitation": "ICLR.cc/2020/Conference/Paper159/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper focused on the problem of semantic segmentation. The author first proposed an efficient segmentation network. Then a semi-supervised learning scheme with an auxiliary network is introduced to annotate the unlabeled images thus help boost the segmentation accuracy.\n\nClarity:\nThere is no novelty for the proposed fast segmentation network detailed in Sec 3.2. The auxiliary network for predicting the confidence map may look interesting, while the experimental results are not convincing.\n\nExperiments:\n1. The design of the fast segmentation network in this paper is boring and not much related to the title.\n\n2. The experimental results regarding the effectiveness of the auxiliary network are based on a weak backbone, some stronger backbones like DeepLab and PSPNet should be included.\n\n3. What the results would be if using all labeled data and the newly added unlabeled data? Can the newly unlabeled data with labels from the auxiliary network help boosting the performance?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper159/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper159/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authors": ["Wei-Hsu Chen", "Hsueh-Ming Hang"], "title": "Semi-supervised Semantic Segmentation using Auxiliary Network", "abstract": "Recently, the convolutional neural networks (CNNs) have shown great success on semantic segmentation task. However, for practical applications such as autonomous driving, the popular supervised learning method faces two challenges: the demand of low computational complexity and the need of huge training dataset accompanied by ground truth. Our focus in this paper is semi-supervised learning. We wish to use both labeled and unlabeled data in the training process. A highly efficient semantic segmentation network is our platform, which achieves high segmentation accuracy at low model size and high inference speed. We propose a semi-supervised learning approach to improve segmentation accuracy by including extra images without labels. While most existing semi-supervised learning methods are designed based on the adversarial learning techniques, we present a new and different approach, which trains an auxiliary CNN network that validates labels (ground-truth) on the unlabeled images. Therefore, in the supervised training phase, both the segmentation network and the auxiliary network are trained using labeled images. Then, in the unsupervised training phase, the unlabeled images are segmented and a subset of image pixels are picked up by the auxiliary network; and then they are used as ground truth to train the segmentation network. Thus, at the end, all dataset images can be used for retraining the segmentation network to improve the segmentation results. We use Cityscapes and CamVid datasets to verify the effectiveness of our semi-supervised scheme, and our experimental results show that it can improve the mean IoU for about 1.2% to 2.9% on the challenging Cityscapes dataset.", "authorids": ["qoososola520.ee06g@nctu.edu.tw", "hmhang@nctu.edu.tw"], "keywords": ["deep learning", "semi-supervised segmentation", "semantic segmentation", "CNN"], "TL;DR": "We design a two-branch semi-supervised segmentation system consisting of a segmentation network and an auxiliary CNN network that validates labels (ground-truth) on the unlabeled images", "pdf": "/pdf/6328ed3363a7714598aae2b277d6f994a793239a.pdf", "paperhash": "chen|semisupervised_semantic_segmentation_using_auxiliary_network", "original_pdf": "/attachment/b933e0770fe299566db15ef611b2af7537718037.pdf", "_bibtex": "@misc{\nchen2020semisupervised,\ntitle={Semi-supervised Semantic Segmentation using Auxiliary Network},\nauthor={Wei-Hsu Chen and Hsueh-Ming Hang},\nyear={2020},\nurl={https://openreview.net/forum?id=BkxFi2VYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkxFi2VYvS", "replyto": "BkxFi2VYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper159/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper159/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575393643110, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper159/Reviewers"], "noninvitees": [], "tcdate": 1570237756156, "tmdate": 1575393643121, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper159/-/Official_Review"}}}], "count": 5}