{"notes": [{"id": "BJe8pkHFwS", "original": "rkl1tNktDS", "number": 1990, "cdate": 1569439678076, "ddate": null, "tcdate": 1569439678076, "tmdate": 1583912051235, "tddate": null, "forum": "BJe8pkHFwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["zengh@usc.edu", "hongkuaz@usc.edu", "ajiteshs@usc.edu", "rajgopal.kannan.civ@mail.mil", "prasanna@usc.edu"], "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method", "authors": ["Hanqing Zeng", "Hongkuan Zhou", "Ajitesh Srivastava", "Rajgopal Kannan", "Viktor Prasanna"], "pdf": "/pdf/b03966e181622789b1b61f35fec0d27eefc1663b.pdf", "TL;DR": "We propose a graph sampling based minibatch construction method for training deep Graph Convolutional Networks on large graphs. ", "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the \"neighbor explosion\" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection).  GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). ", "code": "https://github.com/GraphSAINT/GraphSAINT", "keywords": ["Graph Convolutional Networks", "Graph sampling", "Network embedding"], "paperhash": "zeng|graphsaint_graph_sampling_based_inductive_learning_method", "_bibtex": "@inproceedings{\nZeng2020GraphSAINT:,\ntitle={GraphSAINT: Graph Sampling Based Inductive Learning Method},\nauthor={Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe8pkHFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d858e9b1f138f778aa7fd004ea9449377c1d37.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "QmaaJ-k17w", "original": null, "number": 1, "cdate": 1576798737722, "ddate": null, "tcdate": 1576798737722, "tmdate": 1576800898644, "tddate": null, "forum": "BJe8pkHFwS", "replyto": "BJe8pkHFwS", "invitation": "ICLR.cc/2020/Conference/Paper1990/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "All three reviewers advocated acceptance. The AC agrees, feeling the paper is interesting. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zengh@usc.edu", "hongkuaz@usc.edu", "ajiteshs@usc.edu", "rajgopal.kannan.civ@mail.mil", "prasanna@usc.edu"], "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method", "authors": ["Hanqing Zeng", "Hongkuan Zhou", "Ajitesh Srivastava", "Rajgopal Kannan", "Viktor Prasanna"], "pdf": "/pdf/b03966e181622789b1b61f35fec0d27eefc1663b.pdf", "TL;DR": "We propose a graph sampling based minibatch construction method for training deep Graph Convolutional Networks on large graphs. ", "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the \"neighbor explosion\" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection).  GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). ", "code": "https://github.com/GraphSAINT/GraphSAINT", "keywords": ["Graph Convolutional Networks", "Graph sampling", "Network embedding"], "paperhash": "zeng|graphsaint_graph_sampling_based_inductive_learning_method", "_bibtex": "@inproceedings{\nZeng2020GraphSAINT:,\ntitle={GraphSAINT: Graph Sampling Based Inductive Learning Method},\nauthor={Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe8pkHFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d858e9b1f138f778aa7fd004ea9449377c1d37.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJe8pkHFwS", "replyto": "BJe8pkHFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795706321, "tmdate": 1576800254342, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1990/-/Decision"}}}, {"id": "HJl2Jtsfor", "original": null, "number": 2, "cdate": 1573202147681, "ddate": null, "tcdate": 1573202147681, "tmdate": 1573840490490, "tddate": null, "forum": "BJe8pkHFwS", "replyto": "BJeVLY_aFS", "invitation": "ICLR.cc/2020/Conference/Paper1990/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thanks a lot for your valuable feedback. We will state our assumption on the theorem more explicitly in our next revision. \n\nAnswer to the question:\n\nYes, there is a typo in Equation 3. Thanks for pointing this out! The correct expression should be $\\mathbb{E}(L_\\text{batch})=\\frac{1}{|\\mathbb{G}|} \\sum\\limits_{\\mathcal{G}_s \\in \\mathbb{G}}\\sum\\limits_{v\\in \\mathcal{V}_s} \\frac{L_v}{\\lambda_v}=\\frac{1}{|\\mathcal{V}|}\\sum\\limits_{v\\in\\mathcal{V}} L_v$."}, "signatures": ["ICLR.cc/2020/Conference/Paper1990/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zengh@usc.edu", "hongkuaz@usc.edu", "ajiteshs@usc.edu", "rajgopal.kannan.civ@mail.mil", "prasanna@usc.edu"], "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method", "authors": ["Hanqing Zeng", "Hongkuan Zhou", "Ajitesh Srivastava", "Rajgopal Kannan", "Viktor Prasanna"], "pdf": "/pdf/b03966e181622789b1b61f35fec0d27eefc1663b.pdf", "TL;DR": "We propose a graph sampling based minibatch construction method for training deep Graph Convolutional Networks on large graphs. ", "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the \"neighbor explosion\" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection).  GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). ", "code": "https://github.com/GraphSAINT/GraphSAINT", "keywords": ["Graph Convolutional Networks", "Graph sampling", "Network embedding"], "paperhash": "zeng|graphsaint_graph_sampling_based_inductive_learning_method", "_bibtex": "@inproceedings{\nZeng2020GraphSAINT:,\ntitle={GraphSAINT: Graph Sampling Based Inductive Learning Method},\nauthor={Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe8pkHFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d858e9b1f138f778aa7fd004ea9449377c1d37.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJe8pkHFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference/Paper1990/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1990/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1990/Reviewers", "ICLR.cc/2020/Conference/Paper1990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1990/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1990/Authors|ICLR.cc/2020/Conference/Paper1990/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147924, "tmdate": 1576860543642, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference/Paper1990/Reviewers", "ICLR.cc/2020/Conference/Paper1990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1990/-/Official_Comment"}}}, {"id": "SyeIxl6MiB", "original": null, "number": 3, "cdate": 1573208046163, "ddate": null, "tcdate": 1573208046163, "tmdate": 1573840306051, "tddate": null, "forum": "BJe8pkHFwS", "replyto": "SJe_auDjFr", "invitation": "ICLR.cc/2020/Conference/Paper1990/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "We appreciate the valuable feedback from the reviewer! In the following, we would like to clarify the two \"Cons\":\n\n\n1. Regarding batch size (defined by all the baselines as the number of node samples in the output layer):\n\nAs noted in the review, the four methods (GraphSAGE, FastGCN, S-GCN, AS-GCN) use samples of different sizes across different layers, even when the batch size is set to be the same. This leads to the intuition that the optimal batch size (w.r.t. accuracy) should be different for different methods. In the experiments, we have treated batch size as a hyperparameter dependent on the sampling method as well as the training graph topology.\n\nBy experiments on varying the batch sizes, we observe: for GraphSAGE, S-GCN and AS-GCN, their default batch sizes (512,1000 and 512, respectively) lead to the highest accuracy on all datasets. For FastGCN, increasing the default batch size (from 400 to 4000) leads to noticeable accuracy improvement. For ClusterGCN, different datasets correspond to different optimal batch sizes, and the accuracy in Section 5.1 is already tuned by identifying the optimal batch size on a per graph basis. See also Table 11 (or Table 10 in the original submission) of Appendix D.3 for experiment details. \n\n\n2. Regarding sampling overhead:\n\nAs discussed in Appendix D.2, the two best samplers of GraphSAINT, \"Edge\" and \"RW\", are very light-weight. In addition, similar to ClusterGCN, our sampling can also be done offline since the sampler does not require node features. To be more specific, time to construct one subgraph by \"Edge\" or \"RW\" is always less than 25% of the time to perform one gradient update. On the other hand, as shown in Table 9, time to identify clusters by ClusterGCN can be much longer than its total training time if the training graph is large and dense. For example, clustering time on Amazon is over 5x the total training time of ClusterGCN. \n\nTaking into account the pre-processing time, sampling time and training time altogether, we summarize the total convergence time (in seconds) of GraphSAINT and ClusterGCN in the following (corresponding to Table 2 configuration):\n---------------------------------------------------------------------------------------\n                                      PPI       Flickr   Reddit   Yelp       Amazon\n---------------------------------------------------------------------------------------\nGraphSAINT-Edge     91.0       7.0      16.6       273.9      401.0\nGraphSAINT-RW        103.6     7.5      17.2       310.1      425.6\nClusterGCN                163.2     12.9     55.3      256.0      2804.8\n----------------------------------------------------------------------------------------\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1990/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zengh@usc.edu", "hongkuaz@usc.edu", "ajiteshs@usc.edu", "rajgopal.kannan.civ@mail.mil", "prasanna@usc.edu"], "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method", "authors": ["Hanqing Zeng", "Hongkuan Zhou", "Ajitesh Srivastava", "Rajgopal Kannan", "Viktor Prasanna"], "pdf": "/pdf/b03966e181622789b1b61f35fec0d27eefc1663b.pdf", "TL;DR": "We propose a graph sampling based minibatch construction method for training deep Graph Convolutional Networks on large graphs. ", "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the \"neighbor explosion\" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection).  GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). ", "code": "https://github.com/GraphSAINT/GraphSAINT", "keywords": ["Graph Convolutional Networks", "Graph sampling", "Network embedding"], "paperhash": "zeng|graphsaint_graph_sampling_based_inductive_learning_method", "_bibtex": "@inproceedings{\nZeng2020GraphSAINT:,\ntitle={GraphSAINT: Graph Sampling Based Inductive Learning Method},\nauthor={Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe8pkHFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d858e9b1f138f778aa7fd004ea9449377c1d37.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJe8pkHFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference/Paper1990/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1990/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1990/Reviewers", "ICLR.cc/2020/Conference/Paper1990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1990/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1990/Authors|ICLR.cc/2020/Conference/Paper1990/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147924, "tmdate": 1576860543642, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference/Paper1990/Reviewers", "ICLR.cc/2020/Conference/Paper1990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1990/-/Official_Comment"}}}, {"id": "S1x9OrPhjH", "original": null, "number": 6, "cdate": 1573840242460, "ddate": null, "tcdate": 1573840242460, "tmdate": 1573840242460, "tddate": null, "forum": "BJe8pkHFwS", "replyto": "SyeIxl6MiB", "invitation": "ICLR.cc/2020/Conference/Paper1990/-/Official_Comment", "content": {"title": "New Revision of the Paper Uploaded", "comment": "We have uploaded a new version of the paper which includes new experimental results.\n\nIn Appendix D.3, we have included the test set accuracy of baselines under various batch sizes in Table 11. The results support the \"point 1\" in our previous response. \n\nIn Appendix D.2, we have included an additional table comparing the total convergence time of GraphSAINT and ClusterGCN after considering the pre-processing cost and sampling cost. The results are in line with the \"point 2\" in our previous response. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1990/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zengh@usc.edu", "hongkuaz@usc.edu", "ajiteshs@usc.edu", "rajgopal.kannan.civ@mail.mil", "prasanna@usc.edu"], "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method", "authors": ["Hanqing Zeng", "Hongkuan Zhou", "Ajitesh Srivastava", "Rajgopal Kannan", "Viktor Prasanna"], "pdf": "/pdf/b03966e181622789b1b61f35fec0d27eefc1663b.pdf", "TL;DR": "We propose a graph sampling based minibatch construction method for training deep Graph Convolutional Networks on large graphs. ", "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the \"neighbor explosion\" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection).  GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). ", "code": "https://github.com/GraphSAINT/GraphSAINT", "keywords": ["Graph Convolutional Networks", "Graph sampling", "Network embedding"], "paperhash": "zeng|graphsaint_graph_sampling_based_inductive_learning_method", "_bibtex": "@inproceedings{\nZeng2020GraphSAINT:,\ntitle={GraphSAINT: Graph Sampling Based Inductive Learning Method},\nauthor={Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe8pkHFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d858e9b1f138f778aa7fd004ea9449377c1d37.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJe8pkHFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference/Paper1990/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1990/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1990/Reviewers", "ICLR.cc/2020/Conference/Paper1990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1990/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1990/Authors|ICLR.cc/2020/Conference/Paper1990/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147924, "tmdate": 1576860543642, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference/Paper1990/Reviewers", "ICLR.cc/2020/Conference/Paper1990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1990/-/Official_Comment"}}}, {"id": "BklyDVDhsH", "original": null, "number": 5, "cdate": 1573839959494, "ddate": null, "tcdate": 1573839959494, "tmdate": 1573840071346, "tddate": null, "forum": "BJe8pkHFwS", "replyto": "HJl2Jtsfor", "invitation": "ICLR.cc/2020/Conference/Paper1990/-/Official_Comment", "content": {"title": "New Revision of the Paper Uploaded", "comment": "We have uploaded a new version of the paper after integrating your constructive suggestions. \n\nRegarding clarifying the theorem statement:\n\nWe have updated the text in Section 3.2 as well as the statement of Proposition 3.1. Note that for given ${x}_u^{(\\ell)}$, Proposition 3.1 itself considers a single layer $\\ell+1$ and does not rely on the assumption that \"each layer learns embeddings independently\". On the other hand, as noted by the reviewer, such assumption is required when using the proposition to normalize the multi-layer GCN built by GraphSAINT. Therefore, in the updated paper, we clarify such assumption right before and after the statement of Proposition 3.1."}, "signatures": ["ICLR.cc/2020/Conference/Paper1990/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zengh@usc.edu", "hongkuaz@usc.edu", "ajiteshs@usc.edu", "rajgopal.kannan.civ@mail.mil", "prasanna@usc.edu"], "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method", "authors": ["Hanqing Zeng", "Hongkuan Zhou", "Ajitesh Srivastava", "Rajgopal Kannan", "Viktor Prasanna"], "pdf": "/pdf/b03966e181622789b1b61f35fec0d27eefc1663b.pdf", "TL;DR": "We propose a graph sampling based minibatch construction method for training deep Graph Convolutional Networks on large graphs. ", "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the \"neighbor explosion\" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection).  GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). ", "code": "https://github.com/GraphSAINT/GraphSAINT", "keywords": ["Graph Convolutional Networks", "Graph sampling", "Network embedding"], "paperhash": "zeng|graphsaint_graph_sampling_based_inductive_learning_method", "_bibtex": "@inproceedings{\nZeng2020GraphSAINT:,\ntitle={GraphSAINT: Graph Sampling Based Inductive Learning Method},\nauthor={Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe8pkHFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d858e9b1f138f778aa7fd004ea9449377c1d37.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJe8pkHFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference/Paper1990/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1990/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1990/Reviewers", "ICLR.cc/2020/Conference/Paper1990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1990/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1990/Authors|ICLR.cc/2020/Conference/Paper1990/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147924, "tmdate": 1576860543642, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference/Paper1990/Reviewers", "ICLR.cc/2020/Conference/Paper1990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1990/-/Official_Comment"}}}, {"id": "rygiO3JFjS", "original": null, "number": 4, "cdate": 1573612658669, "ddate": null, "tcdate": 1573612658669, "tmdate": 1573612658669, "tddate": null, "forum": "BJe8pkHFwS", "replyto": "rJlxKLYhcH", "invitation": "ICLR.cc/2020/Conference/Paper1990/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you for your valuable feedback. We agree that properly sampled subgraphs are critical to high accuracy, and sampling parameters need to be carefully chosen. In fact, we design samplers based on the theoretical analysis on bias and variance of the minibatch estimator. \n\nTo eliminate bias introduced by graph sampling, we derive the normalization on feature aggregator and minibatch loss. Note that such normalization ensures unbiasedness for an arbitrary graph sampler (Proposition 3.1) . To minimize the variance of the minibatch estimator, we derive the optimal sampling parameter for \"Edge\" sampler (Theorem 3.2). We further extend the proposed edge sampler to random walk samplers and determine the corresponding sampling parameters, based on insights into the GCN architecture.\n\nNote that our samplers derived from the theoretical analysis also satisfies the intuitive requirement for a \"proper\" sampler -- that nodes influential to each other should have high probability to be sampled together. Please see Section 3.3 for a detailed discussion.\n\nOur experiments show that the choices of normalization and graph samplers based on Proposition 3.1 and Theorem 3.2 do lead to improved accuracy. As shown in Table 2, accuracy results of GraphSAINT are indeed state-of-the-art."}, "signatures": ["ICLR.cc/2020/Conference/Paper1990/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zengh@usc.edu", "hongkuaz@usc.edu", "ajiteshs@usc.edu", "rajgopal.kannan.civ@mail.mil", "prasanna@usc.edu"], "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method", "authors": ["Hanqing Zeng", "Hongkuan Zhou", "Ajitesh Srivastava", "Rajgopal Kannan", "Viktor Prasanna"], "pdf": "/pdf/b03966e181622789b1b61f35fec0d27eefc1663b.pdf", "TL;DR": "We propose a graph sampling based minibatch construction method for training deep Graph Convolutional Networks on large graphs. ", "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the \"neighbor explosion\" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection).  GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). ", "code": "https://github.com/GraphSAINT/GraphSAINT", "keywords": ["Graph Convolutional Networks", "Graph sampling", "Network embedding"], "paperhash": "zeng|graphsaint_graph_sampling_based_inductive_learning_method", "_bibtex": "@inproceedings{\nZeng2020GraphSAINT:,\ntitle={GraphSAINT: Graph Sampling Based Inductive Learning Method},\nauthor={Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe8pkHFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d858e9b1f138f778aa7fd004ea9449377c1d37.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJe8pkHFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference/Paper1990/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1990/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1990/Reviewers", "ICLR.cc/2020/Conference/Paper1990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1990/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1990/Authors|ICLR.cc/2020/Conference/Paper1990/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147924, "tmdate": 1576860543642, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference/Paper1990/Reviewers", "ICLR.cc/2020/Conference/Paper1990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1990/-/Official_Comment"}}}, {"id": "SJe_auDjFr", "original": null, "number": 1, "cdate": 1571678400008, "ddate": null, "tcdate": 1571678400008, "tmdate": 1572972397553, "tddate": null, "forum": "BJe8pkHFwS", "replyto": "BJe8pkHFwS", "invitation": "ICLR.cc/2020/Conference/Paper1990/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper proposed a new sampling method to train GCN in the mini-batch manner. In particular, unlike existing methods which samples the mini-batch in the node-wise way, GraphSAINT proposed to sample a mini-batch in the graph-wise way. As a result, GraphSAINT uses the same graph across different GCN layers, while most existing methods use different graphs across different GCN layers.  In addition, the authors show that this sampling method is unbiased. Extensive experimental results have shown improvement over existing methods. Overall, this idea is interesting and well presented. \n\nPros:\n1. A new sampling method for the stochastic training of GCN. Have good performance.\n2. Extensive experiments to verify the performance of the proposed method.\n3. The theoretical analysis looks sound.\n\nCons:\n1. GraphSAGE and FastGCN use different graphs across different GCN layers, while ClusterGCN and GraphSAINT use the same graph across different GCN layers. To make a fair comparison, it is necessary to have the same batch size for different methods. How do you deal with this issue in your experiment?\n2. For ClusterGCN, the clustering procedure is done before the training. So, it needs much less computational overhead for sampling in the training course. However, GraphSAINT needs to do the heavy sampling online. Thus, it may consume more time than ClusterGCN for large graphs. It's better to show the running time of these two methods. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1990/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1990/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zengh@usc.edu", "hongkuaz@usc.edu", "ajiteshs@usc.edu", "rajgopal.kannan.civ@mail.mil", "prasanna@usc.edu"], "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method", "authors": ["Hanqing Zeng", "Hongkuan Zhou", "Ajitesh Srivastava", "Rajgopal Kannan", "Viktor Prasanna"], "pdf": "/pdf/b03966e181622789b1b61f35fec0d27eefc1663b.pdf", "TL;DR": "We propose a graph sampling based minibatch construction method for training deep Graph Convolutional Networks on large graphs. ", "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the \"neighbor explosion\" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection).  GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). ", "code": "https://github.com/GraphSAINT/GraphSAINT", "keywords": ["Graph Convolutional Networks", "Graph sampling", "Network embedding"], "paperhash": "zeng|graphsaint_graph_sampling_based_inductive_learning_method", "_bibtex": "@inproceedings{\nZeng2020GraphSAINT:,\ntitle={GraphSAINT: Graph Sampling Based Inductive Learning Method},\nauthor={Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe8pkHFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d858e9b1f138f778aa7fd004ea9449377c1d37.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJe8pkHFwS", "replyto": "BJe8pkHFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1990/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1990/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575399277273, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1990/Reviewers"], "noninvitees": [], "tcdate": 1570237729359, "tmdate": 1575399277285, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1990/-/Official_Review"}}}, {"id": "BJeVLY_aFS", "original": null, "number": 2, "cdate": 1571813707804, "ddate": null, "tcdate": 1571813707804, "tmdate": 1572972397517, "tddate": null, "forum": "BJe8pkHFwS", "replyto": "BJe8pkHFwS", "invitation": "ICLR.cc/2020/Conference/Paper1990/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes a training method for graph convolution networks on large graphs. The idea is to train a full GCN on partial samples of the graph. The graph samples are computed based on the graph connectivity, and the authors propose methods for reducing the bias and variance in the training procedure. \n\nThe idea is elegant and intuitive, and the fact that the approach can work with various graph sampling methods adds to its generality. The paper is well-written and the fact that code is published is valuable.\n\nThe results on bias and variance are under the assumption that each layer independently learns an embedding. This would be clearer if added explicitly in the theorem statements (and not as part of the main text). It would be interesting to discuss how realistic this assumption is, and how large the actual bias is. Perhaps this can be measured empirically? \nNevertheless, the empirical result indeed support the claim that this simplifying assumption is enough to derive useful learning rules.\n\nOverall, I believe this is a solid contribution, and I can foresee future extensions that improve the results with more complex graph sampling methods.\n\nQuestion to the authors: I did not understand the second equality in Eq. 3. Could there be a typo?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1990/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1990/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zengh@usc.edu", "hongkuaz@usc.edu", "ajiteshs@usc.edu", "rajgopal.kannan.civ@mail.mil", "prasanna@usc.edu"], "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method", "authors": ["Hanqing Zeng", "Hongkuan Zhou", "Ajitesh Srivastava", "Rajgopal Kannan", "Viktor Prasanna"], "pdf": "/pdf/b03966e181622789b1b61f35fec0d27eefc1663b.pdf", "TL;DR": "We propose a graph sampling based minibatch construction method for training deep Graph Convolutional Networks on large graphs. ", "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the \"neighbor explosion\" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection).  GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). ", "code": "https://github.com/GraphSAINT/GraphSAINT", "keywords": ["Graph Convolutional Networks", "Graph sampling", "Network embedding"], "paperhash": "zeng|graphsaint_graph_sampling_based_inductive_learning_method", "_bibtex": "@inproceedings{\nZeng2020GraphSAINT:,\ntitle={GraphSAINT: Graph Sampling Based Inductive Learning Method},\nauthor={Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe8pkHFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d858e9b1f138f778aa7fd004ea9449377c1d37.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJe8pkHFwS", "replyto": "BJe8pkHFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1990/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1990/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575399277273, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1990/Reviewers"], "noninvitees": [], "tcdate": 1570237729359, "tmdate": 1575399277285, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1990/-/Official_Review"}}}, {"id": "rJlxKLYhcH", "original": null, "number": 3, "cdate": 1572800120162, "ddate": null, "tcdate": 1572800120162, "tmdate": 1572972397471, "tddate": null, "forum": "BJe8pkHFwS", "replyto": "BJe8pkHFwS", "invitation": "ICLR.cc/2020/Conference/Paper1990/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review": "Scaling GCNs to large graphs is important for real applications. Instead of sampling the nodes or edges across GCN layers,  this paper proposes to sample the training graph to improve training efficiency and accuracy. It is a smart idea to construct a complete GCN from the sampled subgraphs.  Convincing experiments can verify the effectiveness of the proposed method.  It is a good work.\n\nQuestion: \n1. How can the authors guarantee that subgraphs are properly sampled? Are there any  theoretical guarantee?  \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1990/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1990/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zengh@usc.edu", "hongkuaz@usc.edu", "ajiteshs@usc.edu", "rajgopal.kannan.civ@mail.mil", "prasanna@usc.edu"], "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method", "authors": ["Hanqing Zeng", "Hongkuan Zhou", "Ajitesh Srivastava", "Rajgopal Kannan", "Viktor Prasanna"], "pdf": "/pdf/b03966e181622789b1b61f35fec0d27eefc1663b.pdf", "TL;DR": "We propose a graph sampling based minibatch construction method for training deep Graph Convolutional Networks on large graphs. ", "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the \"neighbor explosion\" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection).  GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). ", "code": "https://github.com/GraphSAINT/GraphSAINT", "keywords": ["Graph Convolutional Networks", "Graph sampling", "Network embedding"], "paperhash": "zeng|graphsaint_graph_sampling_based_inductive_learning_method", "_bibtex": "@inproceedings{\nZeng2020GraphSAINT:,\ntitle={GraphSAINT: Graph Sampling Based Inductive Learning Method},\nauthor={Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe8pkHFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d858e9b1f138f778aa7fd004ea9449377c1d37.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJe8pkHFwS", "replyto": "BJe8pkHFwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1990/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1990/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575399277273, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1990/Reviewers"], "noninvitees": [], "tcdate": 1570237729359, "tmdate": 1575399277285, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1990/-/Official_Review"}}}, {"id": "rkgtzFwvFB", "original": null, "number": 1, "cdate": 1571416337402, "ddate": null, "tcdate": 1571416337402, "tmdate": 1571416337402, "tddate": null, "forum": "BJe8pkHFwS", "replyto": "B1e-fZBLtr", "invitation": "ICLR.cc/2020/Conference/Paper1990/-/Official_Comment", "content": {"comment": "Thanks for your interest in our paper. This is a valid concern.\n\nAs stated at the beginning of Section 3.2, \u201canalysis of the complete multi-layer GCN is difficult due to non-linear activations. Thus, we analyze the embedding of each layer independently.\u201d In other words, to derive Proposition 3.1, we followed the same assumption as AS-GCN and FastGCN, that each layer independently learns an embedding. Thus, the condition on the previous layer can be removed. This assumption also motivates the proposed edge sampler.\n\nAlternatively, in Section 3.4, we have also performed analysis by assuming no non-linear activations. Then we can collapse L layers of A into an equivalent 1 layer of A^L. Analysis based on this assumption leads to the proposed random walk sampler. \n\nIn practice, it is possible that neither of the above two assumptions are exactly true, but they provide an approach to normalize loss and choose samplers and their parameters. Our experiments show that the choices of normalization and samplers based on these assumptions do lead to improved accuracy.", "title": "GraphSAINT is unbiased under our assumption"}, "signatures": ["ICLR.cc/2020/Conference/Paper1990/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zengh@usc.edu", "hongkuaz@usc.edu", "ajiteshs@usc.edu", "rajgopal.kannan.civ@mail.mil", "prasanna@usc.edu"], "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method", "authors": ["Hanqing Zeng", "Hongkuan Zhou", "Ajitesh Srivastava", "Rajgopal Kannan", "Viktor Prasanna"], "pdf": "/pdf/b03966e181622789b1b61f35fec0d27eefc1663b.pdf", "TL;DR": "We propose a graph sampling based minibatch construction method for training deep Graph Convolutional Networks on large graphs. ", "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the \"neighbor explosion\" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection).  GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). ", "code": "https://github.com/GraphSAINT/GraphSAINT", "keywords": ["Graph Convolutional Networks", "Graph sampling", "Network embedding"], "paperhash": "zeng|graphsaint_graph_sampling_based_inductive_learning_method", "_bibtex": "@inproceedings{\nZeng2020GraphSAINT:,\ntitle={GraphSAINT: Graph Sampling Based Inductive Learning Method},\nauthor={Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe8pkHFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d858e9b1f138f778aa7fd004ea9449377c1d37.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJe8pkHFwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference/Paper1990/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1990/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1990/Reviewers", "ICLR.cc/2020/Conference/Paper1990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1990/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1990/Authors|ICLR.cc/2020/Conference/Paper1990/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504147924, "tmdate": 1576860543642, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference/Paper1990/Reviewers", "ICLR.cc/2020/Conference/Paper1990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1990/-/Official_Comment"}}}, {"id": "B1e-fZBLtr", "original": null, "number": 1, "cdate": 1571340553127, "ddate": null, "tcdate": 1571340553127, "tmdate": 1571340665353, "tddate": null, "forum": "BJe8pkHFwS", "replyto": "BJe8pkHFwS", "invitation": "ICLR.cc/2020/Conference/Paper1990/-/Public_Comment", "content": {"comment": "Thanks for your paper, it is indeed a very interesting idea. However, I cannot agree with all the claims you made.\n\nFor example in Proposition 3.1 you claim as $\\xi_v^{(l+1)}$ is an unbiased estimator of the aggregation of $v$ in full GCN if $\\alpha_{u,v} = p_{u,v}/p_v$, i.e., $\\mathbb{E}(\\xi_v^{(l+1)}) = \\sum_{u\\in V} A_{v,u}x_u^{(l)}$.\n\nHowever, I think $\\xi_v^{(l+1)}$ is unbiased only condition on the last layer feature $x_u^{(l)}$, i.e.,  $\\mathbb{E}(\\xi_v^{(l+1)} | x_u^{(l)}) = \\sum_{u\\in V} A_{v,u}x_u^{(l)} $ due to the non-linear activations. You cannot ignore the non-linear activation in your proof since $\\mathbb{E}(\\sigma(x)) \\neq \\sigma(\\mathbb{E}(x))$ if $\\sigma()$ is an non-linear activation.\n\nPlease clarify if possible. Thanks.", "title": "GraphSaint is not unbiased"}, "signatures": ["~Weilin_Cong1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Weilin_Cong1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["zengh@usc.edu", "hongkuaz@usc.edu", "ajiteshs@usc.edu", "rajgopal.kannan.civ@mail.mil", "prasanna@usc.edu"], "title": "GraphSAINT: Graph Sampling Based Inductive Learning Method", "authors": ["Hanqing Zeng", "Hongkuan Zhou", "Ajitesh Srivastava", "Rajgopal Kannan", "Viktor Prasanna"], "pdf": "/pdf/b03966e181622789b1b61f35fec0d27eefc1663b.pdf", "TL;DR": "We propose a graph sampling based minibatch construction method for training deep Graph Convolutional Networks on large graphs. ", "abstract": "Graph Convolutional Networks (GCNs) are powerful models for learning representations of attributed graphs. To scale GCNs to large graphs, state-of-the-art methods use various layer sampling techniques to alleviate the \"neighbor explosion\" problem during minibatch training. We propose GraphSAINT, a graph sampling based inductive learning method that improves training efficiency and accuracy in a fundamentally different way. By changing perspective, GraphSAINT constructs minibatches by sampling the training graph, rather than the nodes or edges across GCN layers. Each iteration, a complete GCN is built from the properly sampled subgraph. Thus, we ensure fixed number of well-connected nodes in all layers. We further propose normalization technique to eliminate bias, and sampling algorithms for variance reduction. Importantly, we can decouple the sampling from the forward and backward propagation, and extend GraphSAINT with many architecture variants (e.g., graph attention, jumping connection).  GraphSAINT demonstrates superior performance in both accuracy and training time on five large graphs, and achieves new state-of-the-art F1 scores for PPI (0.995) and Reddit (0.970). ", "code": "https://github.com/GraphSAINT/GraphSAINT", "keywords": ["Graph Convolutional Networks", "Graph sampling", "Network embedding"], "paperhash": "zeng|graphsaint_graph_sampling_based_inductive_learning_method", "_bibtex": "@inproceedings{\nZeng2020GraphSAINT:,\ntitle={GraphSAINT: Graph Sampling Based Inductive Learning Method},\nauthor={Hanqing Zeng and Hongkuan Zhou and Ajitesh Srivastava and Rajgopal Kannan and Viktor Prasanna},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=BJe8pkHFwS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d858e9b1f138f778aa7fd004ea9449377c1d37.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BJe8pkHFwS", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504186684, "tmdate": 1576860576977, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper1990/Authors", "ICLR.cc/2020/Conference/Paper1990/Reviewers", "ICLR.cc/2020/Conference/Paper1990/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1990/-/Public_Comment"}}}], "count": 12}