{"notes": [{"id": "Ske9VANKDH", "original": "rJgMf88dPB", "number": 1086, "cdate": 1569439282420, "ddate": null, "tcdate": 1569439282420, "tmdate": 1577168216518, "tddate": null, "forum": "Ske9VANKDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "An Optimization Principle Of Deep Learning?", "authors": ["Cheng Chen", "Junjie Yang", "Yi Zhou"], "authorids": ["u0952128@utah.edu", "yang.4972@buckeyemail.osu.edu", "yi.zhou@utah.edu"], "keywords": [], "abstract": "Training deep neural networks (DNNs) has achieved great success in recent years. Modern DNN trainings utilize various types of training techniques that are developed in different aspects, e.g., activation functions for neurons, batch normalization for hidden layers, skip connections for network architecture and stochastic algorithms for optimization. Despite the effectiveness of these techniques, it is still mysterious how they help accelerate DNN trainings in practice. In this paper, we propose an optimization principle that is parameterized by $\\gamma>0$ for stochastic algorithms in nonconvex and over-parameterized optimization. The principle guarantees the convergence of stochastic algorithms to a global minimum with a monotonically diminishing parameter distance to the minimizer and leads to a $\\mathcal{O}(1/\\gamma K)$ sub-linear convergence rate, where $K$ is the number of iterations. Through extensive experiments, we show that DNN trainings consistently obey the $\\gamma$-optimization principle and its theoretical implications. In particular, we observe that the trainings that apply the training techniques achieve accelerated convergence and obey the principle with a large $\\gamma$, which is consistent with the $\\mathcal{O}(1/\\gamma K)$ convergence rate result under the optimization principle. We think the $\\gamma$-optimization principle captures and quantifies the impacts of various DNN training techniques and can be of independent interest from a theoretical perspective.", "pdf": "/pdf/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "paperhash": "chen|an_optimization_principle_of_deep_learning", "original_pdf": "/attachment/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "_bibtex": "@misc{\nchen2020an,\ntitle={An Optimization Principle Of Deep Learning?},\nauthor={Cheng Chen and Junjie Yang and Yi Zhou},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske9VANKDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "zqwm3tx1F3", "original": null, "number": 1, "cdate": 1576798714162, "ddate": null, "tcdate": 1576798714162, "tmdate": 1576800922317, "tddate": null, "forum": "Ske9VANKDH", "replyto": "Ske9VANKDH", "invitation": "ICLR.cc/2020/Conference/Paper1086/-/Decision", "content": {"decision": "Reject", "comment": "The paper is rejected based on unanimous reviews.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Optimization Principle Of Deep Learning?", "authors": ["Cheng Chen", "Junjie Yang", "Yi Zhou"], "authorids": ["u0952128@utah.edu", "yang.4972@buckeyemail.osu.edu", "yi.zhou@utah.edu"], "keywords": [], "abstract": "Training deep neural networks (DNNs) has achieved great success in recent years. Modern DNN trainings utilize various types of training techniques that are developed in different aspects, e.g., activation functions for neurons, batch normalization for hidden layers, skip connections for network architecture and stochastic algorithms for optimization. Despite the effectiveness of these techniques, it is still mysterious how they help accelerate DNN trainings in practice. In this paper, we propose an optimization principle that is parameterized by $\\gamma>0$ for stochastic algorithms in nonconvex and over-parameterized optimization. The principle guarantees the convergence of stochastic algorithms to a global minimum with a monotonically diminishing parameter distance to the minimizer and leads to a $\\mathcal{O}(1/\\gamma K)$ sub-linear convergence rate, where $K$ is the number of iterations. Through extensive experiments, we show that DNN trainings consistently obey the $\\gamma$-optimization principle and its theoretical implications. In particular, we observe that the trainings that apply the training techniques achieve accelerated convergence and obey the principle with a large $\\gamma$, which is consistent with the $\\mathcal{O}(1/\\gamma K)$ convergence rate result under the optimization principle. We think the $\\gamma$-optimization principle captures and quantifies the impacts of various DNN training techniques and can be of independent interest from a theoretical perspective.", "pdf": "/pdf/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "paperhash": "chen|an_optimization_principle_of_deep_learning", "original_pdf": "/attachment/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "_bibtex": "@misc{\nchen2020an,\ntitle={An Optimization Principle Of Deep Learning?},\nauthor={Cheng Chen and Junjie Yang and Yi Zhou},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske9VANKDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Ske9VANKDH", "replyto": "Ske9VANKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727727, "tmdate": 1576800280019, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1086/-/Decision"}}}, {"id": "S1gTwOG2Yr", "original": null, "number": 3, "cdate": 1571723364837, "ddate": null, "tcdate": 1571723364837, "tmdate": 1574282206479, "tddate": null, "forum": "Ske9VANKDH", "replyto": "Ske9VANKDH", "invitation": "ICLR.cc/2020/Conference/Paper1086/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "This paper proposes a \"gamma principle\" for stochastic updates in deep neural network optimization. First, the authors propose if Eq. (2) is satisfied then convergence is guaranteed (Thm. 1). Second, they use experimental results of Alexnet and Resnet-18 on cifar10/100 to show that Eq. (2) is satisfied by SGD, SGD with momentum, and Adam, with different activations and tricks like batch normalization, skip connection.\n\nPros:\n1. This paper is well written and the presentation is clear.\n2. The experiments are extensive.\n\nCons:\n1. Before Eq. (2), it is assumed that over-parameterized NNs are used as \\theta. But there is no quantization how many parameters are enough? Are the Alexnet/Resnet-18 in experiments enough over-parameterized and how can we tell that? Some quantitative conditions should be provided to show what kind of models this principle hold.\n\n2. The connection between Eq. (2) and Thm. 1 is too obvious and the gamma is just to characterize the progress of each update. Of course, large progress corresponds to fast convergence. Eq. (2) is a strong assumption rather than a theoretical contribution.\n\n3. Experiments are used to show that Eq. (2) is a \"principle\". However, the experiments are problematic as follows.\nFirst, \"we set \\theta^* to be the network parameters produced in the last training iteration\", then how do we make sure \\theta in the last training iteration is \\theta^*, even if the loss is close to (but not exactly) zero? For this point, I suggest using a teacher-student setting, where the optimal \\theta^* is already known.\nSecond, using \\theta in the last training iteration makes the experiments show the following simple fact, that methods/tricks/activations with faster convergence to certain parameter will have larger every update progress to that parameter, which is, of course, true and does not reveal an optimization principle of deep learning.\nThird, it is difficult to claim that this is a principle for general deep learning by using experiments on two datasets.\n\nOverall, I found the theory not inspiring and experiments not convincing.\n\n========Update=========\nThanks for the rebuttal.\nI have read it and using teacher-student setting is an improvement to resolve my question with respect to \\theta^*.\nHowever I would maintain my rating since\n1) the theoretical contribution is actually marginal;\n2) the argument that this \"gamma principle\" holds for over-parameterized NNs is vague in the sense (and the author did not resolve my concern of this) that for what kinds of over-parameterized NNs this would work and for what kinds of NNs it does not hold. In particular, mentioning other theory work of over-parameterized NNs is not enough, because usually in these work, the numbers of parameters in NNs are poly(n), like O(n^4), O(n^6), where n is number of training data. There is an obvious gap between poly(n) and the number of parameters in experiments. From this perspective, the experiments cannot verify the claim that this \"principle\" holds for over-parameterized NNs that mentioned by authors in the rebuttal.\n3) experiments on two datasets are not enough to claim this is a general \"principle\".\nConsidering this claim is for general over-parameterized NN optimization, I think it lacks of specifying types of NNs for which this claim would hold (of course it cannot hold for any NNs, but it possibly can hold for some NNs, and what are these NNs?), and experiments are not enough to show the generality of this claim.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1086/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1086/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Optimization Principle Of Deep Learning?", "authors": ["Cheng Chen", "Junjie Yang", "Yi Zhou"], "authorids": ["u0952128@utah.edu", "yang.4972@buckeyemail.osu.edu", "yi.zhou@utah.edu"], "keywords": [], "abstract": "Training deep neural networks (DNNs) has achieved great success in recent years. Modern DNN trainings utilize various types of training techniques that are developed in different aspects, e.g., activation functions for neurons, batch normalization for hidden layers, skip connections for network architecture and stochastic algorithms for optimization. Despite the effectiveness of these techniques, it is still mysterious how they help accelerate DNN trainings in practice. In this paper, we propose an optimization principle that is parameterized by $\\gamma>0$ for stochastic algorithms in nonconvex and over-parameterized optimization. The principle guarantees the convergence of stochastic algorithms to a global minimum with a monotonically diminishing parameter distance to the minimizer and leads to a $\\mathcal{O}(1/\\gamma K)$ sub-linear convergence rate, where $K$ is the number of iterations. Through extensive experiments, we show that DNN trainings consistently obey the $\\gamma$-optimization principle and its theoretical implications. In particular, we observe that the trainings that apply the training techniques achieve accelerated convergence and obey the principle with a large $\\gamma$, which is consistent with the $\\mathcal{O}(1/\\gamma K)$ convergence rate result under the optimization principle. We think the $\\gamma$-optimization principle captures and quantifies the impacts of various DNN training techniques and can be of independent interest from a theoretical perspective.", "pdf": "/pdf/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "paperhash": "chen|an_optimization_principle_of_deep_learning", "original_pdf": "/attachment/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "_bibtex": "@misc{\nchen2020an,\ntitle={An Optimization Principle Of Deep Learning?},\nauthor={Cheng Chen and Junjie Yang and Yi Zhou},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske9VANKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Ske9VANKDH", "replyto": "Ske9VANKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1086/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1086/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575633282602, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1086/Reviewers"], "noninvitees": [], "tcdate": 1570237742565, "tmdate": 1575633282616, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1086/-/Official_Review"}}}, {"id": "SkgvijzLtB", "original": null, "number": 2, "cdate": 1571330975221, "ddate": null, "tcdate": 1571330975221, "tmdate": 1574226048818, "tddate": null, "forum": "Ske9VANKDH", "replyto": "Ske9VANKDH", "invitation": "ICLR.cc/2020/Conference/Paper1086/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "The paper proposes a new condition: $\\gamma$-optimization principle. \nThe principle states that the inner product between the difference between the current iterate and a fixed global optimum and the stochastic gradient (or more general) is larger than the squared the gradient norm, plus the product of squared norm of the difference between the initialization and the global minimum, and the loss of the current iterate.\n\nUnder this condition, the paper shows sublinear convergence.\n\nMain Comments\uff1a\nThe proposed conditions are similar to many previous works, as pointed out by authors. With these kinds of conditions, proving global convergence is trivial. \nOne question is that the condition holds uniformly for all models and every sampled data point. There is no randomness in the condition. I would expect a condition that has some \"randomness\" in it, e.g., the condition holds in expectation over random sampling over the data.\nThe condition also requires a specific global minimizer. Because of the randomness in initialization and stochastic training, I expect the target global minimizer can change from iteration to iteration, but the current condition does not reflect that.\n\n\n--------------------------------------------------------------------------------------------\nI have read the rebuttal and I maintain the score. \nNote in the student-teacher setting even though teacher is unique, there can be multiple optimal students. I don't think this resolves my concern.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1086/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1086/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Optimization Principle Of Deep Learning?", "authors": ["Cheng Chen", "Junjie Yang", "Yi Zhou"], "authorids": ["u0952128@utah.edu", "yang.4972@buckeyemail.osu.edu", "yi.zhou@utah.edu"], "keywords": [], "abstract": "Training deep neural networks (DNNs) has achieved great success in recent years. Modern DNN trainings utilize various types of training techniques that are developed in different aspects, e.g., activation functions for neurons, batch normalization for hidden layers, skip connections for network architecture and stochastic algorithms for optimization. Despite the effectiveness of these techniques, it is still mysterious how they help accelerate DNN trainings in practice. In this paper, we propose an optimization principle that is parameterized by $\\gamma>0$ for stochastic algorithms in nonconvex and over-parameterized optimization. The principle guarantees the convergence of stochastic algorithms to a global minimum with a monotonically diminishing parameter distance to the minimizer and leads to a $\\mathcal{O}(1/\\gamma K)$ sub-linear convergence rate, where $K$ is the number of iterations. Through extensive experiments, we show that DNN trainings consistently obey the $\\gamma$-optimization principle and its theoretical implications. In particular, we observe that the trainings that apply the training techniques achieve accelerated convergence and obey the principle with a large $\\gamma$, which is consistent with the $\\mathcal{O}(1/\\gamma K)$ convergence rate result under the optimization principle. We think the $\\gamma$-optimization principle captures and quantifies the impacts of various DNN training techniques and can be of independent interest from a theoretical perspective.", "pdf": "/pdf/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "paperhash": "chen|an_optimization_principle_of_deep_learning", "original_pdf": "/attachment/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "_bibtex": "@misc{\nchen2020an,\ntitle={An Optimization Principle Of Deep Learning?},\nauthor={Cheng Chen and Junjie Yang and Yi Zhou},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske9VANKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Ske9VANKDH", "replyto": "Ske9VANKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1086/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1086/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575633282602, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1086/Reviewers"], "noninvitees": [], "tcdate": 1570237742565, "tmdate": 1575633282616, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1086/-/Official_Review"}}}, {"id": "S1eAE0MwsH", "original": null, "number": 4, "cdate": 1573494326402, "ddate": null, "tcdate": 1573494326402, "tmdate": 1573494326402, "tddate": null, "forum": "Ske9VANKDH", "replyto": "H1gGrehCOr", "invitation": "ICLR.cc/2020/Conference/Paper1086/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "We thank the reviewer for providing valuable feedback. Below is our point-to-point response. Any further comment is very welcome.\n\n1) Could you please explain ...\nA: We assume the (neural network) model has enough expressive power to interpolate all the training data samples. In our experiments, we used the non-negative cross-entropy loss function and train the total loss for a sufficient number of epochs to achieve a small value (1e-4). In this case, we empirically found that the model \\theta produced in the last training iteration achieves small loss on all the data samples, and therefore we choose it as an approximation of the \\theta*.   \n\n2) & 3) You have not mentioned the loss function that ...\nA: We thank the reviewer for providing valuable suggestions. We use the cross-entropy loss in the experiments. We understand that an exact minimum cannot be achieved for this loss and use the last training iteration to serve as an approximation. To fix this issue, we will adopt the MSE loss under the teacher-student setting to guarantee the existence of a known common global minimum (Please see our general response). \n\n4) The assumption that the author(s) use in the paper...\nA: In general, one can propose many different optimization conditions that guarantee convergence in nonconvex optimization, e.g., the regularity condition in (Zhang et al. 2017b) for nonconvex phase retrieval, star-convex condition in (Zhou et al. 2019) for deep learning. However, these conditions cannot characterize the effects of the deep learning training techniques explored in this paper. What we empirically found is that the proposed \\gamma-principle well characterizes these effects in terms of the value \\gamma. Regarding the concern on the minimizer \\theta*, we will adopt the teacher-student setting to bridge the gap between our theory and experiments.\n\n5) There is related work that you may...\nA: We thank the reviewer for recommending this paper on stochastic optimization theory. We will cite  and discuss it in the related works.  \n\nMinor: we will use a more informative title in the revision.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1086/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1086/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Optimization Principle Of Deep Learning?", "authors": ["Cheng Chen", "Junjie Yang", "Yi Zhou"], "authorids": ["u0952128@utah.edu", "yang.4972@buckeyemail.osu.edu", "yi.zhou@utah.edu"], "keywords": [], "abstract": "Training deep neural networks (DNNs) has achieved great success in recent years. Modern DNN trainings utilize various types of training techniques that are developed in different aspects, e.g., activation functions for neurons, batch normalization for hidden layers, skip connections for network architecture and stochastic algorithms for optimization. Despite the effectiveness of these techniques, it is still mysterious how they help accelerate DNN trainings in practice. In this paper, we propose an optimization principle that is parameterized by $\\gamma>0$ for stochastic algorithms in nonconvex and over-parameterized optimization. The principle guarantees the convergence of stochastic algorithms to a global minimum with a monotonically diminishing parameter distance to the minimizer and leads to a $\\mathcal{O}(1/\\gamma K)$ sub-linear convergence rate, where $K$ is the number of iterations. Through extensive experiments, we show that DNN trainings consistently obey the $\\gamma$-optimization principle and its theoretical implications. In particular, we observe that the trainings that apply the training techniques achieve accelerated convergence and obey the principle with a large $\\gamma$, which is consistent with the $\\mathcal{O}(1/\\gamma K)$ convergence rate result under the optimization principle. We think the $\\gamma$-optimization principle captures and quantifies the impacts of various DNN training techniques and can be of independent interest from a theoretical perspective.", "pdf": "/pdf/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "paperhash": "chen|an_optimization_principle_of_deep_learning", "original_pdf": "/attachment/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "_bibtex": "@misc{\nchen2020an,\ntitle={An Optimization Principle Of Deep Learning?},\nauthor={Cheng Chen and Junjie Yang and Yi Zhou},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske9VANKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Ske9VANKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1086/Authors", "ICLR.cc/2020/Conference/Paper1086/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1086/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1086/Reviewers", "ICLR.cc/2020/Conference/Paper1086/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1086/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1086/Authors|ICLR.cc/2020/Conference/Paper1086/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161482, "tmdate": 1576860560552, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1086/Authors", "ICLR.cc/2020/Conference/Paper1086/Reviewers", "ICLR.cc/2020/Conference/Paper1086/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1086/-/Official_Comment"}}}, {"id": "ByeL56GDjB", "original": null, "number": 3, "cdate": 1573494158333, "ddate": null, "tcdate": 1573494158333, "tmdate": 1573494158333, "tddate": null, "forum": "Ske9VANKDH", "replyto": "SkgvijzLtB", "invitation": "ICLR.cc/2020/Conference/Paper1086/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "We thank the reviewer for providing valuable feedback. Below is our point-to-point response. Any further comment is very welcome.\n\n1) The proposed conditions are similar...\nA: We agree that the theoretical contribution of this work is not substantial. Our primary goal is to point out that the proposed simple optimization principle matches the optimization in practical deep learning and can characterize the effects of deep learning training techniques, and we further verify via extensive experiments. \n\n2) One question is that the condition holds uniformly...\nA: Our experiments show that such a condition holds in practice for all the sampled data. We think this is because the network model is large enough to guarantee the existence of (approximate) common global minimizers, which encourages all loss functions to have a similar optimization path. In fact, one can propose the condition to be held in expectation over the random sampling and similar theoretical results can be derived. However, such a condition is very difficult to verify as the samples drawn within an epoch are not independent under cyclic sampling with reshuffle.  \n\n3) The condition also requires a specific global minimizer...\nA: We will fix this issue by using a teacher-student setting. Please see our general response for the details.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1086/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1086/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Optimization Principle Of Deep Learning?", "authors": ["Cheng Chen", "Junjie Yang", "Yi Zhou"], "authorids": ["u0952128@utah.edu", "yang.4972@buckeyemail.osu.edu", "yi.zhou@utah.edu"], "keywords": [], "abstract": "Training deep neural networks (DNNs) has achieved great success in recent years. Modern DNN trainings utilize various types of training techniques that are developed in different aspects, e.g., activation functions for neurons, batch normalization for hidden layers, skip connections for network architecture and stochastic algorithms for optimization. Despite the effectiveness of these techniques, it is still mysterious how they help accelerate DNN trainings in practice. In this paper, we propose an optimization principle that is parameterized by $\\gamma>0$ for stochastic algorithms in nonconvex and over-parameterized optimization. The principle guarantees the convergence of stochastic algorithms to a global minimum with a monotonically diminishing parameter distance to the minimizer and leads to a $\\mathcal{O}(1/\\gamma K)$ sub-linear convergence rate, where $K$ is the number of iterations. Through extensive experiments, we show that DNN trainings consistently obey the $\\gamma$-optimization principle and its theoretical implications. In particular, we observe that the trainings that apply the training techniques achieve accelerated convergence and obey the principle with a large $\\gamma$, which is consistent with the $\\mathcal{O}(1/\\gamma K)$ convergence rate result under the optimization principle. We think the $\\gamma$-optimization principle captures and quantifies the impacts of various DNN training techniques and can be of independent interest from a theoretical perspective.", "pdf": "/pdf/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "paperhash": "chen|an_optimization_principle_of_deep_learning", "original_pdf": "/attachment/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "_bibtex": "@misc{\nchen2020an,\ntitle={An Optimization Principle Of Deep Learning?},\nauthor={Cheng Chen and Junjie Yang and Yi Zhou},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske9VANKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Ske9VANKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1086/Authors", "ICLR.cc/2020/Conference/Paper1086/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1086/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1086/Reviewers", "ICLR.cc/2020/Conference/Paper1086/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1086/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1086/Authors|ICLR.cc/2020/Conference/Paper1086/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161482, "tmdate": 1576860560552, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1086/Authors", "ICLR.cc/2020/Conference/Paper1086/Reviewers", "ICLR.cc/2020/Conference/Paper1086/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1086/-/Official_Comment"}}}, {"id": "SygoeaGPsS", "original": null, "number": 2, "cdate": 1573494003210, "ddate": null, "tcdate": 1573494003210, "tmdate": 1573494003210, "tddate": null, "forum": "Ske9VANKDH", "replyto": "S1gTwOG2Yr", "invitation": "ICLR.cc/2020/Conference/Paper1086/-/Official_Comment", "content": {"title": "Response to Review #3 ", "comment": "We thank the reviewer for providing valuable feedback. Below is our point-to-point response. Any further comment is very welcome.\n\n1). Before Eq. (2), ...\nA: In our experiments, we found that all the neural network models can be trained to achieve 1e-4 total loss, which is very close to the global minimum. We think this justifies that these models are over-parameterized. On the other hand, we think a quantitative bound for over-parameterization can be obtained by following other theoretical works, e.g., Gradient Descent Finds Global Minima of Deep Neural Networks. We leave this part of development for future study.\n\n2). The connection between Eq. (2) ...\nA: We agree that fast convergence must correspond to large progress. However, the key point here is to quantify such progress to characterize the effects of different deep learning training techniques. We want to show that the proposed simple principle can quantify these effects and match the empirical observations well.  We have pointed out in the paper that there are other existing optimization principles that guarantee convergence in nonconvex machine learning, but they do not match the experimental observations in training deep networks under various training techniques.\n \n3). Experiments are used to show ...\nA: We thank the reviewer for providing valuable feedback. We appreciate the suggestion to use the teacher-student setting to guarantee the existence of a common global minimizer. Based on your suggestion, we plan to use a teacher-student setting in our experiments where the optimal \\theta* is already known. Please find the details in our general response. \n\n4) Second, using \\theta ...\nA: We want to point out that \\gamma along does not correspond to the per-step progress, which actually corresponds to the last term in eq(4) in the appendix. In that term, the loss gap, in general, varies in the optimization process and therefore the per-step progress is not a constant. Under the proposed principle, we showed that the \\gamma fully determines the convergence speed and is verified in the experiments.\n\n5) Third, it is difficult to claim ...\nA: We plan to do experiments on more large and complex datasets (e.g. ImageNet dataset).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1086/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1086/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Optimization Principle Of Deep Learning?", "authors": ["Cheng Chen", "Junjie Yang", "Yi Zhou"], "authorids": ["u0952128@utah.edu", "yang.4972@buckeyemail.osu.edu", "yi.zhou@utah.edu"], "keywords": [], "abstract": "Training deep neural networks (DNNs) has achieved great success in recent years. Modern DNN trainings utilize various types of training techniques that are developed in different aspects, e.g., activation functions for neurons, batch normalization for hidden layers, skip connections for network architecture and stochastic algorithms for optimization. Despite the effectiveness of these techniques, it is still mysterious how they help accelerate DNN trainings in practice. In this paper, we propose an optimization principle that is parameterized by $\\gamma>0$ for stochastic algorithms in nonconvex and over-parameterized optimization. The principle guarantees the convergence of stochastic algorithms to a global minimum with a monotonically diminishing parameter distance to the minimizer and leads to a $\\mathcal{O}(1/\\gamma K)$ sub-linear convergence rate, where $K$ is the number of iterations. Through extensive experiments, we show that DNN trainings consistently obey the $\\gamma$-optimization principle and its theoretical implications. In particular, we observe that the trainings that apply the training techniques achieve accelerated convergence and obey the principle with a large $\\gamma$, which is consistent with the $\\mathcal{O}(1/\\gamma K)$ convergence rate result under the optimization principle. We think the $\\gamma$-optimization principle captures and quantifies the impacts of various DNN training techniques and can be of independent interest from a theoretical perspective.", "pdf": "/pdf/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "paperhash": "chen|an_optimization_principle_of_deep_learning", "original_pdf": "/attachment/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "_bibtex": "@misc{\nchen2020an,\ntitle={An Optimization Principle Of Deep Learning?},\nauthor={Cheng Chen and Junjie Yang and Yi Zhou},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske9VANKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Ske9VANKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1086/Authors", "ICLR.cc/2020/Conference/Paper1086/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1086/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1086/Reviewers", "ICLR.cc/2020/Conference/Paper1086/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1086/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1086/Authors|ICLR.cc/2020/Conference/Paper1086/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161482, "tmdate": 1576860560552, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1086/Authors", "ICLR.cc/2020/Conference/Paper1086/Reviewers", "ICLR.cc/2020/Conference/Paper1086/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1086/-/Official_Comment"}}}, {"id": "Hkg8nsMwiH", "original": null, "number": 1, "cdate": 1573493677667, "ddate": null, "tcdate": 1573493677667, "tmdate": 1573493677667, "tddate": null, "forum": "Ske9VANKDH", "replyto": "Ske9VANKDH", "invitation": "ICLR.cc/2020/Conference/Paper1086/-/Official_Comment", "content": {"title": "General Response", "comment": "We thank the reviewers for providing much valuable feedback. The reviewers are concerned about the use of the last iteration as an approximate common global minimizer in the current experiments. We plan to fix this issue by using a teacher-student setting as follows. We will train the model first using hard labels (the original labels) and obtain the model in the last training iteration, which is referred to as the teacher model. Then, we use the teacher model to get the soft labels (the probability mass after softmax) for all the training data samples. Then, we will train a student model (with the same architecture as the teacher model) from scratch on the training data with the soft labels. For this training, we will use either KL divergence or MSE loss, and it is clear that the teacher model is the common global minimizer for these loss functions on the modified data with soft labels. Lastly, we will verify the \\gamma-optimization principle using the training path of the student model. We will also explore more datasets such as the imagenet."}, "signatures": ["ICLR.cc/2020/Conference/Paper1086/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1086/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Optimization Principle Of Deep Learning?", "authors": ["Cheng Chen", "Junjie Yang", "Yi Zhou"], "authorids": ["u0952128@utah.edu", "yang.4972@buckeyemail.osu.edu", "yi.zhou@utah.edu"], "keywords": [], "abstract": "Training deep neural networks (DNNs) has achieved great success in recent years. Modern DNN trainings utilize various types of training techniques that are developed in different aspects, e.g., activation functions for neurons, batch normalization for hidden layers, skip connections for network architecture and stochastic algorithms for optimization. Despite the effectiveness of these techniques, it is still mysterious how they help accelerate DNN trainings in practice. In this paper, we propose an optimization principle that is parameterized by $\\gamma>0$ for stochastic algorithms in nonconvex and over-parameterized optimization. The principle guarantees the convergence of stochastic algorithms to a global minimum with a monotonically diminishing parameter distance to the minimizer and leads to a $\\mathcal{O}(1/\\gamma K)$ sub-linear convergence rate, where $K$ is the number of iterations. Through extensive experiments, we show that DNN trainings consistently obey the $\\gamma$-optimization principle and its theoretical implications. In particular, we observe that the trainings that apply the training techniques achieve accelerated convergence and obey the principle with a large $\\gamma$, which is consistent with the $\\mathcal{O}(1/\\gamma K)$ convergence rate result under the optimization principle. We think the $\\gamma$-optimization principle captures and quantifies the impacts of various DNN training techniques and can be of independent interest from a theoretical perspective.", "pdf": "/pdf/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "paperhash": "chen|an_optimization_principle_of_deep_learning", "original_pdf": "/attachment/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "_bibtex": "@misc{\nchen2020an,\ntitle={An Optimization Principle Of Deep Learning?},\nauthor={Cheng Chen and Junjie Yang and Yi Zhou},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske9VANKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Ske9VANKDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1086/Authors", "ICLR.cc/2020/Conference/Paper1086/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1086/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1086/Reviewers", "ICLR.cc/2020/Conference/Paper1086/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1086/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1086/Authors|ICLR.cc/2020/Conference/Paper1086/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504161482, "tmdate": 1576860560552, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1086/Authors", "ICLR.cc/2020/Conference/Paper1086/Reviewers", "ICLR.cc/2020/Conference/Paper1086/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1086/-/Official_Comment"}}}, {"id": "H1gGrehCOr", "original": null, "number": 1, "cdate": 1570844729853, "ddate": null, "tcdate": 1570844729853, "tmdate": 1572972514533, "tddate": null, "forum": "Ske9VANKDH", "replyto": "Ske9VANKDH", "invitation": "ICLR.cc/2020/Conference/Paper1086/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary: \n\nThis paper proposes an optimization principle that is called \\gamma-optimization principle for stochastic algorithms (SA) in nonconvex and over-parametrized optimization. The author(s) provide convergence results under this \u201c\\gamma-optimization principle\u201d assumption. Numerical experiments are conducted on classification datasets CIFAR-10 and CIFAR-100 for Alexnet and Resnet-18 with different activation functions. \n\nComments: \n\n1) Could you please explain how you could achieve the value of \\theta* (common global minimizer for the loss on all individual component functions)? It is unclear to me how you could obtain it. \n\n2) You have not mentioned the loss function that you are using for your numerical experiments. From my view, you are using softmax cross-entropy loss for classification problems (CIFAR-10, CIFAR-100). Can you show that Fact 1 is true for softmax cross-entropy loss? I wonder how you could train the total loss to achieve zero for this loss. \n\n3) Fact 1 with over-parameterized model could be true if the loss, for example, is mean square (for regression problems). Therefore, I would suggest you to consider other numerical examples rather than classification problems. If not, the numerical part is not very consistent with the theoretical part. \n\n4) The assumption that the author(s) use in the paper, that is, \\gamma-optimization principle in Definition 1, is indeed strong and not reasonable. You simply assume what you want in order to achieve the convergence result. It is not easy to verify this assumption since you include \\theta* unless it has only a unique solution. Note that the learning rate (eta) and gamma are very sensitive here and it is not clear how to determine these values. \n\n5) There is related work that you may need to consider: Vaswani et al 2019, \"Fast and Faster Convergence of SGD for Over-Parameterized Models (and an Accelerated Perceptron)\" in AISTATS 2019. \n\nI think the paper still needs lots of work to be ready. Theoretical result is not strong and the numerical experiments are not convincing. I do not support the publication for this paper at the current state. \n\nMinor: \n1) I am not really why you have a question mark (?) in the title. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1086/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1086/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Optimization Principle Of Deep Learning?", "authors": ["Cheng Chen", "Junjie Yang", "Yi Zhou"], "authorids": ["u0952128@utah.edu", "yang.4972@buckeyemail.osu.edu", "yi.zhou@utah.edu"], "keywords": [], "abstract": "Training deep neural networks (DNNs) has achieved great success in recent years. Modern DNN trainings utilize various types of training techniques that are developed in different aspects, e.g., activation functions for neurons, batch normalization for hidden layers, skip connections for network architecture and stochastic algorithms for optimization. Despite the effectiveness of these techniques, it is still mysterious how they help accelerate DNN trainings in practice. In this paper, we propose an optimization principle that is parameterized by $\\gamma>0$ for stochastic algorithms in nonconvex and over-parameterized optimization. The principle guarantees the convergence of stochastic algorithms to a global minimum with a monotonically diminishing parameter distance to the minimizer and leads to a $\\mathcal{O}(1/\\gamma K)$ sub-linear convergence rate, where $K$ is the number of iterations. Through extensive experiments, we show that DNN trainings consistently obey the $\\gamma$-optimization principle and its theoretical implications. In particular, we observe that the trainings that apply the training techniques achieve accelerated convergence and obey the principle with a large $\\gamma$, which is consistent with the $\\mathcal{O}(1/\\gamma K)$ convergence rate result under the optimization principle. We think the $\\gamma$-optimization principle captures and quantifies the impacts of various DNN training techniques and can be of independent interest from a theoretical perspective.", "pdf": "/pdf/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "paperhash": "chen|an_optimization_principle_of_deep_learning", "original_pdf": "/attachment/e08ad8677c7ba430ab90dd492d78d0cff750aebb.pdf", "_bibtex": "@misc{\nchen2020an,\ntitle={An Optimization Principle Of Deep Learning?},\nauthor={Cheng Chen and Junjie Yang and Yi Zhou},\nyear={2020},\nurl={https://openreview.net/forum?id=Ske9VANKDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Ske9VANKDH", "replyto": "Ske9VANKDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1086/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1086/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575633282602, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1086/Reviewers"], "noninvitees": [], "tcdate": 1570237742565, "tmdate": 1575633282616, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1086/-/Official_Review"}}}], "count": 9}