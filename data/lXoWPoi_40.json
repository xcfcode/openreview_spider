{"paper": {"title": "Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule", "authors": ["Nikhil Iyer", "V Thejas", "Nipun Kwatra", "Ramachandran Ramjee", "Muthian Sivathanu"], "authorids": ["~Nikhil_Iyer1", "thejasvenkatesh97@gmail.com", "~Nipun_Kwatra1", "~Ramachandran_Ramjee1", "~Muthian_Sivathanu1"], "summary": "We present a hypothesis on the density of wide and narrow minima in deep learning landscapes, which also motivates a principled explore-exploit learning rate schedule.", "abstract": "Several papers argue that wide minima generalize better than narrow minima. In this paper, through detailed experiments that not only corroborate the generalization properties of wide minima, we also provide empirical evidence for a new hypothesis that the density of wide minima is likely lower than the density of narrow minima. Further, motivated by this hypothesis, we design a novel explore-exploit learning rate schedule. On a variety of image and natural language datasets, compared to their original hand-tuned learning rate baselines, we show that our explore-exploit schedule can result in either up to 0.84% higher absolute accuracy using the original training budget or up to 57% reduced training time while achieving the original reported accuracy. For example, we achieve state-of-the-art (SOTA) accuracy for IWSLT'14 (DE-EN) and WMT'14 (DE-EN) datasets by just modifying the learning rate schedule of a high performing model.", "keywords": ["deep learning", "learning rate", "generalization"]}, "meta": {"decision": "Reject", "comment": "The reviewers are concerned about the novelty of the proposed learning rate schedule, the rigor of the empirical validation, and the relationship between the results and the discussion of sharp vs. local minima. I invite the authors to incorporate reviewers' comments and resubmit to other ML venues."}, "review": {"sSRjZq41mGt": {"type": "rebuttal", "replyto": "YdW-XaJFCP2", "comment": "1. We will compute the delta-w and add it to the paper but our past experience with computing delta-w across runs suggest that i) there is little correlation between delta-w and accuracy and ii) running longer generally increases delta-w, most likely due to the high dimensional landscape.  \n\n\n2. In table-2 the improvement from increasing the number of fine-scale explore stagnates eventually. The issue with changing LR for second stage of the step schedule is that the hyper-parameter search space is very large since we have to identify both LR and step duration. One simplification may be to keep step duration the same and vary LR -- we are happy to run these experiments and add them to the paper. From experience, we found that linear decay worked well mainly because it was able to do the finerscale \u201cexploit\u201d at all learning rates rather than a fixed LR of step schedule.", "title": "Response to questions"}, "6XjWSc60w6v": {"type": "rebuttal", "replyto": "yrXJQIiSDsH", "comment": "\nWe don't agree that we are simply labelling a well-known schedule as knee in this paper. The link you provide is simply a link to papers that use the well-known linear decay schedule (warmup is an *independent* aspect that is not the focus of our paper).  Similarly, your comment \"One could view the proposed schedule as a special case of a polynomial decay with an exponent of 1.\" is again suggesting that knee is simply linear decay, which is incorrect.   Yes, linear decay is used in  the exploit phase of knee but, as we show in the paper, the explore phase of knee that uses a high and constant learning rate (lr) makes a huge difference in terms of final test accuracy.  Can you please point to actual papers that use a schedule 'similar' to knee that includes *both the explore and exploit phases*?  \n\nAnother piece of evidence that substantiates the claim that a knee-like schedule is not common/popular is to look at lr schedules that are supported by well known frameworks. Pytorch, for example, has a number of in-built lr schedulers in its torch.optim.lr_scheduler class (https://pytorch.org/docs/stable/optim.html) such as StepLR, MultiStepLR, ExponentialLR, CosineAnnealingLR, ReduceLROnPlateau, CyclicLR (which includes the triangularLR shape of linear increase/linear decrease that you point in the link above), OneCycleLR, etc. but nothing that is 'similar' to knee.    Similarly, huggingface, a popular repository for training language models has a number of lr schedules supported by default (https://huggingface.co/transformers/main_classes/optimizer_schedules.html) but nothing that is 'similar' to knee. Sure, one can code up the knee schedule without using a separate scheduler class but such an argument is applicable to most of the above schedules as well. Another indicator that knee is not popular/well-known is that not a *single baseline schedule* for any of the popular deep learning tasks that we have evaluated in the paper used a schedule that is 'similar' to knee.\n\nThus, we believe that knee schedule is neither well-known nor commonly used. Knee may be 'similar' to well-known schedules from an abstract point of view (just like, say, RELU is 'similar' to sigmoid) but the actual learning rate values used in each step in knee is different enough that knee is able to significantly outperform all other well-known schedules in terms of higher final test accuracy, including achieving SOTA results in two tasks!", "title": "Knee schedule is neither well-known nor commonly used"}, "NA5C-W-K4PE": {"type": "rebuttal", "replyto": "MCVDXhAetf", "comment": "8. \u201cis early stopping used in all experiments? If not, why?\u201d: It is used in experiments where a separate test and validation set was available. Both baselines and knee used the same policy for all experiments. As reported in the paper \u201cFor all experiments, we used an out of the box policy, where we only change the learning rate schedule, without modifying anything else.\u201d\n\n9. Writing: Thank you for the feedback. We will incorporate the suggestions. \n\n10. Prior work: Thank you for the suggestions. We will add a discussion on these. \n\n11. Experimental setup: Our approach for evaluation has been to take papers which reported SOTA (or close to SOTA) results at the time of publishing and had reasonable compute requirements, and use the LR schedules used in those papers as our baselines. Since learning rate tuning (both tuning the schedule params and trying different schedules) is almost always done to achieve the best results, we regard our baselines to be competitive. That said, we do want to point out that in many experiments, such as BERT pre-training, the WMT, IWSLT translation tasks the competitive baselines were linear decays which do not have a tuning parameter other than the seed LR. \n\n12. Limited novelty: Please see common comment titled \"Response to reviewers (common)\"", "title": "Response to Reviewer #4 (part 2/2)"}, "NaY-1hcLp-9": {"type": "rebuttal", "replyto": "MCVDXhAetf", "comment": "1. Comparison with baselines: \n\n    * \u201cwere the baseline schedules also retuned for the reduced training budget\u201d: For BERT and WMT, IWSLT transformer runs, linear decay was the baseline schedule which has no hyperparameters and thus required no tuning. Please see Table-7 where we outperform the linear decay baseline for these experiments.  \n\n    * \u201cMLPerf competition for ImageNet there have been schedules \u2026.. that have been tuned to reach 75.9% in only 64 epochs\u201d: We assume the reviewer is referring to https://arxiv.org/pdf/1909.09756.pdf  Note that, in order to achieve 75.9 in 64 epochs, they, apart from tuning the learning rate schedule, i) use the adaptive learning rate scaling LARS optimize, ii) change LARS update to use \u2018unscaled\u2019 momentum and iii) tune the momentum parameter. Thus, the comparison of Knee with MLPerf results is not apples-to-apples in so many dimensions, including of course the batch size.  We are happy to run the MLPerf\u2019s quadratic decay on standard small-batch ImageNet training with SGD, but based on our experience, we do not expect it to perform as well as Knee.\n\n2. \u201cSome of the results seem misleading as well; in the training curve figures 6, 7, 8, 9, 10 in the appendix, it seems odd that the proposed method only catches up to the (untuned) baselines towards the very end of training, and that this was not mentioned in the main text.\u201d\n    * We would request the reviewer to clarify what is \u201cmisleading\u201d about this. It is well-known that higher learning rates don\u2019t perform well earlier on in the training (in fact we talk about exactly this in our paper \u2013 see Figure 1). Since we run at a higher learning rate than the baselines during the initial part, the above behavior in the initial stages is *expected*. Only when the learning rate gets low during the end of the decay, the optimizer is able to get to the bottom of the wide minima and the test accuracies see a bump (See Figure 5 for example). \n \n    * Also, please can the reviewer explain why they say \u201cuntuned\u201d baselines? Figures 6-10 compare full budget runs which use heavily tuned baselines which in most cases were used by the authors of the papers which achieved SOTA at the time of publishing (e.g., see the BERT pre-training experiment, or the MAT experiment where the baseline is close to SOTA currently, and we achieved a new SOTA on the IWSLT\u201914 (DE-EN) and WMT\u201914 (DE-EN) datasets).\n\n3. \u201cUsing the same initial LR for the proposed and baseline methods is useful, however it is insufficient to demonstrate that the proposed method could still perform well under different initial conditions\"  \nPlease see Appendix D where we evaluate the learning rate sensitivity of our schedule. We observed that the seed learning rate can impact the final accuracy, but Knee schedule is not highly sensitive to it (see Table 17). In fact, we can achieve higher accuracy than reported by tuning the seed LR as well. However, we did not do that due to time constraints.\n\n4. Sharpness: \n    * \u201cit would help to show curvature metrics at frequent intervals during training to confirm this hypothesis\u201d: Unfortunately, the sharpness / curvature metrics make sense only near the minimum and not far away from it -- e.g. it doesn\u2019t tell much if farther from the minimum we have a high/low curvature. We also verified this by empirically evaluating curvature at intermediate points to see if they can give an estimate of the curvature at the minimum or close to it, but we found that there is little correlation between them. \n\n    * \u201cThe sharpness values in Figure 2 are interesting, but I am unable to determine how impressive they are given that they are not compared to sharpness values for any other schedules, so I don\u2019t know what the baseline numbers should be.\u201d: Just to be clear, we did not use our Knee schedule for Figure 2 and used a step schedule with different durations for the high (explore) LR portion. Since all LR schedules don\u2019t permit an explore duration tuning (e.g. linear/cosine decay), we are not sure how we will do the suggested evaluation on these baselines.\n\n5. \u201cThe maximization problem is solved by applying 1000 iterations of projected gradient ascent\u201d. How was 1000 chosen?\u201d: We simply use a very large number so that the optimization converged. \n\n6. \u201cWhat are the stddevs of the results in Tables 6, 7, 18, 19, 20, 21, 26? \u201c: Tables 18, 19, 20, 21, 26 already report the stddev in parenthesis. For Table 6, 7 the stddevs are mentioned in the detailed tables \u2013 Tables 3-5, 10-13 and 18-24. We will incorporate these into Table 6 itself if needed.\n\n7. \u201cThe curves in Figure 7 seem extremely similar, it would help to plot the loss on a log scale\u201d: Sure, we will update that.", "title": "Response to Reviewer #4 (part 1/2)"}, "wrv0Pu-dDBx": {"type": "rebuttal", "replyto": "XL_eVUnzZ--", "comment": "1. Hypothesis validation: \u201cFor example, it is also possible that wide minima are farther away from the initialization.\u201d -- we already performed an experiment to rule this out (please see Section 2 \u2013 second last paragraph on page-3), where we ran very long training experiment (cifar10 with 10000 epochs or 50 times the normal training run of 200 epochs) with a low LR to allow it enough time to reach farther from initialization. Although the training converged, the final test accuracy was much worse than knee schedule on the 200 epoch run.  \nApart from this, we did several other empirical studies to validate the hypothesis, and found that the hypothesis could explain all the observations. For example, see Figures 2, 3 where we predict the qualitative distribution of accuracies and sharpness from our hypothesis and observe the same experimentally. We also detail more experiments in Appendix A on a different dataset and learning rate schedule and had similar observations.  \nWe believe that we have done rigorous empirical verification of our hypotheses, but would be happy to do more experiments and welcome suggestions from the reviewers. \n\n2. \u201cThe proposed LR schedule does not seem necessary. One could easily achieve the same purpose by existing LR schedules, e.g. use a step decay LR schedule.\u201d: This is actually invalidated in our experiments, where we see that knee schedule gives a bump over such baselines (e.g. see Table 3 where Knee has a 0.8% absolute gain on top-1 accuracy on ImageNet dataset over the step schedule baseline). Moreover, our hypothesis enables a more principled design of learning rate schedules rather than the heuristic approaches used today. \n\n3. Limited novelty: Please see common comment titled \"Response to reviewers (common)\" ", "title": "Response to Reviewer #1"}, "EX_5Mm61zy7": {"type": "rebuttal", "replyto": "yV0Kxzm0rCp", "comment": "1. Validation of our hypothesis / conjecture: We did multiple empirical studies to validate the hypothesis, and found that the hypothesis could explain all the observations. For example, see Figures 2, 3 where we predict the qualitative distribution of accuracies and sharpness from our hypothesis and observe the same experimentally; we ruled out the argument that larger distance covered by large learning rates is the reason for better performance by running a very long training with a low LR which gave much worse performance (see Section-2 where we run Cifar10 for 10000 epochs, 50 times the typical training duration). We did more experiments in Appendix A on a different dataset and learning rate schedule and had similar observations.  \nWe believe that we have done rigorous empirical verification of our hypotheses, but would be happy to do more experiments and welcome suggestions from the reviewers. We acknowledge that a theoretical analysis of this phenomenon would help move our \u2018hypothesis\u2019 towards a \u2018theory\u2019 of wide minima in deep learning but that is outside the scope of this paper.  \n\n2. Explore hyperparameter: Yes, this is a hand tuned parameter, however we found this to be quite easy to tune as typically higher explore helps to a point after which it starts hurting. Thus, a simple binary search suffices.  \nWe are also exploring principled methods for automatically finding the optimal explore duration, by estimating the \u201cwidth\u201d of the landscape during explore phase and switching to exploit as soon as we identify the landscape is wide enough. Getting this estimate, however, is tricky given high dimensionality of DNNs and since intermediate points in the landscape may be far from the minimum. \n\n3. Limited novelty: Please see common comment titled \"Response to reviewers (common)\"", "title": "Response to Reviewer #3"}, "m6f-H0hyLxD": {"type": "rebuttal", "replyto": "lXoWPoi_40", "comment": "We thank the reviewers for their comments.  \n\nLimited novelty: One common comment from several reviewers is that the solution proposed in the paper is too simple or that there is a lack of novelty. We would like to point out that while the knee schedule may be simple, it provides enormous value to researchers and practitioners in terms of higher test accuracy and/or reduced training time. For example, we show that substituting a complex inverse-square-root decay by the simpler knee schedule can help achieve a *state-of-the-art* result, a non-trivial outcome that few papers achieve. Similarly, while ResNet/Imagenet is typically trained with a complicated step schedule over 90 epochs (that requires tuning multiple hyper-parameters such as number of steps, length of each step, decay size between steps, etc.), a simpler knee schedule can achieve the same 90 epoch accuracy in 50 epochs, or almost half the time. Thus, we believe that the simplicity of knee is an advantage, not a drawback, and the community can benefit immensely from using a simpler knee schedule instead of complex schedules like inverse-square-root or step decay that don\u2019t perform as well.", "title": "Response to reviewers (common)"}, "LBfSNOzOZ2Q": {"type": "rebuttal", "replyto": "fyxp7Jdn6Ww", "comment": "1. Justification for keeping LR high even when loss stagnates: According to our hypothesis the density of narrow minima is much higher than that of wide ones. Thus, with a high probability, the optimizer will find itself in a narrow minimum in the initial phase of training. Training at a high enough LR will allow the optimizer to \u201cjump\u201d out of these narrow minima as the step size would be large enough to come out of them. However, once it reaches a wide enough minimum, it will get stuck there. We thus train the optimizer long enough, so that it gets \u201cstuck\u201d in such a wide minimum. We do discuss this aspect in section 2, but will add more details. \nIs our hypothesis the most reasonable explanation: This is hard to say definitively, but the simplicity of our hypothesis and the fact that it explains all the observations of our experiments gives us confidence. We also invoke the Occam's razor in this regard. \n\n2. Table 1: In our experiments, higher explore (up to a point) helped with improving the generalization performance. We saw this behavior consistently across all experiments. The first explore stage is needed to land in a wide minimum region, and the second exploit stage is needed to descend to the bottom of this wide minimum region. \n\n3. Warmup relationship: Warmup is mostly needed for stability reasons, either because optimizers such as Adam have high initial variance (see [1]), or because the learning rate is scaled too high to accommodate high batch sizes. Without warmup, the training procedure typically becomes unstable. We treat warmup as orthogonal to our method, and simply use the same warmup duration as used by baselines. You are right that warmup + exploration + decay has resemblance to cos [-pi/2, pi/2]. Similarly, exploration + decay has resemblance to cos [0, pi/2]. \n\n4. Table 6 standard deviations: These are mentioned in the detailed tables \u2013 Tables 3-5, 10-13 and 18-24. We will incorporate these into Table 6 itself if needed.\n\n[1] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265, 2019 ", "title": "Response to Reviewer #2"}, "MCVDXhAetf": {"type": "review", "replyto": "lXoWPoi_40", "review": "Overview:\nOverall I believe the comparisons to baselines seem too problematic to understand the value of the proposed method. Regarding the reduced training budget results (\u201cKnee schedule can achieve the same accuracy as the baseline with a much reduced training budget\u201d), were the baseline schedules also retuned for the reduced training budget? If not, this seems like an unfair advantage to the proposed method. For example, in the MLPerf competition (https://arxiv.org/abs/1910.01500) for ImageNet there have been schedules (consisting of a linear warmup followed by quadratic decay) that have been tuned to reach 75.9% in only 64 epochs, even at massive batch sizes, implying that the baseline schedule in Table 3 could likely do much better than what is reported if it was retuned with the same number of trials as the proposed method, or if a more competitive baseline schedule was used. Some of the results seem misleading as well; in the training curve figures 6, 7, 8, 9, 10 in the appendix, it seems odd that the proposed method only catches up to the (untuned) baselines towards the very end of training, and that this was not mentioned in the main text. For example:\n-on CIFAR10 the baseline beats the proposed method until the final *5 out of 200* epochs of training\n-on BERT_LARGE pretraining it is unclear from the plots when the proposed method beats the baseline as the curves are so similar\n-on WMT\u201914 (EN-DE) the baseline beats the proposed method until the final 54 out of 70 epochs of training\n-on IWSLT\u201914 (DE-EN) the baseline and proposed method cross each other a few times, the final time being at epoch 41 of 50\n-on IWSLT\u201914 (DE-EN) with the MAT network, the baseline and proposed method cross each other a few times, the final time being at epoch 330 of 400\nWhile it is not invalid for a proposed method to overtake a baseline towards the end of training, these results indicate that perhaps if the baselines were retuned, they could maintain their better performance for the last few epochs of training. Using the same initial LR for the proposed and baseline methods is useful, however it is insufficient to demonstrate that the proposed method could still perform well under different initial conditions. I have additional concerns about the significance of the proposed method over the baselines which I describe below.\n\nRegarding comparing to the sharpness of the baseline LR schedules: \u201cWith fewer explore epochs, a large learning rate might still get lucky occasionally in finding a wide minima but invariably finds only a narrower minima due to their higher density.\u201c it would help to show curvature metrics at frequent intervals during training to confirm this hypothesis, and to also show these for the other learning rate schedules compared to, so that you can demonstrate that the proposed schedule achieves something the baselines cannot. The sharpness values in Figure 2 are interesting, but I am unable to determine how impressive they are given that they are not compared to sharpness values for any other schedules, so I don\u2019t know what the baseline numbers should be.\n\nFinally, it is unclear that the proposed method is novel enough to warrant a standalone paper, without more rigorous theoretical explanations to support the claimed reasons behind its performance.\n\nPros:\n-It is useful to note that definitions of curvature can be problematic, which the authors do discuss (citing https://arxiv.org/abs/1703.04933)\n-The breadth of experiments is genuinely impressive, but unfortunately would be more impressive if the breadth was smaller and more careful tuning was done for the proposed method and baselines\n\nConcerns:\n-In Appendix C when describing your curvature metric, you say \u201cThe maximization problem is solved by applying 1000 iterations of projected gradient ascent\u201d. How was 1000 chosen? Did the sharpness metric stop changing if more steps were used?\n-What are the stddevs of the results in Tables 6, 7, 18, 19, 20, 21, 26? The proposed results seem very close to the (untuned) baselines, and so it would be useful to understand how statistically significant they are.\n-Toy problems can be extremely useful to empirically demonstrate this wide vs sharp minima selection phenomena would be useful, such as in Wu et al. 2018 (https://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective), or the noisy quadratic model in https://arxiv.org/abs/1907.04164\n-The curves in Figure 7 seem extremely similar, it would help to plot the loss on a log scale\n-In Section 4.1, \u201cIn all cases we use the model checkpoint with least loss on the validation set for computing BLEU scores on the test set.\u201d, is early stopping used in all experiments? If not, why?\n-\u201dThus, in the presence of several flatter minimas, GD with lower learning rates does not find them, leading to the conjecture that density of sharper minima is perhaps larger than density of wider minima.\u201d it is unclear to me how their previous results support this hypothesis; couldn\u2019t one retune the learning rate of SGD to find sharper/flatter minima, independently of how many sharp/flat minima exist?\n\nWriting:\n-The experiment details in the intro could be moved to later in the paper (it seems to be repeated in section 2)\n-Overall the paper length seems like it could be drastically reduced by removing repeated statements\n-Figures 6, 7, 8, 9 would be much clearer to read if it was a single plot per row, possibly on a log scale on the vertical axis when applicable\n-For consistency, it would be useful to have \u201cBaseline (short budget)\u201d also be reported in Table 5\n\nPrior work:\nThere are many previous works on explaining the benefits of large learning rates, the most relevant being https://arxiv.org/abs/1907.04595 which seems to make the same case as this paper, but is not cited. Additionally, https://arxiv.org/abs/2003.02218 has more theoretically explanations for this, using the Neural Tangent Kernel literature, and the authors could likely derive similar explanations. In fact, they use a similar schedule as the proposed method, but do not give it a name: \u201cThe network is trained with different initial learning rates, followed by a decay at a fixed physical time t \u00b7 \u03b7 to the same final learning rate. This schedule is introduced in order to ensure that all experiments have the same level of SGD noise toward the end of training.\u201d Finally, there are other works that describe how low curvature directions of the loss landscape will be learned first, benefiting from a higher LR, followed by high curvature/high noise directions, which benefits from a smaller LR, described in https://arxiv.org/abs/1907.04164. I believe that a more formal explanation and analysis of the claims on solution curvature density should be provided.\n\nAdditional feedback, comments, suggestions for improvement and questions for the authors:\nI believe that fairer experimental setup would be similar to the following:\n-pick several competitive LR schedules for each problem (not just the \u201cstandard\u201d ones)\n-identify a similar number of hyperparamters for each*, such as number of warmup steps, decay values, decay curve shapes, etc.\n-retune each schedule and the proposed method for the same number of trials, using similarly sized search spaces for each (ideally one would also retune the initial/final learning rates, momentum, and other hyperparameters for each, but this may be too expensive)\n-select the best performing hyperparameter setting for each schedule, and rerun it over multiple seeds to check for stability\n\n*it can be problematic to make comparisons across methods with different numbers of hyperparameters even with the same tuning budget, because it is impossible to construct the same volume hyperparameter spaces with different numbers of hyperparameters, see https://arxiv.org/abs/2007.01547 for a more thorough treatment", "title": "Unclear novelty and problematic baseline comparisons", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "XL_eVUnzZ--": {"type": "review", "replyto": "lXoWPoi_40", "review": "Summary:\nThis paper did an empirical study on the learning rate (LR) schedule for deep neural networks (DNNs) training. The authors argue that the density of wide minima is lower than sharp minima and then show that this makes keeping high LR necessary. Finally, they propose a new LR schedule that maintains high LR enough long. \n\nPros:\n-\tThe problem this paper studies is import for DNNs training. The proposed LR schedule is simple and has the potential to be used widely.\n-\tThe authors conduct extensive empirical tests to support their claim and the experimental design is reasonable.\n\nCons:\n-\tI\u2019m not fully convinced by the hypothesis that wide minima have lower density. The empirical results can be explained by other hypotheses as well. For example, it is also possible that wide minima are farther away from the initialization. I think the authors need to either provide theoretical analysis or come up with new experiments to further verify this hypothesis.\n-\tThe proposed LR schedule does not seem necessary. One could easily achieve the same purpose by existing LR schedules, e.g. use a step decay LR schedule. \n-\tThe novelty is low. The main novelty of the paper is the above hypothesis, but it is not supported enough. The proposed LR schedule is a slightly modified version of the existing LR schedule.  Thus the contribution of this paper seems incremental. \n\n", "title": "Review", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "yV0Kxzm0rCp": {"type": "review", "replyto": "lXoWPoi_40", "review": "Learning rate schedule plays an important role in DL, which has a large influence over the final performance. Though there have been lots of schedules, achieving SOTA performance still requires careful hand-tuned schedule that may be case by case. Compared with previous learning rate schedules, authors first conjectured that the number of wide minima is significantly lower than the number of sharp minima, and then  proposed to use a large learning rate at the initialization phase for sufficient exploration to achieve a wide minima, which may achieve better generalization performance. Extensive experiments validate the proposed learning rate schedule. \n\nThe observation of this paper looks interesting, and authors have conducted lots of experiments to validate the effects of proposed learning rate schedules. However, the novelty of this paper seems limited. First, authors conjecture that the number of wide minima is significantly lower than the number of sharp minima, but it lacks a thorough investigation of this conjecture, either from related empirical study or theoretical understanding. Second, for the proposed learning rate schedule, it seems not very clear how to set the duration of exploration epochs appropriately across different tasks, as it is still a hand-tuned hyper-parameter. For fixed 50% explore, there is not much difference in terms of the performance compared with previous schedule such as Cosine Decay or linear decay in Table 6.    \n\nOverall, I tend to a weak reject and it would much better if authors could go deeper behind the observation/conjecture.", "title": "Interesting Observations But Limited Novelty", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "fyxp7Jdn6Ww": {"type": "review", "replyto": "lXoWPoi_40", "review": "This work studies the problem of how to define learning rate schedules when training deep models so that the models better generalize.  To this end, the paper proposes and evaluates a learning rate schedule that consists of two stages (knee schedule).  A first stage of exploration adoptes a high learning rate. This initial stage is followed by a second stage where the learning rate decreases in a linear way. Extensive experimental results, both in text and image data, show that the proposed scheme allows one to train faster or to obtain better results with a fixed computational budget. The proposed learning schedule leads to SOTA results on IWSLT\u201914 (DE-EN) and WMT\u201914 (DE-EN) datasets.\n\nThe work relates the good performance of the proposed knee schedule in the hypothesis that wide minima have a lower density (are less common), therefore, a large learning rate  is required initially (and for some time) to avoid shallow minima. The second refinement stage with the learning rate declining linearly allows one to delve into the minimum found in the exploration stage. Recent works indicate that in fact wide minima are the ones that lead the models to generalize better and this is in agreement with the experimental results of the article.\n\nThe main contribution of the article is an exhaustive experimental evaluation in different applications where they analyze different schedules and show how the proposed schedule leads to superior performance. The paper raises a working hypothesis compatible with the success of the LR schedule and in that sense generates an interesting line to continue research. \n\nSome questions:\n\n1. From reading the article it is not clear to me how it is justified to keep the learning rate high even when the loss stagnates. I understand this is based on conducting experiments and then measuring the power of generalization. But it is interesting that from the training point of view it would seem that after training stagnates the network is not learning but pivoting from one side to the other. What do you think can be a good hypothesis of what is happening during training at this stage? I would like if possible that this point is better discussed. And it would also be useful if the work better discussed why the working hypothesis is the most reasonable explanation.\n\n2. Table 1 shows that reducing the learning rate after the exploration stage helps to better minimize the loss. However, this does not translate into a network that generalizes better. Is it reasonable to hypothesize that during this second period the network overfitted to the behavior around this minimum? Does this phenomenon occur in other experiments? If so, why is the second refinement stage needed?\n\n3. Warmup. Some optimizers use a warm up step where the learning rate starts to rise smoothly. It would be interesting to better discuss how this stage is linked to the exploration stage. How long does the warmup stage need to be? If warmup + exploration + decay is put together, at the end it is a curve with a certain resemblance to a cosine. \n\nAdditionally, if the information is available it would be useful to have the standard deviations of the average values \u200b\u200bcalculated in Table 6.\n\n\n", "title": "New learning rate schedule for training deep models leading to better generalization.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}