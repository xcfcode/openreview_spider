{"paper": {"title": "Smart Ternary Quantization", "authors": ["Gregoire Morin", "Ryan Razani", "Vahid Partovi Nia", "Eyyub Sari"], "authorids": ["gregoire.morin@huawei.com", "ryan.razani@huawei.com", "vahid.partovinia@huawei.com", "eyyub.sari@huawei.com"], "summary": "", "abstract": "Neural network models are resource hungry. Low bit quantization such as binary and ternary quantization is a common approach to alleviate this resource requirements. Ternary quantization provides a more flexible model and often beats binary quantization in terms of accuracy, but doubles memory and increases computation cost. Mixed quantization depth models, on another hand, allows a trade-off between accuracy and memory footprint. In such models, quantization depth is often chosen manually (which is a tiring task), or is tuned using a separate optimization routine (which requires training a quantized network multiple times). Here, we propose Smart Ternary Quantization (STQ) in which we modify the quantization depth directly through an adaptive regularization function, so that we train a model only once. This method jumps between binary and ternary quantization while training. We show its application  on image classification.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper studies mixed-precision quantization in deep networks where each layer can be either binarized or ternarized. The proposed regularization method is simple and straightforward. However, many details and equations are not stated clearly. Experiments are performed on small-scale image classification data sets. It will also be more convincing to try larger networks or data sets. More importantly, many recent methods that can train mixed-precision networks are not cited nor compared. Figures 3 and 4 are difficult to interpret, and sensitivity on the new hyper-parameters should be studied. The use of \"best validation accuracy\" as performance metric may not be fair. Finally, writing can be improved. Overall, the proposed idea might have merit, but does not seem to have been developed enough."}, "review": {"S6PlF27iLaS": {"type": "review", "replyto": "SyxhaxBKPS", "review": "This paper studies mixed-precision quantization in deep networks where each layer can be either binarized or ternarized. The authors propose  an adaptive regularization function that can be pushed to either 2-bit or 3-bit through different parameterization, in order to automatically determine the precision of each layer. Experiments are performed on small-scale image classification data sets MNIST and CIFAR-10.\n\nThe proposed regularization method is simple and straightforward. However, many details are not stated clearly enough for reproduction. E.g, since the proposed regularization already promotes binary or ternary weights, whey is there still a thresholding operation at the end of Section 3? Is it because the proposed regularization can not provide strict binary or ternary weights? Does the method require one more hard binarization/ternarization step after \\beta is learned. Indeed, tan(x) is not well-defined when x=pi/2, and the derivative tan'(x)= 1+tan^2(x) can be large when x is near pi/2, and does gradient descent work well in this case?\n\nThe experiments are only performed on small-scale data sets. Thus it is hard to tell if the proposed method also works for larger networks or data sets? Moreover, it is not fair to use \"best validation accuracy\" for comparison with other methods since the validation set is seen during training and it is not clear if the hyper-parameters of the proposed methods are tuned for best performance on the seen validation set. It would be more fair to compare the test accuracy like in the BinaryConnect (BC) paper. Yet another concern is that many recent methods that can train mixed-precision networks are not compared. For instance, the HAQ method [1] searches for precision for each layer using the reinforcement learning method, how does the proposed method perform when compared with it?\n\n[1]. Wang, Kuan, et al. \"HAQ: Hardware-Aware Automated Quantization with Mixed Precision.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019.\n", "title": "Official Blind Review #4", "rating": "3: Weak Reject", "confidence": 3}, "ryxHK6Hhir": {"type": "rebuttal", "replyto": "r1eEnyfrYS", "comment": "Thank you for your comments. We are running experiments for larger datasets and more architectures. \n\nWe will provide run-time performance disucssion in the conclusion section. \n\nWe will add sensitivity study about these parameters in the next version or on our github codes. \n\nMinor comments will be corrected in the next version of the paper.", "title": "Answer to Official Blind Review #3"}, "SygAmaS3sH": {"type": "rebuttal", "replyto": "rJgg6GphYH", "comment": "1)  We believe that the reviewer missed the main point of the paper here. Most of the works that have been published on mixed precision training assumed that the quantization depth is known before training. Other works that optimize quantization depth use a separate optimizer. The novelty of our approach lies in the fact that the neural network learns quantization depth for each layer only by a modifed  back-propagation. We modify back-propagation only by adding a regularization function, and this allows to train weights, and to estimate quantization depth simultaneously.\n\n2)  We will be looking at parameter definition and interpretation again. We only introduced two new parameters gamma, and beta in which we provided interpretation.  Other equations are only the definition of regularization function and they are discussed in detail in reference [16].\n\n3) This is a high-level comment.  We did our best to provide enough details to reproduce the work. We can improve the descriptions if the missing points are pinpointed. We provide a github code source after acceptance of the paper.\n\n4) We agree. We must avoid using the word 'the best' in scientific articles as 'the best' may change through time. We will correct this in the next version.\n\n5) We appreciate this detailed comment. We will add some studies about varying lambda and gamma parameters in the next version, or on our github codes.\n\n6) The point of these figures is to see that the weights are indeed pushed on binary or ternary values depending on the shape parameter beta of the regularization function. More clear explanations will be added in the next version of the paper, see reference [16].\n\nThanks for the generic comments. They will be considered for the revised version.", "title": "Answer to Official Blind Review #2"}, "B1e4AFrhoH": {"type": "rebuttal", "replyto": "BJeJHadbcS", "comment": "1. We agree that experiments on networks and datasets are minimal. The main goal of this paper was to demonstrate that it is possible to train a neural network that jump  between 1-bit or 2-bits while only adding a regularization function to the loss. In the litterature, mostly tuning quantization depth is performed by an independent optimization algorithm like Bayesian Optimization. This is the first attempt to modify back-propagation using only a regularizer that it tunes quantization depth mutually while training. \n\n2. Gamma is a hyperparameter which is fixed before training and controls how aggressive the quantization is towards binary weights. Beta is a trainable parameter which modify the shape of the regularization function and enforce weights to be either binary or ternary. The trained beta values, quantify the relative proportion of binary weights to ternary weights. We did not cover beta in the experimental set up because its value change during training.\n\n3.  The LR was not modified for the described method. LR was changed between the two experiments (LR = 0.01 for MNIST, LR = 0.001 for CIFAR10) to achieve competitive accuracy. ", "title": "Answer to Official Blind Review #1 "}, "r1eEnyfrYS": {"type": "review", "replyto": "SyxhaxBKPS", "review": "The paper discusses a generalization to low bit quantization and combines the approaches of binary and ternary quantization methods. Past methods such as Binary Connect and Binary Weights Network have shown that you can train a network efficiently with 1-bit quantization, and methods such as Ternary Weights Network demonstrate 2-bit quantization with weights taking one of {-1, 0, 1} * mu, with mu being a scale computed per weight tensor. The authors generalize these two methods so that the choice of binary vs ternary weights can be made per layer automatically during training. The primary contribution to make that work is by incorporating a generic regularizer with addition hyper-parameters to trade-off between the binary weight regularizer and ternary weight regularizer. In addition to that, the regularization also includes a prior to make the layers prefer binary weights by default. This is done by adding a cost that penalizes the choice of ternary weights for each layer.\n\nOverall, the paper is well written and explained, with supporting experiments to show on MNIST and on CIFAR10 that this method performs quite competitively compared to an all-binary or all-ternary weights network. Low-bit quantization is an important research area and this paper makes a strong contribution by studying mixed-precision low-bit quantization. The mathematical explanations of the generalized regularizer are well justified. For instance, the regularization constant lambda is normalized for each layer by the total number of weights to evenly weight all layers.\n\nAlthough the experiments cover MNIST and CIFAR10, it's not clear how mixed-precision low-bit methods perform on models more prevalent in the real-world. Supporting the experiments with ResNet variants on ImageNet would help clarify that further.  The paper very well explains the fundamentals of 1-bit and 2-bit methods, and the contribution of this paper (generalization of these two methods) is a somewhat natural extension without significant novelty. Moreover, in addition to the accuracy, it would also be better to understand how the run-time performance of such models (on existing software and hardware implementations) compares to pure binary and ternary networks, as knowledge of that would reveal insights into systems optimizations to be made in future work in this field. Given all of this, my rating is a weak accept.\n\nPros:\n- The problem and the fundamentals (prior work) are very well laid out.\n- The regularization component that generalizes the two types of quantization is sound.\n- Experiments on CIFAR10 strongly show improved accuracy and higher compression ratio compared to ternary weights network.\n\nCons:\n- Lacking more realistic experiments on larger datasets such as ImageNet and models like ResNets.\n- The runtime performance of how STQ compares to 1-bit and 2-bit quantization variants isn't shown.\n- The regularizer introduces more hyper-parameters to tweak and it's not clear how sensitive these are to the choice of the architecture. Different values of gamma are used in the two experiments, and further analysis on how the performance varies for various values of lambda and gamma would shed further insight.\n\nMinor comments:\n- In equation (3), the term under argmin should be mu and not alpha.\n- Section 3, line above equation (9) reads \"very per layer\" instead of \"vary per layer\"", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 3}, "rJgg6GphYH": {"type": "review", "replyto": "SyxhaxBKPS", "review": "This paper presents an approach where the regularisation is used to optimise whether each layer of a DNN is binary or ternary. The paper presents an equation for performing this along with two examples of the process in use.\n\nThe paper is inconsistently written with work described at different levels in different sections and has an inconsistent feeling about it. For example the introduction seems to stop abruptly before it describes all the parts of the paper.\n\nThe paper seems to contain an idea which might have merit. However, the idea just does not seem to have been developed enough.\n\nMajor concerns:\n1) The authors claim that this is the first attempt at a training algorithm for mixed precision training. However, a simple google search throws up many papers in this area. Many of which are not mentioned in the related work.\n\n2) Equations are not discussed in enough detail, nor are the parameters defined. Or if they are defined they are done so much later in the work.\n\n3) There doesn\u2019t seem to be enough material here to reproduce the work.\n\n4) In the results you talk about \u2018the best\u2019. Given that there has been much criticism over the last two years about good academic practice the fact that you don\u2019t say at least \u2018best out of \u2026\u2019 is worrying. \n\n5) You have magic parameters lambda and gamma. You say that these effect the outcome of the work but in your examples you only state values these are set to. One would expect to see at least some analysis of how varying these values effect the outcome. But better would be to show that you have identified good values for both of these parameters. Ideally would be an evaluation of how others could identify the best values.\n\n6) Figures 3 and 4 are difficult to interpret. They need a clear explanation.\n\nSome more generic comments:\n- The abstract seems to assume a huge prior knowledge by the reader.\n\n- \u20181 bit precision\u2019 - precision seems to have no meaning in this context. Surely just \u20181 bit\u2019\n\n- The related work contains a lot of equations, but no real explanation of what they are.\n\n- \u2018we let \u03b2 very per layer\u2019 -> \u2018we let \u03b2 vary per layer\u2019\n\n- In equation 10 what do I and J represent?\n\n- \u201928 \u00d7 28 gray-scales images\u2019 -> \u201928 \u00d7 28 gray-scale images\u2019\n\n- \u2018For the training session, we pad the sides of the images with 4 pixels, then a 32 \u00d7 32 crop is sampled, and flipped horizontally at random.\u2019 - why?\n\n- \u2018As commonly practiced\u2019 - by whom?\n\n- \u2018which is costly, specially if\u2019 -> \u2018which is costly, especially if\u2019\n", "title": "Official Blind Review #2", "rating": "1: Reject", "confidence": 2}, "BJeJHadbcS": {"type": "review", "replyto": "SyxhaxBKPS", "review": "The Paper talks about the Smart Ternary Quantization method that improves the quantization over binary and ternary quantizations by specifying an adaptive quantization. The proposed regularization function is covered in detail and the results are evaluated on MNIST and CIFAR10 datasets\n\nThe authors can improve the submission by \n1. evaluating more modern networks with bigger datasets, as opposed to the ones demonstrated.\n2. describing gamma (eq 10) , it wasn't clear why that parameter was introduced (in addition to the beta) and it's significance, what was more confusing is coverage for this instead of beta in the experimental setup\n3. why the LR needed to be modified for the described method", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 2}}}