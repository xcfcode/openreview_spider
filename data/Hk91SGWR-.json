{"paper": {"title": "Investigating Human Priors for Playing Video Games", "authors": ["Rachit Dubey", "Pulkit Agrawal", "Deepak Pathak", "Thomas L. Griffiths", "Alexei A. Efros"], "authorids": ["rach0012@berkeley.edu", "pulkitag@berkeley.edu", "pathak@berkeley.edu", "tom_griffiths@berkeley.edu", "efros@eecs.berkeley.edu"], "summary": "We investigate the various kinds of prior knowledge that help human learning and find that general priors about objects play the most critical role in guiding human gameplay.", "abstract": "What makes humans so good at solving seemingly complex video games?  Unlike computers, humans bring in a great deal of prior knowledge about the world, enabling efficient decision making. This paper investigates the role of human priors for solving video games. Given a sample game, we conduct a series of ablation studies to quantify the importance of various priors. We do this by modifying the video game environment to systematically mask different types of visual information that could be used by humans as priors. We find that removal of some prior knowledge causes a drastic degradation in the speed with which human players solve the game, e.g. from 2 minutes to over 20 minutes. Furthermore, our results indicate that general priors, such as the importance of objects and visual consistency, are critical for efficient game-play.", "keywords": ["Prior knowledge", "Reinforcement learning", "Cognitive Science"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "This paper turned out to be quite difficult to call.  My take on the pros/cons is:\n\n1. The research topic, how and why humans can massively outperform DQN, is unanimously viewed as highly interesting by all participants.\n\n2. The authors present an original human subject study, aiming to reveal whether human outperformance is due to human knowledge priors.  The study is well conceived and well executed.  I consider the study to be a contribution by itself.\n\n3. The study provides prima facie evidence that human priors play a role in human performance, by changing the visual display so that the priors cannot be used.\n\n4. However, the study is not definitive, as astutely argued by AnonReviewer2.  Experiments using RL agents (with presumably no human priors) yield behavior that is similar to human behavior.  So it is possible that some factor other than human prior may account for the behavior seen in the human experiments.\n\n5. It would indeed be better, as argued by AnonReviewer2, to use some information-theoretic measure to distinguish the normal game from the modified games.\n\n6. The paper has been substantially improved and cleaned up from the original version.\n\n7. AnonReviewer1 provided some thoughtful detailed discussion of how the authors may be overstating the conclusions that one can draw from the paper.\n\nBottom line: Given the procs and cons of the paper, the committee recommends this for workshop.\n"}, "review": {"HJfzeB4Hz": {"type": "rebuttal", "replyto": "Syy7rN67M", "comment": "We have revised our paper and added new experiments to address all of your previous concerns. It would be great if you can please find some time to look at our response and inform us of any other feedback or concerns. This would go a long way in helping us improve the paper further. Thank you so much once again!", "title": "Request for feedback"}, "B1mN4AC4M": {"type": "rebuttal", "replyto": "B1LEVNTXG", "comment": "Thank you for upgrading the rating of our paper. We addressed the concerns raised in your review and reported all experiments you asked for.  It would be great and helpful if you could provide details about your current concerns. Thanks a lot for your time!", "title": "Response to updated review rating"}, "ry07SzQgG": {"type": "review", "replyto": "Hk91SGWR-", "review": "This paper investigates human priors for playing video games.\n\nConsidering a simple video game, where an agent receives a reward when she completes a game board, this paper starts by stating that:\n-\tFirstly, the humans perform better than an RL agent to complete the game board.\n-\tSecondly, with a simple modification of textures the performances of human players collapse, while those of a RL agent stay the same.\n\nIf I have no doubts about these results, I have a concern about the method. \nIn the case of human players the time needed to complete the game is plotted, and in the case of a RL agent the number of steps needed to complete the game is plotted (fig 1). Formally, we cannot conclude that one minute is lesser than 4 million of steps.\n\nThis issue could be easily fixed. Unfortunately, I have other concerns about the method and the conclusions.\n\nFor instance, masking where objects are or suppressing visual similarity between similar objects should also deteriorate the performance of a RL agent. So it cannot be concluded that the change of performances is due to human priors. In these cases, I think that the change of performances is due to the increased difficulty of the game.\n\nThe authors have to include RL agent in all their experiments to be able to dissociate what is due to human priors and what is due to the noise introduced in the game. \n\n\n", "title": "I have concerns about the method and the conclusions", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "S1sHPAWgz": {"type": "review", "replyto": "Hk91SGWR-", "review": "Overall:\nI really enjoyed reading this paper and think the question is super important. I have some reservations about the execution of the experiments as well as some of the conclusions drawn. For this reason I am currently a weak reject (weak because I believe the question is very interesting). However, I believe that many of my criticisms can be assuaged during the rebuttal period.\n\nPaper Summary:\nFor RL to play video games, it has to play many many many many times. In fact, many more times than a human where prior knowledge lets us learn quite fast in new (but related) environments. The authors study, using experiments, what aspects of human priors are the important parts. \n\nThe authors\u2019 Main Claim appears to be: \u201cWhile common wisdom might suggest that prior knowledge about game semantics such as ladders are to be climbed, jumping on spikes is dangerous or the agent must fetch the key before reaching the door are crucial to human performance, we find that instead more general and high-level priors such as the world is composed of objects, object like entities are used as subgoals for exploration, and things that look the same, act the same are more critical.\u201d\n\nOverall, I find this interesting. However, I am not completely convinced by some of the experimental demonstrations. \n\nIssue 0: The experiments seem underpowered / not that well analyzed. \nThere are only 30 participants per condition and so it\u2019s hard to tell whether the large differences in conditions are due to noise and what a stable ranking of conditions actually looks like. I would recommend that the authors triple the sample size and be more clear about reporting the outcomes in each of the conditions. \n\nIt\u2019s not clear what the error bars in figure 1 represent, are they standard deviations of the mean? Are they standard deviations of the data? Are they confidence intervals for the mean effect? \n\nDid you collect any extra data about participants? One potentially helpful example is asking how familiar participants are with platformer video games. This would give at least some proxy to study the importance of priors about \u201chow video games are generally constructed\u201d rather than priors like \u201cobjects are special\u201d.\n\nIssue 1: What do you mean by \u201cobjects\u201d?\nThe authors interpret the fact that performance falls so much between conditions b and c to mean that human priors about \u201cobjects are special\u201d are very important. However, an alternative explanation is that people explore things which look \u201cdifferent\u201d (ie. Orange when everything else is black). \n\nThe problem here comes from an unclear definition of what the authors mean by an \u201cobject\u201d so in revision I would like authors to clarify what precisely they mean by a prior about \u201cthe world is composed of objects\u201d and how this particular experiment differentiates \u201cobject\u201d from a more general prior about \u201cvideo games have clearly defined goals, there are 4 clearly defined boxes here, let me try touching them.\u201d\n\nThis is important because a clear definition will give us an idea for how to actually build this prior into AI systems.\n\nIssue 2: Are the results here really about \u201chigh level\u201d priors?\nThere are two ways to interpret the authors\u2019 main claim: the strong version would maintain that semantic priors aren\u2019t important at all.\n\nThere is no real evidence here for the strong version of the claim. A real test would be to reverse some of the expected game semantics and see if people perform just as well as in the \u201cmasked semantics\u201d condition.\n\nFor example, suppose we had exactly the same game and N different types of objects in various places of the game where N-1 of them caused death but 1 of them opened the door (but it wasn\u2019t the object that looked like a key). My hypothesis would be that performance would fall drastically as semantic priors would quickly lead people in that direction. \n\nThus, we could consider a weaker version of the claim: semantic priors are important but even in the absence of explicit semantic cues (note, this is different from having the wrong semantic cues as above) people can do a good job on the game. This is much more supported by the data, but still I think very particular to this situation. Imagine a slight twist on the game:\n\nThere is a sword (with a lock on it), a key, a slime and the door (and maybe some spikes). The player must do things in exactly this order: first the player must get the key, then they must touch the sword, then they must kill the slime, then they go to the door. Here without semantic priors I would hypothesize that human performance would fall quite far (whereas with semantics people would be able to figure it out quite well).\n\nThus, I think the authors\u2019 claim needs to be qualified quite a bit. It\u2019s also important to take into account how much work general priors about video game playing (games have goals, up jumps, there is basic physics) are doing here (the authors do this when they discuss versions of the game with different physics).", "title": "Investigating Human Priors review", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJZ52L6lf": {"type": "review", "replyto": "Hk91SGWR-", "review": "The authors present a study of priors employed by humans in playing video\ngames -- with a view to providing some direction for RL agents to be more\nhuman-like in their behaviour.\n\nThey conduct a series of experiments that systematically elides visual\ncues that humans can use in order to reason about actions and goals in a\nplatformer game that they have a high degree of control over.\n\nThe results of the experiments, conducted using AMT participants, demonstrates\nthe existence of a taxonomy of features that affect the ability to complete\ntasks in the game to varying degrees.\n\nThe paper is clearly written, and the experiments follow a clean and coherent\nnarrative. Both the premises assumed and the conclusions drawn are quite\nreasonable given the experimental paradigm and domain in which they are\nconducted.\n\nThere were a couple of concerns I did have however:\n\n1. Right at the beginning, and through the manuscript, there is something of an\n   apples-to-oranges comparison when considering how quickly humans can\n   complete the task (order of minutes) and how quickly the SOTA RL agents can\n   complete the task (number of frames).\n\n   While the general spirit of the argument is somewhat understandable despite\n   this, it would help strengthen any inference drawn from human   performance\n   to be applied to RL agents, if the comparison between the two were to be\n   made more rigorous -- say by estimating a rough bijection between human and\n   RL measures.\n\n2. And in a related note to the idea of establishing a comparison, it would be\n   further instructive if the RL agents were also run on the different game\n   manipulations to see what (if any) sense could be made out of their\n   performance.\n\n   I understand that at least one such experiment is shown in Figure 1 which\n   involves consistent semantics, but it would be quite interesting to see how\n   RL agents perform when this consistency is taken away.\n\nOther questions and comments:\n\n1. In the graphs shown in Figure 3, are the meaning of the 'State' variable is\n   not clear -- is it the number of *unique* states visited? If not, is it the\n   total number of states/frames seen? In that case, how is it different from\n   'Time'?\n\n2. The text immediately below Figure 3's caption seems to have an incorrect\n   reference (referring to Figure 2(a) instead of Figure 3(a)).\n\nGiven recent advances in RL and ML that eschew all manner of structured\nrepresentations, I believe this is a well-timed reminder that being able to\ntransfer know-how from human behaviour to artificially-intelligent ones.\n", "title": "Review - Accept", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Syy7rN67M": {"type": "rebuttal", "replyto": "S1sHPAWgz", "comment": "We thank the reviewer for the detailed and very useful feedback. We have addressed all of your concerns below.\n\nIssue 0\n\u201c..there are only 30 participants per condition and so it\u2019s hard to tell whether the large differences in conditions are due to noise and what a stable ranking of conditions actually looks like \u2026\u201d  \nA: Good point! As per your suggestion, we have increased the sample size substantially by recruiting a total of 120 subjects per condition. The results and conclusions remain unchanged. \n\n\u201c... the error bars in figure 1 represent, are they standard deviations of the mean?... \u201d\nA: Sorry for the confusion. The error bars in Figure 1 represent standard error of the mean (we have added that clarification in the revision).\n\n\u201cDid you collect any extra data about participants? One potentially helpful example is asking how familiar participants are with platformer video games \u2026\u201d\nA: Yes! For all of the games, we found only a moderate correlation (around 0.3) between familiarity with video games and average time taken to solve the game. This relatively moderate correlation indicates that familiarity with video games only results in slight improvement in performance of human players. \n\nIssue 1\n\u201cWhat do you mean by \u201cobjects\u201d?\u201d\nA: Thank you for asking this question. We have clarified the definition of objects in the revised version of the manuscript. In the video game setting, objects are simply entities that are visibly distinct from their surroundings. The hypothesis is that humans use these visually distinct entities as subgoals, which results in more efficient exploration than random search. Performance of humans in game manipulation shown in Figure 2(c), demonstrates that when players cannot distinguish entities from the background, their performance drops significantly. We believe that mechanisms to bias exploration towards salient entities would be an interesting step towards improving the efficiency of RL agents.\n \nIssue 2.\n\u201cThere are two ways to interpret the authors\u2019 main claim: the strong version would maintain that semantic priors aren\u2019t important at all..\u201d\nA: We are sorry for the confusion. Our claim is that while prior knowledge about semantics and affordances is important for human players, more general priors about objects (i.e. existence of visually salient entities that are subgoals; entities that look similar have the same semantics)  are more critical to performance. In essence, we agree with the reviewer and we do not claim that semantic priors aren't important (just that general prior about objects are more critical). We have revised the manuscript to clarify this. As per your suggestion, we have also included an additional experiment (refer to section A in Appendix) and indeed find that reversing the semantics leads to worse performance than that of simply masking the semantics. \n\n\u201c..Here without semantic priors I would hypothesize that human performance would fall quite far (whereas with semantics people would be able to figure it out quite well).\u201d\nA: We completely agree. However, at the same time, we believe that the prior of treating visually distinct entities as sub-goals for exploration will be more important than the prior about semantics alone. \n\n\u201c..It\u2019s also important to take into account how much work general priors about video game playing (games have goals, up jumps, there is basic physics) are doing here..\u201d\nA: Great point. Humans bring in various priors about general video game playing such as moving up or right in games is generally correlated with progress, games have goals etc. Quantifying the importance of such priors is an interesting direction of research and we will include this discussion in the next revision of the paper.\n", "title": "Response to review"}, "SJML7NamG": {"type": "rebuttal", "replyto": "BJZ52L6lf", "comment": "We thank the reviewer for the positive and useful feedback. Our response to your concerns below:\n\nQ: \u201cthere is something of an apples-to-oranges comparison when considering how quickly humans can complete the task (order of minutes) and how quickly the SOTA RL agents can complete the task (number of frames).\u201d\nA:  Good point! In Figure 1 of the revised manuscript, we have now reported number of actions taken by both human players and RL agents to solve the games.\n\nQ: \u201c... it would be further instructive if the RL agents were also run on the different game manipulations\u2026\u201d \nA: Thank you for this useful suggestion! We have included additional experiments that quantify the performance of  RL agent on the different game manipulations (Section C, Appendix). The RL agent\u2019s performance is unaffected in all game manipulations except for the version in which visual similarity is removed. \n\nResponse to additional comments:\nQ: \u201c .. graphs shown in Figure 3, are the meaning of the 'State' variable is not clear -- is it the number of *unique* states visited? ..\u201d\nA: In graph 3, the 'State' variable indeed refers to the unique states visited which serves as a measure of how much players explore a game manipulation. We have clarified this in the revised manuscript.\n\nWe have made revisions to the text addressing other minor corrections pointed by you. ", "title": "Response to review"}, "SkMvz4TQM": {"type": "rebuttal", "replyto": "Hk91SGWR-", "comment": "We thank the reviewers for their encouraging comments. We are glad that the reviewers found the questions addressed in the paper to be super important (R3) and the narrative to be coherent (R1). R1 says, \u201cGiven recent advances in RL ... it is a well-timed reminder that being able to transfer know-how from human behaviour to artificially-intelligent ones\u201d.  The reviewers also had a number of great suggestions that we have incorporated in the revised manuscript.  The major changes are as follows:\n\na) We have rewritten the introduction to clarify the main claims of our paper. \n\nb) We increased the sample size significantly for all the human experiments to ensure robustness.\n\nc) We evaluated the performance of RL agent on various game manipulations to shed further light as to how RL agents differ from humans in terms of prior knowledge (Section C, Appendix).\n", "title": "Main Response to Reviewers"}, "B1LEVNTXG": {"type": "rebuttal", "replyto": "ry07SzQgG", "comment": "\nThe reviewer says \u201c..Formally, we cannot conclude that one minute is lesser than 4 million of steps..\u201d\n\nA: In Figure 1 of the revised manuscript,  we have now reported the number of steps taken by both the RL agents and human players for direct comparison. Human players take three orders of magnitude fewer steps to solve the game. Further, please note that the main point of our work was not to compare absolute performance of humans against RL agents, but to show that the performance of human players changes significantly with re-rendering of the game which makes it hard for humans to use their prior knowledge, whereas the performance of RL agent is almost unchanged.\n\nThe reviewer says \u201c..So it cannot be concluded that the change of performances is due to human priors. In these cases, I think that the change of performances is due to the increased difficulty of the game.\u201d  \n\nA: We are afraid that the reviewer might have misunderstood our experimental setup and methodology.  First, note that all games are *exactly the same* in their reward and goal structure - the only difference between the different versions of the game is in the rendering of the game entities. Because there is no other difference between the original and the manipulated versions of the game, it can be inferred that drop in performance is due to the inability of humans to employ their prior knowledge and beliefs in those manipulated games. \n\nThe reviewer says, \u201c...The authors have to include RL agent in all their experiments to be able to dissociate what is due to human priors and what is due to the noise introduced in the game\u201d\n\nA: We do not agree with the reviewer\u2019s comment because the performance of the RL agents on different manipulations of the game has no effect on the conclusions of the human study. At the same time, we do believe that studying the performance of RL agents on all game manipulations is an interesting question; one that is independent of the study of priors employed by humans. We have included the performance of RL agents for various game manipulations in Section C, Appendix. The RL agent\u2019s performance is unaffected in all game manipulations except for the version in which visual similarity is removed. The results provide direct evidence that reviewer\u2019s claim that \u201cmasking where objects are ...should also deteriorate the performance of a RL agent.\u201d is simply not true.\n", "title": "Reviewer might have misunderstood our experimental setup and methodology"}}}