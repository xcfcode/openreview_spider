{"paper": {"title": "BLOCK-DIAGONAL HESSIAN-FREE OPTIMIZATION FOR TRAINING NEURAL NETWORKS", "authors": ["Huishuai Zhang", "Caiming Xiong", "James Bradbury", "Richard Socher"], "authorids": ["hzhan23@syr.edu", "cxiong@salesforce.com", "james.bradbury@salesforce.com", "richard@socher.org"], "summary": "", "abstract": "Second-order methods for neural network optimization have several advantages over methods based on first-order gradient descent, including better scaling to large mini-batch sizes and fewer updates needed for convergence. But they are rarely applied to deep learning in practice because of high computational cost and the need for model-dependent algorithmic variations. We introduce a vari- ant of the Hessian-free method that leverages a block-diagonal approximation of the generalized Gauss-Newton matrix. Our method computes the curvature approximation matrix only for pairs of parameters from the same layer or block of the neural network and performs conjugate gradient updates independently for each block. Experiments on deep autoencoders, deep convolutional networks, and multilayer LSTMs demonstrate better convergence and generalization compared to the original Hessian-free approach and the Adam method.", "keywords": ["deep learning", "second-order optimization", "hessian free"]}, "meta": {"decision": "Reject", "comment": "Pros:\n+ Clearly written paper.\n\nCons:\n- Limited empirical evaluation: paper should compare to first-order methods with well-tuned hyperparameters, since the block Hessian-free hyperparameters likely were well tuned, and plots of convergence as a function of time need to be included.\n- Somewhat limited novelty in that block-diagonal curvature approximations have been used before, though the application to Hessian-free optimization is new.\n\nThe reviewers liked the clear description of the proposed algorithm and well-structured paper, but after discussion were not prepared to accept it primarily because (1) they wanted to see algorithmic comparisons in terms of convergence vs. time in addition to the convergence vs. updates that were provided; (2) they wanted more assurance that the baseline first-order optimizers had been carefully tuned; and (3) they wanted results on larger scale tasks.\n"}, "review": {"SyJaBw1eG": {"type": "review", "replyto": "SJ19eUg0-", "review": "Summary: \nThe paper considers second-order optimization methods for training of neural networks.\nIn particular, the contribution of the paper is a Hessian-free method that works on blocks of parameters (this is a user defined splitting of the parameters in blocks, e.g., parameters of each layer is one block, or parameters in several layers could constitute a block). \nThis results into a block-diagonal approximation to the curvature matrix, in order to improve Hessian-free convergence properties: in the latter, a single step might require many CG steps, so the benefit from using second-order information is not apparent.\nThis is mainly an experimental work, where the authors show the merits of their approach on deep autoencoders, convolutional networks and LSTMs: results show favourable performance compared to the original Hessian-free approach and the Adam method.\n\nOriginality: \nThe paper is based on the works of Collobert (2004) and Le Roux et al. (2008), as well as the work of Martens: the twist is that each layer of the neural network is considered a parameter block, so that gradient interactions among weights in a single layer are more useful than those between weights in different layers. This increases the separability of the problem and reduces the complexity. \n\nImportance: \nUnderstanding the difference between first- and second-order methods for NN training is an important topic. Using second-order methods could be considered at its infancy, compared to the wide variety of first-order methods. Having new results on second-order methods with interesting results would definitely attract some attention at the conference. \n\nPresentation/Clarity: \nThe paper is well structured and well written. The authors clearly place their work w.r.t. state of the art and previous works, so that it is clear what is new and what is known.\n\nComments:\n1. It is not clear why the deficiency of first-order methods on training NNs with big batches motivates us to turn into second-order methods. Is there a reasoning for this statement? Or is it just because second-order methods are kind-of the only other alternative we have?\n\n2. Assuming we can perform a second-order method, like Newton's method, on a deep NN. Since originally Newton's method was designed to find solutions that have gradient equal to zero, and since NNs have saddle points (probably many more than local minima), even if we could perfectly perform second-order Newton motions, there is no guarantee whether we converge to a local minimum or a saddle point. However, since we perform Newton's method approximately in practice, this might help escaping saddle points. Any comment on this aspect (I'm not aware whether this is already commented in Schraudolph 2002, where the Gauss-Newton matrix was proposed instead of the Hessian)?\n", "title": "There is nothing particularly wrong with the paper - it is a nice work, that is based a lot on previous attempts, like those in Martens. Coming from a more theoretical background, I would like to see more theory. Nevetheless this does not lessens the value of the paper. For the moment, weak accept until I also read the other reviews. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkCQ4Ywgz": {"type": "review", "replyto": "SJ19eUg0-", "review": "The paper proposes a block-diagonal hessian-free method for training deep networks. \n\n- The block-diagonal approximation has been used in [1]. Although [1] is using Gauss-Newton matrix, the idea of \"block-diagonal\" approximation is similar. \n\n- Is the computational time (per iteration) of the proposed method similar to SGD/Adam? All the figures are showing the comparison in terms of number of updates, but it is not clear whether this speedup can be reflected in the training time. \n\n- Comparing block-diagonal approximation vs original HF method: \nIt is not clear to me what's the benefit using block-diagonal approximation. Is the time cost per iteration similar or faster? \nOr the main benefit is to reduce #CG iterations? (but it seems #CG iterations are fixed for both methods in the experiments). \nAlso, the paper mentioned that \"the HF method requires many hundreds of CG iterations for one update\". Is this true?\n Usually we can set a stopping condition for solving the Newton system.\n\n- It seems the benefit of block-diagonal approximation is marginal in CNN. \n\n[1] Practical Gauss-Newton Optimisation for Deep Learning. ICML 2017. ", "title": "The paper proposes a block-diagonal second order method for training deep networks. The algorithm is not  novel. Experimental results are good in training auto-encoder and LSTM (in terms of number of updates). ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJ1GzQKxz": {"type": "review", "replyto": "SJ19eUg0-", "review": "In this paper, authors discuss the use of block-diagonal hessian when computing the updates. The block-diagonal hessian makes it easier to solve the \"newton\" directions, as the CG can be run only on smaller blocks (and hence less CG iterations are needed).\n\nThe paper is nicely written and all was clear to me. In general, I agree that having larger batch-size is the way to go, for very large datasets and a pure SGD type of methods are having problems to efficiently utilize large clusters.\n\nThe only negative thing I find in the paper is the lack of more numerical results. Indeed, the paper is clearly not a theoretical paper, is proposing a new algorithm, hence there should be evidence that it works. For example, I would like to see how the choice of hyper-parameters influences the speed of the algorithm. Was \"CG\" cost included in the \"x\"-axis? i.e. if we put \"passes\" over the data as x-axis, then 1 update \\approx 30 CG + some more == 32 batch evaluation.\nSo please try to make the \"x\"-axis more fair.\n\n ", "title": "Nice paper, however, I would be happier if more experiments on larger datasets are presented", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkI3GiqXz": {"type": "rebuttal", "replyto": "BkCQ4Ywgz", "comment": "Thank the reviewer for the thoughtful feedback.\n\n------The block-diagonal approximation has been used in [1]. Although [1] is using Gauss-Newton(GN) matrix, the idea of \"block-diagonal\" approximation is similar.\n\nResponse: The work [1] and earlier works [2,3] uses block-diagonal approximation to the Gauss-Newton matrix or Fisher information matrix and requires explicit representation and inverse of each block of GN matrix or Fisher matrix. [1, 2] are only applied to feedforward networks. Further approximation and assumption are considered for working around convolutional neural networks [3]. It is not obvious how to generalize the algorithm to recurrent networks. The significant difference between ours and [1,2,3] is that we use block-diagonal approximation when evaluating the Hessian (Gauss-Newton) vector product and don\u2019t require explicit inverse of the GN matrix and can work directly with convolutional and recurrent networks.\n \n------Is the computational time (per iteration) of the proposed method similar to SGD/Adam? All the figures are showing the comparison in terms of number of updates, but it is not clear whether this speedup can be reflected in the training time.\n\nResponse: The time consumption of block-diagonal Hessian-free (BHF) and that of the full Hessian-free (HF) are comparable. The time per iteration of BHF and HF is 5-10 times of the Adam method. However, the total number of iterations of BHF and HF are much smaller than Adam, which can offset the per-iteration cost.\n\n \n------Comparing block-diagonal approximation vs original HF method: It is not clear to me what's the benefit using block-diagonal approximation. Is the time cost per iteration similar or faster? Or the main benefit is to reduce #CG iterations? (but it seems #CG iterations are fixed for both methods in the experiments). Also, the paper mentioned that \"the HF method requires many hundreds of CG iterations for one update\". Is this true? Usually we can set a stopping condition for solving the Newton system.\n\nResponse: The BHF algorithm partitions the original HF method into a bunch of sub-problems and solves each sub-problem with the same CG iterations as the full HF method and hence may get more accurate solution. In practice given a fixed budgets of CGs the BHF takes slightly more time (15%) per update than the full HF method if without paralleling the sub-problems onto different workers but achieves better accuracy. Hence the main benefit is to reduce #CGs and achieve better accuracy. We note that the performance of HF cannot be improved by simply increasing the #CGs. It is true that we can set a stopping condition (fixed number of CGs) for solving the Newton system. How to achieve good accuracy given a number of CGs for solving the Newton system is not clear. Our BHF algorithm provides a way that easily achieve good accuracy with a small number of CG runs.\n\n\n \n[1] Practical Gauss-Newton Optimisation for Deep Learning. ICML 2017.\n[2] Optimizing Neural Networks with Kronecker-factored Approximate Curvature. ICML 2015\n[3] A Kronecker-factored Approximate Fisher Matrix for Convolution Layers. ICML 2016\n", "title": "RE: The paper proposes a block-diagonal second order method for training deep networks. The algorithm is not novel. Experimental results are good in training auto-encoder and LSTM (in terms of number of updates)."}, "H11iXicmz": {"type": "rebuttal", "replyto": "SyJaBw1eG", "comment": "We would like to thank the reviewer for appreciating our contribution in this paper. \n\n------It is not clear why the deficiency of first-order methods on training NNs with big batches motivates us to turn into second-order methods. Is there a reasoning for this statement? Or is it just because second-order methods are kind-of the only other alternative we have?\n\nResponse: It have been shown that second-order methods worked well with big batch sizes and in fact small batch size will make the convergence of the second-order methods unstable and hurt their performances. On the contrary, the first-order methods on training NNs with big batches have problem on the speedup and generalization (Keshar et al. 2016; Takac et al. 2013; Dinh et al. 2017). These  deficiencies of first-order methods with large mini batch size motivate us to turn into second-order methods to handle big batch size.\n \n------Assuming we can perform a second-order method, like Newton's method, on a deep NN. Since originally Newton's method was designed to find solutions that have gradient equal to zero, and since NNs have saddle points (probably many more than local minima), even if we could perfectly perform second-order Newton motions, there is no guarantee whether we converge to a local minimum or a saddle point. However, since we perform Newton's method approximately in practice, this might help escaping saddle points. Any comment on this aspect (I'm not aware whether this is already commented in Schraudolph 2002, where the Gauss-Newton matrix was proposed instead of the Hessian)?\n \nResponse: The reviewer proposes an very interesting view of the possible advantage of the Gauss-Newton matrix and the approximate Newton over Newton\u2019s method, which was not commented in (Schraudolph 2002). As far as we know the main problem of Newton\u2019s method on trading  deep NN is that for nonlinear system, the Hessian matrix is not necessarily positive definite so Newton\u2019s method may diverge, which is consistent with the unstable practical performance of Newton\u2019s method in training deep NN. The Gauss-Newton matrix is an approximation of the local curvature with positive semidefinite property as long as the loss function with respect to  the output of the network is convex, which holds for most popular loss functions (MSE, cross entropy). Indeed, these approximations to the curvature matrix may act as noises which help to escape saddle points while the exact Newton\u2019s method may fail. This perspective  requires further exploration.\n \n", "title": "RE: There is nothing particularly wrong with the paper - it is a nice work, that is based a lot on previous attempts, like those in Martens. Coming from a more theoretical background, I would like to see more theory. Nevetheless this does not lessens the value of the paper. For the moment, weak accept until I also read the other reviews."}, "rJ-_ZscQz": {"type": "rebuttal", "replyto": "SJ1GzQKxz", "comment": "We thank the reviewer for appreciating our work.\nThe \u201cx\u201d-axis represents the number of updates and CG cost is not included.\nThe time consumption of block-diagonal Hessian-free (BHF) and that of the full Hessian-free (HF) are comparable. The time per iteration of BHF and HF is 5-10 times of the Adam method. However, the total number of iterations of BHF and HF are much smaller than Adam, which can offset the per-iteration cost.\n", "title": "RE: Nice paper, however, I would be happier if more experiments on larger datasets are presented"}}}