{"paper": {"title": "Practical Locally Private Federated Learning with Communication Efficiency", "authors": ["Yan Feng", "Tao Xiong", "Ruofan Wu", "Yuan Qi"], "authorids": ["~Yan_Feng2", "weilue.xt@antgroup.com", "~Ruofan_Wu1", "yuan.qi@antgroup.com"], "summary": "", "abstract": "Federated learning (FL) is a technique that trains machine learning models from decentralized data sources. We study FL under local differential privacy constraints, which provides strong protection against sensitive data disclosures via obfuscating the data before leaving the client. We identify two major concerns in designing practical privacy-preserving FL algorithms: communication efficiency and high-dimensional compatibility. We then develop a gradient-based learning algorithm called \\emph{sqSGD} (selective quantized stochastic gradient descent) that addresses both concerns. The proposed algorithm is based on a novel privacy-preserving quantization scheme that uses a constant number of bits per dimension per client. Then we improve the base algorithm in two ways: first, we apply a gradient subsampling strategy that offers simultaneously better training performance and smaller communication costs under a fixed privacy budget. Secondly, we utilize randomized rotation as a preprocessing step to reduce quantization error. We also initialize a discussion about the role of quantization and perturbation in FL algorithm design with privacy and communication constraints. Finally, the practicality of the proposed framework is demonstrated on benchmark datasets. Experiment results show that sqSGD successfully learns large models like LeNet and ResNet with local privacy constraints. In addition, with fixed privacy and communication level, the performance of sqSGD significantly dominates that of baseline algorithms.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper studies differentially private, communication-efficient training methods for federated learning. While the problem studied in this paper is well-motivated and interesting, the reviewers raised several concerns about the paper. Despite the authors' reconstruction protection explanation, the concern over large values of epsilon at the scale of 400 persists. There is not too much technical novelty since the main technique is given by prior work. "}, "review": {"-g5UcFoRR2w": {"type": "rebuttal", "replyto": "tC7dMDeRKV1", "comment": "We thank the reviewer for your comments. Below we address your specific points:\n\nOn choosing large epsilons: We provide a detailed introduction to the reconstruction attack and protection guarantees in the revision, and discussed the choice in the experimental section.\n\nStructures and writing: We've adjusted according to your suggestions in the revision.", "title": "Response"}, "JEheugrE6O2": {"type": "rebuttal", "replyto": "2H1XWBRVl-y", "comment": "We thank the reviewer for illuminating comments. Below we address your specific points:\n\nOn baseline comparisons: We've added the comparison with vqSGD. As there're relatively few works on distributed/federated learning under both privacy and communication constraints, it's hard to say whether our proposed baselines are state-of-art or not. So far as we've noticed, this is the only work that is able to train large neural models successfully under the FL setting with privacy and communication constraints. \n\nOn privacy model: We've added a self-contained introduction of reconstruction attacks from Bhowmick et al, 2018. The granularity of privacy is indeed a very interesting point. In our paper, the protection is at the client level, but note that this implies protection at the individual data level under certain assumptions on the adversary. We take cross-silo FL for an example, protection at the client level implies that (under the local model) a strong adversary is not able to infer the precise value of the silo-aggregated gradients. This automatically provides protection against inference attack targeting individuals inside the silo, as long as the silo is a trusted aggregator for all the individuals that belong to it. In this case, the scenario is close to a silo-level central differential privacy model. Large epsilons are also applicable because of the amplification by subsampling phenomenon in central DP. We checked out the multi-central DP paper and it is a very interesting and promising work. However, it appears to us that the multiple-aggregator setup in the paper, where each individual may send its data to multiple aggregators, is somewhat too stringent under the FL setup, for which only one aggregator is needed for each client. \n\nOn model performances: Currently, all results produced by private and communication-limited training methods have significant gaps with their non-private counterparts. It is possible to reduce this gap by adopting a more carefully designed training procedure, for example, we may shrink the gradient norm bound adaptively during training using some extra private communications, and apply more sophisticated selection procedure like the peeling procedure (Su et al, 2016), due to time limits, we will provide studies of these techniques in future versions of this paper. For the performance on FMNIST, we currently understand this as an implication of the baseline procedures high variances in high-dimension setups as ResNet110 is of a much larger dimension than LeNet5\n\nReference:\n[1] Dwork et al, Differential private false discovery rate control, 2016", "title": "Response"}, "bC6lRAmhKmJ": {"type": "rebuttal", "replyto": "g095W6xXt7", "comment": "We thank the reviewer for providing comments. It appears that you misunderstood our privacy model. We provide clarifications below:\n\nOn choosing high epsilons: we've added a detailed introduction of reconstruction protection schemes and showed that picking high epsilons provides decent protection against reconstruction attack, which requires a much weaker adversary as that of inference attacks assumed in local privacy models. \n\nOn the intuition of PrivQuant: the algorithm provides a solution to locally private mean estimation problems use controllable communication levels. Previous works have proposed very efficient estimation procedures like vqSGD, but they work poorly on high-dimensional problems, this is partly because the high communication efficiency trades off accuracy too much in high dimensions. PrivQuant allows a better balance between communication and accuracy, which is important for the success of FL training with large models.\n\nOn the privacy cost of gradient subsampling: since we use uniformly random sampling, there're no additional costs for privacy (in terms of privacy budgets). Gradient subsampling strategy avoids adding too much noise to the almost-zero entries thereby improves overall accuracy, and this holds regardless of whether non-zero gradient dimensions are accrued locally.", "title": "Response"}, "M86Xh8ycyw-": {"type": "rebuttal", "replyto": "819q7DEAbn", "comment": "We thank the reviewer for providing comments. Below we address your specific points:\n\nOn the design of sqSGD: the motivation of sqSGD is to provide communication-efficient solutions that scale properly to high-dimensional models. Existent solutions like PM or vqSGD provides sound solutions for low-dimensional setups but performs poorly for high-dimensional setups. PrivQuant is an extension that allows controllable communication levels so that we could improve accuracy with a slight increase in communication cost. However, good distributed private mean estimation algorithms not necessarily offer good performance on high-dimensional distributed learning due to the large amount of noise injected results in too many perturbations for gradients with many almost-zero entries. Gradient sampling is a technique that exploits this fact and it turns out to be beneficial for a private distributed learning algorithm to scale to large-models. The combination of these techniques is key to the success of training large models with both private and communication constraints.\n\nOn baseline choice: the paper considers privacy protection in a \"local\" sense. cpSGD is a communication-efficient distributed learning algorithm that follows the central model of differential privacy, under which a trusted aggregator is assumed for the learning process which we do not require in our paper. Note that for reconstruction attacks, algorithms with central-DP guarantee do not necessarily satisfy the requirement. Hence cpSGD is not a proper comparison to sqSGD. We provide extra comparisons against vqSGD in the revision, which allows local model and serves as a reasonable baseline.", "title": "Response"}, "pJUl0TTdQe8": {"type": "rebuttal", "replyto": "f0sNwNeqqxx", "comment": "We'd like to thank all the reviewers for providing helpful comments. We've made the following updates to our paper based on your feedback:\n\n- Reconstruction attacks and usage of high epsilons: we provide a self-contained introduction of reconstruction attack originated in Bhowmick et al, 2018 to address the misunderstandings of our experimental setup. We also add a paragraph on why epsilons are chosen as O(\\sqrt{d}). Theorem 1 is also modified to state protection guarantees against both inference attack and reconstruction attack.\n- Insufficient baselines: we enrich the baseline methods by vqSGD (Gandikota et al, 2019), a communication-optimal algorithm for distributed mean estimation. We match the communication level of vqSGD with sqSGD in all our experiments with a much higher effective privacy budget for vqSGD, and results showed that sqSGD outperforms vqSGD significantly in high-dimension setups.\n- Structure and notations: we move the related works section upwards (section 2). We modify notations in algorithm 2 with respect to the local residuals so as not to be confused with sampling ratios. Section 2.4 in the original submission was removed since the problem was not clearly answered.", "title": "Revision"}, "tC7dMDeRKV1": {"type": "review", "replyto": "f0sNwNeqqxx", "review": "This paper studies FL under local differential privacy constraints. They identify two major concerns in designing practical privacy-preserving FL algorithms: communication efficiency and high\u0002dimensional compatibility, and develop a gradient-based learning algorithm sqSGD that addresses both concerns. They improve the base algorithm in two ways: First, apply a gradient subsampling strategy that offers simultaneously better training performance and smaller communication costs. Secondly, utilize randomized rotation as a preprocessing step to reduce quantization error. \n\nThere are also some parts need to be improved. For example, putting the related work as Section 3 is not proper. For the experiments, the epsilon is too large for privacy protection as 400 and 2000,. For this level, the privacy is not well protected. For the baseline PM, which performs well when epsilon is small. But in this paper, the comparison is only about large epsilon.\n\nThey are some typos/errors here:\nAfter Eq(1), it needs a space before \u2018M\u2019\nDifferent fonts for \u201ccross-silo\u201d\nThe first sentence of the last paragraph on Page 2, capitalize the first letter.\nEq(4), it should be k^* rather than k\nIn Algorithm 1, parameter \\tau is not introduced", "title": "Review", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "2H1XWBRVl-y": {"type": "review", "replyto": "f0sNwNeqqxx", "review": "==== Overview====\n\nThis paper studies a low communication algorithm for multivariate mean estimation in the federated learning setting with differentially private communication. The algorithm uses quantization and dimension subsampling (only reporting some coordinates of the vector) to lower communication and randomized rotation (essentially applying a random orthogonal matrix) to reduce quantization error. They then apply this algorithm to ERM, using it as a subroutine in SGD. They experimentally explore the behavior of their algorithm on a number of benchmark datasets. They consider how the performance changes as they vary epsilon, the discretization parameter and the number of epochs (in SGD). \n\n==== Comments ==\n\nI thought PrivQuant was an interesting algorithm. I thought the use of randomized rounding was nice, although it was also used in Bhowmick et. al \u201918. It wasn\u2019t clear to me to what degree this algorithm was an extension of the algorithm in that paper, but I thought it was clever. The series of improvements made seem to all have individually appeared in the literature before (I wasn\u2019t familiar with the random rotation but the authors indicate that this is not new to this work), but the combination of them for solving this problem seems to be unique.\n\nThe paper does a good job of placing itself in the context of prior work. I would have liked more explanation of how vqSGD compares?\n\nThe experiments are well designed. Hyper-parameter tuning is often an issue for algorithms like this and the authors clearly state how they tune hyper-parameters heuristically. They compare their algorithm to an algorithm from the literature, which has low communication via performing dimension subsampling, at a variety of epoch and privacy levels. They always outperform the other algorithm. It wasn\u2019t clear to me if the algorithm they compare against was the current state of the art. \n\nThe discussion of Bhowmick et. Al \u201918 was bit confusing and didn\u2019t seem very self-contained. The authors use this paper as justification for the large local epsilon values used in the experiments. They also say that they will develop algorithms that protect against inference and reconstruction attacks. This is fine, but I think the findings and caveats of Bhowmick \u201918 should be discussed more explicitly if they are used to justify the statement that high local eps algorithms protect against reconstruction attacks.\n\nThis is mainly semantics but I occasionally found the granularity of the privacy confusing. PrivQuant seems to be a locally differentially private in that each data point is privatized before it is sent to a central server. However, sqSGD is not written as a purely local algorithm, instead each client holds several data points, which they aggregate into a gradient, which is then privatized. Since PrivQuant is DP, it seems like the privacy is at the client level, not the level of individual data points? Perhaps the language of multi-central DP (https://arxiv.org/pdf/2009.05401.pdf)\n would be more helpful than local DP in the cross-silo FL setting?\n\nJust a question for the authors: do you have any intuition for what\u2019s happening with FMNIST? It seems much more sensitive than MNIST or EMIST\n\n==== Presentation =\n\nIt would have been nice to see the non-private performance on the Figures to compare. The non-private seems to approach around 90% in most cases, is this around the non-private error? The authors state in the \u201cimpact of communication constraints\u201d section that their algorithm \u201cyields competitive results\u201d, competitive with what?\n\nThe statement of Theorem 1 was a bit confusing. Is condition (5) required for the privacy statement and the unbiased statement? Currently it reads as being required for the unbiasedness, and its not clear whether its required for privacy.\n\nSmall:\nLDP is typically attributed to Dwork, McSherry, Nissim, Smith \u201907 or https://arxiv.org/pdf/0803.0924.pdf.\nIt might have been more intuitive to just state the definition of M(v_1,v_2) in terms of the Hamming distance. \nIt would be nice to have some intuition for the statement about the quantization error at the start of section 2.3 since that wasn\u2019t obvious to me. \nAt one point the authors refer to \u201craw perturbed gradients\u201d, is this unquantized?", "title": "Improvement on low communication algorithm for private SGD.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "g095W6xXt7": {"type": "review", "replyto": "f0sNwNeqqxx", "review": "Federated learning is a distributed learning paradigm where models are learned from decentralized data sources. The paper proposes to train machine learning models with communication-efficient differentially private approaches.\nI have several concerns about the work including the experimental setup.\n\nExperimental setup:\nExperiments use a privacy budget of epsilon = 400 for LeNet Models and 2000 for ResNet models. Note that differential privacy algorithms ensure that the probabilities differ by at most e^{epsilon} with high probability. This means that the probabilities can be off by \u00a0e^{400} ~ 10^170, which is higher than the number of particles in the observed universe.\nThe paper is very hard to accept with the current set of experimental results.\n\nOther:\na. R_s is not defined in Section 1.2.\nb. PrivQuant seems to be a very interesting algorithm, however very little intuition is provided. It would be good to describe why this algorithm is preferred over others.\u00a0\nc. In Section 2.2, authors argue that one can get better results if the gradients are sparse. However note that for this to work, even the non-zero coordinates have to be relayed with differential privacy, which can add to the total privacy cost.\nd. Step 14 in the algorithm: the role of r is not clear and needs to be explained more.", "title": "Federated learning with local differential privacy", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "819q7DEAbn": {"type": "review", "replyto": "f0sNwNeqqxx", "review": "The paper proposed a differentially private training algorithm for federated learning. The target is to achieve communication reduction while keeping differential privacy during training. The proposed algorithm adds a few new components to SGD, including a privacy mechanism, a random rotation to reduce quantization error, a gradient coordinate selection mechanism to reduce communication/computation. Experiments with high \\epsilon local differentially privacy guarantees are conducted. The proposed algorithm outperforms a baseline algorithm.\n\nOverall the paper is well-organized and easy to follow. My main concern is that the paper seems incremental and the comparison with existing works is not sufficient. \n\n1. Out of the three components added to SGD, the random rotation scheme is from existing works and it is not the contribution of this paper. The gradient coordinate selection mechanism is just uniformly picking coordinates which quite straightforward. The main contribution seems to be the privacy mechanism which is an extension of the mechanism in Bhowmick et al., 2018 and it is not technically difficult to extend it. \n\n2. The experiments only compared with one baseline algorithm in literature. However, there are quite a few works on communication-efficient privacy-preserving distributed training. For example, cpSGD in Agarwal et al., 2018 is a closely related algorithm but it is not compared in experiments. If a comparison is not applicable, it is better to add more discussion on comparison with existing works to highlight the difference and contribution of this work.\n\n\n\n\n\n", "title": "A new algorithm for differentially private federated learning", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}