{"paper": {"title": "Learning Accurate Entropy Model with Global Reference for Image Compression", "authors": ["Yichen Qian", "Zhiyu Tan", "Xiuyu Sun", "Ming Lin", "Dongyang Li", "Zhenhong Sun", "Li Hao", "Rong Jin"], "authorids": ["~Yichen_Qian1", "zhiyu.tzy@alibaba-inc.com", "~Xiuyu_Sun1", "~Ming_Lin4", "yingtian.ldy@alibaba-inc.com", "zhenhong.szh@alibaba-inc.com", "~Li_Hao1", "~Rong_Jin1"], "summary": "In this paper, we propose a novel Reference-based Model for image compression to effectively leverage both the local and global context information, which yields an enhanced compression performance.", "abstract": "In recent deep image compression neural networks, the entropy model plays a critical role in estimating the prior distribution of deep image encodings. Existing methods combine hyperprior with local context in the entropy estimation function. This greatly limits their performance due to the absence of a global vision. In this work, we propose a novel Global Reference Model for image compression to effectively leverage both the local and the global context information, leading to an enhanced compression rate. The proposed method scans decoded latents and then finds the most relevant latent to assist the distribution estimating of the current latent. A by-product of this work is the innovation of a mean-shifting GDN module that further improves the performance. Experimental results demonstrate that the proposed model outperforms the rate-distortion performance of most of the state-of-the-art methods in the industry.", "keywords": ["Image compression", "Entropy Model", "Global Reference"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper received moderately good reviews, 3 positives (6, 6, 7) and 1 negative (5). The reviewers are generally positive about the main idea but identified several limitations; performance improvement is marginal compared to existing approaches, the proposed method incurs higher computational complexity, and the presentation is not clear enough. Some of these issues are addressed in the rebuttal, though. Overall, the merits of this work outweigh the drawbacks and I recommend accepting this paper."}, "review": {"SYCEilMwkh6": {"type": "rebuttal", "replyto": "cTbIjyrUVwJ", "comment": "Hello everyone,\n\nThank you for your detailed critique of our paper. We have worked hard to revise our paper and address all of the points you have raised. \n\nThank you - the authors.", "title": "Updated revision"}, "3HMNfq5Mye9": {"type": "review", "replyto": "cTbIjyrUVwJ", "review": "This paper propose two methods for improve deep image compression performance: (i) Global Reference Module and (ii) Mean-shifting GDN Module (GSDN). (i) Global Reference Module searches over the decoded latents to find the relevant latents to the target latent for improve accuracy of entropy estimate. Authors extended Yang et al. 2020 method to using masked patch. (ii) GSDN extends GDN to use subtractive operation.\n\nPros: \n- Proposal seems better Rate-Distortion results than Lee 2019 and Minnen 2018 (Figure 6 and 7).\n\nCons:\u00a0\n- The method for generating U (2D feature maps) is not clearly described; it is unclear how the output channel of the parameter network (768) is calculated in the form of 2D feature maps.\n- In Table 1, the output channel of the Encoder is 384, while the corresponding input channel of the decoder is 192. I couldn't understand why the number of channels are not the same.\n- In Figure 5, the meaning of \u03c3's log is unclear and seems not appear to have been mentioned in the text.\n- The proposed method uses the mix quantization approach (Minnen & Singh 2020), but the evaluation of figure 7 is compared to Minnen 2018 as Context + Hyperprior so it is not fair comparison. For example, to put the results of the Minnen 2018 approach + mix quantization on it and compare it will make the claim of the proposal effect credible.\n- According to Figure 7, GSDN appears to be effective. It is an interesting, but I thought the effects were less explained. It would be more convincing if direct data on the mean-shifting problem were presented.", "title": "Review", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "_HJFDsrk2j": {"type": "rebuttal", "replyto": "33y33CMjzl5", "comment": "\u25cf The statements of \"confidence U\".\n\nWe have rewritten the part about \"confidence U\" sections in order to improve clarity. We hope that the current revision is much clearer. Please let us know if there are any other (or new) parts which you find hard to read, we are happy to make further improvements.\n\n\u25cf The point of the operation for confidence U and similarity S. \n\nOur entropy model provides coarse-to-fine estimations for Gaussian parameters in a progressive way. The first stage (i.e. context model) generates a coarse estimation, which is refined by offset and scaling generated by the next two stages. It is not appropriate to apply all the refinements equally into the estimations. The confidence U and similarity S are used to softly apply the refinements from the reference model. It is intuitive to multiply the refinements by U and S, which has proven efficient in reference-based SR work.\n\nThe multiplication by the confidence U would influence the similarity S. The U map represents the texture score of the reference latent, which measures the complexity of the reference feature. It agrees with what we have assumed that U would compensate for the weakness of S. We have visualized a example of S and U in revision. As shown in Figure.11, the S map is tends to represent shape while the U map is tends to represent texture, e.g., the wood pile on the right of the boat. \n\nHowever, we agree that the confidence map could be applied more efficiently. We will likely do more research to explore a more efficient way to apply the confidence map and the similarity map in the future.\n\n\u25cf The point of progressive process.\n\nThank you for pointing this out. We have constructed a Context+Hyperprior+Preogressive model and provide the results in the appendix. There is minor benefit coming at the progressive design.\n\nAs for the point of \"progressively use the prior from context model, reference model and hyperprior model instead of obtaining priors respectively and then combining them to obtain estimation results\", we tried both ways when reference model is introduced without confidence U. As we mentioned in the above problem, we then introduced confidence U to compensate for the weakness of similarity S in reference-based entropy  model. Since U is generated by the context model, the progressive way is intuitive and necessary for our entropy model.\n\n\u25cf The explanation for the Figure 6.\n\nThe RD-curve visualization in Figure.6 provides a numerical perspective. The rate savings (Figure.7) visualization highlights how the rate savings varies with quality (and thus bit rate) in a more readable way than a standard RD graph. The performance gain is greater in the low bitrate than that in the high bitrate.\n\n\u25cf The comparison results with other learned methods for CLIC valid.\n\nWe agree this would be desirable, but this is limited in practice as other learned methods have no experiments for CLIC valid. As the results for other learned methods on CLIC valid is missing, we have reproduced Minnen (2018) with mix quantization for a comparison. We have added the results in appendix. Due to the short of time, only the PSNR results are provided. The MS-SSIM results would be provided in further revision.\n\n\u25cf The visual comparison results on the CLIC dataset of high resolution.\n\nIt is added in the appendix.\n\n\u25cf References for deep-network based image compression.\n\nThanks, we have added some references of learned image compression. Please let us know if there are any other works missing.", "title": "Response to AnonReviewer4"}, "_jhFaovs8HW": {"type": "rebuttal", "replyto": "zCuTR69TEHt", "comment": "\u25cf Computational complexity.\n\nOur main goal was to optimize for compression performance. To make a fair comparison with previous methods, we have taken care to match model capacities between Context+Hyperprior method (Minnen et al. 2018) and Context-Adaptive method (Lee et al. 2019). We did not to choose a number of filters that would limit the capacities of encoder, decoder and entropy model.\n\nCompared to Context+Hyperprior method (Minnen et al. 2018), the increased computation by GSDN and global reference is about 10% when processing the 512x768 Kodak images. The complexity of search module is O(N^2) about the size of latents, while the complexities of other modules are O(N). We add two tables in appendix to describe computational complexity and FLOPs of our method. \n\nWe have not yet optimized our compression method for computational complexity. Rather, we chose the number of filters high enough to rule out the Reference model. We just set the model  larger than necessary, and let the model determine the number of channels that yield the best performance. Similar to reported in previous work, we found that small channels is enough and does not harm the compression performance when training model to low bit rates. The work (https://arxiv.org/abs/1912.08771) has analyzed for computationally efficient deep leanrning based image compression. \n\n\u25cf  The limits of image size.\n\nAs we have reported computational complexity in terms of image size (Table 3 in Appendix), the most computation lies in encoder and decoder. There is GPU memory limit when processing high-resolution image. The limit mainly depends on the large feature map in the first layer of encoder or the last layer of the decoder. Similar to reported in previous work, we found that small channels is enough and does not harm the compression performance when trainning model to low bit rates. In industry application, efficient network would ease the limits of image size.", "title": "Response to AnonReviewer3"}, "kuY_XliBUQF": {"type": "rebuttal", "replyto": "3HMNfq5Mye9", "comment": "\u25cf The method for generating U.\n\nWe use the context model to predict the Gaussian parameters (i.e. \\mu_1 and \\sigma_1) of latents solely. The latents y_hat are now modeled as Gaussian with mean \\mu_1 and standard deviation \\sigma_1. The probabilities of the latents are then calculated according to \\mu_1 and \\sigma_1 as in eq.(2). As reference model is designed in spatial dimension, confidence U is obtained by averaging the probabilities across channel.\n\nWe hope that the current revision is much clearer.\n\n\u25cf The channel setting of encoder and decoder.\n\nThe channel number in Table 1 denotes the layer's output channel for short description. That means the input channel of the first deconv in decoder is 384, while the output channel of the first deconv in decoder is 192. We have fixed this unclear presentation.\n\n\u25cf The meaning of \\sigma's log.\n\nIt is the Log-Sum-Exp trick for resolving the under- or overflow issue.\n\n\u25cf The results of the Minnen 2018 + mix quantization.\n\nWe have reproduced the Minnen 2018 + mix quantization method and showed the results in appendix. The training procedures (e.g. training data, environment, hyper parameters, etc) are same with the other experiments. Although benifiting from the mix quantization, the results of the reproduced model (the Minnen 2018 + mix quantization) is comparable to the results report in their work. Note that we trained all the models on CLIC training dataset which has 1631 images. Context+Hyperprior method (Minnen et al. 2018) has no claim about the training set, while their previous work used 1 million images. Context-Adaptive method (Lee et al. 2019) used 32420 images. The gap may lie in the difference of training data.\n\n\u25cf GSDN.\n\nWe have added some supporting visualizations of GSDN. Compared to GDN, GSDN relieves the mean-shifting problem of the latents. Figure.10 shows the histograms of the latent representation by GDN-based model and GSDN-based model. Each plot corresponds to one channel of the latents over 24 Kodak images. Four channels with highest entropy are visualized. \n\nThe ides of GDSN is inspired by the zero-mean definition of Gaussian density. And in the orignal work GDN, mean\n removal is necessary for preprocessing. On the other hand, we think that GDN has the potential to subsume the effects of batch normalization. It is intuitive to apply a subtractive operation in GDN.", "title": "Response to AnonReviewer2"}, "pYsFFKBNebA": {"type": "rebuttal", "replyto": "-Zno45rCp4G", "comment": "\u25cf Empirical results on more recent datasets.\n\nThe results on the CLIC 2019 dataset is provided in the appendix.\n\n\u25cf Computational complexity & FLOPs.\n\nOur main goal was to optimize for compression performance. To make a fair comparison with previous methods, we have taken care to match model capacities between Context+Hyperprior method (Minnen et al. 2018) and Context-Adaptive method (Lee et al. 2019). We did not to choose a number of filters that would limit the capacities of encoder, decoder and entropy model.\n\nCompared to Context+Hyperprior method (Minnen et al. 2018), the increased computation by GSDN and global reference is about 10% when processing the 512x768 Kodak images. The complexity of search module is O(N^2) about the size of latents, while the complexities of other modules are O(N). We add two tables in appendix to describe computational complexity and FLOPs of our method. \n\nWe have not yet optimized our compression method for computational complexity. Rather, we chose the number of filters high enough to rule out the Reference model. We just set the model  larger than necessary, and let the model determine the number of channels that yield the best performance. Similar to reported in previous work, we found that small channels is enough and does not harm the compression performance when training model to low bit rates. The work (https://arxiv.org/abs/1912.08771) has analyzed for computationally efficient deep leanrning based image compression. ", "title": "Response to AnonReviewer1"}, "33y33CMjzl5": {"type": "review", "replyto": "cTbIjyrUVwJ", "review": "1\tThe explanation of the proposed \u201cconfidence U\u201d is significantly deficient. Further statements to the calculation process, theory and function are expected. Please provide more details.\n2\tWhat the point of the operation that the output of global reference is multiplied by similarity S and confidence U directly as shown in Figure 5?  Will the direct multiplication by the confidence U impair the function of the attention S? A more reasonable and efficient method is expected for applying the confidence map.\n3\tDifferent from conventional entropy models, this paper proposed to progressively use the prior from context model, reference model and hyperprior model instead of obtaining priors respectively and then combining them to obtain estimation results. But the paper lacks the explanation and experiments to the rationality of the design of the progressive process. The comparison results of whether progressive or not are missing.\n4\tGenerally, the performance gain is very limited as shown in the RD curves in Figure 6. In particular, why the performance gain appears to be so minor in the low bitrate range? \n5\tIn Figure 10, why the comparison results with any learned compression methods are missing? The comparisons with various methods only on Kodak dataset in Fig. 6 are not convincing. \n6\tThe visual comparison results on the CLIC dataset of high resolution are missing. And some key references for deep-network based image compression are also missing.", "title": "This paper transferred the reference-based method in Super-Resolution to help remove the global spatial redundancy in the entropy model for image compression and achieved performance improvement. Generally, the motivation is clear and reasonable. However, some details about the model, experiments, and statements seem to be missing, which makes the whole paper somewhat unconvincing.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "-Zno45rCp4G": {"type": "review", "replyto": "cTbIjyrUVwJ", "review": "The authors introduce global reference into the entropy model for deep image compression. \nThey also develop a reference algorithm to ensemble local context, global reference and hyperprior. \nThis causes the algorithm to be robust to background noise.\nAlso, the authors develop GSDN module to handle mean-shifting issue.\nThe proposed method demonstrates good quality and memory usage gain. \n\nThis paper propose to take into account the global information as well as the local information to perform better image compression. \nThe authors also demonstrate comparison to popular image compression standards and recent deep learning approaches. \nI think this work is a nice work, however I have two main concerns. \nThe dataset used for evaluation is rather outdated. Have the authors tried evaluating on recent image compression datasets, or custom data and compare with the state of the art?\nHave the authors compared computational complexity? The main reasons why industry standards are not enthusiastic about deep learning approaches to compression is due to the computational complexity, not so much memory. Have the authors compared FLOPS? Moreover, since this work is dealing with global image information, it seems the complexity would increase rapidly with image size, while standard jpeg will relatively be not as severe. Have the authors experimented computational time with UHD, QHD, or 4k?\n\nI am leaning towards accept but not by a lot.\nI would like the authors to discuss upon \n- Empirical results on more recent datasets\n- Computational complexity and in terms of image size\n- FLOPS\n- Computational complexity and time with high resolution like UHD to 4k\n\nAfter these comments, I would like to adjust the rating", "title": "This is a good work, but I have concerns on the diversity of test examples, and computational complexity. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "zCuTR69TEHt": {"type": "review", "replyto": "cTbIjyrUVwJ", "review": "**Summary of paper**\nThe paper presents a learning-based approach for image compression. To reduce the compression rate, it describes two novel extensions, one to take the global context into account and an improved version of the commonly used GDN layer. Their advantage has been shown in a thorough ablation study. Overall, the method achieves superior performance compared to standard codecs as well as other state-of-the art learning-based method on the evaluated dataset (from Kodak).\n\n**Strengths**\n\n(S1) The approach is clearly described, and the figures help to follow the paper.\n\n(S2) The proposed approach of using a reference model to consider the global context is novel for the application of image \ncompression. \n\n(S3) Suggestion to improve GDN layer to address the mean shift problem of the existing formulation.\n\n(S4) The ablation study shows the effect of the different modules.\n\n(S5) The method achieves superior results on the tested dataset (Kodak), standard in image compression, and both PSNR/SSIM have been reported.\n\n**Weaknesses**\n\n(W1) Runtime for encoding and decoding not listed. In case, that the global reference model leads to some overhead in runtime, especially for decoding time, that would be worth mentioning. Also are there any limits in terms of image size the method can handle?\n\n**Justification**\nInteresting approach with superior results.\n", "title": "Interesting paper taking into account the global context to efficiently decrease the compression rate for image compression", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}