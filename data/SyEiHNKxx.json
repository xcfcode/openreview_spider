{"paper": {"title": "A Differentiable Physics Engine for Deep Learning in Robotics", "authors": ["Jonas Degrave", "Michiel Hermans", "Joni Dambre", "Francis wyffels"], "authorids": ["Jonas.Degrave@UGent.be", "x@UGent.be", "Joni.Dambre@UGent.be", "Francis.wyffels@UGent.be"], "summary": "We wrote a framework to differentiate through physics and show that this makes training deep learned controllers for robotics remarkably fast and straightforward", "abstract": "One of the most important fields in robotics is the optimization of controllers. Currently, robots are often treated as a black box in this optimization process, which is the reason why derivative-free optimization methods such as evolutionary algorithms or reinforcement learning are omnipresent. When gradient-based methods are used, models are kept small or rely on finite difference approximations for the Jacobian. This method quickly grows expensive with increasing numbers of parameters, such as found in deep learning. We propose an implementation of a modern physics engine, which can differentiate control parameters. This engine is implemented for both CPU and GPU. Firstly, this paper shows how such an engine speeds up the optimization process, even for small problems. Furthermore, it explains why this is an alternative approach to deep Q-learning, for using deep learning in robotics. Finally, we argue that this is a big step for deep learning in robotics, as it opens up new possibilities to optimize robots, both in hardware and software.", "keywords": ["Deep learning"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "Originality, significance:\n  The paper implements a physics-based simulator directly using Theano. This avoids the type of finite differentiation that physics engines such as MuJoCo use to compute derivatives. It is quite an interesting idea, and is demonstrated using learned control for several models. \n \n Quality, clarity:\n  The original version was somewhat loosely written; the current version is improved.\n \n Pros:\n - The nice idea of implementing a physics engine in a language such as Theano, and showing that this is quite feasible.\n - May inspire further work in this direction.\n \n Cons:\n - The speed is not systematically evaluated, as compared to finite-difference-based engines. It is thought to be \"in the same ballpark\" as other more full-featured engines. It is not clear that those using simulators will care whether it uses the true derivatives or finite differences."}, "review": {"Hk0Ob3JPg": {"type": "rebuttal", "replyto": "rkp9hhZEx", "comment": "1. You are correct, this has been corrected in the paper.\n2. In this paper, we did not want to introduce a new way to control robots. Our focus in this paper was not to introduce new paradigms for policies, therefore we did not compare to those other methods. We did introduce a new policy method, as we felt that this method would be most intuitive to people working with deep neural networks. On top of that, this naive method was powerful enough to solve all problems we threw at it in this paper. It is true that other methods rely already on the finite difference approximation of the gradient, but generally in those methods the robots are kept small with a limited number of states in order to keep the calculations fast. On top of that, they usually assume the entire state of the robot is known by the controller at every time step. Our method even works with very abstract sensors, such as cameras.\n\nThe fact that you let a library do the differentiation for you, empowers you to have the exact gradient. This is not only exact, but also only scales better to a bigger number of states (e.g. soft or compliant robots, or robots handling soft tissues come to mind), and to more abstract sensors (like cameras).\n3. We plan on training with noisy parameters, in order for the neural network controller to generalize over the stochasticity of the model. This has however only been explored little in the work for this paper.\n4. We did not compare to those other methods, as this paper is focused on the differential physics engine, and the new approach to implement and use that. Our approach solves these gradients exactly. It is true that we could have compared to these different methods for estimating the gradient, but our approach requires a specific framework. Implementing those other approaches for estimations of these exact gradients in this framework would be cumbersome and not a fair comparison. We rely on the judgement of the reader to evaluate whether the benefits of our approach outweighs the benefits of the other methods. We would argue that at worst, our approach is novel and illustrates an alternative to tackling these problems of finding the gradient for complex robotic systems.\n\nIn short, Natural Policy Gradient and REINFORCE estimate the gradient of stochastic models. In this paper, we show a practical method to differentiate deterministic models exactly. We would deal with stochastic models the same way REINFORCE does, by sampling from them, and optimizing the controller through stochastic gradient descent. This has however not been evaluated in this paper.", "title": "Thank you for your comments"}, "ByU_wJwIl": {"type": "rebuttal", "replyto": "ByTyieGNg", "comment": "Thank you for your remarks. We have indeed removed informal and imprecise wording from our paper to the best of our capabilities, including the ones you have mentioned. Some details have been moved to the appendix and other paragraphs have been made more concise to fit the paper into 8 pages.\n\nHowever, as long as the engine is not feature complete, we feel it is no use to perform quantitative speed comparisons with other engines. The comparison would not be a fair one. We believe the novelty lies indeed in the following:\n\n* it is feasible to evaluate exact gradients of these robotic frameworks within a reasonable time. Exact gradients should form a favorable approach to finite differences when the number of states of the system rises drastically, for instance when manipulating cloth.\n* the recent automatic differentiation libraries are powerful enough to become an important tool in the differentiation of robot models.\n\nInstead, we have made more clear how fast our engine is on our small models.\n\nIn order to add more novelty to this approach in this paper, we have added a fourth example to this paper. In this example, the controller relies on a differentiable camera in the differentiable physics engine in order to control a system. To the best of our knowledge, differentiable cameras are a new approach to learning vision in robotics as well.", "title": "We have updated the paper"}, "H1CY46LIl": {"type": "rebuttal", "replyto": "HkOljnBEg", "comment": "Thank you for agreeing with this idea. We updated the paper, in order to clarify the speed of the engine. The dynamics step in our engine for a 100 state model is 1ms on GPU, so both engines are in the same ballpark. This comparison is however not fair, and we believe also not the point of our paper. We chose not to compare directly with other engines for the following reasons:\n\n* we believe that such a comparison would not be fair, as our engine currently is less feature complete. We think that comparing to systems which do have full collision detection is pointless for this reason. Comparing with engines which cannot differentiate is equally pointless.\n* on the other hand, our gradient is analytically exact.\n* our focus was on efficient parallelization of computation, as we think this is a more relevant requirement for deep learning. This is not the case for these other engines.\n* nevertheless, as we claimed in the paper: \"Our implementation and our model can probably be made more efficient.\" We did not (yet) work to have this approach be as fast as possible.\n\nWe believe that our paper deserves publication for the following novel ideas:\n\n* it is feasible to evaluate exact gradients of these robotic frameworks within a reasonable time. Exact gradients should form a favorable approach to finite differences when the number of states of the system rises drastically, for instance when manipulating cloth.\n* the recent automatic differentiation libraries are powerful enough to become an important tool in the differentiation of robot models.\n\nIn order to add more novelty to this approach in this paper, we have added a fourth example to this paper. In this example, the controller relies on a differentiable camera in the differentiable physics engine in order to control a system. We believe that the use of differentiable cameras is a new approach to learning vision in robotics. We hope that in your view, this pushes this paper above the acceptance threshold.", "title": "Clarified the speed of the engine"}, "SyF2HjUUx": {"type": "rebuttal", "replyto": "Bk17MKU4g", "comment": "Thank you for your kind review. The goal is indeed to open source this library. However, as it currently stands, the reviews are around the acceptance threshold. I would rather not jeopardize this publication at possibly other conferences by opening the code before it is accepted for publication. If it were not for the open review process, the code would already have been included with the paper. As it stands now, all I can give is my word that it will be made open source.\n\nI am however unfamiliar with an open review process, and the reason I see no way around this issue might by my inexperience. Perhaps I could invite you personally to inspect the repository if you mail me an anonymous github account? Perhaps you see another possibility?\n\n> I am not convinced that ICLR is the right venue: robotics conferences such as IROS and ICRA might appreciate it much more.\n\nWe have presented work at those robotics conferences before. In our experience, they usually have little - although growing - focus on machine learning results. We think that this physics engine could form a strong prior for machine learning in robotics, and that it is not yet common to use machine learning libraries this way. Or to formulate a prior for a neural network in this way. We thought this falls within the scope of `learning representations'. On top of that, we have  presented this work at the deep reinforcement learning workshop at NIPS on invitation of the organizers. For these reasons, we believe that this work does fall within the context of ICLR.\n\nOn a more personal level: I am quite environment minded, and ICLR is reachable by train this year, while both IROS and ICRA require intercontinental flights.", "title": "The goal is definitely to open source it"}, "rkp9hhZEx": {"type": "rebuttal", "replyto": "SyEiHNKxx", "comment": "Hi, it would be great if you can clarify the following.\n\n1. Section I: \"Evaluating finite difference approximations, however, requires the same number of model evaluations as the number of parameters with respect to which is differentiated.\" -- this is incorrect. If you have a differentiable policy, you only require model evaluations of the order of number of states (which are typically in 100s) and not number of parameters.\n\n2. Have you tried comparing against numerical derivatives of the Jacobian and propagating these to find derivatives with respect to policy parameters (as opposed to jamming everything into the RNN as noted in the paper)? To me, the entire pitch of the paper lies on the former being difficult. However, the robotics literature is full of success stories where essentially one plans through a model by computing gradients using finite differences (e.g. iLQG, CIO, GPS etc).\n\n3. How to handle cases when the dynamics is stochastic?\n\n4. How does this compare to other gradient based methods, but estimated differently. For example, policy gradient methods like REINFORCE, Natural Policy Gradient, or TRPO?", "title": "some questions and comments"}, "SyoZj4jQe": {"type": "rebuttal", "replyto": "B1njLA9me", "comment": "1) It will be made available. I am currently in the process of cleaning up the source. I hope to release it by January.\n\n2) This is indeed something we will try in our next paper. We are confident it would not work, although we cannot exclude the possibility that the gradient on these parameters could become intractable.", "title": "It will be open sourced"}, "S18M5VsXe": {"type": "rebuttal", "replyto": "H1KtJ-pMe", "comment": "Hi,\n\nI apologize for the late reply.\n\n1. Since the model is implemented rather than learned, it does not require data acquisition for learning the model. However, I foresee more use of this approach as a (very) strong prior on the model.\n\n2. In this paper, we did the machine learning and optimization process as vanilla as possible. Exploration is very likely a problem for some applications, but this could be resolved in obvious ways if it were a problem for a specific application. Local optima are unlikely in deep neural networks, but again, if it were a problem, it could be dealt with by appropriate deep learning techniques. I want to stress that the goal of the paper was not to provide an in depth, thorough method dealing with all corner cases, but rather to explain that we see a different way in which deep learning could be combined with robotics.\n\n3. Speed and fidelity are all lower than standard simulation platforms, as is stated in the paper. On the other hand, it is differentiable.\n\n4. This is an excellent point. We think that the task is hard, but it is indeed surprising that we saw no convergence at all. We note that in Duan et al. ICML 2016 (https://arxiv.org/abs/1604.06778), CMA-ES did not manage to solve all tasks either. Indeed, it seems that the many parameters are not the only issue at hand. We will look into why this is the case and correct our statement in the paper.\n\nWe do not know how other deep RL methods would compare in general. If we look at the results of Schulman et al. (https://arxiv.org/pdf/1506.02438v5.pdf) for the quadruped task, it seems that we require less iterations (500 instead of 1000) with less model-time (8s instead of 2000s). There are however so many differences, that we would draw no conclusions from this observation.\n\n5. This is indeed a question we hope to tackle in our next paper. We found comparing with other simulators not useful, as results would obviously differ. We rather hope to tackle this problem in a different way, as described in the paper. We hope that training our policies with parameter noise (in both hardware and policy) will make them robust against the errors caused by the simulator, and thus more readily transferable to real-world applications.\n\nGiven the questions, I would like to stress that we do not want to claim that we have a new or even a good way to use policies, or to outperform other approaches in robotics. The claim we would like to make, is that the physics engines used in robotics rely on 'nearly'-linear equations, and are therefore differentiable in closed form, and that this closed form can be evaluated quickly. We claim that current day AD libraries are powerful enough to remove the laborious task of differentiation by hand, as was common in the time of adjoint optimization. We claim that this is a new approach to deep learning in robotics, one which deserves more attention. We tried to illustrate this claim by training a vanilla policy network in a vanilla way, and showing how this conceptually simple approach already optimizes fast. Therefore, we ended the paper with a list of the possible use cases we see.", "title": "Apologies for the late reply"}, "B1njLA9me": {"type": "review", "replyto": "SyEiHNKxx", "review": "Very nice work.\n\nSome questions.\n\n1) Will the engine be made available? \n\n2) How straightforward is it to also integrate unknowns in the model? Say, we want to tune our physical model to match real world observations of the true system. E.g. a robot arm of which we have an accurate model, but still believe it could be improved from data? I suppose that this is what one wants to do ultimately. \nI would definitely love to have this and use it for my research. A great tool.\n\nHowever, the paper lacks detail. In particular, I feel that it is impossible for someone to reimplement the research-mostly because of the lack of detail. However, replicability is a crucial part of science. Other publications proposing software (e.g. the tensorflow, theano and edward papers) come along with open source code. This is not the case here and therefore the picture is quite incomplete.\n\nI am not convinced that ICLR is the right venue: robotics conferences such as IROS and ICRA might appreciate it much more. Nevertheless, this is just an encouragement to the authors to interact with those communities.", "title": "identify unknowns in the model?", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "Bk17MKU4g": {"type": "review", "replyto": "SyEiHNKxx", "review": "Very nice work.\n\nSome questions.\n\n1) Will the engine be made available? \n\n2) How straightforward is it to also integrate unknowns in the model? Say, we want to tune our physical model to match real world observations of the true system. E.g. a robot arm of which we have an accurate model, but still believe it could be improved from data? I suppose that this is what one wants to do ultimately. \nI would definitely love to have this and use it for my research. A great tool.\n\nHowever, the paper lacks detail. In particular, I feel that it is impossible for someone to reimplement the research-mostly because of the lack of detail. However, replicability is a crucial part of science. Other publications proposing software (e.g. the tensorflow, theano and edward papers) come along with open source code. This is not the case here and therefore the picture is quite incomplete.\n\nI am not convinced that ICLR is the right venue: robotics conferences such as IROS and ICRA might appreciate it much more. Nevertheless, this is just an encouragement to the authors to interact with those communities.", "title": "identify unknowns in the model?", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "H1KtJ-pMe": {"type": "review", "replyto": "SyEiHNKxx", "review": "Following the ICLR reviewing guidelines, here are a few pre-review questions:\n1. How does this method compare with model-based reinforcement learning methods, where the model is learned?\n\n2. Is exploration and local optimum a problem in your policy search approach? If so, do you do anything to deal with it (e.g. analogous to exploration strategies used in RL)?\n\n3. How does the speed and fidelity of the dynamics simulation compare to standard simulation platforms, such as Bullet, MuJoCo, etc.?\n\n4. The experiments mention that CMA-ES fails to converge for a small neural network. This seems to disagree with the benchmark results in Duan et al. ICML 2016 (https://arxiv.org/abs/1604.06778). Why is this task harder than the suite of tasks in Duan et al.? How would other deep RL methods compare on the tasks in this paper?\n\n5. As discussed in the paper, the performance of the policy learned in this simulator under other conditions is crucial for this method to be applicable for learning behavior in the real-world. Does the policy transfer, particularly to more complex/realistic simulators?This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation. The approach is novel, and is motivated by being able to learn policies for robotics.\n\nMy two key reservations with the paper are as follows:\n1. The method is motivated by learning policies for robotics. However, the proposed method is *only* useful for robotics if the learned policy can transfer the real world. Transferring policies from simulation to real-world is an open research problem, and is particularly challenging with less realistic simulators.\n2. They key novelty/benefit of this approach over other model-based approaches is that the simulator is differentiable. However, the only empirical comparison in the paper is to a model-free approach (CMA-ES). To appropriately demonstrate the approach, it should be compared to other model-based approaches, which do not require analytic derivatives of the model.\n\nFor the reader to fully understand the pros and cons of the approach, the paper should also include quantitative comparison between the speed of the proposed simulator, and that of standard simulation platforms.\n\nBecause the idea is interesting and novel, I think it lies above the acceptance threshold. However, it would be significantly improved with the aforementioned comparisons.\n\nLastly, the writing of the paper could be improved, as it is rather informal and/or imprecise in a number of places. Here are some examples:\n-- \u201cwe model the use of a neural network as a general controller for a robot\u201d - can be more concisely phrased as something like \u201cwe model the robot controller using a neural network\u201d or \u201cthe robot controller is modeled using a neural network\"\n-- \u201cIn previous research, finding a gradient\u2026\u201d - This is a run-on sentence.\n-- \u201cWe basically jam this entire equation into\u201d - This sentence is informal and imprecise.\n-- \u201cdeep learning neural network\u201d - the word \u201clearning\u201d should be omitted\n-- \u201cone big RNN, where we unfold over time\u201d - should be \u201c\u2026RNN, which we unfold over time\u201d or \u201c\u2026RNN, unfolded over time\u201d\n\nThe writing would also be improved by making it more concise and fitting the paper into 8 pages.", "title": "pre-review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ByTyieGNg": {"type": "review", "replyto": "SyEiHNKxx", "review": "Following the ICLR reviewing guidelines, here are a few pre-review questions:\n1. How does this method compare with model-based reinforcement learning methods, where the model is learned?\n\n2. Is exploration and local optimum a problem in your policy search approach? If so, do you do anything to deal with it (e.g. analogous to exploration strategies used in RL)?\n\n3. How does the speed and fidelity of the dynamics simulation compare to standard simulation platforms, such as Bullet, MuJoCo, etc.?\n\n4. The experiments mention that CMA-ES fails to converge for a small neural network. This seems to disagree with the benchmark results in Duan et al. ICML 2016 (https://arxiv.org/abs/1604.06778). Why is this task harder than the suite of tasks in Duan et al.? How would other deep RL methods compare on the tasks in this paper?\n\n5. As discussed in the paper, the performance of the policy learned in this simulator under other conditions is crucial for this method to be applicable for learning behavior in the real-world. Does the policy transfer, particularly to more complex/realistic simulators?This paper creates a physics simulator using theano, and uses it to learn a neural network policy by back propagating gradients through the simulation. The approach is novel, and is motivated by being able to learn policies for robotics.\n\nMy two key reservations with the paper are as follows:\n1. The method is motivated by learning policies for robotics. However, the proposed method is *only* useful for robotics if the learned policy can transfer the real world. Transferring policies from simulation to real-world is an open research problem, and is particularly challenging with less realistic simulators.\n2. They key novelty/benefit of this approach over other model-based approaches is that the simulator is differentiable. However, the only empirical comparison in the paper is to a model-free approach (CMA-ES). To appropriately demonstrate the approach, it should be compared to other model-based approaches, which do not require analytic derivatives of the model.\n\nFor the reader to fully understand the pros and cons of the approach, the paper should also include quantitative comparison between the speed of the proposed simulator, and that of standard simulation platforms.\n\nBecause the idea is interesting and novel, I think it lies above the acceptance threshold. However, it would be significantly improved with the aforementioned comparisons.\n\nLastly, the writing of the paper could be improved, as it is rather informal and/or imprecise in a number of places. Here are some examples:\n-- \u201cwe model the use of a neural network as a general controller for a robot\u201d - can be more concisely phrased as something like \u201cwe model the robot controller using a neural network\u201d or \u201cthe robot controller is modeled using a neural network\"\n-- \u201cIn previous research, finding a gradient\u2026\u201d - This is a run-on sentence.\n-- \u201cWe basically jam this entire equation into\u201d - This sentence is informal and imprecise.\n-- \u201cdeep learning neural network\u201d - the word \u201clearning\u201d should be omitted\n-- \u201cone big RNN, where we unfold over time\u201d - should be \u201c\u2026RNN, which we unfold over time\u201d or \u201c\u2026RNN, unfolded over time\u201d\n\nThe writing would also be improved by making it more concise and fitting the paper into 8 pages.", "title": "pre-review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}