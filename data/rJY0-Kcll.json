{"paper": {"title": "Optimization as a Model for Few-Shot Learning", "authors": ["Sachin Ravi", "Hugo Larochelle"], "authorids": ["sachinr@twitter.com", "hugo@twitter.com", "sachinr@princeton.edu"], "summary": "We propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime", "abstract": "Though deep neural networks have shown great success in the large data domain, they generally perform poorly on few-shot learning tasks, where a model has to quickly generalize after seeing very few examples from each class. The general belief is that gradient-based optimization in high capacity models requires many iterative steps over many examples to perform well. Here, we propose an LSTM-based meta-learner model to learn the exact optimization algorithm used to train another learner neural network in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner network that allows for quick convergence of training. We demonstrate that this meta-learning model is competitive with deep metric-learning techniques for few-shot learning. ", "keywords": []}, "meta": {"decision": "Accept (Oral)", "comment": "The authors propose a meta-learner to address the problem of few-shot learning. The algorithm is interesting, and results are convincing. It's a very timely paper that will receive attention in the community. All three reviewers recommend an accept, with two being particularly enthusiastic. The authors also addressed some issues raised by the more negative reviewer. The AC also agrees that the writing needs a little more work to improve clarity. Overall, this is a clear accept."}, "review": {"ryq49XyLg": {"type": "rebuttal", "replyto": "BJPokH_Vg", "comment": "We appreciate your feedback!\n\n1. \"Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough.\u201d\n\nWe have not yet performed studies to study which inputs are most useful. Though there could be redundancy, by removing some inputs, we expect to gain only in efficiency as the performance with less inputs would be similar or worse.\n\n2. \"It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this?\u201d\n\nThis would be the ideal as the meta-learner could control the structure of the learner more carefully for the task at hand. Allowing the meta-learner to also control the architecture of the learner would give the meta-learner another way to control overfitting on a few-shot task. Because optimizing those parameters means we would be operating in a discrete space, learning would be a bit complicated and would likely require reinforcement learning or approximations using continuous relaxations. This would definitely be interesting to pursue in future work.\n\n3. \"The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs:\u201d\n\nWe apologize for missing some previous work. We have added the references you mentioned and added some discussion about older work in meta-learning in the updated version of the submission.", "title": "Response to AnonReviewer3"}, "BJdgqmkLg": {"type": "rebuttal", "replyto": "r1bVaaUNx", "comment": "Thanks for your thoughts! \n\n1. \"The analogy would be closer to GRUs than LSTMs\"\n\nYes, there is a similarity to the GRU in that meta-learning LSTM uses only the cell state and does not have an additional hidden state. We have added a note about this in the new draft of the paper.\n\n2. \"The description of the data separation in meta sets is hard to follow and could be visualized\u201d\n\nWe have added a figure that gives an example with concrete data to help understand the notation. Any additional feedback is welcome.\n\n3. \"The experimental evaluation is only partly satisfying, especially the effect of the parameters of i_t and f_t would be of interest\u201d\n\"Fig 2 doesn't have much value\u201d\n\nThe plots were mainly to show two points:\n(1) Different update rules (with regard to i_t and f_t) were used for different layers, which benefits training the learner.\n(2) Different update rules were used across episodes, meaning that the meta-learner was adjusting for the data in each episode.\nUnfortunately, it is hard to derive a concrete learning strategy used by the meta-learner. That said, for completeness and transparency\u2019s sake, we felt it was important to show something like Figure 2. \n", "title": "Response to AnonReviewer2"}, "r1W3tXkIx": {"type": "rebuttal", "replyto": "SJ-NzjmNg", "comment": "We mean that a specific 64/100 classes are assigned to meta-training so that for all train/test sets in meta-training we can only randomly pick from those classes. This is true for the 16 and 20 classes picked for meta-validation and meta-testing, respectively.\nWe consider 1-shot, 5-class and 5-shot, 5-class classification, where for 5-shot, 5-class classification (for example), to create a training set for each dataset, we pick 5 random classes from the classes assigned to the meta-set and then we pick 5 random examples for each of these classes. These 5 classes are randomly assigned labels 1-5 for this episode. Thus, the softmax layer will have 5 outputs, indicating predictions for each of the 5 classes.", "title": "Response to question"}, "r1qOYX1Ie": {"type": "rebuttal", "replyto": "SyiRxi7El", "comment": "Thank you for your feedback!\n\n1. \u201cThe writing is at times quite opaque. While it describes very interesting work, I would not call the paper an enjoyable read. It took me multiple passes (as well as consulting related work) to understand the general learning problem. The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area. The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks. This would definitely make it accessible to a wider audience.\u201d\n\nThose are valid points. We added a concrete example to Section 2 so that it is clear what task we will be solving in the experimental section. Thanks for this great suggestion!\n\nWe also slightly reworded some of the text, in the hope to clarify it. Please do not hesitate to request other changes to the text that you think would improve its clarity. Meta-learning isn\u2019t an easy topic to discuss with perfect clarity, so any additional suggestion will be appreciated! Or if you wish to point out specific paragraphs or sentences that you find particularly hard to digest, we\u2019ll be happy to clarify them ASAP in revisions of the draft.\n\n2. \"Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me. Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed through meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)? Many other details like these are unclear (see question).\u201d\n\nSorry for the confusion. We mean that a specific 64/100 classes are assigned to meta-training so that for all train/test sets in meta-training we can only randomly pick from those classes. This is true for the 16 and 20 classes picked for meta-validation and meta-testing, respectively.\nWe consider 1-shot, 5-class and 5-shot, 5-class classification, where for 5-shot, 5-class classification (for example), to create a training set for each dataset, we pick 5 random classes from the classes assigned to the meta-set and then we pick 5 random examples for each of these classes. These 5 classes are randomly assigned labels 1-5 for this episode. Thus, the softmax layer will have 5 outputs, indicating predictions for each of the 5 classes.\n\n3. \"The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them.\u201d\n\nThe plots were mainly to show two points:\n(1) Different update rules were used for different layers, which benefits training the learner.\n(2) Different update rules were used across episodes, meaning that the meta-learner was adjusting for the data in each episode.\nUnfortunately, it is hard to derive a concrete learning strategy used by the meta-learner. That said, for completeness and transparency\u2019s sake, we felt it was important to show something like Figure 2. ", "title": "Response to AnonReviewer1"}, "ryvMtQyUg": {"type": "rebuttal", "replyto": "rJY0-Kcll", "comment": "We have uploaded a new revision with changes suggested by reviews:\n\n1. Added figure with concrete example of meta-learning framework\n2. Reworded some text in order to improve explanation of meta-learning\n3. Added citations suggested by reviewers", "title": "Revision in response to reviews uploaded"}, "SJ-NzjmNg": {"type": "review", "replyto": "rJY0-Kcll", "review": "Can the authors clarify the details of the N-class, few-shot learning problem formulation here. To make things concrete: the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed at all during meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)?In light of the authors' responsiveness and the updates to the manuscript -- in particular to clarify the meta-learning task -- I am updating my score to an 8.\n\n-----\n\nThis manuscript proposes to tackle few-shot learning with neural networks by leveraging meta-learning, a classic idea that has seen a renaissance in the last 12 months. The authors formulate few-shot learning as a sequential meta-learning problem: each \"example\" includes a sequence of batches of \"training\" pairs, followed by a final \"test\" batch. The inputs at each \"step\" include the outputs of a \"base learner\" (e.g., training loss and gradients), as well as the base learner's current state (parameters). The paper applies an LSTM to this meta-learning problem, using the inner memory cells in the *second* layer to directly model the updated parameters of the base learner. In doing this, they note similarities between the respective update rules of LSTM memory cells and gradient descent. Updates to the LSTM meta-learner are computed based on the base learner's prediction loss for the final \"test\" batch. The authors make several simplifying assumptions, such as sharing weights across all second layer cells (analogous to using the same learning rate for all parameters). The paper recreates the Mini-ImageNet data set proposed in Vinyals et al 2016, and shows that the meta-learner LSTM is competitive with the current state-of-the-art (Matchin Networks, Vinyals 2016) on 1- and 5-shot learning.\n\nStrengths:\n- It is intriguing -- and in hindsight, natural -- to cast the few-shot learning problem as a sequential (meta-)learning problem. While the authors did not originate the general idea of persisting learning across a series of learning problems, I think it is fair to say that they have advanced the state of the art, though I cannot confidently assert its novelty as I am not deeply familiar with recent work on meta-learning.\n- The proposed approach is competitive with and outperforms Vinyals 2016 in 1-shot and 5-shot Mini-ImageNet experiments.\n- The base learner in this setting (simple ConvNet classifier) is quite different from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals 2016. It is always exciting when state-of-the-art results can be reported using very different approaches, rather than incremental follow-up work.\n- As far as I know, the insight about the relationship between the memory cell and gradient descent updates is novel here. It is interesting regardless.\n- The paper offers several practical insights about how to design and train an LSTM meta-learner, which should make it easier for others to replicate this work and apply these ideas to new problems. These include proper initialization, weight sharing across coordinates, and the importance of normalizing/rescaling the loss, gradient, and parameter inputs. Some of the insights have been previously described (the importance of simulating test conditions during meta-training; assuming independence between meta-learner and base learner parameters when taking gradients with respect to the meta-learner parameters), but the discussion here is useful nonetheless.\n\nWeaknesses:\n- The writing is at times quite opaque. While it describes very interesting work, I would not call the paper an enjoyable read. It took me multiple passes (as well as consulting related work) to understand the general learning problem. The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area. The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks. This would definitely make it accessible to a wider audience.\n- Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me. Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed through meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)? Many other details like these are unclear (see question).\n- The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them.\n\nThis is an interesting paper with convincing results. It seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved. I will definitely raise my score if the writing is improved.", "title": "Clarify details of sequential N-class, few-shot learning task", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyiRxi7El": {"type": "review", "replyto": "rJY0-Kcll", "review": "Can the authors clarify the details of the N-class, few-shot learning problem formulation here. To make things concrete: the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed at all during meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)?In light of the authors' responsiveness and the updates to the manuscript -- in particular to clarify the meta-learning task -- I am updating my score to an 8.\n\n-----\n\nThis manuscript proposes to tackle few-shot learning with neural networks by leveraging meta-learning, a classic idea that has seen a renaissance in the last 12 months. The authors formulate few-shot learning as a sequential meta-learning problem: each \"example\" includes a sequence of batches of \"training\" pairs, followed by a final \"test\" batch. The inputs at each \"step\" include the outputs of a \"base learner\" (e.g., training loss and gradients), as well as the base learner's current state (parameters). The paper applies an LSTM to this meta-learning problem, using the inner memory cells in the *second* layer to directly model the updated parameters of the base learner. In doing this, they note similarities between the respective update rules of LSTM memory cells and gradient descent. Updates to the LSTM meta-learner are computed based on the base learner's prediction loss for the final \"test\" batch. The authors make several simplifying assumptions, such as sharing weights across all second layer cells (analogous to using the same learning rate for all parameters). The paper recreates the Mini-ImageNet data set proposed in Vinyals et al 2016, and shows that the meta-learner LSTM is competitive with the current state-of-the-art (Matchin Networks, Vinyals 2016) on 1- and 5-shot learning.\n\nStrengths:\n- It is intriguing -- and in hindsight, natural -- to cast the few-shot learning problem as a sequential (meta-)learning problem. While the authors did not originate the general idea of persisting learning across a series of learning problems, I think it is fair to say that they have advanced the state of the art, though I cannot confidently assert its novelty as I am not deeply familiar with recent work on meta-learning.\n- The proposed approach is competitive with and outperforms Vinyals 2016 in 1-shot and 5-shot Mini-ImageNet experiments.\n- The base learner in this setting (simple ConvNet classifier) is quite different from the nearest-neighbor-on-top-of-learned-embedding approach used in Vinyals 2016. It is always exciting when state-of-the-art results can be reported using very different approaches, rather than incremental follow-up work.\n- As far as I know, the insight about the relationship between the memory cell and gradient descent updates is novel here. It is interesting regardless.\n- The paper offers several practical insights about how to design and train an LSTM meta-learner, which should make it easier for others to replicate this work and apply these ideas to new problems. These include proper initialization, weight sharing across coordinates, and the importance of normalizing/rescaling the loss, gradient, and parameter inputs. Some of the insights have been previously described (the importance of simulating test conditions during meta-training; assuming independence between meta-learner and base learner parameters when taking gradients with respect to the meta-learner parameters), but the discussion here is useful nonetheless.\n\nWeaknesses:\n- The writing is at times quite opaque. While it describes very interesting work, I would not call the paper an enjoyable read. It took me multiple passes (as well as consulting related work) to understand the general learning problem. The task description in Section 2 (Page 2) is very abstract and uses notation and language that is not common outside of this sub-area. The paper could benefit from a brief concrete example (based on MNIST is fine), perhaps paired with a diagram illustrating a sequence of few-shot learning tasks. This would definitely make it accessible to a wider audience.\n- Following up on that note, the precise nature of the N-class, few-shot learning problem here is unclear to me. Specifically, the Mini-ImageNet data set has 100 labels, of which 64/16/20 are used during meta-training/validation/testing. Does this mean that only 64/100 classes are observed through meta-training? Or does it mean that only 64/100 are observed in each batch, but on average all 100 are observed during meta-training? If it's the former, how many outputs does the softmax layer of the ConvNet base learner have during meta-training? 64 (only those observed in training) or 100 (of which 36 are never observed)? Many other details like these are unclear (see question).\n- The plots in Figure 2 are pretty uninformative in and of themselves, and the discussion section offers very little insight around them.\n\nThis is an interesting paper with convincing results. It seems like a fairly clear accept, but the presentation of the ideas and work therein could be improved. I will definitely raise my score if the writing is improved.", "title": "Clarify details of sequential N-class, few-shot learning task", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJeJ8On7l": {"type": "rebuttal", "replyto": "rkW31cP7e", "comment": "You are right in that the performance is dependent on the classes and examples picked for the mini-Imagenet dataset. We will release our code and splits we used for mini-Imagenet upon acceptance to allow for comparison.", "title": "nice paper: reply"}, "rkW31cP7e": {"type": "rebuttal", "replyto": "rJY0-Kcll", "comment": "Nice paper! I have one orthogonal question for the mini-imageNet dataset:\n\n\"we create our own version of the Mini-Imagenet dataset by selecting a random 100 classes from ImageNet and picking 600 examples of each class\". \n\nI am curious how sensitive with random selected classes and examples? It would be great if the exact split or the whole dataset is shared publicly, so others can repeat experiments and make comparison on a fixed benchmark dataset.\n\n", "title": "nice paper"}, "S1P-_grQx": {"type": "rebuttal", "replyto": "SyiRPcJ7g", "comment": "We are not 100% sure what you mean by \"learning the hyperparameters with models having increasing number of parameters\" but if it means learning using the meta-learner, for example, the number of hidden units for the model, we have not tried something like that and it is a slightly less straightforward problem to solve because we'd then be optimizing over a discrete space (e.g. integer number of hidden units). We could perhaps use a continuous relaxation or reinforcement-learning to obtain gradients for the meta-learner. Please correct us if we misinterpreted your question!", "title": "Answers to AnonReview3 "}, "SyiRPcJ7g": {"type": "review", "replyto": "rJY0-Kcll", "review": "Have you assessed the importance of learning the hyperparameters using the meta-learner with models having increasing number of parameters?This work presents an LSTM based meta-learning framework to learn the optimization algorithm of a another learning algorithm (here a NN).\nThe paper is globally well written and the presentation of the main material is clear. The crux of the paper: drawing the parallel between Robbins Monroe update rule and the LSTM update rule and exploit it to satisfy the two main desiderata of few shot learning (1- quick acquisition of new knowledge, 2- slower extraction of general transferable knowledge) is intriguing. \n\nSeveral tricks re-used from (Andrychowicz et al. 2016)  such as parameter sharing and normalization, and novel design choices (specific implementation of batch normalization) are well  motivated. \nThe experiments are convincing. This is a strong paper. My only concerns/questions are the following:\n\n1. Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough.\n2. It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this?\n3. The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs:\n     - Samy Bengio PhD thesis (1989) is all about this ;-)\n     - Use of genetic programming for the search of a new learning rule for neural networks (S. Bengio, Y. Bengio, and J. Cloutier. 1994)\n     - I am convince Schmidhuber has done something, make sure you find it and update related work section.  \n\nOverall, I like the paper. I believe the discussed material is relevant to a wide audience at ICLR.  \n", "title": "sample vs model complexity", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "BJPokH_Vg": {"type": "review", "replyto": "rJY0-Kcll", "review": "Have you assessed the importance of learning the hyperparameters using the meta-learner with models having increasing number of parameters?This work presents an LSTM based meta-learning framework to learn the optimization algorithm of a another learning algorithm (here a NN).\nThe paper is globally well written and the presentation of the main material is clear. The crux of the paper: drawing the parallel between Robbins Monroe update rule and the LSTM update rule and exploit it to satisfy the two main desiderata of few shot learning (1- quick acquisition of new knowledge, 2- slower extraction of general transferable knowledge) is intriguing. \n\nSeveral tricks re-used from (Andrychowicz et al. 2016)  such as parameter sharing and normalization, and novel design choices (specific implementation of batch normalization) are well  motivated. \nThe experiments are convincing. This is a strong paper. My only concerns/questions are the following:\n\n1. Can it be redundant to use the loss, gradient and parameters as input to the meta-learner? Did you do ablative studies to make sure simpler combinations are not enough.\n2. It would be great if other architectural components of the network can be learned in a similar fashion (number of neurons, type of units, etc.). Do you have an opinion about this?\n3. The related work section (mainly focused on meta learning) is a bit shallow. Meta-learning is a rather old topic and similar approaches have been tried to solve the same problem even if they were not using LSTMs:\n     - Samy Bengio PhD thesis (1989) is all about this ;-)\n     - Use of genetic programming for the search of a new learning rule for neural networks (S. Bengio, Y. Bengio, and J. Cloutier. 1994)\n     - I am convince Schmidhuber has done something, make sure you find it and update related work section.  \n\nOverall, I like the paper. I believe the discussed material is relevant to a wide audience at ICLR.  \n", "title": "sample vs model complexity", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "H1taFu0zl": {"type": "rebuttal", "replyto": "rJY0-Kcll", "comment": "In the 3rd revision, we fixed a minor typo in description of equation 2.", "title": "Typo for equation 2 fixed"}}}