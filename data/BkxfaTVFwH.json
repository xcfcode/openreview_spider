{"paper": {"title": "GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations", "authors": ["Martin Engelcke", "Adam R. Kosiorek", "Oiwi Parker Jones", "Ingmar Posner"], "authorids": ["martin@robots.ox.ac.uk", "adamk@robots.ox.ac.uk", "oiwi@robots.ox.ac.uk", "ingmar@robots.ox.ac.uk"], "summary": "We present the first object-centric generative model of 3D visual scenes capable of both decomposing and generating scenes.", "abstract": "Generative latent-variable models are emerging as promising tools in robotics and reinforcement learning. Yet, even though tasks in these domains typically involve distinct objects, most state-of-the-art generative models do not explicitly capture the compositional nature of visual scenes. Two recent exceptions, MONet and IODINE, decompose scenes into objects in an unsupervised fashion. Their underlying generative processes, however, do not account for component interactions. Hence, neither of them allows for principled sampling of novel scenes. Here we present GENESIS, the first  object-centric generative model of 3D visual scenes capable of both decomposing and generating scenes by capturing relationships between scene components. GENESIS parameterises a spatial GMM over images which is decoded from a set of object-centric latent variables that are either inferred sequentially in an amortised fashion or sampled from an autoregressive prior. We train GENESIS on several publicly available datasets and evaluate its performance on scene generation, decomposition, and semi-supervised learning.", "keywords": ["Generative modelling", "object-centric representations", "scene generation", "variational inference"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper offers a new method for scene generation.  While there is some debate on the semantics of \u2018generative\u2019 and \u20183d\u2019, on balance the reviewers were positive and more so after rebuttal.  I concur with their view that this paper deserves to be accepted."}, "review": {"HJxO0sg6YH": {"type": "review", "replyto": "BkxfaTVFwH", "review": "UPDATE: I appreciated the authors' discussion. The authors addressed my questions satisfactorily, and I maintain my original rating of accept.\n\n----\nSummary: This papers tackles the question of building an object-centric latent variable generative model of scenes that can sample novel scenes with coherent objects and relationships. To do this, the authors define a generative model, GENESIS, that uses an autoregressive prior over mask variables. The component variables are generated conditioned on these mask variables. The visual appearance of the objects are generated conditioned on the component variables. Inference is done sequentially by inferring some later object varaibles conditioned on others. Results show that the model adopts a consistent strategy in generating and inferring the scene components, first considering the background then the foreground objects. The authors apply GENESIS to three datasets with monochromatic objects and show that GENESIS qualitatively generates coherent scenes and infers coherent scene components.\n\nDecision: Accept. This work clearly addresses a problem beyond current object-centric modeling approaches such as IODINE and MONet, which is the problem of generating novel scenes.\n\nStrengths:\n- The paper is well written and executed.\n- The evaluation is thorough\n- The problem and solution are well motivated\n\nWeaknesses: \n- While the authors demonstrates that GENESIS is able to model static scenes, it is not clear how straightforward it is to extend GENESIS to modeling dynamics for the purpose of robotics and reinforcement learning (as stated in the authors' motivation). Whereas approaches such as IODINE or RNEM (van Steenkiste et al. 2018) treat the object latent that can be propagated through time, maintatining that the same latent models the same object may not be a guarantee for autoregressive approaches such as MONet or GENESIS that re-parse the scene at every frame. Object temporal consistency is especially important when considering tasks that have occlusion, which are important problems in robotics.\n- The results in Appendix D seem to suggest that GENESIS decomposes a scene mostly via color segmentation, as IODINE and MONet do. One concern is that such models that rely mainly on color segmentation are not applicable for real world robotics with various lighting conditions and textures because segmenting based on color may not provide coherent object representations. Would the authors be able to provide an empirical analysis of how GENESIS models a real-world scene, analogous to Figure 11 in the IODINE paper?\n\nVan Steenkiste, S., Chang, M., Greff, K., & Schmidhuber, J. (2018). Relational neural expectation maximization: Unsupervised discovery of objects and their interactions. arXiv preprint arXiv:1802.10353.", "title": "Official Blind Review #3", "rating": "8: Accept", "confidence": 3}, "Bke06RuRYr": {"type": "review", "replyto": "BkxfaTVFwH", "review": "The paper proposes a generative model for images. There's a probability mask per-pixel per-component (which yields mixing probabilities), and then a set of latents per-component that yield an image. The system is tested on a set of scenes like the GQN dataset, stacks of blocks, and the multi-dsprites\u00a0dataset. The system is better than MONet, although there are a few lingering questions.\n\nSummary of positives:\n+ The factoring of the image into various components is eminently sensible and more work should build in the notion of objects\n+ The method is well explained, and I found it easy to understand the entire process.\n+ The method does appear to perform better than MONet, and qualitatively produces good results.\n\nSummary of negatives:\n- There are a few overclaims that should be fixed: namely that the system is a \"generative model of 3D visual scenes\" when in reality it is a generative model of images that has a latent space that is perhaps well-positioned to match images generated by 3D scenes composed of objects.\n- The method section sets up a few questions that are never answered in the paper: GENESIS--S gets introduced as a baseline to test a hypothesis, but this never gets quantitatively evaluated (indeed GENESIS-S beats GENESIS in 2/3 of the categories where they are compared head-to-head); similarly, the fact that GENESIS produces probabilities is repeatedly sold as an advantage over other methods, but it's never used.\n- The experiments are weak: the aforementioned questions aren't tackled,\u00a0as well as a few other issues.\n\n\nOverall, I lean ever so slightly towards rejection. The results are good looking, and the method seems well-explained. However, in my view, the manuscript seems to fall short of the mark. I am not, however, strongly opposed to the paper's acceptance. If it is accepted, I would urge authors to address the issues to make their paper have maximum impact.\n\nBig picture things:\n-I'm baffled by the repeated claim that this is a generative model of 3D scenes. In the introduction \"first object-centric generative model of 3D visual scenes capable of both decomposing and generating scenes\". I'm worried I'm just missing something profoundly obvious and critical -- since it seems obvious to me that it's not 3D.\u00a0\n\nAs far as I can tell, x here is a HxWxC image (where C is the number of channels and probably 3); the mixing probabilities are over HxW images, so it's a 2D segmentation\u00a0system. It may be applied to photos rendered with a perspective camera -- but if the requirement for the system is being applied to images that come via projection, then is faster RCNN a 3D object detector? Are normalized cuts, SLIC superpixels, or any of the other myriad unsupervised segmentation methods from vision then 3D scene segmentation approaches?\n\n-Similarly, it's not really a model for objects -- as can be seen in the multi-colored wall example in Fig 3, and Fig 8, 9. its notion of object is a region with uniform color (presumably due to the Gaussian p(x|z) ). So while it's great that it can model regions of uniform color, the system in large part seems to work (in its current form, reading the current manuscript) because the objects are all one color, and doesn't necessarily decompose the scene into objects.\n\nUnless I'm totally missing something, this sort of claim that should be corrected very quickly. I realize that my personal bias is towards more engineering and harder data and I appreciate that there should be different operating points along the spectrum of how much information is provided. However, I think that it's worth making claims and assumptions clear, rather than claiming to solve the general problem on a specific case because of peculiarities about the specific case.\n\n\nMethod:\n-The writing\u00a0of the method is quite straightforward and written well. The system is clear to me. I largely have no complaints about the method. I do, however, have concerns about the experiments that are done to validate claims in the method section.\n-GENESIS-S appears as this baseline \"To investigate whether separate latents for masks and component appearances are necessary for decomposition\", but then basically disappears. There are, as far as I can see, no qualitative results from it, and it basically appears once quantitatively, in S 4.3 / Table 1, where it appears to be as good if not better than GENESIS. In the appendix it's claimed that GENESIS trains faster than GENESIS-S, but if this is the\u00a0selling point of GENESIS over GENESIS-S, it would be nice to have at least a little quantification of this.\n-There is considerable fuss made over the fact that the system produces probabilities and this is repeatedly mentioned as an advantage over existing systems, but it is never demonstrated that these probabilities are good or useful.\u00a0This leaves the reader hanging a bit. \n\nExperiments:\n\nOverall the experiments are a little on the weak side.\n\n+The qualitative results are good, but the primary selling point it seems is that the system is better than MONET. This does seem clear, and GENESIS does appear quite a bit better.\n\n- While the results on factoring scenes do appear to be good, there's no quantitative evaluation of this (although the abstract promises it). Section 4.2 essentially says that ARI is bad, without demonstrating in a compelling way that it is. The paper says that this behavior can be seen in Appendix D, but this only shows the oversegmentation\u00a0and not that this dramatically throws off performance -- I'd expect a figure showing a reversal of expectations -- a clearly worse result getting a clearly better ARI metric.\u00a0\n\nAdditionally, computer vision has been evaluating unsupervised segmentation for close to two decades (BSDS came out in 2001). The authors should look at the metrics in e.g., \"Contour Detection andHierarchical Image Segmentation\" Arbelaez et al. PAMI 2010. I realize that coming up with metrics is hard, but I think the burden is on the authors to find the metrics to show their conclusions quantitatively.\n\n- The tower stability/height/view experiments seem incomplete. I find them quite interesting, but\u00a0I'm not entirely sure of what to draw from it. The paper obliquely comments that the Groth paper gets better results with a more complex backbone network and blames it on lack of augmentation, use of a subset of the data, and a reduced resolution. But why not just do straightforward like train a MLP\u00a0on an even further spatially downsampled image? Random here isn't a particularly compelling baseline or reference point.\u00a0\n\nSmall stuff that doesn't affect my review:\n1) It might be worth explicitly pointing out that p_\\theta(x_k,z_k^c) is just a Gaussian soon after Eqn 3. It's written after equation 5, but without making the p_\\theta explicit. This may help some readers.\n2) The tower stability is done with a one-hidden-layer (512) MLP. I'm of the personal opinion that readouts on latent variables are more informative if it's a linear transformation (since this can't do much heavy lifting).\n3) The abstract promises semi-supervised learning -- is this the tower experiment? This strikes me more as transfer learning.\n\n-----------------------------\n\nPost review update: I have read the authors' responses, and found them thoughtful and to have answered my questions. I'm happy to accept the paper and would encourage the AC to do so.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 4}, "H1ln2c42oH": {"type": "rebuttal", "replyto": "Hkgd-TrVir", "comment": "We followed the reviewer\u2019s suggestion and adopted the segmentation covering (SC) metric from \"Contour Detection and Hierarchical Image Segmentation\" (Arbelaez et al., PAMI 2010) for quantifying scene decomposition performance.\n\nThe paper has been updated in three locations:\n- Quantitative results using the SC metric were added to section 4.2. We also re-computed the ARI scores on the same 300 images from the ShapeStacks test set as used for computing the SC scores for direct comparability, noting that the authors of IODINE also use 300 images for computing ARI scores.\n- A short section was added to the appendix to explain how the SC metric is computed.\n- The captions of Figures 12 and 13 were updated to also include the SC metric for the shown examples, illustrating how the ARI achieves a better score on a qualitatively worse segmentation while the SC metric reflects the segmentation quality appropriately.\n", "title": "Update from the authors"}, "SJg2Qa7noH": {"type": "rebuttal", "replyto": "Hke-OBuciS", "comment": "Neither IODINE nor OP3 provide a formal guarantee that object indices are consistent across time steps. This is induced by the model and the inference mechanism, in particular the use of \u201cauxiliary variables\u201d to break symmetry. Unlike these methods, GENESIS infers objects sequentially and therefore imposes an ordering which follows a fairly consistent, learned strategy.\n\nThus, we would expect that for small changes in scene state between time steps t-1 and t, the ordering will be very similar (or potentially identical) to the previous time step, even when running the model only on a frame-by-frame basis. Furthermore, we could pass the statistics of the previous posterior for object k, i.e. lambda^{t-1}_k = {mu^{t-1}_k, sigma^{t-1}_k}, as an additional input only at inference step k when inferring lambda^{t}_k, e.g. by updating the approximate posterior in an additive fashion (similar to IODINE):\n\nlambda^{t}_k = lambda^{t-1}_k + Linear(LSTM([lambda^{t-1}_k, e^{t}, e_{<t}], h_{<k}))\n\nwhere e^{t} is an encoding of the current image, e_{<t} is a (possibly optional) encoding capturing image history, h_{<k} is the LSTM hidden state, and [.] denotes concatenation; should be a further strong inductive bias for the model to maintain the same object ordering. SQAIR [1] follows a similar strategy, but it uses a sample from the approximate posterior instead of its statistics as an auxiliary input.\n\nInstead of an additive update, another option would indeed be to move further away from IODINE by learning a transition model from the state at t-1 to obtain a prior which is combined with a data-dependant \u201cstate-likelihood term\u201d to arrive at a new posterior at time step t via Bayes' rule, which can be done analytically if all distributions are Gaussians.\n\nWe admit that trying to enforce a fixed ordering could lead to other issues and that it is difficult to tell whether this would work well without trying it. We very much appreciate the reviewer\u2019s thoughts and find this an interesting discussion.\n\n[1] Kosiorek et al. \"Sequential Attend, Infer, Repeat: Generative Modelling of Moving Objects.\" NeurIPS (2018).\n", "title": "Reply from the authors"}, "rJeMdaSEiB": {"type": "rebuttal", "replyto": "HJxO0sg6YH", "comment": "We thank the reviewer for the positive feedback. Below we comment on the two identified weaknesses.\n\nReviewer:\n\u201cWhile the authors demonstrates that GENESIS is able to model static scenes, it is not clear how straightforward it is to extend GENESIS to modeling dynamics for the purpose of robotics and reinforcement learning (as stated in the authors' motivation).\u201d\nAnswer:\nWe agree that temporal consistency is important in perception. IODINE models dynamic scenes by maintaining a state between time steps and updating the posterior statistics in an additive fashion. It should be possible to modify GENESIS to follow the same procedure: the posterior from step t-1 can be used as an additional input and/or prior when inferring an additive update to the posterior at step t. This would correspond to replacing IODINE\u2019s encoder and decoder with GENESIS, while using the same state transition mechanism.\n\nReviewer:\n\u201cThe results in Appendix D seem to suggest that GENESIS decomposes a scene mostly via color segmentation, as IODINE and MONet do. One concern is that such models that rely mainly on color segmentation are not applicable for real world robotics with various lighting conditions and textures because segmenting based on color may not provide coherent object representations. Would the authors be able to provide an empirical analysis of how GENESIS models a real-world scene, analogous to Figure 11 in the IODINE paper?\u201d\nAnswer:\nWe agree that current methods are not suitable yet for real world applications and we are looking forward to future work on this. Temporal information should be a useful cue to overcome this limitation. Preliminary experiments indicated that like IODINE, GENESIS also appears to segment natural images into regions of similar colour rather than meaningful objects. We are going to further investigate this. Nevertheless, we believe that by providing a fully probabilistic model of scenes that captures relationships between components and a more scalable, probabilistic attention mechanism, GENESIS overcomes key limitations of previous works and therefore makes valuable contributions towards this goal.", "title": "Reply to reviewer 3 from the authors"}, "HJgmtjSEor": {"type": "rebuttal", "replyto": "BylK-ZHRYS", "comment": "We thank the reviewer for the useful feedback. Below we attempt to address the comments and questions.\n\nReviewer:\n\u201cThe contribution of the proposed method from previous works is the introduction of an autoregressive prior on the component latents.\u201d\nAnswer:\nThe main contribution of our work is an unsupervised, object-centric generative model of rendered 3D scenes capable of both decomposing and generating scenes. One mechanism to achieve this is indeed the autoregressive prior. Additional contributions that differentiate GENESIS from prior works include a more efficient parameterisation of the stick-breaking attention process from MONet as well as a fully probabilistic inference mechanism for spatial GMMs (in contrast to the deterministic mechanism in MONet) that does not rely on the computationally expensive iterative refinement used in TAGGER, NEM, R-NEM, and IODINE. Finally, we also hope that the release of our code (implementing both GENESIS and MONET) will be useful for the community.\n\nReviewer:\n\u201cAlthough it would improve over the MONet, I'm nevertheless not sure if the framework of sequential component generation (applying both MONet and GENESIS) can be a robust approach to more complex scenes, e.g., with a larger number of objects.\nAnswer:\nWe would like to note that scenes of practical interest have a limited number of components. For example, the NYU Depth V2 dataset contains labels for a long tail of objects and the largest number of object instances per scene is around 50. LSTMs (as used in GENESIS) and other sequential models have been successfully applied to much longer sequences in large-scale applications. Crawford and Pineau present a model that performs sequential inference on Multi-MNIST, SET, and ATARI games with a very large number of components in \"Spatially Invariant Unsupervised Object Detection with Convolutional Neural Networks\" (AAAI 2019). This indicates that sequential inference of a larger number of scene components should in principle also be feasible for more complex datasets. We added this reference to the related work section.\n\nReviewer:\n\u201cAlso, the mixture-based approach seems not guarantee the object-level decomposition. For example, if in the scene there are many objects of the same shape, size, and color, I think the proposed model may not properly distinguish them, as shown in some of the wall patterns in the experiments.\u201d\nAnswer:\nWe agree that a mixture-model alone does not guarantee object-level decomposition; arguably no unsupervised formulation can provide such a guarantee as \u201cobjectness\u201d is a context-dependent property (e.g. should a car be counted as one object or are the wheels, chassis, etc. individual objects?). Importantly, though, despite these challenges the community has managed to develop models with appropriate inductive biases that discover meaningful structure in increasingly complex datasets and that learn interpretable decompositions. Moreover, GENESIS uses the same mixture formulation as MONet whose authors conduct an analysis that shows that MONet successfully distinguishes between several objects of the same colour. To further support this, we added an additional figure to the appendix (Figure 10) to provide an example from the GQN dataset where both GENESIS and MONet successfully distinguish between two objects of the same shape and colour.\n\nReviewer:\n\u201c1. It would be good to show the qualitative result of the simplified Genesis-s and its qualitative result in the FID experiment.\u201d\nAnswer:\nWe added Figures 8, 9, and 11 to the appendix to show qualitative results for scene generation and decomposition with GENESIS-S after training on GQN. We also added the FID scores on Multi-dSprites and GQN for GENESIS-S to Table 2. The figures illustrate that GENESIS-S struggles to capture the semantic structure of the scenes in contrast to GENESIS (and MONet).\n\nReviewer:\n\u201c2. Is the VAE baselines (BD-VAE and DC-VAE) trained by maximizing the ELBO? If it is the case, will the result of the FID experiment section be different if we train the VAE baseline with GECO?\u201d\nAnswer:\nThe VAE baselines are also trained with GECO to enable a fair comparison.\n\nReviewer:\n\u201c3. Some detail of the implementation is missing, e.g. how do we model the posterior q_{\\phi}(z_{k}^{c} | x, z_{1: k}^{m}).\u201d\nAnswer:\nSimilar to MONet, the posterior of z_{k}^c is encoded and decoded by a `ComponentVAE\u2019 using the same architecture as MONet but replacing ReLUs with ELUs. This is described in appendix B.1 together with the other implementation details.\n", "title": "Reply to reviewer 1 from the authors"}, "Hkgd-TrVir": {"type": "rebuttal", "replyto": "HJxK4nB4iB", "comment": "Reviewer:\n\u201cThere is considerable fuss made over the fact that the system produces probabilities and this is repeatedly mentioned as an advantage over existing systems, but it is never demonstrated that these probabilities are good or useful.\u201d\nAnswer:\nThe probabilistic formulation enables GENESIS to generate new scenes in a principled fashion by sampling from the prior. That said, we found this comment to be thought provoking and we will investigate how to further illustrate the benefits of the probabilistic formulation.\n\nReviewer:\n\u201cWhile the results on factoring scenes do appear to be good, there's no quantitative evaluation of this (although the abstract promises it). Section 4.2 essentially says that ARI is bad, without demonstrating in a compelling way that it is. The paper says that this behavior can be seen in Appendix D, but this only shows the oversegmentation and not that this dramatically throws off performance -- I'd expect a figure showing a reversal of expectations -- a clearly worse result getting a clearly better ARI metric.\u201d\nAnswer:\nThe abstract does not promise a quantitative evaluation of scene decomposition. Yet, we updated section 4.2 and added means and standard deviations for the ARI scores after training Genesis and MONet with five different random seeds on ShapeStacks. Despite its limitations, the ARI metrics still gives some indication of segmentation quality. We also added ARI scores to the captions of Figures 12 and 13, whereby Figure 13 shows a qualitatively worse segmentation achieving a better ARI score to illustrate the aforementioned limitations. \n\nReviewer:\n\u201cThe authors should look at the metrics in e.g., \"Contour Detection andHierarchical Image Segmentation\" Arbelaez et al. PAMI 2010.\u201d\nAnswer:\nWe thank the reviewer for pointing us to this work and we will investigate this.\n\nReviewer:\n\u201cThe tower stability/height/view experiments seem incomplete. I find them quite interesting, but I'm not entirely sure of what to draw from it.\u201d\nAnswer:\nWe do not regard this section as a central component of the manuscript, but rather a set of additional, exploratory experiments to verify that object-centric representations are indeed beneficial for downstream tasks. The experiments give some support to this hypothesis, though a more comprehensive study would be required to make strong claims.\n\nReviewer:\n\u201cThe abstract promises semi-supervised learning -- is this the tower experiment? This strikes me more as transfer learning.\u201d\nAnswer:\nYes, it is. This approach corresponds to the \u201cLatent-feature discriminative model (M1)\u201d from \u201cSemi-supervised Learning with Deep Generative Models\u201d by Kingma, Mohamed, Rezende, and Welling (NeurIPS 2014).", "title": "Reply to reviewer 2 from the authors - part 2/2"}, "HJxK4nB4iB": {"type": "rebuttal", "replyto": "Bke06RuRYr", "comment": "We thank the reviewer for the thorough feedback and constructive criticism. We updated the paper to address several shortcomings: we rephrased our main claim, we added further qualitative and quantitative results for GENESIS-S, we computed ARI scores for GENESIS and MONet for five different random seeds, and we added ARI scores to Figures 12 and 13 (numbering in updated manuscript) to illustrate the limitations of the ARI metric. Further comments follow below.\n\nReviewer:\n\u201cI'm baffled by the repeated claim that this is a generative model of 3D scenes.\u201d\nAnswer:\nThere appears to be a misunderstanding regarding what is meant by a \u201cgenerative model of 3D visual scenes\u201d. We intend to refer to 2D rendered images of environments with 3D spatial structure (i.e. images from the GQN dataset and ShapeStacks). The majority of previous works on structured latent-variable models (AIR, NEM, R-NEM; IODINE is a notable exception) only use datasets such as Multi-MNIST, Multi-dSprites, Tetrominoes, or ATARI games which lack 3D spatial structure, non-uniform backgrounds, etc. This is elaborated on in the related work section, but we will rephrase the affected parts to \u201crendered 3D scenes\u201d to avoid ambiguity.\n\nReviewer:\n\u201cSimilarly, it's not really a model for objects -- as can be seen in the multi-colored wall example in Fig 3, and Fig 8, 9. its notion of object is a region with uniform color (presumably due to the Gaussian p(x|z) ). So while it's great that it can model regions of uniform color, the system in large part seems to work (in its current form, reading the current manuscript) because the objects are all one color, and doesn't necessarily decompose the scene into objects.\u201d\nAnswer:\nThis is an interesting point of discussion. We would like to note that there is no unique assignment of  \u201cobjectness\u201d, e.g. referring to Figure 3: should a wall be considered as one object, or should tiles in the wall be considered as different objects? Unsupervised structured latent-variable models do not contain any explicit constraints with regards to what constitutes an \u201cobject\u201d. How scenes are decomposed depends merely on the structure of data and the inductive biases in the model. Interestingly, though, despite these challenges the community has managed to develop models that manage to decompose scenes into semantically meaningful components without explicit supervision.\nOverall, we agree that it might be more accurate to talk about \u201cscene components\u201d or \u201centities\u201d; it is also unusual to refer to things like the floor or the sky as objects. However, we decided to adopt the same terminology as used by the community in previous works, such as MONet and IODINE. We were hoping our footnote in the introduction (\u201cWe use the terms \u2018object\u2019 and \u2018scene component\u2019 synonymously in this work.\u201d) clarifies this. Nevertheless, we are open to rephrasing these parts if the reviewer is not satisfied with this justification.\nFinally, we agree that current methods rely heavily on colour and texture as cues for learning to decompose scenes and that scaling these methods to more complex datasets such as real-world images remains an open challenge. Nevertheless, we believe that by providing a fully probabilistic model of scenes that captures relationships between components and a more scalable, probabilistic attention mechanism, GENESIS overcomes key limitations of previous works (independent priors for scene components; inefficient, deterministic attention or expensive iterative inference) and therefore makes valuable contributions towards developing more general methods. \n\nReviewer:\n\u201cGENESIS-S appears as this baseline \"To investigate whether separate latents for masks and component appearances are necessary for decomposition\", but then basically disappears.\u201d\nAnswer:\nWe added Figures 8 and 9 which show qualitative results for scene generation with GENESIS-S after training on GQN; we added the FID scores for GENESIS-S on Multi-dSprites and GQN to Table 2; and we added Figure 11 which show the step-by-step decomposition of three images from the GQN dataset.\n\nReviewer:\n\u201cIn the appendix it's claimed that GENESIS trains faster than GENESIS-S, but if this is the selling point of GENESIS over GENESIS-S, it would be nice to have at least a little quantification of this.\u201d\nAnswer:\nWhile it is true that GENESIS trains faster than GENESIS-S, the main selling point of GENESIS over GENESIS-S is that it does a better job at decomposing scenes into meaningful objects and generating interpretable components (as illustrated in the additional Figures mentioned above). We therefore removed the \u201cfaster training\u201d claim as it misses the point and clarified the benefit of GENESIS over GENESIS-S in the main body of the paper (rather than the appendix).", "title": "Reply to reviewer 2 from the authors - part 1/2"}, "BylK-ZHRYS": {"type": "review", "replyto": "BkxfaTVFwH", "review": "The authors propose a probabilistic generative latent variable model representing a 2D image as a mixture of latent components. It formulates the scene generation problem as a spatial Gaussian mixture model where each Gaussian component comes from the decoding of an object-centric latent variable. The contribution of the proposed method from previous works is the introduction of an autoregressive prior on the component latents. This allows the model to capture autoregressive dependencies among different components and thus help generate coherent scenes, which has not been shown in the previous works. In the experiments, the authors compare GENESIS with MONet and VAEs qualitatively and quantitatively and show that the model outperforms the baseline in terms of both scene decomposition and generation.\n\nThe proposed model seems like the right direction to improve upon MONet and it is nice to see the generation results. It is also nice to see the fully probabilistic modeling of the problem. Although it would improve over the MONet, I'm nevertheless not sure if the framework of sequential component generation (applying both MONet and GENESIS) can be a robust approach to more complex scenes, e.g., with a larger number of objects. Also, the mixture-based approach seems not guarantee the object-level decomposition. For example, if in the scene there are many objects of the same shape, size, and color, I think the proposed model may not properly distinguish them, as shown in some of the wall patterns in the experiments. But, I'm not sure what would happen if K is set to a large number to deal with this. Then, it would have the problem of long-term dependency in sequences.\n\nSome comments and questions:\n1. It would be good to show the qualitative result of the simplified Genesis-s and its qualitative result in the FID experiment.\n2. Is the VAE baselines (BD-VAE and DC-VAE) trained by maximizing the ELBO? If it is the case, will the result of the FID experiment section be different if we train the VAE baseline with GECO?\n3. Some detail of the implementation is missing, e.g. how do we model the posterior q_{\\phi}(z_{k}^{c} | x, z_{1: k}^{m}).\n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}}}