{"paper": {"title": "Neural Stored-program Memory", "authors": ["Hung Le", "Truyen Tran", "Svetha Venkatesh"], "authorids": ["lethai@deakin.edu.au", "truyen.tran@deakin.edu.au", "svetha.venkatesh@deakin.edu.au"], "summary": "A neural simulation of Universal Turing Machine", "abstract": "Neural networks powered with external memory simulate computer behaviors. These models, which use the memory to store data for a neural controller, can learn algorithms and other complex tasks. In this paper, we introduce a new memory to store weights for the controller, analogous to the stored-program memory in modern computer architectures. The proposed model, dubbed Neural Stored-program Memory, augments current memory-augmented neural networks, creating differentiable machines that can switch programs through time, adapt to variable contexts and thus fully resemble the Universal Turing Machine. A wide range of experiments demonstrate that the resulting machines not only excel in classical algorithmic problems, but also have potential for compositional, continual, few-shot learning and question-answering tasks. ", "keywords": ["Memory Augmented Neural Networks", "Universal Turing Machine", "fast-weight"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper presents the neural stored-program memory, which is a key-value memory that is used to store weights for another neural network, analogous to having programs in computers. They provide an extensive set of experiments in various domains to show the benefit of the proposed method, including synthetic tasks and few-shot learning experiments.\n\nThis is an interesting paper proposing a new idea. We discuss this submission extensively and based on our discussion I recommend accepting this submission. \n\nA few final comments from reviewers for the authors:\n- Please try to make the paper a bit more self-contained so that it is more useful to a general audience. This can be done by either making more space in the main text (e.g., reducing the size of Figure 1, reducing space between sections, table captions and text, etc.) or adding more details in the Appendix. Importantly, your formatting is a bit off. Please use the correct style file, it will give you more space. All reviewers agree that the paper are missing some important details that would improve the paper.\n- Please cite the original fast weight paper by Malsburg (1981).\n- Regarding fast-weights using outer products, this was actually first done in the 1993 paper instead of the 2016 and 2017 papers."}, "review": {"B1eL9UG5FH": {"type": "review", "replyto": "rkxxA24FDr", "review": "The authors discuss an interesting idea for memory augmented neural networks: storing a subset of the weights of the network in a key-value memory. The actual weights used are chosen dynamically. This is similar to having programs/subprograms in a classical computer. To the best of our knowledge, this idea was not previously studied, and it was a missing part of the study of memory augmented neural networks until now.\n\nIn the abstract authors mention a von Neumann architecture. However, the von Neumann architecture assumes that the program and the data are stored in the same memory, which is not the case for what the authors propose (a significant limitation is that the vector size of the memory doesn\u2019t match the number of network weights). It is more reminiscent of the Harvard architecture. Please correct!\n\nAt the end of section 3.1 and in Eq 6 and 7 the authors claim that they use the regularization loss that makes the keys different to prevent programs from collapsing. However, this is only one way of collapsing. The other way is more reminiscent of what tends to happen in routing networks, where the controller network (here P_I) learns to attend to a single module (here memory slot) or attend to all of the modules with roughly equal weight. Are such collapses experienced during this work? If not, what could be the reason?\n\nThe main difference between routing networks and NSM is that in the former the selection is done in activation space, while in NSM it happens in weight space. Thus one should expect the same exploration/exploitation, transfer/inference tradeoffs, module collapse, etc to happen [1]. What makes it better/worse compared to these methods?\n\nThe method is mainly tested on methods used to test MANNs. However, because of its relation to the routing networks, it would also be interesting to compare them to [2] or [3] to see whether they suffer from the same problems.\n\nBecause the program distribution is rarely close to one-hot (all programs are contributing to some degree), could you please provide as a baseline a version where the program distribution is fixed and uniform?\n\nIn section 3.2, the last 2 sentences of the first paragraph, in one sentence the authors say \u201cwe store both into NSM to completely encode a MANN\u201d and in the next they say \u201cin this paper, we only use NSM to store W^C\u201d. Please make this consistent.\n\nIn the last sentence of the first paragraph of section 3.2, the authors say \u201cis equivalent to the Universal Turing Machine that can simulate any one-state Turing Machine\u201d. Could you further clarify this?\n\nIn the second paragraph of section 3.2, the authors say \u201cP_I simulates \\delta_u of the UTM\u201d. Shouldn\u2019t \\delta_u also include the state transition function, which is not included in P_I?\n\nSeveral sentences mention \u201cdirect attention,\u201d which is described as a neural network producing the weights directly. Why call this attention at all? Isn\u2019t this a form of fast weights? Probably they should be called fast weights or dynamic links. \n\nHow does this relate to the original work on this? Note that the first end-to-end-differentiable systems that learn by gradient descent to quickly manipulate the fast weights of another net (or themselves) were published between 1991 and 1993 - see the references [FAST0-3a] [FAST5] [FASTMETA1-3] in section 8 of http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html\n\nThe authors also claim to \u201cWhen the memory keys are slowly updated, the meta-network will shift its query key generation to match the new memory keys and possibly escape from the local-minima\u201d. Why? What drives the network to use new memory keys in this case?\n\nIn section 3.3, \u201cP_I is a meta learner\u201d. This is not the correct term here, because P_I is not leveraging old knowledge to learn faster (nor in improving the learning algorithm itself, nor in the recently more popular transfer learning sense), but it learns to select which weights to use.\n\nIn figure 2b, could you show the mean with std?\n\n\u201cstates of the generating automation\u201d should be automaton instead.\n\n\u201cAs NUTM requires fewer training samples to converge, it generalizes better to unseen sequences that are longer than training sequences\u201d - Why this indication of a causal relationship here? Why would requiring fewer training samples make it generalize better?\n\nIn the sequencing tasks (section 4.3), how does the network know which task to perform? Is the task label one-hot encoded and concatenated with the input? From Fig 13 and 14, it seems to be a fixed concatenation of inputs, without any indication which task is which, and then a fixed concatenation of the outputs is requested from the network. Could you clarify, please?\n\nWe found Figure 5 very confusing. At first it seemed like the X-axis is \u201ctime\u201d (where we assumed the network is trained on a sequence of tasks C, RC, AR, PS), and the title above the subplots indicates for which task the performance is plotted over \u201ctime\u201d (which in this case would correspond to the time steps after completion of the training phase indicated on the X-axis). However, the second subplot shows perfect RC performance after having been trained only on C. We probably misunderstood the plots: the title plot is the \u201ctime\u201d, and the X-axis shows just the datasets (so the order is not important) - so the plots don\u2019t show the performance on the specific dataset over the course of training as assumed initially. But if so, why are they connected by lines and why not use a bar plot? It would be more interesting to see \u201ctime\u201d on the X-axis, so one can see how performance degrades while training on new tasks. \n\nIn Figure 5, what is the average performance supposed to show? If you average the performance over each training phase, the dataset trained first will yield better performance than the ones trained later. This is because the last trained one will have a performance near chance for most of the time it is measured, while the first one will have the best performance on the first measurement and will degrade somewhat because of catastrophic forgetting - but hopefully it will still be better than chance.\n\nIn Figure 5, numbers on the Y-axis: the first 10 and 100 place digits are misaligned.\n\nIn the last paragraph of section 4.4, \u201c- over 4 tasks\u201d, the - is confusing because it is not part of the formula. Please rewrite the formula in a different style or remove the -.\n\nIn section 4.5, the authors write that the best hyperparameters they found are p=2 for 5 classes and p=3 for 10. What could be the reason for p and the number of classes being so unrelated?\n\nIn section 4.6, \u201cDNC is more powerful and thus suitable for NSM integration\u201d - this suggests that the reason why it is suitable is that it is more powerful, but that is not the case since it was integrated into NTM, too.\n\nIn section 5, \u201cthey impede modularity\u201d. Why? Maybe they don\u2019t increase it, but why impede? \u201cwhich looses modularity\u201d - Why? The original RNN is not modular either. Why is this even less modular?\n\nIn figure 7 etc. it would be nice to have a title for the topmost subplot, too.\n\nWhat is a dynamic n-grams task? Could you clarify or include a reference?\n\nCould you clarify how the white-black-yellow-orange input charts should be understood (Fig 3d, Fig 15, etc)?\n\nHow to understand the preservation plots (Fig 3d)?\n\nAdditional references besides [FAST0-3a] [FAST5] [FASTMETA1-3] mentioned above:\n\n[1] Rosenbaum et al, Routing Networks and the Challenges of Modular and Compositional Computation\n[2] Chang et al, Automatically Composing Representation Transformations as a Means for Generalization\n[3] Kirsch et al, Modular Networks: Learning to Decompose Neural Computation\n\n\nWe think this paper is quite interesting, and might improve our rating provided the comments above were addressed in a satisfactory way in the rebuttal.\n\nEdit after rebuttal: score increased to 8. However, the authors should not forget to cite the original fast weight paper by Malsburg (1981). And one more thing: in the revised version they write \"More recent works implement fast-weight using outer-products,\" citing papers from 2016-17, but this was actually first done in the 1993 paper they already cite. ", "title": "Official Blind Review #3", "rating": "8: Accept", "confidence": 4}, "SJexxPC2YB": {"type": "review", "replyto": "rkxxA24FDr", "review": "## Update\n\nI am changing my score to 7 (not adjusting the discretised score in the openreview interface though)\n\n\n\n\n\nSummary: The authors present the Neural Stored-program Memory (NSM), an architectural addition which is suitable for a large class of existing MANN models (NTM, DNC, LRUA etc). The model contains a second memory of key value pairs, which on each timestep can be read via cosine distance similarity on the keys, and for which the values contain all the weights necessary for the Interface Network (IN). The IN also receives the recurrent controller output, and produces an interface vector which is used to read or write to the regular memory. The definition is sufficiently general to allow the computation between an interface vector and the regular memory to be done according to NTM/DNC/etc schemes.\n\nNSM allows a MANN to effectively switch programs at every timestep, more closely matching classical computers and the Universal Turing Machine. The authors include a wide range of experiments, both showing faster learning on standard NTM tasks, and also introduce \"Sequencing tasks\" for which a single episode contains multiple instances of the standard NTM tasks in sequence. In principle this should allow the whole system to learn different programs for the subproblems, and dynamically switch between them as soon as the task changes. In principle a standard NTM could learn to do many of these tasks in a row, but the authors show that even when an NTM combined with NSM (denoted NUTM) has fewer trainable weights in total than a plain NTM, for some task sequence combinations the NUTM learns much faster.\n\nExperiments on continual learning, few shot learning and text based question answering back up the wide applicability of this technique, with many strong results including new SOTA on bAbI.\n\n\nDecision: Accept. Key reasons are that this is a relatively straightforward application of hypernetworks within a recurrent controller, which both has appealing justifications in the context of both Von Neumann and Turing models of computation. This simplicity is a positive, and the authors make a convincing argument that NSM can be applied to any MANN solving a sufficiently complex problem. A secondary reason is the extensive evaluation & hyperparam details, and while I do have some minor points on which I think the paper could be clarified (see below) I think this is overall a very nice paper. With the clarifications below addressed, I would give this paper a 7 but from the options I have available to choose from, 6 is the best fit for the current manuscript.\n\nSupporting arguments: The visualization of 'program distribution' shows extremely clear phase changes as the underlying task changes - both within a single task (the reading phase vs writing phase for repeat copy) and across task boundaries (copy -> associative recall). Combined with the learning curves / results in the various tables, it is to me clear that the model is performing as designed.\n\nDespite the name of the \"Meta Network\", and having a vague flavour of metalearning, the model does not require any elaborate \"backprop through the inner training loop\" of MAML et al, which is a benefit in my opinion.\n\n\n\n\nAdditional feedback: Some of the experiments could be slightly more convincing - particularly Figure 5 which is lacking error bars. In my experience these architectures can have relatively high variance in performance, compared to other supervised domains, as evidenced by the spicy learning curves even in Figure 2 a). Error bars across multiple runs for figure 5 would be good, particularly for the points where the lines are close (eg after training on PS, the performance for C and RC is close for both models).\n\nThe formatting of some figures and graphs could be improved:\n* Figure 1 - the graph could use some more text labels, rather than mathematical notation which needs to be referred to below. The colours are also slightly confusing - the program memory has slightly different shades of orange for the keys, slightly different shades of green for the values, whereas the regular memory has a slightly wider variety of colours. I was not sure at first whether this should be interpreted as indicating something important. Additionally, the value read from the NSM is reshaped to a 2x4 shape with various shades of blue, which then becomes the weights for a pink network? I think the colour adds little and may confuse people. There are some other issues, such as the $r_t$ value which should really come from the main memory as a result of the interface vector. With the supporting text, understanding the system is not hard, but I feel another pass over the diagram would benefit the camera ready version - consider ditching colour entirely, unless there is going to be some consistent meaning to things being the same colour vs not. Text labels with the various arrows (eg \"vector used to lookup in NSM\", \"used as network weights for Interface Network\") may improve clarity. $c_t$ should also be labelled on the diagram.\n\n* The y axis scale in figure 5 is very confusing - it took me several looks before I noticed that it goes from 50% to 100%, due to each number having digits in different vertical positions and different font sizes\n\n* Figure 2 y axis scale is a bit too small to easily read.\n\n* Figure 3: both read and write locations\u00a0are shown on a single plot, but the green line that separates them is lamost unreadable on a printout. The task dependent, presumably manually chosen, approach to picking where this visualisation toggle should be made is a bit arbitrary - I would prefer to see read and write locations as separate subplots, as in the appendix.\n\n\nI found the exact details of number of programs versus number of heads (and the type of those heads) a bit confusing. In www.github.com/deepmind/dnc the code has 'number of read heads' and 'number of write heads' being two independently set integer parameters. This paper refers to \"$R$ control heads\", but I am not exactly clear on how these are divded between read and write duties. Algorithm 1 references that write heads return zero on line 8 but not other mention of the two mutually exclusive types is made. The text towards the end of section 4.1 refers to the \"no-read\" strategy being assigned mostly to one program - this makes it sound like each program can (softly?) interpolate between reading or writing (or both?). The start of appendix B shows program distribution for read and write heads separately, but this then begs the question of what is happening in the examples for which only one program distribution is shown (eg Figure 3) - clearly we need to read and write for all of these tasks, so is one head with two programs doing both simultaneously? In the interests of reproducability, clarification here is essential.\n\nSeparately, the decision of having a different program memory per control head is interesting - it's not obvious to me why this would be necessary, surely one program memory would be sufficient as long as thre is a different $P_{I,n}$ (alg 1 line 5) network to choose a different program for the head? It would be good to see a line added to the paper justifying this choice.\n\nIt seems like not all training hyperparams are specified in the appendix - eg the settings specified in Table 9 only apply to the few shot learning task, additional hyperparams are specified for bAbI, but I cannot see the training hyperparams for the experiments in sections 4.1 - 4.4.\n\nMinor correction:\n\n\"As Turing Machine is finite state automata\" -> \"As Turing Machines are finite state automata\"\n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "HkxjnCvjiH": {"type": "rebuttal", "replyto": "SklQWQ2cjH", "comment": "Thank you for your reply. Regarding your concern on the loss term (Eq. 6), we explain the necessity of this loss in the second paragraph of Sec. 3.1. We want to avoid the situation where changes in the query key (representing the context) do not lead to changes in the program. If the program keys are close to each others, the distances from the query key to the program keys are almost equal regardless of the value of the query key. This leads to uniform program attentions\u2013one form of program collapse. Hence, program keys should be far away. You can refer to our response to the reviewer 3 (the second paragraph) to understand more on why separated program keys are usually good enough for our problem.\n\nIt is clear from Eq. 6 that our extra loss is designed to separate the program keys. Our ablation study in Sec. 4.2 demonstrates that it helps improve the performance of NUTM. It should be noted that even without the extra loss, NUTM shows better performace than many other baselines (Fig. 2b).  Hence, our model does not depend heavily on tuning with a specialized loss. We introduce the extra loss as a nice-to-have feature for our model. \n\nWe agree with you that when the number of the programs increases, computing Eq. 6 may be harder. However, in our setting, the program memory size is moderate (<6) and the key dimension is small (equal the number of programs). Hence, computing Eq.6 remains efficient. For applications that require many programs, we can reduce the computation cost by sampling pairs of program keys to impose the regularization. The experiments with large scale program memory is out of scope of this paper and will be left for future works.\n\nRegarding your comment on choosing one MANN to describe, we have considered it carefully. In Sec. 2, we describe the general operation of MANNs and providing a specific example may be a good idea. Due to page limit, we cannot put it in the main manuscript. However, we will add the description of a specific MANN (NTM) in the appendix of the final revision. Thank you for your suggestion.", "title": "Response to Discussion of Review#2"}, "HyxLvL7IiH": {"type": "rebuttal", "replyto": "Skx4JPuvYH", "comment": "We thank the reviewer for your helpful comments. We address your concerns one by one as follows,\n\n\u201cHence, modifying the program itself is not possible, ...\u201d. We want to clarify that our working program (the one that is loaded into the interface network) is modified across timestep and our basis program (the one that is stored in the program memory) is modified by backpropagation.\n\n\u201cHowever this read mechanism is weak, ...\u201d. Our program reading mechanism may be simple. Yet, it helps boost the performances of various MANNs. More importantly, it helps the models behave as designed (see Fig. 3 (a,b,c) and other figures in Appendix). \n\n\u201cIt remains unclear where this is leading ...\u201d. Our method leads to a new class of MANNs that can store and query both the weights and data of their own controllers. One reason why MANNs maybe not useful is the lack of program memory. Without program memory, MANNs are inflexible, prone to perseveration and fail to simulate UTM. We aim to fix this weakness in this paper. Our attempts help MANNs improve their performance in various experiments including algorithmic, compositional, continual, few-shot learning and question-answering tasks (Sec. 4).\n\n\u201cThe writing is oddly inconsistent, ...\u201d. As our method is generic and can apply to various MANNs, we cannot describe all memory access mechanisms of the chosen MANNs in the main manuscript. NSM's memory access uses key-value attention, which can be thought of as an extension of content-based attention presented in NTM or DNC. The main difference is that the value in NSM is the weight of a neural network, not the data as in other MANNs. \n\n\u201cOverall, I don't see clear, actionable insights, ...\u201d. In this paper, we provide the reader with a new insight into the simulation capacity of MANNs. We point out the missing part of current MANN literature and propose a more generic architecture that can simulate UTM. We have conducted 6 experiments to validate the benefit of our proposed architecture. \n\nThank you for your minor comment. The sentence is based on our intuition and validated by Table 1. To make it less confusing, in the updated manuscript, we just report facts and do not assume any causality: \u201cNUTM requires fewer training samples to converge and it generalizes better to unseen sequences that are longer than training sequences\u201d.", "title": "Response to reviewer 2"}, "HkgKNSQ8or": {"type": "rebuttal", "replyto": "SylTpEXLir", "comment": "We have changed \u201cautomation\u201d to \u201cautomaton\u201d in this revision. Thank you for correcting our mistake. \n\nRegarding our sentence \u201cAs NUTM requires fewer training samples to converge, ...\u201d, we admit that cannot prove the causality. This is based on intuition and empirical results (Table 1). To make it less confusing, in the updated manuscript, we just report facts and do not assume any causality: \u201cNUTM requires fewer training samples to converge and it generalizes better to unseen sequences that are longer than training sequences\u201d.\n\nIn the first paragraph of Sec. 4.3, we explain \u201cthe order of subtasks in the sequence is dictated by an indicator vector put at the beginning of the sequence\u201d. We do not use task label. Instead, we use order label to tell the model the order of subtasks. For example, in C+RC tasks, we use an indicator [0] to indicate the order C\u2013RC and [1] for RC\u2013C. The indicator vector is then padded to the same size with other input vectors and put at the beginning of the sequence. The order is randomized so the concatenation of inputs is not fixed. \n\nIn Fig. 5, we follow the standard report format in [1] so we do not use bar plot. X-axis is across tasks, not time. We explain in the caption: \u201cbit accuracy on four tasks after finishing a task\u201d and the title of each subplot indicates the finished task. Adding training steps will further complicate the plot so we decide to keep it as is. \n\nYour understanding of the average subplot in Fig. 5 is correct. The average performance tends to lower for later tasks. However, because the tasks are correlated (e.g., most tasks often share the same \u201cwrite all\u201d strategy in the encoding phase), the performance on later tasks can be improved even before we train them with their data (a form of transfer learning). For example, in the first subplot of Fig. 5, after training C, the performances on RC are improved moderately. Thus, we think it is useful to compute the average performance to measure both catastrophic forgetting and knowledge transferring effects. \n\nIn this revision, we have fixed the misalignment of the y-axis in Fig. 5 and remove - in \u201c- over 4 tasks\u201d. Thank you for pointing out these errors.\n\nIn Sec. 4.5, increasing the number of classes normally requires more adaption. For example, each class of images may need a specific program to store into the data memory thus 10 classes should have more programs than 5 classes. However, as the classes are always correlated (e.g., sharing visual features), the number of optimal programs may be actually less than the number of classes. Also, learning with many programs may be uneasy. Hence, we cannot give you an exact reason. Only by tuning the number of programs, we know the ones that work best. \n\nIn Sec 4.6, our complete sentence is \u201cDNC is more powerful and thus suitable for NSM integration to solve non-algorithmic problems such as question answering\u201d. We only assume DNC is more powerful than NTM in question answering and suitability here applies to only this task. We intend to show that our NSM can work with various MANNs. That is why we integrate NSM into both NTM and DNC. \n\nIn Sec. 5, Multiplicative RNN looses modularity because it no longer keeps the form of multiple slow-weights as its precursor-Tensor RNN does. We agree with you that using \u201cimpede\u201d is a bit extreme. we have changed to \u201cdo not support\u201d in this revision. \n\nDynamic N-Grams is introduced with other NTM's synthetic tasks in Sec. 4.1. It is from [2]. \n\nIn Fig. 3, black is bit 0, white is bit 1 in vector data. Orange is prediction error (noted in the caption). In Fig. 15 and some other images, because data vectors not only include value 0-1, but also other float values (e.g., priority score), the color scale is automatically changed. Basically, in priority sort task, yellow is prediction error, and orange is bit 1. We have noted this inconsistency in this revision.\n\nPerseveration in Fig. 3d is explained in the second paragraph of Sec. 4.3. We add more details here. As you can see, there are two input patterns: the first is for Repeat Copy and the second Copy. Our example shows that NTM only executes Repeat Copy with the first pattern. It is easier to look at the reading behaviors. NTM repeatedly reads from the memory slots (0-15) containing the first pattern (its reading is not sharp, which leads to incomplete reconstruction of the first pattern). After 11 repeats, NTM should have changed to memory slots (16-30) to execute Copy on the second pattern. However, it fails to do so even when its writing during encoding is relatively good (both patterns are stored correctly, see Fig. 17). \n\nReference:\n\n[1] Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence. In Proceedings of the 34th International Conference on Machine LearningVolume 70, pp. 3987{3995. JMLR. org, 2017\n\n[2] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014", "title": "Response to reviewer 3 (2/2)"}, "SylTpEXLir": {"type": "rebuttal", "replyto": "B1eL9UG5FH", "comment": "We thank the reviewers for your detailed review. We address your concerns one by one as follows,\n\nOur proposal and Von Neumann Architecture share the stored-program concept. We agree that we should not have claimed that our model resembles Von Neumann Architecture in the abstract. We remove this phrase in the updated manuscript. \n\nWe agree with you that program collapse can happen even when the program keys are different. However, under our MANN setting, given the program keys are different, the collapse or equal weight problems are not serious. As stated in Sec. 3.3, accessing the program memory is basically a regression problem that maps $c_{t}$ to $\\xi_{t}=\\left\\{ k_{t}^{p},\\beta_{t}^{p}\\right\\}$ . Assume that the program collapse or equal weight problems happen, NUTM now roughly becomes normal NTM with a single program. Even with a single program, NTM can learn to partition its state space to some degree (see Fig. 6 (a,c)). That provides enough training signals for the regressor (P_I in this case) to discriminate its input space $c_{t}$, which results in different $k_{t}^{p}$ when there is a significant change in the hidden state space. A small difference amongst $k_{t}^{p}$ will alleviate the program collapse or equal weight problems, which in turn, improves the clusters in the space of $c_{t}$ and then again alleviates further the program collapse or equal weight problems and so on. \n\nThank you for pointing out interesting papers on modular learning. We discuss the relation between our model and module networks in the second paragraph of Sec. 5. Our approach is different since we do not use \u201chard\u201d modules. Our model is different from routing networks since it not only selects but also interpolates the modules and the learning is fully differentiable. Moreover, in this paper, we aim to provide the first working simulation of UTM using MANN rather than focus on modular learning. Within the scope of our paper and due to page limit, we cannot go into details of analyzing problems related to modular learning or comparing our approach with other modular methods. \n\nThank you for your suggestion on another baseline. A baseline with fixed and uniform program distribution will not adapt well with context changes. To verify that, we have added this baseline in the ablation study. The result can be found in Sec. 4.2.\n\nThank you for showing us the inconsistency in the last sentence of the first paragraph of section 3.2. We have changed it to \u201cwe should store both into NSM ...\u201d. \n\nOne-state Turing machines do not use state transition function. They just read from and write to the memory tape. In other words, the interface network simulates a one-state Turing machine. Our proposed model stores only the interface network in the program memory. It can be interpreted as a Universal Turing machine that stores only one-state Turing machines. Hence, the UTM can simulate any stored TM by using the TM program. \n\nYour comment on the second paragraph of Sec. 3.2 is correct. P_I alone does not simulate \\delta_u of the UTM. We have changed it to \u201cTogether with the RNN, P_I simulates \\delta_u of the UTM ...\u201d\n\nIn the third paragraph of Sec. 3.2, we define \u201cdirect attention\u201d as generating the weight $w_{t}^{p}$ directly without matching $k_{t}^{p}$ with $\\mathbf{M}_{p}\\left(i\\right).k. w_{t}^{p}$ is then used to weight (attend to) the program memory (Eq. 5). The generated weight $w_{t}^{p}$ is not the weight of a neural network. Hence, we do not call it fast weight. \n\nIn the third paragraph of Sec. 5, we describe the relationship between our programs and fast-weights. The main difference is that our \u201cfast-weight\u201d is composed by interpolations of a set of programs. The program is slowly updated by back-propagation. Moreover, the \u201cfast-weight\u201d in our paper is motivated by UTM and we focus more on simulating UTM. Amongst additional references on fast-weight you recommend, we realize that [FAST3] and [FASTMETA2] are most related. We have revised the third paragraph of Sec. 5 to include these references. \n\nRegarding our sentence \u201cWhen the memory keys are slowly updated, ...\u201d, we do not mean the network needs to use new memory keys. The network just needs to change the way it generates the query key to match the change of program memory keys. This is inevitable since the gradient backpropagated to P_I contains $\\mathbf{M}_{p}\\left(i\\right).k$ terms. \n\nRegarding our sentence \u201cP_I is a meta learner\u201d, a program can be thought of as the knowledge about a learnt task/subtask. In an ideal setting, P_I stores the knowledge in separated slots of the program memory. Hence, the knowledge is preserved and can be utilized by P_I when it handles new task/subtask. It is fair to say P_I is a meta learner. \n\nIn Fig. 2(b), due to small size, adding std make the plot hard to see. We add a separate bigger plot of Fig. 2(b) with mean and std in Appendix Fig. 23 in this revision.\n\n", "title": "Response to reviewer 3 (1/2)"}, "BkempfXIsB": {"type": "rebuttal", "replyto": "SJexxPC2YB", "comment": "We thank the reviewer for your thoughtful comments. We address your concerns one by one as follows,\n\nWe agree that error bars are necessary for Fig. 5. We have re-run the experiments 5 times to report the results with error bars in this revision. We also fix the misalignment of the y-axis in Fig. 5 and scale-up y-axis in Fig. 2a. Thank you for pointing out these errors.\n\nIn this revision, we have made Fig. 1 clearer by synchronizing the colors and adding a legend box to explain the components of our model. We also agree that the read value should come from the memory M. Thank you for your suggestion.\n\nIn Fig. 3, we plot reading behaviors for the repeat copy (a) and C+AR (c), and writing behavior for the priority sort (b). The green line separates the encoding and decoding phase. Due to space limit, we cannot plot both reading and writing behaviors for each task as in the appendix.\n\nWe want to make it clear that $R=R_{r}+R_{w}$ where $R_{r}$ and $R_{w}$ are the number of read and write heads, respectively. Each head, either read or write, is associated with a memory() function. The implementation of memory() depends on the type of heads (read or write) and MANNs. The duties of read or write are specified inside the function memory(). In this revision, we make it clearer in Line 8 Algo. 1. Moreover, we modify all hyperparam tables in the Appendix to clarify the number of read and write heads used in our experiments. The \u201cno-read\u201d strategy text describes Fig. 3 (a), which is only about reading behavior in repeat copy. The \u201corange\u201d program only represents the strategy for the read head in this task. There is no writing program in this description. In this task, there are 1 read head and 1 write head, each of which has 2 programs. In total, we have 4 programs and we only plot two of them (orange+blue) in Fig. 3 (a). We are sorry that we cannot include visualization for writing program distribution due to space limit. In general, a program cannot do both reading and writing because the set of parameters for reading and writing (the interface vectors) are different. In Fig. 8, the orange program in the last subplot (for write head) is different from the one in the third subplot (for read head). To make it clear, we add head types in Fig. 3 in this revision.\n\nYour question on why not one program memory is interesting. Assume that you have R heads and want to apply P programs per head. If you use single program memory, there should be $R\\times P$ program slots and you have to attend to all of these slots. That is, all slots interact together. In our approach, we make use of an inductive bias that the subset of programs for each head should be separated from each other. One NSM per head is to ensure the programs for one head do not interfere with other heads and thus, encourage functionality separation amongst heads (otherwise, we may not need multiple heads). Also, as each program memory now only has P program slots, program attention should be easier. It is similar to hierarchical attention. You attend to the program memory first, then attend to the program slot in that program memory. In our case, the first attention is straightforward due to the inductive bias (we associate the head with the program memory directly). We have added a brief explanation in the updated manuscript (the fourth paragraph of Sec. 3.2).\n\nHyperparams for Sec. 4.1, 4.2 are in Table 4, Sec, 4.3 in Table 6. We only missed hyperparams for Sec. 4.4. Thank you for pointing this out. In this revision, we list hyperparam table for Sec. 4.4 in Table 8.\n\nYour minor correction is correct. Thank you for correcting us. ", "title": "Response to reviewer 1"}, "ryxDlvQLjS": {"type": "rebuttal", "replyto": "rkxxA24FDr", "comment": "We would like to thank all the reviewers for their constructive feedback. In this revision, major changes include (1) update Fig. 1 with simple colors and more text labels, (2) add uniform program baseline in the ablation study with new result in Fig. 2b, and add a bigger version of Fig. 2b with error bars in Appendix Fig. 23 (3) fix misalignment and add error bars for Fig. 5. Other specific adjustments are mentioned in the responses below.", "title": "Summary of revision"}, "Skx4JPuvYH": {"type": "review", "replyto": "rkxxA24FDr", "review": "= Summary\nA variation of Neural Turing Machines (and derived models) storing the configuration of the controller in a separate memory, which is then \"softly\" read during evaluation of the NTM. Experiments show moderate improvements on some simple multi-task problems.\n\n= Strong/Weak Points\n+ The idea of generalising NTMs to \"universal\" TMs is interesting in itself ...\n- ... however, the presented solution seems to be only half-way there, as the memory used for the \"program\" is still separate from the memory the NUTM operates on. Hence, modifying the program itself is not possible, which UTMs can do (even though it's never useful in practice...)\n- The core novelty relative to standard NTMs is that in principle, several separate programs can be stored, and that at each timestep, the \"correct\" one can be read. However this read mechanism is weak, and requires extra tuning with a specialized loss (Eq. (6))\n~ It remains unclear where this is leading - clearly NTMs and NUTMs (or their DNC siblings) are currently not useful for interesting tasks, and it remains unclear what is missing to get there. The current paper does not try show the way there.\n- The writing is oddly inconsistent, and important technical details (such as the memory read/write mechanism) are not documented. I would prefer the paper to be self-contained, to make it easier to understand the differences and commonalities between NTM memory reads and the proposed NSM mechanism.\n\n= Recommendation\nOverall, I don't see clear, actionable insights in this submission, and thus believe that it will not provide great value to the ICLR audience; hence I would recommend rejecting the paper to allow the authors to clarify their writing and provide more experimental evidence of the usefulness of their contribution.\n\n= Minor Comments\n+ Page 6: \"As NUTM requires fewer training samples to converge, it generalizes better to unseen sequences that are longer than training sequences.\" - I don't understand the connecting between the first and second part of the sentence. This seems pure speculation, not a fact.", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 3}}}