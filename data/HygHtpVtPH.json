{"paper": {"title": "Laplacian Denoising Autoencoder", "authors": ["Jianbo Jiao", "Linchao Bao", "Yunchao Wei", "Shengfeng He", "Honghui Shi", "Rynson Lau", "Thomas Huang"], "authorids": ["jiaojianbo.i@gmail.com", "linchaobao@gmail.com", "wychao1987@gmail.com", "shengfenghe7@gmail.com", "shihonghui3@gmail.com", "rynson.lau@cityu.edu.hk", "t-huang1@illinois.edu"], "summary": "We propose a new denoising autoencoder with Laplacian pyramid editing, results in improved representation learning capability.", "abstract": "While deep neural networks have been shown to perform remarkably well in many machine learning tasks, labeling a large amount of supervised data is usually very costly to scale. Therefore, learning robust representations with unlabeled data is critical in relieving human effort and vital for many downstream applications. Recent advances in unsupervised and self-supervised learning approaches for visual data benefit greatly from domain knowledge. Here we are interested in a more generic unsupervised learning framework that can be easily generalized to other domains. In this paper, we propose to learn data representations with a novel type of denoising autoencoder, where the input noisy data is generated by corrupting the clean data in gradient domain. This can be naturally generalized to span multiple scales with a Laplacian pyramid representation of the input data. In this way, the agent has to learn more robust representations that can exploit the underlying data structures across multiple scales. Experiments on several visual benchmarks demonstrate that better representations can be learned with the proposed approach, compared to its counterpart with single-scale corruption. Besides, we also demonstrate that the learned representations perform well when transferring to other vision tasks.", "keywords": ["unsupervised", "representation learning", "Laplacian"]}, "meta": {"decision": "Reject", "comment": "The main idea proposed by the work is interesting. The reviewers had several concerns about applicability and the extent of the empirical work. The authors responded to all the comments, added more experiments, and as reviewer 2 noted, the method is interesting because of its ability to handle local noise. Despite the author's helpful responses, the ratings were not increased, and it is still hard to assess the exact extent of how the proposed approach improves over state of the art.   Because some concerns remained, and due to a large number of stronger papers, this paper was not accepted at this time."}, "review": {"rkxhzEjk5B": {"type": "review", "replyto": "HygHtpVtPH", "review": "This paper proposes a denoising auto-encoder where the input image is corrupted by adding noises to its Laplacian pyramid representation. Then a DAE is trained to predict the original data and learn a good representation of the data. By corrupting the Laplacian representation, which is multi-scale, the corruption of the image is not local and thus more robust representations are learned.\n\nI personally like this idea. However, it seems a simple extension of the classical DAE. \n\n1. Is it possible to generalize this idea to other representations of the images such as wavelets or sift, or the representations learned by other neural networks? It seems that you can add corruptions to any image representations as long as you can reconstruct the image from the representation. Here, you can reconstruct from  Laplacian pyramid representation.\n\n2. As an empirical work, the experiments in this work is rather small-scale, using only CIFAR10 and MNIST. That seems far from sufficient. ", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "SketDYJioB": {"type": "rebuttal", "replyto": "S1g7TmTQFr", "comment": "Thank you for your positive comments and helpful suggestions. Our response to these suggestions are as follows:\n\n1. From the experiments, standard DAE are harder to train comparing to the proposed LapDAE. In my opinion, this suggests that the local noise are harder to remove comparing to the Laplacian noise. It would be interesting to perform the following experiment to further understand the difference between DAE and LapDAE: train a DAE and LapDAE then \na) Apply LapDAE to reconstruct an image where the random noise is added on the input space (as the standard DAE setting).\nb) Apply DAE to reconstruct an image where the noise is added to the Laplacian pyramid representation.\n\nThanks for the suggestions. We provide the results of the two suggested settings in Fig. 3 (left) and Sec. 4.2 in the revision. Generally, when applying LapDAE to images with corruption on the input space (the first suggested setting), the reconstruction is slightly worse than LapDAE with corruption on the Laplacian space, while still much better than the conventional DAE. On the other hand, when applying DAE to images with corruption on the Laplacian space (the second suggested setting), the reconstruction is improved a bit but worse than the proposed LapDAE. Note that our LapDAE is not trained to denoise spatial noise on the input space but to capture more non-local context information. This is supported by the results where the digits are well reconstructed even with some minor noise in the background.\n\n2. How does the corrupted set C been selected? Moreover, how does the selected layer in the Laplacian pyramid effects the performance? For example, is there a difference in terms of performance of the model when training on LPS4 versus LPS8?\n\nThe corruption set C can be any types of corruptions theoretically. For simplicity, in this paper we choose the random Gaussian noise (Sec. 4.1). As shown in Fig. 1 (a) and discussed in Sec. 3.2, corruption applied to higher levels of the pyramid affects lager spatial scales of the image while that applied to lower levels affects more local area. In the proposed method, we randomly select the levels where the corruption applied to. As a result, the proposed model is able to capture both local and non-local representations through the network optimization. We empirically found that the model performs worse if only apply the corruption on a single level (e.g., LPS4 or LPS8) and it will degrade to the DAE if the corruption is applied at the lowest level (i.e., original image). The model trained on LPS4 performs slightly worse than that trained on LPS8 for deeper layer (e.g., conv5) representation learning while comparable for shallower layer (e.g., conv1).\n\n3. The transformation technique seems to be very helpful for the performance, is it possible to combine it with other benchmark methods like RotNet or Counting?\n\nYes, we believe our technique can be easily combined with other methods like RotNet and Counting, as the proposed technique makes no assumption of the pretext task and thus would not affect the pipeline of other benchmark methods. However, it is difficult to enumerate all possible combinations (each setting has to train a separate model for each dataset) in one single paper and is out-of-scope for our work. Due to the time limit, we perform an initial experiment of combining our technique with the RotNet. Till the time of the revision submission, the preliminary result shows that the proposed LapDAE is able to improve the performance of RotNet as well: (19.0, 31.9, 39.1, 38.6, 37.0) compared to (18.8, 31.7, 38.7, 38.2, 36.5) for the ImageNet performance in Table 1. We will update the final results once we finish the training.\n", "title": "Response to Reviewer #2"}, "BJxlkFJjiS": {"type": "rebuttal", "replyto": "HJgRL-32KH", "comment": "Thank you for your positive comments and insightful suggestions. Our response to the comments and suggestions are provided below:\n\n1. As mentioned above, the results as presented provide a better DAE. The paper would be much stronger if it also provided comparisons to more recent models in representation learning closer to state of the art. For instance, they choose BiGAN as a model for comparison, but these were developed over three years ago and are now outdated in favor of better GAN models.\n\nThanks for the suggestion. As shown in our original submission in Table 1 and Table 2, we compared to several recent models such as RotNet [Gidaris et al. 2018], Domain Adapt [Ren & Lee 2018], and AET-project [Zhang et al. 2019]. As suggested, we further include more recent state-of-the-art works in our revision: Instance [Wu et al. 2018], AND [Huang et al. 2019].\n\n2. The paper would be further strengthened with additional experiments of their representations being qualitatively better than previous models. Their example of image retrieval via nearest neighbor is quite limited when compared to the wealth of tasks GAN models can accomplish.\n\nThanks for the suggestions. We took the image retrieval as an illustration of the effectiveness of the representation learning ability. Since our work is not a GAN-based solution and does not focus on image synthesis, we did not present those image synthesis tasks as in conventional GAN models. We believe our quantitative experiments including those on large-scale datasets demonstrate the effectiveness of the proposed approach in representation learning. Please kindly let us know if there are any other necessary qualitative experiments need to be added and we would be happy to provide.\n\n3. In general, when presenting qualitative results (Figures 3,4, and 5), additional examples should be put into supplementary materials so as to demonstrate the paper did not cherry pick.\n\nAs suggested, we provide additional examples for the qualitative results into the appendix of the revision. Please refer to Fig. 6 \u2013 Fig. 8 in the Appendix for more details.\n\n4. The presentation of the quantitative results is peculiar. The paper chooses to combine their Laplacian DAE with the AET framework. As such, all the results tables should include numbers for AET alone and the conventional DAE with AET.\n\nIn Table 1 of our original submission, we presented the performance for AET alone (the fourth line from the bottom). Since the authors did not present their performance on Pascal VOC in their paper, we re-implement it and provide the corresponding results in Table 2 in the revision. As suggested, we also add the results for the conventional DAE with AET in all the tables in the revision.\n\n5. Lastly, the paper needs a revision for ease of readability, as there are a significant number of grammatical errors that make it hard to read at times. \n\nThanks for pointing out this and sorry for the readability. We have carefully revised the writing and improved the readability in the revision.\n\n\nWe performed the transfer learning experiments following prior works, e.g., [Doersch et al. 2015] [Zhang et al. 2016] [Noroozi et al. 2017] [Gidaris et al. 2018]. As a result, we believe the transfer learning experiments is complete to our knowledge.\n\nPlease kindly let us know if we address your concerns.\n", "title": "Response to Reviewer #3"}, "Skx-N_kooB": {"type": "rebuttal", "replyto": "rkxhzEjk5B", "comment": "Thank you for your appreciation of our idea and the constructive suggestions. Our response to these suggestions are provided below:\n\n1. Is it possible to generalize this idea to other representations of the images such as wavelets or sift, or the representations learned by other neural networks? It seems that you can add corruptions to any image representations as long as you can reconstruct the image from the representation. Here, you can reconstruct from Laplacian pyramid representation.\n\nThanks for the suggestions. While these are interesting directions, they are out-of-scope for this paper. We update in the revision in Section 5 to include them as possible directions to explore. To the best of our knowledge, this work makes the first attempt towards Laplacian space manipulation for self-supervised representation learning and is shown to be effective on both small-scale and large-scale representation learning. We believe our work has the potential to shed light on further progress in this direction.\n\n2. As an empirical work, the experiments in this work is rather small-scale, using only CIFAR10 and MNIST. That seems far from sufficient.\n\nThere might be a misunderstanding here. In the original submission, in addition to the CIFAR10 and MNIST, we did perform experiments and evaluations on large-scale datasets like ImageNet, Places and Pascal VOC, as shown in Sec. 4.4 and Sec. 4.5. We believe these evaluations demonstrated the effectiveness of the proposed method in self-supervised representation learning.\n", "title": "Response to Reviewer #1"}, "S1g7TmTQFr": {"type": "review", "replyto": "HygHtpVtPH", "review": "Denoising auto-encoder (DAE)  is a representation learning framework proposes in [1], which uses noisy input to reconstruct the clean \u201crepaired\u201d input. In the conventional DAE, the noise is directly added to the input space. This paper introduces a novel type of noise, which corrupts the Laplacian pyramid representation, and uses it to train a DAE. The effect of the proposed perturbation is to allow larger scale and semantically meaningful corruption, which may help to learn more transferable representation. Several experiments are conducted showing the effectiveness of the method\n1) On MNIST, LapDAE provides better reconstruction images\n2) On Cifar-10, LapDAE provides better image retrieval results\n3) On Imagenet, combining with the transformation technique in [2], LapDAE achieve state-of-the-art result\n4) On Pascal VOC,  combining with the transformation technique, LapDAE achieve state-of-the-art result on transfer learning\nOverall, I find the idea natural and simple (simplicity of an approach is a good quality to me). At the same time, I also find the contribution a bit limited. The following are further comments and questions.\n\n* From the experiments, standard DAE are harder to train comparing to the proposed LapDAE. In my opinion, this suggests that the local noise are harder to remove comparing to the Laplacian noise. It would be interesting to perform the following experiment to further understand the difference between DAE and LapDAE: train a DAE and LapDAE then \na) Apply LapDAE to reconstruct an image where the random noise is added on the input space (as the standard DAE setting).\nb) Apply DAE to reconstruct an image where the noise is added to the Laplacian pyramid representation.\n\n* How does the corrupted set C been selected? Moreover, how does the selected layer in the Laplacian pyramid effects the performance? For example, is there a difference in terms of performance of the model when training on LPS4 versus LPS8?\n\n* The transformation technique seems to be very helpful for the performance, is it possible to combine it with other benchmark methods like RotNet or Counting?\n\n[1]  Vincent et al. 2010, Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion \n[2] Zhang et al. 2019, AET vs. AED: Unsupervised Representation Learning by Auto-Encoding Transformations rather than Data", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 2}, "HJgRL-32KH": {"type": "review", "replyto": "HygHtpVtPH", "review": "This paper provides a novel approach to learning useful representations with deep learning models. They formulate an entirely unsupervised framework based off autoencoders to accomplish this task. Their data differs from previous works by upsampling a noise-corrupted downsampling of the original inputs. The autoencoder is then trained to reconstruct the original image from this new data. Their experiments demonstrate that using this Laplacian pyramid scheme to generate noisy data leads to an autoencoder that learns better representations compared to a standard DAE. \nOverall, the work in this paper has the potential to be a contribution to ICLR but lacks experimental completeness and clarity. Moreover, the main contribution is a better denoising autoencoder, but in the grand scheme of representation learning, it is unclear how broad of a contribution this is. I would be willing to change my score, upon addressing the following details:\n\u2022\tAs mentioned above, the results as presented provide a better DAE. The paper would be much stronger if it also provided comparisons to more recent models in representation learning closer to state of the art. For instance, they choose BiGAN as a model for comparison, but these were developed over three years ago and are now outdated in favor of better GAN models.\n\u2022\tThe paper would be further strengthened with additional experiments of their representations being qualitatively better than previous models. Their example of image retrieval via nearest neighbor is quite limited when compared to the wealth of tasks GAN models can accomplish.\n\u2022\tIn general, when presenting qualitative results (Figures 3,4, and 5), additional examples should be put into supplementary materials so as to demonstrate the paper did not cherry pick.\n\u2022\tThe presentation of the quantitative results is peculiar. The paper chooses to combine their Laplacian DAE with the AET framework. As such, all the results tables should include numbers for AET alone and the conventional DAE with AET.\n\u2022\tLastly, the paper needs a revision for ease of readability, as there are a significant number of grammatical errors that make it hard to read at times. \nWhile the transfer learning experiments seem incomplete to me, that is not my area of expertise and I cannot judge how convincing that setup is as well as other reviewers. \n", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 3}}}