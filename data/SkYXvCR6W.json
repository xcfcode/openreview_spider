{"paper": {"title": "Compact Encoding of Words for Efficient Character-level Convolutional Neural Networks Text Classification", "authors": ["Wemerson Marinho", "Luis Marti", "Nayat Sanchez-pi"], "authorids": ["wemerson_marinho@id.uff.br", "lmarti@ic.uff.br", "nayat@ime.uerj.br"], "summary": "Using Compressing tecniques to Encoding of Words is a possibility for faster training of CNN and dimensionality reduction of representation", "abstract": "This paper puts forward a new text to tensor representation that relies on information compression techniques to assign shorter codes to the most frequently used characters. This representation is language-independent with no need of pretraining and produces an encoding with no information loss. It provides an adequate description of the morphology of text, as it is able to represent prefixes, declensions, and inflections with similar vectors and are able to represent even unseen words on the training dataset. Similarly, as it is compact yet sparse, is ideal for speed up training times using tensor processing libraries. As part of this paper, we show that this technique is especially effective when coupled with convolutional neural networks (CNNs) for text classification at character-level. We apply two variants of CNN coupled with it. Experimental results show that it drastically reduces the number of parameters to be optimized, resulting in competitive classification accuracy values in only a fraction of the time spent by one-hot encoding representations, thus enabling training in commodity hardware.", "keywords": ["Character Level Convolutional Networks", "Text Classification", "Word Compressing"]}, "meta": {"decision": "Reject", "comment": "meta score: 4\n\nThe paper has been extensively edited during the review process - the edits are so extensive that I think the paper requires a re-review, which is not possible for ICLR 2018\n\nPros:\n - potentially interesting and novel approach to prefix encoding for character level CNN text classification\n - some experimental comparisons\nCons:\n - lacks good comparison with the state-of-the-art, which makes it difficult to determine conclusions\n - writing style lacks clarity.\n\nI would recommend that the authors continue to improve the paper and submit it to a later conference.\n"}, "review": {"rkst5E4Jz": {"type": "review", "replyto": "SkYXvCR6W", "review": "The paper proposed to encode text into a binary matrix by using a compressing code for each word in each matrix row. The idea is interesting, and overall introduction is clear.\n\nHowever, the work lacks justification for this particular way of encoding, and no comparison for any other encoding mechanism is provided except for the one-hot encoding used in Zhang & LeCun 2015. The results using this particular encoding are not better than any previous work.\n\nThe network architecture seems to be arbitrary and unusual. It was designed with 4 convolutional layers stacked together for the first layer, while a common choice is to just make it one convolutional layer with 4 times the output channels. The depth of the network is only 5, even with many layers listed in table 5.\n\nIt uses 1-D convolution across the word dimension (inferred from the feature size in table 5), which means the convolutional layers learn intra-word features for the entire text but not any character-level features. This does not seem to be reasonable.\n\nOverall, the lack of comparisons and the questionable choices for the networks render this work lacking significance to be published in ICLR 2018.", "title": "Main idea lacks significance", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "BJ1fw5Flf": {"type": "review", "replyto": "SkYXvCR6W", "review": "The manuscript proposed to use prefix codes to compress the input to a neural network for text classification. It builds upon the work by Zhang & LeCun (2015) where the same tasks are used.\n\n\nThere are several issues with the paper and I cannot recommend acceptance of the paper in the current state. \n- It looks like it is not finished.\n- the datasets are not described properly. \n- It is not clear to me where the baseline results come from.\n They do not match up to the Zhang paper (I have tried to find the matching accuracies there).\n- It is not clear to me what the baselines actually are or how I can found more info on those.\n- the results are not remarkable. \n\nBecause of this, the paper needs to be updated and cleaned up before it can be properly reviewed. \n\nOn top of this, I do not enjoy the style the paper is written in, the language is convoluted. \nFor example: \u201cThe effort to use Neural Convolution Networks for text classification tasks is justified by the possibility of appropriating tools from the recent developments of techniques, libraries and hardware used especially in the image classification \u201c\nI do not know which message the paper tries to get across here. \nAs a reviewer my impression (which is subjective) is that the authors used difficult language to make the manuscript look more impressive.\nThe acknowledgements should not be included here either. \n\n", "title": "The manuscript needs updating. The current state is not good enough.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rkB5-TcgG": {"type": "review", "replyto": "SkYXvCR6W", "review": "This paper proposes a new character encoding scheme for use with character-convolutional language models. This is a poor quality paper, is unclear in the results (what metric is even reported in Table 6), and has little significance (though this may highlight the opportunity to revisit the encoding scheme for characters).", "title": "Review", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SyLE3TzXz": {"type": "rebuttal", "replyto": "BJ1fw5Flf", "comment": "  Thank you very much for your time and constructive comments. We have addressed the issues pointed out in your remarks and tried to make more evident the contributions of our work. We made important improvements on the quality of the text, the description of our proposal and presentation of the experimental results.\n    \n    In this new version, we provide a better description and justification of our proposal. In particular, we discuss why we took some decisions regarding the encoding. Similarly, we now include an additional neural network architecture in the comparative experiments. For completeness, we included a more detailed description of datasets and base models involved in the experiments.\n    \n    Our main concern with this paper is show that a more compact encoding procedure could reduce the computational footprint of using words codified character-by-character in text classification.  Character-based text classification allows to handle less curated texts or texts in languages that have a lot of declensions, and with poor or none a priori pre-trained language modules (ala word2vec). \n    \n    In terms of classification performance, we have matched the results of Zhang & LeCun (2015) which represent the state of the art in this area and the departing point of this work. It should be noted that like them, we also match traditional methods. In this regard, in order to improve the readability of the paper we changed the comparison metric to accuracy, different of Zhang & LeCun (2015) that report error loss ($(1-accuracy)\\times100$).\n    \n    On the other hand, in terms of computational footprint, our approach is much faster than Zhang & LeCun (2015). We find that is is a relevant result as it makes the approach suitable for extended use. We are providing along with our paper the supporting code. We expect that with the collaboration of the community a streamlined implementation can be obtained and even better times will be reached.\n\n    To our knowledge, our approach is the first who try to codify words into vectors using only characters composition in a sparse way. We are aware that there is room for improvement. In the process of updating the paper we included another neural network with positive results. Nevertheless, this direction should be properly explored in the future. In this paper we focused mainly on the comparison with the previous results by Zhang & Lecun (2015) but we are confident that other architectures will yield better results. \n    \n    Our main line of research is educational data mining where it is crucial to be able to handle texts produced by students, with equations, orthographic errors and not-so-formal language. In this scenario, we have a lot of interest in building better and faster solutions build upon the character-based approach originally put forward by Zhang & Lecun (2015).\n    \n    We appreciate that you take a moment an revise again that paper under the light of these comments and modifications.\n    \n    Regarding the writing style, we really apologize. Sometimes is difficult to express yourself in a foreign language. We did your best in this updated version to not give you these impressions. ", "title": "We did a updated version addressing your key recommendations"}, "Hk2cEo9QG": {"type": "rebuttal", "replyto": "ryxk9FaZM", "comment": "we have not seen your question in time to help you on the reproducibility challenge, but you guess right, we used RELU\n\nWe are glad that you could replicate the main findings using just our instructions, but in this new version, we tried to give more information on how and why we took some decisions, in a way that could be easier to reproduce the same findings.\n    \n We have prepared a Jupyter/IPython notebook that we publish online along with the paper. We did not publish it yet to not infringe the double-blind review policy. \n \nThank you ", "title": "we used RELU"}, "HkRp2TG7z": {"type": "rebuttal", "replyto": "BJ9KbpZMz", "comment": "Thank you for your interest in this approach\n\n    We did a updated version addressing your key recommendations. \n    \n    We are glad that you could replicate the main findings using just our instructions, but in this new version, We tried to give more information on how and why we took some decisions, in a way that could be easier to reproduce the same findings.\n    \n    We have prepared a Jupyter/IPython notebook that we publish online along with the paper. We did not published yet to not infringe the double-blind review policy. \n    \n    We invite you to read it again. We are open to any suggestion to better presents these findings.\n    \n    Thank you.", "title": "We are glad that you could replicate the main findings using just our instructions"}, "BkP93TM7M": {"type": "rebuttal", "replyto": "BJWqQmMGG", "comment": "Thank you for your interest in this approach\n\n    We did a updated version addressing your key recommendations. \n    \n    In this new version, we have tried to give more information on how and why we took some decisions, in a way that could be easier to reproduce the same results.\n    \n    We have a Jupyter/IPython notebook ready with all the experiments that we intend to publish online along with the paper. We did not published yet to not infringe the double-blind review policy. \n    \n    We kindly invite you to read the paper again. We are open to any suggestion to better presents these findings.\n    \n    Thank you.", "title": "Thank you for your interest in this approach"}, "SyLL2pfQM": {"type": "rebuttal", "replyto": "rkst5E4Jz", "comment": "Thank you for your comments.\n\n    We created an updated version where we did our best to improve the quality of our presentation and express it in a clear way. \n    \n    In this new version, we provide a better description and justification of our proposal. In particular, we discuss why we took some decisions regarding the encoding. Similarly, we now include an additional neural network architecture in the comparative experiments. For completeness, we included a more detailed description of datasets and base models involved in the experiments.\n    \n    Regarding encodings comparison, in the initial stage of our research, we investigated others encodings, mainly Huffman and End Tag Dense Codes- ETDC. We discarded these because we did not found a way to represent words in a distinct way once we concatenate each char code. We made more clear this step in the manuscript and still working in a way to modify ETDC to make it distinct for each word.\n    \n    Our main concern with this paper is show that a more compact encoding procedure could reduce the computational footprint of using words codified character-by-character in text classification.  Character-based text classification allows to handle less curated texts or texts in languages that have a lot of declensions and with poor or none a priori pre-trained language modules (ala word2vec). \n    \n    In terms of classification performance, we have matched the results of Zhang & LeCun (2015) which represent the state of the art in this area and the departing point of this work. It should be noted that like them, we also beat traditional methods. In this regard, in order to improve the readability of the paper, we changed the comparison metric to accuracy, different of Zhang & LeCun (2015) that report error loss ($(1-accuracy)\\times100$).\n    \n    On the other hand, in terms of computational footprint, our approach is much faster than Zhang & LeCun (2015). We find that is is a relevant result as it makes the approach suitable for extended use. We are providing along with our paper the supporting code. We expect that with the collaboration of the community a streamlined implementation can be obtained and even better times will be reached.\n\n    To our knowledge, our approach is the first who try to codify words into vectors using only characters composition in a sparse way. We are aware that there is room for improvement. In the process of updating the paper, we included another neural network with positive results. Nevertheless, this direction should be properly explored in the future. In this paper, we focused mainly on the comparison with the previous results by Zhang & Lecun (2015) but we are confident that other architectures will yield better results. \n    \n    Our main line of research is educational data mining where it is crucial to be able to handle texts produced by students, with equations, orthographic errors and not-so-formal language. In this scenario, we have a lot of interest in building better and faster solutions build upon the character-based approach originally put forward by Zhang & Lecun (2015).\n    \n    We appreciate that you take a moment an revise again that paper under the light of these comments and modifications.", "title": "We did a updated version addressing your key recommendations"}, "rygb3pfQf": {"type": "rebuttal", "replyto": "rkB5-TcgG", "comment": "Thank you for your time for helping us to better express our findings.\n\n    We made important improvements in the quality of the text, the description of our proposal and presentation of the experimental results. \n    \n    In this new version, we provide a better description and justification of our proposal. In particular, we discuss why we took some decisions regarding the encoding. Similarly, we now include an additional neural network architecture in the comparative experiments. For completeness, we included a more detailed description of datasets and base models involved in the experiments.\n    \n    Our main concern with this paper is show that a more compact encoding procedure could reduce the computational footprint of using words codified character-by-character in text classification.  Character-based text classification allows to handle less curated texts or texts in languages that have a lot of declensions and with poor or none a priori pre-trained language modules (ala word2vec). \n    \n    In terms of classification performance, we have matched the results of Zhang & LeCun (2015) which represent the state of the art in this area and the departing point of this work. It should be noted that like them, we also beat traditional methods. In this regard, in order to improve the readability of the paper, we changed the comparison metric to accuracy, different of Zhang & LeCun (2015) that report error loss ($(1-accuracy)\\times100$).\n    \n    On the other hand, in terms of computational footprint, our approach is much faster than Zhang & LeCun (2015). We find that is is a relevant result as it makes the approach suitable for extended use. We are providing along with our paper the supporting code. We expect that with the collaboration of the community a streamlined implementation can be obtained and even better times will be reached.\n\n    To our knowledge, our approach is the first who try to codify words into vectors using only characters composition in a sparse way. We are aware that there is room for improvement. In the process of updating the paper, we included another neural network with positive results. Nevertheless, this direction should be properly explored in the future. In this paper, we focused mainly on the comparison with the previous results by Zhang & Lecun (2015) but we are confident that other architectures will yield better results. \n    \n    Our main line of research is educational data mining where it is crucial to be able to handle texts produced by students, with equations, orthographic errors and not-so-formal language. In this scenario, we have a lot of interest in building better and faster solutions build upon the character-based approach originally put forward by Zhang & Lecun (2015).\n    \n    We appreciate that you take a moment an revise again that paper under the light of these comments and modifications.", "title": "We did a updated version where we did our best to improve the quality of our presentation"}, "HJFJeMf-G": {"type": "rebuttal", "replyto": "rkHO_abWG", "comment": "Hello\n\nThank you for you interest in this approach.\n\nTable 6 results are testing error, or 1- accuracy*100. \n\nTheses results came from Table 4 in Zhang, Zhao, LeCun paper: https://arxiv.org/pdf/1509.01626.pdf .  We just compare their results with ours. \n\n\n\n", "title": "Table 6"}}}