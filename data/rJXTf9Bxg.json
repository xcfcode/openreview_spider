{"paper": {"title": "Conditional Image Synthesis With Auxiliary Classifier GANs", "authors": ["Augustus Odena", "Christopher Olah", "Jonathon Shlens"], "authorids": ["augustusodena@google.com", "colah@google.com", "shlens@google.com"], "summary": "We introduce a special GAN architecture that results in high quality 128x128 ImageNet samples; we introduce 2 new quantitative metrics of sample quality.", "abstract": "Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data.", "keywords": ["Deep learning"]}, "meta": {"decision": "Reject", "comment": "Ratings summary:\n 3: Clear rejection\n 6: Marginally above acceptance threshold\n 6: Marginally above acceptance threshold\n \n Clear easy to read paper focusing on generating higher quality higher resolution (128x128) pixel imagery with GANs. There were broad concerns however across reviewers that the work is lacking in clearly identifiable novelty. The author\u00d5s point to a list of novel elements of the work in their rebuttal. However, the most negative reviewer also has issues with the evaluation metrics used.\n \n Thus, unfortunately, the PCs believe that this work isn't ready to appear at the conference."}, "review": {"S1AP0YC8l": {"type": "rebuttal", "replyto": "r1H_OmC8x", "comment": "Hi, thanks for the response.\n\nMost importantly:\n\nA. This response only addresses claims we made about MS-SSIM, and only some of those claims.\nThe reviewer also made claims about the discriminability metric\nthat we feel have been thoroughly rebutted.\n \nB. In our opinion, using pairwise MS-SSIM to evaluate variability is - at best - the \nthird most important contribution of the paper (see list in global response).\nEven if one were to accept (we do not) the reviewer's claims about MS-SSIM,\nit ought not to affect one's evaluation of the paper that greatly.\n\nHaving said that, we address specific claims made here:\n\n1. > Sorry, I should have clarified that I am referring to Section 3.4 of (Theis et al, 2016) \n> on Evaluation based on Nearest Neighbours. \n> I think one can criticise this method along the same lines as mentioned there.\n\nWe assume that the reviewer is referring to the third paragraph of section 3.4,\nsince the first is an introductory paragraph and the second is about euclidean distance.\nThis paragraph is a criticism of using nearest neighbors in the training set.\nWe are not searching for nearest neighbors (we are choosing random pairs of samples)\nand we are not using the training set.\n\nIn particular, the paragraph says: \"Even when overfitting, most models will not reproduce\nperfect or trivially transformed copies of the training data.\n In this case, no distance metric will find a close match in the training set.\"\n\nThis argument doesn't apply in our case.\n\nThe paragraph also says: \"A model which overfits might still never generate a plausible\nimage or might only be able to generate a small fraction of all plausible images\n (e.g., a model as in Equation 10 where instead of training images we store several\n transformed versions of the training images, or a model which only describes data \nin a lower-dimensional subspace).\"\n\nFirst: our model generates plausible images, so that particular concern is not relevant.\nSecond: it's true that a model that could somehow only produce 10% of all natural images\nwould achieve a high diversity score using our method, but we don't claim that our method\nis a good proxy for the entropy of the generator's distribution over pixel space - we \nclaim that it's a useful measure of the perceptual diversity of outputs. \n\n2. The reviewer seems to make the claim that, since MS-SSIM was developed with the \nintention of being used on pairs of images that are already quite similar, it \nis dangerous to try and use it on pairs of images that are not that similar.\nWe already took several steps in the paper to address this concern.\n\nFigure 4 shows various MS-SSIM values for both training data and samples.\nTo our eye, the values do seem to correspond to perceptual diversity.\n\nWe also restricted pairwise evaluations to within-a-class precisely for this reason.\n\nMoreover, the fact that the MS-SSIM values for samples relate to MS-SSIM classes\nfor training data in the way that they do is itself evidence - in the bayesian sense - \nthat the score is working \"as you'd expect\" in this regime.\n\n3. > For point c. I still think that Section 3.4 of (Theis et al, 2016) is relevant.\n\nAgain, that section claims that using nearest neighbors in the training data \nis only good for ruling out the worst forms of overfitting. \nThis is explicitly what we claim to use it for in the above point c.\nWe also use latent space interpolations, which are not addressed.\n\n\n", "title": "Response to Reviewer 2"}, "B1UyEGPHe": {"type": "rebuttal", "replyto": "S1pqbrY4x", "comment": "Thank you for the review. We have used \">\" for quotes in the below response.\n\nAs mentioned in the global response, we believe that the review misstates several key points in our paper.  We have revised our paper for clarity and we respond to each point in detail below.\n\n======================\nVARIABILITY AND MS-SSIM\n======================\n\n1. \u201cThe authors try to quantitatively evaluate diversity of samples by measuring the average MS-SSIM between randomly selected pairs of points within each class. I think this method is significantly flawed and limited, for reasons mentioned in (Theis et al, 2015, A note on the evaluation\u2026). In its behaviour, MS-SSIM is not that dissimilar from Euclidean distance - although it is nonlinear and is bounded between -1 and 1.\u201d\n\nThe main claim of Theis et al (2015) is that log-likelihood might not correspond to sample quality in a generative model. Furthermore, training a model based on one objective will not guarantee good performance under another objective. We view these points as orthogonal to our evaluation framework. If the reviewer views this differently, we would be interested to hear their perspective. That said, Theis et al (2015) provide an explicit motivation for us to build an evaluation method that is independent of the training objective.\n\n2. In regards to the statement that \u2018MS-SSIM is not that dissimilar from Euclidean distance\u2019, we strongly disagree. We believe that this statement is not supported by the literature.\n\nTo start with, SSIM is a highly nonlinear metric that has been constructed to reflect human perceptual judgements [*]. The original SSIM paper as well as follow up works on MS-SSIM have demonstrated that SSIM-based metrics provide substantially improved quantitative estimates of human perceptual judgements compared to simple Euclidean distance measures (and many other quantitative measures of perceptual similarity) [1, 2, 3; and references therein].\n\nA simple intuition for why SSIM is perceptually superior to Euclidean distance can be gained through looking at examples. The SSIM web page (http://www.cns.nyu.edu/~lcv/ssim/ ) shows that images which are drastically different perceptually can have the same MSE and quite different SSIM scores.\n\n[1] Wang, Zhou, Eero P. Simoncelli, and Alan C. Bovik. \"Multiscale structural similarity for image quality assessment.\"\n[2] Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, \"Image quality assessment: From error visibility to structural similarity\"\n[3] Kede Ma, Qingbo Wu, Zhou Wang, Zhengfang Duanmu, Hongwei Yong, Hongliang Li, and Lei Zhang. Group mad competition - a new methodology to compare objective image quality models.\n[*] Note that our MS-SSIM implementation ranges from [0, 1] and not from [-1, 1]. All values on our graphs should be interpreted accordingly.\n \n3. \u201cEvaluating diversity/entropy of samples in high dimensions is very hard, especially if the distributions involved are non-trivial for example concentrated around manifolds. Consider for example a generative model which randomly samples just two images. Assuming that the MSSSIM between these two images is -1, this generative model can easily achieve an average MSSSIM score of 0, implying a conclusion that this model has more diversity than the training data itself.\u201d\n\nWe claim that this example is seriously flawed, for the following reasons:\n\na. The range of MS-SSIM is [0, 1] (see above), so the average would be 0.5, which corresponds to half as much diversity as in the least diverse ImageNet class. \nb. This type of memorization would manifest as high variance of the mean MS-SSIM, but we report lower variance for samples than for training data (In other words, we explicitly account for this in the metric itself). One could also compute higher moments or look at a histogram of pairwise MS-SSIM values, etc.\nc. We have also ruled out memorization explicitly by examining both latent space interpolations and nearest neighbors (both by L1 and MS-SSIM).\nd. We have provided 10,000 sample images, which do not show this type of memorization.\ne. It\u2019s not even clear how well other existing methods of measuring diversity would perform in this case.\n\n4. The reviewer suggests that in an ideal world we would compute the entropy of the generator distribution. We argue that -- even if this were feasible -- we would still want to use perceptually calibrated metrics such as MS-SSIM. Namely, we might want to ignore variability due to contrast or pixel intensity in favor of diversity of image content.\n\n5. \u201cConversely, SSIM is designed not to be sensitive to contrast and average pixel intensity, so if a model is diverse in this sense, that will be ignored by this measure.\u201d\n\nWe think that this is a good thing (see above).\nThis is a sense in which Euclidean distance measures are very different than MS-SSIM, which directly contradicts the reviewer\u2019s earlier point.\n\n=============\nDISCRIMINABILITY\n===============\n\n1. \u201cDiscriminability: Figure 3 doesn\u2019t have legends so it is a bit hard to understand what is going on. Furthermore, my understanding is that when evaluating discriminability the authors downsample and then bicubically upsample the image, which is much more like a blurring, very different from retraining all the models to work on low resolution in the first place.\u201d\n\nWe feel that there was a large misunderstanding about our methods, so we present a quick summary of what was done to measure discriminability:\n\nOur goal in this analysis was to measure how much of the output resolution is actually used by the image synthesis model. In other words, does a 128x128 model just produce 32x32 images that are naively resized to 128x128? This is the goal of the \u2018blurring\u2019 analysis and a question that has not been addressed in the literature. If a sample were a naive resizing, a blurring would not reduce its discriminability.\n\nFor each image analyzed, and for each resolution in [16, 32, 64, 128, 256], we iteratively resized the sample from its original size (using bilinear interpolation) to the resolution in question. We then passed the image to a pretrained Inception model, which resizes all inputs to 299x299 before processing. We took note of whether Inception correctly classified the input, and then we reported these results in the lower left hand corner of figure 3. We have revised the manuscript to better describe these methods.\n\n2. \u201cIt is found - not surprisingly - that higher resolution improves discriminability (because more information is present).\u201d\n\nIf the reviewer means that simply increasing the resolution should result in higher discriminability: We did explicitly test for this by upsampling images to confirm that simply upsampling would not increase discriminability. See Figure 3 and Section 4.1.\n\nIf the reviewer means that it is unsurprising that the model is successfully making use of its available output resolution: Naive resizing is not an idle concern. We tested another model that actually failed to meaningfully increase discriminability score from 64x64 to 128x128.\n\n3.\"[the authors do not retrain] all the models to work on low resolution in the first place\". \n\nWe did retrain models at a lower output resolution. We found that samples from these models are about half as discriminable at the 128x128 resolution as samples from the 128x128 model. The results of this experiment correspond to the blue curve in the lower left of Figure 3. This procedure was also described in section 4.1.\n", "title": "Response to Reviewer 2"}, "S1KOffPHl": {"type": "rebuttal", "replyto": "H1qHMAWVe", "comment": "Thank you for the review. We have used \">\" for quotes in the below response.\n\n> - Diversity metric is of limited use for training non class-conditional GANs.\n\nWe note that the MS-SSIM metric may be used for measuring \u201ccollapsing behavior\u201d in GANs regardless of whether the GAN is class-conditional.\n\nIn regards to employing MS-SSIM for measuring diversity *across* classes, we would like to clarify that this should be doable; it would just require re-fitting the MS-SSIM parameters.\n\n> - No experimental comparison of AC-GAN to other class-conditional models.\n\nWe have not been able make the original conditional GAN [1] train successfully on the full ImageNet dataset - we claim that a primary contribution of this paper is to provide the first image synthesis model to do so.\n\nFrom a model performance perspective, we would argue that the relevant architectural choice is whether label information is used. We do compare to the most favorable quantitative assessment of a GAN that has used label information [2] and show substantially improved results.\n\n> Section 4.2: MS-SSIM is traditionally defined for grayscale images only. How do you\n> extend  MS-SSIM to color images in your work? Were they computed channel-wise across\n> R,G, and B?\n\nWe calculate the mean across all color channels using an open-source implementation:\nhttps://github.com/tensorflow/models/blob/master/compression/msssim.py#L120\n\n> Section 4.4: It is difficult to tell whether a single AC-GAN was trained for all of CIFAR-10 or \n> one for each group\u2026\n\nTo the best of our knowledge, we are performing an analysis identical to that in [2]. (Note that they also perform the splitting of the samples in order to compute score variance.) We have added information to Table 2 of Appendix A to highlight this.\n\n> * Appendix D: The caption of Figure 9 states that the same number of training steps was taken for each model. From this it seems possible that the models with more classes simply did not converge yet.\n\nOur empirical observation (as some others have seen) is that once mode-collapse has occurred, the GAN can not recover. \n\n[1] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784, 2014\n\n[2] Salimans, Tim, et al. \"Improved techniques for training GANs.\" Advances in Neural Information Processing Systems. 2016.", "title": "Response to Reviewer 3"}, "ryNkfzPHg": {"type": "rebuttal", "replyto": "rJPBH1MEg", "comment": "Thank you for the review. We have used \">\" for quotes in the below response.\n\n> The overall novelty of this approach is somewhat lacking in that previous methods have\n> proposed training a classifier head on the discriminator and the discriminability metric \n> proposed is simply the inception score of [1] except with class information. \n\nThe novelty of our discriminability analysis is mostly that we measure the extent to which the model is making use of its given output resolution. Our work is the first to attempt this measurement. Note that this also allows us to single out classes for which high-frequency information is important (e.g. zebra - see the green dot in Figure 3).\n\n> Why do you think splitting the imagenet training into 100 different models improves \n> performance? Is the issue with the representation of the class?\n\nWe stumbled upon this result when debugging previous GAN models. We do not have an explanation for this behavior beyond our empirical results.\n\nWe also claim that this work highlights that the challenge of ImageNet is mainly the large number of classes.  This result is consistent with [1] in which the image synthesis model is restricted to visually similar classes (i.e. birds). We do not know whether this is an issue of representation or learning, but we think it is a fascinating question and one which we hope to address in future work. We have added some comments to the discussion accordingly.\n\n> But can you explain why you would want the discriminator to also maximize the classification \n> accuracy of generated samples? \n\nThe short answer is that it makes the results better. Our suspicion is that this is due to a sort of feedback loop: more training data for the discriminator makes the discriminator better, which is then reflected in the generator.\n\n[1] Reed et al. Generative Adversarial Text to Image Synthesis (https://arxiv.org/abs/1605.05396)\n", "title": "Response to Reviewer 1"}, "SJE9xfvrl": {"type": "rebuttal", "replyto": "rJXTf9Bxg", "comment": "Thank you for your time and for the reviews. We will respond to the details of each review individually, but we wish to highlight some larger points for all reviewers. \n\n+ Novelty: All of the reviewers highlighted some concerns about novelty. We claim that, apart from the model architecture, there is a lot of novelty spread across both the experimental results and the quantitative metrics. This paper is the first work to:\n\n1. Demonstrate an image synthesis model for all 1000 ImageNet classes at a 128x128 spatial resolution (or any spatial resolution) [Section 3].\n2. Measure how much an image synthesis model actually uses its output resolution [Section 4.1].\n3. Measure variability and 'collapsing' behavior in a GAN with a fast, easy-to-compute metric [Section 4.2].\n4. Highlight that a high number of classes is what makes ImageNet synthesis difficult for GANs and provide an explicit solution [Section 2, Appendix D].\n5. Demonstrate experimentally that GANs that perform well perceptually are not those that memorize a small number of examples [Section 4.3].\n6. Achieve state of the art on the Inception score metric when trained on CIFAR-10 without using any of the tricks from \"Improved Techniques for Training GANs\" [Section 4.4].\n\nWe also summarize our rebuttal to 'AnonReviewer2'. Her/his criticisms were particularly sharp and we believe that they are the result of misunderstandings about several key concepts:\n\n+ Diversity: We disagree with the reviewer's statement that 'MS-SSIM is not that dissimilar from Euclidean distance'. The entire purpose of SSIM/MS-SSIM is to be distinct from Euclidean distance. MS-SSIM is highly nonlinear and has been demonstrated in the literature to be quantitatively superior for predicting human perceptual similarity judgments [1,2,3]. We also object strongly to the toy example given in the review and we do not agree that Theis et al (2015) supports the conclusions drawn by the reviewer about our work.\n\n+ Discriminability: The review suggests that higher resolution samples will necessarily lead to more discriminability. This is not correct. In fact, we test this precisely and we demonstrate the opposite. The review also claims that we did not perform an analysis (retraining AC-GAN to output 64x64 images) that we actually did perform.\n\nWe have revised the manuscript to emphasize these points.\n\n[1] Wang, Zhou, Eero P. Simoncelli, and Alan C. Bovik. \"Multiscale structural similarity for image quality assessment.\"\n[2] Z. Wang, A. C. Bovik, H. R. Sheikh and E. P. Simoncelli, \"Image quality assessment: From error visibility to structural similarity\"\n[3] Kede Ma, Qingbo Wu, Zhou Wang, Zhengfang Duanmu, Hongwei Yong, Hongliang Li, and Lei Zhang. Group mad competition - a new methodology to compare objective image quality models.\n", "title": "Global Response to Reviewers"}, "r1Or7ViXx": {"type": "rebuttal", "replyto": "HkcU0jxml", "comment": "Thanks for the questions and sorry for the slow response!\nIt's difficult to get things done during NIPS.\n\n1. We weren't able to get normal conditional GANs to work for ImageNet \n(The auxiliary classifier seems to actually make training easier),\n so a direct quantitative comparison of that sort is tricky. \nWe would have liked to perform quantitative comparisons with e.g. PixelCNN,\nbut sampling from these autoregressive models is too expensive for that to be feasible.\n\nQualitative comparisons are easier: we have included a comparison of our samples with \nthose from \"Improved Techniques for Training GANs\" in the appendix.\nThe comparison is somewhat limited, since prior papers on image synthesis did\nnot include many samples.\n\n2. The MS-SSIM score as employed here would probably not work 'out-of-the-box' to measure\n inter-class diversity (as opposed to intra-class diversity, which we use it for). \nThe score uses several hyper-parameters (e.g. weight variances across spatial scales)\n that would need to be retuned for that use-case.\nIn this work, we used the default settings for MS-SSIM provided by \"MULTI-SCALE STRUCTURAL SIMILARITY \nFOR IMAGE QUALITY ASSESSMENT\".\n\n3. This is a very sensible suggestion!\nWe actually considered doing it originally and went with L1 because it was more standard.\nWe have included an additional analysis in the revised paper, which we have already submitted to arXiv.\n\nBroadly speaking, the neighbors seem somewhat more structurally similar than when L1 is used,\nbut there is still no real sign of memorization. \nThis is in agreement with the results from the ICLR submission \n\"On the Quantitative Analysis of Decoder-Based Generative Models\",\nwhich finds that GANs do not overfit as measured by train/test log-likelihood differences.\n\n\n", "title": "Answers to pre-review questions"}, "HkcU0jxml": {"type": "review", "replyto": "rJXTf9Bxg", "review": "\n1. Did you try a comparison of AC-GAN to other class-conditional models? Specifically, plotting Inception accuracy vs. MS-SSIM for AC-GAN compared to e.g. conditional GANs would be insightful. A qualitative comparison of AC-GAN samples vs. other models would be helpful as well.\n2. Do you think that the MS-SSIM diversity score is useful for unlabeled datasets, or do you feel its primary use is for class-labeled datasets?\n3. In Figure 7, did you try performing the nearest neighbor search using MS-SSIM instead of L1?This paper introduces a class-conditional GAN as a generative model for images. It introduces two main diagnostic tools for training GANs: one to assess whether a model is making full use of its output resolution and another to measure the diversity of generated samples. Experiments are conducted on the CIFAR-10 and ImageNet datasets.\n\nPros:\n+ The paper is clear and well-written.\n+ Experiments performed in the relatively under-explored 128 x 128 ImageNet setting.\n+ The proposed MS-SSIM diversity metric appears to be a useful tool for detecting convergence issues in class-conditional GAN models.\n\nCons:\n- AC-GAN model itself is of limited novelty relative to other GAN approaches that condition on class.\n- Diversity metric is of limited use for training non class-conditional GANs.\n- No experimental comparison of AC-GAN to other class-conditional models.\n\nTo my knowledge training GANs on large, diverse images such as 128 x 128 ImageNet images is under-explored ([1] contains just a few samples in this setting). Though the model is not very novel and a comparison to other class-conditional models is lacking, I feel the community will find the diagnostic tools and the thorough exploration of the ImageNet-trained model to be of interest.\n\n* Section 4.2: MS-SSIM is traditionally defined for grayscale images only. How do you extend MS-SSIM to color images in your work? Were they computed channel-wise across R,G, and B?\n* Section 4.4: It is difficult to tell whether a single AC-GAN was trained for all of CIFAR-10 or one for each group. If single, why were the samples split into groups for computing Inception Score? And if multiple, the comparison to Salimans et al. is not a direct one. Also it would be helpful to include the real data Inception score as a point of comparison.\n* Appendix D: The caption of Figure 9 states that the same number of training steps was taken for each model. From this it seems possible that the models with more classes simply did not converge yet.\n\n[1] Salimans, Tim, et al. \"Improved techniques for training GANs.\" Advances in Neural Information Processing Systems. 2016.", "title": "Pre-review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1qHMAWVe": {"type": "review", "replyto": "rJXTf9Bxg", "review": "\n1. Did you try a comparison of AC-GAN to other class-conditional models? Specifically, plotting Inception accuracy vs. MS-SSIM for AC-GAN compared to e.g. conditional GANs would be insightful. A qualitative comparison of AC-GAN samples vs. other models would be helpful as well.\n2. Do you think that the MS-SSIM diversity score is useful for unlabeled datasets, or do you feel its primary use is for class-labeled datasets?\n3. In Figure 7, did you try performing the nearest neighbor search using MS-SSIM instead of L1?This paper introduces a class-conditional GAN as a generative model for images. It introduces two main diagnostic tools for training GANs: one to assess whether a model is making full use of its output resolution and another to measure the diversity of generated samples. Experiments are conducted on the CIFAR-10 and ImageNet datasets.\n\nPros:\n+ The paper is clear and well-written.\n+ Experiments performed in the relatively under-explored 128 x 128 ImageNet setting.\n+ The proposed MS-SSIM diversity metric appears to be a useful tool for detecting convergence issues in class-conditional GAN models.\n\nCons:\n- AC-GAN model itself is of limited novelty relative to other GAN approaches that condition on class.\n- Diversity metric is of limited use for training non class-conditional GANs.\n- No experimental comparison of AC-GAN to other class-conditional models.\n\nTo my knowledge training GANs on large, diverse images such as 128 x 128 ImageNet images is under-explored ([1] contains just a few samples in this setting). Though the model is not very novel and a comparison to other class-conditional models is lacking, I feel the community will find the diagnostic tools and the thorough exploration of the ImageNet-trained model to be of interest.\n\n* Section 4.2: MS-SSIM is traditionally defined for grayscale images only. How do you extend MS-SSIM to color images in your work? Were they computed channel-wise across R,G, and B?\n* Section 4.4: It is difficult to tell whether a single AC-GAN was trained for all of CIFAR-10 or one for each group. If single, why were the samples split into groups for computing Inception Score? And if multiple, the comparison to Salimans et al. is not a direct one. Also it would be helpful to include the real data Inception score as a point of comparison.\n* Appendix D: The caption of Figure 9 states that the same number of training steps was taken for each model. From this it seems possible that the models with more classes simply did not converge yet.\n\n[1] Salimans, Tim, et al. \"Improved techniques for training GANs.\" Advances in Neural Information Processing Systems. 2016.", "title": "Pre-review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryJRTAtfg": {"type": "rebuttal", "replyto": "Sy9OStwGx", "comment": "Thanks for the comments.\n\nYou're correct about the typo.\n\nWe'll include the CIFAR-10 hyperparameters in the appendix when we make a revision.\nIn the meantime, I can tell you that they are as follows:\n\nThe discriminator is the same as that from the ImageNet experiments.\nThe generator is composed of one linear projection layer and 3 layers of fractionally strided convolution.\nThe layers have feature map counts of 384, 192, 96, and 3. \n\nThe hyperparameters that were searched over were \"generator_learning_rate\", \"discriminator_learning_rate\", and \"discriminator_activation_noise_standard_deviation\", for which we used values of [0, 0.0001, 0.0002], [0, 0.0001, 0.0002] and [0, 0.1, 0.2] respectively.", "title": "Re: Typo in Appendix A"}, "Sy9OStwGx": {"type": "rebuttal", "replyto": "rJXTf9Bxg", "comment": "In the hyperparameter tables it's shown that D(x) receives 128 \u00d7 3 \u00d7 3 input. I assume this is a typo and the input size should be 128 x 128 x 3.\n\nAlso, is it possible to also append the hyperparameters for CIFAR-10 experiments?\n", "title": "Typo in Appendix A"}}}