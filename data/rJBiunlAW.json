{"paper": {"title": "Training RNNs as Fast as CNNs", "authors": ["Tao Lei", "Yu Zhang", "Yoav Artzi"], "authorids": ["tao@asapp.com", "yzhang87@csail.mit.edu", "yoav@cs.cornell.edu"], "summary": "", "abstract": "Common recurrent neural network architectures scale poorly due to the intrinsic difficulty in parallelizing their state computations. In this work, we propose the Simple Recurrent Unit (SRU) architecture, a recurrent unit that simplifies the computation and exposes more parallelism. In SRU, the majority of computation for each step is independent of the recurrence and can be easily parallelized. SRU is as fast as a convolutional layer and 5-10x faster than an optimized LSTM implementation. We study SRUs on a wide range of applications,  including classification, question answering, language modeling, translation and speech recognition. Our experiments demonstrate the effectiveness of SRU and the trade-off it enables between speed and performance. ", "keywords": ["recurrent neural networks", "natural language processing"]}, "meta": {"decision": "Reject", "comment": "The paper presents Simple Recurrent Unit, which is characterised by the lack of state-to-gates connections as used in conventional recurrent architectures. This allows for efficient implementation, and leads to results competitive with the recurrent baselines, as shown on several benchmarks.\n\nThe submission lacks novelty, as the proposed method is essentially a special case of Quasi-RNN [Bradbury et al.], published at ICLR 2017. The comparison in Appendix A confirms that, as well as similar results of SRU and Quasi-RNN in Figures 4 and 5. Quasi-RNN has already been demonstrated to be amenable to efficient implementation and perform on a par with the recurrent baselines, so this submission doesn\u2019t add much to that."}, "review": {"BJsMKkGgf": {"type": "review", "replyto": "rJBiunlAW", "review": "This work presents the Simple Recurrent Unit architecture which allows more parallelism than the LSTM architecture while maintaining high performance.\n\nSignificance, Quality and clarity:\nThe idea is well motivated: Faster training is important for rapid experimentation, and altering the RNN cell so it can be paralleled makes sense. \nThe idea is well explained and the experiments convince that the new architecture is indeed much faster yet performs very well.\n\nA few constructive comments:\n- The experiment\u2019s tables alternate between \u201ctime\u201d and \u201cspeed\u201d, It will be good to just have one of them.\n- Table 4 has time/epoch yet only time is stated", "title": "Nice idea, tested extensively", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyjjOZ5gM": {"type": "review", "replyto": "rJBiunlAW", "review": "The authors introduce SRU, the Simple Recurrent Unit that can be used as a substitute for LSTM or GRU cells in RNNs. SRU is much more parallel than the standard LSTM or GRU, so it trains much faster: almost as fast as a convolutional layer with properly optimized CUDA code. Authors perform experiments on numerous tasks showing that SRU performs on par with LSTMs, but the baselines for these tasks are a little problematic (see below).\n\nOn the positive side, the paper is very clear and well-written, the SRU is a superbly elegant architecture with a fair bit of originality in its structure, and the results show that it could be a significant contribution to the field as it can probably replace LSTMs in most cases but yield fast training. On the negative side, the authors present the results without fully referencing and acknowledging state-of-the-art. Some of this has been pointed out in the comments below already. As another example: Table 5 that presents results for English-German WMT translation only compares to OpenNMT setups with maximum BLEU about 21. But already a long time ago Wu et. al. presented LSTMs reaching 25 BLEU and current SOTA is above 28 with training time much faster than those early models (https://arxiv.org/abs/1706.03762). While the latest are non-RNN architectures, a table like Table 5 should include them too, for a fair presentation. In conclusion: the authors seem to avoid discussing the problem that current non-RNN architectures  could be both faster and yield better results on some of the studied problems. That's bad presentation of related work and should be improved in the next versions (at which point this reviewer is willing to revise the score). But in all cases, this is a significant contribution to deep learning and deserves acceptance.\n\nUpdate: the revised version of the paper addresses all my concerns and the comments show new evidence of potential applications, so I'm increasing my score.", "title": "Very useful RNN cell with ok results but over-hyped presentation.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HyMadv_bz": {"type": "review", "replyto": "rJBiunlAW", "review": "The authors propose to drop the recurrent state-to-gates connections from RNNs to speed up the model. The recurrent connections however are core to an RNN. Without them, the RNN defaults simply to a CNN with gated incremental pooling. This results in a somewhat unfortunate naming (simple *recurrent* unit), but most importantly makes a comparison with autoregressive sequence CNNs [ Bytenet (Kalchbrenner et al 2016), Conv Seq2Seq (Dauphin et al, 2017) ] crucial in order to show that gated incremental pooling is beneficial over a simple CNN architecture baseline. \n\nIn essence, the paper shows that autoregressive CNNs with gated incremental pooling perform comparably to RNNs on a number of tasks while being faster to compute. Since it is already extensively known that autoregressive CNNs and attentional models can achieve this, the *CNN* part of the paper cannot be counted as a novel contribution. What is left is the gated incremental pooling operation; but to show that this operation is beneficial when added to autoregressive CNNs, a thorough comparison with an autoregressive CNN baseline is necessary.\n\nPros:\n- Fairly well presented\n- Wide range of experiments, despite underwhelming absolute results\n\nCons:\n- Quasi-RNNs are almost identical and already have results on small-scale tasks.\n- Slightly unfortunate naming that does not account for autoregressive CNNs\n- Lack of comparison with autoregressive CNN baselines, which signals a major conceptual error in the paper.\n- I would suggest to focus on a small set of tasks and show that the model achieves very good or SOTA performance on them, instead of focussing on many tasks with just relative improvements over the RNN baseline.\n\nI recommend showing exhaustively and experimentally that gated incremental pooling can be helpful for autoregressive CNNs on sequence tasks (MT, LM and ASR). I will adjust my score accordingly if the experiments are presented.\n\n", "title": "Low novelty and lacks comparison with obvious baselines", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SyHeKNHmG": {"type": "rebuttal", "replyto": "BJsMKkGgf", "comment": "The latest revision contains fixes to the tables and unifies the measurements used. Thanks for the suggestion", "title": "Tables clarified"}, "SyT4TchzG": {"type": "rebuttal", "replyto": "rJBiunlAW", "comment": "We updated the paper to include recent state-of-the-art results for the QA and translation tasks to avoid confusion about how the results should be interpreted. We thank AnonReviewer3 for suggesting this. ", "title": "Revision with updated result tables"}, "Bkavi5nGM": {"type": "rebuttal", "replyto": "rycUihjGz", "comment": "Hi,\n\nSorry for the delayed revision. The state-of-the-art results have been included in the tables for both machine translation and reading comprehension tasks. We hope the results are now better presented. \n\nPlease let use know if other related work should be included. We are happy to address additional comments.\n\nAlso, we didn't mean that \"Transformer may not be needed with SRUs\". As discussed in the introduction of the Transformer paper, RNN is discarded in Transformer architecture due to the difficulty to parallelize recurrent computation. Thus, it is perhaps possible to \"achieve the best of both worlds\" by incorporating SRU into Transformer (e.g. substituting the FFN sub-unit). \n\n", "title": "SOTA included"}, "ryHvm3qZG": {"type": "rebuttal", "replyto": "BJsMKkGgf", "comment": "Thank you for the comments and feedback. \n\nWe agree that having both \u201ctime\u201d and \u201cspeed\u201d in the tables are confusing. \u201cTime/epoch\u201d in Table 4 is misleading. We will use \u201cTime per epoch\u201d or simply \u201cTime\u201d instead. \n\nWe will address your feedback in the next version. Thanks!\n", "title": "Review response"}, "BJ_3Gnc-M": {"type": "rebuttal", "replyto": "SyjjOZ5gM", "comment": "Thank you for the comments and feedback.\n\n== Paper revision ==\nWe will include missing SOTA results and related work for translation as pointed by R3, as we already included for language modeling and speech. We will update the table in the next version.\n\n== Clarification on our experiments ==\nThe goal of our experiments is not to outperform previous SOTA. Instead, the experiments were designed to study SRU\u2019s effectiveness on a broad set of realistic applications via fair comparison. Therefore, we emphasized using existing open source implementations for MT and QA. Different implementations (network architectures, data processing etc.) have non-trivial impact on the final numbers. To the best of our effort, we aimed to avoid this influencing our experiments. Therefore, in the current version, Tables 1, 3, and 5 only compare the results of using LSTM / SRU / Conv2d as building blocks in existing models such DrQA and OpenNMT. We definitely agree that including SOTA models in these tables will improve our presentation. Thank you for the suggestion.\n\n== Non-RNN architectures ==\nThank you for the comment. We will include discussions of non-RNN architectures. Our contribution is orthogonal to recent architectures, such as Transformer (https://arxiv.org/abs/1706.03762), which is a novel combination of multi-head attention and feed-forward networks. Part of the motivation behind the Transformer architecture is the computational bottleneck of recurrent architectures. With SRU this is not longer the case. In fact, we observe in the translation model that only 4 minutes are spent per SRU layer, and 96 minutes are spent in the attention+softmax computation. An interesting direction for future work is combining the SRU and Transformer architectures to gain the benefits of both. While this is an important problem, it is beyond the scope of our experiments. ", "title": "Review response"}, "BkTOG25bf": {"type": "rebuttal", "replyto": "HyMadv_bz", "comment": "Thank you for the comments and feedback. We respond to the concerns and questions raised in three section. \n\n== Recurrent or convolution ==\nWe wish to certain aspects pertaining to the distinction between recurrent and convolution architectures as we use in the paper:\n\n(1) SRU only applies simple matrix multiplications (Wx_t) for each x_t. This is not a typical convolution operation that is applied over k consecutive tokens. While matrix multiplication can be considered a convolution operation of k=1, this entails that feed-forward networks (FFN) are also a convolutional network. More important, with k=1 there is no convolution over the words, which is the key aim of CNNs for text processing, for example to reason about n-gram patterns. Therefore, while notationaly correct, we consider the k=1 case to empty the term convolution from the meaning it is intended to convey, and do not use it in this way in the paper. That said, we discuss the relationship of these two types of computations in Appendix A, and will be happy to clarify it further in the body of the paper. \n\n(2) This being said, the effectiveness of SRU comes from the recurrent computation of its internal state c[t] (rather than applying conv operations). This internal state computation (referred to in the review as gated incremental pooling) is commonly used as the key component in gated RNN variants, including LSTM, GRU, RAN, MGU, etc. \n\n(3) Beyond the choice of terms, and even if we were to consider SRU as a special type of CNN (with k=1), to the best of our knowledge, our study is the first to demonstrate that k=1 suffices to work effectively across a range of NLP and speech tasks. This emphasis on efficiency goes beyond prior work (e.g. Bytenet, ConvS2S and Quasi-RNN), where conv operations of k=3,4,etc are used throughout the experiments. This allows us to simplify architecture tuning and significantly speeds up the network, which is the main focus of this work. As shown in Figure 2, SRU operates faster than a single conv operation of k=3.\n\n(4) Quasi-RNN, T-RNN and T-LSTM (https://arxiv.org/pdf/1602.02218.pdf) have also used \u201cRNN\u201d in naming, despite defaulting to CNN with gated incremental pooling. Broadly speaking, we consider any unit that successively updates state c[t] based on current input x[t] and the previous vector c[t-1] (as a function c[t]=f(x[t], c[t-1])) as a recurrent unit. We will clarify this better in the paper. \n\n== Quasi-RNN and scale of tasks ==\nWe discuss the comparison to Quasi-RNN in Appendix A, and emphasize the critical differences. In our experiments, the training time of a single run on machine translation takes about 2 days, and 4 days on speech on a Titan X GPU.\n\n== Wide experiments vs deep experiments ==\nOur experiments are aimed to study SRU\u2019s effectiveness on a broad set of realistic applications via fair comparison. We discuss this more in our response to Reviewer 3. \n\nOur work focuses on practical simplifications, optimizations, and the applicability of SRU to a wide range of realistic tasks. Although we do not perform an exhaustive hyper-parameter / architecture tuning on each task given space and time constraints, we do see an improvement over deep CNNs on speech recognition. Similar results have been reported in prior work such as RCNN (Lei et al; 15,16), KNN (Lei et al; 17) and Quasi-RNN (Bradbury et al; 17), demonstrating that gated pooling is helpful for CNN-type models on tasks such as classification, retrieval, LM etc.\n", "title": "Review response"}, "SyAqQBqyz": {"type": "rebuttal", "replyto": "BJABIzckG", "comment": "Thank you! We will update it.", "title": "Results on Switchboard"}, "H10L7B91G": {"type": "rebuttal", "replyto": "r1XmNCKkz", "comment": "In general we found that increasing depth is more helpful than increasing width as long as the width is in a reasonable size. I think this is because we drop \"the dependency\" between \"h\" and this context needs to be recovered by adding more layers. But since SWB training takes about 4 days, we didn't try all the configuration. That's why we didn't draw a conclusion on depth vs. width. ", "title": "Depth vs. width"}, "HkeDMwt1f": {"type": "rebuttal", "replyto": "r1Yz8IYkG", "comment": "Thanks for your comments.\n\nSorry for the confusing, we didn't use RNN-LM here (only N-gram). So the number we should compare with is 10.0 in Table 8. I think JHU recently have better number using the same language model with lattice-free MMI training. We will try this new loss later. But similar to RNN-LM, this is orthogonal to this paper, we are trying to compare with LSTM only for acoustic modeling.\n\nWe haven't try it on 2000hrs. (1) To my understanding, there still lots of institute use 300hrs setup especially at school. If you check last year ICASSP, there are still many paper use 300hrs set, e.g. http://danielpovey.com/files/2017_spl_tdnnlstm.pdf. (2) In my experiences, 20000hrs vs. 300hrs do make a difference, especially for end-to-end system. But 2000hrs set and 300hrs usually don't have significant difference in term of testing the trend of the model quality (especially for HMM-NN hybrid system, model A > model B for 300hrs usually also hold for the full fisher set). Also, 300hrs usually take 4 days on a single GPU which is a reasonable setup for reproduce results.\n", "title": "Results on Switchboard"}, "BJR39j4yM": {"type": "rebuttal", "replyto": "B1HgFDfJf", "comment": "Thank you for the comment. \n\nThe identity activation (use_tanh=0) and non-zero highway bias are applied only on language modeling following a few of recent papers such as \n  - language modeling via gated convolutional network: https://arxiv.org/pdf/1612.08083.pdf\n  - recurrent highway network: https://arxiv.org/abs/1607.03474\n\nWe expect the model to perform better on other tasks as well by initializing a non-zero highway bias, since it can help to balance gradient propagation and model complexity (non-linearity) from layer stacking. This is recommended in the original highway network paper (https://arxiv.org/abs/1505.00387). However, we choose to use zero highway bias on other tasks for simplicity. \n\nRegarding the choice of activation function:\n  - this could be an empirical question since the best activation varies across tasks / datasets (Appendix A)\n  - identity already works since the pre-activation state (i.e. c[t]) readily encapsulates sequence similarity computation. see the discussed related work (Lei et al 2017; section 2.1 & 2.2) https://arxiv.org/pdf/1705.09037.pdf\n\nThank you again for bringing up the questions.\n  ", "title": "activation and highway bias"}}}