{"paper": {"title": "Generating Interpretable Images with Controllable Structure", "authors": ["Scott Reed", "A\u00e4ron van den Oord", "Nal Kalchbrenner", "Victor Bapst", "Matt Botvinick", "Nando de Freitas"], "authorids": ["reedscot@google.com", "avdnoord@google.com", "nalk@google.com", "vbapst@google.com", "botvinick@google.com", "nandodefreitas@google.com"], "summary": "Autoregressive text-to-image synthesis with controllable spatial structure.", "abstract": "We demonstrate improved text-to-image synthesis with controllable object locations using an extension of Pixel Convolutional Neural Networks (PixelCNN). In addition to conditioning on text, we show how the model can generate images conditioned on part keypoints and segmentation masks. The character-level text encoder and image generation network are jointly trained end-to-end via maximum likelihood. We establish quantitative baselines in terms of text and structure-conditional pixel log-likelihood for three data sets: Caltech-UCSD Birds (CUB), MPII Human Pose (MHP), and Common Objects in Context (MS-COCO).", "keywords": ["Deep learning", "Computer vision", "Multi-modal learning", "Natural language processing"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "The paper extends PixelCNN to do text and location conditional image generation. The reviewers praise the diversity of the generated samples, which seems like the strongest result of the paper. On the other hand, they are concerned with their low resolution. The authors made an effort of showing a few high-resolution samples in the rebuttal, which indeed look better. Two reviewers mention that the work with respect to PixelCNN is very incremental, and the AC agrees. Overall, this paper is very borderline. While all reviewers became slightly more positive, none was particularly swayed. The paper will make a nice workshop contribution."}, "review": {"HkJOASnLl": {"type": "rebuttal", "replyto": "Hyvw0L9el", "comment": "We thank all reviewers for their detailed feedback, and note that all reviewers recommend the paper for acceptance. Based on reviewer feedback about image resolution, we trained a 64x64 and 128x128 version of the model on the CUB dataset, results of which can be seen at sites.google.com/view/iclr2017figures. These and additional higher resolution results will be added to the revised paper. Since low-resolution was one of the main drawbacks to the paper according to the reviews, we hope that this can be reflected in an improved score.\n\nWe posted an updated version of the paper to OpenReview with an important correction to the caption of table 1: likelihoods are in *nats* per dim, not bits.\n\nBelow we respond to each review individually.\n\nAR1:\n\nThe time required for sampling is the main constraint on generating higher-resolution samples. However, we have been able to train some higher-resolution models in time for the rebuttal (see sites.google.com/view/iclr2017figures for some results). We agree that adding many more examples for comparison would help; these will be added in the upcoming version.  Please see the reply to AnonReviewer 3 for more precise details on timing and the experimental setup.\n\nPlease see other replies regarding comparison of GANs to pixelCNNs. In short, there are trade-offs between these two. We accentuate the trade-offs in the paper, in the hope that researchers will then know what are the key problems of each approach, and focus on developing solutions to those problems. A quantitative comparison is problematic because GANs don\u2019t provide us with likelihoods. We can however include more samples. To this extent we will add more high-resolution samples, including the ones already provided via the link above.\n\nAR2:\n\nWe appreciate your suggestion of adding more results to the appendix for the final version, or even better create a website where users can explore the generated images by both approaches. The figures in the paper provide a reasonable depiction of the trade-offs between existing GANs and pixelCNNs, but we agree adding more comparisons will help.\n\nAR3:\n\nMatching GANs:\nIn the paper we demonstrate that autoregressive models can do text- and location-conditional image generation; although as the reviewer points out, the resolution is much lower so \u201cmatch\u201d is not the right word; GANs and auto-regressive models have complementary strengths and weaknesses. We are happy to add a more thorough discussion of these issues earlier in the paper, rather than at the end. Figure 9 queries were from a figure of positive results in the paper to which we compare - so presumably favourable to GAN - but we agree that many more comparisons are needed to study the different types of mistakes each method makes.\n\nReplaying training data:\nOne way to check for this is to compare likelihoods for the training set and held-out sets of data. In our case we did not observe significant overfitting, so copying seems unlikely. We also observe significant diversity of samples even with fixed text and structure constraints. However, as noted in the GAN paper to which we compare, even if the model had largely memorized the (text,location,image) training data, it could still produce novel images by conditioning on novel combinations of (text,location), or in general the combinatorial space of all its conditioning variables.\n\nMore implementation details:\nCurrently the paper says \u201cWe used RMSprop with a learning rate schedule starting at 1e-4 and decaying to 1e-5, trained for 200k steps with batch size of 128\u201d. Additional details: The number of epochs varies by dataset - more for CUB because it is smaller, fewer for MS-COCO. Training took about 4 days and sampling at 32x32 resolution took about 2 minutes per image with batch size of 30. Sampling 64x64 took about 16 minutes per image, and 128x128 took about 2 hours. (However, note that sampling time is highly implementation dependent, and we used only the most naive approach in this paper).\n\nHigh-resolution comparisons:\nWe will add further hi-res comparisons in the revised paper.\n\nAutoregressive approach:\nI (first author) also have a bias toward GANs, having written several papers using them. However, I also think autoregressive approaches have complementary benefits compared to GANs - stable, easy to train, do not overfit to a few modes, best available image density estimators, etc - and are worth further developing. Also, autoregressive and adversarial approaches could be naturally combined; e.g. as likelihood and posterior models in PPGNs (https://arxiv.org/abs/1612.00005).\n\nYou also point out that a coarse-to-fine ordering of pixels makes more sense than raster order. Indeed, this is how we generated 64x64 and 128x128 samples for this rebuttal. This allows the PixelCNN to see into the \u201cfuture\u201d - i.e. pixels below and to the right, but at a coarser scale. We can highlight this possibility more in the revised paper.\n\nIn summary, here are the contributions of our paper that move it beyond the \u201cincremental\u201d category and worth appearing at ICLR:\n* Demonstrating that autoregressive models are capable of text- and structure-conditional image generation, including a new kind of spatial structure (COCO segmentations).\n* Highest-resolution samples yet from a PixelCNN model (128x128) making use of a partially coarse-to-fine pixel ordering. Training is still fast and efficient because we can train the fully convolutional architecture on e.g. 32x32 patches, and still generate 128x128 at test time.\n* Establishing several quantitative baselines for text-to-image synthesis, which are currently lacking.", "title": "Rebuttal, addition of 64x64 and 128x128 samples"}, "BJ2TmJ-Ng": {"type": "rebuttal", "replyto": "Hki-wS9Qg", "comment": "The argument of this paper is not that conditional PixelCNN should always be used instead of conditional GAN. The models have complementary strengths - for example GANs have the advantage of fast sampling even at high resolution, while PixelCNN has the advantage of stable and efficient maximum-likelihood training, and achieves state-of-the-art test likelihood on ImageNet. Depending on the nature of the task, one method or the other may be more appropriate.\n\nSampling from PixelCNN in current practice requires O(N) network evaluations for N pixels, which is the main drawback compared to GANs, which only require 1. Caching tricks can speed up these autoregressive models significantly, but each pixel is sill generated sequentially. Training computation and memory requirements are no more than for GANs, only requiring a single forward and backward pass through the network to compute gradients. Scaling autoregressive models to higher resolutions is definitely on the research frontier, but in this paper we show that the same approach of text- and location-controllable image synthesis that works well in GANs, also works very well for PixelCNNs, which have different (complementary) strengths and weaknesses.", "title": "Resolution"}, "r1rVeJZEx": {"type": "rebuttal", "replyto": "SkJvnQ87e", "comment": "The artifacts would still be apparent, but it is possible that training the GAN originally at 32x32 would result in samples with fewer artifacts. Using the code at https://github.com/reedscot/nips2016, we also trained a 64x64 model on MHP, and did not observe significantly cleaner samples.\n\nPixelCNN can condition on lower-resolution images to produce similar quality samples at higher resolutions. For example, 64x64 PixelCNN can be trained conditioned on 32x32 images (e.g. see fix. 7 in https://arxiv.org/pdf/1606.05328v2.pdf). We will include this result for text- and structure-conditional models in the next revision.\n\nPixelCNN seems to match well the bird appearance and produce appropriate backgrounds and textures for MS-COCO images, but indeed it often fails to capture specific colors of specific parts. For example in fig. 7 \u201cA man in a white shirt and black shorts holding a tennis racket and ball about to hit it on a tennis court.\u201d, the samples show a man in shorts swinging a racket in a court-like setting, but the colors of his clothes do not match the text.\n\nIf conditional GAN has an advantage here, it may be because the discriminator must learn to \u201ccheck\u201d the generated image to see whether it matches all of the constraints, including object positions and text, and these objectives can be weighed against realism. The generator must learn to produce samples that pass these checks. In current practice PixelCNN models the conditional density of images, so its objective does not explicitly require that conditioning variables be recoverable from the samples. However, this could be an interesting direction for future work. \n\nFigure 9 - Thanks for pointing this out; this will be fixed.\n", "title": "comparison with GANs"}, "Hki-wS9Qg": {"type": "review", "replyto": "Hyvw0L9el", "review": "I have basically the same questions as Reviewer2. This paper is all about pixelCNN vs GAN -- why the different resolutions? What are the actual train times, test times, and memory requirements for the two competing methods?\"First, it allows us to assess whether auto-regressive models are able to match the GAN results of Reed et al. (2016a).\" Does it, though? Because the resolution is so bad. And resolution limitations aren't addressed until the second-to-last paragraph of the paper. And Figure 9 only shows 3 results (picked randomly? Picked to be favorable to PixelCNN?). That hardly seems conclusive.\n\nThe segmentation masks and keypoints are pretty strong input constraints. It's hard to tell how much coherent object and scene detail is emerging because the resolution is so low. For example, the cows in figure 5 look like color blobs, basically. Any color blob that follows an exact cow segmentation mask will look cow-like.\n\nThe amount of variation is impressive, though.\n\nHow can we assess how much the model is \"replaying\" training data? Figure 8 tries to get at this, but I wonder how much each of the \"red birds\", for instance, is mostly copied from a particular training example.\n\nI'm unsatisfied with the answers to the pre-review questions. You didn't answer my questions. The paper would benefit from concrete numbers on training time / epochs and testing time. You didn't say why you can't make high resolution comparisons. Yes, it's slower at test time. Is it prohibitively slow? Or is it slightly inconvenient? There really aren't that many comparisons in the paper, anyway, so it if takes an hour to generate a result that doesn't seem prohibitive. \n\nTo be clear about my biases: I don't think PixelCNN is \"the right way\" to do deep image generation. Texture synthesis methods used these causal neighborhoods with some success, but only because there wasn't a clear way to do the optimization more globally (Kwatra et al, Texture Optimization for Example-based Synthesis being one of the first alternatives). It seems simply incorrect to make hard decisions about what pixel values should be in one part of the image before synthesizing another part of the image (Another texture synthesis strategy to help fight back against this strict causality was coarse-to-fine synthesis. And I do see some deep image synthesis methods exploring that). It seems much more correct to have a deeper network and let all output pixels be conditioned on all other pixels (this conditioning will implicitly emerge at intermediate parts of the network). That said, I could be totally wrong, and the advantages stated in the paper could outweigh the disadvantages. But this paper doesn't feel very honest about the disadvantages.\n\nOverall, I think the results are somewhat tantalizing, especially the ability to generate diverse outputs. But the resolution is extremely low, especially compared to the richness of the inputs. The network gets a lot of hand holding from rich inputs (it does at least learn to obey them). \n\nThe deep image synthesis literature is moving very quickly. The field needs to move on from \"proof of concept\" papers (the first to show a particular result is possible) to more thorough comparisons. This paper has an opportunity to be a more in depth comparison, but it's not very deep in that regard. There isn't really an apples to apples comparison between PixelCNN and GAN nor is there a conclusion statement about why that is impossible. There isn't any large scale comparison, either qualitative or quantified by user studies, about the quality of the results.", "title": "Resolution", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SkxqDIMNg": {"type": "review", "replyto": "Hyvw0L9el", "review": "I have basically the same questions as Reviewer2. This paper is all about pixelCNN vs GAN -- why the different resolutions? What are the actual train times, test times, and memory requirements for the two competing methods?\"First, it allows us to assess whether auto-regressive models are able to match the GAN results of Reed et al. (2016a).\" Does it, though? Because the resolution is so bad. And resolution limitations aren't addressed until the second-to-last paragraph of the paper. And Figure 9 only shows 3 results (picked randomly? Picked to be favorable to PixelCNN?). That hardly seems conclusive.\n\nThe segmentation masks and keypoints are pretty strong input constraints. It's hard to tell how much coherent object and scene detail is emerging because the resolution is so low. For example, the cows in figure 5 look like color blobs, basically. Any color blob that follows an exact cow segmentation mask will look cow-like.\n\nThe amount of variation is impressive, though.\n\nHow can we assess how much the model is \"replaying\" training data? Figure 8 tries to get at this, but I wonder how much each of the \"red birds\", for instance, is mostly copied from a particular training example.\n\nI'm unsatisfied with the answers to the pre-review questions. You didn't answer my questions. The paper would benefit from concrete numbers on training time / epochs and testing time. You didn't say why you can't make high resolution comparisons. Yes, it's slower at test time. Is it prohibitively slow? Or is it slightly inconvenient? There really aren't that many comparisons in the paper, anyway, so it if takes an hour to generate a result that doesn't seem prohibitive. \n\nTo be clear about my biases: I don't think PixelCNN is \"the right way\" to do deep image generation. Texture synthesis methods used these causal neighborhoods with some success, but only because there wasn't a clear way to do the optimization more globally (Kwatra et al, Texture Optimization for Example-based Synthesis being one of the first alternatives). It seems simply incorrect to make hard decisions about what pixel values should be in one part of the image before synthesizing another part of the image (Another texture synthesis strategy to help fight back against this strict causality was coarse-to-fine synthesis. And I do see some deep image synthesis methods exploring that). It seems much more correct to have a deeper network and let all output pixels be conditioned on all other pixels (this conditioning will implicitly emerge at intermediate parts of the network). That said, I could be totally wrong, and the advantages stated in the paper could outweigh the disadvantages. But this paper doesn't feel very honest about the disadvantages.\n\nOverall, I think the results are somewhat tantalizing, especially the ability to generate diverse outputs. But the resolution is extremely low, especially compared to the richness of the inputs. The network gets a lot of hand holding from rich inputs (it does at least learn to obey them). \n\nThe deep image synthesis literature is moving very quickly. The field needs to move on from \"proof of concept\" papers (the first to show a particular result is possible) to more thorough comparisons. This paper has an opportunity to be a more in depth comparison, but it's not very deep in that regard. There isn't really an apples to apples comparison between PixelCNN and GAN nor is there a conclusion statement about why that is impossible. There isn't any large scale comparison, either qualitative or quantified by user studies, about the quality of the results.", "title": "Resolution", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SkJvnQ87e": {"type": "review", "replyto": "Hyvw0L9el", "review": "As comparison to prior work must be done based on qualitative results, could you elaborate on the comparison with GAN (Reed 2016a) shown in Figure 9.  In particular, if viewed at the same resolution (eg GAN results downsampled to 32x32), are the GAN artifacts still apparent?  Can the PixelCNN scale to higher resolutions without artifacts?  Secondly, the PixelCNN results seem only weakly influenced by the text, while text appears to have a much stronger influence on the results in (Reed 2016a).  Is this a weakness with respect to the GAN approach?\n\nAlso, within Figure 9, I believe (Reed 2016b) should be (Reed 2016a) (label above images).This work focuses on conditional image synthesis in the autoregressive framework.  Based on PixelCNN, it trains models that condition on text as well as segmentation masks or keypoints.  Experiments show results for keypoint conditional synthesis on the CUB (birds) and MHP (human pose) dataset, and segmentation conditional synthesis on MS-COCO (objects).  This extension to keypoint/segment conditioning is the primary contribution over existing PixelCNN work.  Qualitative comparison is made to GAN approaches for synthesis.\n\nPros:\n(1) The paper demonstrates additional capabilities for image generation in the autoregressive framework, suggesting that it can keep pace with the latest capabilities of GANs.\n(2) Qualitative comparison in Figure 9 suggests that PixelCNN and GAN-based methods may make different kinds of mistakes, with PixelCNN being more robust against introducing artifacts.\n(3) Some effort is put forth to establish quantitative evaluation in terms of log-likelihoods (Table 1).\n\nCons:\n(1) Comparison to other work is difficult and limited to qualitative results.  The qualitative results can still be somewhat difficult to interpret.  I believe supplementary material or an appendix with many additional examples could partially alleviate this problem.\n(2) The extension of PixelCNN to conditioning on additional data is fairly straightforward.  This is a solid engineering contribution, but not a surprising new concept.", "title": "comparison with GANs", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Sy91CMB4g": {"type": "review", "replyto": "Hyvw0L9el", "review": "As comparison to prior work must be done based on qualitative results, could you elaborate on the comparison with GAN (Reed 2016a) shown in Figure 9.  In particular, if viewed at the same resolution (eg GAN results downsampled to 32x32), are the GAN artifacts still apparent?  Can the PixelCNN scale to higher resolutions without artifacts?  Secondly, the PixelCNN results seem only weakly influenced by the text, while text appears to have a much stronger influence on the results in (Reed 2016a).  Is this a weakness with respect to the GAN approach?\n\nAlso, within Figure 9, I believe (Reed 2016b) should be (Reed 2016a) (label above images).This work focuses on conditional image synthesis in the autoregressive framework.  Based on PixelCNN, it trains models that condition on text as well as segmentation masks or keypoints.  Experiments show results for keypoint conditional synthesis on the CUB (birds) and MHP (human pose) dataset, and segmentation conditional synthesis on MS-COCO (objects).  This extension to keypoint/segment conditioning is the primary contribution over existing PixelCNN work.  Qualitative comparison is made to GAN approaches for synthesis.\n\nPros:\n(1) The paper demonstrates additional capabilities for image generation in the autoregressive framework, suggesting that it can keep pace with the latest capabilities of GANs.\n(2) Qualitative comparison in Figure 9 suggests that PixelCNN and GAN-based methods may make different kinds of mistakes, with PixelCNN being more robust against introducing artifacts.\n(3) Some effort is put forth to establish quantitative evaluation in terms of log-likelihoods (Table 1).\n\nCons:\n(1) Comparison to other work is difficult and limited to qualitative results.  The qualitative results can still be somewhat difficult to interpret.  I believe supplementary material or an appendix with many additional examples could partially alleviate this problem.\n(2) The extension of PixelCNN to conditioning on additional data is fairly straightforward.  This is a solid engineering contribution, but not a surprising new concept.", "title": "comparison with GANs", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}