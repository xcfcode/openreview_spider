{"paper": {"title": "Analyzing Privacy Loss in Updates of Natural Language Models", "authors": ["Shruti Tople", "Marc Brockschmidt", "Boris K\u00f6pf", "Olga Ohrimenko", "Santiago Zanella-B\u00e9guelin"], "authorids": ["t-shtopl@microsoft.com", "mabrocks@microsoft.com", "boris.koepf@microsoft.com", "oohrim@microsoft.com", "santiago@microsoft.com"], "summary": "comparing updates of language models reveals many details about changes in training data", "abstract": "To continuously improve quality and reflect changes in data, machine learning-based services have to regularly re-train and update their core models. In the setting of language models, we show that a comparative analysis of model snapshots before and after an update can reveal a surprising amount of detailed information about the changes in the data used for training before and after the update.\nWe discuss the privacy implications of our findings, propose mitigation strategies and evaluate their effect.", "keywords": ["Language Modelling", "Privacy"]}, "meta": {"decision": "Reject", "comment": "This paper report empirical implications of privacy \u2018leaks\u2019 in language models. Reviewers generally agree that the results look promising and interesting, but the paper isn\u2019t fully developed yet. A few pointed out that framing the paper better to better indicate broader implications of the observed symptoms would greatly improve the paper. Another pointed out better placing this work in the context of other related work. Overall, this paper could use another cycle of polishing/enhancing the results. \n"}, "review": {"B1xn_vNhjr": {"type": "rebuttal", "replyto": "B1gdvQXmoH", "comment": "The latest revision adds experimental results on re-training with different data splits and continued training on real-world data from the 20 Newsgroups dataset. See RQ4A, RQ4B and Table 5 in Section 3.3 for an analysis of the results.\nThis expands the experiments on synthetic data (canary phrases) added in the previous revision.\n", "title": "Addendum"}, "B1gdvQXmoH": {"type": "rebuttal", "replyto": "B1xoserKPH", "comment": "We would like to clarify the core contributions of our paper. Training ML models on private data raises concerns about how much of this data is memorized and leaked by the models. In this paper, we advance the state-of-the art in this space as follows:\n\n1.\tWe analyze privacy in an important novel attack scenario: Given API access to two models, one trained on a dataset $D$ and the other on $D + \\Delta$,  where $\\Delta$ includes private data, is it possible to extract information about $\\Delta$? This question needs to be answered, for example, when augmenting models that are pre-trained on massive public datasets with private data, and when deleting a user\u2019s data from a dataset, e.g., following GDPR. \n\n2.\tWe show that the threat to privacy is real: An attacker can successfully recover information about $\\Delta$ from the inference outputs of the models. The attack is effective even without background knowledge about $D$ or $\\Delta$.\n\n\n= Summary of changes we made to the paper:\t\n1. Added missing values for Wikitext-103 model in Table 1.\n2. Added the validation perplexity of each of the models in Table 1.\n3. Added experimental results on re-training with different data splits and continued training. RQ3A and RQ3B and Table 2 summarize the results of our experiments.\n4. Clarified the construction of canaries in Section 3.2.\n5. Clarified DP experiments in Section 4.\n\n= Summary of new experimental results (Table 2, Section 3.2):\nThe original submission extracted information about $\\Delta$ between model $M$, trained on data $D_{orig}$, and updated model $M\u2019$, trained on $D_{orig}$ and $\\Delta$, where $\\Delta$ was either canary phrases or a newsgroup. We added results for the following settings:\n1.\t$M\u2019$ is trained on $D_{orig}$ +canaries+$D_{extra}$ [RQ3A in Section 3.2]:: Additional text $ D_{extra}$ did not affect the differential score (DS) of the canary phrase as the score remained constant for different splits between $D_{orig}$ and $D_{extra}$. Thus, the canaries are susceptible to leakage even when the updated model is trained using additional dataset.\n2.\tContinued training [RQ3B in Section 3.2]: $M\u2019$ is initialized with parameters of $M$ and is trained further with new data $D_{extra}$ and canaries. In this setting, we observed that DS values have increased (i.e., higher susceptibility to leakage) in comparison to when $M\u2019$ is trained from scratch on $D_{orig}$+$D_{extra}$+canaries. \n3.\tContinued training with two stages [RQ3B in Section 3.2]: An intermediate model $\\tilde{M}$ is updated as above, i.e., initialized with $M$ and updated with $D_{extra}$+canaries. The final model, $M\u2019$ is trained starting from $\\tilde {M}$ and extra dataset $D\u2019_{extra}$. We observed that the DS substantially reduces in this setting making it suitable as a potential mitigation strategy.\n", "title": "General Review Response and Overview of Paper Revision"}, "Syl2wVmXjH": {"type": "rebuttal", "replyto": "H1gshz42KB", "comment": "Thank you for recognizing the importance of the problem and giving feedback to our submission.\n\n> Concerns: I don't know how generalizable these results would be on \n> really well-trained language models (rnn, convolution-based, or\n> transformer-based).\n\nIt would be helpful if you could be more precise about what you mean by \u201creally well-trained\" here. The tested models are implementing standard practices of token-level language modeling (i.e., regularization via (recurrent) dropout, tying of token embeddings and output projection, \u2026) but are not aiming to compete with recent high-capacity efforts such as GPT-2. The experiments show that our findings hold for both RNN-based and Transformer-based models (and it would be surprising if convnets would behave differently) of small and medium capacity. It seems intuitive that higher-capacity models would only exacerbate the problem, as they are known to be prone to memorizing substantial input chunks.\n\nOur paper tries to provide experimental evidence of an angle of attack that has not been studied well, and thus serves as a warning signal to practitioners that are deploying language models in the wild.\n\n> The related work section doesn't seem particularly well put together,\n> so its difficult to place the work in appropriate context and gauge\n> its impact.\n\nOur work falls under \u201cattacks on ML models\u201d topic and specifically language models. It is not yet well-understood what language models memorize and can leak; to this end, our paper proposes a new angle of extracting data that ML community has to be aware of. We believe we discuss the most relevant previous works in this topic, but we welcome suggestions about anything we may have missed.\n\n> Other Thoughts: I'd like more thorough error analysis looking at exactly\n> what kinds of strings/more nuanced properties of sequences that get\n> a high differential score. \n\nIn initial experiments, we found that out-of-distribution sequences (such as the \u201cMy social security number is [\u2026]\u201d used in Carlini et al. (2019)) are very easy to extract from the difference between two models. This is why we focused on manually creating sentences that are \"near\" to the training data by following a simple valid grammatical format and then varied the frequency of the used terms. In practice, it turns out that the models behave very much like expected: If canaries use very frequent words, extracting them becomes harder; if they use infrequent words, extracting them becomes easier.\n\n> Overall I think this work is interesting and I would encourage the\n> authors to try and add as much quantitative evaluation as possible,\n> but also try and include qualitative information regarding specific\n> sequences after prodding the models. Those could go a long way in\n> strengthening the paper.\n\nWe have updated the paper with experiments using several new settings of data splits and continued training setup (for example,  where the model is trained on the original data, and then \u201cfine-tuned\u201d on a smaller additional dataset). We refer the reviewer for detailed explanation in the general response. New content is presented in Section 3.2 (RQ3A, RQ3B) of the updated submission. \n", "title": "Discussion of Review #1"}, "ryg4Q4m7or": {"type": "rebuttal", "replyto": "BJg-Ic3hFr", "comment": "Thank you for your feedback and we hope that our comments below can resolve your concerns. Note that this response is split into two parts for character limit reasons.\n\n> the synthetic experiments around which much of the paper is based may\n> not be sufficiently novel\n\nThe paper presents the first study of privacy implications of releasing snapshots of language models trained on overlapping data. Contributions include the new attack scenario and how to carry out the attack in a realistic setting with minimal assumptions on the attacker.\n\n> give little indication of broader implications. \n\nBreadth of implications:The main implication is that practitioners should be very careful when releasing models that have been trained on overlapping datasets since the difference between the datasets, as we show, can be leaked. Since releasing updated models is common (e.g., due to GDPR), we believe it is a serious concern the language modelling community needs to be aware of.\n\n> It would have been more convincing if these results replicated with two\n> splits of the same dataset, rather than identical datasets with one \n> augmented by canary tokens. \n\nSince the submission deadline, we performed a range of experiments to  evaluate information leakage in other data-overlapping scenarios. The updated submission presents these results in Table 2 and in Section 3.2. Summary of this experiment is given in the general response above.\n\n> The qualitative evaluation of subject-specific updates is also not\n> sufficiently informative. It would have been useful to define a\n> specific attack and see under what circumstances such an attack would\n> succeed. In the current results, I am not convinced that any of the\n> phrases in Table 3 represent a privacy violation.\n\nThe qualitative evaluation in Section 3.3 shows that our attacks recover phrases related to the content of the data used to update the model rather than to the rest of the data. If this data has been selected from private conversations instead of public discussions in a newsgroup, an attacker would be able to infer recurrent conversation topics, violating the privacy of the participants.\n\nTo define what it means for a specific attack to succeed, we would need a quantitative measure of success. We are exploring one such measure: train a classifier that discriminates between the public and training data used to update a model and compute the sum of the probabilities with which the discriminator classifies the phrases extracted as belonging to the private data. We think that the phrases output by our attack would be overwhelmingly classified as coming from the private data. We expect to include the results of our experiments when finalizing the submission.\n\n> The differential privacy experiment seems to be missing many details:\n\nWe have updated Section 4 as per reviewer\u2019s comments and outline them below:\n\n> what dataset was this trained on?\n\nWe used the Penn Treebank dataset.\n\n> Are the accuracy values for the training set or a separate testing set?\n\nAll accuracies that we report are for a separate validation set.\n-\tTraining accuracies are (29.73%,11.52%, 13%) for (non-DP, eps=5, eps=111), respectively. \n-\tValidation accuracies are (23%, 11.89%, 13.34%) for (non-DP, eps=5, eps=111), respectively.\n\nWe note that the discrepancy between training and validation accuracies is consistent with previous results on DP training.\n\n> Other works have shown that it is possible to train a differentially\n> private language model without large sacrifices in accuracy, so it\n> would be helpful to know what differentiates this experiment.\n\nThe language models trained in McMahan et al. consider user-level privacy (i.e., batch of token-sequences), while we consider single token-sequence-level privacy. Hence, the gradient clipping is done per sequence in our case and not per batch. The difference between batch-level data in terms of gradients is smaller than that of sequence-level gradients (intuitively the differences are ``averaged\u2019\u2019 in a batch). As a result, updates are not \u201ctoo different\u201d between users. Hence, the noise, which is proportional to the change in the gradient, that needs to be added is much smaller when guaranteeing user-level privacy as compared to sequence-level privacy.\nThe model trained by Carlini et al. is for a character prediction task which is a much simpler task than token prediction considered in our paper.\nThat said, training privacy-preserving models with sequence-level privacy and good utility is an important research question but out of scope for this work.\n", "title": "Discussion of Review #3 (Part 1/2)"}, "H1xFTQQmiS": {"type": "rebuttal", "replyto": "SyejLWDpKr", "comment": "Thank you for engaging with our submission and asking questions!\n\n> According to the current paper, the privacy implication seems to be\n> defined in terms of general sequences in training datasets. If this is\n> the case, I don\u2019t think such privacy implication is meaningful because\n> our language models should memorize some general information to\n> achieve their tasks.\n\nOur experiments show that much more than general information is revealed through model updates, because _specific_ phrases occurring in the training data as rarely as one in a million times can be extracted.\nAs an example, consider the case of a \u201cSmart Compose\u201d feature (i.e., email auto-completion) using a model trained on data from a given company, and from which employees can extract phrases of the form \u201cWe will close [city] office\u201d, because similar phrases occur multiple times in emails of C-suite managers.\n\n> 1. In the experiments, why there are only 20000 vocabulary size for\n> Wikitext-103 datasets?\n\n20k was primarily chosen for performance reasons, both during our experiments as well as when considering the application scenario of predictive keyboards on client devices (where deploying the full Wikitext-103 vocabulary of size 267k would be infeasible in most cases). While 20k is indeed somewhat arbitrary, we are confident that increasing the vocabulary size (i.e., increasing the capacity of the model) would not change the direction of our results in a substantial way.\n\n> 2. It is unclear how to construct canary phrases.\n\nOur experiments use canaries that\n * serve as a proxy for \"private data\", i.e. they should be grammatically correct but must not appear in the original dataset, and\n * exhibit different token frequency characteristics \nTo this end, we choose different valid phrase structures (e.g. Subject, Verb, Adverb, Compound Object) and instantiate each placeholder with a token that has the desired frequency characteristic in the dataset under consideration.\nFor our experiments we construct the canaries manually, but automation is straightforward. We updated the description of the canary construction in Section 3.2. accordingly.\n\n> 3. After constructing the new dataset, the model is retrained or\n> trained in the online way?\n\nThe submitted version of our paper retrained the model from scratch. However, we have updated the paper with experiments using a continued training setup, in which the model is trained on the original data, and then \u201cfine-tuned\u201d on a smaller additional dataset. Please see RQ3B and Table 2 in Section 3.2 and its summary in the general response.\n\n> 4. Since the results for Wikitext-103 is not finished, the authors\n> should remove the results on this dataset.\n\nWe have updated Table 1 in the paper with the results for Wikitext-103.\n\n> 5. What is the perplexity of the trained models?\n\nThe validation perplexity for the trained models is as follows (we added this information to Table 1):\nPenn-Treebank: 120.90\nReddit (RNN): 79.63\nReddit (Transformer): 69.29\nWikitext-103: 48.59 \n\n> 6. How to choose initial sequence in real data experiments?\n\nWhen we compute the differential rank in RQ1,2,4-7, we compare the\nchanges in probability of all token sequences [*], that is, there is no\nneed for the adversary to choose any specific initial sequence.\nIn RQ3 we show that partial knowledge (i.e., knowledge of an initial\nsequence) about data used in the update can lead to more effective attacks.\n\n[*] In practice, we approximate this with a beam search, as discussed in Section 2.4.\n\n> 7. When you applying DP mechanism,how did you define the neighboring\n> datasets, and how did you implement it (what is the clipping level, how\n> did you calculate privacy loss for language models)?\n\nWe use sequence-level differential privacy: i.e., two neighbouring datasets differ in a single sequence of tokens.\nWe used the TensorFlow Privacy library for:\n(1)\tTraining with differentially private SGD. We used the Sampled Gaussian Mechanism that is provided by the library with the following parameters: \n- For eps=5: noise_multiplier=0.7, l2_norm_clip=5.0, sampling probability= .0048\n- For eps=111: noise_multiplier=0.3, l2_norm_clip=5.0, sampling_probability= .0024.\n(2)\tComputing privacy loss. The library uses a Renyi differentially privacy accountant for computing the total privacy loss.\n\n> 8. \\epsilon = 111 seems that the model will provide no privacy guarantee\n> according to the definition of differential privacy?\n\nIndeed, for a large epsilon, DP provides weak theoretical guarantees. However, our experiments show that it can still provide effective protection against our attack. This confirms results reported by Carlini et al. who show that current DP analyses come with (potentially overly) conservative bounds.", "title": "Discussion of Review #2"}, "S1lzB4m7jS": {"type": "rebuttal", "replyto": "BJg-Ic3hFr", "comment": "> I would also note that the motivation, a predictive keyboard, is not a\n> situation in which maximizing accuracy is generally desirable: users\n> tend to find this creepy rather than helpful. This is a nice idea but\n> would benefit from some more polishing and more extensive testing.\n\nIn the Smart Compose setting, the user, as she is typing her email, is given several choices for the next token. In order for Smart Compose to be useful, there should be an intersection between what the user intends to write and the choices suggested. Hence, maximizing accuracy is important, though of course striking a balance to avoid \u201ctoo personal\u201d suggestions is important. How to strike this balance is out of scope for this paper.\nHowever, if models are personalized (or at least customized to a group similar to a specific user), they do have to shift their recommendations slightly to better match the data distribution of the data used for customization. In our submission we argue that this _shift_ is already leaking private information. You seem to be referring to the \u201cMy social security number is\u201d prefix setting of the Secret Sharer work of Carlini et al. (2019), in which the leakage happens because private information is the most likely prediction. However, our analysis of model updates shows that leakage also happens when (a) the leaked data is _not_ the top prediction of any individual model and (b) no prefixes are available.", "title": "Discussion of Review #3 (Part 2/2)"}, "H1gshz42KB": {"type": "review", "replyto": "B1xoserKPH", "review": "Summary: This paper looks at privacy concerns regarding data for a specific model before and after a single update. It discusses the privacy concerns thoroughly and look at language modeling as a representative task. They find that there are plenty of cases namely when the composition of the sequences involve low frequency words, that a lot of information leak occurs.\n\nPositives: The ideas and style of research is nice. This is an important problem and I think this paper does a good job investigating this in the context of language modeling. I do hope the community (and I think the community is) moving towards being aware of these sorts of privacy issues.\n\nConcerns: I don't know how generalizable these results would be on really well-trained language models (rnn, convolution-based, or transformer-based). The related work section doesn't seem particularly well put together, so its difficult to place the work in appropriate context and gauge its impact.\n\nOther Thoughts: I'd like more thorough error analysis looking at exactly what kinds of strings/more nuanced properties of sequences that get a high differential score.\n\nOverall I think this work is interesting and I would encourage the authors to try and add as much quantitative evaluation as possible, but also try and include qualitative information regarding specific sequences after prodding the models. Those could go a long way in strengthening the paper.", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "BJg-Ic3hFr": {"type": "review", "replyto": "B1xoserKPH", "review": "This paper provides an empirical evaluation of the privacy implications of releasing updated versions of language models. The authors show how access to two sequential snapshots of a trained language model can reveal highly specific information about the content of the data used to update the model, even when that data is in-distribution.\n\nThe paper contains easy to understand, concrete experiments and results, but seems altogether a little underdeveloped. The methodology is sound, but the synthetic experiments around which much of the paper is based may not be sufficiently novel and give little indication of broader implications. It would have been more convincing if these results replicated with two splits of the same dataset, rather than identical datasets with one augmented by canary tokens. \n \nThe qualitative evaluation of subject-specific updates is also not sufficiently informative. It would have been useful to define a specific attack and see under what circumstances such an attack would succeed. In the current results, I am not convinced that any of the phrases in Table 3 represent a privacy violation. \n\nThe differential privacy experiment seems to be missing many details: what dataset was this trained on? Are the accuracy values for the training set or a separate testing set? Other works have shown that it is possible to train a differentially private language model without large sacrifices in accuracy, so it would be helpful to know what differentiates this experiment. \n\nI would also note that the motivation, a predictive keyboard, is not a situation in which maximizing accuracy is generally desirable: users tend to find this creepy rather than helpful.\n\nThis is a nice idea but would benefit from some more polishing and more extensive testing.", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 1}, "SyejLWDpKr": {"type": "review", "replyto": "B1xoserKPH", "review": "This paper studies the privacy issue of widely used neural language models in the current literature. The authors consider the privacy implication phenomena of two model snapshots before and after an update. The updating setting considered in this paper is kind of interesting. However, the contribution of the current paper is not strong enough and there are many unclear experimental settings in the current paper.\n\nAccording to the current paper, the privacy implication seems to be defined in terms of general sequences in training datasets. If this is the case, I don\u2019t think such privacy implication is meaningful because our language models should memorize some general information to achieve their tasks. \n\nThere are some unclear settings in the experiments:\n1.In the experiments, why there are only 20000 vocabulary size for Wikitext-103 datasets?\n2.It is unclear how to construct canary phrases. \n3.After constructing the new dataset, the model is retrained or trained in the online way?\n4.Since the results for Wikitext-103 is not finished, the authors should remove the results on this dataset.\n5.What is the perplexity of the trained models?\n6.How to choose initial sequence in real data experiments?\n7.When you applying DP mechanism, how did you define the neighboring datasets, and how did you implement it (what is the clipping level, how did you calculate privacy loss for language models)?\n8.$\\epsilon=111$ seems that the model will provide no privacy guarantee according to the definition of differential privacy?", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 2}}}