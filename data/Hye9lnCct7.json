{"paper": {"title": "Learning Actionable Representations with Goal Conditioned Policies", "authors": ["Dibya Ghosh", "Abhishek Gupta", "Sergey Levine"], "authorids": ["dibya.ghosh@berkeley.edu", "abhigupta@berkeley.edu", "svlevine@eecs.berkeley.edu"], "summary": "Learning state representations which capture factors necessary for control", "abstract": "Representation learning is a central challenge across a range of machine learning areas. In reinforcement learning, effective and functional representations have the potential to tremendously accelerate learning progress and solve more challenging problems. Most prior work on representation learning has focused on generative approaches, learning representations that capture all the underlying factors of variation in the observation space in a more disentangled or well-ordered manner. In this paper, we instead aim to learn functionally salient representations: representations that are not necessarily complete in terms of capturing all factors of variation in the observation space, but rather aim to capture those factors of variation that are important for decision making -- that are \"actionable\". These representations are aware of the dynamics of the environment, and capture only the elements of the observation that are necessary for decision making rather than all factors of variation, eliminating the need for explicit reconstruction. We show how these learned representations can be useful to improve exploration for sparse reward problems, to enable long horizon hierarchical reinforcement learning, and as a state representation for learning policies for downstream tasks. We evaluate our method on a number of simulated environments, and compare it to prior methods for representation learning, exploration, and hierarchical reinforcement learning.", "keywords": ["Representation Learning", "Reinforcement Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "To borrow the succinct summary from R1, \"the paper suggests a method for generating representations that are linked to goals  in reinforcement learning. More precisely, it wishes to learn a representation so that two states are similar if the \npolicies leading to them are similar.\" The reviewers and AC agree that this is a novel and worthy idea.\n\nConcerns about the paper are primarily about the following.\n(i) the method already requires good solutions as input, i.e., in the form of goal-conditioned policies, (GCPs)\nand the paper claims that these are easy to learn in any case.\nAs R3 notes, this then begs the question as to why the actionable representations are needed.\n(ii) reviewers had questions regarding the evaluations, i.e., fairness of baselines, additional comparisons, and \nadditional detail. \n\nAfter much discussion, there is now a fair degree of consensus.  While R1 (the low score) still has a remaining issue with evaluation, particularly hyperparameter evaluation, they are also ok with acceptance. The AC is of the opinion that hyperparameter tuning is of course an important issue, but does not see it as the key issue for this particular paper. \nThe AC is of the opinion that the key issue is issue (i), raised by R3. In the discussion, the authors reconcile the inherent contradiction in (i) based on the need of additional downstream tasks that can then benefit from the actionable representation, and as demonstrated in a number of the evaluation examples (at least in the revised version). The AC believes in this logic, but believes that this should be stated more clearly in the final paper. And it should be explained\nthe extent to which training for auxiliary tasks implicitly solve this problem in any case.\n\nThe AC also suggests nominating R3 for a best-reviewer award."}, "review": {"SyewvpG7sX": {"type": "review", "replyto": "Hye9lnCct7", "review": "In this paper, the authors propose a new approach to representation learning in the context of reinforcement learning.\nThe main idea is that two states should be distinguished *functionally* in terms of the actions that are needed to reach them,\nin contrast with generative methods which try to capture all aspects of the state dynamics, even those which are not relevant for the task at hand.\nThe method of the authors assumes that a goal-conditioned policy is already learned, and they use a Kullback-Leibler-based distance\nbetween policies conditioned by these two states as the loss that the representation learning algorithm should minimize.\nThe experimental study is based on 6 simulated environments and outlines various properties of the framework.\n\nOverall, the idea is interesting, but the paper suffers from many weaknesses both in the framework description and in the experimental study that make me consider that it is not ready for publication at a good conference like ICLR.\n\nThe first weakness of the approach is that it assumes that a learned goal-conditioned policy is already available, and that the representation extracted from it can only be useful for learning \"downstream tasks\" in a second step. But learning the goal-conditioned policy from the raw input representation in the first place might be the most difficult task. In that respect, wouldn't it be possible to *simultaneously* learn a goal-conditioned policy and the representation it is based on? This is partly suggested when the authors mention that the representation could be learned from only a partial goal-conditioned policy, but this idea definitely needs to be investigated further.\n\nA second point is about unsufficiently clear thoughts about the way to intuitively advocate for the approach. The authors first claim that two states are functionally different if they are reached from different actions. Thinking further about what \"functionally\" means, I would rather have said that two states are functionally different if different goals can be reached from them. But when looking at the framework, this is close to what the authors do in practice: they use a distance between two *goal*-conditioned policies, not *state*-conditioned policies. To me, the authors have established their framework thinking of the case where the state space and the goal space are identical (as they can condition the goal-conditioned policy by any state=goal). But thinking further to the case where goals and states are different (or at least goals are only a subset of states), probably they would end-up with a different intuitive presentation of their framework. Shouldn't finally D_{act} be a distance between goals rather than between states?\n\nSection 4 lists the properties that can be expected from the framework. To me, the last paragraph of Section 4 should be a subsection 4.4 with a title such as \"state abstraction (or clustering?) from actionable representation\". And the corresponding properties should come with their own questions and subsection in the experimental study (more about this below).\n\nAbout the related work, a few remarks:\n- The authors do not refer to papers about using auxiliary tasks. Though the purpose of these works is often to supply for additional reward signals in the sparse reward context, then are often concerned with learning efficient representations such as predictive ones.\n- The authors refer to Pathak et al. (2017), but not to the more recent Burda et al. (2018) (Large-scale study of curiosity-driven learning) which insists on the idea of inverse dynamical features which is exactly the approach the authors may want to contrast theirs with. To me, they must read it.\n- The authors should also read Laversanne-Finot et al. (2018, CoRL) who learn goal space representations and show an ability to extract independently controllable features from that.\n\nA positive side of the experimental study is that the 6 simulated environments are well-chosen, as they illustrate various aspects of what it means to learn an adequate representation. Also, the results described in Fig. 5 are interesting. A side note is that the authors address in this Figure a problem pointed in Penedones et al (2018) about \"The Leakage Propagation problem\" and that their solution seems more convincing than in the original paper, maybe they should have a look.\nBut there are also several weaknesses:\n- for all experiments, the way to obtain a goal-conditioned policy in the first place is not described. This definitely hampers reproducibility of the work. A study of the effect of various optimization effort on these goal-conditioned policies might also be of interest.\n- most importantly, in Section 6.4, 6.5 and 6.6, much too few details are given. Particularly in 6.6, the task is hardly described with a few words. The message a reader can get from this section is not much more than \"we are doing something that works, believe us!\". So the authors should choose between two options:\n* either giving less experimental results, but describing them accurately enough so that other people can try to reproduce them, and analyzing them so that people can extract something more interesting than \"with their tuning (which is not described), the framework of the authors outperforms other systems whose tuning is not described either\".\n* or add a huge appendix with all the missing details.\nI'm clearly in favor of the first option.\n\nSome more detailed points or questions about the experimental section:\n- not so important, Section 6.2 could be grouped with Section 6.1, or the various competing methods could be described directly in the sections where they are used.\n- in Fig. 5, in the four room environment, ARC gets 4 separated clusters. How can the system know that transitions between these clusters are possible?\n- in Section 6.3, about the pushing experiment, I would like to argue against the fact that the block position is the important factor and the end-effector position is secundary. Indeed, the end-effector must be correctly positioned so that the block can move. Does ARC capture this important constraint?\n- Globally, although it is interesting, Fig.6 only conveys a quite indirect message about the quality of the learned representation.\n- Still in Fig. 6, what is described as \"blue\" appears as violet in the figures and pink in the caption, this does not help when reading for the first time.\n- In Section 6.4, Fig.7 a, ARC happens to do better than the oracle. The authors should describe the oracle in more details and discuss why it does not provide a \"perfect\" representation.\n- Still in Section 6.4, the authors insist that ARC outperforms VIME, but from Fig.7, VIME is not among the best performing methods. Why insist on this one? And a deeper discussion of the performance of each method would be much more valuable than just showing these curves.\n- Section 6.5 is so short that I do not find it useful at all.\n- Section 6.6 should be split into the HRL question and the clustering question, as mentioned above. But this only makes sense if the experiments are properly described, as is it is not useful.\n\nFinally, the discussion is rather empty, and would be much more interesting if the experiments had been analyzed in more details.\n\ntypos:\n\np1: that can knows => know\np7: euclidean => Euclidean\n", "title": "Quite interesting idea, but unsufficiently mature piece of research", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1lO0VLPhm": {"type": "review", "replyto": "Hye9lnCct7", "review": "The paper presents a method to learn representations where proximity in euclidean distance represents states that are achieved by similar policies. The idea is novel (to the best of my knowledge), interesting and the experiments seem promising. The two main flaws in the paper are the lack of details and missing important experimental comparisons.\n\nMajor remarks:\n\n- The author state they add experimental details and videos via a link to a website. I think doing so is very problematic, as the website can be changed after the deadline but there was no real information on the website so it wasn\u2019t a problem this time.\n\n- While the idea seems very interesting, it is only presented in very high-level. I am very skeptical someone will be able to reproduce these results based only on the given details. For example - in eq.1 what is the distribution over s? How is the distance approximated? How is the goal-conditional policy trained? How many clusters and what clustering algorithm?\n\n- Main missing details is about how the goal reaching policy is trained. The authors admit that having one is \u201ca significant assumption\u201d and state that they will discuss why it is reasonable assumption but I didn\u2019t find any such discussion  (only a sentence in 6.4).  \n\n- While the algorithm compare to a variety of representation learning alternatives, it seems like the more natural comparison are model-based Rl algorithms, e.g. \u201cNeural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning\u201d. This is because the representation tries to implicitly learn the dynamics so it should be compared to models who explicitly learn the dynamics. \n\n- As the goal-conditional policy is quite similar to the original task of navigation, it is important to know for how long it was trained and taken into account.\n\n- I found Fig.6 very interesting and useful, very nice visual help.\n\n- In fig.8 your algorithm seems to flatline while the state keeps rising. It is not clear if the end results is the same, meaning you just learn faster, or does the state reach a better final policy. Should run and show on a longer horizon.\n\n", "title": "Paper lacks many important details.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1lyiF78AX": {"type": "rebuttal", "replyto": "H1ghTFkVR7", "comment": "Thank you for your response and helpful suggestions! The question you raise about the necessity of something beyond a goal-conditioned policy is a valuable one, and we answer it below. We have updated the discussion in Section 4 to reflect the same. We have also added an additional comparison to directly using goal conditioned policies in Section 6.6. \n\nAlthough GCPs trained to reach one state from another are feasible to learn, they possess some fundamental limitations (added discussion in Section 4): they do not generalize very well to new states, and they are limited to solving tasks expressible as just reaching a particular goal.  The unifying explanation for why ARCs are useful over just a GCP is that the learned representation generalizes better than the GCP - to new tasks and to new regions of the environment. In our experimental evaluation, we show that ARCs can help solve tasks that cannot be expressed as goal reaching (Section 6.6, 6.7) and they enable learning policies on larger regions to which GCPs do not generalize (Section 6.5).\n\nAs the GCP is trained with a sparse reaching reward, it is unaware of possible reward structures in the environment, making it hard to adapt to tasks which are not simple goal reaching, such as the \u201creach-while-avoid\u201d task in Section 6.6. For this task, following the GCP directly would cause the ant to walk through the red region and incur a large negative reward; a comparison we now explicitly add to Section 6.6. Tasks which cannot be expressed as simply reaching a goal are abundant in real life scenarios such as navigation with preferences or manipulation with costs on quality of motion, and fast learning on such tasks (as ARC does) is quite beneficial. We have explicitly emphasized this discussion in Section 4.1, and made the limitations of simple goal reaching clear at the start of Section 4. \n\nIn the tasks for Section 6.5, the GCP trained on the 2m region (in green) does not achieve high performance on the larger region of 8m, even when finetuned on the environment using the provided reward (Fig 8). However, shaping the reward function using ARCs enables learning beyond the the original GCP, showing that ARCs generalize better to this new region, and potentially can lead to learning progressively harder GCPs via bootstrapping. \n\nWe agree that the discussion would greatly benefit from an introductory paragraph putting things into context, we have added this discussion at the beginning of Section 4. Please let us know if this resolves the issues you brought up. If not, we\u2019re happy to address any other concerns you might have. \n", "title": "Response to Reviewer 3: Regarding the Goal-Conditioned Policy"}, "HJeOi_mLRQ": {"type": "rebuttal", "replyto": "HkexM-C7RQ", "comment": "Thank you for your response! We are not sure we fully understand your concern about hyperparameter tuning, and were hoping for some additional clarifications regarding this. We have added additional details to the paper regarding hyperparameter tuning in Appendix D. We do not have many hyperparameters to tune for ARCs - the only free parameter is the size of the latent dimension, and for the downstream tasks, we tune the weight of the shaping term for reward shaping and the number of clusters for HRL for each comparison method on each task. \n\nThe size of the latent dimension is selected by performing a sweep on the downstream reward-shaping task for each domain and method. For the reward-shaping task, for each domain and comparison method, the parameter controlling the relative scaling of the shaped reward is selected according to a coarse hyperparameter sweep. The number of clusters for k-means for the hierarchy experiments is similarly selected for each domain and comparison method, although we found that all tasks and methods worked well with the same number of clusters. As you note, this is standard in deep reinforcement learning research, and we are simply following standard practice. Importantly, we give all methods a fair chance by tuning each comparison method separately. While we could certainly adjust hyperparameters differently, we did not find overall that hyperparameters were a major issue for our method. We would appreciate if you could clarify whether you are concerned about this issue in particular, and what a reasonably fair alternative might be?\n\nIf you believe that the issues in the paper have been addressed, we would appreciate it if you would revise your original review, or else point out what remaining issues you see with the paper or experimental evaluation.\n", "title": "Author Response: Adding Clarifications on Hyperparameters"}, "ByeyV10-TX": {"type": "review", "replyto": "Hye9lnCct7", "review": "The paper suggests a method for generating representations that are linked to goals in reinforcement learning. More precisely, it wishes to learn a representation so that two states are similar if the policies leading to them are similar.\n\nThe paper leaves quite a few details unclear. For example, why is this particular metric used to link the feature representation to policy similarity? How is the data collected to obtain the goal-directed policies in the first place? How are the different methods evaluated vis-a-vis data collection?  The current discussion makes me think that the evaluation methodology may be biased. Many unbiased experiment designs are possible. Here are a few:\n\nA. Pre-training with the same data\n\n1. Generate data D from the environment (using an arbitrary policy).\n2. Use D to estimate a model/goal-directed policies and consequenttly features F. \n3. Use the same data D to estimate features F' using some other method.\n4. Use the same online-RL algorithm on the environment and only changing features F, F'.\n\nB. Online training\n\n1. At step t, take action $a_t$, observe $s_{t+1}$, $r_{t+1}$\n2. Update model $m$ (or simply store the data points)\n3. Use the model to get an estimate of the features \n\nIt is probably time consuming to do B at each step t, but I can imagine the authors being able to do it all with stochastic value iteration. \n\nAll in all, I am uncertain that the evaluation is fair.\n", "title": "A good idea, but suffers from lack of clarity", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Bke-uQ1WR7": {"type": "rebuttal", "replyto": "r1gAKUmA3Q", "comment": "Thank you for your interest in our paper and for your insightful comments. \n\nYou are correct that the ARC representation requires us to assume that we can train a goal-conditioned policy in the first place. For our experiments, the GCP was trained with TRPO using a sparse reward (see Section 6.2 and Appendix A.1) -- obtaining such a policy is not especially difficult, and existing methods are quite capable of doing so [1,2,3].  We therefore believe that this assumption is reasonable. \n\nTo ensure the comparisons are fair, every representation learning method that we compare to is trained using the same data (Section 6.2, 6.3). All representations are trained on a dataset of trajectories collected from the goal-conditioned policy, and we have updated the paper with full details of the training scheme (Section 6.3, Appendix A.2, B).\n\nWe also ensure that our experiments fairly account for the data required to train the GCP.\n- In the generalization experiment (Section 6.4), all methods initialize behaviour from the GCP, as policies trained from scratch fail, a new comparison we have added to Figure 7. \n- In the hierarchy experiment (Section 6.7), all representations use the GCP as a low-level controller, so ARC incurs no additional sample cost. Two comparisons (TRPO, Option Critic) which do not use the GCP make zero progress, even with substantially more samples.\n- In the experiment for learning non goal-reaching tasks (Section 6.6), the ARC representation can be re-used across many different tasks without retraining the GCP, amortizing the cost of learning the GCP.  We plan to add an experimental comparison on a family of 100 tasks to demonstrate this amortization, and will update the paper with results. \n\n[1] Nair, Pong, Dalal, Bahl, Lin, and Levine. Visual reinforcement learning with imagined goals.NIPS 2018\n[2] Pong, Gu, Dalal, and Levine. Temporal difference models: Model-free deep rl for model-based control. ICLR 2018\n[3] Andrychowicz, Wolski, Ray, Schneider, Fong, Welinder, P., McGrew, B., Tobin, J., Abbeel, P., and Zaremba, W. (2017). NIPS 2017\n", "title": "Author Response: Added Clarifications about Fair Comparisons"}, "S1gtlIA6TQ": {"type": "rebuttal", "replyto": "SyewvpG7sX", "comment": "Thank you for your insightful comments and suggestions! We have made many changes based on the comments provided by reviewers, which are summarized below. We would appreciate it if the reviewer could take another look at our changes and additional results, and let us know if they would like to revise their score or request additional changes that would alleviate their concerns.\n\nNew comparisons: \nWe have added two more comparisons as suggested - with model based RL methods ([5] Nagabandi et al) and learning representations via inverse dynamics models ([4] Burda et al). These have been described in Section 6.3 and added to plots in Fig 7, 8, 10. We have also added a new comparison to learning from scratch for the reward shaping experiment (Section 6.5, Fig 7). \n\nLack of details: \nWe apologize for the lack of clarity in the submission! We have updated the main text and added an appendix with additional details of the ARC representation and the experimental setup: how a goal-conditioned policy is trained (Sec 6.2, Appendix A.1), how the ARC representation is learned (Sec 6.2, Appendix A.2) , and how the methods are evaluated on downstream applications (Sec 6.5-7, Appendix A.3-6). We increased analysis of the performance of ARC and comparison methods for all the downstream applications (Sec 6.5-6.7), and added a discussion of how all methods are trained (Sec 6.3, Appendix A.2, B)\n\nRequirement for goal-conditioned policy:\nThe ARC representation is extracted from a goal-conditioned policy (GCP), requiring us to assume that we can train such a GCP. This assumption was explicit in our submission, but we have emphasized it more now by editing Section 1 and Section 3. For our experiments, the GCP was trained with existing RL methods using a sparse task-agnostic reward (Section 6.2, Appendix A.1) -- obtaining such a policy is not especially difficult, and existing methods are quite capable of doing so [1,2,3].  We therefore believe that this assumption is reasonable. We also ensure that our experiments fairly account for the data required to train the GCP.\n- In the generalization experiment (Section 6.4), all methods initialize behaviour from the GCP, as policies trained from scratch fail, a new comparison we have added to Figure 7. \n- In the hierarchy experiment (Section 6.7), all representations use the GCP as a low-level controller. Two comparisons (TRPO, Option Critic) which do not use the GCP make zero progress, even when provided with substantially more samples.\n- In learning non goal-reaching tasks (Section 6.6), ARC representation can be re-used across many tasks without retraining the GCP, amortizing the cost of learning the GCP.  We plan to add an experimental comparison on a family of tasks to demonstrate this, and will update the paper.\n", "title": "Response to Reviewer 3"}, "r1x_0rCT6m": {"type": "rebuttal", "replyto": "SyewvpG7sX", "comment": "Find responses to particular comments below: \nRelated work:\n-> We cite and discuss all the papers mentioned in the related work section (Section 5). We additionally added comparison (Fig 7,8,10) to using inverse dynamics models and model-based RL methods, as discussed above. \n\n\u201cShouldn't finally D_{act} be a distance between goals rather than between states?\u201d\n> D_{act} is indeed the actionable distance between goals, but given that the goal and the state space are the same the learned representation can be effectively used as a state representation as seen in Section 6.6.\n\n\u201cin Fig. 5, in the four room environment, ARC gets 4 separated clusters. How can the system know that transitions between these clusters are possible?\u201d\n->  We have added a discussion in Section 6.6 to clarify this. We use model free RL to train the high level policy which directly outputs clusters as described in Section 4.4. This high level policy does not need to explicitly model the transitions between clusters, that is handled by the low level goal reaching policy, and the high-level policy is trained model-free. \n\n\u201cIndeed, the end-effector must be correctly positioned so that the block can move. Does ARC capture this important constraint?\u201d\n-> ARC does not completely ignore the end effector position, this is evidenced from the fact that the blue region in Fig 6 is not a point but is an entire area. What ARC captures is that moving the block induces a greater difference in actions than inducing the arm. Moving the block to different positions requires the arm to move to touch the block and push it to the goal, while moving the arm to different positions can be done by directly moving it to the desired position. While both things are captured, the block is emphasized over the end-effector.\n\n\u201cIn Section 6.4, Fig.7 a, ARC happens to do better than the oracle. why?\u201d\n-> The oracle comparison is a hand-specified reward shaping - we have updated Section 6.5 and Figure 7 to make this point clear. It is likely that the ARC representation is able to find an even better reward shaping, although the difference is fairly small. \n\n\u201cfrom Fig.7, VIME is not among the best performing methods. Why insist on this one?\u201d\n-> We intended to emphasize that ARC is able to outperform a method that is purely designed for better exploration, not just other methods for representation learning. The discussion in Section 6.5 has been appropriately altered.\n\n[1] Nair, Pong, Dalal, Bahl, Lin, and Levine. Visual reinforcement learning with imagined goals.NIPS 2018\n[2] Pong, Gu, Dalal, and Levine. Temporal difference models: Model-free deep rl for model-based control. ICLR 2018\n[3] Andrychowicz, Wolski, Ray, Schneider, Fong, Welinder, P., McGrew, B., Tobin, J., Abbeel, P., and Zaremba, W. (2017). NIPS 2017\n[4] Burda, Edwards, Pathak, Storkey, Darrell, and Efros. Large-scale study of curiosity-driven learning. arXiv preprint\n[5] Nagabandi, Kahn, Fearing and Levine. Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning. ICRA 2018\n", "title": "Response to Reviewer 3 (Continued)"}, "HJlHiBC6pm": {"type": "rebuttal", "replyto": "r1lO0VLPhm", "comment": "Thank you for your insightful comments and suggestions! We have made many changes based on the comments provided by reviewers, which are summarized below. We would appreciate it if the reviewer could take another look at our changes and additional results, and let us know if they would like to request additional changes that would alleviate their concerns.\n\nNew comparisons: We have added a model-based RL algorithm planning with MPC (Nagabandi et al.), as a comparison to learning features. On the \u201creach-while-avoid\u201d task (Fig 8), model-based RL struggles compared to a model-free policy with ARC because of challenges such as model-bias, limited exploration and short-horizon planning. The updated plot and corresponding discussion have been added to Section 6.6. We have also added a comparison to representations from inverse dynamics models (Burda et al), described in Section 6.3.\n\nLack of details: We apologize for the lack of clarity in the submission! We have updated the main text and added an appendix with additional details of the ARC representation and the experimental setup: how a goal-conditioned policy is trained (Sec 6.2, Appendix A.1), how the ARC representation is learned (Sec 6.2, Appendix A.2) , and how the methods are evaluated on downstream applications (Sec 6.5-7, Appendix A.3-6). We have added a discussion of how all comparisons are trained, and measures taken to ensure fairness (Sec 6.3, Appendix A.2, B)\n\nRequirement for goal-conditioned policy: The ARC representation is extracted from a goal-conditioned policy (GCP), requiring us to assume that we can train such a GCP. This assumption was explicit in our submission, but we have emphasized it more now by editing Section 1 and Section 3. For our experiments, the GCP was trained with existing RL methods using a sparse task-agnostic reward (Section 6.2, Appendix A.1) -- obtaining such a policy is not especially difficult, and existing methods are quite capable of doing so [1,2,3].  We therefore believe that this assumption is reasonable, and have added this to the paper in Section 3.  \n\nWe also ensure that our experiments fairly account for the data required to train the GCP.\n- In the generalization experiment (Section 6.4), all methods initialize behaviour from the GCP, as policies trained from scratch fail, a new comparison we have added to Figure 7. \n- In the hierarchy experiment (Section 6.7), all representations use the GCP as a low-level controller, so ARC incurs no additional sample cost in comparison. Two comparisons (TRPO, Option Critic) which do not use the GCP make zero progress, even when provided with substantially more samples.\n- In the experiment for learning non goal-reaching tasks (Section 6.6), the ARC representation can be re-used across many different tasks without retraining the GCP, amortizing the cost of learning the GCP.  We plan to add an experimental comparison on a family of 100 tasks to demonstrate this amortization, and will update the paper with results. ", "title": "Response to Reviewer 2"}, "B1gKDS0TpQ": {"type": "rebuttal", "replyto": "r1lO0VLPhm", "comment": "Find responses to particular questions and comments below: \n\u201cShould run and show on a longer horizon.\u201d\n-> We have updated Figure 8 accordingly. All methods converge to the same average reward.\n\n\u201cAs the goal-conditional policy is quite similar to the original task of navigation, it is important to know for how long it was trained and taken into account.\u201d\n-> We have added these details in Appendix A.1. It is important to note that for the task in Section 6.6, simply using a goal reaching policy would be unable to solve the task, since it has no notion of other rewards, like regions to avoid (shown in red in Fig 8), and would pass straight through the region. \n\n\u201ceq.1 what is the distribution over s?\u201d \n-> It is the distribution over all states over which the goal-conditioned policy is trained. This is done by choosing uniformly from states on trajectories collected with the goal-conditioned policy as described in Section 6.2 and Appendix A.2. \n\n\u201c How is the distance approximated?\u201d\n-> In our experimental setup, we parametrize the action distributions of GCPs with Gaussian distributions - for this class of distributions, the KL divergence, and thus the actionable distance, can be explicitly computed (Appendix A.1).\n\n\u201cHow many clusters and what clustering algorithm?\u201d\n-> We use k-means for clustering, with distance in ARC space as the metric. We perform a hyperparameter sweep over the number of clusters for each method, and thus varies across tasks and methods. We have added this clarification to Section 4.4 and Section 6.6. \n\n\nThe author state they add experimental details and videos via a link to a website.\n> OpenReview does not provide a mechanism for submitting supplementary materials. Providing supplementary materials via an external link is the instruction provided by the conference organizers -- we would encourage the reviewer to check with the AC if they are concerned.\n\n[1] Nair, Pong, Dalal, Bahl, Lin, and Levine. Visual reinforcement learning with imagined goals.NIPS 2018\n[2] Pong, Gu, Dalal, and Levine. Temporal difference models: Model-free deep rl for model-based control. ICLR 2018\n[3] Andrychowicz, Wolski, Ray, Schneider, Fong, Welinder, P., McGrew, B., Tobin, J., Abbeel, P., and Zaremba, W. (2017). NIPS 2017\n[4] Burda, Edwards, Pathak, Storkey, Darrell, and Efros. Large-scale study of curiosity-driven learning. arXiv preprint\n[5] Nagabandi, Kahn, Fearing and Levine. Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning. ICRA 2018\n", "title": "Response to Reviewer 2 (Continued)"}, "HkxTyBRTaX": {"type": "rebuttal", "replyto": "ByeyV10-TX", "comment": "Thank you for your insightful comments and suggestions! We have made many changes based on the comments provided by reviewers, which are summarized below. We would appreciate it if the reviewer could take another look at our changes and additional results, and let us know if they would like to revise their score or request additional changes that would alleviate their concerns.\n\nNew comparisons:\n We have added two more comparisons - with model based RL methods ([1] Nagabandi et al) and learning representations via inverse dynamics models ([2] Burda et al). These have been described in Section 6.3 and added to plots in Fig 7, 8, 10. We have also added a new comparison to learning from scratch for the reward shaping experiment (Section 6.5, Fig 7). \n\nLack of details:\n We apologize for the lack of clarity in the submission! We have updated the main text and added an appendix with additional details of the ARC representation and the experimental setup: goal-conditioned policy (GCP) training (Sec 6.2, Appendix A.1), ARC representation learning (Sec 6.2, Appendix A.2) , downstream evaluation (Sec 4, 6.5-6.7, Appendix A.3-6). We have added a discussion of how all comparisons are trained, and measures taken to ensure fairness (Sec 6.3, Appendix A.2, B). We have clarified the algorithm and task descriptions in Section 4 and Section 6. \n\nFairness of comparisons: \nTo ensure the comparisons are fair, every comparison representation learning method is trained using the same data, and we have updated the paper to emphasize this (Section 6.2, 6.3). All representations are trained on a dataset of trajectories collected from the goal-conditioned policy, similar to the (A) scheme proposed by AnonReviewer1. We have updated the paper to include full details of the training scheme for all methods (Section 6.3, Appendix A.2, B).\n\nWe also ensure that our experiments fairly account for the data required to train the GCP.\n- In the generalization experiment (Section 6.4), all methods initialize behaviour from the GCP, as policies trained from scratch fail, a new comparison we have added to Figure 7. \n- In the hierarchy experiment (Section 6.7), all representations use the GCP as a low-level controller, so ARC incurs no additional sample cost. Two comparisons (TRPO, Option Critic) which do not use the GCP make zero progress, even with substantially more samples.\n- In the experiment for learning non goal-reaching tasks (Section 6.6), the ARC representation can be re-used across many different tasks without retraining the GCP, amortizing the cost of learning the GCP.  We plan to add an experimental comparison on a family of 100 tasks to demonstrate this amortization, and will update the paper with results. \n\nFind responses to particular questions and comments below: \n\n\u201cHow is the data collected to obtain the goal-directed policies in the first place?\u201d\n-> We train a goal-conditioned policy with TRPO using a task-agnostic sparse reward function. We have updated the paper to reflect this (Section 6.2, Appendix A.1).\n\n\u201cwhy is this particular metric used to link the feature representation to policy similarity?\u201d\n-> We add an explicit discussion of this in Section 3. We link feature representation to policy similarity by this metric, because it directly captures the notion that features should represent elements of the state which directly affect the actions. The KL divergence between policy distributions allows us to embed goal states which induce similar actions similarly into feature space. \n\n\n[1] Nagabandi, Kahn, Fearing and Levine. Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning. ICRA 2018\n[2] Burda, Edwards, Pathak, Storkey, Darrell, and Efros. Large-scale study of curiosity-driven learning. arXiv preprint\n", "title": "Response to Reviewer 1"}}}