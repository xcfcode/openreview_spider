{"paper": {"title": "Offline bilingual word vectors, orthogonal transformations and the inverted softmax", "authors": ["Samuel L. Smith", "David H. P. Turban", "Steven Hamblin", "Nils Y. Hammerla"], "authorids": ["samuel.smith@babylonhealth.com", "dt382@cam.ac.uk", "steven.hamblin@babylonhealth.com", "nils.hammerla@babylonhealth.com"], "summary": "We show that a linear transformation between word vector spaces should be orthogonal and can be obtained analytically using the SVD,  and introduce the inverted softmax for information retrieval.", "abstract": "Usually bilingual word vectors are trained \"online''. Mikolov et al. showed they can also be found \"offline\"; whereby two pre-trained embeddings are aligned with a linear transformation, using dictionaries compiled from expert knowledge. In this work, we prove that the linear transformation between two spaces should be orthogonal. This transformation can be obtained using the singular value decomposition. We introduce a novel \"inverted softmax\" for identifying translation pairs, with which we improve the precision @1 of Mikolov's original mapping from 34% to 43%, when translating a test set composed of both common and rare English words into Italian. Orthogonal transformations are more robust to noise, enabling us to learn the transformation without expert bilingual signal by constructing a \"pseudo-dictionary\" from the identical character strings which appear in both languages, achieving 40% precision on the same test set. Finally, we extend our method to retrieve the true translations of English sentences from a corpus of 200k Italian sentences with a precision @1 of 68%.", "keywords": ["Natural language processing", "Transfer Learning", "Applications"]}, "meta": {"decision": "Accept (Poster)", "comment": "This is a nice contribution and that present some novel and interesting ideas. At the same time, the empirical evaluation is somewhat thin and could be improved. Nevertheless, the PCs believe this will make a good contribution to the Conference Track."}, "review": {"HkRLoL1Fg": {"type": "rebuttal", "replyto": "r1Aab85gg", "comment": "We have uploaded the final version. The text is unchanged, but we have modified the title to emphasise the aspects of the paper which have been of most interest to readers (particularly the inverted softmax).\n\nWe'd like to thank the PC for accepting our manuscript,\nSam", "title": "Final version uploaded"}, "r1LvJBprl": {"type": "rebuttal", "replyto": "ByTk9Yt4e", "comment": "Thank you for your interest in our work. \n\n1.\tWe were not aware of the work by Artetxe et al., which was published shortly after we submitted our manuscript to ICLR. We have now cited this work appropriately, and drawn it to the reviewers\u2019 attention.\n2.\tWe have realised that our method, while extremely similar, is not identical to CCA. We apologise for this mistake, which is now corrected in the text. The method we propose is very similar to the method independently proposed by Artetxe et al..\n3.\tWe acknowledge that the use of the term \u201ccognates\u201d was confusing and we have removed it from the manuscript.\n4.\tAlthough the theoretical analysis of bilingual word vectors presented here is similar to the analysis presented by Artetxe et al., we present a number of novel contributions not present in their work, including\n\na)\tHow to perform dimensionality reduction following the SVD\nb)\tThe inverted softmax\nc)\tThe identical-string pseudo dictionary\nd)\tOffline vector alignment using a phrase dictionary\ne)\tSentence retrieval between languages using bilingual vectors\n", "title": "Thank you"}, "SkICA4pSx": {"type": "rebuttal", "replyto": "BJxR_kU4x", "comment": "We would like to thank you for your positive assessment of our work, and of the inverted softmax in particular.\n\nWe have realised that our procedure, while very similar to CCA, is not identical. We apologise for this mistake, which we have corrected in the new version. We believe this realisation strengthens the manuscript. We have included additional experiments, and a discussion of the very close relationship between the methods. The two methods have very similar performance, but our approach is numerically cheaper.\n\n In response to your specific comments,\n\n1.\tWe have now cited this work, and briefly discuss it in the text.\n2.\tSimilarly, this paper is now cited. We still consider Chandar et al. the first to obtain bilingual vectors from monolingual corpora and paired sentences, since Hermann and Blunsom cite an early version of Chandar\u2019s work in their manuscript.\n3.\tWe hope to explore additional language pairs in future. However, we believe that the range of tasks we consider in this manuscript, including a number of new tasks not considered in previous work, does provide a rigorous experimental evaluation. These new tasks include the pseudo dictionary of identical strings, the phrase dictionary, and sentence retrieval between languages. One of the goals of this work was to show that offline bilingual vectors can be used in a number of ways not previously considered in the literature.\n4.\tThis is a very interesting point. The Mahalanobis distance is an alternative to the cosine similarity. Although we do not discuss it in the manuscript, one could devise an alternative alignment procedure, which minimises the Mahalanobis distance of the dictionary rather than maximising the cosine similarity. I suspect that this relates to the slight difference between our procedure and CCA; but we have been unable to find a reference which discusses this.\n5.\tWe do recognise your concern, but I\u2019m afraid we couldn\u2019t come up with an appropriate alternative to \u201ctranslation\u201d.\n6.\tWe fixed this.\n", "title": "Response to review"}, "SkmuCVaSg": {"type": "rebuttal", "replyto": "HkP2OYBEg", "comment": "We would like to thank you for your positive assessment of our work.\nIn response to your remaining comments:\n\n1.\tWe have replaced \u201cWord frequency\u201d with \u201cWord ranking by frequency\u201d\n2.\tYes, these results were removed due to the space constraints, but we have now added them in an appendix.\n3.\tYes, the only difference here is NN vs. inverted softmax. Both columns use exactly the same orthogonal transformation. We have changed the text to make this clearer.\n4.\tWe did try naively combining the expert dictionary with the identical-strings pseudo-dictionary, but this performed worse than the expert dictionary alone. In hindsight this is not surprising, since the pseudo-dictionary is significantly larger but lower quality. It might be possible to get improved performance by introducing a weighting factor between the two dictionaries, but we have not tried this.\n", "title": "Response to review"}, "r1B-RVarl": {"type": "rebuttal", "replyto": "rkM8HK44l", "comment": "Thank you for your review.\n\nWe\u2019d also like to thank you for correcting our use of the term \u201ccognates\u201d, which we have removed from the new version of the text. The pseudo-dictionary is formed from every character string (e.g. \u201cCiao\u201d) in the English vocabulary which also appears in the Italian vocabulary. This dictionary is obtained by searching for perfect character string matches between the two vocabularies. \n\nAlongside theoretical clarification of the existing literature, our work makes a number of novel contributions, including:\n\n1.\tThe inverted softmax\n2.\tThe identical strings pseudo-dictionary\n3.\tOffline bilingual vectors from a phrase dictionary\n4.\tSentence translation retrieval using bilingual vectors\n\nPreviously offline bilingual word vectors have only been obtained using expert word dictionaries, and they have not been used to retrieve sentence translations.\n", "title": "Response to review"}, "Skq-p4TSg": {"type": "rebuttal", "replyto": "r1Aab85gg", "comment": "Dear reviewers and readers,\n\nWe\u2019d like to thank you all for your positive comments about our manuscript. We were particularly pleased that all three reviewers recommended our work be accepted, and by the interest reviewers expressed in the \u201cinverted softmax\u201d. We have uploaded an updated version. There are three main changes we would like to draw readers\u2019 attention to:\n\nOur use of the term \u201ccognates\u201d was misleading and we have removed it from the new version. To be completely clear, we extract the pseudo-dictionary by finding the identical character strings like \u201cDNA\u201d and \u201cCiao\u201d which appear in both the English and Italian vocabularies. These identical strings can be found trivially without any expert knowledge. \n\nWe also realised that our procedure, while very similar to CCA, is not identical. We apologise for this mistake, which we have corrected in the new version. We believe this realisation strengthens the manuscript. We provide additional experiments, and a discussion of the very close relationship between the methods. The two methods have very similar performance, but our approach is numerically cheaper.\n\nShortly after our manuscript was submitted to ICLR, another paper was published [1], which presents a similar theoretical analysis of offline bilingual word vectors. We would like to thank the anonymous reader for bringing this work to our attention, now properly cited. This paper also discusses the need for an orthogonal transformation, and proposes the same novel SVD procedure we propose here to obtain this transformation. However, our work contains a number of contributions not present in their work, including:\n\n1.\tThe use of dimensionality reduction after the SVD\n2.\tThe inverted softmax\n3.\tThe identical strings pseudo-dictionary\n4.\tOffline vector alignment using a phrase dictionary\n5.\tSentence translation retrieval using bilingual vectors\n\nWe will respond to the specific comments of each reviewer underneath their reviews.\nBest wishes,\nSam\n\n[1] Artetxe, M., Labaka, G., & Agirre, E. (2016). Learning principled bilingual mappings of word embeddings while preserving monolingual invariance. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16), 2289\u20132294.\n", "title": "Changes to new version"}, "ByTk9Yt4e": {"type": "rebuttal", "replyto": "r1Aab85gg", "comment": "Thank you for the interesting paper.\n1. Could you elaborate on how your method provides additional theoretical insight into the importance of orthogonality beyond existing work [1] and [2]? We would additionally encourage you to cite [2].\n2. In accordance with the review of AnonReviewer1, could you elaborate how your method is different from the existing use of CCA in [3]?\n3. As AnonReviewer1 pointed out, cognates are words with the same etymological origin but are usually spelled differently (see [4] for more examples). You should replace this term to make your manuscript more accurate.\n4. Given the above points, the main contributions of your paper are a) the inverted softmax and b) the \"cognate\" dictionary. Is that correct?\n\n[1] Xing, C., Liu, C., Wang, D., & Lin, Y. (2015). Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation. NAACL-2015, 1005\u20131010. \n[2] Artetxe, M., Labaka, G., & Agirre, E. (2016). Learning principled bilingual mappings of word embeddings while preserving monolingual invariance. Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP-16), 2289\u20132294. \n[3] Faruqui, M., & Dyer, C. (2014). Improving Vector Space Word Representations Using Multilingual Correlation. Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, 462 \u2013 471.\n[4] https://en.wikipedia.org/wiki/Cognate", "title": "Theory on \"offline\" bilingual word vectors and cognates"}, "BJcEmKNQe": {"type": "rebuttal", "replyto": "BkeeRzQQx", "comment": "we've uploaded a new version, incorporating the comments above.", "title": "(text now updated)"}, "BkeeRzQQx": {"type": "rebuttal", "replyto": "ByM-SP1Xl", "comment": "Thank you for your helpful comments! \nWe will upload a new version soon to incorporate your suggestions and the extra experiments you request, but to answer your specific questions:\n\n1. We apologise for this and will ensure the work you mention is cited properly.\n\n2. We have run the additional experiments you requested, and will include them in the updated version. CCA + inverted softmax outperforms Mikolov and Dinu\u2019s methods in all three cases. The advantage is particularly pronounced when using the pseudo-dictionary, for which the precision @1 when translating from English to Italian was 0.01 for Mikolov\u2019s method and 0.06 for Dinu\u2019s method, compared to 0.40 when using CCA with the inverted softmax. We believe that this difference arises because orthogonal transformations are more robust to the high level of noise present in the pseudo-dictionary. We think these results strengthen the paper, and we are grateful to the referee for requesting them.\n\n3. We certainly do not claim to be the first authors to obtain bilingual word vectors using CCA. Our goal is to show the close connection between the works of Mikolov, Dinu, Faruqui and Xing, and enhance the translation performance using the inverted softmax. Finally, we demonstrate that high-quality offline word vectors can be obtained without dictionaries from either cognate words or aligned sentences, and that bilingual word vectors can be used to retrieve sentence translations. We will ensure this is clearer in the new version.\n", "title": "Thank you"}, "ByM-SP1Xl": {"type": "review", "replyto": "r1Aab85gg", "review": "1. Gouws et al., 2015 were not the first to show that aligned sentences can be used alongside monolingual source to learn online bilingual vectors. It was Chandar et al., 2014 [1] who first showed that you can use both monolingual data and sentence aligned bilingual data to learn bilingual representations. Please correct this.\n\n2. Table 4,5,6 are incomplete without comparison with Dinu and Mikolov. Note that you can also use pseudo-dictionary and phrase dictionary in Dinu or Milovo's models. I would like to see those numbers before accepting the message from these tables.\n\n3. Faruqui and Dyer 2014 already did offiline bilingual word vectors using CCA. I agree that their motivation was different. I also agree that authors have some good theory to back it up. However it is extremely important to highlight the fact that this is not a new model and has been already proposed before. Paper would benefit if authors can list all the major contributions of this paper in the beginning: theory, pseudo-dictionary, inverted softmax, ..\n\nReferences:\n\n[1] Sarath Chandar, Stanislas Lauly, Hugo Larochelle, Mitesh Khapra, Balaraman Ravindran, Vikas C Raykar, and Amrita Saha. An autoencoder approach to learning bilingual word representations. In Advances in\nNeural Information Processing Systems, pp. 1853\u20131861, 2014The paper focuses on bilingual word representation learning with the following setting:\n\n1. Bilingual representation is learnt in an offline manner i.e., we already have monolingual representations for the source and target language and we are learning a common mapping for these two representations.\n2. There is no direct word to word alignments available between the source and target language.\n\nThis is a practically useful setting to consider and authors have done a good job of unifying the existing solutions for this problem by providing theoretical justifications. Even though the authors do not propose a new method for offline bilingual representation learning, the paper is significant for the following contributions:\n\n1. Theory for offline bilingual representation learning.\n2. Inverted softmax.\n3. Using cognate words for languages that share similar scripts.\n4. Showing that this method also works at sentence level (to some extent).\n\nAuthors have addressed all my pre-review questions and I am ok with their response. I have few more comments:\n\n1. Header for table 3 which says \u201cword frequency\u201d is misleading. \u201cword frequency\u201d could mean that rare words occur in row-1 while I guess authors meant to say that rare words occur in row-5.\n2. I see that authors have removed precision @5 and @10 from table-6. Is it because of the space constraints or the results have different trend? I would like to see these results in the appendix.\n3. In table-6 what is the difference between row-3 and row-4? Is the only difference NN vs. inverted softmax? Or there are other differences? Please elaborate.\n4. Another suggestion is to try running an additional experiment where one can use both expert dictionary and cognate dictionary. Comparing all 3 methods in this setting should give more valuable insights about the usefulness of cognate dictionary.\n", "title": "Few questions and comments", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HkP2OYBEg": {"type": "review", "replyto": "r1Aab85gg", "review": "1. Gouws et al., 2015 were not the first to show that aligned sentences can be used alongside monolingual source to learn online bilingual vectors. It was Chandar et al., 2014 [1] who first showed that you can use both monolingual data and sentence aligned bilingual data to learn bilingual representations. Please correct this.\n\n2. Table 4,5,6 are incomplete without comparison with Dinu and Mikolov. Note that you can also use pseudo-dictionary and phrase dictionary in Dinu or Milovo's models. I would like to see those numbers before accepting the message from these tables.\n\n3. Faruqui and Dyer 2014 already did offiline bilingual word vectors using CCA. I agree that their motivation was different. I also agree that authors have some good theory to back it up. However it is extremely important to highlight the fact that this is not a new model and has been already proposed before. Paper would benefit if authors can list all the major contributions of this paper in the beginning: theory, pseudo-dictionary, inverted softmax, ..\n\nReferences:\n\n[1] Sarath Chandar, Stanislas Lauly, Hugo Larochelle, Mitesh Khapra, Balaraman Ravindran, Vikas C Raykar, and Amrita Saha. An autoencoder approach to learning bilingual word representations. In Advances in\nNeural Information Processing Systems, pp. 1853\u20131861, 2014The paper focuses on bilingual word representation learning with the following setting:\n\n1. Bilingual representation is learnt in an offline manner i.e., we already have monolingual representations for the source and target language and we are learning a common mapping for these two representations.\n2. There is no direct word to word alignments available between the source and target language.\n\nThis is a practically useful setting to consider and authors have done a good job of unifying the existing solutions for this problem by providing theoretical justifications. Even though the authors do not propose a new method for offline bilingual representation learning, the paper is significant for the following contributions:\n\n1. Theory for offline bilingual representation learning.\n2. Inverted softmax.\n3. Using cognate words for languages that share similar scripts.\n4. Showing that this method also works at sentence level (to some extent).\n\nAuthors have addressed all my pre-review questions and I am ok with their response. I have few more comments:\n\n1. Header for table 3 which says \u201cword frequency\u201d is misleading. \u201cword frequency\u201d could mean that rare words occur in row-1 while I guess authors meant to say that rare words occur in row-5.\n2. I see that authors have removed precision @5 and @10 from table-6. Is it because of the space constraints or the results have different trend? I would like to see these results in the appendix.\n3. In table-6 what is the difference between row-3 and row-4? Is the only difference NN vs. inverted softmax? Or there are other differences? Please elaborate.\n4. Another suggestion is to try running an additional experiment where one can use both expert dictionary and cognate dictionary. Comparing all 3 methods in this setting should give more valuable insights about the usefulness of cognate dictionary.\n", "title": "Few questions and comments", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}