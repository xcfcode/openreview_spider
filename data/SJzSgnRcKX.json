{"paper": {"title": "What do you learn from context? Probing for sentence structure in contextualized word representations", "authors": ["Ian Tenney", "Patrick Xia", "Berlin Chen", "Alex Wang", "Adam Poliak", "R Thomas McCoy", "Najoung Kim", "Benjamin Van Durme", "Samuel R. Bowman", "Dipanjan Das", "Ellie Pavlick"], "authorids": ["iftenney@google.com", "paxia@cs.jhu.edu", "bchen6@swarthmore.edu", "alexwang@nyu.edu", "azpoliak@cs.jhu.edu", "tom.mccoy@jhu.edu", "n.kim@jhu.edu", "vandurme@cs.jhu.edu", "bowman@nyu.edu", "dipanjand@google.com", "ellie_pavlick@brown.edu"], "summary": "We probe for sentence structure in ELMo and related contextual embedding models. We find existing models efficiently encode syntax and show evidence of long-range dependencies, but only offer small improvements on semantic tasks.", "abstract": "Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.", "keywords": ["natural language processing", "word embeddings", "transfer learning", "interpretability"]}, "meta": {"decision": "Accept (Poster)", "comment": "Pros\n\n- Thorough analysis on a large number of diverse tasks\n- Extending the probing technique typically applied to individual encoder states to testing for presence of certain (linguistic) information based on pairs of encoders states (corresponding to pairs of words)\n- The comparison can be useful when deciding which representations to use for a given task\n\nCons\n\n- Nothing serious, it is solid and important empirical study\n\nThe reviewers are in consensus."}, "review": {"SkxaqkptCX": {"type": "rebuttal", "replyto": "Skey6mqlA7", "comment": "This is definitely related; we'll be sure to add a citation!", "title": "Thanks for the reference"}, "SJlErExtT7": {"type": "rebuttal", "replyto": "rJx0hgbKnX", "comment": "Thank you for the review!\n\nWe agree that it would be interesting to explore more specific tail phenomena. Attachment phenomena in particular can be studied on many of the same datasets if we fix labels and instead predict one of the two spans; this would be an interesting direction for future study.\n\nIt would also be very interesting to explore other languages! While we are limited by available data and encoder models, there\u2019s nothing in the edge probing technique that makes English-specific assumptions. Probing for phenomena that require long contexts could be a good test of advanced encoders, and can be easily quantified in our framework (for example, see Figure 3).", "title": "Author response"}, "HklxVExYpQ": {"type": "rebuttal", "replyto": "SJgb3cQF2Q", "comment": "Thank you for the review! \n\nWe\u2019re very interested in probing for other linguistic attributes - while we present a broad analysis in this paper, there\u2019s certainly room to use edge probing to study more focused phenomena like PP attachment or ambiguities between specific semantic roles. We use a standardized data format that makes it easy to add new tasks, and we hope that our code release will be a useful platform for this kind of analysis.\n\nWe\u2019ll be sure to update the text to more clearly describe the tables.\n\nWhoops! In Figure 2 and 3, the bars/bands are 95% confidence intervals calculated using the Normal approximation. We wanted to emphasize that the SPR and Winograd datasets are quite small and that the differences between models are often not significant. We\u2019ll add this to the caption in the final version.", "title": "Author response"}, "SJekzNxtaX": {"type": "rebuttal", "replyto": "rylJ-ovR2X", "comment": "Thank you for the review! We do hope that this will be of broad interest given recent progress in sentence representations, and hope that our code release will allow continued evaluation of new and better representation models (like BERT).\n\nWe\u2019ll certainly include examples of specific win / loss cases in the final version. ", "title": "Author response"}, "rylJ-ovR2X": {"type": "review", "replyto": "SJzSgnRcKX", "review": "\nThis is a nice paper that attempts to tease apart some questions about the effectiveness of contextual word embeddings (ELMo, CoVe, and the Transformer LM). The main question is about the value of context in these representations, and in particular how their ability to encode context allows them to also (implicitly) represent linguistic properties of words. What I really like about the paper is the \u201cEdge probing\u201d method it introduces. The idea is to probe the representations using diagnostic classifiers\u2014something that\u2019s already widespread practice\u2014but to focus on the relationship between spans rather than individual words. This is really nice because it enables them to look at more than just tagging problems: the paper looks at syntactic constituency, dependencies, entity labels, and semantic role labeling. I think the combination of an interesting research question and a new method (which will probably be picked up by others working in this area) make this a strong candidate for ICLR. The paper is well-written and experimentally thorough.\n\nNitpick: It would be nice to see some examples of cases where the edge probe is correct, and where it isn\u2019t.", "title": "Nice empirical paper", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJgb3cQF2Q": {"type": "review", "replyto": "SJzSgnRcKX", "review": "This paper provides new insights on what is captured contextualized word embeddings by compiling a set of \u201cedge probing\u201d tasks.  This is not the first paper to attempt this type of analysis, but the results seem pretty thorough and cover a wider range of tasks than some similar previous works.  The findings in this paper are very timely and relevant given the increasing usage of these types of embeddings.  I imagine that the edge probing tasks could be extended towards looking for other linguistic attributes getting encoded in these embeddings.\n\nQuestions & other remarks:\n-The discussion of the tables and graphs in the running text feels a bit condensed and at times unclear about which rows are being referred to.\n-In figures 2 & 3: what are the tinted areas around the lines signifying here? Standard deviation?  Standard error?  Confidence intervals?\n-It seems the orthonormal encoder actually outperforms the full elmo model with the learned weights on the Winograd Schema.  Can the authors comment on this a bit more?\n", "title": "Nice discussion of what type of information is actually encoded by contextualized word embeddings", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJx0hgbKnX": {"type": "review", "replyto": "SJzSgnRcKX", "review": "I have no major complaints with this work.  It is well presented and easily understandable. I agree with the claim that the largest gains are largely syntactic, but this leads me to wonder about more tail phenomena.   PP attachment is a classic example of a syntactic decision requiring semantics, but one could also imagine doing a CCG supertagging analysis to see how well the model captures specific long-tail phenomena.  Though a very different task Vaswani et al 16, for example, showed how bi-LSTMs were necessary for certain constructions (presumably current models would perform much better and may capture this information already).\n\nAn important caveat of these results is that the evaluation (by necessity) is occurring in English.  Discourse in a pro-drop language would presumably require longer contexts than many of these approaches currently handle.", "title": "Current work reps capture a surprising amount of structure", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}