{"paper": {"title": "Transfer of View-manifold Learning to Similarity Perception of Novel Objects", "authors": ["Xingyu Lin", "Hao Wang", "Zhihao Li", "Yimeng Zhang", "Alan Yuille", "Tai Sing Lee"], "authorids": ["sean.linxingyu@pku.edu.cn", "hao.wang@pku.edu.cn", "zhihaol@andrew.cmu.edu", "yimengzh@andrew.cmu.edu", "alan.yuille@jhu.edu", "tai@cnbc.cmu.edu"], "summary": "DCNN trained with multiple views of the same object can develop human-like perpetual similarity judgment that can transfer to novel objects", "abstract": "We develop a model of perceptual similarity judgment based on re-training a deep convolution neural network (DCNN) that learns to associate different views of each 3D object to capture the notion of object persistence and continuity in our visual experience. The re-training process effectively performs distance metric learning under the object persistency constraints, to modify the view-manifold of object representations. It reduces the effective distance between the representations of different views of the same object without compromising the distance between those of the views of different objects, resulting in the untangling of the view-manifolds between individual objects within the same category and across categories. This untangling enables the model to discriminate and recognize objects within the same category, independent of viewpoints. We found that this ability is not limited to the trained objects, but transfers to novel objects in both trained and untrained categories, as well as to a variety of completely novel artificial synthetic objects. This transfer in learning suggests the modification of distance metrics in view- manifolds is more general and abstract, likely at the levels of parts, and independent of the specific objects or categories experienced during training. Interestingly, the resulting transformation of feature representation in the deep networks is found to significantly better match human perceptual similarity judgment than AlexNet, suggesting that object persistence could be an important constraint in the development of perceptual similarity judgment in biological neural networks.\n", "keywords": ["Deep learning", "Transfer Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes a model for multi-view learning that uses a triplet loss to encourage different views of the same object to be closer together then the views of two different objects. The technical novelty of the model is somewhat limited, and the reviewers are concerned that experimental evaluations are done exclusively on synthetic data. The connections to human perception appear interesting. Earlier issues with missing references to prior work and comparisons with baseline models appear to have been substantially addressed in revisions of the paper. We strongly encourage the authors to further revise their paper to address the remaining outstanding issues mentioned above."}, "review": {"HyaBSx9Ul": {"type": "rebuttal", "replyto": "B1gtu5ilg", "comment": "We thank all the official and unofficial reviewers for their extremely useful suggestions and are encouraged by the positive feedbacks. \n\nFollowing these suggestions, we have made a number of revisions and uploaded a new version of our manuscript, including adding important references, baselines and an experiment showing performance of features from different layers in Appendix B. We have colored the modifications red in order to provide an easier way to track the changes from the original submission.", "title": "General Responses to the Reviewers:"}, "S1rYdlqUl": {"type": "rebuttal", "replyto": "SyEdJEGEe", "comment": "Thank you for your helpful comments and suggestions.\n\nFirst, the reason we use tree-to-tree distance comparison in 3.1 is that it is hard to obtain the ground truth of human judgment based on the pair-wise similarity judgment. A hierarchical clustering is easier and more accurate for a human to give his judgment.\n\nNext, we would like to address reviewer\u2019s suggestion of tuning down the connection between our work and human vision. \n\nQuoting reviewer 2:\n>\u201cI think the paper goes too far in linking itself to human vision. I would prefer the intro not have as much cognitive science or neuroscience. The second to fourth paragraphs of the intro in particular feels like it over-stating the contribution of this paper as somehow revealing some truth about human vision. Really, the narrative is much simpler -- \"we often want deep feature representations that are viewpoint invariant. We supervise a deep network accordingly. Humans also have some capability to be viewpoint invariant which has been widely studied [citations]\". I am skeptical of any claimed connections bigger than that.\u201d\n\n>\u201cI don't think \"Remarkably\" is justified in the statement \"Remarkably, we found that OPnets similarity judgment matches a set of data on human similarity judgment, significantly better than AlexNet\"\n\nThe motivation of our work articulated in the Introduction (including paragraph 2 and 4) is truly our motivation for starting and doing this project.  Our objective is to investigate what will take for a deep network to develop the ability of similarity grouping and judgment of NOVEL objects or classes of objects not in the training set as in Tenanbaum\u2019s Science paper, and then we hit on the idea of using Siamnese triplet to evaluate whether the object continuity / object persistence constraint hypothesized by theoretical neuroscientists can lead to network that better approximates the performance of human. That view-manifold training will lead to similarity perception close to human is a conjecture that we seek to empirically investigate. Thus, we do find it remarkable that this view-invariant learning allows the network to \u201csee\u201d the similarity of novel objects in a way that is similar to human (as in Tenenbaum\u2019s results), which we must say it is rather unexpected. As we moved on this project, we realized there were similar works in the computer vision works, including those pointed out by reviewer 1 that we were not familiar with at the time, and our work became more technically driven and phrased. Even though we used the image retrieval task and precision-recall metrics for evaluation, we should not lose sight that our goal is to understand how systems can learn to see similarity in novel objects like human. \n\nThat said, we do agree with reviewer 3, the main evidence that directly supported our conjecture is the correlation with Tenenbaum\u2019s results on human perceptual similarity grouping, though we assumed different views of the same object, and between objects of the same categories are considered more similar at semantic level (e.g. Erdogen et al. 2014, Goldstone and Day 2013 see references below ). Nevertheless, we thank the reviewer\u2019s cautionary note and accordingly tuned down our claim on the possible implications of this work on human similarity judgment particularly in the Final Discussion.  However, we wish to keep our motivation in the introduction, because (1) that is the motivation of our project, (2) we believe there are some deep links between deep network and human visual system, so it is important to discuss them and explore their interesting connections, particularly in a deep learning conference like ICLR. \n\n[1] Quian Quiroga, R., Reddy, L., Kreiman, G., Koch, C. & Fried, I.\u00a0\u00a0Invariant visual representation by single neurons in the human brain.\u00a0Nature\u00a0 435, 1102\u20131107 (2005).\n\n[2]\u00a0 Erdogen G, Yildirim and Jacobs R. (2014) Transfer of object shape knowledge across visual and haptic modalities. http://www.mit.edu/~ilkery/papers/ErdoganYildirimJacobs_CogSci.pdf\u00a0\n\n[3]  Goldstone, R. L., & Day, S. B. (2013). \u00a0Similarity. In H. Pashler (Ed.) The Encyclopedia of Mind. SAGE Reference: Thousand Oaks, CA.\u00a0(pp. 696-699)\n", "title": "Response"}, "SyKoKxcUl": {"type": "rebuttal", "replyto": "SJFcgXEEl", "comment": "We thank reviewer 1 for pointing us to many related papers in the field. We have considered those papers and acted on them, and made three revisions per your suggestions. \n\n1. We compare our approach with the joint embedding approach discussed in Su et al\u2019s SIGGRAPH paper, using a model pre-trained on the chair category, provided by the authors. We test this on the chair category of ShapeNet and found that our OPNet outperformed this model on the instance retrieval task by a large margin. See Figure 2b in the paper.  There may be two reasons that the joint embedding approach does not perform as well as the OPNet. 1) Su et al. used the Light Field Descriptor(LFD) as anchor points for the joint embedding and used CNN only for image purification. So the distance among the LFD may not serve well for good similarity judgment; 2) Their application is more about recognizing 3D shapes from cluttered background, and we test the images without background. When Su et al. did compare the joint embedding approach with the Siamese network, the models were trained and tested with backgrounds which caused confusion to the Siamese network.  \n\nHowever, an important distinction between OPNet and Su et al\u2019s work is that our goal is to understand transferability of view-manifold learning to novel objects and novel categories that the system has not seen before. When tested with novel categories, such as synthesized objects or Pokemon (Figure 2c,d,e), the joint embedding approach does not perform well. The joint embedding approach needs to train the network for each specific category while our network can transfer view-manifold learning from learned categories to completely novel categories. Thus the joint embedding approach performs much worse on novel categories than our approach.  We would like to emphasize that the most novel contribution of our work is the transferability aspect of the learned view-manifold. Prior to our work, it is not clear that training with objects in one class will necessarily transfer the similarity judgment of objects to other classes, particularly to classes of unreal objects. The fact, on instance retrieval of \u201cnovel objects\u201d, i.e. given a novel object, find similar novel objects (assuming views of the same object, or objects of the same class would be considered similar),  OPNet can generalize much better than AlexNet and Joint Embedding, suggesting strong transferability of the learned view-manifold across object classes, which we attributed to the learning of some underlying universal parts. \n\n2. We have also evaluated OPNet in the instance retrieval task using features from different layers, as suggested by reviewer 1. We have explored both Euclidean distance and cosine distance for the AlexNet for comparison and found cosine distance worked better than Euclidean distance, so we compared the performance of the different layers using cosine distance. The results are shown in Table 1 as well as in the Appendix B in the paper. We found that feature representation in deeper layers give better results, except for the fc8 layer of AlexNet which indicated object categories. \n\n3. We have added references on related works in learning a generative 3D representation. Although all the references are related to our work in some technical aspects and goals, there is an important distinction.  Even though we used instance retrieval as a metric for evaluation technically, and might appear to be just like other computer vision work, the motivation of our project is to investigate what will take for a deep network to develop the ability of human-like similarity judgment of NOVEL objects or novel classes of objects not in the training set as in Tenanbaum\u2019s Science paper. The other works while related, did not investigate the transferability of view-manifold learning to different novel objects or novel classes of objects. Obviously, Siamese network and triplet cost is not new, and image retrieval of different views of 3D objects has also been intensely worked on. The novelty of our contribution is in the transferability of the view-manifold learning based on object persistence principle that allows our network to relate one novel object to other novel objects that we have never seen before. Image retrieval is just a method and precision-recall a metrics we used to measure this similarity judgment ability more systematically and comprehensively.\n", "title": "Additional Baselines and Experiments"}, "ryAEDecUg": {"type": "rebuttal", "replyto": "BJB-lc-Nl", "comment": "We thank reviewer 3 for his positive reviews. Upon reviewer 3\u2019s advice, we evaluated the performance of our network on the real images in ALOI viewpoint dataset. The dataset contains 1000 different objects, each with different views. In the instance retrieval task, both OPNet and AlexNet exhibited transfer learning, but the improvement of our OPNet relative to AlexNet, is only in the order of 0.5% in the mean average precision.\n\nA problem of this dataset is that it does not have category labels so for the instance retrieval task we have to retrieve different views of the same object from all other 999 objects. OPNet\u2019s superiority over AlexNet lies in its capability of better discriminating among objects of the same category(e.g. discrimination between chair A and chair B, instead of between a chair and a cup). OPnet performs better when the distractors are more similar to the reference object, which is a harder situation. In the ALOI dataset, since the 1000 objects significantly differ from each other, this advantage cannot be brought out and thus OPnet achieves a comparable results with AlexNet.\n\nIn addition, real world images also have other properties such as lighting changes, reflectance and scale changes that are not considered in our training, but object persistence principle can generalize to take care of those nuisance variables. We added comments on this issue in the Final Discussion section of the revised paper. We would indeed want to explore manifold learning to discount varieties of nuisance variables present in real world scenes in future work.", "title": "Response"}, "ryW3C1pmx": {"type": "rebuttal", "replyto": "SJz5kS9Xg", "comment": "Thank you for your question.\n\n1) It is true that utilizing inherent information as supervisory signal has been a popular approach and such object persistency constraint has not been used explicitly to the best of our knowledge. We have actually cited Wang and Gupta\u2019s works, and in fact used the same Triplet Siamese network. In Wang and Gupta\u2019s work \u201cUnsupervised Learning of Visual Representations using Videos\u201d, they use visual tracking in the videos as supervision, forcing a similar representation for tracked patches of the same object. In a way, they did implicitly assume object persistence. But the change in views tend to be very limited in their case. Their main contribution was in demonstrating that they can learn \u201cfeature representation\u201d based on optical flow without explicit supervising object classification signals. Here, we modify the feature representation of AlexNet by asserting the similarity of different 3D views of the same object. The main contribution here is on \u201ctransferability\u201d of learning.\n\n2) In \"Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge by Zeng et al\u201d, the multi-view images, combined with spatial information, are used to calculate the background pixel, which then serves as a supervision for foreground segmentation and thus is different to our object persistency constraint.\n\nWe appreciate your advice for the clarification and we added the discussion on recent weak supervised works at the end of the introduction. Please see the revised version.", "title": "Response"}, "HkiADkaXl": {"type": "rebuttal", "replyto": "ryvQZcDQx", "comment": "Thank you for the suggestion. In our paper, we applied the t-SNE for visualization of the similarity judgement based on the pairwise euclidean distance of the output feature vector, to demonstrate that our training with object persistence constraint is effective in separating multi-views of the same object with other objects. As I see it, the t-STE approach deals with the situation where the pairwise similarity judgement is not available and only triplet constraints are provided(a is more similar to b than to c). As our pairwise similarity judgement can be easily evaluated, the t-STE should not be necessary for the visualization. On the other hand, t-STE could potentially be another way to utilize the object persistency constraint, in contrast to the distance metric learning.", "title": "Response"}, "SJz5kS9Xg": {"type": "review", "replyto": "B1gtu5ilg", "review": "This paper strikes me as similar to the recent trend of \"unsupervised\" learning, but the paper is not really motivated in that way. The similar work of Wang and Gupta isn't mentioned in the intro, nor are many other recent works using weak supervision signals to train deep networks. It would be nice to hear more about the relationship of this work to those works. If object persistence hasn't been used as a supervision signal before then great. Although Wang and Gupta (and maybe others in this space) are explicitly motivated by object persistence -- their narrative is that it is useful for a network to try and learn that views of a 3d object as it rotated are still the same object.\n\nMany of these works are robotics applications, e.g. Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge, Zeng et al. Section 6. \n\nAnyway, it would be nice to clarify how this work relates to these \"supervision from multiple views of the same objects\" works.I think learning a deep feature representation that is supervised to group dissimilar views of the same object is interesting. The paper isn't technically especially novel but that doesn't bother me at all. It does a good job exploring a new form of supervision with a new dataset. I'm also not bothered that the dataset is synthetic, but it would be good to have more experiments with real data, as well. \n\nI think the paper goes too far in linking itself to human vision. I would prefer the intro not have as much cognitive science or neuroscience. The second to fourth paragraphs of the intro in particular feels like it over-stating the contribution of this paper as somehow revealing some truth about human vision. Really, the narrative is much simpler -- \"we often want deep feature representations that are viewpoint invariant. We supervise a deep network accordingly. Humans also have some capability to be viewpoint invariant which has been widely studied [citations]\". I am skeptical of any claimed connections bigger than that.\n\nI think 3.1 should not be based on tree-to-tree distance comparisons but instead based on the entire matrix of instance-to-instance similarity assessments. Why do the lossy conversion to trees first? I don't think \"Remarkably\" is justified in the statement \"Remarkably, we found that OPnets similarity judgement matches a set of data on human similarity judgement, significantly better than AlexNet\"\n\nI'm not an expert on human vision, but from browsing online and from what I've learned before it seems that \"object persistence\" frequently relates to the concept of occlusion. Occlusion is never mentioned in this paper. I feel like the use of human vision terms might be misleading or overclaiming.\n\n", "title": "Similarity to \"unsupervised\" feature learning", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyEdJEGEe": {"type": "review", "replyto": "B1gtu5ilg", "review": "This paper strikes me as similar to the recent trend of \"unsupervised\" learning, but the paper is not really motivated in that way. The similar work of Wang and Gupta isn't mentioned in the intro, nor are many other recent works using weak supervision signals to train deep networks. It would be nice to hear more about the relationship of this work to those works. If object persistence hasn't been used as a supervision signal before then great. Although Wang and Gupta (and maybe others in this space) are explicitly motivated by object persistence -- their narrative is that it is useful for a network to try and learn that views of a 3d object as it rotated are still the same object.\n\nMany of these works are robotics applications, e.g. Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge, Zeng et al. Section 6. \n\nAnyway, it would be nice to clarify how this work relates to these \"supervision from multiple views of the same objects\" works.I think learning a deep feature representation that is supervised to group dissimilar views of the same object is interesting. The paper isn't technically especially novel but that doesn't bother me at all. It does a good job exploring a new form of supervision with a new dataset. I'm also not bothered that the dataset is synthetic, but it would be good to have more experiments with real data, as well. \n\nI think the paper goes too far in linking itself to human vision. I would prefer the intro not have as much cognitive science or neuroscience. The second to fourth paragraphs of the intro in particular feels like it over-stating the contribution of this paper as somehow revealing some truth about human vision. Really, the narrative is much simpler -- \"we often want deep feature representations that are viewpoint invariant. We supervise a deep network accordingly. Humans also have some capability to be viewpoint invariant which has been widely studied [citations]\". I am skeptical of any claimed connections bigger than that.\n\nI think 3.1 should not be based on tree-to-tree distance comparisons but instead based on the entire matrix of instance-to-instance similarity assessments. Why do the lossy conversion to trees first? I don't think \"Remarkably\" is justified in the statement \"Remarkably, we found that OPnets similarity judgement matches a set of data on human similarity judgement, significantly better than AlexNet\"\n\nI'm not an expert on human vision, but from browsing online and from what I've learned before it seems that \"object persistence\" frequently relates to the concept of occlusion. Occlusion is never mentioned in this paper. I feel like the use of human vision terms might be misleading or overclaiming.\n\n", "title": "Similarity to \"unsupervised\" feature learning", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryvQZcDQx": {"type": "review", "replyto": "B1gtu5ilg", "review": "Van der Maaten had a followup work to t-SNE (with Weinberger) called t-STE (t-Distributed Stochastic Triplet Embedding). It seems like it could be useful here, at least for visualization. Were you aware of it, and did you consider using it?On one hand this paper is fairly standard in that it uses deep metric learning with a Siamese architecture. On the other, the connections to human perception involving persistence is quite interesting. I'm not an expert in human vision, but the comparison in general and the induced hierarchical groupings in particular seem like something that should interest people in this community. The experimental suite is ok but I was disappointed that it is 100% synthetic. The authors could have used a minimally viable real dataset such as ALOI http://aloi.science.uva.nl .\n\nIn summary, the mechanics of the proposed approach are not new, but the findings about the transfer of similarity judgement to novel object classes are interesting.", "title": "applicability of t-STE", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJB-lc-Nl": {"type": "review", "replyto": "B1gtu5ilg", "review": "Van der Maaten had a followup work to t-SNE (with Weinberger) called t-STE (t-Distributed Stochastic Triplet Embedding). It seems like it could be useful here, at least for visualization. Were you aware of it, and did you consider using it?On one hand this paper is fairly standard in that it uses deep metric learning with a Siamese architecture. On the other, the connections to human perception involving persistence is quite interesting. I'm not an expert in human vision, but the comparison in general and the induced hierarchical groupings in particular seem like something that should interest people in this community. The experimental suite is ok but I was disappointed that it is 100% synthetic. The authors could have used a minimally viable real dataset such as ALOI http://aloi.science.uva.nl .\n\nIn summary, the mechanics of the proposed approach are not new, but the findings about the transfer of similarity judgement to novel object classes are interesting.", "title": "applicability of t-STE", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}