{"paper": {"title": "CoT: Cooperative Training for Generative Modeling of Discrete Data", "authors": ["Sidi Lu", "Lantao Yu", "Siyuan Feng", "Yaoming Zhu", "Weinan Zhang", "Yong Yu"], "authorids": ["steve_lu@apex.sjtu.edu.cn", "yulantao@apex.sjtu.edu.cn", "siyuanfeng@apex.sjtu.edu", "ymzhu@apex.sjtu.edu.cn", "wnzhang@apex.sjtu.edu.cn", "yyu@apex.sjtu.edu.cn"], "summary": "We proposed Cooperative Training, a novel training algorithm for generative modeling of discrete data.", "abstract": "We propose Cooperative Training (CoT) for training generative models that measure a tractable density for discrete data. CoT coordinately trains a generator G and an auxiliary predictive mediator M. The training target of M is to estimate a mixture density of the learned distribution G and the target distribution P, and that of G is to minimize the Jensen-Shannon divergence estimated through M. CoT achieves independent success without the necessity of pre-training via Maximum Likelihood Estimation or involving high-variance algorithms like REINFORCE. This low-variance algorithm is theoretically proved to be superior for both sample generation and likelihood prediction. We also theoretically and empirically show the superiority of CoT over most previous algorithms in terms of generative quality and diversity, predictive generalization ability and computational cost.", "keywords": ["Generative Models", "Sequence Modeling", "Text Generation"]}, "meta": {"decision": "Reject", "comment": "The paper proposes an original and interesting alternative to GANs for optimizing a (proxy to) Jensen-Shannon divergence for discrete sequence data. Experimental results seem promising. Official reviewers were largely positive based on originality and results. However, as it currently stands, the paper still makes false claims that are not well explained or supported, in particular its repeated central claim to provide a \"low-variance, bias-free algorithm\" to optimize JS.  Given that these central issues were clearly pointed out in a review from a prior submission of this work to another venue (review reposted on the current OpenReview thread on Nov. 6), the AC feels that the authors had had plenty of time to look into them and address them in the paper, as well as occasions to reference and discuss relevant related work pointed in that review. The current version of the paper does neither. The algorithm is not unbiased for at least two reasons pointed out in discussions: a) in practice a parameterized mediator will be unable to match the true P+G, at best yielding a useful biased estimate (not unlike how GAN's parameterized discriminator induces bias). b) One would need to use REINFORCE (or similar) to get an unbiased estimate of the gradient in Eq. 13, a key detail omitted from the paper. From the discussion thread it is possible that authors were initially confused about the fact that this fundamental issue did not disappear with Eq. 13 (they commented \"most important idea we want to present in this paper is HOW TO avoid incorporating REINFORCE. Please refer to Eq.13, which is the key to the success of this.\"). But rather, as guessed by a commentator, that a heuristic implementation, not explained in the paper, dropped the REINFORCE term thus effectively trading variance for bias. \nOn December 4th authors posted a justification confirming heuristically dropping the REINFORCE terms when taking the gradient of Eq. 13, and said they could attach detailed analysis and experiment results in the camera-ready version.  However if one of the \"most important idea\" of the paper is how to avoid REINFORCE (as still implied and highlighted in the abstract), the AC finds it worrisome that the paper had no explanation of when and how this was done, and no analysis of the bias induced by (unreportedly) dropping the term. \n\nThe approach remains original, interesting, and potentially promising, but as it currently stands, AC and SAC agreed that inexact theoretical over-claiming and insufficient justification and in-depth analysis of key heuristic shortcuts/tradeoffs (however useful) are too important for their fixing to be entrusted to a final camera-ready revision step. A major revision that clearly adresses these issues in depth (both in how the approach is presented and in supporting experiments) will constitute a much more convincing, sound, and impactful research contribution.\n\n"}, "review": {"SJefME_ZeN": {"type": "rebuttal", "replyto": "rkgfHtIWx4", "comment": "Thank you for your advice.\n\n1. The paper was revised in the revision period to address most of the comments on experiments.  We consider it now to be more solid against the previous comments and criticisms. You may want to re-check it.\n\n2. We agree, as we promise the corresponding discussion would be included in the final version of the paper.\n\n3. Actually, the experiment of robustness (see Sec 4.1.1) shows that in pracice, whether the mediator is trained to be optimal is not so important. See Fig2(b) and its legend. We state that the algorithm still works well under a variety of g-m balancing settings. In the extreme case, even if the mediator is remarkably less trained (g-steps=3, m-steps=1), the algorithm still works well and suceeds to converge.\n\nWe appreciate your comments, as we promise that these content would definitely be added to the final version of the paper.", "title": "Response to response"}, "Bygvu1w2jm": {"type": "review", "replyto": "SkxxIs0qY7", "review": "Pros:\nThis paper is easy to follow. The idea is nice in three folds. \n1. By changing the auxiliary model's role from a discriminator to a mediator, it directly optimizes the JSD measure, which is a symmetrized and smoothed version of KL divergence.  \n2. Moreover, the mediator and the generator follow similar predictive goals, rather than the opposite  goals of G and D in GANs. \n3. For discrete sequential data, it avoids approximating expected rewards using Markov rollouts.  \n \nCons:\nSome details are missing in the experiments. \n1. In Table 2 of [A], LeakGAN, SeqGAN and RankGAN all show significantly better performances in terms of BLEU on EMNLP2017 WMT, compared to results reported in Table 3 of the submission. Any difference?\n2. The Word Mover Distance is computed by training a discriminator, which could be unstable. Could you provide other metrics to evaluate diveristy like self-bleu?\n\n[A] Guo, Jiaxian, et al. \"Long text generation via adversarial training with leaked information.\" arXiv preprint arXiv:1709.08624 (2017).\n\nMisc:\n1. How will the number of samples (i.e. batch size) affect CoT ?\n2. How is the applicability of CoT for continuous data? It seems to me there is no theoretical difficulties to apply CoT on continuous data.", "title": "nice idea", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1eZN4i4JN": {"type": "rebuttal", "replyto": "Skx7dQKjC7", "comment": "To get better reading experience, you may want to use a LaTeX compiler.\n\nEq 13:\n\\begin{align*}\n&\\nabla_\\theta J_g(\\theta)\\\\\n=&\\nabla_\\theta(\\sum_{t=0}^{n-1}\\mathop{\\mathbb{E}}_{s_t \\sim G_\\theta} \\pi_g(s_t)^T (\\log \\frac{\\pi_m(s_t)}{\\pi_g(s_t)})\n\\end{align*}\n\nFor time step $t$, each term of Eq 13 equals to:\n\\begin{align*}\n&\\nabla_\\theta J_{g, t}(\\theta)\\\\\n=&\\nabla_\\theta \\left[ \\mathop{\\mathbb{E}}_{s_t \\sim G_\\theta} \\pi_g(s_t)^T(\\log \\frac{\\pi_m(s_t)}{\\pi_g(s_t)})\\right]\\\\\n=&\\nabla_\\theta \\left[ \\sum_{s_t} G_\\theta(s_t) (\\pi_g(s_t)^T(\\log \\frac{\\pi_m(s_t)}{\\pi_g(s_t)})) \\right]\\\\\n=&\\sum_{s_t} \\nabla_\\theta \\left[ G_\\theta(s_t) (\\pi_g(s_t)^T(\\log \\frac{\\pi_m(s_t)}{\\pi_g(s_t)})) \\right]\n\\end{align*}\nLet $L(s_t) = \\pi_g(s_t)^T(\\log \\frac{\\pi_m(s_t)}{\\pi_g(s_t)})$\n\n\\begin{align*}\n  &\\nabla_\\theta J_{g, t}(\\theta) \\\\\n  =&\\sum_{s_t}(\\frac{\\partial G_\\theta (s_t)}{\\partial \\theta} L(s_t) + G_\\theta(s_t) \\frac{\\partial L(s_t)}{\\partial \\theta}) \\\\\n  =&\\sum_{s_t}(G_\\theta(s_t)(\\frac{\\partial \\log G_\\theta(s_t)}{\\partial \\theta}L(s_t) + \\frac{\\partial L(s_t)}{\\partial \\theta})) \\\\\n  =&\\mathop{\\mathbb{E}}_{s_t \\sim G_\\theta} \\nabla_\\theta (\\text{stop\\_gradient}(L(s_t))\\log G_\\theta(s_t) + L(s_t))\n\\end{align*}\n\nAs you may notice, the total gradient in each step consists of two terms. The first term $(\\text{stop\\_gradient}(L(s_t))\\log G_\\theta(s_t)$ behaves like REINFORCE, which introduces variance to the optimization process. The second non-REINFORCE term is comparitively less noisy, though for the first sight it seems not to be working alone.\n\nIf we think about the effects of the two terms, we may notice that they have similar optimization directions (towards minimization of $KL(G_\\theta || M_\\phi)$ ). Thus, empirically, basic version of CoT introduces an extra hyperparameter $\\gamma \\in [0, 1]$, to control the balance of the high-variance first term and low-variance second term. The objective in each time step thus becomes:\n\\begin{align*}\n  &\\nabla_\\theta J^{\\gamma}_{g, t}(\\theta) \\\\\n  =&\\mathop{\\mathbb{E}}_{s_t \\sim G_\\theta} \\left[ \\nabla_\\theta \\gamma (\\text{stop\\_gradient}(L(s_t))\\log G_\\theta(s_t) + L(s_t)) \\right]\n\\end{align*}\n\nHowever, in practice we find that in all our attempts, the algorithm works best when $\\gamma = 0.0$. Thus, we directly drop the REINFORCE term. If you think it necessary, we would attach detailed analysis and experiment results in the camera-ready version.", "title": "Detailed analysis of why REINFORCE is dropped"}, "BklMQW7oRQ": {"type": "rebuttal", "replyto": "rygI5D8q0Q", "comment": "We've had several updates to address the arguments. Please refer to the submitted version of the paper.", "title": "Please refer to the submitted version of the paper, instead of the preprint version."}, "BklAEn3K67": {"type": "rebuttal", "replyto": "SkxxIs0qY7", "comment": "The major updates include:\n1. Most of the mentioned typos are fixed.\n2. The color scheme is changed for better readability in gray scale printing.\n3. Standard deviation of the calculated eWMD is provided, in order to make the comparison statistically sound. ", "title": "Paper Revised by the Authors"}, "B1l29bRkpm": {"type": "rebuttal", "replyto": "HkgJzPJP2X", "comment": "Thanks for reviewing our paper!\n\nResponse to your concerns:\n\nWe will provide with a carefully-revised version of the paper according to your generous suggestions.\n\nAnswer to the questions:\n1. If there is no constraint on the entropy of the solution of the objective function, the objective would not be equivalent to minimization of JSD. Instead, it would simply be calculating the entropy of M, which is useless.\n\n2. Figure 2 (a)(b) shows CoT is more robust under its main evaluation to g-m balance compared to the g-d balance of SeqGAN. Ideally, Figure 2(a) should also be showing SeqGAN's performance under evaluation of JSD, however, in our attempts, SeqGAN always diverges under such evaluation. Figure 2 (c) show that the convergence of CoT is steady and quite fast under evaluation of NLL_{oracle}, which is biased on quality. ", "title": "Response to AnonReviewer2"}, "HJl2Eka7pX": {"type": "rebuttal", "replyto": "r1eMNq-faQ", "comment": "One good property of CoT is that its entropy estimation is more accurate than that of MLE.\u00a0\n\nAs is analyzed in some previous work (e.g. Ian's GAN Tutorial in NIPS 2016), models trained to optimize forward KL, which is equivalent to MLE's objective, tend to over-estimate the entropy of the data.\n\nIn our opinion, the ability of correctly estimating data entropy is important for discrete generative models, since in practice, the entropy of the data is not directly available. For real data, the entropy of the model is not a hyperparameter, but a trainable parameter learned via gradient descent. The manipulation of temperature may help in producing better samples at inference, but it is not a reason to get satisfied in merely doing so.\n\nBesides, your synthetic experiment results with MLE and CoT is a bit different from ours, where CoT performs worse and MLE performs remarkably better than that in ours. In our observed results, LMs trained via MLE almost always tends to overfit quickly after about 40 epochs. Without adopting the training techniques you've incorporated in your repo (i.e. Variational Dropout and Many-fold Cross-validation), it is difficult to reproduce your results about MLE, especially for a naive one. While all other models share the same training/testing framework in your code, CoT is absent and instead you used our unfinished repository. \n\nThus, the comparison seems a little bit unfair. While the training techniques for LMs with MLE are well-studied for years, there is much space for investigation of that for LMs trained via CoT (and, of course, discrete GANs). As a result, in our opinion, your experiment shows that CoT with current progress is capable of obtaining comparable results to a well regularized MLE-LM, and even better under a range of entropy settings, where NLL_{oracle} ranges from about 7.1 to 8.8. Please notice that the estimated entropy of the data by CoT given limited observation (10000 samples) lies in such a range. We admit that we are not sure about the reason why CoT cannot keep such advantages in marginal entropy settings (lower and/or higher), but as an educated guess, it may due to that it is more easy for MLE to memorize the training samples while CoT enforces the network to explore-and-improve. As a consequence, if the mediator is not strong enough (i.e. not perfectly matching the assumptions in our theory), the trained model may behave slightly worse in extreme cases.\n\nHowever, despite these minor arguments, in general, we agree with the opinions in your paper and that there needs to be a revolution in the field of language GAN researches before it actually becomes fruitful. Good job!", "title": "Response to ''Language GANs Falling Short''"}, "S1xBB06y6Q": {"type": "rebuttal", "replyto": "SJx1up316X", "comment": "Thanks for your attention!\n\nWe have seen your review before when submitted to a previous conference and here we would like to make an official response to it.\n\nIn case you haven't noticed, we want to emphasize that this version of the paper is quite different from the version you've already seen, especially in the parts you've mentioned.\n\nResponse to the first part of the review\n\nThe reviewer makes the claims that using the mediator is not necessary and the proposed algorithm still incorporates REINFORCE algorithm. According to our submitted version of the paper, however, our presented algorithm disagrees with either of the claims.\u00a0\n\nWe are confused why the reviewer claims so since one of the most important idea we want to present in this paper is HOW TO avoid incorporating REINFORCE. Please refer to Eq.13, which is the key to the success of this. The paragraphs around Eq.13 describe our approach in details.\n\nFor the necessity of the mediator, please check Sec 3.4.2, which is quite different from the version you have read. Your suggested version of the model may not be practically implementable because it cannot provide a probability prediction IN EACH TIMESTEP, which is very important if this module is to be used by CoT. Notice that M(x|s_t) is not equal to (G(x|s_t) + P(x|s_t)) / 2, making the factorization actually non-trivial.\n\nResponse to the second part of the review\n\nBefore this version of the paper is submitted to ICLR, we have deleted Theorem 4 in an earlier version of the paper as we consider your concerns about it to be correct. (thanks!) In our current version, we treat Mediator as an IMPLICIT estimator of JSD. The reason why such estimation is implicit is that the calculation of entropy of the input data is non-trivial. However, if you pay attention to Figure 3 and the paragraphs around it, we've shown balanced NLL, which, in theory, is actually only different in a constant from the estimated JSD. In practice, as the model's estimation of data entropy also improves, such difference may also change steadily as the training proceeds.\n\nAbout the behavior of the model when the mediator is not trained to optimality, we have empirically shown that it is still stable. Note that GANs also do not have such a guarantee, and in practice it is much more unstable than our approach. Please refer to Figure 2(b).\n\n\nWe have collected samples from three typical models and shown them in the appendix. Please also check it.\n\nResponse to minor comments:\n\nWe appreciate your suggestions. However, as the paper has length limit, we are not able to cover all aspects. We will consider your suggestions seriously and incorporate them as much as possible.", "title": "Reply to \"major concerns with algorithm and evaluation\""}, "BklsMW0kpX": {"type": "rebuttal", "replyto": "Bygvu1w2jm", "comment": "Thanks for reviewing our paper!\n\nResponse to your concerns:\n\n1. We have contacted one of the authors of the LeakGAN, finding that the data pre-processing and post-processing of ours and theirs are different. This makes the results quite different.\n2. We will present with an error bar in the coming revised version.\n\nResponse to Misc:\n\n1. This is an interesting topic, we would have some discussion about it if we have found interesting conclusions.\n\n2. We've actually implemented a continous version of CoT, of which the prior distribution is replaced by Beta Distribution instead of Multinomial Distribution in the current discrete version of CoT. However such a model does not perform well. This is an interesting direction for further research and survey.", "title": "Reply to ICLR AnonReviewer1"}, "rJelZVAJpX": {"type": "rebuttal", "replyto": "r1lfAuiyaQ", "comment": "Thanks for reviewing our paper.\n\n1. For reproducibility, we are preparing for a open-source code base. After the paper is de-anonymized, we will attach a link to it.\n\n2. Before the paper of CoT is completed, we have had attempts at several different divergences, including JSD(CoT), Reverse KL(as is described in the appendix), Wasserstein-1 distance, etc. However, only CoT and Reverse KL succeed in getting rid of pre-training via MLE. Reverse KL appears to have mode collapsing problem, therefore CoT is finally the chosen model. However, this is a good direction for further research. We are also interested.", "title": "Response to AnonReviewer3"}, "r1lfAuiyaQ": {"type": "review", "replyto": "SkxxIs0qY7", "review": "*Summary*\nA clear an interresting presentation on learning sequences distributions. It achieve this objective by replacing the discriminator with a \"mediator\", a mixture between the training distribution and the target distribution which is estimated via maximum likelihood.\n\n*Pros*\n- Original idea for modelling distribution of sequence data\n- Theoretical convergence in the Jensen Shanon divergence sense\n- Promising experiments\n\n*Cons*\n- No major cons to the best of my knowledge\n\n*Typos*\n- It would be very nice to have black and white / color blind friendly graphs\n- Eq 10 too long\n- Introduce J_m & J_g in  sentence\n- Coma at the end of Eq 5, and maybe align Generator and Discriminator in some position (e.g. at the semi colon).\n- missing dot at Eq 8.\n\n*Question*\n- How would you ensure reproducibility (e.g. link to some code?)\n- Is there any hope to obtain consistency (convergence) wrt other metrics?", "title": "Original idea, clear presentation", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "HkgJzPJP2X": {"type": "review", "replyto": "SkxxIs0qY7", "review": "The paper proposes an interesting method, where the discriminator is replaced by a component that estimates the density that is the mixture of the data and the generator's distributions. In a sense, that component is only a device that allows estimating a Jensen-Shannon divergence for the generator to then be optimized against. Other GAN papers have replaced their discriminator by a similar device (e.g., WGANs, ..), but the present formulation seems novel. The numerical experiments presented on a synthetic Turing test and text generation from EMNLP's 2017 news dataset appear promising. \n\nOverall, the mediator seems to allow to achieve lower Jensen-Shannon (JS) divergence values in the experiments (and is kind of designed for that). Although this may be an improvement with respect to existing methods for discrete sequential data, it may also be limited in that it may not easily extend to other types of divergences that have proved superior to JS in some continuous settings.\n\nThe paper is rather clear, although there are lots of small grammatical errors as well as odd formulations which end up being distracting or confusing. The language should be proof-read carefully. \n\nPros:\n- Generative modeling of sequence data still in its infancy\n- Potentially lower variance than policy gradient approaches\n- Experiments are promising\n\nCons:\n- Lots of grammatical errors and odd formulations\n\nQuestions:\n- Equation 14: what does it mean to find the \"maximum entropy solution\" for the given optimization problem?\n- Figure 2: how do (b) and (c) relate to each other?\n\nRemarks, small typos and odd formulations:\n- \"for measuring M_\\/phi\": what does measuring mean in this context?\n- What does small m refer to? Algorithm 1 says the total number of steps  but it is also used in the main text as an index for J and \\pi (for mediator?)\n- Equation block 8: J_m has not been defined yet\n- \"the supports of distributions G and P\"... -> G without subscript has now been defined in this context\n- \"if the training being perfect\"\n- \"tend to get stuck in some sub-optimals\"\n- the learned distribution \"collapseS\"\n- \"since  the data distribution is, thus ...\"\n- \"that measures a\" -> \"that estimates a ...\"?\n- \"a predictive module\": a bit unclear - generative v. discriminative is more usual terminology\n- \"is well ensured\"\n- \"with the cost of diversity\" -> \"at the cost of diversity\"?\n- \"has theoretical guarantee\"\n- in the references: \"ALIAS PARTH GOYAL\" (all caps)\n- \"let p denote the intermediate states\": I don't understand what this is. Where is \"p\" used? (proof of Theorem 3)\n- \"CoT theoretically guarantees the training effectiveness\": what does that mean?\n- Figure 3: \"epochs\" -> \"Epochs\"\n- Algorithm 1: what does \"mixed balanced samples\" mean? Make this more precise\n- \"wide-ranged\"\n- Equation 10 is too long and equation number is not properly formatted\n- Figures hard to read in black & white\n- Figure 2 doesn't use the same limits for the Y axis of the two NLL plots, making comparisons difficult. The two NLL plots are also not side-by-side", "title": "Interesting and promising method for generative modelling of sequence data without policy gradient", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "SJlvFB_anQ": {"type": "rebuttal", "replyto": "BJerU0WThX", "comment": "Thanks for your attention. We would like to explain our considerations when we were writting this part. In the paper of LeakGAN, as is proposed by the authors, a typical setting of LeakGAN shall be:\n\n1. When used for generative purposes, alpha is always set to be 1.5 (as they said ``conservative strategy'').\n2. When used for evaluating NLL or sampling trajectories for reinforcement learning, alpha is always set to be 1.0.\n\nIn other words, in LeakGAN's original paper, they do not guarantee the model would also perform well under the settings as you've described. \n\nHowever, the authors of LeakGAN did update a new version in their official github code base, where the temperature trick is completely removed. We would update a new version with related results when revision is enabled. Thank you.", "title": "Please refer to LeakGAN's original paper"}, "SyxDOevL2Q": {"type": "rebuttal", "replyto": "r1eBMvIInm", "comment": "Please refer to our notations at Sec 2.\nIn this case, for any given sequence S_t, its t-1-length prefix S_{t-1} is unique.", "title": "Please refer to our notations"}, "S1gFX_4NjQ": {"type": "rebuttal", "replyto": "Hyl6DtGNjm", "comment": "If you pay attention to the paper, we've actually provided with data pre-processing details (see page 8 Sec 4.2). The sequence length limit is set to be 51.", "title": "Sequence length information has been already provided, with which reproducibility is actually possible"}, "HJx0MGM4sX": {"type": "rebuttal", "replyto": "BJxIadb4iQ", "comment": "Hi,\n\nWhat we are trying to say is that even if when evaluting the test NLL, pad tokens should not be eliminated so that any unstability of keeping the padding segment consistent (i.e. the predicted likelihood of pad token should be almost always 1.0 since the first pad token is generated) would be detected and penalized by doing so. Such setting does not introduce any unfairness of the comparison, since for all evaluated models, the padded sequence length (max sequence length) is the same. One could not have arbitrarily good NLL performance by doing what you've described, since in our setting it is not allowed to place additional <PAD> after the sequence (otherwise it makes the padded sequence longer than the maximum sequence length limit).\n\nHere is an example of our consideration:\n\nSuppose there are two evaluated models, namely A and B.\n\nEvaluated Sequence: I have a pen . <PAD> <PAD> <PAD>\n\nProbability prediction of each step:\n     I      have     a     pen    .       <PAD>   <PAD>  <PAD>  <PAD>\nA: 0.3    0.5     0.1   0.5    0.7       0.99      0.8         0.999   0.9999\nB: 0.3    0.5     0.1   0.5    0.7       0.99      0.999     0.999   0.9999\n\nWe prefer model B, since if the prefix \"I have a pen . <PAD>\" is given, model B would have almost 1.0 probability to generate a correctly padded sequence, while for model A it would have only 0.8 probability to do so, even if for non-padding part the two models are actually the same.\n\nAs for making it easier for other researchers to make correct comparison, we recommend implementing and evaluating with Texygen, which can automatically deal with these data preprocessing issues.\n\nBest", "title": "PAD tokens should be included in this case even in testing the NLL"}, "rJxfLb-4j7": {"type": "rebuttal", "replyto": "S1xrZlyNjQ", "comment": "Hi,\n\nFor all evalutated models, the padding tokens are included in the calculation of NLL. The reason of doing so is that we observed some model incorrectly generate non-padding tokens even if the model has generated a few padding tokens. The reason may be that in text generation tasks, generated tokens are SAMPLED from each time's probability prediction over the vocabulary instead of directly MAGINALIZED to be the argmax token. This, however, could be considered as an important feature when evaluating different training algorithms.\n\nExample:\nOne typical failure:\n\nA cat is sleeping on the table . <PAD> <PAD> with <PAD> <PAD> <PAD> . <PAD> ......\n\nTo penalize such failures, when calculating the NLL, all paddings are not eliminated. We hope our such consideration makes sense to you.\n\n\n", "title": "PAD tokens are included."}, "Bygfh6S2t7": {"type": "rebuttal", "replyto": "H1lnfB-ntX", "comment": "Thank you for your attention! As we would like to point out, there is one thing special about LeakGAN when compared to other baseline models.  In LeakGAN's default settings,when it is used to generate samples, the temperature of the generator is adjusted to be a little bit lower (i.e. \\alpha = 1.5). However, when LeakGAN is used to compute the predicted NLL, \\alpha will be set to 1.0. To keep the comparison fair, we show results of CoT-strong under different settings (\\alpha = 1.0 and \\alpha = 1.5) to support our claim that:\n\n1. When we set \\alpha = 1.0 i.e. keep the temperature parameters as they originally are, CoT-basic and CoT-strong outperforms baseline models with the same settings, including MLE, SeqGAN, RankGAN and MaliGAN. \n\n2. When \\alpha is set to 1.5, CoT-strong outperforms LeakGAN.\n\nThe proposed CoT does reach the state-of-the-art, since in both cases CoT reaches the state-of-the-art. As for diversity benchmarks, since every evaluated model sets \\alpha to 1.0 in this case, such classified discussion is not necessary.\n\n", "title": "Table 2 and 3 are designed as such to make fair comparison with LeakGAN"}}}