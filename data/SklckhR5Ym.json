{"paper": {"title": "Improved Language Modeling by Decoding the Past", "authors": ["Siddhartha Brahma"], "authorids": ["sidbrahma@gmail.com"], "summary": "Decoding the last token in the context using the predicted next token distribution acts as a regularizer and improves language modeling.", "abstract": "Highly regularized LSTMs achieve impressive results on several benchmark datasets in language modeling. We propose a new regularization method based on decoding the last token in the context using the predicted distribution of the next token. This biases the model towards retaining more contextual information, in turn improving its ability to predict the next token. With negligible overhead in the number of parameters and training time, our Past Decode Regularization (PDR) method achieves a word level perplexity of 55.6 on the Penn Treebank and 63.5 on the WikiText-2 datasets using a single softmax. We also show gains by using PDR in combination with a mixture-of-softmaxes, achieving a word level perplexity of 53.8 and 60.5 on these datasets. In addition, our method achieves 1.169 bits-per-character on the Penn Treebank Character dataset for character level language modeling. These results constitute a new state-of-the-art in their respective settings.", "keywords": ["language modeling", "regularization", "LSTM"]}, "meta": {"decision": "Reject", "comment": "The paper proposes an additional module to train language models, adding\na new loss that tries to predict the previous token given the next one, thus\nenforcing the model to remember the past. Two out of 3 reviewers recommend to\naccept the paper; the third one said it was misleading to claim SOTA since\nauthors didn't try the mixture-of-softmax model that is actually currently SOTA.\nThe authors acknowledged and modified the paper accordingly, and added a few\nmore experiments. The reviewer still thinks the improvements are\nnot important enough to claim significant novelty. Overall, I think the idea is simple and\nadds some structure to language modeling, but I also concur with the reviewer about\nlimited improvements, which makes it a borderline paper. When\ncalibrating with other area chairs, I decided to recommend to reject the paper."}, "review": {"Bkg7KrSUA7": {"type": "rebuttal", "replyto": "SklckhR5Ym", "comment": "We have uploaded a revised version of the paper. The main changes in brief, which have been discussed in detail in the comments below, are as follows\n\n1. Addition of PDR to the Mixture-of-Softmaxes model of (Yang et al. 2017) produces further improvements for both PTB and WikiText-2 over the perplexities achieved by Yang et al. 2017.\n\n                                                        AWD-LSTM-MoS+PDR  || AWD-LSTM-MoS (Yang et al. 2017)\nPenn Treebank with finetuning -                   56.2/53.8   ||  56.5/54.4\nPenn Treebank with dynamic evaluation -   48.0/47.3   ||  48.3/47.7\n\nWikiText-2 with finetuning -                            63.0/60.5   ||  63.9/61.5\nWikiText-2 with dynamic evaluation -            42.0/40.3   ||  42.4/40.7\n\nThe results have been added to Section 4. \n\n2. PDR applied to a baseline LSTM model and trained on the Gigaword dataset gives modest improvements. We obtained a valid/test perplexity of 44.0/42.5 for the model with PDR and 44.3/43.1 for the model without PDR, showing a gain of 0.6 in the test perplexity by using PDR. The results have have been added to Section 4. \n\n3. Section 3 has been condensed while more discussion has been added to Section 7 (related work). We also discuss the applicability of our PDR to other tasks which can be solved using seq2seq models and a shared vocabulary between the inputs and outputs.\n\n4. A few typos have also been corrected. We have also updated the abstract to remove confusion about \"state-of-the-art\" and clearly specify the two cases of single softmax and mixture-of-softmaxes.\n\nReferences\nYang, Zhilin, et al. \"Breaking the softmax bottleneck: A high-rank RNN language model.\" arXiv preprint arXiv:1711.03953 (2017).\n\n", "title": "Revised version uploaded"}, "HkeEorv1hm": {"type": "review", "replyto": "SklckhR5Ym", "review": "In their abstract, the authors claim to provide state-of-the-art perplexity on Penn Treebank, which is not true. As the authors state, their notion of \"state-of-the-art\" excludes exactly that earlier work, which does provide state-of-the-art perplexity on Penn Treebank (Yang et al. 2017), as stated in Sec. 4.1. The question is, why one would exlude the mixture-of-softmax approach here? This is clearly misleading.\n\nThe authors introduce the idea of past decoding for the purpose of regularization. It remains somewhat unclear, why this bigram-centered regularization would strongly contribute for prediction in general.\n\nThe results obtained show moderate improvements of approx. 1 point in perplexity on top of their best current result on Penn Treebank. Considering the small size of the corpus for the evaluation of a regularization method, the results even seem optimistic - it remains unclear, if this approach would readily scale to larger datasets. The mode of language modeling evaluation presented here, without considering an actual language or speech processing task, provides limited insight w.r.t. its utility in actual applications. Moreover, the very limited size of the language modeling tasks chosen here is highly advantageous for smoothing/regularization approaches. It remains totally unclear, how the presented approaches would perform on more realistically sized tasks and within actual applications.\n", "title": "At best misleading notion of state-of-the-art, optimistic evaluation", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "BJe0cdJfC7": {"type": "rebuttal", "replyto": "HkeEorv1hm", "comment": "We thank the reviewer for reading the paper and the comments. As already stated in the comments below, our claim of state-of-the-art in the original manuscript pertains to models with a single softmax, which we clearly state in section 4.1. We will update the abstract to remove any confusion. As suggested by multiple reviewers, we have performed further experiments by incorporating our Past Decode Regularization (PDR) in the mixture-of-softmax (AWD-LSTM-MoS) model of (Yang et al. 2017). We use the same model sizes as used in the paper. As shown below, we observe gains of 0.4 and 1.0 perplexity points for PTB and WT2, while with dynamic evaluation the gains are 0.4 in both cases.\n\n                                                        AWD-LSTM-MoS+PDR  || AWD-LSTM-MoS (Yang et al. 2017)\nPenn Treebank with finetuning -                   56.2/53.8   ||  56.5/54.4\nPenn Treebank with dynamic evaluation -   48.0/47.3   ||  48.3/47.7\n\nWikiText-2 with finetuning -                            63.0/60.5   ||  63.9/61.5\nWikiText-2 with dynamic evaluation -            42.0/40.3   ||  42.4/40.7\n\nNote that, we performed very limited hyperparameter tuning in the vicinity of the hyperparameters used by (Yang et al. 2017) and a more exhaustive search is likely to lead to better gains. Thus, the gains due to PDR generalize to more complex models like AWD-LSTM-MoS+PDR.\n\nWe can justify PDR theoretically as an inductive bias on the language model. The observed bigrams in a language are not random and the distribution of the second word given the first word in a bigram is not uniform. Similarly, the distribution of the first word given the second word will be far from uniform. A RNN based language model models the first dependence (and more long range ones) and our proposed PDR tries to model the second one. In a unidirectional language model, we cannot look into the future tokens and hence we use the output distribution as a proxy for the \"true second word\" and decode the distribution of the first word. Thus the PDR term can be thought of as biasing the language model to retain more information about the distribution of the first word given the second word in a bigram. \n\nWe believe language modeling is a fundamental problem in NLP and our work continues a long stream of papers that have achieved steadily lower perplexities over the past few years. We evaluated our approach on two standard datasets that have been used as a benchmark in most of these papers. \n\nAs suggested by multiple reviewers, we have conducted further experiments on the Gigaword corpus to test PDR on larger corpora. Specifically,  we use a 2-layer LSTM with hidden dimension 1024 and a word embedding dimension of 1024. We truncated the vocabulary by keeping approximately 100k words with the highest frequency and used the same validation and test sets as (Yang et al. 2017). We obtained a valid/test perplexity of 44.0/42.5 for the model with PDR and 44.3/43.1 for the model without PDR, showing a gain of 0.6 points in the test perplexity. Note that we tuned the PDR loss coefficient very coarsely and tuning it further could lead to higher gains. We will update the manuscript with these additional results and discussion and post it shortly.\n\nYang et al. 2017. Breaking the softmax bottleneck: A high-rank RNN language model. arXiv:1711.03953.\n", "title": "Re: At best misleading notion of state-of-the-art, optimistic evaluation "}, "BJlh2rJGCm": {"type": "rebuttal", "replyto": "ryevn2r93m", "comment": "We thank the reviewer for a careful reading of the paper and the constructive comments. Although we proposed Past Decode Regularization (PDR) with language modeling in mind to exploit the symmetry between the input and output vocabulary (and the corresponding word embedding and softmax layer), any model/task that has this symmetry can potentially use a PDR term. As suggested by the reviewer, models for tasks like text summarization and neural machine translation (using a byte-pair encoding vocabulary as in Ofir & Wolf 2016) that use an encoder/decoder seq2seq architecture can benefit from PDR and is a topic of future research. We will incorporate this discussion in the updated version of the paper.  \n\nWe can justify PDR theoretically as an inductive bias on the language model. The observed bigrams in a language are not random and the distribution of the second word given the first word in a bigram is not uniform. Similarly, the distribution of the first word given the second word will be far from uniform. A RNN based language model models the first dependence (and more long range ones) and our proposed PDR tries to model the second one. In a unidirectional language model, we cannot look into the future tokens and hence we use the output distribution as a proxy for the \"true second word\" and decode the distribution of the first word. Thus the PDR term can be thought of as biasing the language model to retain more information about the distribution of the first word given the second word in a bigram. \n\nFinally, we have conducted further experiments on larger corpora, specifically the Gigaword corpus. We use a 2-layer LSTM with a word embedding dimension of 1024 and hidden dimension of 1024. We truncated the vocabulary by keeping approximately 100k words with the highest frequency. We compare the performance of the model with and without PDR and using no other regularization. We used the same validation and test sets as (Yang et al. 2017). We obtained a valid/test perplexity of 44.0/42.5 for the model with PDR and 44.3/43.1 for the model without PDR, showing a gain of 0.6 in the test perplexity by using PDR. We will incorporate these results in the experiments section and post the updated manuscript shortly.\n\n\nPress, Ofir, and Lior Wolf. \"Using the output embedding to improve language models.\" arXiv preprint arXiv:1608.05859 (2016).\nYang, Zhilin, et al. \"Breaking the softmax bottleneck: A high-rank RNN language model.\" arXiv preprint arXiv:1711.03953 (2017).", "title": "Re: Weak accept"}, "HylZSxkGAQ": {"type": "rebuttal", "replyto": "S1gyA_wmhX", "comment": "We thank the reviewer very much for reading the paper carefully and providing us with constructive comments. We have conducted further experiments applying our Past Decode Regularization (PDR) to the mixture-of-softmax (AWD-LSTM-MoS) model of (Yang et al. 2017). We use the same model sizes as in the paper. Even with the very limited hyperparameter search in the vicinity of those used in the paper and fixing the PDR loss coefficient to 0.001 (as used in the other models in our paper), we see consistent gains on the Penn Treebank and WikiText-2 datasets. The validation/test perplexities are as follows - \n\n                                                        AWD-LSTM-MoS+PDR  || AWD-LSTM-MoS (Yang et al. 2017)\nPenn Treebank with finetuning -                   56.2/53.8   ||  56.5/54.4\nPenn Treebank with dynamic evaluation -   48.0/47.3   ||  48.3/47.7\n\nWikiText-2 with finetuning -                            63.0/60.5   ||  63.9/61.5\nWikiText-2 with dynamic evaluation -            42.0/40.3   ||  42.4/40.7\n\nThus we observe gains of 0.6 and 1.0 points in test perplexity for PTB and WT2. With dynamic evaluation, the gains for both datasets is 0.4 points. Note again that we did a very limited hyperparameter search and more exhaustive experiments will likely lead to even better gains by using PDR. We will update and reorganize the experiments section in the paper accordingly. The updated manuscript will be posted shortly.\n\nYang et al. 2017. Breaking the softmax bottleneck: A high-rank RNN language model. arXiv:1711.03953.", "title": "Re: A useful regularization for RNN language models"}, "ryevn2r93m": {"type": "review", "replyto": "SklckhR5Ym", "review": "This paper proposes an additional loss term to use when training an LSTM LM.  The authors argue that, intuitively, we want the output distribution to retain some information about the context, or \"past\".  Given this, they use the output distribution as input to a one layer network that must predict the current token.  The loss for this network is incorporated as an additional term used when training the LM.  The authors show that by adding this loss term they can achieve SOTA (for single softmax model) perplexity on a number of LM benchmarks.\n\nThe technical contribution is proposing a new loss term to use when training a language model.  The idea is clear, simple, and well explained, and it seems to be effective in practice.  One drawback is that it is highly specific to language models.  Other recent works which have demonstrated effective regularization of LSTM LMs have proposed methods that can be used in any LSTM model, but that is not the case here.  In addition, there is not much theoretical justification for it, it seems like a one-off trick.  The loss term is motivated by the idea that we want the output distribution to retain some information about the context, but why should that be the case?\n\nAlthough it is specific to language models, there are a few reasons it might be of broader significance:\n- It falls in the recent line of work in incorporating auxiliary losses for various tasks.  This idea has touched many problems and seen success in practice.\n- Perhaps it can be applied to other sequence models.  For example in encoder-decoder models, the decoder can be thought of as a conditional LM.\n\nExperiments are comprehensive and rigorous.  They might be more convincing if there were results on a very large corpus such as 1 billion word corpus.\n\nPros:\n- New SOTA for single softmax model on LM benchmarks.\n- Simple, clearly explained idea.\n- Demonstrates effectiveness of auxiliary losses.\n- Rigorous experiments.\n\nCons\n- Trick is specific to LM.\n- No large corpus results.\n", "title": "Weak accept", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "S1gyA_wmhX": {"type": "review", "replyto": "SklckhR5Ym", "review": "The paper suggests a new regularization technique which can be added on top of those used in AWD-LSTM of Merity et al. (2017) with little overhead.\n\nThis is a well-written paper with a clear structure. The experiments are presented in a clear and understandable fashion, and the evaluation seems thorough. The methodology seems sound, and the authors present the reader with all the information needed to replicate the experiments.\n\nI would only suggest evaluating this technique on AWD-LSTM-MoS of Yang et al. (2017) to get a more complete picture.\n\nReferences\n- Merity, S., Keskar, N.S. and Socher, R., 2017. Regularizing and optimizing LSTM language models. arXiv preprint arXiv:1708.02182.\n- Yang, Z., Dai, Z., Salakhutdinov, R. and Cohen, W.W., 2017. Breaking the softmax bottleneck: A high-rank RNN language model. arXiv preprint arXiv:1711.03953.", "title": "A useful regularization for RNN language models", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SJedESYsj7": {"type": "rebuttal", "replyto": "BklBzOjYiQ", "comment": "Hi,\n\nThanks for pointing this out. Will surely reference this in the updated version of the paper. ", "title": "Re: Another related work"}, "ryltyRlNim": {"type": "rebuttal", "replyto": "ByeYhB1EjX", "comment": "Thanks for the clarification! I do get your point and will definitely update the abstract in the final version to make things clearer.", "title": "Got your point!"}, "B1xug9umjX": {"type": "rebuttal", "replyto": "ryeLx7S7jm", "comment": "We respectfully but strongly disagree with your statement. We have no intention to mislead anybody. The work of Yang et al. is now well known in the language modeling world and a distinction is always made between models that use multiple softmaxes and those that don't, with the implicit understanding that the use of multiple softmaxes can lead to a further boost in performance. ", "title": "Disagree with your statement!"}, "rJxSPfnC5X": {"type": "rebuttal", "replyto": "HkeleVGa5Q", "comment": "Hi,\n\nThank you for reading the paper and giving constructive comments. \n\nWe are very much aware of the work of Yang et al., 2017 on breaking the softmax bottleneck and their results on language modeling. In fact we explicitly mention in section 4.1, line 6,7 that our result is the first sub-50 perplexity on PTB without the use of multiple softmaxes and add Yang et al. as a reference. We agree that this qualification should have been put in the abstract as well, and we will do so in the final version of the paper. As such, our Past Decode Regularization (PDR) method is orthogonal to the use of multiple softmaxes and can also be applied to such models as well. \n\nThank you for pointing out the PMI paper, which we were not aware of. On preliminary study, apart from being applied to different problems (language modeling vs. sequence generation), we can observe several differences between their approach and ours. In their formulation, they use $p(S|T)$ as a measure to rerank responses (see section 4.2) at test time, where the candidate $T$ are themselves generated by ranking the outputs according to $p(T|S)$.  If we let $S$ to be the current token and $T$ to be the next token to pose language modeling in their framework, we do not compute $p(S|T)$ for any discrete token $T$, rather we use the predicted distribution on the next token $T$ to obtain a distribution over $S$. The backward probability we compute is not explicitly conditioned on $T$, rather it is conditioned on a distribution over the tokens that the LSTM predicts for $T$.  This also allows our method to be trained end to end, rather than being used as an adhoc reranking measure during test time. \n\nWe believe our method is cleaner and a more effective way of biasing the LSTM to have more fidelity about past tokens. It would be interesting to see how an an algorithm that follows the approach presented in the paper performs on the language modeling task. The Twin Networks approach proposed in https://arxiv.org/pdf/1708.06742.pdf is also a closely related work. We will add this discussion to the final version of the paper.  ", "title": "Regarding Sub 50 perplexity on PTB and PMI approach"}}}