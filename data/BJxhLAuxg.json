{"paper": {"title": "A Deep Learning Approach for Joint Video Frame and Reward Prediction in Atari Games", "authors": ["Felix Leibfried", "Nate Kushman", "Katja Hofmann"], "authorids": ["felix.leibfried@gmail.com", "nkushman@microsoft.com", "katja.hofmann@microsoft.com"], "summary": "", "abstract": "Reinforcement learning is concerned with learning to interact with environments that are initially unknown. State-of-the-art reinforcement learning approaches, such as DQN, are model-free and learn to act effectively across a wide range of environments such as Atari games, but require huge amounts of data. Model-based techniques are more data-efficient, but need to acquire explicit knowledge about the environment dynamics or the reward structure. \n\nIn this paper we take a step towards using model-based techniques in environments with high-dimensional visual state space when system dynamics and the reward structure are both unknown and need to be learned, by demonstrating that it is possible to learn both jointly.\nEmpirical evaluation on five Atari games demonstrate accurate cumulative reward prediction of up to 200 frames. We consider these positive results as opening up important directions for model-based RL in complex, initially unknown environments.", "keywords": []}, "meta": {"decision": "Reject", "comment": "The authors have combined two known areas of research - frame prediction and reward prediction - and combined them in a feedforward network trained on sequences from Atari games. The fact that this should train well is unsurprising for this domain, and the research yields no other interesting results. Pros - the paper is clearly written and the experiments are sound. Cons - there is very little novelty or contribution."}, "review": {"rJxQAODUg": {"type": "rebuttal", "replyto": "HJLeVAMml", "comment": "We agree with the reviewer that it would be instructive to compare the reward prediction error of the proposed predictive model for joint video frame and reward prediction to a baseline model with separately trained components for video frame and reward prediction. \n\nWe are in the process of evaluating two different versions of this baseline:\n\n- Training two completely independent models for video frame and reward prediction using a decoupled architecture. The video-frame-predictive part is trained according to Oh et al. 2015, and the reward prediction part is trained with a separate convolutional feedforward network that receives as input the current state of the world in terms of the past four video frames plus the current action taken by the agent in order to predict the current reward.\n\n- Training a shared latent architecture using the same network architecture as proposed in our paper but making video frame and reward prediction explicitly independent. This means that the video-frame-predictive part of the network becomes independent from the reward-predictive part---the reward-predictive part is trained separately with the compressed action-integrated hidden encoding of the video frame prediction network as input.\n\nWe currently perform experiments with both aforementioned baseline models and compare against the joint prediction architecture from our paper with a compound training objective including both video frame reconstruction loss and reward loss. Initial results indicate that the joint prediction architecture performs best in terms of reward prediction measured according to the reward training loss. The decoupled architecture seems to perform better than the shared latent architecture but worse than the joint prediction architecture. These early findings seem to support the hypothesis that without a compound joint prediction objective, the network abstracts away reward-relevant aspects of the video frame input to which the reconstruction loss alone is insensitive. However, evaluations on the test set  including cumulative reward error analysis have not been completed yet---so at the moment we cannot confirm that the joint prediction architecture also improves performance on the test set.\n\nReferences\n\nJ Oh, X Guo, H Lee, R Lewis, and S Singh. Action-conditional video prediction using deep networks in Atari games. In Advances in Neural Information Processing Systems, 2015.", "title": "Answer concerning comparison to baseline"}, "Sk8aQCzQg": {"type": "rebuttal", "replyto": "SJR58MoMl", "comment": "Thank you for your comment. The ultimate goal of future work is to utilize the presented model-based  approach for joint video frame and reward prediction in reinforcement learning settings like the Arcade Learning Environment (or similar) in two ways: \n\n- By augmenting training samples with artificial samples generated by the predictive model---in line with Dyna learning (Sutton 1990)---to reduce agent-environment interactions when training a DQN-agent (Mnih et al. 2015) under the assumption that agent-environment interactions are time-consuming.\n\n- And by Monte-Carlo tree search (similar to Guo et al. 2014) for planning when environment dynamics and the reward function are both unknown and need to be learned, in combination with DQN to estimate future cumulative reward values at the planning horizon. The hope here is to gain a performance increase when compared to model-free reinforcement learning like e.g. plain DQN.\n\nBoth aforementioned approaches require a joint predictive model for future environment states and future instantaneous reward values as a prerequisite---the aim of this paper is to create such a prerequisite since it has been absent in the literature so far (to the best of our knowledge).\n\nCurrent state-of-the-art reward prediction approaches like DQN (Mnih et al. 2015) and CNC (Veness et al. 2015) deal with cumulative reward prediction but not with instantaneous reward prediction, and are hence not directly applicable for our purposes. Since our model predicts instantaneous reward values, it can be used to predict undiscounted cumulative reward values with finite but arbitrary horizon as demonstrated in our evaluations. Predicting undiscounted cumulative reward values with finite and arbitrary horizon is not straightforwardly possible with DQN (discounted infinite horizon) and CNC (undiscounted fixed horizon).\n\nOur approach extends a recently proposed deep transcoder network for video frame prediction in Atari games to enable reward prediction as well---see Oh et al. 2015 where the authors compare the transcoder network\u2019s architecture to alternative architectures for video frame prediction. Since we use the same architecture for video frame prediction here, we felt there is no further need to compare to other video frame prediction methods.\n\nReferences\n\nX Guo, S Singh, H Lee, R Lewis, and X Wang. Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning. In Advances in Neural Information Processing Systems, 2014.\n\nV Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran, D Wierstra, S Legg, and D Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.\n\nJ Oh, X Guo, H Lee, R Lewis, and S Singh. Action-conditional video prediction using deep networks in Atari games. In Advances in Neural Information Processing Systems, 2015.\n\nR S Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Proceedings of the International Conference on Machine Learning, 1990.\n\nJ Veness, M G Bellemare, M Hutter, A Chua, and G Desjardins. Compress and control. In Proceedings of the AAAI Conference on Artificial Intelligence, 2015.", "title": "Answer concerning comparison to state-of-the-art"}, "B1Gh4AMmx": {"type": "rebuttal", "replyto": "Sky8bKkXl", "comment": "Thank you for your comment. It is correct that we extend recent work on video frame prediction (Oh et al. 2015) to enable instantaneous reward prediction as well. Since the model can predict instantaneous reward values, it can also be used to predict undiscounted cumulative reward values with finite and arbitrary horizon as demonstrated by our evaluations.\n\nAs stated in our reply to AnonReviewer3, the main goal of future work is to utilize the presented model-based approach in reinforcement learning settings like the Arcade Learning Environment (or similar) in two ways. First by Dyna learning (Sutton 1990) in combination with DQN (Mnih et al. 2015) to augment training samples with artificial samples generated by our predictive model so as to reduce agent-environment interactions under the assumption that such agent-environment interactions are time-consuming. And second, by planning with Monte-Carlo tree search (Guo et al. 2014) when environment dynamics and the reward function are both unknown and need to be learned with the hope to gain an increase in performance compared to model-free reinforcement learning.\n\nBoth future work proposals require as a prerequisite a joint predictive model for future environment state and future instantaneous reward prediction. The aim of this paper is to create such a prerequisite since it has been absent in the literature so far. Since our proposed model does predict future environment states and future instantaneous reward values in a joint manner and since such a predictive model is necessary for both Dyna learning and Monte-Carlo tree search in unknown environments, our paper provides a meaningful first step towards model-based reinforcement learning in settings like the Arcade Learning Environment or similar (under the explicit assumption that agents do not know environment dynamics and the reward function in advance).\n\nThe scientific contribution of our paper is to extend the architecture from Oh et al. 2015 to enable reward prediction as well which we demonstrate in our evaluations. We do not claim that our results are surprising---the design and the evaluation of a joint future state and reward prediction model is in fact a necessary precondition for model-based reinforcement learning approaches that rely on such prediction models. The purpose of our paper is to establish such a precondition as it has been absent in the literature so far.\n\nIn more realistic scenarios, reward values and state transitions might be probabilistic. While our model can cope with probabilistic reward values due to the categorical nature of the reward prediction layer, probabilistic state transitions require a modification of the video-frame-prediction transcoder network proposed by Oh et al. 2015 to handle uncertainty---as evidenced by our qualitative analysis visualizing agent game play in Seaquest where the model has problems to correctly predict objects that randomly enter the scene, this finding is in line with evaluations performed by Oh et al. 2015. Extending the model to handle uncertainty is therefore subject to future work. Two promising ways of achieving this goal is to combine the transcoder network with dropout (Srivastava et al. 2014) or variational autoencoder schemes (Kingma and Welling 2014, Rezende et al 2014).\n\nReferences\n\nX Guo, S Singh, H Lee, R Lewis, and X Wang. Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning. In Advances in Neural Information Processing Systems, 2014.\n\nD P Kingma and M Welling. Auto-encoding variational Bayes. In Proceedings of the International Conference on Learning Representations, 2014.\n\nV Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran, D Wierstra, S Legg, and D Hassabis. Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533, 2015.\n\nJ Oh, X Guo, H Lee, R Lewis, and S Singh. Action-conditional video prediction using deep networks in Atari games. In Advances in Neural Information Processing Systems, 2015.\n\nD J Rezende, S Mohamed, and D Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the International Conference on Machine Learning, 2014.\n\nN Srivastava, G E Hinton, A Krizhevsky, I Sutskever, and R Salakhutdinov. Dropout : a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15:1929\u20131958, 2014.\n\nR S Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Proceedings of the International Conference on Machine Learning, 1990.", "title": "Answer concerning what's the take-away here?"}, "HJLeVAMml": {"type": "review", "replyto": "BJxhLAuxg", "review": "Does joint prediction of rewards and frames help? It would be instructive to compare the reward prediction error to the baseline of networks trained on reward prediction only.The topic of the paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are too incremental. Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do. However neither the methodology not the results are novel / surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.\n\nI\u2019m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors. ", "title": "Comparison to baseline", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJKENmk4l": {"type": "review", "replyto": "BJxhLAuxg", "review": "Does joint prediction of rewards and frames help? It would be instructive to compare the reward prediction error to the baseline of networks trained on reward prediction only.The topic of the paper, model-based RL with a learned model, is important and timely. The paper is well written. I feel that the presented results are too incremental. Augmenting the frame prediction network with another head that predicts the reward is a very sensible thing to do. However neither the methodology not the results are novel / surprising, given that the original method of [Oh et al. 2015] already learns to successfully increment score counters in predicted frames in many games.\n\nI\u2019m very much looking forward to seeing the results of applying the learned joint model of frames and rewards to model-based RL as proposed by the authors. ", "title": "Comparison to baseline", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Sky8bKkXl": {"type": "review", "replyto": "BJxhLAuxg", "review": "This paper is well written so I mainly have a high level discussion question.\n\nIf I understand correctly, the authors propose adding a reward prediction branch to a neural architecture that is known to predict future frames, and show that it can predict cumulative reward as well as future video frames.\n\nMy question is: what's the take-away here? The stated motivation is that agents in the real world may need to learn both system dynamics and rewards and the goal is to provide first steps in that direction. My main qualm is that I don't see how this paper makes a meaningful first step. Atari games are deterministic and the reward is tightly coupled in a deterministic fashion to the environment, with barely any hidden information. It has already been shown in prior work that the network can predict the dynamics of the game far ahead, so why is it any surprise that it can also learn the reward function? Why should this result generalize to a more realistic scenario?\n\nThe most interesting parts of this paper are the proposed next steps (not the immediate next steps, but the future ones about planning with uncertainty and doing planning in latent spaces). If the paper was about those, it would be far more interesting and impactful. Instead, the results seem very incremental. I understand that there's a lot of literature about playing Atari games, but one needs to be realistic about claims and impact with such an artificial setting.This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. In Atari game playing scenarios, the authors show that this model can successfully predict both reward and next frames.\n\nPros:\n- Paper is well written and easy to follow.\n- Model is clear to understand.\n\nCons:\n- The model is incrementally different than the baseline. The authors state that their purpose is to establish a pre-condition, which they achieve. But this makes the paper quite limited in scope.\n\nThis paper reads like the start of a really good long paper, or a good short paper. Following through on the future work proposed by the authors would make a great paper. As it stands, the paper is a bit thin on new contributions.", "title": "What's the take-away here?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryuwhyQ4e": {"type": "review", "replyto": "BJxhLAuxg", "review": "This paper is well written so I mainly have a high level discussion question.\n\nIf I understand correctly, the authors propose adding a reward prediction branch to a neural architecture that is known to predict future frames, and show that it can predict cumulative reward as well as future video frames.\n\nMy question is: what's the take-away here? The stated motivation is that agents in the real world may need to learn both system dynamics and rewards and the goal is to provide first steps in that direction. My main qualm is that I don't see how this paper makes a meaningful first step. Atari games are deterministic and the reward is tightly coupled in a deterministic fashion to the environment, with barely any hidden information. It has already been shown in prior work that the network can predict the dynamics of the game far ahead, so why is it any surprise that it can also learn the reward function? Why should this result generalize to a more realistic scenario?\n\nThe most interesting parts of this paper are the proposed next steps (not the immediate next steps, but the future ones about planning with uncertainty and doing planning in latent spaces). If the paper was about those, it would be far more interesting and impactful. Instead, the results seem very incremental. I understand that there's a lot of literature about playing Atari games, but one needs to be realistic about claims and impact with such an artificial setting.This paper introduces an additional reward-predicting head to an existing NN architecture for video frame prediction. In Atari game playing scenarios, the authors show that this model can successfully predict both reward and next frames.\n\nPros:\n- Paper is well written and easy to follow.\n- Model is clear to understand.\n\nCons:\n- The model is incrementally different than the baseline. The authors state that their purpose is to establish a pre-condition, which they achieve. But this makes the paper quite limited in scope.\n\nThis paper reads like the start of a really good long paper, or a good short paper. Following through on the future work proposed by the authors would make a great paper. As it stands, the paper is a bit thin on new contributions.", "title": "What's the take-away here?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJR58MoMl": {"type": "review", "replyto": "BJxhLAuxg", "review": "No comparisons are done to other model-based or model-free reward prediction methods, nor to other frame-prediction algorithms. Right now I feel the quality of the results is hard to judge.The paper extends a recently proposed video frame prediction method with reward prediction in order to learn the unknown system dynamics and reward structure of an environment. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps. The paper is very well written, focussed and is quite clear about its contribution to the literature. The experiments and methods are sound. However, the results are not really surprising given that the system state and the reward are linked deterministically in Atari games. In other words, we can always decode the reward from a network that successfully encodes future system states in its latent representation. The contribution of the paper is therefore minor. The paper would be much stronger if the authors could include experiments on the two future work directions they suggest in the conclusions: augmenting training with artificial samples and adding Monte-Carlo tree search. The suggestions might decrease the number of real-world training samples and increase performance, both of which would be very interesting and impactful.", "title": "Comparison to state-of-the-art", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SkchXXWVe": {"type": "review", "replyto": "BJxhLAuxg", "review": "No comparisons are done to other model-based or model-free reward prediction methods, nor to other frame-prediction algorithms. Right now I feel the quality of the results is hard to judge.The paper extends a recently proposed video frame prediction method with reward prediction in order to learn the unknown system dynamics and reward structure of an environment. The method is tested on several Atari games and is able to predict the reward quite well within a range of about 50 steps. The paper is very well written, focussed and is quite clear about its contribution to the literature. The experiments and methods are sound. However, the results are not really surprising given that the system state and the reward are linked deterministically in Atari games. In other words, we can always decode the reward from a network that successfully encodes future system states in its latent representation. The contribution of the paper is therefore minor. The paper would be much stronger if the authors could include experiments on the two future work directions they suggest in the conclusions: augmenting training with artificial samples and adding Monte-Carlo tree search. The suggestions might decrease the number of real-world training samples and increase performance, both of which would be very interesting and impactful.", "title": "Comparison to state-of-the-art", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}