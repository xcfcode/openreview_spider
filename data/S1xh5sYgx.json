{"paper": {"title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size", "authors": ["Forrest N. Iandola", "Song Han", "Matthew W. Moskewicz", "Khalid Ashraf", "William J. Dally", "Kurt Keutzer"], "authorids": ["forresti@eecs.berkeley.edu", "songhan@stanford.edu", "moskewcz@eecs.berkeley.edu", "kashraf@eecs.berkeley.edu", "dally@stanford.edu", "keutzer@eecs.berkeley.edu"], "summary": "Small CNN models", "abstract": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).", "keywords": ["Computer vision", "Deep learning"]}, "meta": {"decision": "Reject", "comment": "The paper proposes a ConvNet architecture (\"SqueezeNet\") and a building block (\"Fire module\") aimed at reducing the model size while maintaining the AlexNet level of accuracy. The novelty of the submission is very limited as very similar design choices have already been used for model complexity reduction in Inception and ResNet. Because of this, we recommend rejection and invite the authors to further develop their method."}, "review": {"HyVnMnZSl": {"type": "rebuttal", "replyto": "ryx9sViEg", "comment": "Thank you for your feedback and encouragement. Let me take a moment to discuss the weaknesses/questions that you mentioned.\n\n1. SqueezeNet on other tasks.\nWe recently added a detection layer at the end of SqueezeNet and fine-tuned it on KITTI object detection. We identified a variant of SqueezeNet that is simultaneously the fastest, smallest, and most accurate (in terms of mean-average precision) model on the KITTI detection task, compared to previous reported results. We will be posting this to the KITTI leaderboard soon, but meanwhile we have released some details in this paper:\nhttps://arxiv.org/abs/1612.01051\n\n2. Theoretical analysis.\nWe certainly appreciate the theoretical aspects of deep neural network research. If you ask a more specific question, we will do our best to answer it.\n\n3. Placing SqueezeNet in the context of GoogLeNet and ResNet.\nTake a look at our response to the earlier comment thread, \"SqueezeNet versus other models than AlexNet.\"\n\n\n", "title": "SqueezeNet on other tasks"}, "r1mz4TVVg": {"type": "rebuttal", "replyto": "H1BSKEJEg", "comment": "Thanks for the encouraging and positive feedback. ", "title": "Thanks!"}, "BJ74HQWNg": {"type": "rebuttal", "replyto": "Byk2d_y4x", "comment": "Good questions. I think we address most of your questions in an earlier comment thread. Search this page for \"SqueezeNet versus other models than AlexNet.\"", "title": "See previous comments"}, "rJUAKWzXe": {"type": "rebuttal", "replyto": "Bkt52_jGx", "comment": "First, we are quite enthusiastic about Gschwend's work. In his \"ZynqNet\" technical report, Gschwend says, \"[We used] SqueezeNet as the basis for our own CNN topology, due to its good fit for an FPGA-based implementation. The tiny parameter set could even be fit into the on-chip SRAM of a medium-sized FPGA, and optimizations are relatively easy to try thanks to the fast training cycles and the clear network structure.\" We are glad that Gschwend found value in using SqueezeNet as a starting point for some of his work.\n\nWhen executing a CNN, it is correct that some activations need to be stored. Specifically, it's necessary to allocate enough memory to store the activations of the two contiguous layers with the largest total quantity of activations. In the case of SqueezeNet, the largest activation storage requirement is in {conv1 (3.27MB) + fire2/conv1x1_1 (0.19MB)}, which means we need to allocate 3.46MB of storage for activations. So, for uncompressed SqueezeNet, the activation storage (3.46MB) is a bit smaller than the parameter storage (4.80MB).\n\nA side note -- in total, SqueezeNet produces 11.05MB of activations over all layers combined. Here is a table that includes the quantity of activations in each SqueezeNet layer:\nhttps://docs.google.com/document/d/1X9ENqsU6sMkjj2DVjEJfI16Nu3JPXRVFcu7tqMoVl3o\n\n", "title": "ZynqNet and memory space for activations"}, "SyU3L-GXx": {"type": "rebuttal", "replyto": "HytpAbsMg", "comment": "Let us break this into three questions:\n1. Architecturally, what do SqueezeNet's Fire modules have in common with Inception modules?\nAs in Inception modules, Fire modules have multiple sizes of filters at the same level of depth in the NN. For example, Inception-v1 modules have multiple instances with 1x1, 3x3, and 5x5 filters alongside each other. \nOne of our biggest questions when looking at the Inception paper was \"how does a CNN architect decide how many of each size of filter to have in each module?\" Some versions of the inception modules have 10 or more filter banks (layers?) per module. Doing careful A/B comparisons of \"how many of each type of filter\" would lead to a combinatorial explosion. But, in the Fire modules, we have just 3 filter banks (1x1_1, 1x1_2, and 3x3_2). With this setup, we were able to ask:\n\n- What are the tradeoffs in \"many 1x1_2 and few 3x3_2\" vs \"few 1x1_2 and many 3x3_2\" in terms of metrics such as model size and accuracy? In Section 5.3, we learn that 50% 1x1_2 and 50% 3x3_2 filters generates the same accuracy level as 99% 3x3_2. However, there's a significant difference in the model size and computational footprint of these these models. The lesson is: look for the point where adding more spatial resolution to the filters stops improving accuracy, and stop there; otherwise computation and model parameters are being wasted.\n\n- In GoogLeNet-v1, some of the Inception-v1 modules are set up such that the early filter banks have 75% the number of filters as the late filter banks. In our terminology, this is like they have a \"squeeze ratio\" (SR) of 0.75. We wanted to understand what the tradeoffs that emerge if we more aggressively cut down the number of filters at the beginning of each module. We were able to do this experiment in Section 5.2, and we found (again) that there's a saturation point where going from SR=0.75 to SR=1.0 increases the computational footprint and model size, but it does not improve accuracy. \n\n- Thus, the Fire modules have served as a good vehicle for understanding the tradeoffs that emerge when selecting the number of filters inside of CNN modules.\n\n2. Architecturally, what does SqueezeNet have in common ResNet?\nWe spend most of the paper presenting and evaluating SqueezeNet with no residual connections (Figure 2, left side). As a bonus, we experiment with adding residual connections to SqueezeNet, and we find that this does improve the accuracy. In some ways, the paper might deliver a stronger message if we had just focused on the standard SqueezeNet model. But, we thought the residual results were interesting, and we felt compelled to share them with the community.\n\n3. How does SqueezeNet's accuracy compare with other modern CNN architectures' accuracy?\nGoogLeNet (BVLC_googlenet) achieves single-crop top-5 ImageNet accuracy of 87.3% and is a 53MB model. In Section 5.2 and Figure 3a, we describe a 13MB (uncompressed) version of the SqueezeNet model that achieves 85.3% top-5 accuracy while being 4x smaller than GoogLeNet.", "title": "Broader architectural context on SqueezeNet and its variants"}, "B1P9rWMmx": {"type": "rebuttal", "replyto": "SJHRKU1Xe", "comment": "In all of our experiments, we are optimizing classification accuracy using a softmax at the end of our CNN. In Knowledge Distillation, the approach is to optimize a small CNN to replicate the output of a large CNN. \n\nSo, while we don't use Knowledge Distillation here, it is a complementary approach that may enable the SqueezeNet family of models to further improve in accuracy.\n", "title": "Knowledge Distillation is a complementary approach"}, "SJHRKU1Xe": {"type": "review", "replyto": "S1xh5sYgx", "review": "How does your method compare with Knowledge Distillation method?\nStrengths\n\uf06e-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. \n\uf06e-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules.\n\uf06e-- x50 less memory usage than AlexNet, keeping similar accuracy \n\uf06e-- strong experimental results\n\nWeaknesses\n\uf06e--Would be nice to test Sqeezenet on multiple tasks\n\n\uf06e--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the \u201cby-pass\u201d architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more rigorously on theoretical analysis?\n", "title": "Comparisons with Knowledge Distillation", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryx9sViEg": {"type": "review", "replyto": "S1xh5sYgx", "review": "How does your method compare with Knowledge Distillation method?\nStrengths\n\uf06e-- An interesting proposal for a smaller CNN architecture designed for embedded CNN applications. \n\uf06e-- Balanced exploration of CNN macroarchitecture and microarchitecture with fire modules.\n\uf06e-- x50 less memory usage than AlexNet, keeping similar accuracy \n\uf06e-- strong experimental results\n\nWeaknesses\n\uf06e--Would be nice to test Sqeezenet on multiple tasks\n\n\uf06e--lack of insights and rigorous analysis into what factors are responsible for the success of SqueezeNet. For example, how are ResNet and GoogleNet connected to the current architecture? Another old paper (Analysis of correlation structure for a neural predictive model with application to speech recognition, Neural Networks, 1994) also showed that the \u201cby-pass\u201d architecture by mixing linear and nonlinear prediction terms improves long term dependency in NN based on rigorous perturbation analysis. Can the current work be placed more rigorously on theoretical analysis?\n", "title": "Comparisons with Knowledge Distillation", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Bkt52_jGx": {"type": "review", "replyto": "S1xh5sYgx", "review": "The authors analyze the amount of space required for storing the parameters of SqueezeNet (which is valuable to estimate for downloading speed, etc.) But, the Gschwend thesis also analyzes the amount of space required to store the activations during run time. Gschwend's results seem to show that the activation memory is much larger than the parameter memory.\n\nComments? Is Gschwend right? If so, can you add this to the paper?The Squeezenet paper came out in Feb 2016, and I read it with interest. It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition (imagenet). The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. (Looks like ~500x if combined with Han, 2015). So, very nice results, definitely worth publishing.\n\nSince the arxiv paper came out, people have noticed and worked to extend the paper. This is already evidence that this paper will have impact --- and deserves to have a permanent published home.\n\nOn the negative side, the architecture was only tested on ImageNet -- unclear whether the ideas transfer to other tasks (e.g., audio or text recognition). And, as with many other architecture-tweaking papers, there is no real mathematical or theoretical support for the ideas: they are just sensible and empirically work.\n\nOh the whole, I think the paper deserves to appear at ICLR, being in the mainline of work on deep learning architectures.", "title": "Memory for storing activations?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1BSKEJEg": {"type": "review", "replyto": "S1xh5sYgx", "review": "The authors analyze the amount of space required for storing the parameters of SqueezeNet (which is valuable to estimate for downloading speed, etc.) But, the Gschwend thesis also analyzes the amount of space required to store the activations during run time. Gschwend's results seem to show that the activation memory is much larger than the parameter memory.\n\nComments? Is Gschwend right? If so, can you add this to the paper?The Squeezenet paper came out in Feb 2016, and I read it with interest. It has a series of completely reasonable engineering suggestions for how to save parameter memory for CNNs for object recognition (imagenet). The suggestions make a lot of sense, and provide an excellent compression of about 50x versus AlexNet. (Looks like ~500x if combined with Han, 2015). So, very nice results, definitely worth publishing.\n\nSince the arxiv paper came out, people have noticed and worked to extend the paper. This is already evidence that this paper will have impact --- and deserves to have a permanent published home.\n\nOn the negative side, the architecture was only tested on ImageNet -- unclear whether the ideas transfer to other tasks (e.g., audio or text recognition). And, as with many other architecture-tweaking papers, there is no real mathematical or theoretical support for the ideas: they are just sensible and empirically work.\n\nOh the whole, I think the paper deserves to appear at ICLR, being in the mainline of work on deep learning architectures.", "title": "Memory for storing activations?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HytpAbsMg": {"type": "review", "replyto": "S1xh5sYgx", "review": "The SqueezeNet seems to be an application of ideas from inception modules (CNN Microarchitecture) and ResNet modules (CNN Macroarchitecture). Question: how does SqueezeNet performs compared to GoogLeNet and ResNet in terms of accuracy?Summary: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitecture and microarchitecture to develop SqueezeNet, which is composed of fire modules.\n\nPros: \nAchieves x50 less memory usage than AlexNet while keeping similar accuracy.\n\nCons & Questions:\nComplex by-pass has less accuracy than simple by-pass. And simple by-pass is like ResNet bottlenecks and complex by-pass is like inception modules in GoogLeNet. Can we say that these two valiants of SqueezeNet are adaptation of concepts seen in GoogLeNet and ResNet? If so, then shouldn\u2019t be there a SqueezeNet like model that achieves similar accuracy compared with GoogLeNet and ResNet?\n", "title": "SqueezeNet versus other models than AlexNet", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Byk2d_y4x": {"type": "review", "replyto": "S1xh5sYgx", "review": "The SqueezeNet seems to be an application of ideas from inception modules (CNN Microarchitecture) and ResNet modules (CNN Macroarchitecture). Question: how does SqueezeNet performs compared to GoogLeNet and ResNet in terms of accuracy?Summary: The paper presents a smaller CNN architecture called SqueezeNet for embedded deployment. The paper explores CNN macroarchitecture and microarchitecture to develop SqueezeNet, which is composed of fire modules.\n\nPros: \nAchieves x50 less memory usage than AlexNet while keeping similar accuracy.\n\nCons & Questions:\nComplex by-pass has less accuracy than simple by-pass. And simple by-pass is like ResNet bottlenecks and complex by-pass is like inception modules in GoogLeNet. Can we say that these two valiants of SqueezeNet are adaptation of concepts seen in GoogLeNet and ResNet? If so, then shouldn\u2019t be there a SqueezeNet like model that achieves similar accuracy compared with GoogLeNet and ResNet?\n", "title": "SqueezeNet versus other models than AlexNet", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}