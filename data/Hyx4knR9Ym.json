{"paper": {"title": "Generalizable Adversarial Training via Spectral Normalization", "authors": ["Farzan Farnia", "Jesse Zhang", "David Tse"], "authorids": ["farnia@stanford.edu", "jessez@stanford.edu", "dntse@stanford.edu"], "summary": "", "abstract": "Deep neural networks (DNNs) have set benchmarks on a wide array of supervised learning tasks. Trained DNNs, however, often lack robustness to minor adversarial perturbations to the input, which undermines their true practicality. Recent works have increased the robustness of DNNs by fitting networks using adversarially-perturbed training samples, but the improved performance can still be far below the performance seen in non-adversarial settings. A significant portion of this gap can be attributed to the decrease in generalization performance due to adversarial training. In this work, we extend the notion of margin loss to adversarial settings and bound the generalization error for DNNs trained under several well-known gradient-based attack schemes, motivating an effective regularization scheme based on spectral normalization of the DNN's weight matrices. We also provide a computationally-efficient method for normalizing the spectral norm of convolutional layers with arbitrary stride and padding schemes in deep convolutional networks. We evaluate the power of spectral normalization extensively on combinations of datasets, network architectures, and adversarial training schemes.", "keywords": ["Adversarial attacks", "adversarial training", "spectral normalization", "generalization guarantee"]}, "meta": {"decision": "Accept (Poster)", "comment": "Adversarial training has quickly become important for training robust neural networks.  However this training generally results in poor generalization behavior. This paper proposes using margin loss with adversarial training for better generalization. The paper provides generalization bounds for this adversarial training setup motivating the use of spectral regularization. The experimental results using the spectral regularization with adversarial training are very promising and all the reviewers agree that they show non-trivial improvement. Even though the spectral regularization techniques have been tried in different settings, hence of limited novelty, the experimental results in the paper are encouraging and I believe will motivate further study on this topic. Reviewers also opined that the writing in the paper is currently not that great with limited explanation of the theoretical results. More discussions interpreting the theoretical results and their significance can help the readers appreciate the paper better."}, "review": {"H1x3aUom2X": {"type": "review", "replyto": "Hyx4knR9Ym", "review": "This paper is well set-up to target the interesting problem of degraded generalisation after adversarial training. The proposal of applying spectral normalisation (SN) is well motivated, and is supported by margin-based bounds. However, the experimental results are weak in justifying the paper's claims.\n\nPros:\n* The problem is interesting and well explained\n* The proposed method is clearly motivated\n* The proposal looks theoretically solid\n\nCons:\n\n* It is unclear to me whether the \"efficient method for SN in convolutional nets\" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides. There is no direct comparison of performance.\n\n* Fig. 3 needs more explanation. The horizontal axes are unlabelled, and \"margin normalization\" is confusing when shown together with SN without an explanation. Perhaps it's helpful to briefly introduce it in addition to citing Bartlett et al. 2017.\n\n* The epsilons in Fig. 5 have very different scales (0 - 0.5 vs. 0 - 5). Are these relevant to the specific algorithms and why?\n\n* Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem. However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal. Furthermore, the baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm). It is thus unclear whether the advantage can be maintained after applying these standard regularsisers.\n\nA typo in page 6, last line: wth -> with", "title": "The idea is well explained, but results are less clear", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJg4GCdkR7": {"type": "rebuttal", "replyto": "Hyx4knR9Ym", "comment": "We thank the reviewers for their valuable time and constructive feedback. In response to the comments raised in the reviews, we have modified Figures 3, 5, and 6 in the main text to more clearly convey their messages. We have also performed the following additional numerical experiments and added the results to the Appendix:\n\n1. We reran and timed all 42 experiments in Table 1 for 40 epochs with and without spectral normalization to clearly illustrate the difference in training time when using our proposed spectral normalization method (Appendix Table 2). We see that the training time with our proposed method is comparable, often being roughly the same and in the worst case taking 1.84 times as long.\n\n2. We provide an extensive comparison of our spectral normalization method for convolutional layers to that proposed by Miyato et al. (2018) in Appendix A.1. We provide numerical evidence that our method properly controls the spectral norm of convolution layers through figures and the estimated spectral norms of the layers post-training. The proposed normalization scheme also results in better generalization performance (Figure 10). We also compare the runtimes of architectures trained using our spectral normalization method versus Miyato et al.\u2019s spectral normalization method (Table 3) and observe that our method takes only slightly longer, as expected.\n\n3. We empirically compare spectral normalization to other common regularization techniques for deep neural nets (DNNs): batch normalization, weight decay, and dropout. We see that spectral normalization achieves the best generalization performance in adversarial training settings. The results are provided in Appendix A.2.\n\nWe have also made the appropriate modifications in the main text and cited relevant works raised by the reviewers. We provide our code in an anonymous zip file that can be accessed at: https://www.dropbox.com/s/hl9q2f6epdu80qp/dl_spectral_normalization.zip?dl=0.", "title": "Author Response Summary (Draft Updated)"}, "ByeNL6dJAQ": {"type": "rebuttal", "replyto": "SkeXPX7hhm", "comment": "We thank Reviewer 1 for the constructive feedback. Here is our point-to-point response to the comments and questions raised in the review:\n\n1. \u201cThe numbers reported in Figure 5 do not match with the performance of adversarial training in previous paper\u2026 I wonder why the numbers are so different.\u201d \n\nTable 1 of \"Obfuscated Gradients Give a False Sense of Security\" reports an accuracy of 47% under 0.031 norm-inf perturbation for the CIFAR10 dataset (55% is reported for the MNIST dataset), approximately the same as the 44% accuracy in our Figure 5. The difference in performance stems from how we preprocessed the CIFAR10 images: exactly in the manner described by (Zhang et al., 2017)\u2019s ICLR paper \u201cUnderstanding deep learning requires rethinking generalization\u201d (we whiten and crop each image). \n\n2. \u201cWhat's the training time of the proposed method compared with vanilla adversarial training?\u201d \n\nWe have added Table 2 to the Appendix which reports the increase in runtime for each of the 42 experiments discussed in Table 1 after introducing spectral normalization. For 39 of the cases, our TensorFlow implementation of the proposed method results in longer training times (from 1.02 to 1.84 times longer). In the 3 cases of iterative adversarial attacks with the Inception architecture, the proposed method actually results in faster training time. This is likely due to how TensorFlow handles training in the backend. We provide the code for full transparency.\n\n3. \u201cThe idea of using SN to improve robustness has been introduced in the following paper: \"Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks\" (but this paper did not combine it with adv training).\u201d\n\nThank you for bringing this recent work to our attention. We cite and discuss this NIPS paper in our updated draft.", "title": "Author Response to AnonReviewer1"}, "HyxOkadJAX": {"type": "rebuttal", "replyto": "B1xw3F5KhQ", "comment": "We thank Reviewer 3 for the constructive feedback. Here is our point-to-point response to the comments and questions raised in this review:\n\n1. \u201cThe novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily\u201d\n\nGAN inference and adversarial training seek different goals. Adversarial training addresses a supervised learning task while GAN inference focuses on an unsupervised learning problem. Due to the inherent difference between supervised and unsupervised learning problems, the notion of generalization is defined differently between them. Arora et al. (2017) provide the standard definition of generalization error for GANs which is very different from the standard generalization error considered in supervised learning. Furthermore, no work in the literature theoretically guarantees that spectral normalization closes the generalization gap for either adversarial supervised learning or GAN unsupervised learning.\n\n2. \u201cIt is not clear to me that these are some novel results that can better help adversarial training\u201d\n\nOur work\u2019s main contribution is the theoretical generalization guarantees for spectrally-normalized adversarially-trained DNNs. Introducing the adversary can significantly grow the capacity of a DNN. Therefore, existing DNN generalization bounds are not applicable to adversarial training settings. Our work, to our best knowledge, is the first to show that the adversarial learning capacity of a DNN for FGM, PGM, WRM training schemes can be effectively controlled by regularizing the spectral norm of the DNN\u2019s weight matrices. Our numerical results further support our theoretical contribution.", "title": "Author Response to AnonReviewer3"}, "H1eRd3dyCQ": {"type": "rebuttal", "replyto": "H1x3aUom2X", "comment": "We thank Reviewer 2 for the constructive feedback. Here is our point-to-point response to the comments and questions raised in the review:\n\n1. \u201cIt is unclear to me whether the \"efficient method for SN in convolutional nets\" is more efficient than the power iteration algorithm employed in previous work, such as Miyato et al. 2018, which also used SN in conv nets with different strides. There is no direct comparison of performance.\u201d\n\nWe do not claim that our method is more efficient than Miyato et al.\u2019s method, which uses the spectral norm of the convolution kernel matrix to approximate the spectral norm of the convolution operation. In fact, our proposed method is computationally more expensive than their approximate scheme because each power iteration in our method requires a conv/deconv operation rather than a simple division used by Miyato et al.\u2019s. \n\nWe introduce our new spectral normalization scheme for convolutional layers because there exist examples where the true spectral norm of a convolution operation can be arbitrarily larger than Miyato et al.\u2019s approximation. Therefore, Miyato et al.\u2019s normalization scheme is not guaranteed to control the spectral norm of convolutional layers which is critical for controlling a DNN\u2019s generalization performance (please see our generalization bounds in Section 3). To further support our argument, we performed additional experiments demonstrating how our proposed method better controls the spectral norm of convolution layers, resulting in better generalization and test performance. The results are presented in Appendix A.1. Furthermore, we run several experiments to show that our method is not significantly slower than Miyato et al.\u2019s method, and we report the results in Appendix A.1, Table 3. \n\n2. \u201cFig. 3 needs more explanation. The horizontal axes are unlabelled, and \"margin normalization\" is confusing\u201d\n\nWe relabel the axes and add a more thorough explanation in the caption. We note that the text explaining Figure 3 mentions how the margin normalization is performed (paragraph 3 in section 5.1): the margin normalization factor is exactly the capacity norm \\Phi described in Theorems 1-4. We clarify that we divide the obtained margins by the values of \\Phi estimated on the dataset.\n\n3. \u201cThe epsilons in Fig. 5 have very different scales (0 - 0.5 vs. 0 - 5). Are these relevant to the specific algorithms and why?\u201d \n\nYes, the epsilons are chosen to be different depending on whether we are looking at norm_inf attacks or norm_2 attacks. This is because the two norms can behave very differently in adversarial attack experiments. For example, a norm_inf attack of 0.5 implies that all pixels can be changed by 0.5. On the other hand, a norm_2 attack of 0.5 means the overall Euclidean norm of perturbation across all pixels is bounded by 0.5, resulting in a much less powerful attack. Based on this comment, we update the plots with the same attack-norm to have the same scale.\n\n4. \"Section 5.3 (Fig. 6) is the part most relevant to the generalisation problem. However, the results are unconvincing: only the results for epsilon = 0.1 are shown, and even so the advantage is marginal.\" \n\nWe redo the visualization in Figure 6 to make the gains provided by SN clearer. We see that using SN can improve the test performance by over 12% for some FGM, PGM, and WRM cases.\n\n5. \"The baseline models did not use other almost standard regularisation techniques (weight decay, dropout, batch-norm). It is thus unclear whether the advantage can be maintained after applying these standard regularisers.\"\n\nWe did not originally discuss weight decay, dropout, and batch normalization as none of these methods were motivated by the theory we introduced in section 3. However, due to the reviewers\u2019 concern in the updated draft we compare spectrally-normalized networks to networks with the same architecture except with weight decay, dropout, or batch norm in Appendix A.2. In our experiments, the SN-regularized network still performs better in terms of test accuracy. ", "title": "Author Response to AnonReviewer2"}, "SkeXPX7hhm": {"type": "review", "replyto": "Hyx4knR9Ym", "review": "The paper first provides a generalization bounds for adversarial training, showing that the error bound depends on Lipschitz constant. This motivates the use of spectral regularization (similar to Miyato et al 2018) in adversarial training. Using spectral regularization to improve robustness is not new, but it's interesting to combine spectral regularization and adversarial training. Experimental results show significant improvement over vanilla adversarial training. \n\nThe paper is nicely written and the experimental results are quite strong and comprehensive. I really like the paper but I have two questions about the results: \n\n1. The numbers reported in Figure 5 do not match with the performance of adversarial training in previous paper. In PGM L_inf adversarial training/attack (column 3 of Figure 5), the prediction accuracy is roughly 50% under 0.1 infinity norm perturbation. However, previous papers (e.g., \"Obfuscated Gradients Give a False Sense of Security\") reported 55% accuracy under 0.031 infinity norm perturbation. I wonder why the numbers are so different. \n\nMaybe it's because of different scales? Previous works usually scale each pixel to [0,1] or [-1,1], maybe the authors use the [0, 255] scale? But 0.1/255 will be much smaller than 0.031. \n\nAnother factor might be the model structure. If Alexnet has much lower accuracy, it's probably worthwhile to conduct experiments on the same structure with previous works (Madry et al and Athalye et al) to make the conclusion more clear. \n\n2. What's the training time of the proposed method compared with vanilla adversarial training? \n\n3. The idea of using SN to improve robustness has been introduced in the following paper: \n\"Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks\"\n(but this paper did not combine it with adv training). \n", "title": "Good paper, but I have some questions about the experimental results", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "B1xw3F5KhQ": {"type": "review", "replyto": "Hyx4knR9Ym", "review": "This paper proposes using spectral normalization (SN) as a regularization for adversarial training, which is based on [Miyato et. al., ICLR 2018], where the original paper used SN for GAN training. The paper also uses the results from [Neyshabur et. al., ICLR 2018], where the original paper provided generalization bounds that depends on spectral norm of each layer. \n\nThe paper is well written in general, the experiments are extensive. \n\nThe idea of studying based on the combination of the results from two previous papers is quite natural, since one uses spectral normalization in practice for GAN training, and the other provides generalization bound that depends on spectral norm. \n\nThe novelty of the algorithm itself is limited, since GAN and adversarial training are both minmax problems, and the original algorithm can be carried over easily. The experimental result itself is quite comprehensive. \n\nOn the other hand, this paper provides specific generalization bounds under three adversarial attack methods, which explains the power of SN under those settings. However, it is not clear to me that these are some novel results that can better help adversarial training.\n", "title": "spectral normalization for adversarial training", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SkljOqLS9X": {"type": "rebuttal", "replyto": "rkgEjrnm9Q", "comment": "Hello, thank you for your feedback and your interest in our work. Regarding your comments:\n\n1) References [1] and [2] propose standard ERM training while regularizing the Lipschitz constant to improve robustness of the trained network against future adversarial attacks. On the other hand, the main concern of our work is the lack of generalizability in *adversarial* training settings, e.g. FGM and PGM training, which can be significantly worse than in the ERM case as demonstrated by Schmidt et al. (2018). This observation is further supported by the generalization bounds in Theorems 1-4, which motivate the regularization of spectral norms. While there exist multiple approaches for regularizing the Lipschitz constant, we specifically propose applying spectral normalization because this allows us to directly enforce our adversarial generalization bounds.\n\n2) Thank you for bringing the recent NIPS work [3] to our attention. We note that while the two iterative approaches for computing a convolution layer\u2019s spectral norm both yield the same result, the implementations are different. [3]\u2019s computation of spectral norm requires computing the gradient of the Euclidean norm of the convolution operation. Ours leverages the deconvolution operation, which circumvents needing to take the gradient.\n\n3) We observed in several experiments (e.g. for training Inception over CIFAR10) that batch normalization helps with training speed but does not offer a considerable improvement in adversarial test accuracy over the no-regularization case. ", "title": "Re: On empirical contributions"}}}