{"paper": {"title": "Self-Supervised GAN Compression", "authors": ["Chong Yu", "Jeff Pool"], "authorids": ["chongy@nvidia.com", "jpool@nvidia.com"], "summary": "Existing pruning methods fail when applied to GANs tackling complex tasks, so we present a simple and robust method to prune generators that works well for a wide variety of networks and tasks.", "abstract": "Deep learning's success has led to larger and larger models to handle more and more complex tasks; trained models can contain millions of parameters. These large models are compute- and memory-intensive, which makes it a challenge to deploy them with minimized latency, throughput, and storage requirements. Some model compression methods have been successfully applied on image classification and detection or language models, but there has been very little work compressing generative adversarial networks (GANs) performing complex tasks. In this paper, we show that a standard model compression technique, weight pruning, cannot be applied to GANs using existing methods. We then develop a self-supervised compression technique which uses the trained discriminator to supervise the training of a compressed generator. We show that this framework has a compelling performance to high degrees of sparsity, generalizes well to new tasks and models, and enables meaningful comparisons between different pruning granularities.", "keywords": ["compression", "pruning", "generative adversarial networks", "GAN"]}, "meta": {"decision": "Reject", "comment": "The paper develops a new method for pruning generators of GANs. It has received a mixed set of reviews. Basically, the reviewers agree that the problem is interesting and appreciate that the authors have tried some baseline approaches and verified/demonstrated that they do not work. \n\nWhere the reviewers diverge is on whether the authors have been successful with the new method. In the opinion of the first reviewer, there is little value in achieving low levels (e.g. 50%) of fine-grained sparsity, while the authors have not managed to achieve good performance with filter-level sparsity (as evidenced by Figure 7, Table 3 as well as figures in the appendices). The authors admit that the sparsity levels achieved with their approach cannot be turned into speed improvement without future work.\n\nFurthermore, as pointed out by the first reviewer, the comparison with prior art, in particular with LIT method, which has been reported to successfully compress the same GAN, is missing and the results of LIT have been misrepresented. While the authors argue that their pruning is an \"orthogonal technique\", and can be applied on top of LIT, this is not verified in any way. In practice, combination of different compression techniques is known to be non-trivial, since they aim to explain the same types of redundancies.\n\nOverall, while this paper comes close, the problems highlighted by the first reviewer have not been resolved convincingly enough for acceptance."}, "review": {"r1x6iL19iH": {"type": "rebuttal", "replyto": "HkxGS3W9FH", "comment": "Thank you for your feedback and comments - these suggestions will make our submission stronger. Some responses to particular points follow:\n\n>> ... why put a GAN generator on a mobile device?\n\nAny real-time service using GANs, on a mobile device or otherwise, can benefit from model compression. General examples include mobile applications that perform style transfer, or video players that perform super-resolution on the client to save broadcast bandwidth. In the future, visual artists may rely on inpainting or other texture-generation techniques to save on asset storage space or interactive video generation to save rendering time, and musical artists may want a backing track to generate novel accompaniment that responds in real-time; all these tasks can be approached with GANs and may not work well with the latency associated with server-side execution. GANs have been used to augment training data, so, even in data center scenarios, having a more efficient generator can leave more resources available to training what may be a much more complex network.\n\n>> ... particular compression method ... pruning limits contribution\n\nCorrect, we only present results for network pruning.  However, given the much better results with our method, we believe it may help other techniques achieve more aggressive compression rates. (We leave this as future work in our conclusion.)  Further, we have shown that network pruning fails spectacularly on generators in the absence of our technique, which is a surprising result we have not seen reported before.\n\n>> \"Generalizes\" --> \"Applies to\"\n\nWe've addressed this to avoid making too broad a claim.\n\n>> In Section 4 the authors write: \u201cOur main insight is found,\u201d but then they describe the GAN method. What is the actual insight there?\n\nOur insight is, in fact, the next paragraph \u2013 we've fixed this to make it clear that reviewing the GAN method (the paragraph in question) leads to the insight (the next paragraph) of how to make fine-tuning a compressed model more stable and successful by using the (pre-trained) discriminator.\n\n>> Scores from Table 2 also support the claims, but the table itself is not referenced anywhere in the text.\n\nWhat an oversight!  We've fixed this.\n\n>> The analysis in Section 6 seems out of context with the rest of the paper. It is not clear how it relates to the \u201cself-supervised\u201d method.\n\nWeight pruning can be used to remove entire filters, not just individual elements (as noted by Reviewer #4), and 50% compression is somewhat modest (as noted by Reviewer #2). So, we included results for both filter pruning and pruning (elements and filters) more aggressively, to show that our method is successful, at least in more aggressive sparsity. While filter pruning was not as successful, this is yet more empirical evidence that pruning generators is not as straightforward a task as pruning classification or detection networks.\n\n>> Missing related work\n\nThanks for pointing these out - we've added this extra background.\n\n>> It would be good to first refer to Table 1.\n\nThank you for suggestion; this helps make the notation clear.\n\n>> Table 1: why is there a \u201c?\u201d only on the \u201cFixed\u201d column?\n\nWe've removed the \u2018?\u2019 from the \u201cCompressed\u201d and \u201cFixed\u201d columns.\n\nFinally, we'll work on fixing the font size for Figure 2 and finding a better solution to the sizable appendix.", "title": "Response to Review #1"}, "SkeoHSycjH": {"type": "rebuttal", "replyto": "Sklg5QfVFS", "comment": "Thank you for your thorough review. We'd like to respond to some particular comments:\n\n>> Only 50% compression\n\nWe chose to focus on 50% compression in early sections to highlight how easily existing methods fail at pruning generators and, in contrast, that ours succeeds. We share results for higher compression rates (up to 90%) using our method in Section 6, though we stress that we haven't spent any effort trying to find fancy fine-tuning schedules for each task, which may result in even higher compression rates.\n\n>> Generative loss\n\nEach task already has a generative loss; this is not a term that we have added. Rather, we modify the existing loss function to take into account the new (compressed) generator.  Here's what the TensorFlow DCGAN (image synthesis) tutorial has to say about the generator's loss term:\n\"The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, we will compare the discriminators decisions on the generated images to an array of 1s.\n\ndef generator_loss(fake_output):\n    return cross_entropy(tf.ones_like(fake_output), fake_output)\"\n(https://www.tensorflow.org/tutorials/generative/dcgan)\nWe simply add a new loss term, identical to this one, for the compressed generator. Since each model and task will have its own unique loss terms, we feel the best place to see these particular loss functions is in the baseline implementations.\n\n>> If I understood the framework properly, here, the compressed generator is trained from a random initialization.\n\nYour understanding is slightly incorrect \u2013 the compressed generator is initialized from the uncompressed generator (\u201cInit Scheme\u201d column in Table 1), which is trivial when the applied compression is something like pruning or quantization. In this way, the compressed sample distribution starts out close to the dense generator\u2019s. We've added some clarification to Section 4. We have used different random seeds in some experiments with stable results.\n\n>> Figures 2, 9, 42 and 43 are unreadable when printed with a regular color office printer.\n\nWe're working on Figure 2!  We've made it larger in the current revision.  The others, all in the appendix, may simply be best viewed in a digital format that supports zooming.  (As suggested by another reviewer, we may move the appendix material to a website.)\n\n>> Extra epochs\n10% is simply what we found to be the maximum needed for good results; others took only an extra 1% of the original training epochs. This is a hyperparameter that one may choose to tune for more aggressive sparsities.", "title": "Response to Review #2"}, "B1g3omy5jB": {"type": "rebuttal", "replyto": "SJx-uwtJjH", "comment": "Thank you for your time and feedback.  Please see our responses, below.\n\n>> Considering the success of large generative models (like BigGAN) one may wonder if these models can be compressed after being trained to improve practical applicability.\n\nThis is a great question - we look forward to extending our technique to the state-of-the-art GANs available today. Looking at other domains, pruning has still been successful on larger, more capable networks.\n\n>> The message of the article is misleading. The whole goal of pruning a neural network is to remove filters from it\n\nWe respectfully disagree with the assertion that the goal of pruning a network is to remove filters. This is one common approach used, but fine-grained (per-weight) pruning is not without merit - we show that our technique makes fine-grained pruning of generators possible; without this step, pruning filters would have no hope. Further, there are architectures (e.g. Cnvlutin2[1], SCNN[2]) that accelerate fine-grained pruning, as well as software approaches that achieve higher performance on more traditional architectures ([3],[4]). While our presented results for filter pruning show that it does not perform as well as on other tasks, we see this as an exciting area for future research. As with those other tasks for which filter pruning succeeded, work demonstrating the success of fine-grained pruning preceded filter pruning.\n\n[1] https://arxiv.org/abs/1705.00125\n[2] https://arxiv.org/abs/1708.04485\n[3] https://arxiv.org/abs/1802.10280\n[4] https://arxiv.org/abs/1804.10223 (not directly applicable to convolution-based GANs, but it shows that fine-grained pruning can offer a measurable benefit to real problems)\n\n>> The comparison in Figure 1 is arguably misleading as well. For example, one of the methods that were mentioned (LIT) does achieve a factor of 1.8 model compression, yet the comparison was not carried out directly with that method, but a modification proposed by the authors of this paper.\n\nWe agree \u2013 LIT, as originally reported, does achieve a 1.8x compression rate (noted at the bottom of our original page 3). The results in Table 1 and Figure 1 are with past approaches to compressed training or fine-tuning applied to model pruning. Put another way: using distillation on intermediate representations (LIT) and removing layers and removing layers achieves a reported compression rate of 1.8x. When performing distillation on intermediate representations (LIT) and pruning the generator, the quality at a compression rate of 2x is quantitatively and qualitatively worse than the original generator. We've tried to make the context for the results we report in Figure 1 clear in the caption, as well as in the supporting text.\n\n>> FLOPs or inference time\n\nPerformance will vary wildly based on particular architecture and hardware selected and is unfortunately out of the scope of this submission, which is hardware and architecture agnostic. Spending time aggressively pruning, optimizing for performance, and measuring against other baselines may be future work.\n\n>> Weights pruning is simply one of the approaches for model compression, so you cannot ignore the alternatives.\n\nWe agree that weight pruning is one of many approaches to compression, but we also point out that many approaches are orthogonal: taking the example of LIT, from above, one could apply weight pruning on top of the shallower network that results from the original LIT application. Quantization, also successful at compressing GANs in the past, has seen success when applied with pruning in other domains. This large cross product of comparisons is also out of the scope of this submission. Finally, without our submission, there would be no possibility of combining pruning with other techniques, as pruning has not been shown to succeed on generators prior to our work.\n\nHad our investigation into pruning generators been straightforward (\"We tried it, and it works\"), then performance would have been the a primary focus. Instead, we focused on explaining why naive pruning fails and devising a robust method that is able to prune some tasks up to 90% sparsity.\n\n>> Section 4 unorthodox notation\n\nWe added more general forms of equations 1,2,4, and 5 - thanks for the suggestions! Does this ease the understanding of section 4?", "title": "Response to Review #4"}, "BkxJ87yqjH": {"type": "rebuttal", "replyto": "Skl8EkSFDr", "comment": "We've updated the submission to address questions and take advantage of suggestions offered by the reviewers.", "title": "Updated submission"}, "SJx-uwtJjH": {"type": "review", "replyto": "Skl8EkSFDr", "review": "The problem tackled in the paper is the compression of generators in adversarially trained models. Considering the success of large generative models (like BigGAN) one may wonder if these models can be compressed after being trained to improve practical applicability. \n\nThis paper is focused on the compression of image to image translation models and uses distillation on discriminator's outputs to achieve better results.\n\nMy decision is weak reject.\n\nThe message of the article is misleading. The whole goal of pruning a neural network is to remove filters from it, therefore, reducing the computation or, at least, storage space for the parameters. The attempt to remove filters was presented in the last figure, and it does not work as good as all other results presented in the paper.\n\nThe comparison in Figure 1 is arguably misleading as well. For example, one of the methods that were mentioned (LIT) does achieve a factor of 1.8 model compression, yet the comparison was not carried out directly with that method, but a modification proposed by the authors of this paper.\n\nI would like to see more comparisons in terms of FLOPs or inference time between the baselines, SotA methods, and your proposed method. Weights pruning is simply one of the approaches for model compression, so you cannot ignore the alternatives.\n\nAlso, section 4 probably has to be rewritten, since some unorthodox notation is used. The authors should consider using some reference paper for the notations, like CycleGAN, that was mentioned in the paper. That will improve the clarity and readability of the used objectives.", "title": "Official Blind Review #4", "rating": "3: Weak Reject", "confidence": 2}, "Sklg5QfVFS": {"type": "review", "replyto": "Skl8EkSFDr", "review": "In this paper, the authors tackle the task of compressing a network. While there\nare many effective solutions so far regular computer vision tasks, as they demonstrate,\nthey fail catastrophically  when applied to generative adversarial networks(GANs).\nThey propose a modification to the classic distillation method, where a\n\"student\" network tries to imitate the uncompressed one under the supervision of\na fully converged discriminator network. They perform evaluation on multiple\ntasks from image synthesis to super-resolution. They also study the influence\nof the compression factor on the quality of the generated images.\n\nThe task is well motivated and situated in the related literature. The first\nsection is very thorough and extremely efficient at describing the failure modes\nof existing methods. On one side, the results demonstrated in the evaluation are\ncompelling, on the other side, the compression factor is only 50%, which is much\nlower than seen in related work. However, as it is shown in section 3 the task\nmay be much harder for GANS than regular models so I still consider it a \nsizeable contribution.\n\nThere are a couple of points that require clarification. I personally found the\ndescription of the method (Section 4) rather confusing. It is clear\nwhat \"discriminative loss\" is as it is the one used in every GAN.\nUnfortunately, I could not understand what \"generative loss\" means in the general\ncase. An example is given for StarGAN in equation (7) and I have a rough idea of\nwhat to choose for Style Transfer, Domain Translation, Super Resolution and\nImage translation. Though, it is unclear to me what to use in the case of\nimage synthesis. The experiments clearly show that it is possible so I think\nit is necessary to show how this framework is concretely applied to each task at\nhand.\n\nDuring training, the discriminator only ever saw pictures from the true distribution\nand the distribution generated by the generator (at each of its training steps).\nIf I understood the framework properly, here, the compressed generator is trained\nfrom a random initialization. The distribution it outputs is therefore\ncompletely unknown and potentially non overlapping with either of the true or\nthe generator ones. In that case it is hard to predict what the discriminator\nwould do on completely out of distribution samples. I seems reasonable to\nconjecture that it might consider them \"true\" because it was never trained on\nthem. Could you provide an explanation of why it is not a problem in practice?\nDo you have to try multiple initializations? Is the generative\nloss enough to force the compressed discriminator to match the support\nof the distribution of the dense generator?\n\nI think this paper is novel, tackles a hard task and presents compelling results\n(albeit using very mild compression ratios). It should be accepted if some\nclarifications are made in section 3.\n\nMinor Remarks:\n\n- Figures 2, 9, 42 and 43 are unreadable when printed with a regular color office\n  printer.\n- It is unclear what it would take an extra 10% of the original number of epochs to train the compressed network. Why couldn't it be faster, or much longer?\n\n\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 3}, "HkxGS3W9FH": {"type": "review", "replyto": "Skl8EkSFDr", "review": "This paper proposes a method to compress GANs. The motivation is that the current compression methods work for other kinds of neural networks (classification, detection), but perform poorly in the GAN scenario. The authors present intuitive reasons for why this is the case. \n\nHowever, the motivation why we would like to compress GANs is unclear to us. The intro mentions: reducing memory requirements and improving their performance. Sure, compressing networks for object detection and classification on mobile devices is really useful. But GANs are mainly used for unsupervised density estimation, why put a GAN generator on a mobile device? But maybe we are missing something here. \n\nTheir \u201cself-supervised\u201d method works by using the pre-trained discriminator network, while compressing only the generator. They show both qualitative and quantitative gains.\n\nThe paper is clear and well-written. It presents a way of pruning GAN generator network and although of limited novelty, it might be an interesting read as it provides extensive and convincing experiments in a clear manner. It does have several parts though which require additional clarification.\n\nThe idea of using the pre-trained discriminator network seems reasonable, but I am missing what the compression method for the generator network actually is (Section 4). From Table 2 I would assume it is pruning, in which case the paper\u2019s contribution is very limited.\n\nThe authors claim that the \u201cself-supervised\u201d method generalizes well to new tasks and models. \"Generalizes\" seems a strong word here, since the procedure compresses only the generator network. A more appropriate way of putting it might be \u2018can be applied to other tasks and models.'\n\nIn Section 4 the authors write: \u201cOur main insight is found,\u201d but then they describe the GAN method. What is the actual insight there?\n\nThe qualitative results in Figure 1 suggest that their \u201cself-supervised\u201d method is better than the other baselines. \n\nScores from Table 2 also support the claims, but the table itself is not referenced anywhere in the text.\n\nThe analysis in Section 6 seems out of context with the rest of the paper. It is not clear how it relates to the \u201cself-supervised\u201d method.\n\nMissing related work: 1st paragraph: compressing or distilling one network into another is much older than 2015, dating back to 1991 - see references in section 2 of the overview http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html \nThe GAN principle itself is also much older (1990) - see references in section 5 of the link above.  \n\nGeneral remarks:\n\nIn the first read of Section 3 it is not clear what [a], [b], [c] are.\n\nIt would be good to first refer to Table 1.\n\nTable 1: why is there a \u201c?\u201d only on the \u201cFixed\u201d column?\n\nIt would be good to have a larger font size in Figure 2, at least the size of the main text font.\n\nIn its current form, the pdf file has 100MBs (8MBs the main paper and the rest is the appendix). One could instead move the images from the appendix to a website and provide a link.\n\nWe might improve our rating provided the comments above were addressed in a satisfactory way in the rebuttal.\n\n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 4}}}