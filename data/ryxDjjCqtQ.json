{"paper": {"title": "Deconfounding Reinforcement Learning in Observational Settings", "authors": ["Chaochao Lu", "Jos\u00e9 Miguel Hern\u00e1ndez Lobato"], "authorids": ["cl641@cam.ac.uk", "jmh233@cam.ac.uk"], "summary": "This is the first attempt to build a bridge between confounding and the full reinforcement learning problem.", "abstract": "In this paper, we propose a general formulation to cope with a family of reinforcement learning tasks in observational settings, that is, learning good policies solely from the historical data produced by real environments with confounders (i.e., the factors affecting both actions and rewards). Based on the proposed approach, we extend one representative of reinforcement learning algorithms: the Actor-Critic method, to its deconfounding variant, which is also straightforward to be applied to other algorithms. In addition, due to lack of datasets in this direction, a benchmark is developed for deconfounding reinforcement learning algorithms by revising OpenAI Gym and MNIST. We demonstrate that the proposed algorithms are superior to traditional reinforcement learning algorithms in confounded environments. To the best of our knowledge, this is the first time that confounders are taken into consideration for addressing full reinforcement learning problems.", "keywords": ["confounder", "causal inference", "reinforcement learning"]}, "meta": {"decision": "Reject", "comment": "The paper studies RL based on data with confounders, where the confounders can affect both rewards and actions.  The setting is relevant in many problems and can have much potential.  This work is an interesting and useful attempt.  However, reviewers raised many questions regarding the problem setup and its comparison to related areas like causal inference.  While the author response provided further helpful details, the questions remained among the reviewers.  Therefore, the paper is not recommended for acceptance in its current stage; more work is needed to better motivate the setting and clarify its relation to other areas.\n\nFurthermore, the paper should probably discuss its relation to (1) partially observable MDP; and (2) off-policy RL."}, "review": {"Bkx0NwKtCX": {"type": "rebuttal", "replyto": "BJelvuEjnm", "comment": "Re (1): Refer to Section 2.3 and Section 2.4, where we describe more methods of adjusting for confounders.\n \nRe (2): The kidney stone example is used throughout the paper, referring to Section 1, Section 2.1, Section 2.2, Section 3.1, Footnote 2, Appendix F.2, and Appendix H.3.\n \nRe (3): Z, sampled using Equation (6), has to be used in reinforcement learning algorithms, because we need the state transition when generating trajectories/rollout. Refer to Section 4.1, Section 4.4, and Appendix E.\n \nRe (4): Refer to Appendix F.2 for an intuition of the difference, and to Section 4.4 in which, as shown in Figure 3(c), in each episode our deconfounding algorithm using p(r_{t+1}|z_t, do(a_t=a)) almost chooses the optimal action at each time step, whilst the vanilla algorithm using p(r_{t+1}|z_t, a_t) makes a wrong decision for more than half time.\n \nRe (5): Refer to Section 4.2 and Appendix H.3 for the details about how to define the confounding datasets in which the reward exactly depends on the action and the confounder. Also, a straightforward analogy of kidney stones to this confounding dataset is provided in Appendix H.3 as well.\n \nRe (6): Refer to Section 4.3.\nActually, to demonstrate the validity of our deconfounding model, denoted by M_decon, we compare with the original model (i.e., the model similar to that shown in Figure 1(b) but without the confounder u), denoted by M_orin. We train M_decon by optimizing Equation (19) but train Morin by a little different loss function excluding the confounder u whose full derivation can be found in Appendix C. Both models are separately trained in a batch manner on the training set (i.e., 140K sequences of length five of images) of the confounding dataset. Afterwards, following the steps depicted in Section 4.1, we use each trained model to perform the reconstruction task on the training set, and both reconstruction and counterfactual reasoning tasks on the testing set (i.e., 28K sequences of length five of images). Figure 2 presents a comparison of M_decon and M_orin, in terms of reconstruction and counterfactual reasoning on the confounding Pendulum dataset. The second row is based on M_decon (Figure 1(b)), whilst the top row comes from Morin. It is evident that the results generated by the deconfounding model is superior to those produced by the model not taking into account the confounder. To be more specific, as shown in the zoom of samples on the bottom row, Morin generates more blurry images than M_decon, because, without modelling the confounder u, M_orin is forced to average over its multiple latent states resulting in more blurry samples. \n \nRe (7): Refer to Section 4.4.\nwe will evaluate the proposed deconfounding actor-critic (AC) method by comparing with its vanilla version on the confounding Pendulum dataset. In the vanilla AC method, given a learned M_orin, we optimize the policy by calculating the gradient presented in Equation (21) on the basis of the trajectories/rollouts generated through M_orin. Equation (21) involves two functions: V (z_t; \u03c6_V ) and \u03c0(a_t|z_t; \u03b8), whose parameters can be found in Appendix J. It is worth noting that, in this vanilla case, each reward r_{t+1} is produced from the conditional distribution p(r_{t+1}|z_t, a_t). In contrast, the proposed deconfounding AC method is built on M_decon. Although the same gradient method (Equation (21)) is utilized to optimize the policy, we base the deconfounding AC approach on the different trajectories/rollouts generated by M_decon in which each reward r_{t+1} relies on the interventional distribution p(r_{t+1}|z_t, do(a_t)) computed using Equation (20).\n\nIn the training phase, for both vanilla AC and deconfounding AC, we run a respective experiment over 1500 episodes with 200 time steps each. In order to reduce non-stationarity and to decorrelate updates, the generated data is stored in an experience replay memory and then randomly sampled in a batch manner (Mnih et al., 2013; Riedmiller, 2005; Schulman et al., 2015; Van Hasselt et al., 2016). In each episode, we summarize all the rewards and further average the sums over a window of 100 episodes to obtain a smoother curve. As shown in Figure 3(a), obviously our deconfounding AC algorithm performs significantly better than the vanilla AC algorithm in the confounded environment.\n\nIn the testing phase, we first randomly select 100 samples from the testing set, each starting a new episode, and then use the learned policies to perform reasoning over 200 time steps as we did during the training time. From the resulting 100 episodes, we plot the total reward for each, shown in Figure 3(b), and compute the percentage of the optimal action T1 in each episode, presented in Figure 3(c). It is worth noting that Figure 3(c) tells us that in each episode our deconfounding AC almost chooses the optimal action at each time step, whilst the vanilla AC makes a wrong decision for more than half time.\n", "title": "More Detailed Rebuttal about Specific Comments"}, "S1xZZDFFCX": {"type": "rebuttal", "replyto": "BJelvuEjnm", "comment": "Regarding High-level Comments:\n \nRe (1): Refer to Abstract, Section 1, and especially to the last paragraph of Appendix A in which we explained the reason why we consider only adjusting for the confounder u.\nNote that, in our model we have to differentiate the two types of confounders: the time-independent confounder u and time-dependent confounders {z_t}(t=1,...,T), each playing a respective role in the model. The former, as a global confounder, will affect the whole course of treatment, and therefore should be adjusted for. In the example of kidney stones, the existence of the confounder (i.e., the size of stones) will lead to a wrong treatment if not adjusting for it. In contrast, the time-varying confounders {z_t} act as states in RL, which, in principle, should not be adjusted for, because the goal in RL is to learn a good policy in which any action is indeed supposed to be conditional on a specific state. On the other hand, in terms of rewards, what an agent expects at each time step is exactly the immediate reward when taking a specific action at a specific state, without the need of adjusting for states. It is worth noting that the case with time-varying confounders {z_t} can be thought of to meet a pseudo or weak causal sufficiency assumption under which the causal effects of actions on rewards will not be influenced by states at each time step (Zhang et al., 2017; 2015). This key difference motivates us to only adjust for the time-independent confounder u in this paper. In addition, as shown in Figure 1(b), in our case where a policy depending only on the confounder is applied to generating the data (Section 4.2), the arrow from z_t to at is not necessary so that z_t can be viewed as not a confounder of at and r_{t+1}. This also provides another reason why we do not need to adjust for z_t.\n \nRe (2)-(3): Refer to Section 2.4, Section 3.3, and Appendix A for the solution to identification. The main idea is that we used proxy variables to help identify causal effects of our model (Section 2.4 and Section 3.3). Besides, the causal parameters of our deconfounding model can be identified in the existence of multiple observed proxy variables (Appendix A).\n \nRe (4-5): Refer to the last paragraph of Appendix A for the difference between Z and u, and to Section 4.3 and Section 4.4 for both experiments with and without u. It is worth noting that, as shown in Figure 3(c), in each episode our deconfounding algorithm considering u almost chooses the optimal action at each time step, whilst the vanilla algorithm not considering u makes a wrong decision for more than half time.\n", "title": "More Detailed Rebuttal about High-level Comments"}, "S1gEh7FYR7": {"type": "rebuttal", "replyto": "SygU8Vhl0X", "comment": "Batch Norm is necessay, see the last page of the updated paper.", "title": "Batch-Norm"}, "H1lrLmtKA7": {"type": "rebuttal", "replyto": "Hyx9C2Vqhm", "comment": "Thanks for your comments. We have updated the paper and made the paper much clearer. We recommend you re-read the full updated paper to get some refreshed stuff.\n \nRe motivation and identification: Refer to Section 1.\n In this paper, we propose a general formulation to cope with a family of reinforcement learning tasks in observational settings, that is, learning good policies solely from the historical data produced by real environments with confounders (i.e., the factors affecting both actions and rewards). Actually, in recent years, reinforcement learning (RL) has made great progress, spawning a large number of successful applications especially in terms of games (Silver et al., 2016; Mnih et al., 2013; Ope- nAI, 2018). Within this background, much attention has been devoted to the development of RL algorithms with the goal of improving treatment policies in healthcare (Gottesman et al., 2018). In fact, various RL algorithms have been proposed to infer better decision-making strategies for me- chanical ventilation (Prasad et al., 2017), sepsis management (Raghu et al., 2017a;b), and treatment of schizophrenia (Shortreed et al., 2011). In healthcare, a common practice is to focus on the ob- servational setting, because ones do not wish to experiment with patients\u2019 lives without evidence that the proposed treatment strategy is better than the current practice (Gottesman et al., 2018) 1. As pointed out in (Raghu et al., 2017a), even if in the observational setting, RL also has advantages over other machine learning algorithms especially in two situations: when the ground truth of a \u201cgood\u201d treatment strategy is unclear in medical literature (Marik, 2015), and when training examples do not represent optimal behavior. On the other hand, although causal inference (Pearl, 2009) has been ex- tensively explored and used in healthcare and medicine (Liu et al.; Soleimani et al., 2017; Schulam & Saria, 2017; Alaa et al., 2017; Alaa & van der Schaar, 2018; Atan et al., 2016), the efficient ap- proach to dealing with time-varying data is still unclear (Peters et al., 2017; Hernn & Robins, 2018). On the basis of the discussion above, in this paper we attempt to combine advantages on both sides to cope with an important family of RL problems in the observational setting, that is, learning good policies solely from the historical data produced by real environments with confounding bias.\n \nRe the method: Refer to Section 2.4, Section 3.3, and Appendix A. The main idea is that we used proxy variables to help identify causal effects of our model (Section 2.4 and Section 3.3). Besides, the causal parameters of our deconfounding model can be identified in the existence of multiple observed proxy variables (Appendix A).\n \n \nRe quibbles: Refer to Section 3.1 and Section 3.4. We have to emphasize that the factorization assumption in terms of Gaussian is a widely used technique in machine learning communities. I agree that the real data is not made up of factorized Gaussians, but the assumption of factorized Gaussian is the first step and also the easiest way to deeply understand how the proposed approach works. It is not necessary to get ourselves lost in the complicated distributions, which is not beneficial to capturing some insights about the nature of the model. More importantly, even with such simplified assumption, experiments presented in Section 4.3 and Section 4.4 show that our model work much better. Especially as shown in Figure 3(c), in each episode our deconfounding algorithm almost chooses the optimal action at each time step, whilst the vanilla algorithm makes a wrong decision for more than half time.\n \nRe experiments: Refer to the whole Section 4 and the corresponding appendices. Note that, in this new draft we include all the details about the experiments, especially about how to design a reasonable reward and Appendix H.3 also provides a straightforward analogy to help readers understand the design. As mentioned above, Figure 3(c) demonstrated that the confounder u plays an extremely important role in reinforcement learning algorithms, because in that experiment, in each episode our deconfounding RL algorithm almost chooses the optimal action at each time step, whilst the vanilla RL algorithm makes a wrong decision for more than half time.", "title": "More Detailed Rebuttal"}, "SJl8CZFK0Q": {"type": "rebuttal", "replyto": "Byg9XiEs3X", "comment": "Thanks for your comments. We have updated the paper and please refer to Section 2.4, Section 3.3, and Appendix A of the new version for the solution to identification. The main idea is that we used proxy variables to help identify causal effects of our model (Section 2.4 and Section 3.3). Besides, the causal parameters of our deconfounding model can be identified in the existence of multiple observed proxy variables (Appendix A).", "title": "More Detailed Rebuttal"}, "HJxY5yhdC7": {"type": "rebuttal", "replyto": "Hyx9C2Vqhm", "comment": "Thanks for your comments. We have updated the paper and made the paper much clearer. We recommend you re-read the full updated paper to get some refreshed stuff.\n\nRe motivation: Refer to Section 1.\n\nRe the method: Refer to Section 2.4, Section 3.3, and Appendix A. \n\nRe quibbles: Refer to Section 3.1 and Section 3.4. We have to emphasize that the factorization assumption in terms of Gaussian is a widely used technique in machine learning communities. I agree that the real data is not made up of factorized Gaussians, but the assumption of factorized Gaussian is the first step and also the easiest way to deeply understand how the proposed approach works. It is not necessary to get ourselves lost in the complicated distributions, which is not beneficial to capturing some insights about the nature of the model. More importantly, even with such simplified assumption, experiments presented in Section 4.3 and Section 4.4 show that our model work much better. Especially as shown in Figure 3(c), in each episode our deconfounding algorithm almost chooses the optimal action at each time step, whilst the vanilla algorithm makes a wrong decision for more than half time.\n\nRe experiments: Refer to the whole Section 4 and the corresponding appendices. Note that, in this new draft we include all the details about the experiments, especially about how to design a reasonable reward and Appendix H.3 also provides a straightforward analogy to help readers understand the design. As mentioned above, Figure 3(c) demonstrated that the confounder u plays an extremely important role in reinforcement learning algorithms, because in that experiment, in each episode our deconfounding RL algorithm almost chooses the optimal action at each time step, whilst the vanilla RL algorithm makes a wrong decision for more than half time.\n", "title": "Rebuttal"}, "Skeu0k3O0Q": {"type": "rebuttal", "replyto": "ryxDjjCqtQ", "comment": "Thanks to all the reviewers. In the updated paper, we have addressed all the issues the reviewers are concerned about. If you have more questions, please feel free to contact us.", "title": "To all reviewers"}, "ryxNPyhd0X": {"type": "rebuttal", "replyto": "BJelvuEjnm", "comment": "Thanks for your comment. We have updated the paper and solved all the issues you are concerned about. In the following, we will point out which part in our new draft answers your question, respectively. Nevertheless, we still recommend you re-read the full updated paper to get some refreshed stuff.\n\nRegarding High-level Comments:\n\nRe (1): Refer to Abstract, Section 1, and especially to the last paragraph of Appendix A in which we explained the reason why we consider only adjusting for the confounder u.\n\nRe (2)-(3): Refer to Section 2.4, Section 3.3, and Appendix A for the solution to identification.\n\nRe (4-5): Refer to the last paragraph of Appendix A for the difference between Z and u, and to Section 4.3 and Section 4.4 for both experiments with and without u. It is worth noting that, as shown in Figure 3(c), in each episode our deconfounding algorithm considering u almost chooses the optimal action at each time step, whilst the vanilla algorithm not considering u makes a wrong decision for more than half time.\n\nRegarding Specific Comments:\n\nRe (1): Refer to Section 2.3 and Section 2.4.\n\nRe (2): The kidney stone example is used throughout the paper, referring to Section 1, Section 2.1, Section 2.2, Section 3.1, Footnote 2, Appendix F.2, and Appendix H.3.\n\nRe (3): Z, sampled using Equation (6), has to be used in reinforcement learning algorithms, because we need the state transition when generating trajectories/rollout. Refer to Section 4.1, Section 4.4, and Appendix E.\n\nRe (4): Refer to Appendix F.2 for an intuition of the difference, and to Section 4.4 in which, as shown in Figure 3(c), in each episode our deconfounding algorithm using p(r_{t+1}|z_t, do(a_t=a)) almost chooses the optimal action at each time step, whilst the vanilla algorithm using p(r_{t+1}|z_t,a_t) makes a wrong decision for more than half time.\nRe (5): Refer to Section 4.2 and Appendix H.3 in which a straightforward analogy is provided as well.\nRe (6): Refer to Section 4.3.\nRe (7): Refer to Section 4.4.\n", "title": "Rebuttal"}, "Bygl71nuC7": {"type": "rebuttal", "replyto": "Byg9XiEs3X", "comment": "Thanks for your comments. We have updated the paper and please refer to Section 2.4, Section 3.3, and Appendix A of the new version for the solution to identification. ", "title": "Rebuttal"}, "Byg9XiEs3X": {"type": "review", "replyto": "ryxDjjCqtQ", "review": "I have read the discussion from the authors. my evaluation stays the same.\n--------\nthis paper studies an interesting question of how to learn causal effects from observational data generated from reinforcement learning. they work with a very challenging setting where an unobserved confounder exists at each time step that affects actions, rewards and the confounder at next time step.\n\nthe authors fit latent variables models to the observational data and perform experiments.\n\nthe major concern is on the causal inference side, where it is not easy to claim anything causal in such a complicated system with unobserved confounders. causal inference with unobserved confounders cannot be simply solved by fitting a latent variable model. there exists negative examples even in the simplest setting that two distinct causal structure can lead to the same observational distribution. for example here, https://www.alexdamour.com/blog/public/2018/05/18/non-identification-in-latent-confounder-models/\n\nit could be helpful if the authors can lay out the identification assumptions for causal effects. before claiming anything causal and justifying experimental results.", "title": "interesting problem", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJelvuEjnm": {"type": "review", "replyto": "ryxDjjCqtQ", "review": "The paper addresses an important and often overlooked issue in off-policy reinforcement learning - the possibility of confounding between the agent's actions and the rewards. This is a subject which has been exhaustively explored in the causal inference literature, and the authors are very correct in suggesting that it should be incorporated into the world of reinforcement learning.  Specifically they propose a generative model with a global latent confounder that is inferred using a variational autoencoder architecture.  \n\nThe paper is generally well-written, though some points could be made clearer in my opinion, as detailed below. The experiments are constructed by introducing confounding into existing datasets; performance seems to be good, but I am not entirely sure whether the given architecture is necessary, see comments below. \n\nHigh-level comments:\n(1) Classic RL deals with confounders all the time. The state is a confounder between the action and the reward. The issue of confounding becomes less trivial when one is performing off-policy RL when the original policy is *unknown*. This is exactly the case that the authors mention when they cite the recent work by Gottesman et al. (2018) who deal with using RL to learn from the actions of physicians in a hospital.  While I am sure the authors are aware of these distinctions, I think the paper would be better if this is spelled out very explicitly. This includes explaining why this issue doesn't come up in classic RL.\n\n(2) Assuming the case above - off-policy RL with unknown confounders - one would usually assume \"no unmeasured confounding\", i.e. that the observed actions are an unknown but learnable function of the observed states. That is basically the scenario of most off-policy RL.\n\n(3) However, the authors strive to go one step beyond the case (2), to a situation where there is an *unmeasured* confounder affecting both observed actions and rewards. If nothing is known about this unmeasured confounder, then it is generally impossible to learn effective policies, as the causal effects of actions are not identifiable from the observed data. In this paper, the authors make an implicit assumption that while the confounder is unmeasured, it can still be inferred from the data. This is an intermediate step between \"no unmeasured confounding\" and \"complete unmeasured confounding\". This is related to work on using proxy variables e.g. Kuroki & Pearl (2014) and even more closely related to the work cited by Louizos et al. (2017).\nAgain, I think the paper would be much improved if all this is addressed explicitly. \n\n(4) An important consequence of point (3) above is that in fact adding the single global latent-confounder U is not, in itself, very important from a causal perspective. The sequence of variables Z_1... Z_T are already latent confounders that are assumed to be inferrable from data. It is true that the addition of the global U might change the statistical and optimization properties of the model. This leads to a very important conclusion: the authors should test their model with and without U. I think this specific ablation experiment is crucial. In many cases I am sure that the assumption of a global latent confounder is a good one and is especially useful in the VAE case where it will make optimization more stable. However, in principle, all of U's roles could be taken within the sequence of Z's, and I am curious to see in practice how big of an effect it has.\n\n(5) I wish to add that even if the U variable turns out to not add much empirically, this work is still valid since the sequence of Z's can themselves be considered inferred latent confounders.\n\nSpecific comments:\n(1) 2.3: there are more than 2 ways of computing the do-operator. RCTs and backdoor are the best known approaches, but not the only ones, e.g. there is frontdoor adjustment. \n\n(2) I think the paper would be easier to follow if there was one concrete example used throughout. This will make it easier to understand and possibly verify/criticize the assumptions of the generative model.\n\n(3) Related to \"higher-level point (4)\" above, in eqs. 17 & 18 note that Z_t is unknown, same as U. Both are inferred. This also leads to the question which Z_t is actually used in practice? Is it the mean, or is it also sampled from the approximate posterior q?\n\n(4) Below eq. 19, it would be very useful for the readers if you could explain exactly when would there be a difference between the two versions p(r_{t+1}|z_t,a_t) and p(r_{t+1}|z_t, do(a_t=a))\n\n(5) In the description of all the experiments I was missing a crucial point: how does the introduced confounder affect the reward? Is it only through the different actions? The way it is currently explained, it seems like the added variable introduces lack of *overlap*, but not strictly confounding.\n\n(6) The description of the experiment in 4.3 could be more detailed. What exactly was the training and test? What RL method was used? What did the baseline optimize for? I would like to see an ablation experiment where U is not included in the model. \n\n(7) In 4.5, what is the \"vanilla\" method? And as mentioned above, I would like to see an ablation experiment where U is not included in the model.  \n\n\n", "title": "Strong and important idea - presentation and execution can be improved ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hyx9C2Vqhm": {"type": "review", "replyto": "ryxDjjCqtQ", "review": "This paper presents a method for reinforcement learning (RL) in settings where the relationship between action and reward is confounded by a latent variable (unobserved confounder). While I firmly believe that RL would benefit from taking causality more seriously, this paper has many fatal flaws that make it not ready for publication. \n\nFirst, and most importantly, the paper is unclear about the problem it is trying to solve. It talks about confounded RL as being settings in which a confounder affects both the action and reward. In typical RL settings this wouldn\u2019t make sense: in RL you get to choose the policy so it doesn\u2019t make sense to assume that the choice of action is confounded while you\u2019re doing RL. To get around this, the authors assume that they\u2019re working with observational data and doing RL on a generative model leant from the observational data. But by doing this, they have assumed away the key advantage that RL has over causal inference: the ability to experiment in the world. The authors justify this assumption by considering high-stakes settings where experimentation is either too risky or too costly, but they don\u2019t explain why you would want to do RL at all when you could just do causal inference directly. If you can\u2019t experiment, RL offers no advantages over standard causal inference methods and bring serious disadvantages (sample-efficiency, computational cost, etc.). \n\n# Method\nThe authors learn a variational approximation to a particular graphical model that they assume for their RL setting. They then treat the variational approximation as the true distribution which allows them to perform causal inference via the backdoor correction. They claim this is identified but this is false - it is only identified with respect to the variational distribution, not the true distribution and we have no a priori reason to  believe that the variational distribution well-approximated the true distribution. In principle, the authors could have tested how well this works experimentally but their experimental setup has problems which prevent this being evaluated. \n\nQuibbles:\n - Page 3: the authors claim the model is \u201cwithout loss of generality\u201d but this is false - there are many settings that would not conform to this model: e.g. the multi agent settings that economics studies; health settings with placebo effects where reward depends on observations directly; etc.\n  - Page 4 above the equations: either the equations describe the variational approximation to the generative model or the equations shouldn\u2019t all be factorized normal distributions. Real data isn\u2019t made up of factorized normals.\n\n# Experiments\n\nThe authors evaluate their method on three simulated datasets: Confounding MNIST, Confounding Cartpole and Confounding Pendulum. All three have the same methodological problems so I\u2019ll only focus on the MNIST dataset. They synthesize their MNIST dataset but corrupting a subset of MNIST digits with noise and treating actions as rotations. Rewards are given by the absolute difference in angle between the rotated digit and the original unrotated digit. \u201cConfounding\u201d is added by having a binary latent variable affect the amount that the digit is rotated - but importantly, the reward isn\u2019t affected directly by the latent variable. Because of this, there isn\u2019t actually a confounding problem - the \u201cconfounder\u201d simply changes the rotation of the digit and can be treated as additional experimentation from the perspective of causal inference. The authors evaluate their method by examining reconstructions of the MNIST digit, but this simply checks how well the variational inference is working, not whether the causal inference is working (there would be no way to evaluate the latter on this dataset because there is no confounding). Effectively all they find is a better-designed variational distribution will do a better job of reconstructing the input (without modelling the latent u, the VAE is forced to average over its two states resulting in more blurry samples). \n\nThe RL evaluations aren\u2019t described in enough detail to conclusively explain the difference observed, but it seems to be driven by the fact that the standard RL methods are working with worse variational approximation distributions.\n\n# Summary\nThis work studies a setting in which the correct baselines would be causal inference algorithms (but they aren\u2019t considered) and their experimental evaluation has serious flaws that prevent it supporting the claims made in the paper. \n", "title": "Setting doesn't make sense for RL and experiments don't evaluate causal questions", "rating": "2: Strong rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyeZ0H9h2Q": {"type": "rebuttal", "replyto": "S1gO5nK3nm", "comment": "In our case, as shown in Fig 5, the posterior of u has an obvious difference from its unit Gaussian prior, even though their KL loss converged. Therefore, sampling from q is better.", "title": "Response"}, "HkguruuhnQ": {"type": "rebuttal", "replyto": "H1lpVE_3nQ", "comment": "Re \"Why use q anyway?\"\nPlease keep in mind that u is a latent variable. q(u|x, a, r) is the posterior containing the information from the data whilst p(u) is nothing but a prior. \n\nRe \"Why need to estimate x,a,r from z.\"\nBecause q(u|x, a, r) depends on (x, a, r) which are unknown during the testing phase. ", "title": "Response"}, "r1x1gx_hhm": {"type": "rebuttal", "replyto": "Skluaf-shX", "comment": "Yes, u is sampled from the model. In Equation (18), (x, a, r) in the posterior q(u|x, a, r) are estimated from the model rather than the observations. We will use different notations in the updated version.", "title": "Response"}, "ByxwiAPhhQ": {"type": "rebuttal", "replyto": "BJxhQ0ejhX", "comment": "You can think so, because all Gaussians in the paper are assumed to be with diagonal covariance matrices.", "title": "Response"}, "SJg7qLoY2X": {"type": "rebuttal", "replyto": "BJg3yE_t27", "comment": "Re 1. As described in Section 3.3.2, q(u|x, a, r) is modelled in the same way as q(z |x, a, r), each of which is parameterised by a bi-directional LSTM. However, unlike q(z |x, a, r) in which z is time-dependent (i.e., z_t corresponds to x_t, a_t, r_t at each time step), in q(u|x, a, r) u is independent of time steps, meaning that u combines all the information of the whole sequence.\n\nRe 2. Yes. ", "title": "Response"}, "r1xS8HyYn7": {"type": "rebuttal", "replyto": "SJe23dR_3X", "comment": "Exactly, we trained a separate architecture without u.", "title": "Response"}, "HyxkLV9DhQ": {"type": "rebuttal", "replyto": "rJlf2Fdv3m", "comment": "Thank you for your comment.\n\nRe 1. We exactly followed the same setting of Healing MNIST in [1]. As [1] said, \u201cthe squares within the sequences are intended to be analogous to seasonal flu or other ailments that a patient could exhibit that are independent of the actions and which last several timesteps\u201d. We want to show that our model can learn long-range patterns, which plays an important role in medical applications. The squares are added after the rotation. \n\nRe 2. \u201cSome policy\u201d can be any policy, e.g., random rotations or rotations toward vertical. But in our case, considering the confounder, we used the policy where action is affected by the confounder u. \n\nRe 3. Simply speaking, the confounder not only affects the action through the magnitude, 22.5 \u2264 |a| \u2264 45 or 0 \u2264 |a| < 22.5, but also affects the reward through the direction (i.e., clockwise or counterclockwise), a or -a, where a and -a will result in different rewards.\n\nWe will clarify these in the new draft. Thank you for your suggestion. \n\n[1] Rahul G Krishnan, Uri Shalit, and David Sontag. Deep kalman filters. arXiv preprint arXiv:1511.05121, 2015.\n", "title": "Response"}, "SJewqGpE37": {"type": "rebuttal", "replyto": "SyelGt3E3m", "comment": "Dear reader,\n\nRe 1: Actually dimension of u can be any number. In the paper we let u=2 because it is easy to visualise it in the 2D plot as shown in Figure 5.\n\nRe 2: 512 = 4 x 4 x 32, so it can be reshaped to a square 4 x 4 with 32 channels.\n\nWe hope this helps.", "title": "Just as described in the paper"}, "SJgBBuGQ2m": {"type": "rebuttal", "replyto": "B1gSHMy7nX", "comment": "Thank you for your comment.\n\nRe 1: Exactly.\n\nRe 2: In default, softplus is applied after each intermediate layer, and the explicitly claimed activation functions are applied to the output of the final layer. Here, take f5 and f6 for example, f5 and f6 share the parameters of the first five FC100 layers, each followed by softplus, but have different parameters in their own final layer, i.e., FC1 has two outputs (the output of f5 and the output of f6) which are respectively followed by sigmoid and softplus.\n\nRe 3: In our architecture, each function modelled with a neural network has only one output: mu or sigma^2. Since each pair of functions (e.g., f1 and f2, f5 and f6, etc.) share the parameters of all layers except the final layer, that is, they have their own final layers which are parallel outputs, each followed by a respective activation function. In the case of f5 and f6, as described in Re 2, FC1 has two outputs, each followed by sigmoid and softplus, representing mu (the output of f5) and sigma^2 (the output of f6), respectively. We will clarify this with a figure in the appendix of the new draft. Thank you for your suggestion.\n", "title": "Response"}, "SygNdtNXsQ": {"type": "rebuttal", "replyto": "H1lyWAJXoX", "comment": "As we know, there always is a trade-off between time and accuracy in MC methods. Considering this balance, we set N= 400 for experimental results. ", "title": "Trade-off"}, "rJeq_vV7sm": {"type": "rebuttal", "replyto": "HyxA4Dozom", "comment": "Thank you for your interest in our paper. Happy to discuss with you all the stuff about causal concepts.\n\nRe Thought (1): What you understand about counterfactual reasoning is exactly right, which is also what we did in the paper. Given the fact that we know the training set (e.g., we knew the fact that what happened when we took action a_1 at x_1 in the training set), we want to know \u201cwhat would have happened had we taken a different action a_2 at an unseen x_2 in the testing set?\u201d That is exactly what counterfactual reasoning does. Note that, like ones did in [1], we primarily emphasise the inference on unseen data, which plays a pivotal role in the following RL part.\n\nRe Thought (2): in our paper, we do not have the interventional distribution p(a | x). \na) If what you meant is q(a | x), it is an auxiliary distribution in the variational inference part, for which we did not take intervention into account, because that is not what we studied in the paper;\nb) If you meant p(a | z), that is actually the policy function in RL. Usually we also do not factor the intervention into the policy, because the definition of the policy is a conditional distribution.\nActually we only considered the intervention in the reward function p(r | z, do(a)) where z is treated as a fixed constant at each time step.\n\nWe hope this helps.\n\n[1] Rahul G Krishnan, Uri Shalit, and David Sontag. Deep kalman filters. arXiv preprint arXiv:1511.05121, 2015.\n", "title": "It actually is."}, "Byx_yQIbim": {"type": "rebuttal", "replyto": "Bkg9HFNWsX", "comment": "Thank you for your useful feedback and we are glad you like the idea.\n\nAs described in the last paragraph on Page 1, generally speaking, the training pipeline consists of two steps: \nStep 1: Given the time-independent confounding assumption (Section 3.1), we learn the deconfounding model, as presented in Fig 2, from the observational data;\nStep 2: We optimise the policy or Q-function based on the deconfounding model we learned in Step 1.\n\nMore specifically, in Step 1, given the observational data (x, a, r), we optimise the variational lower bound (Eq.11) with two extra terms (Eq.15 and Eq.16). Once the deconfounding model is learned, we know the state transition function p(z_t | z_{t-1}, a_{t-1}) and can also calculate the deconfounding reward function p(r_t | z_t, do(a_t)) according to Eq.17. In Step 2, we treat the learned deconfounding model as a RL environment like CartPole in OpenAI Gym, and directly exploit it to generate trajectories/rollouts through the state transition function and the deconfounding reward function. On the basis of the generated trajectories/rollouts, we can train the Q-network using Eq.19 or the policy network using Eq.20.\n\nWe will clarify this in the new draft. Thank you.\n", "title": "Thank you for your useful feedback."}, "rJxsAvR1jX": {"type": "rebuttal", "replyto": "BylZvATko7", "comment": "Thank you for your interest in our paper and for checking our formula. \n\nIn the model part, you are exactly right, where p(z, u | x, a, r) != p(u | x, a , r) p(z | x, a, r). However, in the variational inference part, for simplicity, we use the well known trick, factorization assumption, to obtain q(z, u | x, a, r) = q(u | x, a , r) q(z | x, a, r) as we claimed on Page 13. \n\nActually, we can think q(u | z, x, a, r) = q(u | x, a, r) because (x, a, r) already contains all the information about z.", "title": "It is actually the widely used factorization assumption in variational inference."}}}