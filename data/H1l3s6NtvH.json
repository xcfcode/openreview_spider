{"paper": {"title": "A Bayes-Optimal View on Adversarial Examples", "authors": ["Eitan Richardson", "Yair Weiss"], "authorids": ["eitan.richardson@gmail.com", "yweiss@cs.huji.ac.il"], "summary": "We show analytically and empirically that the Bayes-optimal classifiers are, in some settings, vulnerable to adversarial examples. We then show that even when the optimal classifier is robust, trained CNNs are vulnerable.", "abstract": "Adversarial attacks on CNN classifiers can make an imperceptible change to an input image and alter the classification result. The source of these failures is still poorly understood, and many explanations invoke the \"unreasonably linear extrapolation\" used by CNNs along with the geometry of high dimensions.\nIn this paper we show that similar attacks can be used against the Bayes-Optimal classifier for certain class distributions, while for others the optimal classifier is robust to such attacks. We present analytical results showing conditions on the data distribution under which all points can be made arbitrarily close to the optimal decision boundary and show that this can happen even when the classes are easy to separate, when the ideal classifier has a smooth decision surface and when the data lies in low dimensions. We introduce new datasets of realistic images of faces and digits where the Bayes-Optimal classifier can be calculated efficiently and show that for some of these datasets the optimal classifier is robust and for others it is vulnerable to adversarial examples. In systematic experiments with many such datasets, we find that standard CNN training consistently finds a vulnerable classifier even when the optimal classifier is robust while large-margin methods often find a robust classifier with the exact same training data. Our results suggest that adversarial vulnerability is not an unavoidable consequence of machine learning in high dimensions, and may often be a result of suboptimal training methods used in current practice.", "keywords": ["Adversarial Examples", "Generative Models"]}, "meta": {"decision": "Reject", "comment": "The paper studies how adversarial robustness and Bayes optimality relate in a simple gaussian mixture setting. The paper received two recommendations for rejection and one weak accept. One of the central complaints was whether the study had any bearing on \"real world\" adversarial examples. I think this is a fair concern, given how limited the model appears on the surface, although perhaps the model is a good model of any local \"piece\" of a decision boundary in a real problem. That said, I do not agree with the strong rejection (1) in most places. The weak reject asked for some experiments. The revision produced these experiments, but I'm not sure how convincing these are since only one robust training method was used, and it's not clear that it's the best one could do among SOTA methods. For whatever reason, the reviewers did not update their scores. I am not certain that they reviewed the revision, despite my prodding."}, "review": {"GDMg3HsuL": {"type": "rebuttal", "replyto": "H1l3s6NtvH", "comment": "We believe we have fully addressed the reviewers' concerns in the revised paper. Unfortunately, as the meta-review  states: \"I am not certain that they reviewed the revision, despite my prodding\".\n\nA revised and improved version of this paper is now submitted to a different venue.\n\nThe one reviewer who did read the revised version wrote: \"The contribution of the two datasets is, in my opinion, an extremely important contribution in studying adversarial robustness.\" \n\"Overall, this paper is a very promising step in studying adversarial robustness\".", "title": "Revised version"}, "S1x7ZrQnsB": {"type": "rebuttal", "replyto": "H1l3s6NtvH", "comment": "We thank the reviewers for their comments and interaction during the rebuttal phase. We encourage all of them to read the latest version, which has been significantly revised following their comments. Specifically we have improved the discussion of related work, clarified the conclusions and added details about the MFA model. Perhaps more importantly, we have added new experiments including adversarial training of CNNs and experiments on real data. Overall we believe the new version supports the conclusions in our previous version but using stronger experimental evidence and more context.", "title": "Significantly Revised Version"}, "B1x6oNXhjB": {"type": "rebuttal", "replyto": "BkeYHjwqsB", "comment": "We appreciate the clarifications about the reviewer\u2019s concerns.\n\nThe sentence \u201cFrankly speaking,  these observations seem straightforward as the optimal decision plane would be biased towards the direction with the minimum variance. \u201c  seems to suggest a misunderstanding. Our analysis is for general classifiers (not linear classifiers) and the directions of the adversarial examples are not necessarily in a direction of minimum variance. The situation with general classifiers is more subtle than the sentence suggests, and by understanding this situation we have been able to construct realistic image datasets where all the points are arbitrarily close to the decision boundary for an optimal, nonlinear classifier.  Note also that our analysis is not just for a mixture of Gaussians (see observations 3 and 4). \n\nRegarding \u201cAren't data or model  the only two factors that would affect the training of any algorithms? \u201c.  Note that many authors have suggested that high dimensionality is critical for adversarial examples, and we have shown that this is not a necessary nor a sufficient factor for the presence of adversarial vulnerability. We have also suggested a third factor (expressive power) that explains why the linear SVM fails to find a robust classifier while the RBF SVM does. \n\nWe are not sure we understand how analyzing the distribution of some deep layer features can help, since due to the strong non-linearity, the features could be separable with a large margin while at the same time the input images (in the original image space) might not be.\n\nRegarding the relevance of our symmetric dataset to real data, we added an experiment (Table 2) that shows that CNNs trained on the symmetric data can classify real test images (with some expected degradation in accuracy). We also trained the three models (CNN, Linear and RBF SVM) on real CelebA Male/Female data and got similar results to our symmetric data experiment (i.e. CNN and linear SVM are vulnerable and RBF SVM is robust), see figure 12.\n\nRegarding robustified CNNs, we added an experiment in which we train the CNN using state-of-the-art adversarial training method and got only a small improvement in robustness (figure 10).\n\nWe hope that you will read the revised version and our clarifications and see if it changes your opinion.\n", "title": "Additional response to Reviewer #3"}, "rygw74Q2iS": {"type": "rebuttal", "replyto": "HklATUMvir", "comment": "Thank you for your comments. \n\nPlease note that in the final version, we have added an additional experiment following your suggestion. Specifically, we train CNNs, Linear SVMs and RBF SVMs on real data. We find a similar pattern in the real data that we found in our synthetic data.  In particular, for the real Male/Female data, CNN and Linear SVMs learn a very vulnerable classifier, while RBF SVM learns a robust classifier whose adversarial examples are perceptually meaningful (figure 12).\n", "title": "Additional response to Reviewer #1"}, "SkehB7mnsH": {"type": "rebuttal", "replyto": "rJlF6V3Osr", "comment": "Thank you again for your comments and suggestions. \n\nRegarding the suggested experiments with projecting all images down to a linear subspace, note that in the MFA model there are many linear subspaces, and different regions of $R^d$ would need to be projected to a different subspace, so this is a complicated idea to try.  We have added a figure (figure 8) in the appendix to better illustrate the geometry of MFA models, and the roles of $A$ and $\\sigma$. \nWe have also added a new experiment with real data, suggesting that the robustness of different classifiers on real data is similar to what we observed in the synthetic data. In particular, for the real Male/Female data, CNN and Linear SVMs learn a very vulnerable classifier, while RBF SVM learns a robust classifier whose adversarial examples are perceptually meaningful.\n", "title": "Additional response to Reviewer #2"}, "HJeV9gIkqr": {"type": "review", "replyto": "H1l3s6NtvH", "review": "The paper analyzed the adversarial examples from the Bayes-optimal view. Specifically, the authors analyzed the relationship between the symmetry of covariance of data distribution and the amount of data which are close to the decision boundary. The authors proved that when the covariance of data distribution is asymmetric, a large amount of data will be close to the decision boundary (easy to be attacked). The authors also provided the new datasets which is easy to compute for the bayes-optimal classifier so as to verify the effect of symmetry of covariance on vulnerability of classifier. Moreover, the paper indicated that the vulnerability of CNNs is due to asymmetric distributions or non-optimal learning. \n\nIt is interesting that the paper investigated the adversarial examples from the Bayes-optimal view. However, there are some drawbacks: \n\n1.\tThe motivation of this paper is not clear to me. In other words, what is the benefit of analyzing the adversarial examples from the Bayes-optimal viewpoint, since Bayes model mentioned in this paper is easy to attack. I am not fully convinced by the presentation of the paper.\n\n2.\tThe theorem or the observation in the paper appears too straightforward. And the \u2018observation 1\u2019is not general. The authors may need to consider more general cases that when the standard deviation of eigen value of covariance matrix is large, the Bayes model will be easily attacked. (not just the case that one of eigen value is zero). \n\n3.\tOne minor point, it appears somewhat strange that \u201cobservations\u201d were proved. It is better to change observations to theorems or lemmas.\n\n4.\tThe authors tried to explain directly the vulnerability of CNN in a same way. However, CNN is a totally different model compared with the Bayes model (one is a discriminative model and the other is a generative model). For generative models, the classification boundary is closely related to all training samples. Therefore, the variance of data distribution is important for attack. For discriminative models, the decision boundary is related to local information. It may not be proper to analyze CNN in the way same as the Bayes model. This should be further clarified and discussed. \n", "title": "Official Blind Review #3", "rating": "1: Reject", "confidence": 4}, "rJxFL5b0YB": {"type": "review", "replyto": "H1l3s6NtvH", "review": "This paper proposes studying adversarial examples from the perspective of Bayes-optimal classifiers. They construct a pair of synthetic but somewhat realistic datasets\u2014in one case, the Bayes-optimal classifier is *not* robust, demonstrating that the Bayes-optimal classifier may not be robust for real-world datasets. In the other case, the Bayes-optimal classifier is robust, but neural networks fail to learn the robust decision boundary. This demonstrates that even when the Bayes-optimal classifier is robust, we may need to explicitly regularize/incentivize neural networks to learn the correct decision boundary.\n\nThe contribution of the two datasets (the symmetric and asymetric CelebA) is, in my opinion, an extremely important contribution in studying adversarial robustness and on their own these datasets warrant further study. Previously, all studies of this sort had to be done with small-scale classifiers and simplistic datasets such as Gaussians. The paper also definitively proves that there are realistic datasets where the Bayes-optimal classifier is non-robust, which goes against quite a bit of conventional wisdom in the field and opens up many new paths for research. However, there are a few (in my opinion) critical concerns that currently bar me from strongly recommending acceptance of the paper. I outline these below.\n\n1. Prior work: the paper seems to ignore a plethora of prior work around studying adversarial robustness and understanding its roots. For example, a few very closely related works are as follows:\n   - Adversarial examples are not Bugs, they are Features (https://arxiv.org/abs/1905.02175): Ilyas et al (2019) demonstrate that adversarial perturbations are not in meaningless directions with respect to the data distribution, and in fact a classifier can be recovered from a labeled dataset of adversarial examples. While not in conflict with this work, it does closely relate and discuss many of the same issues discussed in this work, so relating them would be fruitful.\n\n   - A Discussion of Adversarial Examples are not Bugs they are Features (https://distill.pub/2019/advex-bugs-discussion/): Nakkiran (2019) actually constructs a dataset (called adversarial squares) where the Bayes-optimal classifier is robust but neural networks learn a non-robust classifier due to label noise and overfitting. Interestingly, they also construct a dataset where they Bayes-optimal classifier is robust and neural networks *do* learn a robust classifier (adversarial squares sans label noise). While I think the datasets presented in this work are much more interesting and certainly more realistic, this work should be put in context.\n\n    - Excessive Invariance causes Adversarial Vulnerability (https://arxiv.org/abs/1811.00401v3): Jacobsen et al offers an explanation for adversarial examples based on the fact that NNs are not sensitive to many task-relevant changes in inputs, which seems to tie in nicely to the discussion in this paper, as under the presented setup the Bayes-optimal classifier will certainly exploit (and be somewhat sensitive) to such changes.\n\n    - Adversarially robust generalization requires more data (https://arxiv.org/abs/1804.11285): Schmidt et al show a setup where many more samples are required for adversarial robustness than for standard classification error. And it seems to have very relevant connections to your work.\n\n    - In general this list is not comprehensive either: there are many relevant connections to the robustness-accuracy tradeoff (https://arxiv.org/abs/1901.08573, https://arxiv.org/abs/1805.12152), and other works. \n\n2. Discussion/interpretation of the results: \n    - Sufficient vs necessary: While the experimental design and results are both of very high quality, I am slightly confused about the interpretation of the results. First, if my understanding of the paper is correct, the experiments show that (a) the Bayes-optimal classifier can be non-robust in real-world settings, and (b) even when the Bayes-optimal classifier is robust, NNs can learn a non-robust decision boundary. In particular, (b) indicates that it may be *necessary* to design regularization methods that steer NNs towards the correct decision boundary\u2014it says nothing about whether these regularization methods will be *sufficient*, which the paper seems to suggest, e.g. in the abstract \"our results suggest that adversarial vulnerability is not an unavoidable consequence of machine learning in high dimensions, and may often be a result of suboptimal training methods used in current practice.\" In fact, if real-world datasets end up being like the asymmetric dataset, then the results of this paper would actually indicate the *opposite* of the above statement. It is unclear on what basis one can say that real-world datasets are more like the symmetric case or the asymmetric case. I believe a more measured conclusion (perhaps that we *need* more regularization methods, but even then we may not be able to get perfect robustness and accuracy) would better fit the strong results presented in the paper.\n\n    - CNN vs Linear SVM: I am confused about why we would expect a CNN to be able to learn the Bayes-optimal decision boundary but not the Linear SVM. The paper justifies the adversarial vulnerability of the Linear SVM by arguing that the Bayes-optimal classifier is not in the Linear SVM hypothesis class, which makes sense. The RBF SVM, for small enough bandwidth can express any function and is convex, so no argument needs to be made about its ability to find the Bayes-optimal classifier. For CNNs, however, it is unclear if the Bayes-optimal classifier lies in the hypothesis class (there are \"universal approximation\" arguments but these usually require arbitrarily wide networks and are non-constructive)\u2014couldn't it be that the CNNs used here is in the same boat as the Linear SVM (i.e. the Bayes-optimal decision boundary is not expressible by the CNN?) \n\n3. Experimental setup: \n    - One somewhat concerning (but perhaps unavoidable) thing about the experimental setup is that all the considered datasets are not perfectly linearly separable, i.e. the Bayes-optimal classifier has non-zero test error in expectation, and moreover the data variance is full-rank in the embedded space. This is in stark contrast to real datasets, where there seem to be many different ways to perfectly separate say, dogs from cats, and the variance of the data seems to be very heavily concentrated in a small subset of directions. I am concerned that these properties are what drive the Bayes-optimal classifier for the symmetric dataset to be robust (concretely, if 0.01 * Identity was not added to the covariance matrix of the symmetric model and the covariance was left to be low-rank, then any classifier which was Bayes-optimal along the positive-variance directions would be Bayes-optimal, and could behave arbitrarily poorly along the zero-variance directions, still being vulnerable). This concern does not make the contribution of the symmetric dataset less valuable, but a discussion of such caveats would help further elucidate the similarities and differences of this setup from real datasets. \n\n    - It is unclear if what is lacking from the NN is explicit regularization, or just more data. In particular, with such low-variance directions, at standard dataset sizes the distributions generated here are most likely statistically indistinguishable from their robust/non-robust counterparts (you can see hints of this in the fact that the CNN gets . While completely alleviating this concern may once again be quite difficult/impossible, it could be significantly alleviated by generating training samples dynamically (at every iteration) instead of generating a dataset in one shot and training on it. It would be very interesting to see whether these results differ at all from the one-shot approach here.\n\n4. A suggestion rather than a concern and not impacting my current score: but it would be very interesting to see what happens for robustly trained classifiers on the symmetric and asymmetric datasets.\n\nOverall, this paper is a very promising step in studying adversarial robustness, but concerns about discussion of prior work, discussion of experimental setup, and conclusions drawn, currently bar me from recommending acceptance. I would be more than happy to significantly improve my score if these concerns can be addressed in the revision and corresponding rebuttal.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 4}, "rklS7BYOjB": {"type": "rebuttal", "replyto": "HJx7phQwjH", "comment": "Thank you again for these detailed and constructive comments.\n\nPrior works. We agree that the most closely related work should be discussed more and have modified the version accordingly. Note that the analysis in section 4 of  Ilyas et al. is for linear classifiers (or equivalently Gaussians with the same covariance for the two classes). When the covariance of the Gaussians is different between the two classes, the Bayes Optimal classifier is nonlinear, so observation 1 is already quite different from the analysis in Ilyas et al. (and of course observations 2,3,4 are even more different). We have also discussed Nakkiran\u2019s construction in section 4 (along with a very similar construction that appeared in Tanay and Griffin). \n\nExperiments:\n\nThe graph of data points vs robustness is in the appendix of the revised version (figure 8, left). \n\nIndeed in a mixture of factor analyzers (MFA) model, each component has covariance which is a sum of a low rank matrix (this models the covariance on the local manifold of images) plus a diagonal matrix (this measures the covariance outside the manifold). Without the diagonal component, the covariance is not full rank and hence is not a valid covariance matrix. In order for an MFA model to produce realistic images, the sigma needs to be small (once sigma is above 0.03 approximately, the generated images appear quite noisy and unrealistic). We have now added additional experiments (figure 9) with ranges of sigma ranging from 0.001 (as was used in the original version) and up to 0.05. There does seem to be a small increase in robustness of the learned CNN but it is still very brittle and far from the robustness of the Bayes-Optimal. Note also that increasing sigma is equivalent to simply adding Gaussian IID noise to each pixel of the training examples, which has previously been reported to give a small increase in robustness of CNNs, but is usually considered to be inferior to adversarial training.\n\nRegarding the \u201cdogs\u201d vs. \u201ccats\u201d, we think this is perhaps a philosophical argument. In our view, for most realistic classification experiments, there is a unique Bayes Optimal classifier that would maximize accuracy. We agree that when there are regions of the input space that have exactly zero probability under both classes, then the Bayes-Optimal classifier is not unique, but for images that are measured with physical cameras, that seems to us highly implausible (and if it were true, then adversarial attacks should be restricted to not use these zero probability parts of the space). We do agree with the concern that the sigma we used was so small as to be effectively zero, but as noted above, our results still hold with a sigma that is 50 times larger, where the noise is visible even in a single image.\n    \nRegarding Nakkiran\u2019s squares, we believe it is consistent with our discussion at the end of the paper: several authors have pointed out that CNNs trained using SGD appear to have some implicit form of regularization that favors large margins, but the strength of this regularization depends in a very complicated manner on the exact details of the training. For simple toy problems, even a tiny amount of regularization may be enough to ensure robustness, and we believe this is the case with Nakkiran\u2019s squares (where a linear SVM would already be robust). But for more realistic and complicated  datasets, such as the ones that we have presented in our paper,  the amount of  regularization that is given by \u201cstandard\u201d CNN training is not enough, and the challenge is to develop explicit regularization methods that would allow us to efficiently explore accuracy and robustness in such datasets.\n", "title": "Re: Re: Response to Reviewer #2"}, "HkgDaPzvsr": {"type": "rebuttal", "replyto": "HJeV9gIkqr", "comment": "Thank you for reading our paper. We hope we can clarify (below) some misunderstandings and hope that you will read it again in the light of our comments.\n\n\u201cWhat is the benefit of analyzing the adversarial examples from the Bayes-Optimal viewpoint, since Bayes model mentioned in this paper is easy to attack.\u201d Actually,  as we show in the paper, the Bayes-Optimal classifier may be easy or hard to attack depending on the data distribution. More generally, adversarial examples present an intriguing and complex phenomena, and we believe that analyzing them from the Bayes-Optimal perspective allows us to disentangle the different possible causes. \n\nWe didn\u2019t fully understand the comment that \u201cobservation 1 is not general\u201d. Specifically, we do not require zero eigenvalue for the vulnerability condition to occur (but asymmetry in the variance).\n\n\u201cCNN is a totally different model compared with the Bayes model (one is a discriminative model and the other is a generative model)\u201d. Actually the Bayes-Optimal model (as its name suggests) gives the best possible accuracy in the discrimination task. Thus if the CNN wants to optimize the accuracy, then it should agree with the Bayes-Optimal model. \n", "title": "Response to Reviewer #3"}, "HklATUMvir": {"type": "rebuttal", "replyto": "SyxjYpuXqH", "comment": "Thank you for your review. We have uploaded a new version with additional experiments based on your suggestions. We hope you take a second look at the paper and see if it changes your recommendation.\n\nRegarding your major comments:\n\nAdversarial training. We included an additional experiment where we used robust training for the CNN (We used the TRADES method, from Zhang et al. 2019, which was the winner in a recent adversarial training challenge). As shown in the appendix, this gave only a modest increase in robustness. Still far from the robustness of the Bayes-Optimal classifier (or the RBF SVM). There are of course many different variants of robust training, but this result shows that the problem of effectively optimizing for both accuracy and robustness is much more difficult in CNNs compared to shallow architectures.\n\nSimilarities between synthetic datasets and real datasets. We do believe that the synthetic datasets capture much of the variability that appears in the real datasets. An additional experiment we performed (now in the appendix) is to train a CNN on the synthetic data and measure its performance on the real data. The results show that training on synthetic data, generalizes reasonably well to the real data (with approximately 5% drop in accuracy). \n\nWe focused on experiments with synthetic datasets because they allow us to systematically explore the different possible causes of adversarial examples (for example, having a robust optimal classifier indicates that the cause of the vulnerability of a trained model does not lie in the data). While we agree that the robustness of RBF SVMs on real data is interesting to pursue, this is not the main focus of our work, which aims to disentangle the different causes of adversarial vulnerability. \n\nWe have also changed the manuscript in light of your \u201cadditional comments\u201d (e.g. we added the missing index i in equation 8).  Regarding equation 9, if $g_{ik}(x)$ is a delta function (as is assumed above equation 9) then equation 9 is exact (see e.g. Neal and Hinton 98).\n", "title": "Response to Reviewer #1"}, "rke6KrMPjS": {"type": "rebuttal", "replyto": "rJxFL5b0YB", "comment": "Thank you for your review. It is a pleasure to read such a detailed and constructive review.  \n\nWe have updated the paper to reflect your suggested improvements. Specifically:\n\nPrior Work. We agree that all these related works are relevant and not in conflict with our results, and  have updated the text to include them.\n\nSufficient vs. Necessary. We agree with the comment and have updated the text to more accurately reflect our view that from the Bayes-Optimal perspective there can be two different causes for adversarial vulnerability: asymmetries in the dataset or suboptimal learning. \n\nCNN vs Linear SVM. To check whether the failure of CNNs to learn a robust classifier is due to a lack of expressive power, we systematically increased the number of channels in the model and measured mean L2 as a function of the expressiveness. The results (shown in figure 8) do not show any increase in robustness as expressive power grows. We believe that these experiments, together with the well-known results on the \u201cuniversal approximation\u201d property of CNNs, strongly suggest that the problem with CNNs is not that they cannot approximate a robust classifier for this task. In particular, note that the RBF robust classifier is a smooth function of the input and the rate of approximation in the universal approximation literature depends on the smoothness. CNNs have been shown to be capable of expressing random functions of more than one million images (Zhang et al. 2017), so expressing a smooth function, such as the RBF classifier or the Bayes-Optimal classifier, seems perfectly reasonable. \n\nSimilarities between synthetic datasets and real datasets. We do believe that the synthetic datasets capture much of the variability that appears in the real datasets. An additional experiment we performed (now in the appendix) is to train a CNN on the synthetic data and measure its performance on the real data. The results show that training on synthetic data, generalizes reasonably well to the real data (with approximately 5% drop in accuracy). While we agree that there are many ways to perfectly discriminate a finite dataset of dogs and cats, this is also true for a finite sample from the synthetic dataset. \n\nIncreasing the amount of training examples. Following your suggestion, we conducted additional experiments where the data samples were generated dynamically from the MFA model, and measured robustness as a function of the number of examples (figure 8). We did not see any significant improvement in robustness, even when we went as far as 1 Million training examples for a binary classification task.  Note also that the RBF SVM did learn a robust classifier with as little as 10,000 examples for the same binary tasks.\n\nRobust training. We included an additional experiment where we used robust training for the CNN (We used the TRADES method, from Zhang et al. 2019, which was the winner in a recent adversarial training challenge). As shown in the appendix, this gave only a modest improvement in robustness. Still far from the robustness of the Bayes-Optimal classifier (or the RBF SVM). There are of course many different variants of robust training, but this result shows that the problem of effectively optimizing for both accuracy and robustness is much more difficult in CNNs compared to shallow architectures.\n", "title": "Response to Reviewer #2"}, "SyxjYpuXqH": {"type": "review", "replyto": "H1l3s6NtvH", "review": "The paper studies the adversarial robustness of the Bayes-optimal classifier (i.e., optimal for the standard \"benign\" risk). To do so, the authors construct various synthetic distributions and show that in some cases the Bayes-optimal classifier is also adversarially robust, while in other cases it is not. In the main experiment, the authors construct two high-dimensional synthetic distributions of human faces via a generative model. In one of the distributions, even the Bayes-optimal classifier is vulnerable to adversarial examples. In the other distribution (where the Bayes-optimal classifier is robust), CNNs do not achieve high robustness while other approaches such as an RBF SVM are more robust.\n\nOverall I find the experiment in the paper interesting, but it is unclear how representative the experiments are for adversarial robustness on real data. There are natural follow-up experiments that would shed some light on this question and could substantially strengthen the paper. Hence I unfortunately recommend to reject the paper at this point and encourage the authors to deepen their experimental investigation. For instance, the following points would be relevant:\n\n- Does adversarial training / robust optimization result in a robust neural network on the synthetic data distribution where the Bayes-optimal classifier is robust (and the RBF SVM is more robust than a CNN)?\n\n- Do RBF SVMs also exhibit higher adversarial robustness than CNNs on comparable real datasets? This would indicate to what extent the synthetic distributions are representative of real data w.r.t. adversarial robustness.\n\n\nAdditional comments:\n\n- Briefly defining the MFA model in the main text would provide helpful context.\n\n- A few more details about the experiments could be informative in the main text, e.g., the CNN architecture and the accuracies the various methods achieve.\n\n- An end-of-proof symbol at the end of proofs would be helpful to the reader.\n\n- Is there an index i missing in \\pi in Equation (8)?\n\n- Is the probability given in (9) exact? Gaussians are supported on all of R^d, so even a Gaussian component far away will contribute to the probability of a point under p_1, at least a (very) small amount.\n\n- The proof of Observation 2 is more a sketch. It would be good to include a more formal proof in the appendix.", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 4}}}