{"paper": {"title": "On Evaluating Explainability Algorithms", "authors": ["Gokula Krishnan Santhanam", "Ali Alami-Idrissi", "Nuno Mota", "Anika Schumann", "Ioana Giurgiu"], "authorids": ["gst@zurich.ibm.com", "aai@zurich.ibm.com", "nuno.motagoncalves@epfl.ch", "ikh@zurich.ibm.com", "igi@zurich.ibm.com"], "summary": "We propose a suite of metrics that capture desired properties of explainability algorithms and use it to objectively compare and evaluate such methods", "abstract": "A plethora of methods attempting to explain predictions of black-box models have been proposed by the Explainable Artificial Intelligence (XAI) community. Yet, measuring the quality of the generated explanations is largely unexplored, making quantitative comparisons non-trivial. In this work, we propose a suite of multifaceted metrics that enables us to objectively compare explainers based on the correctness, consistency, as well as the confidence of the generated explanations. These metrics are computationally inexpensive, do not require model-retraining and can be used across different data modalities. We evaluate them on common explainers such as Grad-CAM, SmoothGrad, LIME and Integrated Gradients. Our experiments show that the proposed metrics reflect qualitative observations reported in earlier works.", "keywords": ["interpretability", "Deep Learning"]}, "meta": {"decision": "Reject", "comment": "The paper proposes metrics for comparing explainability metrics.\n\nBoth reviewers and authors have engaged in a thorough discussion of the paper and feedback. The reviewers, although appreciating aspects of the paper, all see major issues with the paper. \n\nAll reviewers recommend reject.  "}, "review": {"SklqelnnKB": {"type": "review", "replyto": "B1xBAA4FwH", "review": "See post-rebuttal updates below!\n\nSummary\n---\n\n(motivation)\nThere are lots of heat map/saliency/visual explanation approaches that try to deep image classifiers more interpretable.\nIt's hard to tell which ones are good, so we need better ways of evaluating explanations.\nThis paper proposes 3 such explanation evaluation metrics, correctness, consistency, and confidence.\n\n(approach - correctness)\nAn explanation is correct if it highlights enough of an image for a classifier to tell the correct class with only the highlight parts of the image.\nThe default way to evaluate on only highlighted portions is to set the non-highlighted bckground to black/grey.\nInstead, this method finds images with the same ground truth class which the classifier scored the lowest of all such images, forming a low-confidence baseline.\nIt copies the background from one of these images instead of using a black/grey background\nto try and put the masked image back into the distribution of images from the ground truth class.\nThis style of masking is used to compute correctness.\n\n(approach - consistency)\nAn explanation is consistent if it is invariance w.r.t. a number of mostly semantically invariant transformations.\nThese include small affine transformations, horizontal flips, vertical flips, and adding noise.\n\n(approach - confidence)\nAn explanation is confident if the masked images it produces still have high condidence under the classifier.\nMasked images are produced as for correctness, by copying a distractor from the same class into the background.\n\n(experiments)\nThe experiments compare existing explanations (LIME, Grad-CAM, Integrated Gradients, SmoothGrad) using the proposed metrics.\n1. Correctness: Classifiers have higher accuracy on explanation-masked images than on images they were least confident on (the ones used to fill in the background).\n2. Grad-CAM is most correct, followed by SmoothGrad, Integrated Gradients, and LIME.\n3. Consistency: Grad-CAM explanations are most resilient to the proposed transformations with Integrated Gradients, SmoothGrad, and LIME being successively less invariant.\n4. Confidence: Explanation-masked images have higher scores for their ground truth class than the low-confidence baseline images.\n5. Hyperparameter variations in the correcness/confidence metrics mostly preserve the ranking of methods, though the absolute values of performance do change substantially.\n\n(conclusion)\nThe paper concludes that Grad-CAM is usually the best of the methods tested according to the new metrics and that LIME is the worst.\n\n\nStrengths\n---\n\nI really like the related work section. It could be a valuable resource going forward.\n\nI like the research direction of this paper very much. I think that enumerating a suite of complementary benchmarks is a good way to measure explanation quality because we can only come up with benchmarks that capture a small part of what we want so far.\n\n\nWeaknesses\n---\n\n\nI see some major conceptual flaws with these metrics:\n\n* In section 3.1 it seems like the first reasons that normal masking failed is not solved by the proposed approach. The generated images are still out of distribution because the \"foreground\" and the \"background\" don't match.\n\n* I'm concerned about the low-confidence distractor images used in the background. They are from the same ground truth class as the high confidence images they are pasted into the background of, correct? The correctness metric is supposed to capture whether or not an explanation highlights all the class-relevant content in an image and no more. However, information that the explanation did not highlight (the background) can inform the classifier of the ground truth class because the background came from an image of that class (even if a low confidence one). This is especially true because the relevant objects might be in differrent positions in the two images. Thus it could be that the explanation did not highlight informative content but the classifier still gets the corresponding masked image correct because of the background. How often does this happen?\n\n* Consistency is supposed to measure \"the ability of the explainer to capture the relevant components\" under semantically invariant transformations.\nThe reported metric is mimized when the explanation is the same before and after a variety of transformations.\nIf this were the case then at least one of them must be wrong in the sense that it would not have captured some relevant components\n(unless perhaps it just highlighted everything and was thus useless).\nBecause of the transformation (e.g. 15 degree rotation) the relevant components would have been at a different position, but the best explanation according\nto this metric would have been at the same position. Thus this metric seems to reward explanations for not capturing relevant components.\n\n\nParts I Didn't Understand:\n\n* In section 3.1, I don't understand the second reason that masking failed. In what sense is masking made meaningless? How is that sense different from the out of distribution concern from the first point?\n\n\nMissing Details / Presentation Weaknesses:\n\n* Missing reference to [1] which provides more metrics.\n\n* The meaning of confidence is different than it normally is and this may be confusing.\nNeural networks should be well calibrated, not necessarily confident (in the commonly used sense of [3]).\n\n\nMinor flaws:\n\n* Masking by replacing the background with grey (i.e., the bias of the first conv layer) rather than black is more common (e.g., [2] and Grad-CAM). A grey background negates the bias. It's not clear that the background should cancel the bias, but it would be nice to compare to both grey and black masking in Table 7.\n\n\n[1]: Adebayo, Julius et al. \u201cSanity Checks for Saliency Maps.\u201d NeurIPS (2018).\n[2]: Zeiler, Matthew D. and Rob Fergus. \u201cVisualizing and Understanding Convolutional Networks.\u201d ECCV (2013).\n[3]: Guo, Chuan et al. \u201cOn Calibration of Modern Neural Networks.\u201d ICML (2017).\n\n\nFinal Evaluation\n---\n\nThis paper relies solely on theoretical arguments to show its metrics capture meaningful information. Empirically, it only shows that the proposed metrics can differentiate between some popular explanations. It does not empirically show that the differentiation is meaningful (e.g., by measuring agreement with human judgement). This by itself isn't a problem. However, above I detailed significant flaws in the theoretical justification for the metrics, so I can't recommend these metrics (this paper) on either a theoretical or an empirical basis.\n\nQuality: Per above, I do not think the arguments/evidence in the paper support its conclusions.\nClarity: The paper could be clearer, but can be understood without too much effort.\nOriginality: These metrics are new enough, being novel variations on prior approaches.\nSignificance: If I was convinced the metrics made sense then I would guess this paper would be very impactful. As is, I don't think it will have much impact.\n\nThe quality of the paper is my reason for the low rating. I'm interested to see whether what others think to make sure I've understood the paper correctly and analyzed it accurately. If my understanding is incorrect I could definitely raise my rating.\n\nPost-Rebuttal Evaluation\n---\nAfter reading the other reviews and the author responses and taking a brief look at the updated paper I still think this paper should be rejected.\n\nThe authors' response to my comments clarified my understanding of the consistency metric. Now I understand it and think it is a useful metric.\n\nHowever, I did not find clarification about the confidence or correctness metrics, though I agree they are not redundant. They still don't really quite make sense to me. This puts me in about the same position as R3, who also doubts those metrics. In the end, this leaves my initial evaluation essentially unchanged. I still recommend rejection because the paper relies on a theoretical understanding of what makes confidence and correctness metrics useful and that understanding is not provided.", "title": "Official Blind Review #2", "rating": "1: Reject", "confidence": 3}, "SJx1EXq0YH": {"type": "review", "replyto": "B1xBAA4FwH", "review": "--------- AFTER rebuttal\n\n1) \"We identify issues with current masking procedures as proposed in other papers\"\n\nOne of the major issues with the current masking procedure is that the resulting image is out of the data distribution. Even though your method achieved high accuracy in Table 2 for correctness, the generates images is still out of the data distribution. \n\n2) \"We propose a cost-effective masking technique that doesn\u2019t require retraining of the underlying classifier\"\n\nThe authors compared against zero and gray masking for correctness. None of those masking methods require retraining of the underlying classifier. It is not clear, which previous masking technique required retraining of the underlying classifier?\n\n3) We also further show that when performing a comprehensive evaluation, there is no one clearly better explainer and thus practitioners need to be careful about which explainer they choose.\n\nThis is an observation made upon through exploratory analysis and is not a technical novelty.\n\n4) \" Confidence on the other hand, also informs us about the per-instance behavior.\"\n\nThe confidence measures the change in probability assigned to the ground truth class. Table 3 should also show the variance in the confidence to understand the instance-level behavior.\n\nThe experiments given in the paper, it looks like confidence and correctness are positively correlated. An example of the model where they are not positively correlated will help the reader understand the importance of each of these terms.\n\n5) \" Effect of Thresholding on results\"\nThank you for the explanation and new experimental results.\n\n\n\n\n------------------------- BEFORE rebuttal\nThe paper proposed different metrics for comparing explainers based on their correctness (ability to find most relevant features in an input, used in prediction), consistency (ability to capture the relevant components while input is transformed), and the confidence of the generated explanations. To evaluate correctness, the authors proposed to study the change in the classification accuracy of the target model, under a perturbed dataset where the most relevant regions (as given by explainer) of the image is preserved and the remaining content is replaced with non-informative backgrounds for the target class. For consistency evaluation, the authors proposed to apply transformations like rotation, translation and flip that doesn\u2019t semantically change the input image. For confidence evaluation, they compared the prediction performance on the original image, masked image (only salient regions) and inverted masked image (only non-salient regions). \n\nMajor\n\u2022\tThe paper lack technical novelty.\n\u2022\tThe confidence component looks redundant and can be incorporated in the correctness component.\n\u2022\tThe inverse saliency map idea is already proposed in \u201cEvaluating the visualization of what a deep neural network has learned\u201d for evaluating saliency maps. There the authors gradually replace the most salient regions with random noise and observe a decrease in prediction accuracy.\n\u2022\tMost of the saliency maps producing methods, generate continuous maps. For making, we need to convert the continuous map to binary by using a threshold. An analysis of choosing different values as threshold is missing. By choosing an appropriate threshold, the size of the most salient region can be controlled. Thus, although Grad-Cam spread saliency over large area, we can use a higher threshold to define the binary mask.\n\u2022\tGrad-CAM, integrated grad and smooth grad are all gradient-based saliency maps. There are perturbation-based saliency maps, which aims to find most salient regions such that removing those regions produce a maximum drop in prediction accuracy. Example \u201cInterpretable Explanations of Black Boxes by Meaningful Perturbation.\u201d, \u201cObject detectors emerge in deep scene cnns\u201d .  An evaluation of such methods is missing.\n\nMinor:\n\u2022\tThe text in the figures has very small font size and is not readable.\n\u2022\t\n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 2}, "SJxA9XP3ir": {"type": "rebuttal", "replyto": "B1xBAA4FwH", "comment": "We would like to thank the reviewers for their valuable feedback. \nThe following are the changes to the paper that we have made in response to their comments.\n\n1. Added experimental results on the effect of thresholding on the masking process.\n2. Added experimental results on the effect of grey background as compared to our proposed masking technique.\n3. Added additional equations to clarify how our metrics are computed.\n4. Added more more images in the appendix that exemplify our proposed masking used in the correctness metric.\n5. A clearer explanation of how the consistency metric is evaluated.\n6. More in-depth discussion into why confidence and correctness are not redundant. \n7. Minor rewrites to fix grammar and typos.\n8. Added additional citations to related works.\n\nWe hope that these address the reviewers' concerns and that they would reconsider their evaluation of our work.\n\nRegards,\nThe Authors", "title": "Summary of changes to the paper"}, "Hke_3jWhir": {"type": "rebuttal", "replyto": "BJgz9iZ3jB", "comment": "Efficacy of confidence vs. number of pixels comparisons\nResponse: \nIt is true that confidence alone is not enough to judge the quality of an explainer as it favorizes low sparsity explanation maps (i.e a naive explainer which flags all pixels in the image will.have the highest score). This is the reason why we report the average size of the selected patch which is a proxy for measuring the precision of our metric. Hence the best explainer should select the fewest number of pixels in an image while keeping a high masking score and low inverse masking score. \n\nDifference between confidence and correctness:\nResponse: \nWhile the two numbers do come from the same experiment, we would like to point out that they are complementary to each other. Correctness gives us a coarse, distribution level view, i.e, how well the explainer performs on average. Confidence on the other hand, also informs us about the per-instance behaviour. In some instances, especially in the medical diagnosis domains, per-instance behaviour is more important than a distribution-level statistic. Additionally,  if method A and B have similar correctness scores, Confidence and Entropy (reported in table. 3) gives us more in-depth information about the per-sample performance of these methods, allowing us to differentiate between the performance of both models \n\n\nThanks, \nThe Authors", "title": "Response to ReviewerIII - pt 2"}, "BJgz9iZ3jB": {"type": "rebuttal", "replyto": "BJxYPItUYr", "comment": "\n\nWe would like to thank the reviewer for their valuable comments. We address their concerns below.\n\nNot compared with prior work: \nResponse: \nIn our literature review, we have not encountered prior work that deals with multi-faceted evaluation of explainers without involving human-in-the-loop baselines. We would be glad to know from the reviewer of any relevant work that we might have missed in our review.\n\nProposed metrics do not lead to consistent ranking of models (explainers): \nResponse: \nIf every metric gives the same ranking then there\u2019s no need to evaluate across multiple dimensions (which is the purpose of our paper). The fact that we find inconsistent rankings across the different metrics exemplifies that there\u2019s no obviously one best explainer. This further justifies evaluating across multiple dimensions. This also emphasizes that users must be more explicit in their requirements and choose the explainer that\u2019s best suited for the task at hand (eg. medical diagnosis, self driving cars etc.) and not blindly follow suggestions from other use cases.\n\nThe combined images are still out of distribution: \nResponse: \nKindly see responses to reviewer1 and reviewer2\n\nWhy is our proposed masking method better than white / black background.\nResponse:\nIn our experiments, we saw that the white / back background do not result in explanations that behave as expected (See Tables 9 and 10 in the appendix). Additionally, we also refer the reviewer to our responses to Reviewer2 where we have addressed a similar concern in a more detailed manner.\n\nDoes the evaluation framework favor visualization methods with a blob-shaped saliency map over scattered dots shaped saliency map?\nResponse: \nIn our experience, that seems to be the case. This is also because the underlying classifier, a CNN, also prefers the blobs due to its inductive biases. As a result, the  accuracy increase is observed more prominently for blob shaped explanations. \n\nDoes it favor methods with larger salient regions? \nResponse: \nNo, because we also measure inverse masking. If the explainer naively captures a large region to \u201ccheat\u201d the correctness metric, it will be penalized because the inverse mask would be too small and we will not see expected changes in accuracy. Additionally, we also report average explanation size (Avg. proportion of pixels in the explanations wrt to the total number of pixels in the image) which further allows us to detect if the explainer is trying to fool the metric by producing large explanations.\n\nIf the original image is already incorrectly classified (since they are the ones where the classifier assigns the lowest probability) it is hard to imagine that adding random background can make the performance worse.\nResponse: \nWe actually only use the incorrectly classified/low confidence image for defining a background and the explanation map on the latter is ignored. The foreground (i.e., the relevant features) is extracted from the highly confident image. Since the prediction confidence is high, a proper explainer should highlight relevant class pixels on this image. \n\nMetrics like Precision, Recall and F1 are not well justified.\nResponse: \nThese metrics are widely used in classification and ranking tasks. One can indeed view the task of generation of explanation as being analogous to the retrieval task (as noted in [1]). Following this formulation, we try to map the changes in accuracy we see with our normal masking and inverse masking experiments to these metrics which are well understood by the community. We do acknowledge that the mapping is not one-to-one and thus label our metrics with a \u201cpseudo\u201d prefix. \n\nReferences:\n[1] Samek et.al, Evaluating the visualization of what a Deep Neural Network has learned \n", "title": "Response to ReviewerIII - pt 1"}, "BJxTMj-hor": {"type": "rebuttal", "replyto": "SklqelnnKB", "comment": "Thank you for your kind words and the succinct summary  of our paper. We would like to address your comments one by one.\n\nComment 1: Proposed masking generates image that are still out of the data distribution. \nResponse: \nAs we have reported in table 9, doing normal masking (with black/gray pixels) doesn\u2019t give us any accuracy or confidence improvements. Thus it does not help us at comparing different explainability methods. Since the background of the masked image is taken from an image of the same class in our dataset, the latter is more relevant than a constant value background and helps us in preserving spatial correlations that CNNs rely on to make their predictions. Moreover, by pasting salient pixels from a high confidence images to a low confidence one, we can observe an accuracy/confidence increase which is directly correlated to the goodness of an explainer (i.e., the accuracy increase is due to the fact that the explainer captured relevant pixels on the high confidence image). We have further elaborated on this in the responses below.\n\nComment 2: Concerns about information leakage from background image used in masking. \nResponse: \nWe would like to emphasize that we do not consider the explanation map of the low confidence image. The latter is only used as a background onto which pixels highlighted by the explainer on the high confidence image are pasted.\n\nAdditionally, we would like to summarize our correctness evaluation method below to clarify any further confusions.\n\nOur hypothesis is that, if the explainers are indeed capturing only the relevant pixels and nothing more, we should see an increase in accuracy if we pass only the relevant pixels as the neural network will not be confused by non-relevant pixels. Additionally, we should see a very small change in accuracy if we pass ONLY the non-important pixels as there is little to no information in these pixels for the discriminative task. \n\nWe further performed experiments to show that a simple masking with empty pixels as background was not sufficient to see the above expected phenomena as the resultant images are out of distribution as well as not locally correlated (i.e., nearby pixels did not tend to have similar pixel values). \n\nThus we proposed a new masking technique whereby we use low-confidence (as measured by the network\u2019s prediction) images as background for the high confidence images belonging to the same class.\n\nWe first measure base-line accuracies for the set of low-confidence images (say BL).\n\nFor the normal masking experiments, we take the relevant pixels from the high confidence images and paste them on top of the low confidence images and remeasure the accuracies (say I).\n\nSimilarly, for the inverse masking experiments, we take the non-relevant pixels from the same high confidence images and paste them on top of the low confidence images and measure the accuracies again (say II).\n\nWe define correctness as the changes in accuracies in the above two experiments (Table 2 in the paper) (i) between the baseline and masked images (ii) baseline and inverse masked images.\n\nComment 3: Concerns about how consistency is evaluated\nResponse: \nThe reviewer points out that Consistency is minimized when \u201cthe explanation is the same before and after a variety of transformations.\u201d. This seems to be an unfortunate misunderstanding. We have described in the first paragraph of Sec. 3.2.2 that a consistent explainer would produce similar explanations modulo transformations for the transformed images. Therefore, we perform an inverse transformation (t^{-1}) before we compute the difference between the generated heatmaps. We have mentioned this in Eq. 3 as well. In this case we only reward the explainer when the explanation that it yields is still consistent under different semantically invariant transformations.\n\nFor example, if we apply vertical flip as a semantically invariant transformation, we flip the generated heatmap from the transformed image before comparing with the heatmap generated by the untransformed image.\n\nComment 4: Effect of grey masking instead of black masking\nResponse: \nWe have added experimental results for this configuration in the appendix. You can find them in table 10. We find that using the grey background does not solve the issue either. \n\n\nWe hope these explanations address your concerns satisfactorily.\n\nRegards,\nThe Authors ", "title": "Response to ReviewerII"}, "Hyl21jZ3oH": {"type": "rebuttal", "replyto": "SJx1EXq0YH", "comment": "We would like to thank the reviewer for their valuable insights. Please find below our responses addressing your concerns. \n\nComment 1. Lacks Technical Novelty\nResponse:\nWe would like to emphasize our major contributions:\n1. We identify issues with current masking procedures as proposed in other papers\n2. We propose a cost-effective masking technique that doesn\u2019t require retraining of the underlying classifier\n3. We identify the one-dimensionality of current research into evaluating explainers, proposing other important components. \nWe also further show that when performing a comprehensive evaluation, there is no one clearly better explainer and thus practitioners need to be careful about which explainer they choose.\n\nComment 2: Confidence is redundant relative to correctness.\nResponse: \nWhile the two numbers do come from the same experiment, we would like to point out that they are complementary to each other. Correctness gives us a coarse, distribution level view, i.e, how well the explainer performs on average. Confidence on the other hand, also informs us about the per-instance behaviour. In some instances, especially in the medical diagnosis domains, per-instance behaviour is more important than a distribution-level statistic. Additionally,  if method A and B have similar correctness scores, Confidence and Entropy (reported in table. 3) gives us more in-depth information about the per-sample performance of these methods, allowing us to differentiate between the performance of both models \n\nComment 3: Inverse Saliency is already proposed\nResponse: \nWe are aware of the paper referred to by the reviewer. In fact, we have cited it in the related work section as well. However, our approach differs from it in the following ways.\n1. We do not add noise when computing the inverse saliency maps\n2. We do not perform an iterative process to compute AuC, instead we compute the inverse only once. In the experiments, we repeat the matching process multiple times only to remove any potential correlations between the randomly chosen background and foreground images. This is an additional step and is not integral to our proposed method.\n3. Unlike the referred paper, we perform the inverse masking to make sure that the explainers have not missed any relevant pixels and is only a small part of our proposed framework.\n\n\nComment 4: Effect of Thresholding on results\nResponse: \nThe reviewer is right in pointing out that the size of the generated explanation depends on the thresholding used. We conducted experiments using different percentile-based thresholds and found that the relative trends between the explainers does not change with the threshold. We have included the results in Table 6 and 7 in the Appendix\n\nComment 5: Evaluating Perturbation based methods\nResponse: \nIn addition to gradient based methods like GradCAM, SmoothGrad and Integrated Gradients, we also evaluated our metric suite on LIME, the most widely used perturbation based explanation method for our comparison. The other perturbation methods can be similarly evaluated as well. \n\nWe would like to emphasize that our work focuses on proposing a comprehensive suite of metrics to evaluate explainers and performing a comparative study using common explainers to show the suite\u2019s behaviour. An exhaustive comparison of every explainer is unfortunately out of the scope of this paper but we do consider it as an important future work that can be collaboratively taken up by the XAI community. \n\n\nThanks\nThe Authors", "title": "Response to Reviewer1"}, "BJxYPItUYr": {"type": "review", "replyto": "B1xBAA4FwH", "review": "This paper studies the interesting question of comparing the deep network visualization algorithms quantitatively. Several metrics are proposed, including correctness, consistency and confidence. \n\nI like the notion of consistency, where an explainer should produce the same explanation under transformations of the image that does not change its \u201csemantic content\u201d. \n\nHowever, I am confused or unconvinced by several arguments made in the paper, and if the authors can clarify them I am willing to increase my review. I think the major issue is that most metrics are justified with flimsy arguments, not compared with prior work, and do not lead to consistent ranking of the models. \n\nCorrectness: I am not convinced by the correctness evaluation for several reasons\n\n1. The combined image is still out of distribution, and it is unclear why this is better compared to e.g. using a white background. \n2. Does it favor visualization methods with a blob-shaped saliency map vs. scattered dots shaped saliency map? Does it favor methods with a larger salient region? For example, just from visual appeal, I do not think smoothgrad is worse than gradCAM, but the number says otherwise. I think the arbitrariness of this metric makes the numbers hard to believe. \n3. If the original image is already incorrectly classified (since they are the ones where the classifier assigns the lowest probability) it is hard to imagine that adding random background can make the performance worse.\n\nTherefore, it is also unclear what to make of the numbers in e.g. Table 2. There are so many metrics, precision, recall, F1, and none of them seem particularly well justified. They also do not rank the model in the same way. Which result should a practitioner believe? \n\nConfidence: I am not sure the confidence vs. number of pixels comparison are useful. Across all methods, it seems to be more pixels -> increased confidence, which is unsurprising. I think the results are only useful if one method pareto dominates another, which is not what is observed in the experiments. \n\nI do not understand the difference between confidence and correctness. It seems like both measure how well a model can predict the correct class given only the salient region. For example, if method A has higher confidence and lower correctness compared to method B, what does that mean? Under which situation should one choose method A over method B? ", "title": "Official Blind Review #3", "rating": "1: Reject", "confidence": 3}, "ByeP5jZy5r": {"type": "rebuttal", "replyto": "BklPydLsdS", "comment": "Dear TING TING SUN,\nThank you for your kind words and your comments. \n\nYou are right in pointing out that the masked images could still be out of distribution. We hypothesized (in Sec. 3.2.1) that our masking provides samples that are closer to the data distribution than those with a black / blank background. \n\nWe have further calculated the Inception Score[1] and FID[2] of the samples produced by our method and the black background. These results will be included in an update during the rebuttal phase. \n\nHere's the table of the scores we computed. \n\nInception Score\nExplainer\t       Black background  Our Method\nInteg. Grad   \t        21.01\t\t  89.1564\nSmoothGrad\t  \t24.4578\t          137.6948\nGradCAM \t\t231.4284\t\t  428.9047\nLIME \t\t\t60.2835\t\t 137.9088\n\n\nFID Score\nExplainer\t       Black background  Our Method\nInteg. Grad \t  108.8858\t \t66.0593\nSmoothGrad \t   91.0726\t\t46.9035\nGradCAM           409.6676\t\t1.0368\nLIME\t\t   62.2233\t\t32.9805\n\n\nAs you can see, in both the metric, our method outperforms the simple blank pixel background.We see from these results that, indeed our method produces inputs closer to the data distribution.\n\nRegarding your note on using rotations as a sematically invariant transform while evaluating consistency, our decision to include the rotation was motivated by the fact that real-world objects under rotation remain unchanged to an human observer (expect for rare cases like the digit 9). This decision was not influenced by whether neural networks were invariant to the transformation or not.\n\nFurthermore, we only take those inputs for which the prediction of the underlying classifier does not change under the transformation as described in Sec. 3.2.2. Additionally, we also showed that the classifier accuracy on the entire dataset doesn't change significantly under the rotations we consider (Please see Table. 1 in section 4).\n\nFinally, we would like to re-emphasize that this is an evaluation towards the explainability algorithms. In fact, we keep the underlying classifier unchanged under all the experimental setups. This allows us to factor out the any influence that pathologies of the underlying classifier might have on the observed results.\n\nRegards,\nThe Authors\n\n[1] Improved Techniques for Training GANs https://arxiv.org/abs/1606.03498\n[2] GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium https://arxiv.org/abs/1706.08500", "title": "Clarifications"}}}