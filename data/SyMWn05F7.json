{"paper": {"title": "Learning Exploration Policies for Navigation", "authors": ["Tao Chen", "Saurabh Gupta", "Abhinav Gupta"], "authorids": ["taoc1@andrew.cmu.edu", "sgupta@eecs.berkeley.edu", "abhinavg@cs.cmu.edu"], "summary": "", "abstract": "Numerous past works have tackled the problem of task-driven navigation. But, how to effectively explore a new environment to enable a variety of down-stream tasks has received much less attention. In this work, we study how agents can autonomously explore realistic and complex 3D environments without the context of task-rewards. We propose a learning-based approach and investigate different policy architectures, reward functions, and training paradigms. We find that use of policies with spatial memory that are bootstrapped with imitation learning and finally finetuned with coverage rewards derived purely from on-board sensors can be effective at exploring novel environments. We show that our learned exploration policies can explore better than classical approaches based on geometry alone and generic learning-based exploration techniques. Finally, we also show how such task-agnostic exploration can be used for down-stream tasks. Videos are available at https://sites.google.com/view/exploration-for-nav/.", "keywords": ["Exploration", "navigation", "reinforcement learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors have proposed an approach for directly learning a spatial exploration policy which is effective in unseen environments. Rather than use external task rewards, the proposed approach uses an internally computed coverage reward derived from on-board sensors. The authors use imitation learning to bootstrap the training and then fine-tune using the intrinsic coverage reward. Multiple experiments and ablations are given to support and understand the approach. The paper is well-written and interesting. The experiments are appropriate, although further evaluations in real-world settings really ought to be done to fully explore the significance of the approach. The reviewers were divided, with one reviewer finding fault with the paper in terms of the claims made, the positioning against prior art, and the chosen baselines. The other two reviewers supported publication even after considering the opposition of R1, noting that they believe that the baselines are sufficient, and the contribution is novel. After reviewing the long exchange and discussion, the AC sides with accepting the paper. Although R1 raises some valid concerns, the authors defend themselves convincingly and the arguments do not, in any case, detract substantially from what is a solid submission."}, "review": {"HJgsJ_y2kV": {"type": "rebuttal", "replyto": "Hygjz8oskN", "comment": "Before we respond to R1, we point out that R3 supports the paper over and above R1's comments. \n\n**SPTM**\nWe quote N. Savinov from his follow-up paper \"Episodic Curiosity Through Reachibility\" that is available on arXiv:  \"SPTM (Savinov et al., 2018) does compare to the episodic memory buffer but solves a different task \u2014 given an already provided exploration video, navigate to a goal \u2014 which is complementary to the task in our work.\" Furthermore, SPTM paper does not describe their automated exploration policy in enough detail, and itself acknowledges: \"Effective exploration is a challenging task in itself, and a comprehensive study of this problem is outside the scope of the present paper. However, as a first step, we experiment with providing our method with walkthrough sequences generated fully autonomously \u2013 by our baseline agents trained with reinforcement learning. This is only possible in simple mazes, where these agents were able to reach all goals. We used the best-performing baseline for each maze and repeated exploration multiple times, until all goals were located.\"\n\n**ZSVI** \nFirst, we emphasize that we have consulted with one of the authors of ZSVI and they agree: \"ZSVI does not attempt to solve long-term navigation problem\" (more details on this below).\n\na) 20-30 steps vs 59 steps for ZSVI. R1 is not only wrong but is also making a petty point. We picked 20-30 from the text of ZSVI: see Sect. 3.2 > 1. Goal Finding: \"To test the extrapolative generalization, we keep the Turtlebot approximately 20-30 steps away from the target location in a way that current and goal observations have no overlap as shown in Figure 4.\" 20-30 or 59, our argument still holds.\nb) Intermediate waypoints necessary or not. Our previous response was in consultation with authors of ZSVI. We reiterate: \u201cThus, ZSVI does not attempt to solve long-term navigation problem by itself and requires an expert to break long-term navigation into several short-term navigation problems.\u201d Once again, finding goal tasks in ZSVI are limited to when the goal is 20-30 (or 59 whichever number you want to use here) steps away. Read relevant portions of text from their paper.\n\n** Frontier-based Method **\nR1 has changed their arguments on this over their different responses:\na) In the first review, R1 missed frontier-based method all together. \nb) When we pointed that we already have much stronger baselines (frontier-based method) than R1s suggestion of using a \"greedy\" policy, R1 claimed frontier-based method is not state-of-the-art.\nc) On reiterating that our implementation is indeed very strong, R1 has flipped their argument again, stating that we did not describe it accurately in the original version. This is again incorrect, Section 4.1 > Baselines > 1. Frontier-based Exploration was and is an accurate description of our implementation. R1 had missed the frontier-based method altogether in their original review, and we suggest R1 to read the relevant part of the paper again.\n\n** Collision Avoidance **\nOur action space permits the agent to be stationary, or move around in a small circle. Such a behavior maximizes collision avoidance reward. We explicitly experimented with it in our setup and reported what we found in Appendix C4. Given our setup, what we found makes perfect sense. We are not sure what more R1 wants here, is there a different experiment you want us to run?\n\n**Real World Scenes** \nHouse3D environments are realistic layouts of houses (made by people on the Internet). Infact, their paper is itself called: \u201cBuilding Generalizable Agents With a Realistic And Rich 3D Environment\u201d. Computer vision algorithms trained on this dataset have been shown to transfer to the real world. See the original SUNCG paper.", "title": "Response to new arguments"}, "rkgGSK0Mhm": {"type": "review", "replyto": "SyMWn05F7", "review": "This paper proposes learning exploration policies for navigation. The problem is motivated well. The learning is conducted using reinforcement learning, bootstrapped by imitation learning. Notably, RL is done using sensor-derived intrinsic rewards, rather than extrinsic rewards provided by the environment. The results are good.\n\nI like this paper a lot. It addresses an important problem. It is written well. The approach is not surprising but is reasonable and is a good addition to the literature.\n\nOne reservation is that the method relies on an oracle for state estimation. In some experiments, synthetic noise is added, but this is not a realistic noise model and the underlying data still comes from an oracle that would not be available in real-world deployment. I recommend that the authors do one of the following: (a) use a real (monocular, stereo, or visual-inertial) odometry system for state estimation, or (b) acknowledge clearly that the presented method relies on unrealistic oracle odometry.\n\nEven with this reservation, I support accepting the paper.\n\nMinor: In Section 3.4, \"existing a room\" -> \"exiting a room\"", "title": "good paper", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ByeP-Bzc1E": {"type": "rebuttal", "replyto": "HJl1dciYkV", "comment": "Thanks for your suggestions and appreciation for our paper. In our initial response, we were trying to keep the original paper intact and add any changes in the appendix so as to make it easy for reviewers to see what we have changed. As we mentioned in the first response publicly, we promise to make the changes you requested in the final version. We sincerely hope the AC and reviewer do not penalize us for a well-intentioned but not aligned update since we did not realize the update to main paper is a necessity.\n\nWe have an updated paper on the website linked in the abstract with these changes. We are listing the changes we made in this update below. These will be included in the final version. We will additionally also include references to the related works that came up in discussion with R1 to the final paper as well. \n\n(1) We will add the following text to the \u201cWith Estimation Noise\u201d paragraph on page 7.\n***\nEven though such a noise model leads to compounding errors over time (as in the case of a real robot), we acknowledge that this simple noise model may not perfectly match noise in the real world.\n***\n\n\n(2) We will make italic the following existing text in the \u201cWithout Estimation Noise:\u201d paragraph on page 7. \n***\nNote that this setting is not very realistic as there is always observation error in an agent\u2019s estimate of its location.\n***\n\n\n(3) We will add the following text to the Appendix C.7:\n***\nDetails of noise generation for experiments with estimation noise in Section 4.2: \n1. Without loss of generality, we initialize the agent at the origin, that is $x(0) = \\mathbf{0}$.\n2. The agent takes an action a(t).  We add truncated Gaussian noise to the action primitive(e.g., move forward 0.25m) to get the estimated pose x(t+1),  i.e., x(t+1)=x(t)+ (a(t) with noise)  where x(t) is the estimated pose in time step t.\n3.  Iterate the second step until the maximum number of steps is reached.\nThus, in this noise model, the agent estimates its new pose based on the estimated pose from the last time step and the executed action. Thus, we don\u2019t use oracle odometry in the noise experiments. This noise model leads to compounding errors over time (as in the case of a real robot), though we acknowledge that this simple noise model may not perfectly match noise in the real world.\n***\n\n(4) We have fixed the typo (\u201cexiting a room\u201d). \n\n\nWe very much appreciate your understanding and kindly request you to keep the original rating. We will be happy to rephrase these changes and add further clarifications if you think they will be necessary. Please let us know. \n\nThanks.\n", "title": "Requested Changes Made"}, "HyxtU5AVJE": {"type": "rebuttal", "replyto": "rJeZGq04y4", "comment": "\n**Comparison with other learning methods**:\nWe would like to remind R1 that we added an experiment where the RL agent only gets collision avoidance reward in Figure C.4(a) in the Appendix C4. It shows that the agent does not learn anything meaningful (which is not surprising as even a stay-in-place policy will get a perfect reward). Sadeghi and Levine 2017 and other works on collision avoidance, either explicitly additionally use rewards for moving forward, or appropriately engineer the action space by forcing the agent to move forward in each time step. We did not pick the action space ourselves but used whatever came with House3D, making direct comparisons to such approaches infeasible. Also, the policy in Sadeghi and Levine 2017 shows some exploration behaviors in narrow hallways as the only way to keep agent moving and not colliding the walls, in this specific case, is to move forward. However, it would fail to show exploration behavior in large open space such as living rooms (as in our experiments), because the agent can simply keep turning in a circle to stay away from any wall. Again, we argue that the major purpose of Sadeghi and Levine 2017 is to learn a collision-free policy that keeps the agent moving without a specific intent to keep agent exploring the environments. Experiments that we included (going forward and randomly turning at collision) uses *ground truth collision checking*, and thus already has an advantage over a policy that uses a learned model for collision checking.\n\nAs for the comparison to Pathak et al. GSP method in \u2018Zero-shot visual imitation\u2019. We would like to emphasize to AC and R1 that we have personally communicated with one of the authors of Pathak et al. before formulating this reply: GSP tackles a completely different problem that of acquiring skills using self-supervision. While, GSP can do local navigation, GSP is NOT designed for long-horizon navigation tasks. When GSP is applied to the navigation task, first, it requires the **goal** positions to be within **20-30 steps** from the agent\u2019s current position and the agent will see the target observation within the first 5-10 steps. But in our experiments, our agent is exploring whole houses in House3D, which easily takes thousands of steps and the agent rarely ever sees the target observation within the first 5-10 (or for that matter even 100s of) steps. Second, GSP requires **an expert** to provide a sequence of **landmark images** to guide the agent to move to a far target location while our agent explores the house environment efficiently on its own without the need of experts. Thus, ZSVI does not attempt to solve long-term navigation problem by itself and requires an expert to break long-term navigation into several short-term navigation problems. If we attempt to use ZSVI to solve long-term navigation (without \u201cexpert waypoints\" as in our experiments) it would fail.\n\nZhu et al. 2017 proposed a target-driven navigation policy that can find the object given an image. However, the training and testing environment are the same in Zhu et al.\u2019s case. Their goal is to learn a policy to find the object in the same room which requires millions of interaction in the training/testing environment. Our work focuses on learning an exploration policy that generalizes to new environments in a zero-shot manner. During our experiments, we confirmed the same: the policy learned in Zhu et al. fails to generalize to new environments without re-training for millions of iterations as mentioned in their paper.\n\n\nReferences:\nDornhege, Christian, and Alexander Kleiner. \"A frontier-void-based approach for autonomous exploration in 3d.\" Advanced Robotics 27.6 (2013): 459-468.\n\nWang, Yiheng, Alei Liang, and Haibing Guan. \"Frontier-based multi-robot map exploration using particle swarm optimization.\" Swarm Intelligence (SIS), 2011 IEEE Symposium on. IEEE, 2011.\n\nFraundorfer, Friedrich, et al. \"Vision-based autonomous mapping and exploration using a quadrotor MAV.\" Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on. IEEE, 2012.\n\nMannucci, Anna, Simone Nardi, and Lucia Pallottino. \"Autonomous 3D exploration of large areas: a cooperative frontier-based approach.\" International Conference on Modelling and Simulation for Autonomous Systems. Springer, Cham, 2017.\n\nCampos, Francisco M., et al. \"A complete frontier-based exploration method for Pose-SLAM.\" Autonomous Robot Systems and Competitions (ICARSC), 2017 IEEE International Conference on. IEEE, 2017.\n\nMahdoui, Nesrine, Vincent Fr\u00e9mont, and Enrico Natalizio. \"Cooperative Frontier-Based Exploration Strategy for Multi-Robot System.\" 2018 13th Annual Conference on System of Systems Engineering (SoSE). IEEE, 2018.\n", "title": "Clarifications to R1's incorrect understanding (2) "}, "rJeZGq04y4": {"type": "rebuttal", "replyto": "rkgpAvVGkV", "comment": "\n**Frontier-based exploration**:\nWe want to clarify that the frontier-based exploration we have implemented is an improved version of the original algorithm. The original algorithm commands the agent to go along with the frontier grids in 2D. However, in our version of the frontier exploration, we sampled a target point which, in most cases, are far away from the agent\u2019s current locations. Then the agent uses a shortest path planning algorithm to go to that target position. Since we are using a vision-based RGBD sensor, we don\u2019t have to go through the frontier grids one by one as the robot can see many frontier grids in one time, which greatly improves the efficiency. This is a fairly efficient algorithm if pose estimates are accurate. In fact, in our implementation, we need to sample less than 10 target points to cover majority of the area in most houses. Our implemented frontier-based exploration method, in some sense, is more similar to the frontier-based exploration in 3D proposed in Dornhege et al. 2013 (we will open source the code). We cited Yamauchi, 1997 because, to our knowledge, this paper is the earliest paper that proposed the frontier-based exploration method. Also, we would like to clarify that frontier-based exploration is not an outdated technology. It\u2019s still being used in the robotics community. Just to give a few examples, Wang et al. 2011, Fraundorfer et al. 2012, Mannucci et al. 2017, Campos et al. 2017, Mahdoui et al. 2018 all use frontier-based exploration method.\n\n**Related work**:\nThanks for the suggestion on the related work. We are happy to add these relevant works in the final version of the paper as R1 requires. However, our work is different from these works. The main focus on Xu et al. 2017 is on generating smooth movement path for high-quality camera scan. Bai et al. 2016 proposed an information-theoretic exploration method using Gaussian process regression. This is computationally inefficient when the kernel matrix becomes large. Thus, Bai et al. 2016 only show experiments on simplistic map environments. GPs are computationally expensive when maps are complicated, which is the case in our experiments. Kollar et al. 2008, assume access to the ground-truth map and learn an optimized trajectory that maximizes the accuracy of the SLAM-derived map. In contrast, our learning policy directly tells the action that the agent should take next and estimates the map on the fly. \n\n", "title": "Clarifications to R1's incorrect understanding (1)"}, "H1gw_OAN14": {"type": "rebuttal", "replyto": "SyMWn05F7", "comment": "We thank R1 for reading through our paper more carefully. Unfortunately, we still believe R1\u2019s understanding of the paper is incorrect. This is clearly highlighted by the following statement made by R1: \n\n\u201cWhile the proposed method also uses human demonstration, authors argue that SPTM requires a human to demonstrate the environment, which is impractical in real-world scenarios. This is a contradicting statement.\u201d\n\nSPTM requires the expert demonstration trajectories even at test time. Our work only uses human demonstration for imitation learning during training time ONLY. Again we emphasize, unlike SPTM, we *do not* require human demonstrations at test time. \n\nWe individually address *ALL* the other points raised by R1 below.", "title": "Response Summary to R1's mis-undertandings"}, "SygDhGjoT7": {"type": "rebuttal", "replyto": "S1epUX832m", "comment": "We thank R1 for their comments. R1\u2019s primary concerns are about novelty and missing empirical comparison. These perhaps stem from some misunderstandings about our paper as some requested comparisons are either irrelevant or stronger comparisons are already presented in the paper.  Therefore, we urge the reviewer to take a second look at the paper in light of the rebuttal.\n\n1. Novelty: In this paper, we learn policies for exploring novel 3D environments (Section 3 through Section 4.2), and show that exploration data, gathered by executing our learned exploration policies, improves performance at downstream navigation tasks (Section 4.3). To the best of our knowledge, this is the first work that studies learned exploration policies for navigation, systematically compares them to classical and learning-based baselines, and shows the effectiveness of exploration data for downstream tasks. In doing so, we adopt existing learning techniques (imitation learning + reinforcement learning), and map building techniques. Our novelties are orthogonal to these aspects:\n      (a) Problem formulation: Framing exploration as a learning problem, and showing the utility of exploration data for downstream tasks.\n      (b) Map based policy architectures and reward functions. Classical SLAM based approaches indeed produce maps but: (a) it still needs a policy for exploration during the map-building phase; (b) does not solve navigation rather uses geometric analysis for path planning. Our approach focuses on (a) and unlike heuristic approaches used in SLAM, we use a learning-based approach.\n      (c) We also show maps can also be used for learning effective policies, and for computing reward signals.\n      (d) Use of IL + RL to optimize our policy, as opposed to pure RL that is typically used.\n\n2. Comparison with other exploration approaches: \na) Simple Greedy Baseline: We experimented with the suggested one-step greedy policy. Here we virtually simulate all possible actions that the agent can take, and compute the gain in coverage. We then execute the action that results in the maximum gain in coverage. At 1000 steps such a policy only covers 40m^2, as opposed to our policies that cover up to 125 m^2. This is not surprising as the policy gets stuck inside local regions of full coverage. No action leads to any increase in coverage and the agents move back and forth. The full performance plot is provided in Fig C3(a) in the updated PDF. \n\nNote, in the paper, we have provided a more compelling comparison point to classical exploration approaches: frontier-based method. Reviewer seems to have missed this comparison as R1 still asks for comparisons to classical approaches.\n\nb) Collision Avoiding Policy: A policy that purely avoids collisions has a degenerate solution of the agent staying in-place, resulting in negligible coverage (Fig C4(a)). We also tried a more sophisticated version, where the agent moves straight unless a collision happens (Fig C3(a)), at which point it randomly rotates (by angle between 0 and 2pi), and continues to move straight. To help the policy further, we used ground truth collision-checking. This policy covers 75m^2, still much lower than our performance (125m^2).", "title": "Additional Experiments, Pointers to Existing Experiments and Clarifications (1)"}, "BkgYymsoTm": {"type": "rebuttal", "replyto": "SygDhGjoT7", "comment": "3. Comparison with other learning-based navigation works: First, we do not study a specific navigation task, but instead our contribution is a task-independent exploration policy. We do however show that exploration helps in downstream navigation tasks (Section 4.3). We use well-established Classical Path Planning, the simplest navigation algorithm for doing these experiments. This was a conscious choice so as to not-conflate quality of learned navigation policy with the quality of our learned exploration policy. Our contribution is orthogonal to navigation task itself and therefore our approach can be used in conjunction with any navigation approach. For example, the exploration data by running our policy can be used \u2018as is\u2019 with Savinov et al\u2019s state-of-the-art SPTM approach [A]. SPTM otherwise requires a human to demonstrate the environment, which is impractical in real-world scenarios. \n\nR1 suggests we should compare to Pathak et al\u2019s \u201cZero-shot visual imitation\u201d (ZSVI) as it uses \u201cexploration strategies\u201d and \u201cimitation learning\u201d for navigation. While they indeed use both terms (exploration and imitation), the context and usage is completely different.\n      (a) *Exploration for Imitation (ZSVI) vs. Imitation for Exploration (Ours)*\n           In ZSVI, exploration is used in training to collect trajectories and imitation is used in testing to follow a path. On the other hand, ours is completely the opposite. We use imitation in training to learn how to explore at test time. Again we emphasize: ZSVI does not run any explicit exploration policy during testing.\n      (b) This leads to completely different behavior of two algorithms. The time/distance range in ZSVI is much smaller as compared to ours. Either the goal is in the same room or they need a lot of waypoint images to solve the navigation task.\n\nIn order to show a comparison to \u201cRL with a good exploration \u2026 without explicit exploration\u201c, we have implemented navigation on top of Curiosity Driven Exploration using Self-Supervision. As shown in Appendix C.6 (will be added to Sec 4.3),  the comparison is in our favor. \n\n4. More Experimental Details: We have added additional details in Appendix C. We have included:\na) Stats and floor-plans of houses used for training and testing (Appendix C1).\nb) Coverage plots for when we run the agent for 2000 steps (Appendix C4, Fig C3). Conclusions are the same as for the original 1000 steps plots as presented in the paper.\nc) Agent details. Step size is 0.25m forward motion, 9 degree rotations (already provided in the paper). Real world performance depends on how fast a robot is. A turtlebot-2 can move at a peak speed of 0.65 m/s, if that\u2019s what you were looking for.\n\n5. More Technical Details: \na) We have added details about map construction in Appendix C2. Yes, we can use known-loop closure techniques in SLAM, though there may still be error and we wanted to show that learning is robust to it (Fig 2 (center), video on website).\nb) Imitation learning details are in Appendix C5.\nc) 3D Information: Yes, you are right depth images only give 2.5D information, however, we integrate information from different views, to obtain a more complete sense of the environment than given by a single depth image. 3D information can also be extracted from RGB images, see [B] and numerous others for example.\n\nWe will incorporate your suggestions on presentation in the final version.\n\n[A] Semi-parametric Topological Memory for Navigation Nikolay Savinov, Alexey Dosovitskiy, Vladlen Koltun. ICLR 2018.\n[B] Factoring Shape, Pose, and Layout from the 2D Image of a 3D Scene Shubham Tulsiani, Saurabh Gupta, David Fouhey, Alexei A. Efros, Jitendra Malik. CVPR 2018.\n", "title": "Additional Experiments, Pointers to Existing Experiments and Clarifications (2) "}, "rJxWrMsipX": {"type": "rebuttal", "replyto": "rkgGSK0Mhm", "comment": "Thank you for your comments and suggestions. We acknowledge that most of our evaluation is in the perfect odometry setting which is unrealistic. We experimented with a reasonable noise model that compounds over time within the episode, but we admit it may not be very realistic. We will prominently note both these points in the final version of the paper upon acceptance.\n\n", "title": "We agree, will Incorporate Feedback into Manuscript"}, "rkgdffoi6m": {"type": "rebuttal", "replyto": "S1eK3wbjnX", "comment": "Thanks for your comments and suggestions. We address your specific concerns below:\n\n1. Explicit mapping is hand-engineering. We acknowledge (and will explicitly state in the paper) that using occupancy map as the policy input is based on domain/task knowledge. Using the occupancy map gives the agent a better representation of long-horizon memory and show great improvement compared to the policy without the map as input. We do agree ego-motion estimation in real-world might be noisy. To handle that we performed experiments with noise and show that our model seems robust (See video on the website, Fig 4b in the paper).  \n\nWith regard to end-to-end approaches, approaches like Zhang et al. (2017) uses a differentiable map structure to mimic the SLAM techniques. These works are orthogonal to our effort on exploration. Indeed, our exploration policy can benefit from their learned maps instead of only using reconstructed occupancy map. We also believe our current approach provides a strong baseline for future end-to-end versions.\n\n2. Explicit environment rewards for exploration: We agree that the use of reward yielding objects throughout the environment will lead to a very similar outcome as our approach. The key distinction is that our approach instruments the agent (with a depth sensor) as opposed to instrumenting the environment. This makes our proposed formulation more amenable to being trained and deployed in the real world: all we need is an RGB-D sensor. This is a big advantage over spreading reward yielding objects that disappear as the agents arrive at those locations, which is almost impractical in the real world. With this key distinction being said, we did do several experiments where our policy is trained with external rewards. The performance is shown in Fig C4(c) in Appendix C4. The results show that our coverage map reward is much more effective than external rewards generated by reward-yielding objects. Our method covers 125m^2 on average while even 4 reward yielding objects per square meter is 91m^2.\n\n3. Role of collision avoidance penalty: We added the performance of the agent trained with our policy but with only coverage reward (no collision penalty) in Fig C4(b) in Appendix C4. We observe that adding collision penalty indeed helps improve performance slightly (125m^2 with penalty as opposed to 120m^2 without penalty). Thus, our policy explores well even without explicit collision avoidance penalty.\n\nWe will add more references to the related work and improve the writing as you suggested in the final version of the paper.\n", "title": "Additional Experiments"}, "Skx8ACqjp7": {"type": "rebuttal", "replyto": "SyMWn05F7", "comment": "We thank the reviewers for their comments and suggestions. We are glad that the reviewers found:\n        (a) our paper to tackle an important and clearly motivated problem (R1, R3)\n        (b) our approach to be a great idea (R2), a good addition to the literature (R3) and not-complicated (R1).\n        (c) our paper to be \u201cwell-executed\u201d, with \u201cvarious ablations\u201d, and \u201ccomparisons to \u2026 commendably a classical SLAM baseline\u201d (R2)\n        (d) our paper to be well-written (R1, R3), and well-explained (R2).\nWe have answered *ALL* questions that the reviewers posed by providing additional experimental comparisons, pointing to relevant existing experiments and providing clarifications. Hopefully, this clarifies some of the misunderstandings that R1 has about our paper. Additional experiments have been added to Appendix C of the updated PDF. We will incorporate these experiments and other suggestions in camera-ready upon acceptance.\n\n", "title": "Response Overview"}, "S1epUX832m": {"type": "review", "replyto": "SyMWn05F7", "review": "This paper proposes a method for learning how to explore environments. The paper mentions that the \u201cexploration task\u201d that is defined in this paper can be used for improving the well-known navigation tasks. For solving this task, a reward function a network architecture that uses RGBD images + reconstructed map + imitation learning + PPO is designed.\n\n<<Pros>>\n\n-The paper is well-written (except for a few typos).\n-The overall approach is simple and does not have much complications. \n-The underling idea and motivation is clearly narrated in the intro and abstract and the paper has a easy-to-understand flow.  \n\n<<Cons>>\n\n**The technical novelty is not significant**\n\n-This paper does not provide significant technical novelty. It is a combination of known prior methods: imitation learning + ppo (prior RL work). The presented exploration task is not properly justified as to how it could be useful for the navigation task. The reconstruction of maps for solving the navigation problem is a well-explored problem in prior SLAM and 3D reconstruction methods. Overall the novelty of the approach and the proposed problem is incremental. \n\n**The paper has major short comings in the experimental section. The presented experiments do not support the main claim of the paper which is improving the performance in the well-known navigation task. Major baselines are missing. Also, the provided results are not convincing in doing the right comparison with the baselines. **\n\n-Experimental details are missing. The major experimental evaluations (Fig. 2 and Fig. 3) are based on the m^2 coverage after k steps and the plots are cut at 1000 steps. What are the statistical properties of the 3D houses used for training and testing? E.g what is their area in m^2? How big is each step in meters?  Why are the graphs cut at 1000 steps? How would different methods converge after more than 1000 steps, e.g. 2000 steps? I would like to see how would the different methods converge after larger number of steps? How long would each step take in terms of time? How could these numbers convey the significance of the proposed method in a real would problem settings? \n\n-The experiments do not convey if learning has significantly resulted in improved exploration. Consider a simple baseline that follows a similar approach as explained in the paper for constructing the occupancy map using the depth sensor. A non-learning agent could use this map at each step to make a greedy choice about its next action which greedily maximizes the coverage gain based on its current belief of the map. While the performance of random policy is shown in Fig.2 the performance of this greedy baseline is a better representative of the lower bound of the performance on the proposed task and problem setup.\n\n-What is the performance of a learning-based method that only performs collision avoidance? Collision avoidance methods tend to implicitly learn to do a good map coverage. This simple baseline can show a tangible lower bound of a learning-based approach that does not rely on map.\n\n-The major promise of the paper is that the proposed exploration task can improve navigation. However, the navigation experiment does not compare the proposed method with any of prior works in navigation. There is a huge list of prior methods for navigation some of which are cited in the \u201clearning for Navigation\u201d section of the related works and the comparison provided in Fig. 4 is incomplete compared to the state-of-the-arts in navigation. For example, while the curiosity driven approach is compared for the exploration, the more related curiosity based navigation method which uses both \u201cexploration strategy\u201d and \u201cimitation learning\u201d : \u201cPathak, Deepak, et al. \"Zero-shot visual imitation.\"\u00a0International Conference on Learning Representations. 2018.\n\u201c is missed in navigation comparison. The aforementioned paper is also missed in the references.  \n\n-Algorithmic-wise, it would make the argument of the paper clearer if results were conducted by running different exploration strategies for navigation to see if running RL with a good exploration strategy could solve the exploration challenge of the navigation problem without needing an explicit exploration stage (similar to the proposed method) which first explores and constructs the map and then does navigation by planning.\n\n-The navigation problem as explained in section is solved based on planning approach that uses a reconstructed map. This is a fairly conventional approach that SLAM based methods use. Therefore, comparison with a SLAM method that constructs the map and then does navigation would be necessary. \n\n\n** Technical details are missing or not explained clearly**\n\n- Section 3.1 does not clearly explain the map construction. It seems that the constructed map is just a 2D reconstruction of the space (and not 3D) using the depth sensor which does not need transformation of the 3D point cloud. What is the exact 3D transformation that you have done using the intrinsic camera parameters? This section mentions that there can be error in such map reconstruction because of robot noise but alignment is not needed because the proposed learning method provides robustness against miss-alignment. How is this justified? Why not using the known loop closure techniques in SLAM? \n\n-The technical details about the incorporated imitation learning method are missing. What imitation learning method is used? How is the policy trained during the imitation learning phase? \n\n-Last paragraph of intro mentions that the proposed method uses 3D information efficiently for doing exploration. The point of this sentence is unclear. What 3D information is used efficiently in the paper? Isn\u2019t it only 2.5D (information obtained by depth sensor) used in the proposed method?\n\n**Presentation can be improved**\n\n-The left and right plots of the Figure 3 contains lots of repetitions which brings in confusion in comparing the performance of runs with different settings. These two plots should be presented in a single plot. \n\n- Interpretation of \u201cgreen vs white vs black\u201d in the reconstructed maps is left to the reader in Fig. 1. \n\n- Last line in page 5: there is no need for reiteration. It is already clear.\n\n**Missing references**\n\n-Since the paper is about learning to explore, discussion about \u201cexploration techniques in RL\u201d is recommended to be added in at least the related work section. \n\n-A big list papers for 3D map reconstruction is missing. Since the proposed method relies on a map reconstruction, those papers are relevant to this work and can potentially be used for comparison (as explained above). It is highly recommended that relevant prior 3D map reconstruction papers be added to the related work sections. \n\n\n", "title": "No significant novelty, lack of experimental evaluations, missing technical details", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "S1eK3wbjnX": {"type": "review", "replyto": "SyMWn05F7", "review": "This is a well explained and well executed paper on using classical SLAM-like 2D maps for helping a standard Deep RL navigation agent (convnet + LSTM) explore efficiently an environment and without the need for extrinsinc rewards. The agent relies on 3 convnets, one processing RGB images, one the image of a coarse map in egocentric referential, and one of the image of a fine-grained map in egocentric referential (using pre-trained ResNet-18 convnets). Features produced by the convnets are fed into a recurrent policy trained using PPO. Two rewards are used: the increase in the map's coverage and an obstacle avoidance penalty. The agent is further bootstrapped through imitation learning in a goal-driven task executed by a human controlling the agent. The authors analyze the behavior of the navigation algorithm by various ablations, a baseline consisting of Pathak's (2017) Intrinsic Curiosity Module-based navigation and, commendably, a classical SLAM baseline with path planning to empty, unexplored spaces.\n\nUsing an explicit map is a great idea but the authors need to acknowledge how hand-engineered all this is, when comparing it to actual end-to-end methods. First, the map reconstruction is done by back-projections of a depth image (using known projective geometry parameters) onto a 3D point cloud, then by slicing it to get a 2D map, accumulated over time using nearly perfect odometry. SLAM was an extremely hard problem to start with, and it took decades and particle filters to get to the quality of the images shown in this paper as obvious. Normally, there is drift and catastrophic map errors, whereas the videos show a nearly perfect map reconstruction. Is the motion model of the agent unrealistic? Would this ever work out of the box on a robot in a real world? The authors brush off the need for bundle adjustment, saying that the convnet can handle noisy local maps. Second, how do you get and maintain such nice ego-centric maps? Compared to other end-to-end work on learning how to map (see Wayne et al. or Zhang et al. or Parisotto et al., referred to later in the paper), it looks like the authors took a giant shortcut. All this SLAM apparatus should be learned!\n\nOne crucial baseline that is missing is that of explicit extrinsic rewards encouraging exploration. These rewards merely scatter reward-yielding objects throughout the environment; over the course of an episode, an object reward that is picked does not re-appear until the next exploration episode, meaning that the agent needs to cover the whole space to forage for rewards. Examples of such rewards have been published in Mnih et al. (2016) \"Asynchronous methods for deep reinforcement learning\" and are implemented in DeepMind Lab (Beatie et al., 2016). Such an extrinsic reward would be directly related to the increase of coverage.\n\nA second point of discussion that is missing is that of the collision avoidance penalty: roboticists working on SLAM know well that they need to keep their robot away from plain-texture walls, otherwise the image processing cannot pick useful features for visual odometry, image matching or ICP. What happens if that penalty is dropped in this navigation agent?\n\nFinally, the authors mention the Neural Map paper but do not discuss Zhang et al. (2017) \"Neural SLAM\" or Wayne et al. (2018) \"Unsupervised Predictive Memory in a Goal-Directed Agent\", where a differentiable memory is used to store map information over the course of an episode and can store information relative to the agent's position and objects' / obstacles' positions as well.\n\nMinor remark: the word \"finally\" is repeated twice at the end of the introduction.", "title": "Good use of mapping for exploration", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}