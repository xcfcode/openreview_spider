{"paper": {"title": "The Preimage of Rectifier Network Activities", "authors": ["Stefan Carlsson", "Hossein Azizpour", "Ali Razavian"], "authorids": ["stefanc@kth.se", "azizpour@kth.se", "razavian@kth.se"], "summary": "", "abstract": "The preimage of the activity at a certain level of a deep network is the set of inputs that result in the same node activity. For fully connected multi layer rectifier networks we demonstrate how to compute the preimages of activities at arbitrary levels from knowledge of the parameters in a deep rectifying network. If the preimage set of a certain activity in the network contains elements from more than one class it means that these classes are irreversibly mixed. This implies that preimage sets which are piecewise linear manifolds are building blocks for describing the input manifolds specific classes, i.e. all preimages should ideally be from the same class. We believe that the knowledge of how to compute preimages will be valuable in understanding the efficiency displayed by deep learning networks and could potentially be used in designing more efficient training algorithms.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper studies the invertibility properties of deep rectified networks, and more generally the piecewise linear structure that they implicitly define. The authors introduce a 'pseudocode' to compute preimages of (generally non-invertible) half-rectified layers, and discuss potential implications of their method with manifold-type models for signal classes. \n \n The reviewers agreed that, while this is an interesting and important question, the paper is currently poorly organized, and leaves the reader a bit disoriented, since the analysis is incomplete. The AC thus recommends rejection of the manuscript. \n \n As an addendum, the AC thinks that the authors should make an effort to reorganize the paper and clearly state the contributions, and not expect the reader to find them out on their own. In this field of machine learning, I see contributions as being either (i) theoretical, in which case we expect to see theorems and proofs, (ii) algorithmical, in which case an algorithmic is presented, studied, extensively tested, and laid out in such a way that the interested reader can use it, or (iii) experimental, in which case we expect to see improved numerical performance in some dataset. Currently the paper has none of these contributions. \n Also, a very related reference seems to be missing: \"Signal Recovery from Pooling Representations\", Bruna ,Szlam and Lecun, ICML'14, in which the invertibility of ReLU layers is precisely characterized (proposition 2.2)."}, "review": {"BJmJToywl": {"type": "rebuttal", "replyto": "B13A9k4Vl", "comment": "I appreciate the general positive tone of all the reviews that all seem to boil down to \"note ready yet\". The paper is definitely just a first in a series where implications of it will be investigated. I suppose by \"being ready\" the reviewers mean: 1. A novel sofar unnoticed way of directly computing preimages to rectifier networks (which we have) 2. An algorithm to actually perform the computation (which we don't have yet but could easily write down in terms of finding nullspaces of linear mappings) and 3. \"Empirical results\", i.e. testing the algorithm on real cases of trained rectifier networks (which we don't have but intend to produce). \n\nWhat we do have is a previously unknown mathematical demonstration of how to directly compute exact preimages as opposed to previous work (Mahendran, vedaldi 2015, Dosovitskiy, Brox \tarXiv:1506.02753 where heuristic methods for computing preimages are presented. We also give a clear demonstration of the fact that rectifier networks are not in general invertable which seem to contradict the claims given in recent submissions to ICLR, Towards Understanding the Invertibility of Convolutional Neural Networks (ICLR 2017), Why are deep nets reversible ?: A simple theory, with implications for training (ICLR ws 2016) although these papers seem to make some assumptions that are not made by us.\n\nThe paper Montufar et al. mentions the fact that multiple inputs are mapped to the same output but does not contain any extensive discussion of preimages or their computation. ", "title": "To all reviewers"}, "HyxUPSEXe": {"type": "rebuttal", "replyto": "r15S0r17l", "comment": "Figure 1 illustrates the transformation occuring between layer (l) and layer (l+1) for a two node\nmulti layer network. In math terms this transformation is described in eq. 5 and 6 as  affine transformations\nrelating   (x^(l)_1, x^(l)_2) with (x^(l+1)_1, x^(l+2)_2).\n\nIn detail for x^(l+1)_1:\n\nx(l+1)_1 = w_1,1 x^(l)_1 + w_1,2 x^(l)_2  + w_1,3\nx(l+1)_2 = w_2,1 x^(l)_1 + w_2,2 x^(l)_2  + w_2,3\n\nIn order to get a more compact notation I introduce the vector notation\nx^T = (x_1, x_2,  1) which always has a 1 as the last element.\nThis allows us to write the transformations in the compact way: x^(l+1) = w^T x^(l)\nwhich would be  a hyperplane passing through the origin\nif x was a general 3D vector. However since the 3:rd element of x is always 1 it\nis just a compact way of writing the affine transformations of eq. 5 and 6, i.e as\ntwo general lines in (x^(l)_1, x^(l)_2) space\n\n", "title": "Answer and explanation of figure 1"}, "SJ23gQN7l": {"type": "rebuttal", "replyto": "ByXo3xEXl", "comment": "Q1: I am aware of that paper and I thought we had referenced it but I was wrong. The ref. Glorot et. al which we have, is a more original reference of rectifier networks. I will update the reference list adding the suggested paper. That paper and the work we cited notes that the input space will be divided into piecewise linear regions that map to the same output. We give an more detailed and explicit way of constructing these regions (preimages) that map to the same output.\n\nQ2: Yes, this is standard MLP if by that is meant Multi Layer Perceptrons of which convnets are a special case. \n\nQ3: In fig2 we have a coordinate system associated eith each layer. This coordinate system refers to the concatenated linear mappings from the first layer to layer (l). This means it illustrates how  the input space is mapped to layer (l). The ReLU elements mean that we only consider the positive quadrant of the coordinate systems for the linear mappings. The effect of this is that we get the preimages of a certain layer's activity in the input space.\n\nQ4: For any activity at any layer we give a procedure of finding the preimage of the previous layer. This procedure involves computing a set of nullspaces and constructing a coordinate system, i.e. just standard linear algebra procedures. We have not yet studied the actual complexity of this but we believe that it should not offer any major problems. By concatenating this procedure we can get the preimage of any  layer expressed in any previous layer as well as the input space\n\nGeneral question: Yes we will update a revised version wednesday 7 december", "title": "Answers to questions 1-4 and general question about revision"}, "r15S0r17l": {"type": "review", "replyto": "HJcLcw9xg", "review": "Kernel is referring to the nullspace? If so, should it not pass through the origin and what is a transformation kernel? \nWhat is the considered x^{(l+1)} in Figure 1? \n\nI have not read the revised version in detail yet. \n\nSUMMARY \nThis paper studies the preimages of outputs of a feedforward neural network with ReLUs. \n\nPROS \nThe paper presents a neat idea for changes of coordinates at the individual layers. \n\nCONS \nQuite unpolished / not enough contributions for a finished paper. \n\nCOMMENTS \n- In the first version the paper contains many typos and appears to be still quite unpolished. \n\n- The paper contains nice ideas but in my opinion it does not contribute sufficiently many results for a Conference paper. \nI would be happy to recommend for the Workshop track. \n\n- Irreversibly mixed and several other notions from the present paper are closely related to the concepts discussed in [Montufar, Pascanu, Cho, Bengio, NIPS 2014]. I feel that that paper should be cited here and the connections should be discussed. In particular, that paper also contains a discussion on the local linear maps of ReLU networks. \n\n- I am curious about the practical considerations when computing the pre-images. The definition should be rather straight forward really, but the implementation / computation could be troublesome. \n\n\nDETAILED COMMENTS \n- On page 1 ``can easily be shown to be many to one'' in general. \n\n- On page 2 ``For each point x^{l+1}'' The parentheses in the superscript are missing. \n\n- After eq. 6 ``the mapping is unique''  is missing `when w1 and w2 are linearly independent' \n\n- Eq. 1 should be a vector. \n\n- Above eq. 3. ``collected the weights a_i into the vector w'' and bias b. Period is missing. \n\n- On page 2 ``... illustrate the preimage for the case of points on the lines ... respectively'' \nPlease indicate which is which.  \n\n- In Figure 1. Is this a sketch, or the actual illustration of a network. In the latter case, please state the specific value of x and the weights that are depicted. Also define and explain the arrows precisely. \nWhat are the arrows in the gray part? \n\n- On page 3 `` This means that the preimage is just the point x^{(l)}''  the points that W maps to x^{(l+1)}. \n\n- On page 3 the first display equation. There is an index i on the left but not on the right hand side. \nThe quantifier in the right hand side is not clear. \n\n- ``generated by the mapping ... w^i '' subscript\n\n- ``get mapped to this hyperplane'' to zero \n\n- ``remaining'' remaining from what? \n\n- ``using e.g. Grassmann-Cayley algebra'' \nHow about using elementary linear algebra?!\n\n- ``gives rise to a linear manifold with dimension one lower at each intersection'' \nThis holds if the hyperplanes are in general position. \n\n- ``is complete in the input space'' forms a basis \n\n- ``remaining kernel'' remaining from what? \n\n- ``kernel'' Here kernel is referring to nullspace or to a matrix of orthonormal basis vectors of the nullspace, or to what specifically? \n\n- Figure 3. Nullspaces of linear maps should pass through the origin. \n\n- `` from pairwise intersections'' \\cap \n\n- ``indicated as arrows or the shaded area'' this description is far from clear. \n\n- typos: peieces, diminsions, netork, me, \n \n\n", "title": "terminology", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJrQkpHVg": {"type": "review", "replyto": "HJcLcw9xg", "review": "Kernel is referring to the nullspace? If so, should it not pass through the origin and what is a transformation kernel? \nWhat is the considered x^{(l+1)} in Figure 1? \n\nI have not read the revised version in detail yet. \n\nSUMMARY \nThis paper studies the preimages of outputs of a feedforward neural network with ReLUs. \n\nPROS \nThe paper presents a neat idea for changes of coordinates at the individual layers. \n\nCONS \nQuite unpolished / not enough contributions for a finished paper. \n\nCOMMENTS \n- In the first version the paper contains many typos and appears to be still quite unpolished. \n\n- The paper contains nice ideas but in my opinion it does not contribute sufficiently many results for a Conference paper. \nI would be happy to recommend for the Workshop track. \n\n- Irreversibly mixed and several other notions from the present paper are closely related to the concepts discussed in [Montufar, Pascanu, Cho, Bengio, NIPS 2014]. I feel that that paper should be cited here and the connections should be discussed. In particular, that paper also contains a discussion on the local linear maps of ReLU networks. \n\n- I am curious about the practical considerations when computing the pre-images. The definition should be rather straight forward really, but the implementation / computation could be troublesome. \n\n\nDETAILED COMMENTS \n- On page 1 ``can easily be shown to be many to one'' in general. \n\n- On page 2 ``For each point x^{l+1}'' The parentheses in the superscript are missing. \n\n- After eq. 6 ``the mapping is unique''  is missing `when w1 and w2 are linearly independent' \n\n- Eq. 1 should be a vector. \n\n- Above eq. 3. ``collected the weights a_i into the vector w'' and bias b. Period is missing. \n\n- On page 2 ``... illustrate the preimage for the case of points on the lines ... respectively'' \nPlease indicate which is which.  \n\n- In Figure 1. Is this a sketch, or the actual illustration of a network. In the latter case, please state the specific value of x and the weights that are depicted. Also define and explain the arrows precisely. \nWhat are the arrows in the gray part? \n\n- On page 3 `` This means that the preimage is just the point x^{(l)}''  the points that W maps to x^{(l+1)}. \n\n- On page 3 the first display equation. There is an index i on the left but not on the right hand side. \nThe quantifier in the right hand side is not clear. \n\n- ``generated by the mapping ... w^i '' subscript\n\n- ``get mapped to this hyperplane'' to zero \n\n- ``remaining'' remaining from what? \n\n- ``using e.g. Grassmann-Cayley algebra'' \nHow about using elementary linear algebra?!\n\n- ``gives rise to a linear manifold with dimension one lower at each intersection'' \nThis holds if the hyperplanes are in general position. \n\n- ``is complete in the input space'' forms a basis \n\n- ``remaining kernel'' remaining from what? \n\n- ``kernel'' Here kernel is referring to nullspace or to a matrix of orthonormal basis vectors of the nullspace, or to what specifically? \n\n- Figure 3. Nullspaces of linear maps should pass through the origin. \n\n- `` from pairwise intersections'' \\cap \n\n- ``indicated as arrows or the shaded area'' this description is far from clear. \n\n- typos: peieces, diminsions, netork, me, \n \n\n", "title": "terminology", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}