{"paper": {"title": "Unsupervised Neural Multi-Document Abstractive Summarization of Reviews", "authors": ["Eric Chu", "Peter J. Liu"], "authorids": ["echu@mit.edu", "peterjliu@google.com"], "summary": "We propose an end-to-end neural model for unsupervised multi-document abstractive summarization, applying it to business and product reviews.", "abstract": "Abstractive summarization has been studied using neural sequence transduction methods with datasets of large, paired document-summary examples. However, such datasets are rare and the models trained from them do not generalize to other domains. Recently, some progress has been made in learning sequence-to-sequence mappings with only unpaired examples. In our work, we consider the setting where there are only documents (product or business reviews) with no summaries provided, and propose an end-to-end, neural model architecture to perform unsupervised abstractive summarization. Our proposed model consists of an auto-encoder trained so that the mean of the representations of the input reviews decodes to a reasonable summary-review.  We consider variants of the proposed architecture and perform an ablation study to show the importance of specific components.  We show through metrics and human evaluation that the generated summaries are highly abstractive, fluent, relevant, and representative of the average sentiment of the input reviews.", "keywords": ["unsupervised learning", "abstractive summarization", "reviews", "text generation"]}, "meta": {"decision": "Reject", "comment": "This paper introduces a method for unsupervised abstractive summarization of reviews.\n\nStrengths:\n\n(1) The direction (developing unsupervised multi-document summarization systems) is exciting\n\n(2) There are interesting aspects to the model\n\nWeaknesses:\n\n(1)  The authors are clearly undecided how to position this work: either as introducing a generic document summarization framework or as an approach specific to summarization of reviews. If this is the former, the underlying assumptions, e.g., that the summary looks like a single document in a group is problematic. If this is the latter, then comparison to some more specialized methods are lacking (see comments of R1).\n\n(2) Evaluation, though improved since the first submitted version (when human evaluation was added), is still not great (see R1 / R3). The automatic metrics are not very convincing and do not seem to be very consistent with the results of human eval. I believe that instead or along with human eval, the authors should create human written summaries and evaluate against them. It has been done for extractive multi-document summarization and can be done here. Without this, it would be impossible to compare to this submission in the future work.  \n\n(3) It is not very clear that generating abstractive summaries of the form proposed in the paper is an effective way to summarize documents.  Basically, a good summary should reflect diversity of the opinions rather than reflect an average / most frequent opinion from tin the review collection.  By generating the summary from a review LM, the authors make sure that there is no redundancy (e.g., alternative views) or contradictions. That's not really what one would want from a summary  (See R3 and also non-public discussion with R1)\n\nOverall, I'd definitely like to see this work published but my take is that it is not ready yet.\n\nR1 and R2 are relatively negative and generally in agreement.  R3 is very positive. I share excitement about the research direction with R3 but I believe that concerns of R1 and R2 are valid and need to be addressed before the paper gets published.\n\n\n\n\n\n\n"}, "review": {"r1gFz6AaAQ": {"type": "rebuttal", "replyto": "rylhToC5YQ", "comment": "Regarding usefulness/practicality of abstractive summarization, we believe the most natural form of summary for humans is language, i.e. sentences/paragraphs. Certainly extracting common bi-grams could be done, is straightforward, and has been done in review-specific summarization systems in prior work, but is in our opinion less natural. That is a different problem than what we\u2019re trying to solve.\n\nRegarding the comments for comparisons with opinion-based summarization models, we also sought to create a general, domain-agnostic model architecture that could be applied to non-review documents by not relying on any review-specific features.", "title": "We avoid review-specific features in our model to be more generally applicable"}, "B1xGr10aCm": {"type": "rebuttal", "replyto": "B1gpdP5K37", "comment": "We want to clarify that the automatic metrics are used to guide model development. However, the final evaluation is done with humans. In future comparisons with our method, we expect a human evaluation to be done as well.\n\nWe did not focus on comparing to review-specific algorithms because our method does not rely on any review-specific properties, domain-knowledge, or highly engineered features, unlike the above papers. Review-specific choices could improve the algorithm, but would take away from the generalizability of the proposed architecture/model which is our goal. The spirit of this conference is learning generic representations of data that can be widely applicable, and not to be focused on domain-specific feature engineering. We thus focused our comparison on generic approaches without heavy feature engineering. We expect follow-up work to use our model architecture on other domains with minimal changes.\n\nP.S. Just a point of clarification, despite the title, the authors of Ganesan et al. (Opinosis), say in the paper their method is \u201cword-level extractive summarization\u201d and not actually abstractive.", "title": "response to Nov 24 review modification"}, "B1gpdP5K37": {"type": "review", "replyto": "rylhToC5YQ", "review": "This paper proposes a method for multi-document abstractive summarization. The model has two main components, one part is an autoencoder used to help learn encoded document representations which can be used to reconstruct the original documents, and a second component for the summarization step which also aims to ensure that the summary is similar to the original document. \n\nThe biggest problem with this paper is in its evaluation methodology. I don't really know what any of the three evaluation measures are actually measuring, and there is no human subject evaluation back them up.\n- Rating Accuracy seems to depend on the choice of CLF used, and at best says whether the summary conveys the same average opinion as the original reviews. This captures a small amount about the actual contents of the reviews. For example, it does not capture the distribution of opinions, or the actual contents that are conveyed.\n- Word Overlap with the original documents does not seem to be a good measure of quality for abstractive systems, as there could easily be abstractive summaries with low overlap that are nevertheless very good exactly because they aggregate information and generalize. It is certainly not appropriate to use to compare between extractive and abstractive systems.\n-There are many well-known problems with using log likelihood as a measure of fluency and grammaticality, such as biases around length, and frequency of the words.\nIt also seems that these evaluation measures would interact with the length of the summary being evaluated in ways which systems could game.\n\nOther points:\n- Multi-Lead-1: The lead baseline works very well in single-document news summarization. Since this model is being applied in a multi-document setting to something that is not news, it is hard to see how this baseline is justified.\n\n- Despite the fact that the model is only applied to product reviews, and there seem to be modelling decisions tailored to this domain, the paper title does not specify so, which in my opinion is a type of over-claiming.\n\nHaving a paper with poor evaluation measure may set a precedent that causes damage to an entire line of research. For this reason, I am not comfortable with recommending an accept.\n\n\n---\nThank you for responding to my comments and updating the paper. I have slightly raised my score to reflect this effort.\n\nThere are new claims in the results section that do not seem to be warranted given the human evaluation. The claim is that the human evaluation results validate the use of the automatic metrics. The new human evaluation results show that the proposed abstractive model performs on par with the extractive model in terms of conveying the overall sentiment and information (Table 2), whereas it substantially outperforms the extractive model on the automatic measures (Table 1). This seems to be evidence that the automatic measures do not correlate with human judgments, and should not be used as evaluation measures.\n\nI am also glad that the title was changed to reflect the scope of the experiments. I would now suggest comparing against previous work in opinion summarization which do not assume gold-standard summaries for training. Here are two representative papers:\n\nGanesan et al. Opinosis: A Graph-Based Approach to Abstractive Summarization of Highly Redundant Opinions. COLING 2010.\nCarenini et al. Multi-Document Summarization of Evaluative Text. Computational Intellgience 2012.", "title": "Evaluation methodology and measures are questionable and should not be adopted by the community", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJeQ9oU_6X": {"type": "rebuttal", "replyto": "BJgnUKPvhX", "comment": "Thank you for your feedback. \n\n-We clarified in the paper that a limitation here is the summary is assumed to be in the form of a review with similar stylistic characteristics as the input reviews.\n-Although ROUGE is often used in the summarization literature, it attempts to approximate human evaluation of summaries, which is the gold standard. We have shown that the metrics we used in model development have guided us to good human evaluations.\n- Indeed the symptoms discussed in the error analysis affect all current neural text generation models, and we included them to ensure we didn't claim to have solved it.", "title": "Added human evaluation and further discussed limitations. "}, "SylKksg_pm": {"type": "rebuttal", "replyto": "rylhToC5YQ", "comment": "We thank all 3 reviewers for their feedback, which we have used to improve the paper. We summarize the changes here and replied individually as well to each reviewer below. \n\nWhile the proxy metrics defined are useful in model development, the gold standard for evaluating summaries, human evaluation, was missing and we have now added it to validate our model. These results agree (rank order) with the automatic metrics and show that the abstractive model has comparable sentiment agreement, information agreement, and fluency with the extractive method.\n\nWe made many changes to clarify the problem more formally and described the models in more detail. We also clarified limitations of the model and metrics. \n\nWhile we believe the architecture proposed can be applied to data other than reviews, we added \u201c... of Reviews\u201d to the title since we only showed results on reviews.\n", "title": "Summary of changes, Nov 12, 2018."}, "B1eUccxupQ": {"type": "rebuttal", "replyto": "B1gpdP5K37", "comment": "Thank you for your feedback, which we have incorporated, and resulting in, we believe, a much stronger paper. We agree the lack of human evaluation to validate our methods was a glaring omission in the original paper. As a result, we added Table 2 with human evaluation results of the summaries on multiple dimensions, showing our model is competitive with the extractive baseline with respect to representing the overall sentiment and information in the input reviews and also the fluency. We clarified that the automatic metrics (which rank order the methods similarly) are useful for guiding model development, but the only gold standard here is human evaluation.\n\nRegarding the points about the metrics:\nRating accuracy: The sentiment of a good summary should be reflective of the overall sentiment of the reviews. We approximate this overall segment by the average rating. We clarify that this captures a necessary aspect of the summary, but by itself is not sufficient, which is why we have other things we look at, including now human evaluation.   The \u201cactual contents that are conveyed\u201d is meant to be covered by the word overlap score and one of our human eval questions.\nWord overlap: we agree with your point that abstractive systems could have lower overlap because they aggregate information and generalize. We clarified in the paper that this word overlap score is included as a sanity check: it\u2019s possible to get a high rating accuracy while talking about something completely unrelated, and a very low word-overlap would suggest something pathological. That said, it appears to rank-order similarly as our human evaluation question.\nNegative log-likelihood: In this paper now we only use this metric to compare abstractive variants. It\u2019s true that using it to compare to the \u201cConcatenation\u201d baseline (now removed) was inappropriate. The gold standard for measuring fluency would be a human evaluation which we added.\n\nOther points:\nMulti-lead 1: Having proved to be a strong baseline in other summarization tasks, we sought to create an analog of Multi-lead 1 in our multi-document setting. This proved to be a reasonably strong baseline. In any case, this is simply one of several baselines which we compare against.\nWe don\u2019t want to overclaim, and we\u2019ve modified the title of our paper to include \u201cof Reviews\u201d and added clarification of limitations in the Conclusion.", "title": "Added human evaluation (that corresponds to our metrics)"}, "SJgmRtgdT7": {"type": "rebuttal", "replyto": "BygCDbac37", "comment": "Thank you for your review and the very helpful, comprehensive feedback, which we strove to address. We agree that the biggest previous issue was uncertainty around the evaluation. As a result we added results of a human evaluation that directly assesses various aspects of summarization quality and showing similar results as our proxy metrics. Regarding your specific points:\n\n1. We\u2019ve added a more formal, mathematical description of the problem setup. Overall, we\u2019ve tried to be clearer and and consistent with our notation.\n\n3. Good question. We did try a larger weight on l_sim (as intuitively, this loss helps the model produce outputs that actually summarize the original review), but we did not find meaningful differences and pointed this out in the paper.\n\n4. Although there are no ground-truth summaries for Equation 2, there are ground truth reconstructions in Equation 1. As shown in the ablations, it is crucial that the decoders are tied in this architecture. In one of our experiments, the \u201cEarly cosine loss\u201d (also shown schematically in Appendix A), we did not need to use the Gumbel-softmax estimator and simply decoded auto-regressively from the mean vector. That experiment shows that the decoding the summary as part of training significantly improves results.\n\n5. (1) We\u2019ve added details about how the language model was trained in the Experimental Setup section, as well as how the reviews were generated using the language model in the \u201cNo Training\u201d baseline in the Baselines section. The reviews in the \u201cNo training\u201d model are generated in the same fashion as the proposed model. The purpose of this baseline is to show that optimizing the proposed loss improves the output over simply using pre-trained language models.\n\n(2) Great point. We\u2019ve added human evaluation experiments on Mechanical Turk regarding the quality of the summaries to assess the validity of our metrics. Briefly, the results show that our metrics guided us to a good model -- the extractive and abstractive models obtain comparable results on how well they summarize information and sentiment, and the abstractive summaries are similarly fluent.\n\n(3) There are no known neural, end-to-end models for this problem setup and this being the first is one of our main contributions. We hoped that reasonable model variations and ablations would probe into the efficacy of various aspects of our model. \n\n6. We\u2019ve modified the Rating accuracy description to hopefully make clear that the classifier is trained by taking as input a review x (sequence of tokens) and producing probabilities over the 5 possible ratings (i.e. it is a classification problem and not a regression problem). There are no hand-engineered features. The rating with the highest probability is the predicted rating. This is then compared to the average rating of the original reviews (rounded to the nearest 1-5 star rating).\n\nIn general, we agree that the classifier baseline should be applied carefully. We\u2019ve removed the concatenation baseline because we believe it\u2019s outside the input space of the classifier. However, we believe the rating accuracy still applies to the other models and is a useful metric. For instance, the summaries produced by our model are constrained to the review space due to the tying of the decoders. Our human evaluation experiments also agree with the trends provided by our rating accuracy metric.\n\n7. The assumption we make is the summary should be in some sense the \u201ccentroid\u201d of the documents it is summarizing. If there are some positive reviews, but they are mostly negative, the summary will be mostly negative which is representative. If a priori we have a notion of review importance, we could weight some reviews higher in Equation (3) rather than equally. Or as you suggest we could summarize different clusters; in this case the most natural clustering is by review rating. In Figure 3, we show how multiple reviews could be generated for the same business, but pre-clustered by rating. We also clarify that our model architecture produces summaries in the form of a single review.\n", "title": "Added human evaluation; additions to improve clarity of paper"}, "BygCDbac37": {"type": "review", "replyto": "rylhToC5YQ", "review": "Overall and positives:\n\nThe paper investigates the problem of multidocument summarization\nwithout paired documents to summary data, thus using an unsupervised\napproach. The main model is constructed using a pair of locked\nautoencoders and decoders. The model is trained to optimize the\ncombination of 1. Loss between reconstructions of the original reviews\n(from the encoded reviews) and original the reviews, 2. And the\naverage similarity of the encoded version of the docs with the encoded\nrepresentation of the summary, generated from the mean representation\nof the given documents.\n\nBy comparing with a few simple baseline models, the authors were able\nto demonstrate the potential of the design against several naive\napproaches (on real datasets, YELP and AMAZON reviews). \nThe necessity of several model components is demonstrated\nthrough ablation studies. The paper is relatively well structured and\ncomplete. The topic of the paper fits well with ICLR. The paper\nprovides decent technical contributions with some novel ideas about\nmulti-doc summary learning models without a (supervised) paired\ndata set.\n\nComments / Issues\n\n[ issue 6 is most important ]\n\n1.  Problem presentation. The problem was not properly introduced and\nelaborated. In fact, there is not a formal and mathematical\nintroduction of the problem, input, output, dataset and model\nparameters. The notations used are not very clearly defined and are\nquite handwavy, (e.g. what is V, dimensions of inputs x_i was not\nmentioned until much later in the paper). The authors should make\nthese more precise. Similar problem with presentations of the models,\nparameters, and hyperparameters.\n\n3.  How does non-equal weighted linear combinations of l_rec and l_sim\nchange the results? Other variation of the overall loss function? How\ndo we see the loss function interaction in the training, validation\nand test data? With the proposed model, these could be interesting to\nobserve.\n\n4.  In equation two, the decoder seems to be very directly affecting\nthe quality of the output summary. Teacher forcing was used to train\nthe decoder in part (1) of the model, but without ground truth, I\nwould expect more discussions and experiments on how the Gumbel\nsoftmax trick affect or help the performance of the output.\n\n5.  Baseline models and metrics\n\n(1) There should be more details on how the language model is trained,\nsome examples, and how the reviews are generated from the language\nmodel as a base model (in supplement?).\n\n(2). It is difficult to get a sense of how these metrics corresponds\nto the actual perceived quality of the summary from the\npresentation. (see next)\n\n(3). It will be more relevant to evaluate the proposed design\nvs. other neural models, and/or more tested and proved methods.\n\n6. The rating classifier (CLF) is intriguing, but it's not clearly\nexplained and its effect on the evaluation of the performance is not\nclear: One of the key metrics used in the evaluation relies on the\noutput rating of a classifier, CLF, that predicts reader ratings on\nreviews (eg on YELP).  The classifier is said to have 72%\naccuracy. First, the accuracy is not clearly defined, and the details\nof the classifier and its training is not explained (what features are\nits input, is the output ordinal regression).  Equation 4 is not\nexplained clearly: what does 'comparing' in 'by comparing the\npredicted rating given the summary rating..' mean?  The classifier may\nhave good performance, but it's unclear how this accuracy should\naffect the results of the model comparisons.\n\nThe CLF is used to evaluate the rating of output\nreviews from various models. There is no justification these outputs\nare in the same space or generally the same type of document with the\ntraining sample (assuming real Yelp reviews).  That is probably\nparticularly true for concatenation of the reviews, and the CLF classifier\nscores the concatenation very high (or  eq 4 somehow leads to highest value\nfor the concatenation of reviews )... It's not clear whether such a classifier is \nbeneficial in this context.\n\n7. Summary vs Reviews. It seems that the model is built on an implicit\nassumption that the output summary of the multi-doc should be\nsufficiently similar with the individual input docs.  This may be not\ntrue in many cases, which affects whether the approach generalizes.\nDoc inputs could be covering different aspects of the review subject\n(heterogeneity among the input docs, including topics, sentiment etc),\nor they could have very different writing styles or length compared to\na summary.  The evaluation metrics may not work well in such\nscenarios.  Maybe some pre-classification or clustering of the inputs,\nand then doing summarization for each, would  help?  In the conclusions section, the\nauthors do mention summarizing negative and positive reviews\nseparately.\n\n\n\n\n\n", "title": "Promising unsupervised approach, but clarity issues", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJgnUKPvhX": {"type": "review", "replyto": "rylhToC5YQ", "review": "# Positive aspects of this submission\n\n- This submission presents a really novel, creative, and useful way to achieve unsupervised abstractive multi-document summarization, which is quite an impressive feat.\n\n- The alternative metrics in the absence of ground-truth summaries seem really useful and can be reused for other summarization problems where ground-truth summaries are missing. In particular, the prediction of review/summary score as a summarization metric is very well thought of.\n\n- The model variations and experiments clearly demonstrate the usefulness of every aspect of the proposed model.\n\n# Criticism\n\n- The proposed model assumes that the output summary is similar in writing style and length to each of the inputs, which is not the case for most summarization tasks. This makes the proposed model hard to compare to the majority of previous works in supervised multi-document summarization like the ones evaluated on the DUC 2004 dataset.\n\n- The lack of applicability to existing supervised summarization use cases leaves unanswered the question of how much correlation there is between the proposed unsupervised metrics and existing metrics like the ROUGE score, even if they seem intuitively correlated.\n\n- This model suffers from the usual symptoms of other abstractive summarization models (fluency errors, factual inaccuracies). But this shouldn't overshadow the bigger contributions of this paper, since dealing with these specific issues is still an open research problem.", "title": "Novel work breaking ground on abstractive unsupervised multi-document summarization", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}