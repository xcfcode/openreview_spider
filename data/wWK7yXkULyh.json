{"paper": {"title": "MONGOOSE: A Learnable LSH Framework for Efficient Neural Network Training", "authors": ["Beidi Chen", "Zichang Liu", "Binghui Peng", "Zhaozhuo Xu", "Jonathan Lingjie Li", "Tri Dao", "Zhao Song", "Anshumali Shrivastava", "Christopher Re"], "authorids": ["~Beidi_Chen1", "~Zichang_Liu1", "~Binghui_Peng1", "~Zhaozhuo_Xu1", "~Jonathan_Lingjie_Li1", "~Tri_Dao1", "~Zhao_Song3", "~Anshumali_Shrivastava1", "~Christopher_Re1"], "summary": "We propose MONGOOSE,  a learnable LSH framework for efficient neural network training.", "abstract": "Recent advances by practitioners in the deep learning community have breathed new life into Locality Sensitive Hashing (LSH), using it to reduce memory and time bottlenecks in neural network (NN) training. However, while LSH has sub-linear guarantees for approximate near-neighbor search in theory, it is known to have inefficient query time in practice due to its use of random hash functions. Moreover, when model parameters are changing, LSH suffers from update overhead. This work is motivated by an observation that model parameters evolve slowly, such that the changes do not always require an LSH update to maintain performance. This phenomenon points to the potential for a reduction in update time and allows for a modified learnable version of data-dependent LSH to improve query time at a low cost. We use the above insights to build MONGOOSE, an end-to-end LSH framework for efficient NN training. In particular, MONGOOSE is equipped with a scheduling algorithm to adaptively perform LSH updates with provable guarantees and learnable hash functions to improve query efficiency. Empirically, we validate MONGOOSE on large-scale deep learning models for recommendation systems and language modeling. We find that it achieves up to 8% better accuracy compared to previous LSH approaches, with $6.5 \\times$ speed-up and $6\\times$ reduction in memory usage.", "keywords": ["Large-scale Deep Learning", "Large-scale Machine Learning", "Efficient Training", "Randomized Algorithms"]}, "meta": {"decision": "Accept (Oral)", "comment": "Thanks for your submission to ICLR.\n\nWhen the initial reviews were written, three of the four reviewers were positive about the paper.  Everyone felt it was overall a solid contribution, but there were some concerns about the clarity and presentation, as well as some suggestions for additional experiments.  During the rebuttal/response period, the authors did a very nice job in responding to the concerns of the reviewers.  Ultimately, all of the reviews were in agreement after discussion that the paper is strong and ready for publication.   I also like this paper a lot, and find it to be a nice way to combine LSH with NN training.  I am happy to recommend this paper for publication."}, "review": {"DWM-xFtZlNL": {"type": "rebuttal", "replyto": "wWK7yXkULyh", "comment": "We thank all the reviewers for the time and effort in helping us improve the quality of the paper. We were glad that the reviewers found the problem **interesting, necessary and critical** ( R2, R4), the observation  **smart, inspiring and impressive** (R1, R2, R3, R4), and the approach or algorithm **principle, novel and clever** (R1, R2, R3, R4). The reviewers also agreed that the theoretical analysis was **solid and believable** (R1, R3) and the experiments were **ample and effective** (R1, R4).    \n\nWe have updated the paper to incorporate constructive suggestions. We summarize the major changes:\n1. [R4] an analysis of the speedup and memory savings of the linear layer where MONGOOSE is applied during training in Section 4.1.1 and Table 2.\n2. [R4] the memory usage in Table 1 and Section 4.1.2 along with more detailed comparisons and discussions in Appendix D. \n3. [R4] an ablation study of parameters of the scheduler in Section 4.2 and also learnable LSH in Section 4.3. \n4. [R2] a comparison between the updating time of HNSW (a graph-based ANNS data structure) and our learnable LSH in Section 4.3.\n5. [R2] updated our main result of the extreme classification task with an additional larger dataset in Section 4.1.1 and the language modeling task with a 10-layer model in Section 4.1.2.\n6. [R1, R2, R3, R4] a notation section in Section 3.2.1 and addressed all other notation concerns in Section 3. \n7. [R4] a discussion on the connection between the MONGOOSE scheduler and a former work (Cohen et al., 2019) in Appendix B.\n8. [R2] a broader impact discussion in Appendix G.\n9. [R1] illustrations for Algorithm 1 in Figure 8 (Appendix B.1) and Algorithm 2 in Figure 9 (Appendix C.1) along with descriptions. \n", "title": "Revision Summary"}, "tVmc8rRA7Vj": {"type": "rebuttal", "replyto": "PXRfhCdvPZ", "comment": "**Q1. I agree that rebuilding HNSW from scratch can be slow, and thanks for adding this discussion and measurements. The measurements add weight to the choice of LSH. However, I would still be open to the possibility of updating graph based indices -- this is ongoing work in the community and would not conclude that graph-based ANNS is not easy to update.**\n\nYes, we agree that it is an ongoing work to reduce the updating overhead of graph-based NNS-ds in the community, and we will be excited to include it in our framework in the near future. We have revised Section 4.3 to reflect this.\n\nIt would be very helpful to further enrich our related work if you can also point us to any work we have missed on dynamic maintenance of graph-based indices.  \n\n**Q2. Thanks for the context. I understand a quantitative summary of impact would be difficult to produce so I am fine with your arguments.**\n\nThanks for your understanding!\n\n**Q3. On the XC repo (http://manikvarma.org/downloads/XC/XMLRepository.html), there are datasets much larger than the wiki-325K dataset you have experimented with. Is there a reason you picked this and not the largest? If there is no strong reason to the contrary, please consider presenting results on the largest dataset.**\n\nThanks for pointing this out! We have been in the process of running larger experiments and focused on the enwik8 experiment for our last revision. \n\nAmazon-670K and Amazon-3M are the two largest publicly available datasets (Ads-1M and 9M are not). We have added the results for Amazon-670K, which shows 6.5$\\times$ speedup (compared to the 3$\\times$ on Wiki-325K) in Section 4.1.1. In fact, the empirical results, especially the ones we added based on your suggestions, show that MONGOOSE would have more advantage when the NN layer's size grows because there is more room for MONGOOSE to accelerate. We appreciate your suggestion which helps us better understand the scalability of our algorithm!    \n\nDue to the limited time and resources during the rebuttal, we have added the results for Amazon-670K to show the advantage of MONGOOSE on even larger models. The experiments for Amazon-3M are currently in progress, and we will definitely add the results to the next version when we are allowed to make the edits again.\n", "title": "Reply to Additional Comments by AnonReviewer2 "}, "HoOYB8CSC2P": {"type": "rebuttal", "replyto": "F-O03j6eqq", "comment": "We are glad our revision has addressed the concerns and we thank the reviewer for more strongly supporting our paper!", "title": "Thank You!"}, "F-O03j6eqq": {"type": "review", "replyto": "wWK7yXkULyh", "review": "Summary of the paper:\n\nThis paper introduces a framework that uses an LSH sketches to improve time an memory bottlenecks neural network training. Specifically, an LSH sketch is used to approximate the matrix multiplications involved in training. It is observed that networks' weights get stable after small number of epochs therefore frequent updates to LSH sketches (which is expensive) are not required. This paper uses data dependent/learnable LSH methods that better adapt to data in order to improve the performance (query and update) of the sketches. Ample experiments are provided to validate the results.\n\nQuality:\n\nNeeds improvements in notations. For example, in assumption 3.1, sum is over what? If the indexing is over $i$, then does that means over rows of $w$s (since $w \\in \\mathbb{R}^{n \\times d}$)? Then should the quantities be L2 norms since they are vectors?\n\nClarity and the presentation can be improved significantly. For example, \"Rehash\", \"Rebuild\" functions in algorithm 2 needs to be defined or explained. In general it is better to explain the intuition behind both algorithms 1 and 2.  I believe the figure 1 should clarify the LSH update scheduling, but it is not very clear.\n\nOriginality and significance:\n\nI believe that the ideas presented in this paper adds nice contributions to the ICLR community. The idea of using learnable LSH that adapts to the data together with the observation that the weights stabilize after a few epochs is a clever approach to improve the bottlenecks associated with using vanilla LSH sketches.\n\nOther comments:\n\nIn regards to the observation with figure 3, I am curious what properties of a dataset leads to this kind of behavior? are there any quantifiable properties? Is this true for any dataset? Are there previous works that explains why this is the case?\n\nFinal feedback:\nI understand the idea of the proposed framework and the theoretical claims look natural and believable, but the presentation needs to be improved. I am willing to increase my score if the concerns mentioned above are properly addressed.\n\n\n=====================================================================================================\n\nAdded after author response\n\n----------------------------------------\n\nI believe authors have clarified many things I asked and addressed the issues I and other reviewers raised. Therefore, I increase my score. I believe the idea of using LSH for efficient training has a lot of promise and this paper brings a possible way to do this into light.", "title": "The framework presented in the paper adds decent contributions, but substantial improvements are needed in writing and presentation.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BIVs4zjKyFz": {"type": "rebuttal", "replyto": "GbE9SrmbgMM", "comment": "Thank you again for helping us improve the paper!", "title": "Thank you!"}, "pdxkWtrShCE": {"type": "review", "replyto": "wWK7yXkULyh", "review": "Summary:\nThe authors make a good insight into the slowly changing of Locality-Sensitive Hashing (LSH) hash codes for the weights (or model parameters) during the Neural Network (NN) training. With this new insight, they introduce a framework Mongoose with a newly designed schedule mechanism to reduce the LSH update overhead. The authors also analyse their model and show some bounds for batch speedup and LSH maintenance. Experimental results validate the efficiency and effectiveness of Mongoose over original LSH methods in NN training.\n\nPros:\n1. First of all, I like the problem the authors focus on. Efficient NN training is very necessary. The idea of using LSH for acceleration is a good direction. \n2. The observation of the slow change of LSH hash codes is good and impressive. The authors also conduct many experiments to validate this observation. \n3. The idea of using a scheduler for lazy updating is interesting, and they add theoretical analysis about it. And based on the experiments, it seems that this scheduler can capture the changing of model parameters. \n4. The proposed framework Mongoose seems to be general for different deep learning model.\n\nCons:\n1. Compared to the good insight and the promising framework, the practical improvement is fair. Although the authors provide many experiments to demonstrate the effectiveness of Mongoose, I still suggest the authors conduct more experiments about parameter studies and memory usage which may be good to improve the paper quality. More details can be found in minor comments (1 & 2) later. \n2. Since the idea of the scheduler is inspired by a former work (Cohen et al., 2019), I suggest the authors add a discussion about their connection and difference. \n3. The presentation is not very clear. Many notations are used without pre-defined. More details can also be found in minor comments (3 ~ 6) later.\n4. For the learnable LSH (Section 3.3.1), when selecting positive/negative samples, the authors use the inner product, while for the loss function, they consider cosine similarity. I suggest the authors make an illustration about the use of these two different measures. \n\nMinor Comments or Typos:\n1. The framework Mongoose consists of many parameters. A fresh user may do not know how to set up them. A discussion of parameter settings or some experiments about parameters studies seems to be necessary.\n2. The authors claim Mongoose is memory-efficient for NN training. However, I cannot find analysis about space overhead or experiments about memory usage. It should be done to correspond to this claim. \n3. For Assumption 3.1, what are the definitions of C_1 and C_2? Is user-defined or auto determined? Since Theorem 3.3 and some Lemma in the appendix also use these two notations, are they the same? For a rigorous expression, I think some declarations are necessary. Also, in Theorem 3.3, the condition of r for g_r is not clear.  \n4. More problems can be found in Algorithm 1. It seems the core idea of this paper, but there are many typos and errors. For example, \nThe parameter \u2018A\u2019 is not defined and may be unnecessary in Initialize function (line 2). \nIn line 4, what is the definition of \\epsilon_{mp}? Does it have any connection with \\epsilon_{mds}? In order to have guarantee for LSH, c_1 - \\epsilon_{mp} should be larger than c_2 + \\epsilon_{mp}. How to ensure this?\nWhy setting 1.5 for a smooth cut (lines 13-15)? Is it an empirical value or has any benefit?\nIn line 17, LSH may update with w^{new}_{\\pi([r])} instead of \\pi([r]).  \n5. The y-axis of Figure 3 should be \\Delta H (left-subfigure) and \\Delta W (right-subfigure) instead of Hamming and L2 distance. \n6. The caption of Figure 5 is not clear. And for the sentence \u201cFigure 5 shows P@1 and P@5 for Mongoose and other baselines\u2026\u201d in the paragraph of results in Section 4.1.1, \u201cand P@5\u201d should be removed. \n\nIn summary, I like the motivation and the observation in this paper, and the method Mongoose looks good, but I still have many concerns about the idea. So at this stage, I first give a borderline for this paper. I hope the authors can address my concerns. \n\n====================================================================================================\n\nUpdate: Thank you for your new experiments and detailed feedback. Most of the concerns have been addressed and the experimental results look better. I believe this paper will provide new insight into efficient neural network training. Thus, I raise my rating and recommend this paper to be accepted.\n", "title": "Good intuition and novel method, but fair practical performance", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "K5v82DUOeUS": {"type": "rebuttal", "replyto": "pdxkWtrShCE", "comment": "We thank the reviewer for the constructive and detailed suggestions. We appreciate your generous comments on the \u201cimpressive\u201d observation and \u201cnovel\u201d method. \n\n**Q1.1: Compared to the good insight and the promising framework, the practical improvement is fair.**\n\nResponse: We have added a closer analysis of the speedup and memory savings of the linear layer where MONGOOSE is applied during training in Section 4.1.1 and Table 2. It shows that the speedup is up to 20$\\times$, and memory is 4.5$\\times$ in Table 2 compared to 3$\\times$ (time) and 1.5$\\times$ (mem) in Table 1 in the original version. We want to clarify that the main result in Table 1 is based on MONGOOSE's end-to-end performance (the whole model) during training. We have noticed this presentation does not show MONGOOSE's actual gain and benefit. With this additional analysis, we can separate the effect of an earlier large embedding layer. (The efficient embedding layers is an independent interest and has been explored in [3,4], which we do not aim to accelerate in this work.) We sincerely thank the reviewer for pointing this out so that we can improve the way of presenting our results. \n\n**Q1.2:  Although the authors provide many experiments to demonstrate the effectiveness of Mongoose, I still suggest the authors conduct more experiments about parameter studies and memory usage which may be good to improve the paper quality. More details can be found in minor comments (1 & 2) later.**\n\nResponse:  We deeply agree that conducting a more in-depth ablation and sensitivity study on MONGOOSE\u2019s parameters and memory usage can lead to better practicality and usability of our framework for wider audiences. Therefore, we have added:\n1. the memory usage in Table 1, Table 2, and Section 4.1.2 along with more detailed comparisons and discussions in Appendix D. Specifically, Table 2 shows MONGOOSE has up to 4.5$\\times$ memory reduction when isolating the first embedding layer where it does not operate in extreme classification tasks. Table 1 shows that it has a 1.5$\\times$ reduction. Section 4.1.2 presents MONGOOSE has up to 6$\\times$ memory reduction in language modeling tasks. \n2. the ablation study of parameters of the scheduler in Section 4.2 and also learnable LSH in Section 4.3. In general, most of the parameters are robust, except for the number of hash functions of LSH. Specifically, for the scheduler, recall that in Algorithm 1, $\\epsilon_{mds}$ is the key parameter because the number of neurons exceeding this threshold determines if LSH updates are needed. In practice, instead of choosing a single $\\epsilon_{mds}$, we use some fraction of the $\\ell_2$ norm of the weights as the update threshold. That fraction is robust to any choice of the fraction in the range 0.1-0.3. For learnable LSH, we found it is not sensitive to the threshold of positive and negative pairs selection but a good choice of the number of hashes would maximize the advantage of MONGOOSE.  \n\n\n**Q2: Since the idea of the scheduler is inspired by a former work (Cohen et al., 2019), I suggest the authors add a discussion about their connection and difference.**\n\nResponse: The discussion about MONGOOSE scheduler and Cohen et al., 2019 [1] is added in Appendix B. (Cohen et al. 2019) originally designed the data structure for dynamic matrix inversion maintenance, and we adopt it for dynamic LSH maintenance problems. Given the underlying problem is completely different, it requires several non-trivial generalizations and some new ideas. This includes more fine-grained analysis in Lemma B2, B3, B4. Moreover, in our proof, we care about the absolute distance between vectors (i.e., $||w_i - v_i||_2$) rather than the relative difference between real numbers [1], and therefore, we need a new construction of the potential function (see Lemma B8).\n\n**Q3: The presentation is not very clear. Many notations are used without pre-defined. More details can also be found in minor comments (3 ~ 6) later.**\n\nResponse: Thanks for pointing out the presentation problem. We have updated our paper to address the notation issues: \n1. Definition: We add a paragraph summarizing the notations used in Algorithm 1 in Section 3.2.1. \n2. Consistency: We have changed all $\\epsilon_{mp}$ to $\\epsilon_{mds}$ and corrected the definition of $g_r$. We also remove redundant variables like A that are no longer used in the paper.\n3. Clarity: We add a detailed explanation to the $C_1$, $C_2$, $\\epsilon_{mds}$, $g_r$ and the remark that only $\\epsilon_{mds}$ needs fine-tuning in NN training (see remark 3.4). \n", "title": "Reply to Review by AnonReviewer4"}, "C8NYfnlvRH": {"type": "rebuttal", "replyto": "Ar-iE15aBR1", "comment": "We thank the reviewer for the strong support of our work! We have addressed definition and typo issues in the updated paper. \n\n**Q: By the way, I\u2019m curious about why you named your method \u201cMONGOOSE\u201d? Could you give some reasons?**\n\n**Response**: We name our framework MONGOOSE from the story Rikki-tikki-tavi (https://www.cs.cmu.edu/~mongoose/rtt.html): \u201cA **FULL** meal makes a **SLOW** mongoose, and if wanted all his strength and quickness ready, he must keep himself thin.\u201d Our framework is **SPARSE** and **FAST**. Thanks for asking! We are very proud of this name and hope you like it.", "title": "Reply to Review by AnonReviewer3"}, "Q77OG_pL8qY": {"type": "rebuttal", "replyto": "K5v82DUOeUS", "comment": "For your detailed comments.\n\n**For Assumption 3.1, what are the definitions of C_1 and C_2? Is user-defined or auto determined? Since Theorem 3.3 and some Lemma in the appendix also use these two notations, are they the same? For a rigorous expression, I think some declarations are necessary. Also, in Theorem 3.3, the condition of r for g_r is not clear.**\n\n**Response:** We have added the detailed definition of $C_1$ and $C_2$ in Section 3.1. Briefly speaking, $C_1$ is an upper bound on the (expected) movement of the weights of the neural network, $C_2$ is an upper bound on the variance. They need not be known in advance or fine-tuned (see remark 3.4). We correct the expression of $g_r$. \n\n**More problems can be found in Algorithm 1. It seems the core idea of this paper, but there are many typos and errors. For example, The parameter \u2018A\u2019 is not defined and may be unnecessary in the Initialize function (line 2). In line 4, what is the definition of \\epsilon_{mp}? Does it have any connection with \\epsilon_{mds}? In order to have guarantee for LSH, c_1 - \\epsilon_{mp} should be larger than c_2 + \\epsilon_{mp}. How to ensure this? Why setting 1.5 for a smooth cut (lines 13-15)? Is it an empirical value or has any benefit? In line 17, LSH may update with w^{new}_{\\pi([r])} instead of \\pi([r]).**\n\n**Response:** We correct these typos in Algorithm 1. In particular, (1) eps_{mp} equals eps_{mds}, (2) we remove redundant notation like A, (3) the constant 1.5 is arbitrary, the theoretical property of our scheduler holds as long as it belongs to (1, 2), (4) we take $\\epsilon_{mds}$ to be sufficiently small (and greater than $\\frac{1}{log^2 n}$) in theory. In practice, we need fine-tune $\\epsilon_{mds}$. We also update some discussion on the influence of this parameter (see Section 4.2).\n\n**The y-axis of Figure 3 should be \\Delta H (left-subfigure) and \\Delta W (right-subfigure) instead of Hamming and L2 distance.**\n\n**Response:** We correct the caption of Figure 5 and rewrite the sentence.\n\n**Q4: For the learnable LSH (Section 3.3.1), when selecting positive/negative samples, the authors use the inner product, while for the loss function, they consider cosine similarity. I suggest the authors make an illustration of the use of these two different measures.**\n\n**Response:** Because hashing operates on unit-length vectors, [2] normalizes key vectors: \u201c we additionally normalize the length of the keys K\u201d. Therefore, selecting samples based on inner product and cosine would be the same. But to avoid confusion, we have added this Section 3.3.1.\n   \n[1] Cohen, Michael B, et al \u201cSolving linear programs in the current matrix multiplication time\u201d STOC 2019\n\n[2] Kitaev et al. \"Reformer: The efficient transformer.\" ICLR 2020.\n\n[3] Maxim et al \u201cDeep Learning Recommendation Model for Personalization and Recommendation Systems\u201d Arxiv 2020\n\n[4] Xiangyu et al \u201cMemory-efficient Embedding for Recommendations\u201d Arxiv 2020\n", "title": "continued"}, "S98rDYi9iur": {"type": "rebuttal", "replyto": "AKKQCeBxBTF", "comment": "[1] Gong et al. iDEC: Indexable Distance Estimating Codes for Approximate Nearest Neighbor Search. VLDB 20.\n\n[2] Wang et al. Randomized Algorithms Accelerated over CPU-GPU for Ultra-High Dimensional Similarity Search. SIGMOD 18.\n\n[3] Deng et al. Pyramid: A General Framework for Distributed Similarity Search.\n\n[4] Xiangnan he et al \u201cNeural factorization machines for sparse predictive analytics\u201d SIGIR 2017\n\n[5] Medini et al \u201cExtreme Classification in Log Memory using Count-Min Sketch: A Case Study of Amazon Search with 50M Products\u201d NeurIPS 2019\n\n[6] Vaswani et al \u201cAttention is all you need\u201d NeurIPS 2017\n\n[7] Deng et al \u201cImagenet: A large-scale hierarchical image database\u201d CVPR 2009\n\n[8] Brown et al \u201cLanguage models are few-shot learners\u201d Arxiv 2020\n\n[9] Yiqiu et al \u201cRandomized Algorithms Accelerated over CPU-GPU for Ultra-High Dimensional Similarity Search \u201d SIGMOD 2020\n", "title": "References"}, "AKKQCeBxBTF": {"type": "rebuttal", "replyto": "1R222uVdrJG", "comment": "We appreciate your concise and precise summarization of our work! We have carefully thought through all your great questions and added corresponding experiments and detailed discussions to answer them in the updated paper. We provide details below:\n\n**Q1: Why no serious consideration of graph based ANNS? They are data dependent and SOTA for search efficiency and it is possible to update them. Why is LSH the better choice for ANNS here? This needs a rigorous argument.**\n\n**Response:** Because our key observation is general, theoretically, our scheduler can generalize to any near-neighbor search data structure, as mentioned in Section 3.2. Our work could open up opportunities for more NNS data structures besides LSH. However, we choose an LSH-based data structure in the current framework because other methods (e.g., graph-based) [1, 2, 3] optimize for fast retrieval speed, but we need a data structure that has a low update or rebuild overhead. LSH is known to have an efficient and simple updating and rebuilding process [9]. We include the discussion of data-dependent indexing in Appendix E.2.\n\nTo verify and solidify the above claims, we conduct two additional experiments comparing the updating/rebuilding time of HNSW[1] and our learnable LSH in Section 4.3 (Page 9). Specifically,\n1. We benchmark the update-time of HNSW and learnable LSH. HNSW is up to 20$\\times$ slower at the scale of 300k data points.\n2. We perform an end-to-end evaluation of using learnable LSH and HNSW in MONGOOSE. HNSW is 5$\\times$ slower and 5 point drop in precision@1. \n\nThe experimental details are in \u201cOther data-dependent NNS data structures\u201d, Section 4.3. \n\nIn conclusion, currently, LSH-based data structure has been chosen in MONGOOSE based on the above reasons. But this motivates us for future work to accelerate the other ANNS data structures or even designing new ANNS data structures. Our framework can also include them to accelerate NN training further.   \n\n**Q2: Is this really a general and serious enough problem amongst practitioners that a solution merits publication at a popular conference? It might be, in which case a better quantification of potential impact can help.**\n\n**Response:** Thanks for the great suggestion. We have added a broader impact discussion in Appendix G and extended our conclusion section.\n\nWith the exponentially growing data volume, the scale of NN models keeps increasing to achieve unprecedented performance on tasks in many domains, such as recommendation systems [4,5], natural language processing [6], and computer vision [7]. However, training those large-scale models imposes challenges on the computational and memory resources even with advanced modern hardware. For example, the recent GPT-3 model [8] has a capacity of 175 billion parameters. Therefore, there is an emergent need to reduce the cost of training those giant models. \n\nRecent advances in using LSH to break the computational or memory bottleneck in NN training have achieved notable results. However, LSH has been originally explored in the \u201cstatic\u201d setting where queries are changing, but data is fixed, like approximate near-neighbor search(NNS). Directly applying LSH in the \u201cdynamic\u201d setting where both queries and data are changing has introduced high overhead. Fortunately, in MONGOOSE, we have made a key observation, which provides the opportunity to overcome this overhead in a principal way.\n\nWe now quantify the potential impacts of our work. MONGOOSE\u2019s slow change observation and the smart scheduler demonstrate the possibility of applying ANNS data structures with larger indexing overhead for dynamic similarity search where the distribution of data changes slowly. \n\n1. We have shown applications of MONGOOSE on language modeling and recommendation tasks. But since our key or fundamental observations are general, MONGOOSE could potentially generalize to more deep learning models and tasks.\n2. On the other hand, more ANNS data structures equipped with our observations and scheduler could be involved in the deep learning community to tackle the efficiency issue. This motivates future work of designing new ANNS data structures or improving the update efficiency of existing ones.\n\n\n**Q3: Is wiki-325K really the largest dataset for XC? What about larger language models -- sec 4.1.2 seems to study more medium-sized networks. Larger scale experiments could make this paper more compelling.**\n\n**Response:** We have updated our main result of the language modeling task (Section 4.1.2) with a 10-layer model (rather than the 6-layer one in the original version) to verify the superiority of MONGOOSE on even larger models.  \n\nWe hope the above answers the questions and addresses your concerns. Please let us know what other things we could do to improve our paper.  \n", "title": "Reply to Review by AnonReviewer2"}, "RF1cnQ_Rg8F": {"type": "rebuttal", "replyto": "F-O03j6eqq", "comment": "Thanks for your encouragement and suggestions, which have helped us improve the paper!\n\n**Q: Needs improvements in notations. For example, in assumption 3.1, sum is over what? If the indexing is over i, then does that means over rows of ws (since w \\in R^{n \\times d})? Then should the quantities be L2 norms since they are vectors?**\n\n**Response:** We have updated our paper to address the notation issues. In particular, we add a paragraph summarizing the notations in Section 3.2.1 and add more explanations. For your question, yes, the sum is over rows in assumption 3.1. The assumption is to bound the $L_2$ norm of the weight changes, which corresponds to the observation in Figure 3 above.\n\n**Q: Clarity and the presentation can be improved significantly. For example, \"Rehash\", \"Rebuild\" functions in algorithm 2 needs to be defined or explained. In general, it is better to explain the intuition behind both algorithms 1 and 2. I believe the figure 1 should clarify the LSH update scheduling, but it is not very clear.**\n\n**Response:** We have added Figure 8 along with a description of intuition for Algorithm 1 in Appendix B (Page 16) and Figure 9 for Algorithm 2 in Appendix C1 (Page 25). The operation of updating or training LSH hash functions is defined as REHASH, and the operation of updating the hash tables based on the updated hash functions and weights is defined as REBUILD. We have added it to Section 3.3.1.\n\n**Q: In regards to the observation with figure 3, I am curious what properties of a dataset leads to this kind of behavior? are there any quantifiable properties? Is this true for any dataset? Are there previous works that explain why this is the case?**\n\n**Response:** We have added a corresponding interesting discussion about the phenomenon that a sharp drop of weight changes at the early stages of training in Appendix A (Page 15). \n\nThis phenomenon can be observed in the text (Transformers), recommendation (embedding models), and image(CNN) datasets, so it is a general one empirically. We have not observed specific dataset properties for this behavior. But several works in the literature conjecture it is related to the optimization algorithm (i.e., SGD), based on empirical observations in CNN training dynamics. For example, [1] conjectures that initially, SGD visits increasingly sharp regions, reaching a maximum sharpness determined by both the learning rate and the batch-size of SGD; [1] infers that it optimizes faster while finding a good sharp region. It is also discussed in a concurrent paper [2], which connects the phenomenon to critical learning periods defined in [3]. Thanks for this profound question, and we have added the related works to support further our key observation of slowly changing hash codes.\n\nWe hope the updated draft and the above responses answer your questions. Especially Figure 8 and 9 have made a better presentation of our algorithms. Please let us know if you have more concerns.\n\n[1] Jastrz\u0119bski et al. On the relation between the sharpest directions of DNN loss and the SGD step length. ICLR19.\n\n[2] Agarwal et al. Accordion: Adaptive Gradient Communication via Critical Learning Regime Identification.\n\n[3] Achille et al. Critical Learning Periods in Deep Networks. ICLR19.\n\n", "title": "Reply to Review by AnonReviewer1"}, "Ar-iE15aBR1": {"type": "review", "replyto": "wWK7yXkULyh", "review": "+++Pros.  \n-----The observation that model parameters evolve slowly is quite inspiring for more efficient neural network training.  \n-----The paper proposes MONGOOSE, which is equipped with a scheduler to adaptively perform LSH updates and learnable hash functions to improve query efficiency.  \n-----Experiments demonstrates the effectiveness of the proposed method, and ablation studies give the readers further insights.  \n\n+++Cons.  \n-----The paper is overall good, but with some minors, such as \u201cFigure 5 shows P@1 and P@5 for MONGOOSE and other baselines during the training process.\u201d in Section 4.1.1.  \n-----Besides, there are several mathematical symbols should be explained clearly when they first appeared, such as \u201cw\u201d in definition 2.1, \u201cC1, C2\u201d in assumption 3.1, \u201ct_r\u201d in assumption 3.2.  \n\n+++Conclusion.  \n-----Based on the above analysis, I would prefer to make an \u201cACCEPT\u201d recommendation.  \n-----By the way, I\u2019m curious about why you named your method \u201cMONGOOSE\u201d? Could you give some reasons?  \n\n+++Suggestions.  \n-----Better make the mathematical symbols more clearly for readers.  \n\n\n", "title": "Enabling LSH in a learnable framework to speed up neural network training is quite interesting and critical in deep learning era. Experiments on recommendation and language modeling tasks verify the effectiveness of MONGOOSE. Besides, the related formal analysis also seems solid.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "1R222uVdrJG": {"type": "review", "replyto": "wWK7yXkULyh", "review": "Some neural network runs involve layers with a large number of neurons. These require large matrix-vector or matrix-multiplication which can slow their training/inference. However, if the output of mat-vec/mul is dominated by a few neurons with which the activation has large inner product (a matmul can be thought of as a weighted sum of inner products), then the computation can be sped up by approximating mat-vec/mul by a limited weighted sum with the dominant terms. This requires maintaining an ANNS data structure that is up to data with the back prop. These updates to ANNS have to be done carefully -- too frequent and the training will slow down, or too infrequent and the results of the mat-mul are way off.  This paper studies how to do this in a principled way using data-dependent LSH updates and backs it up with experimental data.\n\nThe ideas and algorithms explored in the papers are as follows:\n1. The weights changed in a limited manner over time, so it should be possible to take advantage of this.\n2. Concentrated changes to a subset of weights can be tracked and patched upon.\n3. An LSH update rule to make these changes effectively\n4. An earlier algorithm that is reused to decide when the LSH scheme is updated.\n\nThe paper also talks about how to identify the layers that benefit the most from this scheme. then it goes on to show the training time benefits of the smart scheduler and the update scheme on extreme classifications tasks as well as transfomers.\n\n\nFew questions:\n1. Why no serious consideration of graph based ANNS? They are data dependent and SOTA for search efficiency and it is possible to update them. Why is LSH the better choice for ANNS here? This needs a rigorous argument. \n2. Is this really a general and serious enough problem amongst practitioners that a solution merits publication at a popular conference? It might be, in which case a better quantification of potential impact can help. \n3.   Is wiki-325K really the largest dataset for XC? What about larger language models -- sec 4.1.2 seems to study more medium sized networks. Larger scale experiments could make this paper more compelling.\n\nI would more strongly recommend this paper if these questions  can be addressed.", "title": "A principled approach to updating LSH based ANNS for faster matrix multiplication in NN training", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}