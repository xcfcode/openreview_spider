{"paper": {"title": "The Natural Language Decathlon: Multitask Learning as Question Answering", "authors": ["Bryan McCann", "Nitish Shirish Keskar", "Caiming Xiong", "Richard Socher"], "authorids": ["bmccann@salesforce.com", "nkeskar@salesforce.com", "cxiong@salesforce.com", "rsocher@salesforce.com"], "summary": "We introduce a multitask learning challenge that spans ten natural language processing tasks and propose a new model that jointly learns them. ", "abstract": "Deep learning has improved performance on many natural language processing (NLP) tasks individually.\nHowever, general NLP models cannot emerge within a paradigm that focuses on the particularities of a single metric, dataset, and task.\nWe introduce the Natural Language Decathlon (decaNLP), a challenge that spans ten tasks:\nquestion answering, machine translation, summarization, natural language inference, sentiment analysis, semantic role labeling, relation extraction, goal-oriented dialogue, semantic parsing, and commonsense pronoun resolution.\nWe cast all tasks as question answering over a context.\nFurthermore, we present a new multitask question answering network (MQAN) that jointly learns all tasks in decaNLP without any task-specific modules or parameters more effectively than sequence-to-sequence and reading comprehension baselines.\nMQAN shows improvements in transfer learning for machine translation and named entity recognition, domain adaptation for sentiment analysis and natural language inference, and zero-shot capabilities for text classification.\nWe demonstrate that the MQAN's multi-pointer-generator decoder is key to this success and that performance further improves with an anti-curriculum training strategy.\nThough designed for decaNLP, MQAN also achieves state of the art results on the WikiSQL semantic parsing task in the single-task setting. \nWe also release code for procuring and processing data, training and evaluating models, and reproducing all experiments for decaNLP.", "keywords": ["multitask learning", "natural language processing", "question answering", "machine translation", "relation extraction", "semantic parsing", "commensense reasoning", "summarization", "entailment", "sentiment", "dialog"]}, "meta": {"decision": "Reject", "comment": "This paper presents a new multi-task training and evaluation set up called the Natural Language Decathlon, and evaluates models on it. While this AC is sympathetic to any work which introduces new datasets and evaluation tasks, the reviewers agreed amongst themselves that the paper is not quite ready for publication. The main concern is that multi-task learning should show benefits of transferring representations or other model components between tasks, demonstrating better generalisation and less task-specific overfitting, but that the results in the paper do not properly show this effect. A more thorough study of which tasks \"interact constructively\" and what model changes can properly exploit this needs to be done. With this further work, the AC has no doubt that this dataset and task suite, and associated models, will be very valuable to the NLP community.\n\nI should note that there were some issues during the review period which lead to AC-confidential communication between AC and authors, and AC and reviewers, to be leaked to the reviewers. It was due to an OpenReview bug, and no party is at fault. Through private discussion with the interested parties, we were able to resolve this matter, and through careful examination of the discussion, I am satisfied that the reviews and final recommendations of the reviewers were properly argued for and presented in good faith."}, "review": {"SygF6GHkkN": {"type": "rebuttal", "replyto": "HyeuPA-JkN", "comment": "[I'm following the AC lead with this post and setting Readers: Everyone, so like the AC I believe this response will only be seen by the authors, AC and above.]\n\nThank you for the notes. I don't think this should be escalated.\n\nI think the review does make some substantial arguments, so I'm perfectly content with the reviewer's explanations. It really demonstrates that there are two readings of this paper. We wrote it to show how QA can expand possibilities for multitask learning in NLP, but the other reading sees it as a paper trying to show multitask learning helps individual tasks. Depending on which reading you go in with, the results either look positive or negative, respectively. We'll have to keep this in mind for future work and see if we can address the second reading more effectively.\n\nCan you help me understand this system and its visibility settings? Why is it that R2 got an email with the text of my Red Flag post even though it has \"Readers:  ICLR 2019 Conference Paper1522 Area Chairs, Program Chairs\"? And why do they not get notified of the AC post with \"Readers: Everyone\"?\n\nI think this actually happened earlier as well. I got an email before reviews were released with this body:\n\n\"\"\"\nYour submission to ICLR 2019 has received a comment.\n\nComment title: Some comments\n\nComment: Thank you for your review, Reviewer 2. As AC, I should clarify two things.\n\n1. References to authors by first names are vague. It would be helpful for Reviewer 2 to clarify who they mean by \"Luheng and Omer\" (one would assume Luheng He and Omer Levy) and which paper they are referring to.\n\n2. Reserving judgement on whether or not the reviewer is right in suggesting that the paper tries to fit too much content in its main body, it is abiding by the formatting rules of the conference. Readers and reviewers are not required to read the supplementary materials, and thus the paper must stand on its own without them when you evaluate it. However, the length of the appendix is not grounds for desk rejecting it, and it should go through the full review process.\n\"\"\"\n\nThat doesn't seem like it was supposed to be sent to us, so I don't know what's going on with the email system. I'm disappointed that R2 got such an email, knew it was meant to be a private post from the authors, but posted it publicly anyways instead of following up privately. That felt like a betrayal, but I assume they had some positive intent that I can't see. There shouldn't be any more escalation here, and I apologize if I contributed to a negative or adversarial review environment even if unintentionally. I appreciate that you weighed in publicly, and I'll follow up in an attempt to convey my genuine respect for R2's constructive feedback while also responding to the requests in R2's better attempt at point 1 (which I found helpful).\n\nThanks again.", "title": "No need to escalate"}, "HJeuil0sRX": {"type": "rebuttal", "replyto": "BkekRkTsR7", "comment": "We\u2019ll keep working on the gaps and make sure to provide additional analysis of task relatedness in future work. ", "title": "Thanks for the feedback!"}, "Syx1siQK37": {"type": "review", "replyto": "B1lfHhR9tm", "review": "Update: I've updated my score based on the clarifications from the authors to some of my questions/concerns about the experimental set-up and multi-task/single-task differences.\n\nOriginal Review:\nThis paper provides a new framework for multitask learning in nlp by taking advantage of the similarities in 10 common NLP tasks. The modeling is building on pre-existing qa models but has some original aspects that were augmented to accommodate the various tasks.  The decaNLP framework could be a useful benchmark for other nlp researchers.  \n\nExperiments indicate that the multi-task set-up does worse on average than the single-task set-up.  I wish there was more analysis on why multi-task setups are helpful in some tasks and not others.  With a bit more fine-grained analysis, the experiments and framework in this paper could be very beneficial towards other researchers who want to experiment with multi-task learning or who want to use the decaNLP framework as a benchmark.\n\nI also found the adaptation to new tasks and zero-shot experiments very interesting but the set-up was not described very concretely: \n  -in the transfer learning section, I hope the writers will elaborate on whether the performance gain is coming from the model being pretrained on a multi-task objective or if there would still be performance gain by pretraining a model on only one of those tasks.  For example, would a model pre-trained solely on IWSLT see the same performance gain when transferred to English->Czech as in Figure 4? Or is it actually the multi-task training that is causing the improvement in transfer learning? \n  -Can you please add more detail about the setup for replacing +/- with happy/angry or supportive/unsupportive? What were the (empirical) results of that experiment?\n\nI think the paper doesn\u2019t quite stand on its own without the appendix, which is a major weakness in terms of clarity.  The related work, for example, should really be included in the main body of the paper.  I also recommend that more of the original insights (such as the experimentation with curriculum learning) should be included in the body of the text to count towards original contributions.  \n\nAs a suggestion, the authors may be able to condense the discussion of the 10 tasks in order to make more room in the main text for a related work section plus more of their motivations and experimental results.  If necessary, the main paper *can* exceed 8 pages and still fit ICLR guidelines.\n\nVery minor detail: I noticed some inconsistency in the bibliography regarding full names vs. first initials only.", "title": "New framework has a lot of potential, but the experiments, motivations, and related work are missing details", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJgYq03KCQ": {"type": "rebuttal", "replyto": "HJeLFjOtA7", "comment": "Yes, I understand that your intent was probably not rudeness. I didn't think it was my place to publicly comment on your writing compared to say, R1, who makes nearly all the same criticisms and gives an equally low score without using terms that are condescending like 'misguided'. I did not post this publicly because I am both an author and a reviewer, and I understand that I am biased towards reading this review  as more negative than it should be read. That is why I posted this to ACs and Higher so that they could evaluate. For some reason, the system must have some unintuitive behavior (too me at least) that sends you an email for comments on your posts regardless of the chosen visibility. Not sure what happened since the original post is still visible to me and was not deleted. Now that you've posted it to Everyone, I might as well clarify.\n\nAs a reviewer, my criticism of this review has nothing to do with QA or the paper itself. Title and 1) seem to be written too combatively (perhaps to use this platform to balance out \"very prominent, public voice[s] advocating for it\"?). I don't think this is the place for that. On my view, authors submit for review to get valuable criticism. The reviewer's ultimate goal should be to tell authors how to improve their research; it should not be to combat the research agenda. The paper has problems, especially in total content and organization. As mentioned in the post to ACs and Higher, you raise important criticisms in points 2) and 3). But, I think Title and 1) deviated from what I see as a reviewer's goal too much. I just think you could have done without 1), and you should also avoid using words like 'misguided' unless you intend to up the rudeness factor by a few notches. We might just have to agree to disagree here about tone and word choice, maybe even about the goal of reviewing. \n\nNow switching back into author mode.  \n\n2) You're right that the transfer learning experiments for any one task are not new results. What we find interesting here is that the multi-task model retains transferability to all of the tasks it has been trained on. In this sense, these experiments verify that the representations of the multi-task model are somehow compressing the transferable utility of ten single-task models into a single model (10x smaller).\n\nRegarding your point about the gap between single- and multi-task performance, I'll point you to our response to R3 so that you don't have to do redundant reading. \n\nRegarding switching classification labels. Yes, this is a rough approximation for something we were trying to study -- whether the model could adapt to new, but related kinds of questions and adapt its output space. Certainly this experimental design has some problems, but we do think it demonstrates the more general capacity of the model to switch output spaces based on the question because the model must realize that even though the context is the same, it must use different output labels based on different questions.\n\n3) No objections here. Organizing all this information into a reasonable order is tough, and clearly one big take-away from this reviewing process is to break things down into more conference-sized chunks rather than cram everything into appendices. Definitely don't put related works in an appendix -- it is disrespectful even if the intentions were good (more space to expand on it all).\n\nFinal paragraph) A lot of additional valuable feedback here. This gives a good sense of how we might restructure and support claims with new experiments. Very much appreciated.\n\nOverall, thanks for the discussion. Even though I disagreed with your reviewing style for 1), I think you make really good points in the remainder of your review. Thanks for offering so much of your time.", "title": "Clarifications and thanks for your helpful feedback"}, "BkgxS6EKAm": {"type": "rebuttal", "replyto": "HJe-iuo5qm", "comment": "Thanks for taking the time to suggest how we could prioritize all of this information more effectively. \n\nYou're right that even though we cite the work you mention (Collobert and Weston 2008) along with the follow up (Collobert et al. 2011) in our original submission, we only do so in the related works, which are currently placed in the appendices. \n\nI assure you that we meant no disrespect to these related works by placing them in an appendix. Our thinking at the time was that we could only do justice to the significant literature in both multi-task learning and single-task learning for all these tasks by moving such discussions to sections that had no page limit. \n\nWe have a lot of information in the appendices that we view as just as important as the information in the main body. We just didn't have the same interpretation of appendices (as lesser material) going into this submission. We simply ordered on what we thought would need to be read first in order to understand the benchmark and the progress so far. For example, the details on anti-curriculum pre-training are actually quite important to us as a contribution, but they didn't seem as essential as understanding the nature of all the tasks. Our motivations for the tasks, the related works, and the task-specific related works are all important. The fact that they are in appendices is only because of the total amount of information in the submission.\n\nThat being said, feedback from multiple sources has indicated that at least some of these related works need to be in the main body, and we will reorganize the paper accordingly.", "title": "Related work"}, "rJglodNF0Q": {"type": "rebuttal", "replyto": "S1gNRnjt5m", "comment": "Exciting! Glad you're finding decaNLP to be a good resource for further research!\n\nIn our experiments, the generative vocabulary in Eq. 11 contains the most frequent 50000 words in the\ncombined training sets for all tasks in decaNLP. A lot of these training details are way down in Appendix D on Preprocessing and Training Details. They aren't necessarily optimal if you have a bigger memory budget than we did or have a more clever motivation for how these kinds of decsision should be based on individual tasks. ", "title": "50k most common words across all tasks in decaNLP"}, "Bye7ZuVKCX": {"type": "rebuttal", "replyto": "Syx1siQK37", "comment": "Regarding your point about the gap between single- and multi-task performance, I'll point you to our response to R3 so that you don't have redundant reading. \n\nRegarding the transfer learning experiments. The performance gain does not come from the multi-task objective, as single-task models would exhibit similar behavior for their respective task. What we find interesting here is that the multi-task model retains transferability to all of the tasks it has been trained on. In this sense, these experiments verify that the representations of the multi-task model are somehow compressing the transferable utility of ten single task models into a single model (10x smaller!).\n\nFor the label replacement on the SST dataset, the empirical results show a minor degradation in performance (~1%, so ~86 vs ~87 according to Table 2 and subsection 4.3). This was a naive replacement mapping all answers that were 'positive' to 'happy' and all answers that were 'negative' to 'angry'. This shows how the model is learning to capitalize on the common output space (all of English in GloVe) to adapt to new labels without any additional training. This is advantageous over models that do not actually generate answer sequences because it allows them to be more robust in intuitive ways.\n\nYou're certainly right that the appendix carries a lot of useful information and some of the details about contributions. We had moved the related works to the appendix because that was the only way we found we could sufficiently do justice to the long line of literature in multi-task learning as well as all of the literature for each task, but it does seem we will need to include at least a part of our full related works in the main body. There is quite a bit of material overall, and we thank you for your suggestions about where to cut/condense and how to prioritize information.\n\nThank you again for your questions and your feedback about organization.", "title": "Response to R1: Thanks for your review, and some clarificaitons"}, "r1xjw4VK0Q": {"type": "rebuttal", "replyto": "B1xIPcoqh7", "comment": "First of all, thank you for your review. You touch upon a crucial point that does require clarification: the gap between the single- and multi-task performance.\n\nAs you mentioned, the multi-task learning literature has taught us at least one thing: related tasks tend to help each other, and unrelated tasks tend to interfere with each other. The latter is an interesting phenomenon, and it is what we see as the primary multi-task learning problem of concern in this paper, and we are proposing decaNLP as a benchmark for measuring progress on this problem.\n\nThere are two ways in which unrelated tasks tend to interfere. The first is during the modeling phase where some tasks prevent us from using priors (like span prediction for QA or a German-only output vocabulary) that would be useful for some tasks. The second is during the training phase where some tasks tend to interfere with representation learning.\n\nThese two kinds of interference lead to two kinds of gaps that we measure with this benchmark. The first is the gap between the current best decaNLP model (in the single- and multi-task settings) and a combination of state-of-the-art models for each task. The second is the gap between the best decaNLP model in the multi-task setting and a combination of ten of those best decaNLP models each trained for a single task. \n\nThe concrete contributions of this paper are 1) the preparation of benchmark along with reasonable sequence-to-sequence baselines, 2) progress on the first kind of gap by switching from seq2seq to multi-sequence-to-sequence with MQAN (by transforming problems into QA triplets), and 3) progress on the second kind of gap by demonstrating the superiority of anti-curriculum learning (or pre-training on harder tasks) over the baseline fully joint training. 3) actually ties multi-task learning back to transfer learning as an effective means of representation learning.\n\nBut yes, we have not yet entirely closed these gaps; as you mentioned, that is a key part of the challenge to the community. We have chosen to introduce this challenge now because we believe solutions to this problems are within reach in the near future if the community focuses on them.\n\nAnd yes, though this approach will likely be successful whenever tasks are related (just based on what we know from the rest of the multi-task learning literature), it is sometimes not yet the best way to optimize for single-task performance. Keep in mind though that it did lead to new state-of-the-art results on WikiSQL despite no direct modeling or tuning for that task. \n\nThanks again for your time. ", "title": "Respone to R3: Thank you for your review; more on single- vs. multi-task performance"}, "B1xIPcoqh7": {"type": "review", "replyto": "B1lfHhR9tm", "review": "The paper formulates several different NLP problems as Q&A problem and proposed a general deep learning architecture. All these tasks are trained together. \n\nIf the goal is to achieve general AI, the paper gives a good starting point. One technical novelty is the deep learning architecture for this general Q&A problem including the multi-pointer-generator. The paper presents an example of how to do a multi-task learning for 10 different tasks. It raises a very challenging problem or in some way release a new dataset.\n\nIf our goal is to optimize a single task, the usefulness of the method proposed by the paper is questionable. \nAs we know, multi-task learning works well if some important knowledge shared by different tasks can be learned and leveraged. From table 2, we see for many problems, the results of the single task training are better than the multi-task training, meaning that other tasks can't really help at least under this framework. This makes me doubt if this multi-task learning is useful if our goal is to optimize the performance of a single task. This general model also sacrifices some important prior knowledge of an individual task. For example, for the Squad, the prior that the answer is a continuous span. Ideally, the prior knowledge should be leveraged.\n\n", "title": "A good example to treat different NLP problems as Q&A and trained together. Results for some problems are worse than their state-of-the-art.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1lGsQAShm": {"type": "review", "replyto": "B1lfHhR9tm", "review": "I appreciate the work that went into creating this paper, but I'm afraid I see little justification for accepting it.  I have three major complaints with this paper:                                                                         \n                                                                                                     \n1. I think the framing of decaNLP presented in this paper does more harm than good, because it perpetuates a misguided view of question answering.\n                                                                                                     \nQuestion answering is not a unified phenomenon.  There is no such thing as \"general question answering\", not even for humans.  Consider \"What is 2 + 3?\", \"What's the terminal velocity of a rain drop?\", and \"What is the meaning of life?\"  All of these questions require very different systems to answer, and trying to pretend they are the same doesn't help anyone solve any problems.\n                                                                                                     \nQuestion answering is a _format_ for studying particular phenomena.  Sometimes it is useful to pose a task as QA, and sometimes it is not.  QA is not a useful format for studying problems when you only have a single question (like \"what is the sentiment?\" or \"what is the translation?\"), and there is no hope of transfer from a related task.  Posing translation or classification as QA serves no useful purpose and gives people the wrong impression about question answering as a format for studying problems.\n\nWe have plenty of work that studies multiple datasets at a time (including in the context of semi-supervised / transfer learning), without doing this misguided framing of all of them as QA (see, e.g., the ELMo and BERT papers, which evaluated on many separate tasks).  I don't see any compelling justification for setting things up this way.\n                                                                                                     \n2. One of the main claims of this paper is transfer from one task to another by posing them all as question answering.  There is nothing new in the transfer results that were presented here, however.  For QA-SRL / QA-ZRE, transfer from SQuAD / other QA tasks has already been shown by Luheng He (http://aclweb.org/anthology/N18-2089) and Omer Levy (that was the whole point of the QA-ZRE paper), so this is merely reproducing that result (without mentioning that they did it first).  For all other tasks, performance drops when you try to train all tasks together, sometimes significantly (as in translation, unsurprisingly).  For the Czech task, fine tuning a pre-trained model has already been shown to help.  Transfer from MNLI to SNLI is known already and not surprising - one of the main points of MNLI was domain transfer, so obviously this has been studied before.  The claims about transfer to new classification tasks are misleading, as you really have the _same_ classification task, you've just arbitrarily changed how you're encoding the class label.  It _might_ be the case that you still get transfer if you actually switch to a related classification task, but you haven't examined that case.\n                                                                                                     \n3. This paper tries to put three separate ideas into a single conference paper, and all three ideas suffer as a result, because there is not enough space to do any of them justice.  Giving 15 pages of appendix for an 8 page paper, where some of the main content of the paper is pushed to the appendix, is egregious.  Putting your work in the context of related work is not something that should be pushed into an appendix, and we should not encourage this behavior.\n                                                                                                     \nThe three ideas here seem to me to be (1) decaNLP, (2) the model architecture of MQAN, (3) transfer results.  Any of these three could have been a single conference paper, had it been done well.  As it stands, decaNLP isn't described or motivated well enough, and there isn't any space left in the paper to address my severe criticisms of it in my first point.  Perhaps if you had dedicated the paper to decaNLP, you could have given arguments that the framing is worthwhile, and described the tasks and their setup as QA sufficiently (as it is, I don't see any description anywhere of how the context is constructed for WikiSQL; did I miss it somewhere?).  For MQAN, there's more than a page of the core new architecture that's pushed into the appendix.  And for the transfer results, there is very little comparison to other transfer methods (e.g., ELMo, CoVe), or any deep analysis of what's going on - as I mentioned above, basically all of the results presented are just confirming what has already been done elsewhere.", "title": "Misguided and overcrowded", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}