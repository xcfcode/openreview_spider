{"paper": {"title": "On the Variance of the Adaptive Learning Rate and Beyond", "authors": ["Liyuan Liu", "Haoming Jiang", "Pengcheng He", "Weizhu Chen", "Xiaodong Liu", "Jianfeng Gao", "Jiawei Han"], "authorids": ["ll2@illinois.edu", "jianghm@gatech.edu", "penhe@microsoft.com", "wzchen@microsoft.com", "xiaodl@microsoft.com", "jfgao@microsoft.com", "hanj@illinois.edu"], "summary": "If warmup is the answer, what is the question?", "abstract": "The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate -- its variance is problematically large in the early stage, and presume warmup works as a variance reduction technique. We provide both empirical and theoretical evidence to verify our hypothesis. We further propose Rectified Adam (RAdam), a novel variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the efficacy and robustness of RAdam. ", "keywords": ["warmup", "adam", "adaptive learning rate", "variance"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper considers an important topic of the warmup in deep learning, and investigates the problem of the adaptive learning rate. While the paper is somewhat borderline, the reviewers agree that it might be useful to present it to the  ICLR community."}, "review": {"HkxT4aeSor": {"type": "rebuttal", "replyto": "Hye4w1Tyjr", "comment": "> **Math Derivations**\n\nOne purpose of our study is to rigorously explore the underpinning of warmup \u2014 we believe that only functioning as the outcome of math derivations, our method can explicitly handle the variance issue. However, the comment criticizes math derivations as insufficient to justify our algorithm design, referring to them as \u201cmagic masquerading\u201d and \u201csmoke and mirrors\u201d.\n\nWe respectfully disagree with this statement, as math is an important and powerful tool to formulate and verify our understanding. We believe in order to perform a rigorous analysis, math derivations are necessary and helpful.\n\nIn our study, we conduct theoretical analysis besides controlled experiments and find that math agrees with our hypothesis (it is the large variance caused the training instability). Specifically, our theory shows that, at the beginning of training, the variance of the adaptive learning rate can be undesirably large, verifying the existence of the variance issue. Additionally, this inspires us to introduce a mathematically sound rectification term to handle the variance. \n\n> **Downgrading to SGDM**\n\nA large portion of the comment is about the issue of the RAdam downgrading (i.e.,\"4 timesteps of momentum SGD\"), a byproduct determined by math derivations. Although this is not the main result of our paper, we'd like to clarify why it is designed this way below.\n\nHere, we find the troublesome large variance of the adaptive learning rate can cause training instability (thus we focus on the magnitude instead of the divergence). From our theoretical analysis of the adaptive learning rate variance, we derive the rectification term to handle this issue. However, we are constrained from applying such a rectification term at the very beginning of training, since the variance of the estimated adaptive learning rate is not well-defined (in other words, divergent). Therefore, we downgrade the algorithm to SGDM in this stage. \n\nAlthough this stage only contains several gradient updates, these updates could be quite damaging (e.g., in our Figure 2, the gradient distribution is distorted within 10 gradient updates). Intuitively, updates with divergent adaptive learning rate variance could be more damaging than the ones with converged variance, as divergent variance implies more instability. As a case study, we performed experiments on the CIFAR10 dataset. Five-run average results are summarized below. The optimizer fails to get an equally reliably model when changing the first 4 updates to Adam, yet the influence of switching is less deleterious when we change 5-8 updates instead. This result verifies our intuition and is in agreement with our theory \u2014 the first few updates could be more damaging than later updates. By saying that, we still want to emphasize that this part (downgrading to SGDM) is only a minor part of our algorithm design whereas our main focus is on the mechanism of warmup and the derivation of the rectification term.\n\n+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n|                                                                            Performance on CIFAR10 (lr = 0.1)                                                              |\n+-----------------------------------------------------------------------------------------------------+-------------+---------------+------------------+\n|                                                                                                                                    |   test acc |  train loss |   train error |\n+-----------------------------------------------------------------------------------------------------+-------------+---------------+------------------+\n| All steps: RAdam                                                                                                     |       91.08 |         0.021 |             0.74  |\n+-----------------------------------------------------------------------------------------------------+-------------+---------------+------------------+\n| 1-4 steps (divergent variance): Adam; 5+ steps: RAdam                                 |      89.98 |          0.060 |             2.12  |\n+-----------------------------------------------------------------------------------------------------+-------------+---------------+------------------+\n| 1-4 steps: SGD; 5-8 steps (convergent variance): Adam; 8+ steps: Radam  |      90.29 |         0.038 |              1.34  |\n+-----------------------------------------------------------------------------------------------------+-------------+---------------+------------------+\n", "title": "Official Response (2/2)"}, "Hyl4O6eHiH": {"type": "rebuttal", "replyto": "Hye4w1Tyjr", "comment": "The comment summarizes our work as \u201cincremental and poorly-justified\u201d, mainly based on the similarity between RAdam and two warmup schedulers; however, we find that such similarity ends up supporting our intuition \u2014 the needs of warmup comes from the problematic adaptive learning rate, since these two schedulers are entirely controlled by $\\beta_2$ (the only hyper-parameter for calculating the adaptive learning rate). \n\nBefore we address the comment in detail, we want to point out that the main focus of our study is on exploring the underlying mechanism of warmup. Our analysis reveals that the training instability (at least in our NMT case) is mainly caused by adaptive learning rate, and warmup is needed to handle the large variance of the adaptive learning rate at the early stage of training. Inspired by the analysis, we propose a new variant of Adam by introducing a rectification term to explicitly control the variance of the adaptive learning rate.\n\n> **Comparing with Heuristic LR Scheduler**\n\nIn their manuscript, it says \u201cRAdam and the untuned rule-of-thumb warmup schedules are more or less interchangeable\u201d, thus \u201cRAdam: perform 4 iterations of momentum SGD, then use Adam with fixed warmup\u201d.\n\nHowever, since the fixed warmup schedulers (in the above argument) are controlled solely by the choice of $\\beta_2$, only the adaptive learning rate is taken into consideration. Therefore, their designs are based on an intuition similar to ours \u2014 in the first stage of training, due to the use of insufficient samples for estimating the adaptive learning rate, a warmup stage is needed (and should be customized via the choice of \\beta_2). Also, as shown in Figure 3 of the shared manuscript, the compared lr scheduler is very similar to our current design; it is not a surprise to find that their performances are also similar. \n\nTo put it differently, we think that this phenomenon supports our intuition on the relationship between training instability and the lack of enough samples for adaptive learning rate estimation. \n\n> **Adaptive Learning Rate Variance**\n\nIn the comment, it says \u201cThe manuscript does not show a causal relationship between the variance of the adaptive learning rate and training instability.\u201d \n\nVariance is a measure of variability, i.e., lack of consistency or fixed pattern. Therefore, we pick variance as the measure to study stability [1]. Intuitively, if the adaptive learning rate is not stable, it will have a large variance.\n\nTo seek the origin of training instability, we designed two controlled experiments (Adam-2k and Adam-eps; see Sec 3.1). Specifically, Adam-2k narrows down the problem to the adaptive learning rate, and Adam-eps shows the convergent problem can be avoided by stabilizing the adaptive learning rate (large eps can be viewed as a small-gradient filter). Accordingly, these experimental results suggest that the adaptive learning rate\u2019s instability (large variance) is a major, if not the only, contributor to training instability. \n\n[1] Beard R.E., Pentik\u00e4inen T., Pesonen E. (1984) Variance as a measure of stability. In: Risk Theory. Monographs on Statistics and Applied Probability, vol 20. Springer, Dordrecht\n", "title": "Official Response (1/2)"}, "Skl7-oeHiB": {"type": "rebuttal", "replyto": "HJxzMg4AYH", "comment": "Thank you for your review and feedback. \n\nAlthough the warmup technology has been demonstrated to be useful in several applications and domains, it is not regarded as a common practice, partially because it is unclear why we need such technologies. In this study, we aim to uncover its underpinnings and identify an important yet long-overlooked issue: the adaptive learning rate has a problematically large variance in the early stage of training, due to the lack of enough samples. Based on our analysis, we further propose a rectification term to address this issue. In our experiments, we show that it works well on various tasks/domains. \n", "title": "Re: Official Blind Review #3"}, "SyenwjxriB": {"type": "rebuttal", "replyto": "SyeHRMd1cS", "comment": "Thank you for your review and feedback. We understand your concern and hope that our response will alleviate it. More specifically,\n\n> On the comparison between SGDM and adaptive optimization algorithms\n\nIt is true that for datasets like ImageNet, the state of the art resnet performance is usually achieved by SGD with momentum (SGDM). In our case, we observe a similar phenomenon, and we suggest it because the hyper-parameters are tuned for the SGDM and may be sub-optimal for other algorithms.\n\nComparing to SGDM, the adaptive optimization algorithms usually converge faster and are more robust to the choice of hyper-parameters, thus have been viewed as the default choice in many applications [1,2]. Based on our experience, it requires non-trivial efforts to make SGDM achieve similar performance for cases like training Transformers. \n\n1. Reimers, Nils, and Iryna Gurevych. \"Optimal hyperparameters for deep lstm-networks for sequence labeling tasks.\" arXiv preprint arXiv:1707.06799 (2017).\n2. Popel, Martin, and Ond\u0159ej Bojar. \"Training tips for the transformer model.\" The Prague Bulletin of Mathematical Linguistics 110.1 (2018): 43-70.\n", "title": "Re: Official Blind Review #1"}, "BJeTkngrsH": {"type": "rebuttal", "replyto": "Hkxsh6bZcS", "comment": "Thank you for your review and feedback. We understand your concern and hope that our response will alleviate them. More specifically,\n\n> On theoretic analysis of the impact of the large variance\n\nIn our study, we focus on exploring the underlying mechanism of the warmup technology, and find that it is non-trivial to theoretically identify the existence of the variance issue. Due to the complicated nature of neural networks, we believe that it would be even more challenging to establish a general and direct connection between large variance and the neural network behavior \u2014 in fact, a theoretical analysis of the neural network behavior by itself is a big challenge. Therefore, we believe that these questions deserve a more in-depth analysis, and we would like to leave it to future work. \nHere, we want to borrow some insights from recently proposed theories to intuitively illustrate why large variance of the adaptive learning rate is harmful in a simplified case. It is worth mentioning that these results are based on simple model structures (e.g. two-layer CNN/ResNet) and strong data distribution assumptions [1, 2, 3]. \n\nIt has been shown that there are bad local optima when optimizing neural networks [2, 3]; and it requires the learning rate of the gradient descent algorithm (not SGD) to be controlled within a range, in order to avoid being trapped in the regions of bad local optima and converging to the global optimum [2, 3]. In other words, the learning rate cannot be too large and has to be set within a range. Such condition might be compromised by the large variance of the adaptive learning rate \u2014 as variance is a measure of variability (i.e., lack of consistency or fixed pattern), the adaptive learning rate with a larger variance is less likely to be held within the desired range.\n\n1. Ge, Rong, et al. \"Learning two-layer neural networks with symmetric inputs.\" International Conference on Learning Representations (ICLR), 2019.\n2. Du, Simon S., et al. \"Gradient descent learns one-hidden-layer cnn: Don't be afraid of spurious local minima.\" International Conference on Machine Learning (ICML) 2019.\n3. Liu, Tianyi, et al. \"Towards Understanding the Importance of Shortcut Connections in Residual Networks.\"Annual Conference on Neural Information Processing Systems (NeurIPS), 2019.\n\n> On the accuracy of SGDM and adaptive optimization algorithms\n\nIt is true that for datasets like ImageNet, the state of the art resnet performance is usually achieved by SGD with momentum (SGDM). In our case, we observe a similar phenomenon, and we suggest it because the hyper-parameters are tuned for SGDM and may be sub-optimal for other algorithms. At the same time, our proposed RAdam algorithm shows more robustness towards learning rate changes. It is also worth mentioning that, although RAdam fails to outperform SGD in terms of test accuracy, it results in faster convergence, lower training loss and better training performance (e.g., the training accuracy of SGD, Adam, and RAdam on ImageNet are 69.57, 69.12 and 70.30 respectively).", "title": "Re: Official Blind Review #2"}, "HJxzMg4AYH": {"type": "review", "replyto": "rkgz2aEKDr", "review": "Authors propose a way to rectify the variance of the adaptive learning rate (RAdam) and apply the optimizer to applications in image classification, language modeling and neural machine translation. The experiments demonstrate not only a strong results over baseline Adam with warmup learning rate but the robustness of the optimizer. The authors additionally demonstrate the theoretical justification behind their optimizer, however I am not very qualified to make the judgement on the theory. Overall judging from the authors description of approach and experimental results, I recommend acceptance.\n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 4}, "SyeHRMd1cS": {"type": "review", "replyto": "rkgz2aEKDr", "review": "I haven't worked in this area before, and my knowledge to the topic of designing optimizers for improving stochastic gradient descent is limited to the level of advanced ML courses at graduate school. Nevertheless, below I try my best to evaluate the technicality of this paper\n\n====================\nIn this work the authors studied the variance issue of the adaptive learning rate and aim to justify the warm-start heuristic. They also demonstrate that  the convergence issue of many of the stochastic gradient descent algorithms is due to large variance induced by the adaptive learning rate in the early stage of training. To tackle this issue, they proposed a variant of ADAM, which is known as rectified ADAM, whose learning rate not only takes the momentum into the account, but it also adapts to the variance of the previous gradient updates.  \n\nIn the first part of the paper, the authors analyzed the variance issue exists in the existing ADAM algorithm, such that with limited samples in the early stage of training, the variance of the adaptive learning rate becomes rather large and it induces high variance to the gradient update to ADAM. In general I found this theoretical justification on the observation of variance issue in ADAM sound, and quite intuitive. In the second part, they proposed the algorithm, namely rectified ADAM, where the difference here to take the second moment  of the gradient into account when updating the adaptive learning rate. They showed that the variance of the adaptive learning rate with such rectification is more numerically stable (especially when variance is intractable in vanilla ADAM), and under some regularity assumption it decreases in the order or O(1/\\rho_t).\n\nIn extensive numerical studies of supervised learning, the authors showed that RADAM achieves a better accuracy than ADAM (although in Table 1, I am a bit puzzled why the best accuracy is indeed from SGD, if so, what's the point of all adaptive learning rates, is that because SGD requires extensive lr tuning?) Because the superiority in accuracy they also showed that RADAM manages to have more stable training and achieves lower training loss, which is quite interesting. \n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 1}, "Hkxsh6bZcS": {"type": "review", "replyto": "rkgz2aEKDr", "review": "In this work, authors show that the bad performance of Adam is from the large variance of adaptive learning rate at the beginning of the training.\nPros:\n1.\tAuthors demonstrate that the variance of the first few stages is large, which may interpret the degradation in the performance of Adam.\n2.\tThe empirical study supports the claim about the large variance.\n\nCons:\n1.\tTheoretically, authors didn\u2019t illustrate why the large variance can result in the bad performance in terms of, e.g., convergence rate, generalization error, etc.\n2.\tThe performance of the proposed algorithm is still worse than SGD and it makes the analysis less attractive.\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 2}, "HJgZiW0ddH": {"type": "rebuttal", "replyto": "S1ew6gRO_B", "comment": "Yes, your understanding is correct. ", "title": "Re: Adam-2k"}, "BkgVzLSdOS": {"type": "rebuttal", "replyto": "HJla5-ydOH", "comment": "Thanks for asking. I did an experiment and it shows: treating the first 2001 batches as one large batch suffers from the same convergence problem with the vanilla-Adam (in our NMT experiment as in Figure 1). It further verifies our hypothesis that the adaptive learning rate has undesirably large variance and blocks the algorithm from getting a reasonable performance. \n\nEmpirically, improper learning rate setting can lead to convergence problems. Still, to explain the underlying mechanism of warmup, it requires a detailed analysis on the difference between the initial phrase and the later phrase (e.g., why it requires a smaller learning rate in the beginning; or with the same update rule and learning rate, why the update is more problematic in the beginning).\n\nBased on our experiments, after getting enough samples to estimate the adaptive learning rate (instead of the gradient direction), we can avoid the convergence problem as in Figure 1.\n\nWe will explore the problem why warmup helps SGD in the future, and thanks for pointing out one potential explanation. ", "title": "Re: Why does warmup help"}, "S1ljoi3Dur": {"type": "rebuttal", "replyto": "rkgz2aEKDr", "comment": "We've found several typos (i.e., in Algorithm 1, line 5, Equation 4 and Equation 7). Sorry for the mistake and the corrected version can be found in: https://drive.google.com/open?id=1UQbRm66IMPP8_HycUIP-txV2vNx9SneT", "title": "Corrections of Typos"}, "SJlhBzow_H": {"type": "rebuttal", "replyto": "B1l02LcDuB", "comment": "Thanks for asking and pointing out the typo, we will fix it in the next version. : -)\n\n1. Our study is motivated by the phenomenon that: Adam-without-warmup fails for NMT, even with small batch size. As in Theorem 1, the adaptive learning rate has a larger variance in the early stage of training than the later stage. Also, by providing two thousand additional batches to estimate the learning rate, we can avoid the convergence problem met by the vanilla-Adam. \n\n2. We agree the warmup is originally designed for large-minibatch-SGD [0], based on the intuition that the network changes rapidly in the early stage. However, we find that this intuition does not explain why Adam requires warmup. Notice that, Adam-2k can also avoid the convergence problem: it uses the same learning rate and initialization but has a better estimation of the adaptive learning rate (more details below). Therefore, we suggest that although warmup helps both Adam and SGD, they are for different reasons. \n\nIn the first two thousand batches, Adam-2k will only update the moving average of the second moment, and freeze both the first moment and the parameter value; after these two thousand batches, Adam-2k will start to update parameters in the vanilla-Adam way. That is to say, Adam-2k (avoids convergence problem) and Adam (suffers from convergence problem) has the same initialization, learning rate scheduler and update rule; their only difference is Adam-2k has additional samples to estimate the adaptive learning rate. Accordingly, we think the root cause of the convergence issue is the lack of sufficient data samples in the early stage (to estimate the adaptive learning rate). \n\nThe reason why sometimes warmup also helps SGD still lacks theoretical support. We believe this topic deserves more in-depth analysis and is beyond the scope of this study. \n\n[0] Goyal et al, Accurate, Large Minibatch SGD: Training Imagenet in 1 Hour, 2017\n", "title": "Although warmup helps both Adam and SGD, they are for different reasons. "}}}