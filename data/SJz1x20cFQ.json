{"paper": {"title": "Hierarchical RL Using an Ensemble of Proprioceptive Periodic Policies", "authors": ["Kenneth Marino", "Abhinav Gupta", "Rob Fergus", "Arthur Szlam"], "authorids": ["kdmarino@cs.cmu.edu", "abhinavg@cs.cmu.edu", "fergus@cs.nyu.edu", "aszlam@fb.com"], "summary": "", "abstract": "In this paper we introduce a simple, robust approach to hierarchically training an agent in the setting of sparse reward tasks.\nThe agent is split into a low-level and a high-level policy. The low-level policy only accesses internal, proprioceptive dimensions of the state observation. The low-level policies are trained with a simple reward that encourages changing the values of the non-proprioceptive dimensions. Furthermore, it is induced to be periodic with the use a ``phase function.'' The high-level policy is trained using a sparse, task-dependent reward, and operates by choosing which of the low-level policies to run at any given time. Using this approach, we solve difficult maze and navigation tasks with sparse rewards using the Mujoco Ant and Humanoid agents and show improvement over recent hierarchical methods. ", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "Strengths \n\nThe paper presents a method of training two-level hierarchies that is based on relatively intuitive ideas and that performs well.\nThe challenges of hierarchical RL makes this an important problem. The benefits of periodicity and the\nseparation of internal state from external state is a clean principle that can potentially be broadly employed. \nThe method does well in outperforming the alternative baselines.\n\nWeaknesses\n\nThere is no video of the results. There is related work, i.e., [Peng et al. 2016] (rev 4) uses \na policy ensemble;  phase info is used in DeepLoco/DeepMimic; methods such as \"Virtual Windup Toys for Animation\" \nexploited periodicity (25y ago);  More comparisons with prior work such as Florensa et al. would help. \nThe separation of internal and external state is an assumption that may not hold in many cases.\nThe results are locomotion focussed. There are only two timescales.\n\nDecision\n\nThe reviewers are largely in agreement to accept the paper. \nThere are fairly-simple-but-useful lessons to be found in the paper\nfor those working on HRL problems, particularly those for movement and locomotion. \nThe AC sees the novely with respect to different pieces of related work is the weakest point of the paper.  \nThe reviews contain good suggestions for revisions and improvements;  the latest version may take care\nof these (uploaded after the last reviewer comments). Overall, the paper will make a good contribution\nto ICLR 2019.\n"}, "review": {"HJxrb0bXRQ": {"type": "rebuttal", "replyto": "SJgSt3agRX", "comment": "We went ahead and uploaded a revised version making the couple minor changes we promised in our responses.", "title": "Revised version"}, "Syxli5rPnm": {"type": "review", "replyto": "SJz1x20cFQ", "review": "Brief summary: \nHRL method which uses a 2 level hierarchy for sparse reward tasks. The low level policies are only provided access to proprioceptive parts of the observation, and are trained to maximize change in the non-proprioceptive part of the state as reward. The higher level policy is trained as usual by commanding lower level policies. \n\nOverall impression:\nI think the paper has a major assumption about the separation of internal and external state, thereby setting the form of the low level primitives. This may not be fully general, but is particularly useful for the classes of tasks shown here as seen from the strong results. I would like to see the method applied more generally to other robotic tasks, and a comparison to Florensa et al. And perhaps the addition of a video which shows the learned behaviors. \n\nIntroduction: \nthe difficulty of learning a high-level controller when the low-level policies shifts -> look at \u201cdata efficient hierarchical reinforcement learning\u201d (Nachum et al)\n\nThe basic assumption that we can separate out observations into proprioceptive and not proprioceptive can often be difficult. For example with visual inputs or entangled state representations, this might be very challenging to extract. This idea seems to be very heavily based on what is \u201cinternal\u201d and what is \u201cexternal\u201d to the agent, which may be quite challenging to separate. \n\nThe introduction of phase functions seems to be very specific to locomotion?\n\u2028Related work: \nThe connection of learning diverse policies should be discussed with Florensa et al, since they also perform something similar with their mutual information term. DeepMimic, DeepLoco (Peng et al) also use phase information in the state, worthwhile to cite. \n\nSection 3.1:\nThe pros and cons of making the assumption that representation is disentangled enough to make this separation, should be discussed. \n\nAlso, the internal and external state should be discussed with a concrete example, for the ant for example. \n\nSection 3.2:\nThe objective for learning diverse policies is in some sense more general than Florensa et al, but in the same vein of thinking. What are the pros and cons of this approach over that?\u2028\u2028The objective is greedy in the change of external state. We\u2019d instead like something that over the whole trajectory maximizes change?\n\nSection 3.3: \u2028How well would these cyclic objectives work in a non-locomotion setting? For example manipulation\n\nSection 3.4:\u2028This formulation is really quite standard in many HRL methods such as options framework. The details can be significantly cut down, and not presented as a novel contribution. \n\nExperiments:\nIt is quite cool that Figure 2 shows very significant movement, but in some sense this is already supervised to say \u201cmove the CoM a lot\u201d. This should be compared with explicitly optimizing for such an objective, as in Florensa et al. I\u2019m not sure that this would qualify as \u201cunsupervised\u201d per se. As in it too is using a particular set of pre-training tasks, just decided by the form of choosing internal and external state.\n\nall of the baselines fail to get close to the goal locations.-> this is a bit surprising? Why are all the methods performing this poorly even when rewarded for moving the agent as much as possible.\n\nOverall, the results are pretty impressive. A video would be a great addition to the paper. \n\nComparison to Eysenbach et al isn\u2019t quite fair since that method receives less information. If given the extra information, the HRL method performs much better (as indicated by the ant waypoint plot in that paper).", "title": "Good results, major assumption", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SylIOSTYTm": {"type": "rebuttal", "replyto": "HkgHBApm6X", "comment": "We thank the reviewer for their time. We will try to answer your questions and concerns here.\n\nPeng et al is definitely worth citing. We will add that citation.\n\nPhase function\nThe phase training does also benefit the ant, although not as much perhaps as humanoid. For humanoid, there definitely have been works that have trained humanoid without phase information with techniques such as Soft-Actor Critic. In practice, this seems to be difficult to tune. We chose a widely used PPO implementation in Kostrikov (2018), but default parameters and a grid search over parameters, we were unable to train a humanoid that receives reasonable movement reward.\n \nState space selection\nIt is possible that you could learn the high-level and low-level policy with the entire state space, although not necessarily as efficiently. But the idea of the paper is that we want to abstract away low-level details from the high-level controller so it can focus on the planning and high-level problems. Especially as RL starts to tackle more complicated problems, and as it moves to real-world robotics, this abstraction is very useful for efficiently learning difficult high-level policies.\n\nConfusion in notation\nb_phi is a learned parameter in our network, like a bias term, that depends on the phase index. These are input to our network and the value is updated by back-propogation. d is just the choice for the dimensionality of b_phi, in our case 16.\nWe will clarify this in revision.\n\nChoice of K\nK=10 is approximately the time a trained Mujoco ant model will take to make a complete cycle of action. The simulation step length is 0.05sec.\n\nEnergy consumption\nAs we say in S3.2, we do train with the Mujoco environment rewards including energy penalty.", "title": "Response to reviewer 4"}, "Byxy_ETYa7": {"type": "rebuttal", "replyto": "SkeJoQQI37", "comment": "We thank the reviewer for their time. We will try to answer your questions and concerns here.\n\nOverall algorithm\nWe will put a high-level algorithm in the appendix to make this more clear. To summarize:\n\n1. Train K low-level policies on our low-level objective using PPO/A2C/DQN\n2. Train a high level policy on the task reward using PPO/A2C/DQN where the action space is choosing one of the K low-level policies to run for T timesteps.\n\nWe will also add an appendix where we can describe the algorithm in terms of pseudo-code for one of the algorithms.\n\nTimescales\nWe have a timescale for the low-level policies and a time-scale for the high-level policy, which operates on a longer timescale since you don\u2019t need to change skills as often. \n\nGenerality\nPlease see the discussion with reviewer 1 on when our external/internal assumption holds, and when the periodic assumption holds. We believe the idea of abstracting away low-level details from the high-level controller so it can focus on the planning and high-level problems is quite general in nature. As RL starts to tackle more complicated problems, and as it moves to real-world robotics, this abstraction is very useful for efficiently learning difficult high-level policies\n\nFits into RL/control:\nHierarchical RL is important but data-inefficient; our separation into internal and external, our use of a skills framework, and the way we specifically train the low-level policies improves performance on sparse reward tasks in Mujoco. See related work where we discuss some prior work in hierarchical RL and how it compares to our method.", "title": "Response to reviewer 2"}, "HyxIZNptpm": {"type": "rebuttal", "replyto": "Byx307aKaX", "comment": "Data efficient hierarchical RL\nWe did not intend to suggest that it is impossible to learn hierarchical policies end to end, just that it is difficult. This argument is also made in Florensa et al. Nachum et al. attempt to learn the high & low level policies jointly, which is in contrast to our approach that side-steps the problem by learning them separately. \n\nLevel of supervision\nWe agree that our low-level reward is not unsupervised. We state in the introduction that we use a weak form of supervision for training the low-level policy, since the reward does not come from the final task environment.\n\nEysenbach comparison\nIn the DIAYN+prior model (which is what we compare to in Figure 7A), they do use a center of mass reward to train, which is similar to our movement reward. If you mean that we give more information to our model in the form of the split between the external and internal states, we agree. Our intention was less to claim that we beat Eysenbach et al given exactly the same information and more to say that with the assumptions we make, we are in fact able to beat DIYAN. We will make this clearer in the paper.\n\nNon-locomotion settings\nWe believe that locomotion by itself is an important class of problems, so having a method that is more aimed to those isn\u2019t a bad thing. If the cyclic objectives are less effective in other domains (e.g., grasping), they can also be removed from our method. However, we would like to point that even in some manipulation tasks  such as stirring, or repeatedly performing a task on objects as they come through on an assembly line, the cyclic prior makes sense. \n\nFormulation \nWe  will move formulation to appendix. Note, we did not claim this as a novel contribution. We did cite Frans et al. (2017) in this section, and we do make the connection to options in the related work, as well as make the connection to Eysenbach in a later section using the same high-level methodology. Overall, we will clarify this on revision.\n\nBaseline failures\nThis was surprising to us too. In Haarnoja et al. (2018a), those baselines do eventually go toward the goal, but in that setting, the goals aren't changing between episodes. The worse results could be because we are changing the goal location randomly, and that makes it far more difficult to earn reward and learn about the different goals, despite the movement prior.\n\nOther citations to add\nThank you for the additional citations for the phase information. We will add these to related work.", "title": "Response to reviewer 1 (part 2)"}, "Byx307aKaX": {"type": "rebuttal", "replyto": "Syxli5rPnm", "comment": "We thank the reviewer for their time. We will try to answer your questions and concerns here.\n\nInternal/External assumption\nWhile we agree that the separation of inputs is not fully general, we think it is appropriate in many reasonable settings.  In particular, for any actuated robot, it is only reasonable that the robot be designed to know which are proprioceptive sensors. More generally for agents whose action spaces are complex and have sensors to directly measure that action space, there is essentially no cost to provide this information to such agents. It is also common for researchers in robotics get this information with localization techniques such as visual odometry, SLAM or particle filters. \nOne may argue that we measure success with HRL in these settings as a proxy task, and what we are really interested in is an HRL algorithm(s) that can learn anywhere; but in our view, clean, cheap, widely applicable assumptions are the best hope for real progress.  Moreover, even if one is searching for the primal-generic HRL algorithms, our work is useful: (i) because it shows that this simple assumption leads to good results on these tasks and (ii) because in the current literature, these tasks are the standard testbeds, this work allows a researcher to recognize a mechanism that a more general algorithm might be using to achieve success.\nTo answer your specific question about the ant: we use the center of mass position as external, and the body angles, as well as the joint configurations as internal. Velocities we consider to be the same as the positions for categorizing as internal/external.\n\n\u201ccomparison to Florensa et al.\u201d: the main differences between this work and that one are that we do not try to use stochastic neural networks, we do not try to compress the one-hot representation of the low level networks into a dense vector during low level training (instead, keeping them fully separate, what they call the \u201cmulti-policy\u201d architecture), and we do not attempt to impose any regularization to encourage the low level networks to be diverse. These can be considered simplifications;  what we show is that the simple thing works quite well.   \n\nHowever, one might argue that one of the main points of Florensa et. al. was to be able to save sample complexity via compressing the multiple policies at train time into a single network. Our empirical results suggest that the situation is not so clear cut.  First, we are able to do well on the Ant task, whereas they have trouble making the high level policy work well there (see appendix d in that work). We also do better on humanoid, which is yet more difficult.  Moreover, if we correctly understand their measurements of sample complexity, our method, even accounting for the multiple independent models, is using far fewer environmental interactions at both the high and low level. Thus while training multiple independent models may seem wasteful on paper, it seems to work well in practice, and has superior sample complexity for the low-level policy. \n\nWe briefly discuss Florensa et al in our related work, but we can expand it to go into more detail in the comparison.", "title": "Response to reviewer 1 (part 1)"}, "HkgHBApm6X": {"type": "review", "replyto": "SJz1x20cFQ", "review": "This paper proposes a method to train high- and low-level controllers to tackle hierarchical RL.  The novelty is framing hierarchical RL as a problem of training a diverse set of LL controllers such that they can be used by a HL controller to solve high-level tasks.  By dividing state representation into proprioceptive and task-specific, the reward used to train LL and HL controllers are simplified.  Experimental result shows that the method is effective at solving maze environments for both ant and humanoid.\n\nThe good parts:\n- The method of training diversified LL controller and a single high-level controller seems to work unreasonably well.  And one benefit of this approach is that the rewards for both high (sparse) and low (difference) level controllers can be trivially defined.\n\n- The separation of proprioceptive and task-specific states seems to be gaining popularity.  For the maze environment (and any task that involves locomotion), this can be done intuitively.\n\nPlace to improve:\n- Terrain-Adaptive Locomotion (Peng et al. 2016) used a similar approach of phase-indexed motion, as well as selecting from a mixture of experts to generate action sequence for the next cycle.  Perhaps worthwhile to cite.\n\n- In fact, it seems that phase in this work only benefited training of low-level controller for humanoid.  But it should be possible to train humanoid locomotion with using phase information.\n\n- This hierarchical approach shouldn't depend on the selection of state space.  What would happen when LL and HL controllers all receive the same inputs?\n\n- The paper is difficult to follow at places.  Ex. b_phi element of R^d in Section 3.3.  I'm still not sure what is b_phi, and what is d here.\n\n- The choice of K = 10 feels arbitrary.  Since K corresponds to the length of a cycle, it should make sense to choose K such that the period is reasonable compared to average human stride period, etc.  What is the simulation step length?\n\n- Since LL policies control the style of the motion and the only reward it gets is to keep moving, presumably the resulting motion would look unnatural or exhibit excessive energy consumption.  Does \"keep moving\" reward work with other common rewards like energy penalty, etc?\n", "title": "Interesting approach to hierarchical RL", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SkeJoQQI37": {"type": "review", "replyto": "SJz1x20cFQ", "review": "This paper presents an approach for hierarchical RL based on an ensemble of low-level controllers.\nFrom what I can tell, you train K randomly initialized models to maximize displacement (optionally with a periodic implementation).\nThis ensemble of low-level models is then presented to a high-level controller, that can use them for actions.\nWhen you do this, the resultant algorithm performs well on a selection of deep RL tasks.\n\nThere are several things to like about this paper:\n- Hierarchical RL is an important area of research, and this algorithm appears to make progress beyond the state of the art.\n\n- The ideas of using ensemble of low-level policies is intuitive and appealing.\n\n- The authors provide a reasonable explanation of their \"periodicity\" ideas, together with evidence that it can be beneficial, but is not always essential to the algorithm.\n\n- Overall the writing is good... but I did find the main statement of the algorithm confusing! I think this deserves a proper appendix with everything spelled out.\n\n\nThere are several places this paper could be improved:\n- First, the statement of the *main* algorithm needs to be brought together so that people can follow it clearly. I understand one of the main reasons this is complicated is because the authors have tried to make this \"general\" or to be used with DQN/PPO/A3C... but if you present a clear implementation for *one* of them (PPO?) then I think this will be a huge improvement.\n\n- Something *feels* a little hacky about this... why are there only two timescales? Is this a general procedure that we should always expect to work? *why* are we doing this... and what can its downsides be? The ablation studies are good, but I think a little more thought/discussion on how this fits in with a bigger picture of RL/control would be good.\n\nOverall, I hope that I understood the main idea correctly... and if so, I generally like it.\nI think it will be possible to make this much clearer even with some simple amendments.", "title": "An interesting paper; but I found the algorithm description very hard to parse!", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}