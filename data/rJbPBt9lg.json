{"paper": {"title": "Neural Code Completion", "authors": ["Chang Liu", "Xin Wang", "Richard Shin", "Joseph E. Gonzalez", "Dawn Song"], "authorids": ["xinw@eecs.berkeley.edu", "liuchang@eecs.berkeley.edu", "ricshin@berkeley.edu", "jegonzal@berkeley.edu", "dawnsong@cs.berkeley.edu"], "summary": "", "abstract": "Code completion, an essential part of modern software development, yet can bechallenging for dynamically typed programming languages.  In this paper we ex-plore the use of neural network techniques to automatically learn code completionfrom  a  large  corpus  of  dynamically  typed  JavaScript  code.   We  show  differentneural networks that leverage not only token level information but also structuralinformation,  and  evaluate  their  performance  on  different  prediction  tasks.   Wedemonstrate that our models can outperform the state-of-the-art approach, whichis based on decision tree techniques, on both next non-terminal and next terminalprediction tasks by 3.8 points and 0.5 points respectively.  We believe that neuralnetwork techniques can play a transformative role in helping software developersmanage the growing complexity of software systems, and we see this work as afirst step in that direction.", "keywords": ["Deep learning", "Applications"]}, "meta": {"decision": "Reject", "comment": "The paper extends existing code completion methods over discrete symbols with an LSTM-based neural network. This constitutes a novel application of neural networks to this domain, but is rather incremental. Alone, I don't think this would be a bad thing as good work can be incremental and make a useful contribution, but the scores do not show an amazingly significant improvement over the baseline. There are many design choices that could be better justified empirically with the introduction of neural benchmarks or an ablation study. We encourage the authors to further refine this work and re-submit."}, "review": {"Bku0gxQDx": {"type": "rebuttal", "replyto": "Hyie45fwe", "comment": "We have added a revision. We summarize the responses below.\n\n1) Yes.\n2) We train them end-to-end. No pre-training is employed for the embedding; they are initialized randomly.\n3) Since only the first segment of a program will update the initial states h0 and c0, we provide them so that h0 and c0 can be better trained. We are still working to see how this change affects the performance.\n4) The training process stops when all buckets pull segments from the queue at the same time, i.e., all segments in all programs have been used for training at least 8 times (from the first 8 epochs).\n5) We only pad EOF tokens in the training process. In the test process, these padded EOFs are not counted toward accuracy. Note that during training,the loss function includes the contribution from the padded EOF tokens.\n6) When training models on the entire dataset, we do not use a validation dataset but just train 8 epochs. However, this number 8 is determined on a small scale experiment in which we sampled from the training set 1/10 of all programs for training and 1/20 for validation. We evaluate the test accuracy on the validation set after every epoch and see that after 8 epochs, the accuracy reaches to be optimal. Unfortunately, we did not keep that dataset, so we cannot report the numbers now.\n7) We count the frequency of tokens in the training dataset. The vocabulary is constructed without touching the test set.\n8) The total loss function is explained in Section 4.4, We use the standard cross-entropy loss for $l$.\n9) h0, c0 are initialized to be 0. All other parameters (i.e., LSTM parameters and embedding parameters) are uniformly randomly initialized from [-0.05, 0.05].\n10) The models used to report the results in the paper are trained without dropout.\n11) No sampled softmax is used.\n12) We train all models using different GPUs so the duration varies a lot. Using one Pascal Titan X with 12G GPU memory, training one model with 8 epochs of the entire training data requires 3.5 days. To train a model with 8 epochs of a sampled subset takes around 0.5 day to 1 day. Reducing the size of hidden states will also decrease performance. We are working on a systematic experiment to examine the impact.", "title": "Responses"}, "Hyie45fwe": {"type": "rebuttal", "replyto": "rJbPBt9lg", "comment": "Given that the #\u2019s changed by few % between the two versions, the authors should also include further technical details describing how the models were trained. In particular, several details should be easy to include such that results are reproducible, such as:\n\n1) Is the size of the embedding the same as the hidden size (i.e., 1500)?\n2) Are the embeddings pre-trained or trained simultaneously with the model? How are they pre-trained?\n3) What is the motivation behind using only the first segment of a program for training after 8 epochs?\n4) After 8 epochs only the first segment of every program is used for training. What is the criterion for stopping training once this process of only using the first segments has started?\n5) \u201cThe last segment of a program, which may not be full, is padded with EOF tokens.\u201d Do these EOF predictions count as correct prediction? \n6) How is the training and validation dataset selected and how many samples it contains?\n7) Which dataset is used to build the vocabulary of frequent terminals used for the UNK token replacement? That is, which of the training, validation and testing datasets were used to built it?\n8) What loss function is used?\n9) Can you provide details on how the network parameters are randomly initialized?\n10) Is regularization used? E.g. dropout? If so, what is the dropout rate?\n11) Is sampled softmax used during training? If so, what is the sample size?\n12) What is a typical training duration for one of the experiments from the evaluation section? Including the results for smaller models in the appendix e.g. with reduced hidden size would also be useful.\n", "title": "Technical Details of the Approach"}, "Hk0u9yAIg": {"type": "rebuttal", "replyto": "rJbPBt9lg", "comment": "The latest revision adds the following content beyond Jan-16's version:\n\n1) We add the results for one single models' performance (rather than an ensemble's performance) for the next non-terminal and terminal predictions in Section 5.3.\n2) A new paragraph in Section 5.3 to report the effectiveness of setting the UNK threshold.\n3) We add a new subsection, Section 5.5, to report the deny prediction experiments. We also add a paragraph in Sec 5.5 to examine how different choices of alpha affect a model's accuracy.\n", "title": "A new revision to address some of reviewers' comments"}, "SJeGKQsUl": {"type": "rebuttal", "replyto": "rJbPBt9lg", "comment": "Dear reviewers,\n\nWe have improved our approaches and revised the paper based on the comments. We thank all reviewers for the valuable comments, and hope our work has the chance to be discussed further!\n\nWe are still working to update the paper, and some new results will be available by tomorrow.\n\nHere is the list of all changes as of Jan-15-2017:\n\n1) We have fine-tuned the model to achieve better performance. Now, our approaches can achieve a better performance than prior art. We have updated the following related section:\n   a) Abstract & Introduction\n   b) Section 5.2. Training details\n   c) All results in Section 5.3 & 5.4. In 5.3, the baseline is updated to Raychev et al. (2016a).\n   d) Some results in Section 5.5 are still pending. So this section is removed right now.\n\n2) The related work section and items in the reference have been revised.\n\n3) We have added a paragraph in Section 3.3 to explain the difference between next node prediction and next token prediction.\n\n", "title": "Methods have been improved, and paper is updated"}, "H1kMOXo8x": {"type": "rebuttal", "replyto": "ByK953bEx", "comment": "We thank the reviewer for the useful comments. We have tuned the model, and re-run all experiments. The current results suggest that our approaches can beat the state-of-the-art on both non-terminal and terminal prediction tasks.\n\nWe have added a paragraph in Section 3.3 to explain why we think the next node prediction task is more meaningful than the next token prediction task. Also, the performance of these two tasks are not directly comparable, and recent works have focused more on the node prediction task. Thus we chose the former rather than the latter.\n\nFor NT2NT, we have added an observation in Section 5.4 to show that, even though the model seems to predict the next non-terminal and the next terminal independently (conditioned on the hidden states), our experiment results show that this is not the case.\n\nWe have revised the related work section.\n", "title": "Revision"}, "r1DyO7sIl": {"type": "rebuttal", "replyto": "ryOXS6N4g", "comment": "We thank the reviewer for the useful comments. The goal of this work is to enable code completion. With this target application scenario in mind, our latest results (Section 5.3) have shown that over 96% of the time when prediction is restricted to the top 50,000 most frequent tokens in the training set, our approach is able to provide a candidate list so that users can select from the list rather than inputting manually. Indeed, we agree with the reviewer that our approach does not guarantee the generated code is syntactically correct, but we would like to argue that 96% accuracy indicates that our approach is effective enough for our target application.\n\nIn the current version, we have removed the top5 accuracy for Table 5, since these results are lower priority. We will update these results, later, by maximizing the joint posterior probability, and only top5 non-terminals from NT2N and top5 terminals from NTN2T will be considered.\n\nWe will update the analysis about UNK tokens and alpha in one to two days, as well as Section 5.5 (which is now removed).", "title": "revision to respond the comments"}, "SytiwXo8g": {"type": "rebuttal", "replyto": "rJ20rGD4x", "comment": "We thank the reviewer for the useful comments. We have updated the related work section to revise the imprecise claims.\n\nWe have worked on updated analysis for UNK tokens and deny prediction experiments. We are still waiting for the results, and we will update them within one day.", "title": "revision to respond the comments"}, "By6vH8H7e": {"type": "rebuttal", "replyto": "HJW4Uh0fl", "comment": "We preserve EMPTY terminal tokens to make our work comparable to the PHOG paper (Bielik et al. 2016), which uses the same encoding.\n\nThe non-terminal points contains the program structure information that cannot be recovered from only the terminals. For example, in the dataset that we use, a non-terminal ReturnStatement only has an EMPTY terminal, instead of a \u201creturn\u201d keyword. Predicting this non-terminal node is equivalent to predicting \u201creturn\u201d for the code-completion task. We have updated our paper (page 4) to give a brief explanation on the reason.\n\nResetting only at the beginning is a bug that we accidentally introduced during training. We have fixed this bug. We will update the paper to reflect new results within 10 days.\n", "title": "Responses to the questions"}, "Bk5kr8Sme": {"type": "rebuttal", "replyto": "r1qnpikmg", "comment": "We will update the paper for the use of different models in the next couple of weeks.\n\n\nWe have updated the paper (page 4) to give an brief explanation why non-terminal is relevant. We also refer to our response to another question above.\n\n\nTable 1: yes, \u201coverall\u201d means the total vocabulary size.\n\n\nPrograms < 30,000 tokens have ~5.3*10^7 queries, as we have explained in Sec 5.1.\n\n\nWe are trying to increase the vocabulary size. Notice that this will result in a larger model, whose size is beyond our current GPU memory capacity. We may not update any results in this issue in the near future. But one thing we have tried is to shrink the vocabulary size, and we indeed observed the accuracy drops dramatically.\n\n\nWe view predicting UNK as a sign that the model predicts that the user will input a rare token, so it does not bother to show up suggestions. In fact, we have different metrics to measure Table 6. For Top-1 and Top-5 accuracy, yes, if only UNK is in the vocabulary, then both these two cases should be 100%. \n\nWe have updated the precision and recall of +D models. We hope this can provide more insights about the frequency that NTN2T+D deny prediction when it should not.\n\n\nFor the performance evaluation, yes, the model should be more fine-tuned and many practical optimizations should be implemented to build a full-fledge code completion engine. However, as we have stated in the paper, \u201cthese numbers are just a proof of concept and we do not optimize the code\u201d. Therefore, we think our evaluation can support our claim that our approaches may have potential practical usage.", "title": "Responses"}, "r1DjfLrQx": {"type": "rebuttal", "replyto": "HJumZS47l", "comment": "We consider the depth-first serialization of the AST mainly because in doing so, we can compare with the state-of-the-art work on this dataset, i.e., Bielik  et al. 2016 and Raychev et al. 2016. In both these two works, the data is provided as a serialized sequence of the AST, although the tree structure (parent-child links) is preserved. \n\nBoth these two works artificially insert an EMPTY terminal as the child of each non-terminal without a real terminal child. Therefore we follow this procedure to process the data, so that the results can be comparable. ", "title": "A non-terminal node without a terminal child is artificially inserted an EMPTY terminal node"}, "HJumZS47l": {"type": "review", "replyto": "rJbPBt9lg", "review": "Could you explain briefly the motivation behind considering a depth-first serialization of the AST? Also, it isn't clear to me why a depth-first serialization results in a sequence (N_i, T_i) of interleaving non-terminal N_i and terminal T_i nodes. Wouldn't the depth-first serialization from the root result in sequence of non-terminal nodes, e.g. , interspersed with a terminal nodes, e.g.  N_1, N_2, N_3 T_1, N_4, T_2. Could you clarify? Thanks! This paper considers the code completion problem: given partially written source code produce a distribution over the next token or sequence of tokens. This is an interesting and important problem with relevance to industry and research. The authors propose an LSTM model that sequentially generates a depth-first traversal over an AST. Not surprisingly the results improve over previous approaches with more brittle conditioning mechanisms (Bielik et al. 2016). Still, simply augmenting previous work with LSTM-based conditioning is not enough of a contribution to justify an entire paper. Some directions that would greatly improve the contribution include: considering distinct traversal orders, does this change the predictive accuracy? Any other ways of dealing with UNK tokens? The ultimate goal of this paper is to improve code completion, and it would be great to go beyond simply neurifying previous methods.\n\nComments:\n\n- Last two sentences of related work claim that other methods can only \"examine a limited subset of source code\". Aside from being a vague statement, it isn't accurate. The models described in Bielik et al. 2016 and Maddison & Tarlow 2014 can in principle condition on any part of the AST already generated. The difference in this work is that the LSTM can learn to condition in a flexible way that doesn't increase the complexity of the computation.\n\n- In the denying prediction experiments, the most interesting number is the Prediction Accuracy, which is P(accurate | model doesn't predict UNK). I think it would also be interesting to see P(accurate | UNK is not ground truth). Clearly the models trained to ignore UNK losses will do worse overall, but do they do worse on non-UNK tokens?", "title": "Clarifications", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJ20rGD4x": {"type": "review", "replyto": "rJbPBt9lg", "review": "Could you explain briefly the motivation behind considering a depth-first serialization of the AST? Also, it isn't clear to me why a depth-first serialization results in a sequence (N_i, T_i) of interleaving non-terminal N_i and terminal T_i nodes. Wouldn't the depth-first serialization from the root result in sequence of non-terminal nodes, e.g. , interspersed with a terminal nodes, e.g.  N_1, N_2, N_3 T_1, N_4, T_2. Could you clarify? Thanks! This paper considers the code completion problem: given partially written source code produce a distribution over the next token or sequence of tokens. This is an interesting and important problem with relevance to industry and research. The authors propose an LSTM model that sequentially generates a depth-first traversal over an AST. Not surprisingly the results improve over previous approaches with more brittle conditioning mechanisms (Bielik et al. 2016). Still, simply augmenting previous work with LSTM-based conditioning is not enough of a contribution to justify an entire paper. Some directions that would greatly improve the contribution include: considering distinct traversal orders, does this change the predictive accuracy? Any other ways of dealing with UNK tokens? The ultimate goal of this paper is to improve code completion, and it would be great to go beyond simply neurifying previous methods.\n\nComments:\n\n- Last two sentences of related work claim that other methods can only \"examine a limited subset of source code\". Aside from being a vague statement, it isn't accurate. The models described in Bielik et al. 2016 and Maddison & Tarlow 2014 can in principle condition on any part of the AST already generated. The difference in this work is that the LSTM can learn to condition in a flexible way that doesn't increase the complexity of the computation.\n\n- In the denying prediction experiments, the most interesting number is the Prediction Accuracy, which is P(accurate | model doesn't predict UNK). I think it would also be interesting to see P(accurate | UNK is not ground truth). Clearly the models trained to ignore UNK losses will do worse overall, but do they do worse on non-UNK tokens?", "title": "Clarifications", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1qnpikmg": {"type": "review", "replyto": "rJbPBt9lg", "review": "Could the authors give examples, code snippets, which represents the possible use of the various predictors (NT2N, NT2NT, NTN2T)?\nWhy is it useful to measure the non-terminal prediction accuracy? From the user viewpoint only (visible) terminal prediction accuracy makes sense.\n\nTable 1.: \"Overall\": should that mean the vocabulary size?\nHow many queries are used when not all queries are used (programs < 30000 tokens)?\n\nHave the authors tried to use bigger lexicon > 50k? To support the argument to the result in table 3.\n\nDenying prediction: have the authors tried to use 0<\\alpha<1 for y_i = UNK, to down weight the possibility of UNK?\nPredicting UNK in the test set is not positive results, UNK means out-of-vocabulary word, so it is still a prediction error, because it is not the reference word. Pathological case: only UNK in the vocabulary, would mean 100% accuracy?\nThe authors might argue that maybe for a user not predicting is less bad than bad prediction.\nIt would be also interesting to see how many times NTN2T+D reject to predict when it should (not UNK in the reference).\n\nWriting e.g. assignment expression changes the AST -> changes the in-order depth-first sequence -> the whole LSTM should be rerun not just adding one node to the sequence as the author stated.\nAlternatively, LSTM states can be cached in nodes of the tree, to reuse the biggest possible common partial-tree.\n\n\nPros:\n  using neural network on a new domain.\nCons:\n  It is not clear how it is guaranteed that the network generates syntactically correct code.\n\nQuestions, comments:\n  How is the NT2N+NTN2T top 5 accuracy is computed? Maximizing the multiplied posterior probability of the two classifications?\n  Were all combinations of NT2N decision with all possible NTN2T considered?\n\n  Using UNK is obvious and should be included from the very beginning in all models, since the authors selected the size of the\n  lexicon, thus limited the possible predictions.\n  The question should then more likely be what is the optimal value of alpha for UNK.\n  See also my previous comment on estimating and using UNK.\n\n  Section 5.5, second paragraph, compares numbers which are not comparable.\n\n", "title": "questions", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryOXS6N4g": {"type": "review", "replyto": "rJbPBt9lg", "review": "Could the authors give examples, code snippets, which represents the possible use of the various predictors (NT2N, NT2NT, NTN2T)?\nWhy is it useful to measure the non-terminal prediction accuracy? From the user viewpoint only (visible) terminal prediction accuracy makes sense.\n\nTable 1.: \"Overall\": should that mean the vocabulary size?\nHow many queries are used when not all queries are used (programs < 30000 tokens)?\n\nHave the authors tried to use bigger lexicon > 50k? To support the argument to the result in table 3.\n\nDenying prediction: have the authors tried to use 0<\\alpha<1 for y_i = UNK, to down weight the possibility of UNK?\nPredicting UNK in the test set is not positive results, UNK means out-of-vocabulary word, so it is still a prediction error, because it is not the reference word. Pathological case: only UNK in the vocabulary, would mean 100% accuracy?\nThe authors might argue that maybe for a user not predicting is less bad than bad prediction.\nIt would be also interesting to see how many times NTN2T+D reject to predict when it should (not UNK in the reference).\n\nWriting e.g. assignment expression changes the AST -> changes the in-order depth-first sequence -> the whole LSTM should be rerun not just adding one node to the sequence as the author stated.\nAlternatively, LSTM states can be cached in nodes of the tree, to reuse the biggest possible common partial-tree.\n\n\nPros:\n  using neural network on a new domain.\nCons:\n  It is not clear how it is guaranteed that the network generates syntactically correct code.\n\nQuestions, comments:\n  How is the NT2N+NTN2T top 5 accuracy is computed? Maximizing the multiplied posterior probability of the two classifications?\n  Were all combinations of NT2N decision with all possible NTN2T considered?\n\n  Using UNK is obvious and should be included from the very beginning in all models, since the authors selected the size of the\n  lexicon, thus limited the possible predictions.\n  The question should then more likely be what is the optimal value of alpha for UNK.\n  See also my previous comment on estimating and using UNK.\n\n  Section 5.5, second paragraph, compares numbers which are not comparable.\n\n", "title": "questions", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJW4Uh0fl": {"type": "review", "replyto": "rJbPBt9lg", "review": "A few questions:\n\n- It seems a strange to have all the EMPTY terminal tokens in the encoding described in section 4. What is the justification for using the described encoding of trees instead of an encoding like in Grammar as a Foreign Language [1]?\n\n- What is the point of predicting non-terminals in a code completion context? To make code completion suggestions, terminals need to be predicted, right?\n\n- Can you elaborate on the training details in 5.2? Specifically, I didn't understand why it makes sense to only reset hidden states at the beginning of an epoch.\n\n\n[1] Vinyals, Oriol, et al. \"Grammar as a foreign language.\" Advances in Neural Information Processing Systems. 2015.This paper studies the problem of source code completion using neural network models. A variety of models are presented, all of which are simple variations on LSTMs, adapted to the peculiarities of the data representation chosen (code is represented as a sequence of (nonterminal, terminal) pairs with terminals being allowed to be EMPTY). Another minor tweak is the option to \"deny prediction,\" which makes sense in the context of code completion in an IDE, as it's probably better to not make a prediction if the model is very unsure about what comes next.\n\nEmpirically, results show that performance is worse than previous work on predicting terminals but better at predicting nonterminals. However, I find the split between terminals and nonterminals to be strange, and it's not clear to me what the takeaway is. Surely a simple proxy for what we care about is how often the system is going to suggest the next token that actually appears in the code. Why not compute this and report a single number to summarize the performance?\n\nOverall the paper is OK, but it has a flavor of \"we ran LSTMs on an existing dataset\". The results are OK but not amazing. There are also some issues with the writing that could be improved (see below). In total, I don't think there is a big enough contribution to warrant publication at ICLR.\n\nDetailed comments:\n\n* I find the NT2NT model strange, in that it predicts the nonterminal and the terminal independently conditional upon the hidden state.\n\n* The discussion of related work needs reworking. For example, Bielik et al. does not generalize all of the works listed at the start of section 2, and the Maddison (2016) citation is wrong\n", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ByK953bEx": {"type": "review", "replyto": "rJbPBt9lg", "review": "A few questions:\n\n- It seems a strange to have all the EMPTY terminal tokens in the encoding described in section 4. What is the justification for using the described encoding of trees instead of an encoding like in Grammar as a Foreign Language [1]?\n\n- What is the point of predicting non-terminals in a code completion context? To make code completion suggestions, terminals need to be predicted, right?\n\n- Can you elaborate on the training details in 5.2? Specifically, I didn't understand why it makes sense to only reset hidden states at the beginning of an epoch.\n\n\n[1] Vinyals, Oriol, et al. \"Grammar as a foreign language.\" Advances in Neural Information Processing Systems. 2015.This paper studies the problem of source code completion using neural network models. A variety of models are presented, all of which are simple variations on LSTMs, adapted to the peculiarities of the data representation chosen (code is represented as a sequence of (nonterminal, terminal) pairs with terminals being allowed to be EMPTY). Another minor tweak is the option to \"deny prediction,\" which makes sense in the context of code completion in an IDE, as it's probably better to not make a prediction if the model is very unsure about what comes next.\n\nEmpirically, results show that performance is worse than previous work on predicting terminals but better at predicting nonterminals. However, I find the split between terminals and nonterminals to be strange, and it's not clear to me what the takeaway is. Surely a simple proxy for what we care about is how often the system is going to suggest the next token that actually appears in the code. Why not compute this and report a single number to summarize the performance?\n\nOverall the paper is OK, but it has a flavor of \"we ran LSTMs on an existing dataset\". The results are OK but not amazing. There are also some issues with the writing that could be improved (see below). In total, I don't think there is a big enough contribution to warrant publication at ICLR.\n\nDetailed comments:\n\n* I find the NT2NT model strange, in that it predicts the nonterminal and the terminal independently conditional upon the hidden state.\n\n* The discussion of related work needs reworking. For example, Bielik et al. does not generalize all of the works listed at the start of section 2, and the Maddison (2016) citation is wrong\n", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}