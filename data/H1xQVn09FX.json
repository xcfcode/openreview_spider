{"paper": {"title": "GANSynth: Adversarial Neural Audio Synthesis", "authors": ["Jesse Engel", "Kumar Krishna Agrawal", "Shuo Chen", "Ishaan Gulrajani", "Chris Donahue", "Adam Roberts"], "authorids": ["jesseengel@google.com", "kumarkagrawal@gmail.com", "chenshuo@google.com", "igul222@gmail.com", "christopherdonahue@gmail.com", "adarob@google.com"], "summary": "High-quality audio synthesis with GANs", "abstract": "Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies  with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.\n", "keywords": ["GAN", "Audio", "WaveNet", "NSynth", "Music"]}, "meta": {"decision": "Accept (Poster)", "comment": "1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.\n\n- novel approach to audio synthesis\n- strong qualitative and quantitative results\n- extensive evaluation\n \n2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.\n\n- small grammatical issues (mostly resolved in the revision).\n \n3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it\u2019s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.\n\nNo major points of contention.\n \n4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.\n\nThe reviewers reached a consensus that the paper should be accepted.\n"}, "review": {"Skgsl-CCnm": {"type": "review", "replyto": "H1xQVn09FX", "review": "This paper proposes an approach that uses GAN framework to generate audio through modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Experiments on NSynth dataset show that it gives better results then WaveNet. The most successful deep generative models are WaveNET,  Parallel WaveNet and Tacotran that are applied to speech synthesis, the method should be tested for speech synthesis and compared with WaveNet, Parallel WaveNet as well as Tacotran.\n\nFor WaveNet, the inputs are text features, but for Tacotran, the inputs are mel-spectrogram. Here the inputs are log magnitudes and instantaneous frequencies. So the idea is not that much new.\n\nGAN has been used in speech synthesis, see \nStatistical Parametric Speech Synthesis Incorporating Generative Adversarial Networks\nIEEE/ACM Transactions on Audio, Speech, and Language Processing ( Volume: 26 , Issue: 1 , Jan. 2018 )\n\nSo for this work, GAN's application to sound generation is not new.", "title": "This paper proposes an approach that uses GAN framework to generate audio.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "r1xjgxknaX": {"type": "rebuttal", "replyto": "rke0sgTc2X", "comment": "Thank you for your time and expertise in your review, we've addressed the key points below:\n\n> \u201c...what do the authors see as 'high' resolution vis a vis audio signals?\u201d\n\nIn the context of these audio datasets, we use \u201chigh\u201d resolution to refer more to the dimensionality of the signal to model with a single latent vector, rather than the temporal resolution of the audio. The spectral \u201cimages\u201d that GANSynth models, have 1024 frequencies, 128 timesteps, and 2 channels, [1024, 128, 2], which is roughly equivalent to a [295, 295, 3] RGB image. This puts the task comparable to some of the higher-resolution GANs for images.\n\n> \u201cI am curious if we can adapt these ideas for recurrent generators as might appear in TTS problems.\u201c\n\nWe agree that would be an interesting development. Recurrent generators, and even discriminators, would allow for variable-length sequences and variable-length conditioning as is common in speech synthesis or music generation beyond single notes. Our initial experiments at using recurring generators were not very successful, so we opted to adopt a better tested architecture for this study, but this is definitely still an area ripe for exploration.\n", "title": "Response to AnonReviewer2"}, "rJxQcyy26m": {"type": "rebuttal", "replyto": "BkgZIiNcn7", "comment": "Thank you for your time and insight in your review. We've incorporated changes to the paper and respond to your main points below:\n\n> \u201cWhy didn't you train a WaveNet on the high-resolution instantaneous frequency representations?\u201d\n\nThat\u2019s an interesting avenue of research to explore. We trained WaveNets on the raw audio waveforms to provide strong and proven baseline models to compare against. Generating spectra with WaveNets is relatively unexplored and complicated by the high dimensionality at each timestep (number of frequencies * 2), each of which would have to be quantized in a traditional autoregressive treatment. It\u2019s quite possible that 2 dimensional convolutions and autoregression could help overcome this, but then the model would most resemble pixelCNN and be far from a proven audio generation method for a strong baseline.\n\n> \u201cI'm still not clear on unrolled phase which is central to this work. If you can, spend more time explaining this in detail, maybe with examples / diagrams? In figure 1,  in unrolled phase, why is time in reverse?\u201d\n\nApologies for the confusion. To help clarify, we\u2019ve renamed the \u201cunrolled\u201d phase as \u201cunwrapped\u201d throughout the paper, which is better alignment to standards in the literature and popular software packages such as Matlab and Numpy (for example https://www.mathworks.com/help/dsp/ref/unwrap.html). We have also added text further describing figure 1 (2nd to last paragraph of introduction) to help explain unwrapping to be the process of adding 2*Pi to the wrapped phase whenever it crosses a phase discontinuity such as to recover the monotonically increasing phase. The time derivative of this unwrapped phase is then the radial instantaneous frequency.\n\n> \u201cFigure 1 & 2: label the x-axis as time. Makes it a lot easier to understand.\n\nThank you for the helpful pointer. We\u2019ve added time axis labels to the figures and have also labeled the interpolation amounts for the interpolation figure.\n\n> \u201csentence before sec 2.2, and other small grammatical mistakes. Reread every sentence carefully for grammar.\u201d\n\nWe have read through the paper several times to revise grammatical mistakes including the sentence you highlighted.\n\n> \u201cFigure 5 is low-res. Please fix. All the other figures are beautiful - nice work!\u201d\n\nThanks for catching this! We\u2019ve updated the figure to be high resolution.\n\n", "title": "Response to AnonReviewer1"}, "S1xnyJyhpm": {"type": "rebuttal", "replyto": "Skgsl-CCnm", "comment": "Thank you for your review. We've done our best to address your concerns with paper revisions and in the comments below:\n\n> \u201cThe method should be tested for speech synthesis and compared with WaveNet, Parallel WaveNet as well as Tacotran\u201d\n\nWe agree that it would be very interesting to adapt these methods to speech synthesis tasks, but believe that this lies outside the scope of this initial paper on adversarial audio synthesis. As we note in AnonReviewer2\u2019s comments, adapting the current methods to incorporate variable-length conditioning and generate variable-length sequences is a non-trivial extension and requires further research. In the context of this study, we\u2019ve done our best to provide strong autoregressive baselines from state-of-the-art implementations of WaveNet models (including 8-bit and 16-bit output representations). \n\nThank you for highlighting that this is an important direction for this research. We have updated the text of the paper with a paragraph highlighting the importance and difficulty of pushing the current methods forward for more general audio synthesis tasks. \n", "title": "Response to AnonReviewer3"}, "r1xhvRAoTX": {"type": "rebuttal", "replyto": "H1xQVn09FX", "comment": "We would like to thank all the reviewers for their thoughtful and helpful reviews. In addition to answering the points of each individual reviewer below, we also want to highlight several additions we have made to the appendix to hopefully improve clarity and reproducibility.\n\n* An additional figure displaying spectrograms for a Bach Prelude synthesized both with and without latent interpolation, the audio for which can be found in the supplemental. \n* Substantial experimental details to improve reproducibility, including detailed architecture parameters and training procedures.\n* An additional NDB figure highlighting the lack of diversity of WaveNet baseline samples. \n* A table of additional baseline comparisons, justifying the use of WaveGAN and 8-bit WaveNet as the strongest baselines. \n", "title": "Updates"}, "rke0sgTc2X": {"type": "review", "replyto": "H1xQVn09FX", "review": "This paper proposes a strategy to generate audio samples from noise with GANs. The treatment is analogous to image generation with GANs, with the emphasis being the changes to the architecture and representation necessary to make it possible to generate convincing audio that contains an interpretable latent code and is much faster than an autoregressive Wavenet based model (\"Neural Audio Synthesis of Musical Notes with WaveNet AutoEncoders\" - Engel et al (2017)). Like the other two related works (WaveGAN - \"Adversarial Audio Synthesis\" - Donahue et al 2018) and the Wavenet model above, it uses the NSynth dataset for its experiments. \n\nMuch of the discussion is on the representation itself - in that, it is argued that using audio (WaveGAN) and log magnitude/phase spectrograms  (PhaseGAN) produce poorer results as compared with the version with the unrolled phase that they call 'IF' GANs, with high frequency resolution and log scaling to separate scales.  \n\nThe architecture of the network is similar to the recently published paper  (Donahue et al 2018), with convolutions and transpose convolutions adapted for audio. However, there seem to be two important developments. The current paper uses progressive growing of GANs (the current state of the art for producing high resolution images), and pitch conditioning (Odena et al, where labels are used to help training dynamics). \n\nFor validation, the paper presents several metrics, with the recently proposed \"NDB\" metric figuring in the evaluations, which I think is interesting. The IF-Mel + high frequency resolution model seems to outperform the others in most of the evaluations, with good phase coherence and interpolation between latent codes. \n\nMy thoughts: \nOverall, it seems that the paper's contributions are centered around the representation (with \"IF-Mel\" being the best). The architecture itself is not very different from commonly used DCGAN variants - the authors say that using PGGAN is desirable, but not critical, and the use of labels from Odena et al. \n\nMany of my own experiments with GANs were plagued by instability (especially at higher resolution) and mode collapse problems without special treatment (largely documented, such as adding noise, adjusting learning rates and so forth). To this end, what do the authors see as 'high' resolution vis a vis audio signals? \n\nI am curious if we can adapt these ideas for recurrent generators as might appear in TTS problems. \n\nI rate this paper as an accept since this is one of the few existing works that demonstrate successful audio generation from noise using GANs, and  owing to its novelty in exploring representation for audio. \n", "title": "Interesting take on GAN audio synthesis - accept", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkgZIiNcn7": {"type": "review", "replyto": "H1xQVn09FX", "review": "This is an exciting paper with a simple idea for better representing audio data so that convolutional models such as generative adversarial networks can be applied. The authors demonstrate the reliability of their method on a large dataset of acoustic instruments and report human evaluation metrics. I expect their proposed method of preprocessing audio to become standard practice.\n\nWhy didn't you train a WaveNet on the high-resolution instantaneous frequency representations? In addition to conditioning on the notes, this seems like it would be the right fair comparison. \n\nI'm still not clear on unrolled phase which is central to this work. If you can, spend more time explaining this in detail, maybe with examples / diagrams? In figure 1,  in unrolled phase, why is time in reverse?\n\nSmall comments:\n\n- Figure 1 & 2: label the x-axis as time. Makes it a lot easier to understand.\n\n- I appreciate the plethora of metrics. The inception score you propose is interesting. Very cool that number of statistically-different bins tracks human eval!\n\n- sentence before sec 2.2, and other small grammatical mistakes. Reread every sentence carefully for grammar. \n\n- Figure 5 is low-res. Please fix. All the other figures are beautiful - nice work!", "title": "Exciting work", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}