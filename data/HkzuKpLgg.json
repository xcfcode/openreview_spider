{"paper": {"title": "Efficient Communications in Training Large Scale Neural Networks", "authors": ["Linnan Wang", "Wei Wu", "George Bosilca", "Richard Vuduc", "Zenglin Xu"], "authorids": ["linnan.wang@gatech.edu", "wwu12@vols.utk.edu", "bosilca@icl.utk.edu", "richie@cc.gatech.edu", "zlxu@uestc.edu.cn"], "summary": "Tackle the communications in the parallel training of neural networks", "abstract": "We consider the problem of how to reduce the cost of communication that is re- quired for the parallel training of a neural network. The state-of-the-art method, Bulk Synchronous Parallel Stochastic Gradient Descent (BSP-SGD), requires a many collective communication operations, like broadcasts of parameters or reduc- tions for sub-gradient aggregations, which for large messages quickly dominates overall execution time and limits parallel scalability. To address this problem, we develop a new technique for collective operations, referred to as Linear Pipelining (LP). It is tuned to the message sizes that arise in BSP-SGD, and works effectively on multi-GPU systems. Theoretically, the cost of LP is invariant to P , where P is the number of GPUs, while the cost of more conventional Minimum Spanning Tree (MST) scales like O(log P ). LP also demonstrate up to 2x faster bandwidth than Bidirectional Exchange (BE) techniques that are widely adopted by current MPI implementations. We apply these collectives to BSP-SGD, showing that the proposed implementations reduce communication bottlenecks in practice while preserving the attractive convergence properties of BSP-SGD.", "keywords": ["Applications", "Deep learning"]}, "meta": {"decision": "Reject", "comment": "The authors propose improvements for the utilization of modern hardware when training using stochastic gradient. However, the reviewers bring up several issues with the paper, including major clarity issues as well as notational issues and some comments about the theory vs. practice."}, "review": {"Hk0ktomrg": {"type": "rebuttal", "replyto": "HkzuKpLgg", "comment": "To Reviewers,\n\nWe have improved the readability according to the feedback from Reviewer 3. Please check at the revision.", "title": "Revisions"}, "Hy-jG7GBx": {"type": "rebuttal", "replyto": "rkHPo9eNg", "comment": "Thank you for the constructive criticism and I will update the paper accordingly to improve the readability.\n\nQ1==I am not convinced that the empirical comparison to OpenMPI is completely fair.  Based on remarks in the paper (\"An extraordinary speedup against MPI is observable due to inefficient data movement in OpenMPI. It moves data to host RAM to perform reduce operations on the CPU before being copied to the target GPU.\") it appears that the OpenMPI library used in the empirical work is not a CUDA-aware implementation -- one that supports sending and receiving CUDA device memory directly (https://www.open-mpi.org/faq/?category=runcuda).  This seems like a significant shortcoming in the paper.\n\n\nSome authors of these paper belong to Open MPI development team, and I guarantee the correct use of it (e.g. cuda-aware MPI). Current MPI implementations need to be reconsidered to address the communication challenges in Deep Learning, and this is why we draft this paper.\n\nQ2==While the theory says that the costs of LP collectives should be invariant to the number of devices in a multi-GPU system, the empirical work shows that in practice this does not hold going from 4 to 5 devices (in the tested configuration) because in a 5-device system messages must traverse the QPI.  Are there other practical considerations that the authors are aware of that affect the scaling of the LP collectives?  If so, these should be mentioned in the paper.\n\nThis is the only factor that we are aware on the multiGPU system, while 'being topology-aware' is also an important factor for LP to work on the multiGPU cluster.\n\nQ3==In Figure 3, it appears that if the message size were further increased, the MST collectives might outperform the LP collectives.  This seems at odds with the theoretical results that LP collective cost is independent of P, but MST collective cost scales as log(p).  Please comment on this.\n\nBE and LP dissect the message into tiles, while MST moves the message as a whole. Increasing the message size will increase the number of tiles and the startup time (\u03b1 in Eq.1) for BE and LP, while MST always stay at a constant. Please note the graph is in log scale, and a minor difference is still huge. MST won't surpass BE and LP assuming good implementations.\n\n\nQ4==In the sentence \"Worringen (2003) proposed a pipeline collective model in shared memory environment for CPU data, but communications of different MPI processes sharing the same CPU memory bus within the same CPU socket.\" I really can't figure out what the words after \"but communications of different MPI processes\" are trying to convey.  This sentence is not comprehensible.\n\nA CPU may have many cores, thereby many MPI process running on each core. In this case, these processes share the same CPU memory bus on a single CPU. If these process request data exchange simultaneously, they will compete for the limited memory bus bandwidth on the CPU.\n\nQ5==\"However, the parameters are not consistent after several iterations due to the precision issues of float multiplications in Gradient Update.\"  Are you sure the inconsistency in weight estimates across devices is due to multiplication?  I would expect that it would be due to gradients being accumulated in different orders; that is, because floating point addition is not commutative.\n\nYes, it can be both. Here is the logic,\n1.  all GPUs start with W0\n2. each GPU receives the same accumulated gradient (G0) through all-reduce, and W1 = W0 - lr*G0; please note each GPU conducts the update independently.\n3. Go back to 1.\n\nAfter a few loops like this, parameters W is slightly different on each GPU. In terms of math, they should be identical. However, a + b != a + b in computers. That's why we need sync weight parameters in a few iterations.\n\n", "title": "reply"}, "HyVWVQMSe": {"type": "rebuttal", "replyto": "BkRqLgPNl", "comment": "- The authors compare their proposed approach with several alternative approaches and demonstrate strong performance of the proposed approaches. But it is unclear if the improvement is from the proposed approach or from the implementation.  \n\n=> It is from both algorithm and implementation. MST is a straightforward example, and the cost increases O(log p) ~ p = GPU counts, while our approach is invariant to p. This is observable in experiments (Fig.4).\n\n \n- The paper is not easy to follow and the writing can be improved in many place (aside from typos and missing references). Specifically, the authors should provide more intuitions of the proposed approach in the introduction and in Section 3. \n\n=> We will address the readability, thank you.\n\n- The proposition and the analysis in Section 3.2 do not suggest the communication cost of linear pipeline is approximately 2x and log p faster than BE and MST, respectively, as claimed in many places in the paper. Instead, it suggests LP *cannot* be faster than these methods by 2x and log p  times. More specifically, Eq (2) shows T_broadcase_BE/ T_broadcase_LP < 2. This does not provide an upper-bound of T_broadcase_LP and it can be arbitrary worse when comparing with T_broadcase_BE from this inequality. Therefore, instead of showing T_broadcase_BE/ T_broadcase_LP < 2, the authors should state T_broadcase_BE/ T_broadcase_LP > 1 when n approaches infinity. \n\n=>Thank you for the insight, I will add it into the paper. But  T_broadcase_BE/ T_broadcase_LP > 1 only tells us LP can be faster than BE assuming perfect implementations. The upper bound limits the speedup of proposal approach.\n\n", "title": "reply"}, "r1OSPuX4e": {"type": "rebuttal", "replyto": "B1KD4MX4x", "comment": "Thank you for your suggestion. We will fix the name in the final version of the paper. ", "title": "Correct the name"}, "rkgrtzMVx": {"type": "rebuttal", "replyto": "rJkNApWNx", "comment": "Thank you for your comments, I'd like to clarify the following points.\n\n3) The ring-based allreduce approach is already supported by NVidia\u2019s NCCL library, although the authors claim that their implementation comes earlier than the NCCL implementation.\n\nYes, ring-based all reduce is supported by NCCL, but this does not mean our design is identical to NCCL. Both of us have different optimization techniques toward the architecture and different pipeline design (e.g. message exchange protocols, GPU streams, and the consideration of kernel occupancy).\n\nMost importantly, NCCL does not elucidate the technical importance of communication pipeline for Deep Learning. We have demonstrated the interesting new insights, and we justify the optimality of the pipeline method.\n\nNCCL is only applied to multiGPU system, whether it works for the distributed system is unclear. In this paper, our focus is to elucidate a new pipeline design for the communication collectives. Since multiGPU system is easier to deal with, we conduct the experiments on it. However, we have extended the design to work for the distributed heterogeneous system. In practice, we have achieved \"invariant communication costs\" on 40 GPUs. We will present such results in a followup paper, and will integrate in a mainstream MPI package.\n\n4) The overlap of communication of computation is an already applied technique in systems such as TensorFlow and MXNet. The schedule proposed by the authors exploits the overlap partially, doing backprop of t-1 while doing reduce.  Note that the dependency pattern can be further exploited; with the forward of layer t depend on update of parameter of layer t in last iteration. This can be done by a dependency scheduler.\t\nYes, we don't claim credit on this part. It is more like a background study, but we will have some fancier stuff than TensorFlow and MXNet coming out in 2017 :)\n", "title": "Reply to Reviewer 3"}, "Sk_do1GNl": {"type": "rebuttal", "replyto": "rJkNApWNx", "comment": "Thank you very much for your comments. I think some of your comments is not very accurate.\n1. \"The name linear pipeline is somewhat confusing to the readers, as the technique is usually referred as ring based approach in Allreduce literature\" \nIt is true the name linear pipeline is confusing. I think pipeline chain would be a better name. In this paper, we developed collective algorithms targeted to high performance neural network training on multi-GPU system. The collective operations we discussed in our paper are broadcast, reduce and allreduce, which are the typical communications used when training neural network. We called the algorithm \"linear pipeline\" is because we connect all GPUs as a linear chain, and apply pipelining to overlap communications. The ring topology is just a special case of \"linear pipeline\" to handle allreduce. Again, we do not only focus on allreduce, but also on broadcast and reduce. The topology of broadcast and reduce is chain, and ring for allreduce. \n\n2. Since this paper is about analysis of Allreduce, it would be nice to include detailed analysis of tree-shape reduction, ring-based approach and all-to-all approach. The discussion of all-to-all approach is missing in the current paper. \nI guess the \"all to all approach\" for Allreduce is called recursive doubling: every two processes exchange data and iterate till any pair is done with data exchange. We indeed discussed this approach in our paper, which is called \"bidirectional exchange\". It is just a another name of \"recursive doubling\". ", "title": "Reply to Reviewer 3"}, "B1u4TcTfg": {"type": "rebuttal", "replyto": "HJbG5HpMg", "comment": "Yes, that's correct. \n\nWe have credited the cost model to Thakur et al. (2005), Thakur & Gropp (2003). The following text appears at the beginning of the Section 3.2 THEORETICAL ANALYSIS\n \n\"We adopt a cost model widely used by the MPI community to analyze collective operations (Thakur et al. (2005), Thakur & Gropp (2003)).\"\n \nIf you suggest we should make it clear in another place, please let us know and I will revise the paper accordingly.\n\nStill, the existing cost model does not demonstrate those important insights listed above.", "title": "Reply to communication cost analysis"}, "H1kY4BpMg": {"type": "rebuttal", "replyto": "Sk7oL7pfx", "comment": "There are several different collective algorithms in the context of MPI. The communication cost analysis of MPI usually focuses on relevantly small messages, which is much smaller than the message size used in the BSP SGD. Our algorithm is specifically targeted to single node GPU system. If our model is applied to regular CPU system, it could be completely not working because of the shared memory bus.   ", "title": "Additional reply to \"with respect to constant communication cost\""}, "HyJy6Q6Ml": {"type": "rebuttal", "replyto": "Sk7oL7pfx", "comment": "There are several critical assumptions for this claim to be valid. \n1) the message length has to be long enough, which is the case of large-scale neural networks.\n2) the latency term (alpha term in equation 1) has to be small enough to be negligible, which is the case of GPU communications. \n\nIf we're talking about the MPI without placing it under the context of large-scale NN (e.g. n->inf an alpha->0), equation 2 & 3 will not be valid. \n\nAssuming alpha term is 0, in the equation (bp \u2212 p + n)\u03b2, bp - p term may dominate if n is not relatively large. \n\nTherefore, we believe these are important new insights derived from the MPI collective algorithm by studying them specifically under the deep learning.\n\n", "title": "REPLY TO with respect to constant communication cost"}, "SyFmzMpMx": {"type": "rebuttal", "replyto": "HyxfuZTGg", "comment": "Yes, the algorithm design is essentially a type of ring algorithm. We want to show researchers in the DL field that a collective co-design can bring significant benefit for speedup the training. There are interesting contributions in this paper:\n1)\tWe elucidate a new variant of ring algorithm modified w.r.t GPU architecture.\n2)\tWe proof that the algorithm is the optimal to satisfy the communication needs for Deep Learning with the following attractive properties. \n3)\tThe communication cost is invariant to the number of GPUs. Please check Fig. 4, and we have achieved the same result over 40 GPUs. This nice property was not well recognized by both MPI and DL community.\n4)\tBidirectional Exchange is pervasively used in current MPI for long message, while we proof that the proposed method can theoretically improve the bandwidth by a factor of 2 ONLY for Deep Learning.\n5)\tWe achieve significant convergence speedup on BSP SGD.\n\nWhen we develop this algorithm, NCCL has several serious issues such as deadlock when it across the QPI. NCCL is a good library, and the performance won't be drastically different as both approaches have almost push to the limit. \n\nWe justify the optimality of the ring algorithm for Deep Learning, which I think is the main contribution of this work. In addition, we're extending the algorithm to the distributed environment. We have achieved all the promising properties predicted in this paper, which are missing in major MPI implementations.\n\n", "title": "reply"}, "HyxfuZTGg": {"type": "review", "replyto": "HkzuKpLgg", "review": "The model used in the paper appears to be ring based allreduce strategy, which is typical implemented in the MPI collectives. see, e.g.  http://www.cs.fsu.edu/~xyuan/paper/09jpdc.pdf  http://www.mcs.anl.gov/~thakur/papers/ijhpca-coll.pdf\n\n- The ring base strategy seems to be implemented in the NCCL library, have you compare against thatThis paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net.\nComments\n1) The name linear pipeline is somewhat confusing to the readers, as the technique is usually referred as ring based approach in Allreduce literature. The author should use the standard name to make the connection easier. \n2) The cost analysis of ring-based Allreduce is already provided in the existing literature. This paper applied the analysis to the case of multi-GPU deep net training, and concluded that the scaling is invariant of number of GPUs.\n3) The ring-based allreduce approach is already supported by NVidia\u2019s NCCL library, although the authors claim that their implementation comes earlier than the NCCL implementation.\n4) The overlap of communication of computation is an already applied technique in systems such as TensorFlow and MXNet. The schedule proposed by the authors exploits the overlap partially, doing backprop of t-1 while doing reduce.  Note that the dependency pattern can be further exploited; with the forward of layer t depend on update of parameter of layer t in last iteration. This can be done by a dependency scheduler.\t\n5) Since this paper is about analysis of Allreduce, it would be nice to include detailed analysis of tree-shape reduction, ring-based approach and all-to-all approach. The discussion of all-to-all approach is missing in the current paper. \nIn summary, this is a paper discussed existing Allreduce techniques for data parallel multi-GPU training of deep net, with cost analysis based on existing results. While I personally find the claimed result not surprising as it follows from existing analysis of Allreduce, the analysis might help some other readers. I view this as a baseline paper. The analysis of Allreduce could also been improved (see comment 5).\n\n\n\n\n\n\n\n\n\n", "title": "The communication is typical ring based allreduce strategy?", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJkNApWNx": {"type": "review", "replyto": "HkzuKpLgg", "review": "The model used in the paper appears to be ring based allreduce strategy, which is typical implemented in the MPI collectives. see, e.g.  http://www.cs.fsu.edu/~xyuan/paper/09jpdc.pdf  http://www.mcs.anl.gov/~thakur/papers/ijhpca-coll.pdf\n\n- The ring base strategy seems to be implemented in the NCCL library, have you compare against thatThis paper analyzes the ring-based AllReduce approach for multi-GPU data parallel training of deep net.\nComments\n1) The name linear pipeline is somewhat confusing to the readers, as the technique is usually referred as ring based approach in Allreduce literature. The author should use the standard name to make the connection easier. \n2) The cost analysis of ring-based Allreduce is already provided in the existing literature. This paper applied the analysis to the case of multi-GPU deep net training, and concluded that the scaling is invariant of number of GPUs.\n3) The ring-based allreduce approach is already supported by NVidia\u2019s NCCL library, although the authors claim that their implementation comes earlier than the NCCL implementation.\n4) The overlap of communication of computation is an already applied technique in systems such as TensorFlow and MXNet. The schedule proposed by the authors exploits the overlap partially, doing backprop of t-1 while doing reduce.  Note that the dependency pattern can be further exploited; with the forward of layer t depend on update of parameter of layer t in last iteration. This can be done by a dependency scheduler.\t\n5) Since this paper is about analysis of Allreduce, it would be nice to include detailed analysis of tree-shape reduction, ring-based approach and all-to-all approach. The discussion of all-to-all approach is missing in the current paper. \nIn summary, this is a paper discussed existing Allreduce techniques for data parallel multi-GPU training of deep net, with cost analysis based on existing results. While I personally find the claimed result not surprising as it follows from existing analysis of Allreduce, the analysis might help some other readers. I view this as a baseline paper. The analysis of Allreduce could also been improved (see comment 5).\n\n\n\n\n\n\n\n\n\n", "title": "The communication is typical ring based allreduce strategy?", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rk-VTW6ze": {"type": "rebuttal", "replyto": "HkyYbkcfe", "comment": "Thank you for providing us this interesting legacy paper, we will add it into related works if you suggest doing so. Though the word of \u201cRing Topology\u201d appears in both papers, there are important differences:\n1)\tWe address the communication issue in the CNN training from the algorithmic design, while the main contribution of RAP paper address the computations with the customized hardware architecture.\n2)\tThough the paper indicates RAP place each node on a ring (Paragraph 4, Section Hardware, Page 298), the paper targets for (Paragraph 2, Section Background, Page 296) Programmable Gate Arrays. Our algorithm is specifically designed for the GPU architecture, and being able to extend to large scale heterogeneous GPU clusters.\n3)\tThe \u201cLinear\u201d in Linear Pipeline indicates the topology of the communication pattern is a line/chain. Ring is a special case of chain by connecting the head with tail. the communication pattern is our paper is a chain/ring, but it does not requires the topology of physical hardware to be chain/ring (actually, modern GPU network is not a chain). The chain/ring in our paper is a logical concept, which the entire data-flow is like a chain/ring.\n4)\tThe \u201cPipeline\u201d in Linear Pipeline is similar to the Pipeline in modern processor. We segment a piece of long message into small chunks as the basic communication elements. Then, the data movement between 2 GPUs via an independent PCI-E link.\n", "title": "reply"}}}