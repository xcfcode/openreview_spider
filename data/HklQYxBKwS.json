{"paper": {"title": "Neural tangent kernels, transportation mappings, and universal approximation", "authors": ["Ziwei Ji", "Matus Telgarsky", "Ruicheng Xian"], "authorids": ["ziweiji2@illinois.edu", "mjt@illinois.edu", "rxian2@illinois.edu"], "summary": "The NTK linearization is a universal approximator, even when looking arbitrarily close to initialization", "abstract": "This paper establishes rates of universal approximation for the shallow neural tangent kernel (NTK): network weights are only allowed microscopic changes from random initialization, which entails that activations are mostly unchanged, and the network is nearly equivalent to its linearization. Concretely, the paper has two main contributions: a generic scheme to approximate functions with the NTK by sampling from transport mappings between the initial weights and their desired values, and the construction of transport mappings via Fourier transforms. Regarding the first contribution, the proof scheme provides another perspective on how the NTK regime arises from rescaling: redundancy in the weights due to resampling allows individual weights to be scaled down. Regarding the second contribution, the most notable transport mapping asserts that roughly $1 / \\delta^{10d}$ nodes are sufficient to approximate continuous functions, where $\\delta$ depends on the continuity properties of the target function. By contrast, nearly the same proof yields a bound of $1 / \\delta^{2d}$ for shallow ReLU networks; this gap suggests a tantalizing direction for future work, separating shallow ReLU networks and their linearization.\n", "keywords": ["Neural Tangent Kernel", "universal approximation", "Barron", "transport mapping"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper considers representational aspects of neural tangent kernels (NTKs). More precisely, recent literature on overparametrized neural networks has identified NTKs as a way to characterize the behavior of gradient descent on wide neural networks as fitting these types of kernels. This paper focuses on the representational aspect: namely that functions of appropriate \"complexity\" can be written as an NTK with parameters close to initialization (comparably close to what results on gradient descent get).  \n\nThe reviewers agree this content is of general interest to the community and with the proposed revisions there is general agreement that the paper has merits to recommend acceptance."}, "review": {"bxhWVT1osg": {"type": "rebuttal", "replyto": "Hyge3AC3FB", "comment": "Thank you for evaluating our revised paper; it was a lot of work for you, given our extensive changes.  We thank you for your very thorough and valuable comments.\n\nIn response to your post-rebuttal comments, we've updated our \"open problems\" section to expand on these gaps.\n\nTo respond informally to you here, I agree, it is odd.  Of course, the \"10\" we have must be an an analytic artifact, however I don't know what it should be.  In that new open problem comment (which is admittedly quite brief), I highlight both the choice you mention (which layers do you train), but also the question of norm (our paper here is in the \"NTK standard\" (2,infty) norm).   Would be nice to know all the gaps, which choices are relevant in practice, how they affect optimization and generalization, how depth changes things, ...\n\nThanks again!", "title": "Thank you for the detailed post-rebuttal comments"}, "Hyge3AC3FB": {"type": "review", "replyto": "HklQYxBKwS", "review": "The paper studies approximation properties (in L2 over some data distribution P) of two-layer ReLU networks in the NTK setting, that is, where weights remain close to initialization and the model behaves like a kernel method given by its linearization around initialization.\n\nThe authors obtain a variety of results in order to obtain such approximation guarantees, which are obtained by sampling from a so-called 'transport mapping', which is essentially a function T:R^{d+1}->R^{d+1} with a bound on sup_w ||T(w)||, which can approximate well various classes of target functions (section 3).\nIn particular, they show that such a sampling leads to weights close to initialization, that the neural network function is close to its linearization in L2(P), and that the linearization is close to the target function in L2(P).\nTogether with a control of the norm of T required to approximate the target function, this leads to approximation bounds in Theorem 1.3.\n\nThe techniques used to obtain transport mappings are quite interesting and seem novel, and the general approach for controlling various steps from neural network function to the target function in L2(P) norm in the NTK setting is insightful and novel as far as I know.\nThat said, the presentation lacks a certain amount of polish in its present form, which makes me lean towards the reject side. I also have some comments related to novelty of certain aspects.\n\nComments:\n* the paper is not well organized, with the main result appearing in the introduction with little details on the involved quantities, no clear separation or connection between intermediate lemmas in later sections, and very little motivation and explanation of some results. Further, there are many typos and inconsistent notations throughout which make the paper hard to read.\n\n* the sampling result in Lemma 2.1 is very similar in flavor to random feature approximation results (for the NTK here), e.g., [1, Proposition 1], which could perhaps be more precise in practice as it is data-dependent, and only needs an L2 control on the function T. Can this be applied here or would the initialization term mess things up? A comparison would be helpful either way.\n\n* the approximation rates should be discussed more (are they optimal?), and compared to prior work, both on general two-layer networks, and kernels arising from a similar setup, in particular in [2] and the cited Sun et al. (note that the NTK behaves similarly in terms of approximation, see [3])\n\n* the section on the \"natural RKHS\" is largely unclear, as is the corresponding bound in Theorem 1.3 (shouldn't B be proportional to the RKHS norm?)\n\n* how these results apply to networks obtained via optimization in the NTK regime should probably be discussed more\n\nsmaller things:\n* eq (1.2): what is the mearning of the epsilon factor? is it standard?\n* p.2 \"to not yield\" -> do not yield\n* \"with scaling... width\": rephrase (and, do you mean dataset size?)\n* \"one 1/m is then pushed\" -> one 1/sqrt(m)?\n* throughout: pick a consistent notation for derivative of relu (sometimes it's sigma', sometimes an indicator)\n* section 2: here it seems like T(w)/(eps sqrt(m)) is just the movement from initialization and T_m,eps(w) are the final weights, while in the introduction T(w) indicates the final weights, this is confusing notation. Also, shouldn't T_m,eps appear in the bounds?\n* section 3: should -> shows?\n* lemma 3.1, 3.2: specify that the other coordinates are 0, also missing dG(w) in the definition of g. What do we lose from the use of truncation?\n* lemma 3.5: sup_x or just L2(P)?\n* section 3.3: H is just L2(G) here? Also, the kernel is not universal with only even terms, but the bias fixes that, see e.g. [2,4].\n\n\n[1] Bach. On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions (2017)\n[2] Bach. Breaking the Curse of Dimensionality with Convex Neural Networks (2017)\n[3] Bietti and Mairal. On the Inductive Bias of Neural Tangent Kernels (2019)\n[4] Basri et al. The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies (2019)\n\n\n===== update post rebuttal =====\n\nThanks for the detailed response, I am increasing my score as the new version looks much better.\n\nI am a bit puzzled (and surprised) by the gap in the rates between NTK and relu random features, as it seems to suggest that only training second-layer weights while leaving the first layer at random initialization yields better rates than the NTK regime, if I understand correctly? If so, is this due mainly to the linearization step, i.e. Lemma 2.6? It would be good to include some further discussion on this in the paper.\n\nAs per approximation by random features/sampling, note that [1, Prop. 1] only requires a sup control on random features (which is quite trivial here with bounded data), not on the \"transport\" (beta in their statement).", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 3}, "B1edT2t3sr": {"type": "rebuttal", "replyto": "HklQYxBKwS", "comment": "We have restructured the paper and improved presentation, thanks to\nhelpful feedback from AnonReviewer3.\n\nA summary of the major re-arrangements is as follows.\n\n- In order for the main theorem to be more digestible, it has been pared\n  down to only discuss continuous functions, and its other components\n  have been pushed to later sections.\n\n- To further aid in the exposition of the main theorem, more\n  explanations have been moved before it; notably the central\n  description of the sampling method, and how it gives rise to the NTK\n  setting of small weight changes, has been moved from section 2 to page\n  2 of the introduction.\n\n- The remaining sections have been made more modular, and there are now\n  six sections in place of four.\n\n- In a bit more detail: section 2 still contains the old sampling\n  routines, though with re-organized exposition, and a single main\n  sampling theorem at the start encapsulating the technique; section 3\n  now contains only the Fourier-based transportation mappings; section 4\n  approximates continuous functions; section 5 briefly describes\n  \"abstract\" mappings, including the old RKHS-based transports; section\n  6 concludes with open problems.\n\n(Note that uploading of new abstracts is seemingly disabled; the revision\nhas a new abstract corresponding to the newly focused presentation.)\n\nIn order to flesh out the story and aid exposition (but not changing the core\nresults and specifically not trying to burden reviewers), a few new extensions\nhave been added:\n\n- The tools of the paper have been applied directly, in Theorem 4.5, to\n  approximation via regular shallow networks (and not the NTK); the\n  approximation rates improve, revealing a tantalizing direction for\n  future work.\n\n- Appendix B summarizes the low level sampling routines, and now\n  includes uniform norm sampling tools, which are applied in the proof\n  of Theorem 4.5 as a demonstration; in particular, L_2(P) was not\n  essential.\n\nThe revisions also include numerous other typo fixes, missing\nreferences, and other corrections, with many thanks to the reviewers.\n\n", "title": "List of revisions."}, "HklU-TY2iB": {"type": "rebuttal", "replyto": "H1edMNKV9r", "comment": "We thank the reviewer for their comments and support.\n\nIn particular, we are grateful for the comment that the \"paper is\nwritten well, and easy to read\", and hope it does not seem to odd that\nwe went through a significant restructuring.  We hope the paper has only\nmore of what AnonReviewer2 liked, not less.\n\nRegarding specific comments:\n\n- Rather than merely re-ordering the main theorem, we have simplified it\n  as discussed in our revision summary, moving all but the continuous\n  function approximation to other sections.\n\n- We agree that the title is suggestive of optimal transport, and indeed\n  worked both before and after the deadline to develop such a\n  connection.  Unfortunately, it seems somewhat elusive to derive\n  anything concrete, and have included a brief remark in the open\n  problem section; thank you for highlighting this.\n\n", "title": "Response to AnonReviewer2."}, "r1eVwpYnjH": {"type": "rebuttal", "replyto": "HkgFVpK2oS", "comment": "In response to other comments:\n\n- Regarding the sampling tool in the reviewer's reference [1,\n  Proposition 1], in fact the dependence is nearly identical to ours:\n  our proof also gives a supremum over the \"basis\" (in this case\n  depending on the transport T) and an L_2 dependence on the data\n  measure; for simplicity we've enforced ||x|| <= 1 and hidden this L_2\n  as it is generally small compared to the dependence on T.  We further\n  note that these sampling tradeoffs are also similar to those in [2,\n  Proposition 1], cited by the reviewer (and by the same author).\n\n- Regarding optimality of our rates, we have included concrete\n  discussions of lower bounds, one from the reviewer's reference [2],\n  and another from a paper by Yarotsky in our revisions.  Concretely,\n  both of these reference suggest lower bounds of the form 1/eps^{d/2}\n  if weights are not required to be close to initialization, where in\n  our case our upper bounds are closer to 1/eps^{10d} while lying close\n  to initialization.  We further note that the lower bound in [2] has\n  further restrictions (e.g., data on the sphere), and Yarotsky's paper\n  also has a higher lower bound 1/eps^{d} when the constructions are\n  \"continuous\", which we discuss in our revisions.  The upper bounds\n  (not in the NTK setting) presented in [2] are roughly tight with the\n  lower bounds, though as we mentioned the setting there is uniform on\n  the sphere.  We also note that our tools here (giving 1/eps^{10d} in\n  the NTK setting) give a much better 1/eps^{2d} when applied to regular\n  networks with a single hidden layer, as now included in section 4.\n\n- Despite pushing the RKHS material farther back, we have expanded it,\n  and hope it is clearer now.  We have removed the universal\n  approximation comments, since they are contained in the work by Sun et\n  al.\n\n- Regarding the NTK optimization literature, we hope that the present\n  results can be helpful in proving good test error bounds.  As a\n  concrete example, a work we cite by Arora et al generalizes well if\n  the quantity y^T (H^\\infty)^{-1} y is small, which is saying that the\n  features and labels share some structure.  Writing the optimal labels\n  as function of the inputs (i.e., the least squares solution over the\n  population), one can now introduce our tools, and hopefully develop\n  more concrete rates; we have included a version of this comment\n  in the concluding open problems section.\n\n- The epsilon factor in the NTK definition appears in a variety of\n  works, and corresponds to the initialization of the second layer\n  weights; see for instance the work by [ Allen-Zhu, Li, Liang ] which\n  we cite.  This scaling factor appears crucial and we highlight it\n  better in our new version (e.g., see the blue bolded terms on pages\n  3 and 4).\n\n- We have standardized our notation in the body to write sigma' in place\n  of an indicator, though on page 2 we write the indicator once for sake\n  of concreteness, and also write it in some proofs.\n\n- The new lemmas 3.2 and 3.3 clearly fix the first d coordinates as 0.\n  The second part of lemma 3.1, as we discuss in section 5, can be used\n  to form a mapping that uses all coordinates, but it does not seem to give\n  better bounds (after all it is similarly just a rescaling).  Regarding\n  what is lost with truncation, unfortunately it is unclear.  To\n  highlight where the inefficiencies in our techniques may lie, we have\n  included Theorem 4.5, which uses our techniques to prove a width upper\n  bound on approximating continuous functions with random features, and\n  it improves the earlier 1/eps^{10d} to 1/eps^{2d}, and uses no\n  truncations.  We are not asserting here that the truncations lead to\n  the extra factors, however certainly they make the proofs and bounds\n  significantly messier, and correspondingly this makes it harder to\n  produce tight bounds.\n\n- The \"B\" for the RKHS transports indeed depends on the RKHS norm; the\n  earlier attempt at rushing to the main theorem within two pages made\n  this unclear, but it should now be clear, with two separate bounds\n  now appearing in section 5, both depending on the RKHS norm.\n\n- The Hilbert space we develop to discuss RKHSes is indeed the same as\n  L2(G); we have included this clarification, thank you.\n\n- Thank you for the four references, we have included them.\n", "title": "(Continuation of response to AnonReviewer3.)"}, "HkgFVpK2oS": {"type": "rebuttal", "replyto": "Hyge3AC3FB", "comment": "We thank the reviewer for their thorough comments.\n\nWhat we wish to primarily highlight is that we have heavily restructured\nand polished the paper.  As the paper only has two reviews, we hope the\nreviewer is able to find the time to at least go through the new\nabstract and introduction.  We have summarized the changes in a separate\ncomment above, but a list of notable changes which match the comments of\nthe reviewer are as follows:\n\n- In response to \"the main result appearing in the introduction with\n  little details on the involved quantities\", we have (a) pushed the\n  main result back to page 3, (b) pushed before it the key idea,\n  originally in section 2, that one can easily take a network written\n  with a transport and obtain another (wider) network where weights are\n  close to initialization, (c) focused the main theorem and introduction\n  on the approximation of continuous functions, with all relevant\n  quantities defined within the theorem or before, (d) moved the other\n  parts of the theorem later, and better modularized and pre-empted\n  them.\n\n- Regarding \"no clear separation or connection between intermediate\n  lemmas in later sections, and very little motivation and explanation\n  of some results\", after moving parts of the main theorem out and\n  pushing them to other sections, we then better modularized the\n  different results and sections, and included further explanations.  It\n  is for this reason that the paper is now longer.  As a concrete\n  example, Section 2, on sampling, now starts with a single main theorem\n  encapsulating the technique; before, this theorem was split into\n  parts, and much more unwieldy to apply.\n\n- Regarding \"many typos and inconsistent notations\", we have performed\n  extensive editing.\n\nWe hope the reviewer finds the presentation vastly improved, and\nappreciate further comments.\n", "title": "Response to AnonReviewer3."}, "H1edMNKV9r": {"type": "review", "replyto": "HklQYxBKwS", "review": "Summary: the paper consider representational aspects of neural tangent kernels (NTKs). More precisely, recent literature on overparametrized neural networks has identified NTKs as a way to characterize the behavior of gradient descent on wide neural networks as fitting these types of kernels. This paper focuses on the representational aspect: namely that functions of appropriate \"complexity\" can be written as an NTK with parameters close to initialization (comparably close to what results on gradient descent get).  \nThe main technical ingredients are a constructing a \"transport\" map via a Fourier-expansion style averaging (ala Baron), and subsequently subsampling this average ala Maurey-style analyses to get a finite width average. \nThe authors also identify function classes which are well-behaved with respect to these techniques: smoothed functions (via convolving with a Gaussian), functions which have a small RKHS norm (for an appropriate RKHS derived from NTKs), functions with small modulus of continuity. \n\nEvaluation: the paper is a strong contribution, on a topic which is of great current interest, and I recommend acceptance. It is very nice that many of the standard tools in approximation theory (Fourier expansions, Maurey sampling, etc.) play nicely with NTKs, and also that the scaling of the # of neurons necessary that appears in the current literature can be also recovered via a representation theoretic viewpoint. The paper is written well, and is easy to read. \n\nMinor comments: \n* I'd rearrange the bullets bounding B_{f,\\epsilon} for the various subcases of Theorem 1.3: I think the RKHS is the most \"vanilla\" bound, given that you can extract a RKHS; bounds in terms of the modulus of continuity should go next (this is the \"weakest\" assumption); smoothed f's should go last (this is like a smoothed complexity kind of result) \n* w_f isn't defined until section 3.2 -- I'd put a pointer in the statement of Theorem 1.3 to the equation, not just the section. \n* I'm not sure \"transport\" is the ideal term -- it brings to mind \"optimal transport\", and I kept expecting some Wasserstein connection. \n\n", "title": "Official Blind Review #2", "rating": "8: Accept", "confidence": 4}}}