{"paper": {"title": "VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation", "authors": ["Manoj Kumar", "Mohammad Babaeizadeh", "Dumitru Erhan", "Chelsea Finn", "Sergey Levine", "Laurent Dinh", "Durk Kingma"], "authorids": ["manojkumarsivaraj334@gmail.com", "mb2@uiuc.edu", "dumitru@google.com", "cbfinn@eecs.berkeley.edu", "slevine@google.com", "laurentdinh@google.com", "d.p.kingma@uva.nl"], "summary": "We demonstrate that flow-based generative models offer a viable and competitive approach to generative modeling of video.", "abstract": "Generative models that can model and predict sequences of future events can, in principle, learn to capture complex real-world phenomena, such as physical interactions. However, a central challenge in video prediction is that the future is highly uncertain: a sequence of past observations of events can imply many possible futures. Although a number of recent works have studied probabilistic models that can represent uncertain futures, such models are either extremely expensive computationally as in the case of pixel-level autoregressive models, or do not directly optimize the likelihood of the data. To our knowledge, our work is the first to propose multi-frame video prediction with normalizing flows, which allows for direct optimization of the data likelihood, and produces high-quality stochastic predictions. We describe an approach for modeling the latent space dynamics, and demonstrate that flow-based generative models offer a viable and competitive approach to generative modeling of video.", "keywords": ["Video generation", "flow-based generative models", "stochastic video prediction"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors explore the use of flow-based models for video prediction. The idea is interesting. The paper is well-written. It is a good paper worthwhile presenting in ICLR.\n\nFor final version, we suggest that the authors can significantly improve the experiments: (1) report results on human motion datasets; (2) include the results by the FVD metric. \n"}, "review": {"HJe8zpxjYr": {"type": "review", "replyto": "rJgUfTEYvH", "review": "This paper presents a stochastic model based on Glow for conditional video generation. The major novelty of this work is to introduce the flow-based models to video modeling and learn the video dynamics via the dependencies of the latent variables. The general idea is reasonable and the proposed model is technically correct, but I have the following concerns mainly about the originality and the experiments.  \n\n**Above all, most of the text in Section 3 is very similar (or exactly the same) to the text of the Glow paper (the background section).**\n\n\u2014 Significance and originality \u2014\na.1) In Section 1, the authors discussed some possible application scenarios of video prediction models, e.g. learning from unlabeled data and being used for downstream tasks. However, all models mentioned here, including [Mathieu et al. 2016] and [Finn et al. 2016], are deterministic models. Thus, in what way can the stochastic model proposed in this paper be used in real applications?\n\na.2) VideoFlow can be viewed as an extension of the Glow model. There are two problems. First, the originality is limited. I don\u2019t think modeling the temporal dependencies of the latent variables with a convolutional network is a significant contribution to the conditional flow-based methods. Second, this paper is not self-contained. After reading Section 4.2, I have to check the previous literature to find the objective function, the network details, or the training procedure.\n\n\u2014 Experiments \u2014\nb.1) Throughout the experiments, the VideoFlow model is mainly compared with two stochastic video prediction models that were probably proposed by the same research group. If it is possible, the authors might include other stochastic models such as the SVG-LP [Denton & Fergus 2018], and at least one deterministic model such as the E3D-LSTM [Wang et al. 2019] as well.\n[Wang et al. 2019] E3D-LSTM: A Model for Video Prediction and Beyond.\n\nb.2) The evaluation metric bits-per-pixel was not directly optimized by the previous video generation/prediction models. Thus, the comparisons in Table 2 might be unfair.\n\nb.3) Since training the Glow model requires a huge computational cost, how is the training efficiency of the VideoFlow model compared with other stochastic video generation models?\n\n\u2014 Other \u2014\nc.1) In Section 4, it is not clear what the temporal border effect means?\n\nAFTER REBUTTAL:\nThough the overall novelty is still not fully convincing, this paper may shed some insights into video generation by introducing flow-based models to this topic. I have increased my score from 3 to 6.", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 3}, "BJx9IOSisS": {"type": "rebuttal", "replyto": "HJe8zpxjYr", "comment": "We thank you for your reviews. We believe we have addressed your concern about the writing in the revision. Please find attached our detailed response below.\n\nExperiments\n-----------------\nb1) SVG-LP: As requested, we added results from SVG-LP [Denton & Fergus 2018], another strong baseline to Figure 4 and Figure 3 in our revision. VideoFlow either outperforms or is competent with SVG-LP on all the metrics used in the paper.\n\nDeterministic baseline: We evaluated two deterministic baselines CDNA [Finn et al, 2016] and EPVA [Wichers et al, 2017] on the BAIR Robot pushing dataset. The samples from the CDNA model were qualitatively better as compared to the samples from the EPVA model. So, we added results from [Finn et al, 2016] on multiple metrics to Figure 4 of our revision.\n\nb2) We believe we made a good-faith effort; We estimated the best-possible beta for the video-VAE\u2019s post training.  We also employed importance sampling using 100 samples from the posterior which gives a much tighter bound on the bits-per-pixel. Our goal was to measure how the VAE models perform out-of-the-box on a density estimation task.  In other words, in addition to being better / competent with the VAE approaches, our model also has the additional advantage of good likelihood numbers.\n\nb3) Our VideoFlow model reported in the paper has 45M parameters as compared to SVG-LP that has around 23M parameters. But, we performed the following experiments to make a convincing case.\n\n* We trained a smaller version of our model (VideoFlow small) with 12M parameters. We report our results in Section I of the appendix (VideoFlow: low parameter regime) in the revision. We are slightly better than SVG-LP on VGG perceptual metrics despite being 2x smaller.  We lose ~0.2 bpp as compared to VideoFlow large but our samples are still largely coherent. [2,3]\n\n* For VideoFlow small and VideoFlow large, we reported our results after 5 days and 2 weeks (600K steps) on 8 GPU\u2019s respectively. But we gain very little (around 0.04-0.05 bpp) between training our model for 200K and 600K steps.  We attached our bits-per-pixel on the validation set as a function of training steps over here which validates this claim [1]. We were able to generate high quality samples within 150-200K steps. We expect our results should be comparable or slightly worse if at all, when evaluated on a checkpoint at 200K steps. In comparison, the video-VAE models were trained between 2-3 days on 1 GPU.\n\nIn future, we could leverage improvements in normalizing flows to further close this gap.\n\nSignificance\n----------------\na.1) Good examples are [Hafner et al 2018] and [Kaiser at al 2018] where they report higher scores in numerous planning and reinforcement learning tasks by utilizing stochastic video prediction models. Also, [Nair et al 2018], leverage a stochastic video prediction model for self-supervision.\n\nIn short, a deterministic model cannot accurately model settings where there are multiple possible outcomes, (most real-life settings) and is obliged to predict a statistic of all the possible outcomes.\n\nOther\n--------\nWe replaced this with \u201cwithout introducing such artifacts\u201d in our latest version.\n\n\n[1] https://ibb.co/NNrwfGm\n[2] https://gifyu.com/image/v9Rp\n[3]  https://gifyu.com/image/v9RT", "title": "Response to AnonReviewer3 [1/2]"}, "HylEO9Hoir": {"type": "rebuttal", "replyto": "BJx9IOSisS", "comment": "Originality\n---------------\n\na.2)\n\nChanges made to the writing\n----------------------------------------\n\nWe agree that some parts of the paper assumes background knowledge of the multi-scale architecture used. We have made the following changes to our paper to alleviate that concern.\n\n* We added Section 4.1 \u201cInvertible Multi-Scale Architecture\u201d to our revision. In our new section, we briefly explain the multi-scale architecture. We describe the invertible transformations used in the multi-scale architecture and how the per-frame latent variables per level (scale) are inferred to make it more self-contained.\n\n* We added the second last paragraph under Section 4.2 where we explain how the invertible multi-scale architecture and the autoregressive latent dynamics model contribute to different parts of the objective.\n\n* We restructured Section 3, to describe the training objective.\n\nTechnical contributions\n---------------------------------\n\nWe believe we have made the following technical contributions via our paper:\n\n* A stochastic invertible flow-based model for video that is able to compute exact likelihoods, exact latent inference and fast sampling (as compared to autoregressive models). In addition to the above advantages, we provide extensive evaluations demonstrating that VideoFlow is either comparable or outperforms very strong baselines on a number of metrics. We qualitatively show latent-space interpolations demonstrating that VideoFlow learns meaningful latent representations [1] (Section 5.3).\n\n* We describe the first approach that scales the normalizing flow technique to video. Our model generates very high quality samples encouraging further investigation of flow-based models in the video generation literature.\n\n* We augmented a standard convolutional network with modifications in Section B of the appendix; we ablate these modifications demonstrating performance improvement in Section C of the appendix. We added a couple of lines at the end of Section 4.2 and provide a network diagram (Figure 8 in the appendix) to make our architecture clear.\n\n* We provide well documented code for other researchers to build on top of our work.\n\n[1] https://sites.google.com/view/videoflow/home#h.p_T96KYHZ0jtCQ", "title": "Response to AnonReviewer3 [2/2]: Originality"}, "Byxtvr4ssr": {"type": "rebuttal", "replyto": "rkgkwXVoir", "comment": "Q: If such a model is able to efficiently encode the dynamics and the stochasticity of the video. In fact, a given z_t does not encode any dynamics nor uncertainty at that point, only the image.\n\nOur hypothesis agrees with your insight. We hypothesize that the flow model by itself encodes the frame into meaningful latent codes that are state (frame) specific while the sequential model learns the dynamics of the video, i.e how the latent codes evolve over time. We provide additional insight using latent space interpolations between the first and last frame using the trained invertible flow encoder here [1]. We observe that the robotic arm and objects move in a smooth trajectory in between the first and last position. This indicates that the flow encoder encodes useful state-specific information such as the position of the robotic arm and background objects as latent codes.\n\nQ: Imagine that at a given point, two very different scenarios can follow, with very different following frames. In that case, how could the next state could encode these two different futures with a simple gaussian in the space ? \n\nLet us say that the flow encoder encodes meaningful information such as the position of the arm as latent codes. In that case, we can effectively model, the position of the robot arm at time T as a gaussian, whose mean and variance are parametrized by a network conditioned on the previous few positions. By doing this, we allow the sequential model to infer the dynamics of the video for e.g, velocity. The invertible flow model can then generate a high-quality frame using the latent code at time T.\n\nWe empirically show that this uncertainty in latent space as given by a gaussian prior translates to highly diverse trajectories in pixel space over multiple time-steps both qualitatively [2] and quantitatively (Figure 3 in our paper)\n\nWe can also control this uncertainty in latent space using temperature; indeed we show that when we reduce uncertainty / diversity in latent space, relates to reduced stochasticity and diversity in trajectories in pixel space [3].\n\n[1] https://sites.google.com/view/videoflow/home#h.p_T96KYHZ0jtCQ\n[2] https://sites.google.com/view/videoflow/home#h.p_qrclMoIvHzNC\n[3] https://sites.google.com/view/videoflow/home#h.p_NCseKczbThPX", "title": "Response to AnonReviewer [2/2]"}, "rkgkwXVoir": {"type": "rebuttal", "replyto": "SygDedWRtH", "comment": "We thank you for your reviews. Please find attached our response.\n\nQ: Clarity can be improved:\n\nWe added the following changes to our revision.\n1. We added network diagrams of the 3-D residual network used to model temporal dependencies (Figure 8) in the appendix, to assist the description of the sequential model in the appendix (Section B).\n2. We added Section 4.1, where we briefly explain the multi-scale architecture before moving on to the autoregressive latent dynamics model. We describe the invertible transformations used in the multi-scale architecture and how the per-frame latent variables per level (scale) are inferred.\n3. We added the second last paragraph under Section 4.2 that describes how the invertible multi-scale architecture and the autoregressive latent dynamics model contribute to different parts of the training objective.\n\nQ: Additional baseline and dataset\n\nAs requested, we added results from SVG-LP [Denton & Fergus 2018], another strong baseline to Figure 4 and Figure 3. VideoFlow either outperforms or is comparable to SVG-LP on all metrics in the paper. We also added results from CDNA [Finn et al. 2016], a strong deterministic baseline to our results in Figure 4.\n\nIn regard to our choice of the BAIR dataset for comparisons, it is a standard evaluation benchmark used in the stochastic video prediction literature. We believe the BAIR robot dataset is challenging due to its stochasticity i.e. there are multiple possible futures for the robot arm in the absence of supervision via actions as well as unknown physical properties of the objects. We agree that including experiments on larger and high resolution datasets would indeed be even more interesting to explore in future\n\nQ: Learning the invertible encoder and sequential model separately\n\nWe did attempt training the invertible encoder and sequential model separately in our initial experiments. We compared:\n1. Training the sequential model and the invertible flow encoder jointly. (our current version)\n2. Two stage training process:\n      Stage a): Pretraining the invertible flow encoder to model individual frames that provides stable latent representations.\n      Stage b): (Training the sequential model + Fine-tuning the flow encoder) on video.\n\nWe found out that after pre-training the invertible flow encoder (i.e 2a), (2b) does indeed converge faster as compared to 1.\nBut the total compute time of 2 (2a + 2b), was similar to (1). In addition this training scheme added increased complexity to our model, so we disbanded this after our initial efforts.\n\nQ: Impact of hierarchy depth\n\nWith a flow level of 1, our generated samples were able to capture the global structure of the robotic arm (for eg, a red blob).  This is similar to Fig 9 in [Kingma & Dhariwal, 2018], where a flow model with a lower number of levels of hierarchy captures global structure. We also show qualitatively in [1] and Section 5.3, that the latents at lower levels encode background objects as smaller scales while higher levels encode larger objects, such as the robotic arm.\n\nQ: .... The next code is conditioned by the whole past sequence of codes, besides the increasing complexity induced.....\n\nFor computational efficiency, we limit the history of the codes that we condition on to a window of 3 frames. We report this in Section 5.2 and the first paragraphs of Section 5.4. We empirically find that this sufficient to infer the dynamics of the dataset. This works quite well, but we do see that this Markovian assumption does have artifacts in the case of occlusions and obstructions that we report in Section 5.4 (Longer predictions)", "title": "Response to AnonReviewer [1/2]"}, "S1eSqUQojS": {"type": "rebuttal", "replyto": "HJetRuM0KH", "comment": "We thank for your reviews. Please find attached our response.\n\nLow temperature Sampling\n--------------------------------------\nAs suggested, in the updated version of the paper, we included experiments in which we applied low-temperature sampling to the latent gaussian priors of SV2P and SAVP-VAE. We report our results in Section D (Effect of Temperature on SAVP-VAE and SV2P ) of the appendix in the revision. We empirically find that decreasing temperature from 1.0 to 0.0 monotonically decreases the performance of the VAE models.\n\nOur insight is that the VideoFlow model gains by low-temperature sampling (upto a certain temperature) due to the following reason. By decreasing the temperature of the flow model, we trade-off between a performance gain by noise removal from the background and a performance hit due to reduced stochasticity of the robot arm.\n\nOn the other hand, the VAE models have a clear but slightly blurry background throughout from T=1.0 to T=0.0. Reducing T in this case, solely reduces the stochasticity of the arm motion thus hurting performance.\n\nReporting best vs mean\n---------------------------------\nIn the updated version paper, we added a summary of our response below to the sub-section \"Accuracy of the best sample\" in Section 5.2 to make this clear.\n\nThe BAIR dataset is highly stochastic and the number of plausible futures are high. Each generated video can be super realistic, can represent a plausible future in theory but can be far from the single ground truth video perceptually. The best values according to the PSNR, SSIM and VGG metrics from a finite number of samples is a proxy to help us understand if the ground truth can lie in the set of possible futures as per the model.\n\nConsider a hypothetical scenario where there are eight plausible but completely diverse future frames, such that the pairwise perceptual similarity between the frames ~= 0.0. Let us also consider the perfect model that is capable of generating each of these future frames accurately.\n\nIf we, compute the similarity of each sample with the ground truth video, and average this across multiple samples, we would get a similarity of ~ 12.5%. The \u201cmean\u201d in this case is not a useful statistic and the \u201cbest\u201d quantifies the performance of the stochastic model better. This metric should be also used in combination with the Amazon MTurk results in Figure 3 and Section 5.2 to assess if the generated videos are realistic.\n\nBAIR Robot dataset\n----------------------------\nIn regard to our choice of the BAIR dataset for comparisons, it is a standard evaluation benchmark used in the stochastic video prediction literature. [Babazeidah et al 2018, Lee et al 2018, Unterthiner et al. 2018, Denton & Fergus 2018, Weissenborn et al. 2019]. We believe the BAIR robot dataset is challenging due to its stochasticity i.e. there are multiple possible futures for the robot arm in the absence of supervision via actions as well as unknown physical properties of the objects. A network unable to model the stochasticity (e.g. a deterministic network) would blur the arm out in all possible directions. We agree that including experiments on larger and high resolution datasets would indeed be even more interesting to explore in future.", "title": "Response to AnonReviewer2"}, "SkgfX6HsjS": {"type": "rebuttal", "replyto": "rJgUfTEYvH", "comment": "Thanks everyone for the time and reviews. Here is a summary of changes made to the draft.\n\nExperiments\n------------------\n1. Added SVG-LP baseline to Figure 3 and Figure 4.\n2. Added low-temperature sampling results on the video-VAE models to Section D of the Appendix.\n3. Added CDNA, a deterministic video prediction baseline to Figure 4.\n4. Added comparison with SVG-LP on VGG perceptual metrics in the low-parameter regime to Section I of the appendix.\n\nWriting\n--------\n1. Added Section 4.1 \u201cInvertible multi-scale architecture\u201d that briefly summarizes the invertible flow architecture.\n2. Added Figure 8, network diagrams to assist the description of the residual network architecture in Section C of the appendix.\n3. Added a bit on the training objective (second last paragraph) of Section 4.2.\n4. Restructured Section 3.", "title": "Rebuttal"}, "SygDedWRtH": {"type": "review", "replyto": "rJgUfTEYvH", "review": "The paper \"VideoFlow: A Conditional Flow-Based Model for Stochastic Video \" proposes a new model for video prediction from a starting sequence of conditionning frames. It is based on a state-space model that encodes successive frames in a continuous hierarchical state, with contraints on trajectories of the codes in this state. \n\nI like the invertible NN framework the model relies on. It allows to avoid variational autoencoding of frames via invertible deterministic transforms. Learning the dynamics of the video is therefore easier, since there is no need of any stochastic inference process.    However, is there no risk of high latent vacancy in the representation space? Uncertainty of stochastic inference usually helps filling the space by considering larger areas of codes than deterministic process. Also, since at each step, the next code is conditionned by the whole past sequence of codes, besides the increasing complexity induced, I am wondering if such a model is able to efficiently encode the dynamics and the stochasticity of the video. In fact, a given z_t does not encode any dynamics nor uncertainty at that point, only the image (it cannot since it is fully determined via the invertible function from the image). Imagine that at a given point, two very different scenarios can follow, with very different following frames. In that case, how could the next state could encode these two different futures with a simple gaussian in the space ? Also,  it would be useful to compare the model with a version where the invertible frame encoder and the sequential model would be learned separately, to better understand what the model really does during training. A study of the impact of the hierarchy depth would also be useful.  \n\nAlso, an additional real-world dataset would be useful for really assessing the performance of the model, since BAIR is known to be fully random and the past does not highly impact the future. A possible dataset would be KTH. Other baselines could also be considered, notably the famous approach from  [Denton et al., 2017]. \n\nAt last, the clarity of some parts could be improved. Notably the description of the sequential model in the space, whih is succintly given in the appendix.   \n\n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "HJetRuM0KH": {"type": "review", "replyto": "rJgUfTEYvH", "review": "This paper extended the flow-based generative model for stochastic video prediction. The proposed model takes an advantage of the flow-based models which provide exact latent-variable inference, exact log-likelihood evaluation, and efficiency. The paper used the autoregressive model and the multi-scale Glow architecture. The experiments on the stochastic movement dataset (synthetic) and the BAIR Robot push dataset show the performance improvement against other state-of-the-art stochastic video generation models (SV2P and SAVP-VAE). \n\nThe main contribution in this paper is the use of flow-based models for video prediction, and it is the first work in this direction. The major idea sounds and the paper is clearly written. \n\nBelow is my concerns and the feedback. \n\nIt looks like the low-temperature sampling is important to achieve the better scores for prediction. Can the low-temperature sampling trick be applied for SV2P and SAVP-VAE as well? If then, how is the performance difference compare to the proposed model?\n\nThe authors reported the best possible values of PSNR, SSIM and VGG perceptual metrics by choosing the video closest to the ground-truth. However, I believe this evaluation does not present the benefit of the stochastic models. The better comparison I believe is to report the median/mean with the range between best and worst values. \n\nThe BAIR robot push dataset is with a pretty limited setting: a small robot and/or object motion between frames and a small variation of the background between videos. It would be interesting to see more dynamic scenarios such as driving or human motion scenes. ", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 4}}}