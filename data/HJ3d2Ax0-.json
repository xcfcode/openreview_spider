{"paper": {"title": "Benefits of Depth for Long-Term Memory of Recurrent Networks", "authors": ["Yoav Levine", "Or Sharir", "Amnon Shashua"], "authorids": ["yoavlevine@cs.huji.ac.il", "or.sharir@cs.huji.ac.il", "shashua@cs.huji.ac.il"], "summary": "We propose a measure of long-term memory and prove that deep recurrent networks are much better fit to model long-term temporal dependencies than shallow ones.", "abstract": "The key attribute that drives the unprecedented success of modern Recurrent Neural Networks (RNNs) on learning tasks which involve sequential data, is their ever-improving ability to model intricate long-term temporal dependencies. However, a well established measure of RNNs' long-term memory capacity is lacking, and thus formal understanding of their ability to correlate data throughout time is limited. Though depth efficiency in convolutional networks is well established by now, it does not suffice in order to account for the success of deep RNNs on inputs of varying lengths, and the need to address their 'time-series expressive power' arises. In this paper, we analyze the effect of depth on the ability of recurrent networks to express correlations ranging over long time-scales. To meet the above need, we introduce a measure of the information flow across time that can be supported by the network, referred to as the Start-End separation rank. Essentially, this measure reflects the distance of the function realized by the recurrent network from a function that models no interaction whatsoever between the beginning and end of the input sequence. We prove that deep recurrent networks support Start-End separation ranks which are exponentially higher than those supported by their shallow counterparts. Moreover, we show that the ability of deep recurrent networks to correlate different parts of the input sequence increases exponentially as the input sequence extends, while that of vanilla shallow recurrent networks does not adapt to the sequence length at all. Thus, we establish that depth brings forth an overwhelming advantage in the ability of recurrent networks to model long-term dependencies, and provide an exemplar of quantifying this key attribute which may be readily extended to other RNN architectures of interest, e.g. variants of LSTM networks. We obtain our results by considering a class of recurrent networks referred to as Recurrent Arithmetic Circuits (RACs), which merge the hidden state with the input via the Multiplicative Integration operation.", "keywords": ["recurrent neural networks", "deep networks", "correlations", "long term memory", "tensor networks", "tensor analysis"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "This paper attempts a theoretical treatment of the influence of depth in RNNs on their ability to capture dependencies in the data. All reviewers found the theoretical contribution of the paper interesting, and while there were problems raised regarding formalisation, they appear to have been adequately addressed in the revisions to the paper. The main concern in all three reviews surrounds the evaluation, and weakness thereof. The overarching point of contention seems to be that the theory relates to a particular formulation of RNNs (RAC), causing doubts that the results lift to other architectural variants which are used to obtain state-of-the-art results on tasks such as language modelling. It seems that the paper could be significantly improved by the provision of stronger empirical results to support the theory, or a more convincing argument as to why the results should transfer from, say, RAC to LSTMs. The authors point to two papers on the matter in their response, but it is not clear this is a substitute for experimental validation. I find the paper a bit borderline because of this, and recommend redirection to the workshop."}, "review": {"B1L-7zDgz": {"type": "review", "replyto": "HJ3d2Ax0-", "review": "This paper investigates an effect of time dependencies in a specific type of RNN.\n\nThe idea is important and this paper seems sound. However, I am not sure that the main result (Theorem 1) explains an effect of depth sufficiently.\n\n--Main comment\nAbout the deep network case in Theorem 1, how $L$ affects the bound on ranks? In the current statement, the result seems independent to $L$ when $L \\geq 2$. I think that this paper should quantify the effect of an increase of $L$.\n\n--Sub comment\nNumerical experiments for calculating the separation rank is necessary to provide evidence of the main result. Only a simple example will make this paper more convincing.", "title": "An effect of increase of $L$ should be evaluated.", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "SJl6_ov-G": {"type": "review", "replyto": "HJ3d2Ax0-", "review": "After reading the authors's rebuttal I increased my score from a 7 to a 6.  I do think the paper would benefit from experimental results, but agree with the authors that the theoretical results are non-trivial and interesting on their own merit.\n\n------------------------\nThe paper presents a theoretical analysis of depth in RNNs (technically a variant called RACs) i.e. stacking RNNs on top of one another, so that h_t^l (i.e. hidden state at time t and layer l is a function of h_t^{l-1} and h_{t-1}^{l})\n\nThe work is inspired by previous results for feed forward nets and CNNs. However, what is unique to RNNs is their ability to model long term dependencies across time. \n\nTo analyze this specific property, the authors propose a concept called \"start-end rank\" that essentially models the richness of the dependency between two disjoint subsets of inputs. Specifically, let S = {1, . . . , T/2} and E === {T/2 + 1, . . . , T}. sep_{S,E}(y) models the dependence between these two sets of time points. Specifically sep_{S,E}(y) = K means there exists g_s^k and g_e^k for k=1...K such that y(x) = \\sum_{k} g_s^k(x_S) g_e^k(x_E).\n\nTherefore sep_{S,E}(y) is the rank of a particular matricization of y (with respect to the partition S,E). If sep_{S,E}=1 then it is rank 1 (and would correspond to independence if y(x) was a probability distribution). A higher rank would correspond to more dependence across time. \n\n(Comment: I believe if I understood the above correctly, it would be easier to explain tensors/matricization first and then introduce separation rank, since I think it much makes it clearer to explain. Right now the authors explain separation rank first and then discuss tensors / matricization).\n\nUsing this concept, the authors prove that deep recurrent networks can express functions that have exponentially higher start/end ranks than shallow RNNs.\n\nI overall like the paper's theoretical results, but I have the following complaints:\n\n(1)  I have the same question as the other reviewer. Why is Theorem 1 not a function of L?  Do the papers that prove similar theorems about ConvNets able to handle general L? What makes this more challenging? I feel if comparing L=2 vs L=3 is hard, the authors should be more up front about that in the introduction/abstract.\n\n(2) I think it would have been stronger if the authors would have provided some empirical results validating their claims. \n\n", "title": "Interesting theory, could benefit from some experiments", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJpAdoezz": {"type": "review", "replyto": "HJ3d2Ax0-", "review": "The paper proposes to use the start-end rank to measure the long-term dependency in RNNs. It shows that deep RNN is signficantly better than shallow one in this metric. \n\nThe theory part seems to be technical enough and interesting, though I haven't checked all the details. The main concern with the paper is that I am not sure whether the RAC studied by the paper is realistic enough for practice. Certain gating in RNN is very useful but I don't know whether one can train any reasonable RNN with all multiplicative gates. The paper will be much stronger if it has some experiments along this line. ", "title": "review ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Hkb2oX6XG": {"type": "rebuttal", "replyto": "HJ3d2Ax0-", "comment": "In accordance with main comments raised by reviewers 2 and 4 we have uploaded a version of the paper that has a new subsection, enumerated 4.2. Our main result (theorem 1),  rigorously proves a lower bound on the Start-End separation rank of depth L=2 recurrent networks. This proved L=2 lower bound also trivially applies to all networks of depth L>2, and thus constitutes a first of its kind exponential separation in memory capacity between deep recurrent networks and shallow ones. In the added subsection, we present a quantitative conjecture by which a tighter, depth dependent, lower bound holds for recurrent networks of depth L>2. We formally motivate this conjecture by the Tensor Networks construction of deep RACs. We emphasize that the originally submitted version included the Tensor Networks construction of deep RACs (Appendices A1-A3), which yields the added conjecture in a straightforward manner, as described in a newly added appendix section A4. Beyond this, the paper kept its original form. \n\nThis addition, which meets central questions raised by the reviewers, outlines further insight that is achieved by our analysis regarding the dependence of the Start-End separation rank on depth, and poses further investigation of this avenue as an open problem. We believe that the presented novel approach for theoretical analysis of long term memory in recurrent networks, along with the solidly proved main results separating L=2 deep networks from L=1 shallow networks, constitutes an important contribution, which is well-supplemented by the formally motivated conjecture in the added section 4.2.", "title": "Uploaded revision with an added subsection "}, "BJCcFQTXG": {"type": "rebuttal", "replyto": "B1L-7zDgz", "comment": "We encourage the reviewer to examine our separate official comment regarding the upload of a paper revision, which addresses the dependence on L of the bounds.", "title": "Dependence on L is addressed"}, "S1XOyeaQf": {"type": "rebuttal", "replyto": "BJpAdoezz", "comment": "We thank the reviewer for the time and feedback. Our response follows. \n\nRACs are brought forth in our paper as a theoretical platform to investigate more common RNNs. The depth related effects studied in this paper depend on the recurrent network's structure rather than on specific activations. Empirical support for the specific RAC activations can be found in [1], which we mention in the paper. There, Multiplicative Integration Networks are shown to outperform common RNNs with additive integration. In section 3.1 they investigate RNNs with only multiplicative gates (in our terms - RACs) and find they preform comparably to vanilla RNNs in [2]. Furthermore, reference [1] shows evidence that under multiplicative integration the effect of squashing nonlinearities is diminished as they mostly operate in their linear regime, as opposed to the additive case where they are heavily influential (Fig. 1 (c),(d)). \n\nThus, there is a clear empirical validation addressing the reviewer's concern, that RACs can be trained to perform relevant sequential tasks. Our paper merely uses RACs as surrogates to common RNNs and does not propose to use them in practice - even though, as shown in empirical studies mentioned above, they can be used in practice.\n\nReferences\n___________________________\n[1] Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Ruslan R Salakhutdinov. On multiplicative integration with recurrent neural networks. In Advances in Neural Information Processing Systems, 2016.\n[2] David Krueger and Roland Memisevic. Regularizing rnns by stabilizing activations. arXiv:1511.08400, 2015.", "title": "Response to Reviewer 3"}, "rJvf61TXM": {"type": "rebuttal", "replyto": "SJl6_ov-G", "comment": "We thank the reviewer for considering our response and for supporting the paper.", "title": "Response to Reviewer 4"}}}