{"paper": {"title": "DCN+: Mixed Objective And Deep Residual Coattention for Question Answering", "authors": ["Caiming Xiong", "Victor Zhong", "Richard Socher"], "authorids": ["cxiong@salesforce.com", "richard@socher.org", "victor@victorzhong.com"], "summary": "We introduce the DCN+ with deep residual coattention and mixed-objective RL, which achieves state of the art performance on the Stanford Question Answering Dataset.", "abstract": "Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning, using rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we introduce a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state of the art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1.", "keywords": ["question answering", "deep learning", "natural language processing", "reinforcement learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This is an interesting paper that provides modeling improvements over several strong baselines and presents SOTA on Squad.  One criticism of the paper is that it evaluates only on Squad, which is somewhat of an artificial task, but we think for publication purposes at ICLR, the paper has a reasonable set of components."}, "review": {"HyeSGKwgf": {"type": "review", "replyto": "H1meywxRW", "review": "Summary:\nThis paper proposed an extension of the dynamic coattention network (DCN) with deeper residual layers and self-attention. It also introduced a mixed objective with self-critical policy learning to encourage predictions with high word overlap with the gold answer span. The resulting DCN+ model achieved significant improvement over DCN.\n\nStrengths:\nThe model and the mixed objective is well-motivated and clearly explained.\nNear state-of-the-art performance on SQuAD dataset (according to the SQuAD leaderboard).\n\nOther questions and comments:\nThe ablation shows 0.7 improvement on EM with mixed objective. It is interesting that the mixed objective (which targets F1) also brings improvement on EM. \n", "title": "Review", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkf-R8qlz": {"type": "review", "replyto": "H1meywxRW", "review": "This paper proposed an improved version of dynamic coattention networks, which is used for question answering tasks. Specifically, there are 2 aspects to improve DCN: one is to use a mixed objective that combines cross entropy with self-critical policy learning, the other one is to imporve DCN with deep residual coattention encoder. The proposed model achieved STOA performance on Stanford Question Asnwering Dataset and several ablation experiments show the effectiveness of these two improvements. Although DCN+ is an improvement of DCN, I think the improvement is not incremental. \n\nOne question is that since the model is compicated, will the authors release the source code to repeat all the experimental results?", "title": "An improvement of DCN model", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJ9UPyaeG": {"type": "review", "replyto": "H1meywxRW", "review": "The authors of this paper propose some extensions to the Dynamic Coattention Networks models presented last year at ICLR. First they modify the architecture of the answer selection model by adding an extra coattention layer to improve the capture of dependencies between question and answer descriptions. The other main modification is to train their DCN+ model using both cross entropy loss and F1 score (using RL supervision) in order to  reward the system for making partial matching predictions. Empirical evaluations conducted on the SQuAD dataset indicates that this architecture achieves an improvement of at least 3%, both on F1 and exact match accuracy, over other comparable systems. An ablation study clearly shows the contribution of the deep coattention mechanism and mixed objective training on the model performance. \n\nThe paper is well written, ideas are presented clearly and the experiments section provide interesting insights such as the impact of RL on system training or the capability of the model to handle long questions and/or answers. It seems to me that this paper is a significant contribution to the field of question answering systems. \n", "title": "Significant improvement of DCN answer selection models using mixed objectives and 2 stacked levels of coattention", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "SJIhjddQG": {"type": "rebuttal", "replyto": "Sk7GJVNZf", "comment": "We have performed another ablation study in which we train our DCN+ model without CoVe. This variant obtained 81.4% F1 on the dev set of SQuAD. This number also outperforms all other models studied in the paper, which suggests that our proposed changes to the original DCN are significant. For reference, DCN+ with CoVe obtained 83.1% on the dev set, and the test numbers tend to be a bit higher.", "title": "RE: RE: RE: Ablation without CoVe"}, "rkUPcK4mG": {"type": "rebuttal", "replyto": "HyeSGKwgf", "comment": "Thanks for your comments. In practice the F1 and EM metrics are closely correlated. We chose to use the F1 score as a metric because it offers fine grain signals as to how well the span predicted matches the ground truth span, whereas the EM score only rewards exact gloss matches.", "title": "RE: Review"}, "HyABctE7G": {"type": "rebuttal", "replyto": "rkf-R8qlz", "comment": "Thanks for your comments. We can try to release the source code after the decision process.", "title": "RE: An improvement of DCN model"}, "SkPEcKVQG": {"type": "rebuttal", "replyto": "rJ9UPyaeG", "comment": "Thank you for your comments!\n", "title": "RE: Significant improvement of DCN answer selection models using mixed objectives and 2 stacked levels of coattention"}, "HkzhLBVZf": {"type": "rebuttal", "replyto": "Skuf674Wf", "comment": "It is not our intention to characterize the other two datasets as bad. In fact, we think highly of the TriviaQA dataset and are investigating potential applications for it. We simply meant that each dataset has its own merits and drawbacks.\n\nFurthermore, it sounds like the anonymous reviewer thinks that the adversarial methods by Jia et al demonstrates the limitations of SQuAD. We disagree. We think that Robin's work rather demonstrates the limitations of state of the art reading comprehension models. In particular, we speculate that similar methods can be applied to the other datasets. Finally, to say that SQuAD models do not do reading comprehension is, in my opinion, unfair, and trivializes genuine hard work by the community.", "title": "RE: RE: RE:"}, "S1UPKfXbM": {"type": "rebuttal", "replyto": "BJfdokG-M", "comment": "Hi,\n\nYou are correct in that we only evaluate on the SQuAD dataset. In short, we agree with your sentiment that it would be interesting to evaluate on other datasets, however we respectfully disagree that SQuAD is known to be a bad dataset. In fact, we feel that it is one of the best datasets for reading comprehension. There seems to be agreement within the community about this in that\n\n1. SQuAD received the best resource paper award at EMNLP\n2. It is highly competitive, drawing significant participation from not only the authors' institution but other well-regarded academic and industry institutions (e.g. AI2, MSR (A), FAIR, CMU, Google, Stanford, Montreal, NYU ...).\n3. It has shown very useful downstream applications and impact (e.g. http://nlp.cs.washington.edu/zeroshot/)\n\nGiven these points, we feel that the performance gains afforded by our proposal (~3% F1) is significant, given that the top models on the leaderboard are within ~1% F1 of each other. We think these techniques are beneficial to the community at large.\n\nOf course, the fact that the other datasets do not (yet) have the above distinctions does not make them less interesting. We think that each dataset has its pros and cons. For example, TriviaQA is labeled via distant supervision. NewsQA  frankly has not been very popular (2 leaderboard submissions in ~1 year), and there seems to be concerns regarding its evaluation (see https://openreview.net/forum?id=ry3iBFqgl), though the authors seems to have made some enhancements since then. Given the higher popularity and competitiveness of SQuAD, we felt that it is a better choice on which to compare our proposal with the best models developed by the community.\n\nNevertheless, the two datasets you mentioned are either larger and longer (TriviaQA) or at least longer (NewsQA) than SQuAD. We will attempt to evaluate on one of these two datasets, however given the time constraints we are unlikely to be able to fine tune the model. I will update here with results once we obtain them.", "title": "RE: SQuAD evaluation"}, "H1UVzfmZf": {"type": "rebuttal", "replyto": "SJtORWmZM", "comment": "Hi,\n\nYou are correct in that CoVe does provide a significant performance gain, as demonstrated by McCann et al. (https://arxiv.org/abs/1708.00107). However, CoVe itself, when combined with the original DCN, does not obtain state of the art performance whereas this work does (please see Table 1 of our paper). In addition, we feel that the performance gain provided by deep residual coattention and mixed objective are significant (3.2% dev F1) given the competitive nature of the task. For reference on how significant a 3% F1 gain is, the top 5 state of the art models on the leaderboard are within ~1% dev F1 of each other.\n\nIn addition, CoVe is focused on the encoder of the model, whereas our work focuses on the coattention and the mixed objective. Our additions are applicable to other types of encoders as well.\n\nWe decided to perform ablation studies with respect to DCN with CoVe because it seemed like a natural foundation to build upon, but I agree with your sentiment that we should also evaluate our proposed additions without CoVe. We can try to perform this experiment and update here with the results.\n\nThanks!", "title": "RE: Ablation without CoVe"}, "rJdarOXyG": {"type": "rebuttal", "replyto": "H1meywxRW", "comment": "In Equation 17 (page 5), we made a typo in that we did not include the regularization terms $$\\log \\sigma_{ce}^2 + \\log \\sigma_{rl}^2$$.", "title": "Errata"}}}