{"paper": {"title": "Stochastic Neural Networks for Hierarchical Reinforcement Learning", "authors": ["Carlos Florensa", "Yan Duan", "Pieter Abbeel"], "authorids": ["florensa@berkeley.edu", "rocky@openai.com", "pieter@openai.com"], "summary": "We propose a framework for learning a diverse set of skills using stochastic neural networks with minimum supervision, and utilize these skills in a hierarchical architecture to solve challenging tasks with sparse rewards", "abstract": "Deep reinforcement learning has achieved many impressive results in recent years. However, tasks with sparse rewards or long horizons continue to pose significant challenges. To tackle these important problems, we propose a general framework that first learns useful skills in a pre-training environment, and then leverages the acquired skills for learning faster in downstream tasks.\nOur approach brings together some of the strengths of intrinsic motivation and hierarchical methods: the learning of useful skill is guided by a single proxy reward, the design of which requires very minimal domain knowledge about the downstream tasks. Then a high-level policy is trained on top of these skills, providing a significant improvement of the exploration and allowing to tackle sparse rewards in the downstream tasks. To efficiently pre-train a large span of skills, we use Stochastic Neural Networks combined with an information-theoretic regularizer. Our experiments show that this combination is effective in learning a wide span of interpretable skills in a sample-efficient way, and can significantly boost the learning performance uniformly across a wide range of downstream tasks.", "keywords": ["Deep learning", "Unsupervised Learning", "Reinforcement Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors provide an approach to policy learning skills via a stochastic neural network representation. The overall idea seems novel, and the reviewers are in agreement that the method provides an interesting take on skill learning. The additional visualization presented in response to the reviewer comments were also quite useful.\n \n Pros:\n + Conceptually nice approach to skill learning in RL\n + Strong results on benchmark tasks\n \n Cons:\n - The overall clarity of the paper could still be improved: the actual algorithmic section seems to present the methods still at a very high level, though code release may also help with this (it would not be at all trivial to re-implement the method just given the paper).\nWe hope the authors can address these outstanding issues in the camera ready version of their paper."}, "review": {"SyAvZcTTe": {"type": "rebuttal", "replyto": "SJSXTMUOe", "comment": "The code has been release at https://github.com/florensacc/snn4hrl\n\nFurthermore, the method section has been revised, and now includes more details to easily re-implement the algorithm. In particular, we have added an algorithm pseudo-code and further detailed the MI computation.", "title": "Code released and method section further detailed"}, "H14OnFVvg": {"type": "rebuttal", "replyto": "B1oK8aoxe", "comment": "Here we summarize the updates performed on the paper (uploaded):\n         \u2192 Use term \u201cProxy reward\u201d to refer to the reward in our pre-train environment.\n         \u2192 Videos added on: skill diversity of \u201cAnt\u201d, and \u201cSnake\u201d solving Maze 0 and Gather. (see https://goo.gl/5wp4VP )\n         \u2192 Sec. 2 (Related work): include Option-critic comparison.\n         \u2192 Sec. 5.3 (Information-theoretic regularization): clarify space discretization to compute the MI bonus.\n         \u2192 Sec. 6 (Experiments): clarifying agent/rest MDP factorization in the benchmark tasks for replicability.\n         \u2192 Sec. 7.3. (Results on Maze and Gather): further analysis on the observed solutions. See also new Appendix C.4.    \n         \u2192 Sec. 8 (Discussion and Future work): include end-to-end directions as well as limitations for unstable robots.\n         \u2192 Appendix C.1-C.2: report extensive experiments for more complex robot (5-link swimmer \u201cSnake\u201d): results are as strong as for 3-link swimmer (\u201cSwimmer\u201d).\n         \u2192 Appendix C.3: Switch time analysis shows the mild effect of the fixed number of time steps per sub-policy on learning performance.\n         \u2192 Appendix D studies limits of our approach for unstable robots (\"Ant\").\n\nWe thank all our reviewers for their great feedback that has made these improvements possible. We hope to have clarified and resolved most of the critical points.\nCode will be released upon publication.", "title": "Updates summary"}, "ryI1_Uh8l": {"type": "rebuttal", "replyto": "BkWsYu-Nx", "comment": "We thank the reviewer for the very valuable feedback and we discuss in the following our point of view on the insights provided, as well as the improvements performed on our paper.\n\nIndeed we think that our baseline of having the intrinsic reward in the sparse environments (Maze and Gather) might be hindered by not having a diversity encouragement, hence reducing the exploration (this insight has been added to Section 7.3). \nIt is a challenging open direction how to use diversity encouragement to improve performance in tasks with sparse rewards. Both the Hierarchical REPS framework (already cited in our related work) and the Option-critic one (now added) are very interesting on the option discovery side, but none shows a clear hierarchical use of the learned skills to improve learning across multiple tasks. On the other hand our two-phases algorithm manages to both unsupervisedly discover a variety of skills, and use them efficiently to improve learning in a collection of downstream sparse environments. Also, as noted in point (2) below, our method could be made end-to-end thanks to recent work on straight-through gradient estimations of stochastic computation graphs with discrete latents.\n           \u2192 Related work updated with Option-critic\n\nAs for the specific points that were brought up:\n\n(1) Apart from going in different directions, the gaits observed under different sub-policies do look quite diverse, as can be observed in the new experiments and animations in Appendix D with a more complex robot (\u201cAnt\u201d, described in Duan et al. 2016). We have not yet reached the limit of how diverse the sub-policies can be by just changing the first hidden weights of the architecture, and we are excited to try our approach on manipulation tasks given that the concept extends straightforwardly. Nevertheless, there is no established benchmark on such tasks, so it will require a better study of what are the hard problems that would benefit from skills hierarchy.\n\t  \u2192 Appendix D.1 and videos added on skill diversity of \u201cAnt\u201d\n\n(2) We agree that our current work is limited by having fixed sub-policies during the training of the downstream tasks. Nevertheless, we have added a discussion in  the Future work Section about how to use the very recent work (also ICLR submissions) on the Gumbel soft-max trick to obtain gradient estimations of stochastic computation graphs with discrete latents, like our Manager-NN + SNN. Therefore, end-to-end training of our architecture is possible, getting rid of the fixed sub-policy limitation. As for the fixed time steps for sub-policies, we show in the new Appendix C that, for the kind of task we consider, it is not a critical issue and not a hard hyper-parameter to tune. To go towards more dynamic tasks that require fast and precise reactions to changes in the environment, this might become a stronger limitation, but it should not be too complicated to learn a stopping policy concurrently with the Manager policy that dictates when to switch actions. We leave these very promising lines of work for future investigation.\n          \u2192 Future work Section updated with end-to-end directions.\n          \u2192 Switch time analysis added in Appendix C (effect of the fixed number of time steps per sub-policy on learning performance)\n\n(3) As discussed in more depth in the reply to Reviewer3, we agree that the intrinsic/unsupervised/proxy reward is indeed domain-specific. But the reward proposed is very easy to set up and is solely based on the robot and not the specifications of the hard downstream tasks. If what seems inappropriate is the terminology, as we wrote in response to Reviewer3, we believe switching our terminology to Proxy Reward could be the solution: in the initial training phase the agent does not have access to the more complex final environments, hence can\u2019t train directly against its final goals.  Instead the agent acts in a simpler environment and is provided with a \u201cproxy reward function\u201d catered towards letting it prepare for the more complex final environments.  What do you think about the \u201cproxy\u201d terminology?  ", "title": "Skills diversity for more complex robots + Future directions on end-to-end training"}, "SkyviX3Lx": {"type": "rebuttal", "replyto": "SJvLtKxNg", "comment": "We are grateful to the reviewer as these questions and criticisms have helped us improve the clarity of our paper. We detail here the modifications:\n\n(1) We have added Appendix D section on failure modes. This includes new experiments with the \u201cAnt\u201d robot (Duan et al. 2016), which can very explicitly fail by falling over.\n          \u2192 New Appendix D studies limits of our approach for unstable robots\n\n(2) We updated our explanations on space discretization in Section 5.3: \"We further partition the x-y coordinate space into discrete grids, and map the continuous-valued coordinate into the closest grid. This way, calculation of empirical mutual information only requires maintaining visitation counts of the latent variables in each grid.\u201d \nWe hope this clarifies that the discretization of the space is solely done to make the computation of the estimated MI more efficient. In particular, the discretization does not affect the MDP, which is fully continuous in actions and states. It is possible to avoid discretizing the space, in which case the MI estimation would require fitting a regressor at every iteration.\n          \u2192 Section 5.3 updated\n\n(3) We have tried more complicated embodiments. In particular, we have followed the suggestion of the reviewer and tested our approach on a 5-link swimmer (\u201cSnake\u201d).  \n          \u2192 We report extensive experiments in Appendix C: results are as strong as for 3-link swimmer (\u201cSwimmer\u201d).\n\n(4) We agree that the reward proposed in this paper is particularly suited for locomotion tasks. This still covers a large variety of control tasks, the hardest ones proposed in the benchmark paper by Duan et al. (2016).  Furthermore, although designed to help on future navigation problems, our pre-train task is very easy to set-up and satisfies our problems statement of not having direct access to any specific information of the downstream tasks (sensor readings, wall physics, goals,\u2026).", "title": "Strong results for 5-link robot (\"Snake\") + failure modes for unstable robot (\"Ant\")"}, "ryLRSGn8x": {"type": "rebuttal", "replyto": "B1s9wyM4l", "comment": "We thank the reviewer for the positive feedback. Based on the comments, we have improved the experiment description and analysis, as well as included in the new Appendix C a further study on the switching behavior of the agent. Next we treat point by point the specific requests, starting with a terminology discussion we would appreciate feedback on: \n\n(1) We agree that the use of \u201cintrinsic\u201d in our paper stretches the usual use of the term in RL, so we would love to discuss which is the most adequate terminology: In our work we demonstrate that, with a very simple speed reward and the use of the SNN architecture, we can learn useful skills for downstream tasks. We called this simple reward intrinsic as, although inspired by the navigation tasks that might come, it has no notion of the actual goal in there, and it can be understood as encouraging the robot to just \u201cmove for the sake of moving\u201d. Another possible term could be \u201cproxy reward\u201d -- as, in absence of the final environments during the initial training phase, the agent cannot directly train against the ultimate rewards in those final environments, hence is given \u201cproxy rewards\u201d in the meantime.  \n          \u2192 What do you think about using the \u201cproxy\u201d terminology instead?\n\n(2) The MI bonus is a powerful tool to consistently obtain a wide range of skills while training the SNN. Nevertheless, the Maze environment offers a way of turning that does not really require a different skill than going forward: the robot can use the wall to push itself and re-orient towards the goal. This explains why in some cases (like mazes 1, 2 and 3) the hierarchy given by the methods without MI bonus might be good enough.\n          \u2192 Further analysis on the observed solutions has been added to section 7.3.\n\n(3) Both the Maze (the one denoted by 0) and the Gather task are fully described in Duan et al. (2016), but they describe them as a monolithic task. In our work we just propose a natural break down of the state-space of these hard hierarchical tasks into S_agent (the robot) and S_rest (the walls, the sensor readings, the goals,\u2026). Actually, the swimmer locomotion task described in Duan et al. (2016) corresponds exactly to our pretrain task, as we also solely reward speed in a plain environment.\n          \u2192 The Section 6 has been updated clarifying this point.\n\n(4.1) The switching behavior of the agent is an interesting topic to which we have devoted more attention in the Appendix C: \nThe switching time (\u201cskill commitment length\u201d in the old version of the paper, now \u201cswitch time T\u201d has been adopted) effects on the learning performance are studied for a new robot, Snake, a higher dimensional version of Swimmer.  We show that for the tasks considered, the switching time barely affects performance! This is due to the tasks being basically static: none requires fast and precise reactions to changes in the environment. Therefore, the robot will just use the pre-learned skills in a different way. \n          \u2192 Appendix C.3 has been added \n\n(4.2) Failure modes are studied with  Ant, a more challenging robot with 27-dimensional observations and 8-dimensional actions. This robot is unstable in the sense that it might fall over and cannot recover. We show that despite being able to learn well differentiated skills with SNNs, switching between skills is a hard task because of the instability. This reveals a potential limitation of our method when applied without considering this issue. We also point at several future directions that should alleviate the problem.\n\t   \u2192 Appendix D (with Ant failure modes) has been added\n\t   \u2192 The Discussion and future work section now includes these points", "title": "Improved experiment description and analysis + switching behavior study"}, "Sk8ir5gEl": {"type": "rebuttal", "replyto": "BkwDoQyXg", "comment": "We have been able to scale the setup to more complex agents.\nFor the sake of including figures, please refer to this short report: \nhttps://docs.google.com/document/d/1Eif0KTelnk1CR4sAEhxHw4bg3OqUhflkkAkmkpXVaZs/edit?usp=sharing\n\nThese results will be integrated as an appendix to the current paper.", "title": "Performance on more complex environments"}, "SJlbE9e4g": {"type": "rebuttal", "replyto": "ByGNpwy7l", "comment": "- Hierarchical REPS: Their setup is very interesting but they only try with options that last one time-step and with environments considerably simpler than the ones considered in our paper. It will be interesting to see the scalability of their method. We now discuss the relationship in the paper, Related work Section 2).\n\n- Indeed we use the word \u201cintrinsic\u201d in a fairly general sense as we do use a specific subset of the state-space to give the motivation. Nevertheless, we disagree about this being \u201chand-specified rewards specific to domains and expected downstream tasks\u201d. The pre-training rewards used  are very mild modifications of the standard rewards given to these agents for encouraging locomotion, as can be seen in OpenAI Gym[https://gym.openai.com/envs#mujoco]. No tailored modifications for navigation in mazes, use of the sensors (adding 20 dimensions to the observations) or knowing the value of different parts of the downstream environments is used. As mentioned in our work, having access to the robot (the common sub-MDP of all downstream tasks) to pre-train locomotion is an assumption, and we believe our pre-train reward to be the least hand-specified among current/past work for such high-dimensional tasks. Nevertheless, we agree that it is less \u201cintrinsic\u201d than other intrinsic rewards seen in the literature (prediction-error based, full-state count, \u2026 ). If the reviewer opposes to the use of the term \u201cintrinsic\u201d, we can use the term \u201cunsupervised\u201d reward, given that this is the desired feature: be able to perform the pre-training and training steps without any expert demonstrations or highly instrumented setups.\n\n- The same setup could be extended to manipulation tasks. The same kind of simple reward based on motion (speed or displacement) could be used, but related to the objects instead. This is a promising future work. But indeed, our approach does rely on the \u201cprior knowledge\u201d that the robot has to move itself or something to somewhere.\n", "title": "Unsupervised reward"}, "S1m3RtgNg": {"type": "rebuttal", "replyto": "rJWWgkh7g", "comment": "Indeed, both observations are correct. This is now mentioned when describing the architectures and referring to Figure 1. Thanks for the insight!", "title": "Integration of latents in terms of changes in the first hidden layer."}, "Hkz40FgVx": {"type": "rebuttal", "replyto": "rkAv3J37l", "comment": "Question (adding index to questions): Clarification on experimental results\nComment: Sorry for some last minute questions. (a) What is the intuition for why the strong baseline in section 7.3 perform very poorly and can't solve the task? (b) Assuming \"CoM maze reward\"is this baseline. (c) Does the baseline also take into account the extra experience pre-trained policies have got? (d) What is multi-policy, as compared to snn?\n--------\nAnswer:\n\n(b) \u201cCoM maze reward\u201d is the baseline of having the same intrinsic motivation that the agent had in the pre-train task, but directly  in the downstream tasks (without any previous pre-training). It is in the spirit of the state-of-the-art methods that try to solve sparse tasks by guiding exploration with intrinsic rewards (Schmidhuber, 1991; 2010; Houthooft et al., 2016; Bellemare et al., 2016).\n\n(c) Therefore this baseline does not have any pre-training experience, as all other curves have. Nevertheless, the pre-training sampling cost (10 million time-steps for our swimmer SNNs) is only a fraction of the sampling cost of any of the downstream tasks (200 millions for Gather, 14 to 25 millions for Maze). Furthermore, the pre-training sampling cost is amortized among all downstream tasks as it is only done once.\n\n(a) The intuition of why the \u201cCoM reward\u201d performs quite poorly in Mazes is simply because of the lack of hierarchy. Only with the intrinsic motivation it is hard to reach the goal and long time-horizons are needed. This furthermore increases the problem of credit assignment. Note also that the reward signal is still weak as it encourages to move, but not necessarily towards the goal. For the Gather task, some extra discussion can be found in Appendix B.\n\n(d) The multi-policy is the hierarchical architecture that uses the one-hot vector -sampled from the categorical distribution parametrized by the manager neural network- to select which of the K policies to use during the following aggregated time steps (500 for the mazes, 10 for Gather). Note that it used 6 times more training data and has more parameters than our SNN architecture. Its description has been improved in Section 7.2.", "title": "Detailing the baselines"}, "ByGNpwy7l": {"type": "review", "replyto": "B1oK8aoxe", "review": "It is an interesting step toward hierarchical RL. Comments/questions:\n- Hierarchical REPS [C Daniel, G Neumann, J Peters, 2012] imposes a hard constraint on the entropy of conditional of option given state and action to encourage identifiability of skill/\u201doption\u201d, which is similar to your information-theoretic regularization. It is reasonable to discuss the differences. \n- The rewards for pre-training the SNN policy network are called intrinsic rewards/unsupervised rewards, but they seem hand-specified rewards specific to domains and expected downstream tasks. It is helpful if you cite the paper which defines intrinsics reward as you do when you give examples.\n- It would be good to see other examples of hierarchical RL besides locomotion + navigation. In locomotion + navigation, reward functions are easy to define for each stage, and most of the benefits seem to rely on the knowledge that if the agent has learned basic locomotion, any future navigation-related tasks will benefit. \n*Edited the score 6->7.\n\nThe paper presents a method for hierarchical RL using stochastic neural networks. The paper has introduced using information-theoretic measure of option identifiability as an additional reward for learning a diverse mixture of sub-policies. One nice result in the paper is the comparison with strong baseline which directly combines the intrinsic rewards with sparse rewards and shows that this supposedly smooth reward can\u2019t solve tasks. Besides the argument made from the authors on difficulty on long-term credit assignment/benefits from hierarchical abstraction, one possible explanation for this might be the diversity requirement imposed in sub-policy training, which is assumed to be off in the baseline case. Wonder if this can shed insights into improving the baseline and proposing new end-to-end hierarchical policy learning as hierarchical REPS/option-critic etc. papers do. Nice visualizations.\n\nThe paper presents a promising direction, and it may be strengthened further by possibly addressing some of the following points. \n\n1) Limited diversification of sub-policies: Both concatenation and bilinear integration allow only minimal differentiations in sub-policies through first hidden weight, which is not a problem in the tested tasks because they essentially require same locomotion policies with minimal diversification, but such limitation can be more obvious in other tasks where ideal sub-policies are more diverse. Thus it is interesting to see it apply on harder, non-locomotion domains, where ideal sub-policies are not that similar, e.g. for manipulation, solving some task from one state can be very different from solving it from another state. \n\n2) Limitation on hierarchical policies: Manager network is trained while the sub-policies are fixed. Furthermore, the time steps for sub-policies are fixed. This requires \u201cintrinsic\u201d rewards and their learned sub-policies to be very good for solving down-stream tasks. It would be nice to see some more discussions/results on handling such cases, ideally connecting to end-to-end hierarchical policy learning.\n\n3) Intrinsic/unsupervised rewards seem domain-specific/supervised rewards: Because of (2), this seems unavoidable. \n", "title": "preview question", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkWsYu-Nx": {"type": "review", "replyto": "B1oK8aoxe", "review": "It is an interesting step toward hierarchical RL. Comments/questions:\n- Hierarchical REPS [C Daniel, G Neumann, J Peters, 2012] imposes a hard constraint on the entropy of conditional of option given state and action to encourage identifiability of skill/\u201doption\u201d, which is similar to your information-theoretic regularization. It is reasonable to discuss the differences. \n- The rewards for pre-training the SNN policy network are called intrinsic rewards/unsupervised rewards, but they seem hand-specified rewards specific to domains and expected downstream tasks. It is helpful if you cite the paper which defines intrinsics reward as you do when you give examples.\n- It would be good to see other examples of hierarchical RL besides locomotion + navigation. In locomotion + navigation, reward functions are easy to define for each stage, and most of the benefits seem to rely on the knowledge that if the agent has learned basic locomotion, any future navigation-related tasks will benefit. \n*Edited the score 6->7.\n\nThe paper presents a method for hierarchical RL using stochastic neural networks. The paper has introduced using information-theoretic measure of option identifiability as an additional reward for learning a diverse mixture of sub-policies. One nice result in the paper is the comparison with strong baseline which directly combines the intrinsic rewards with sparse rewards and shows that this supposedly smooth reward can\u2019t solve tasks. Besides the argument made from the authors on difficulty on long-term credit assignment/benefits from hierarchical abstraction, one possible explanation for this might be the diversity requirement imposed in sub-policy training, which is assumed to be off in the baseline case. Wonder if this can shed insights into improving the baseline and proposing new end-to-end hierarchical policy learning as hierarchical REPS/option-critic etc. papers do. Nice visualizations.\n\nThe paper presents a promising direction, and it may be strengthened further by possibly addressing some of the following points. \n\n1) Limited diversification of sub-policies: Both concatenation and bilinear integration allow only minimal differentiations in sub-policies through first hidden weight, which is not a problem in the tested tasks because they essentially require same locomotion policies with minimal diversification, but such limitation can be more obvious in other tasks where ideal sub-policies are more diverse. Thus it is interesting to see it apply on harder, non-locomotion domains, where ideal sub-policies are not that similar, e.g. for manipulation, solving some task from one state can be very different from solving it from another state. \n\n2) Limitation on hierarchical policies: Manager network is trained while the sub-policies are fixed. Furthermore, the time steps for sub-policies are fixed. This requires \u201cintrinsic\u201d rewards and their learned sub-policies to be very good for solving down-stream tasks. It would be nice to see some more discussions/results on handling such cases, ideally connecting to end-to-end hierarchical policy learning.\n\n3) Intrinsic/unsupervised rewards seem domain-specific/supervised rewards: Because of (2), this seems unavoidable. \n", "title": "preview question", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkwDoQyXg": {"type": "review", "replyto": "B1oK8aoxe", "review": "The setting in this paper is very interesting but I would have liked the authors to discuss and report failure models. Especially as the richness of behaviors gets more complex, I expect this approach to have diversity issues with the skills learnt. Did the authors try the approach with an agent more complicated than the 3-link swimmer? I guess part of what I am wondering is whether the idea in this paper has been pushed to its limits. If it has, it would be good to know where it fails. \n\nAlso looking at Sec 5.3 -- \" let X be a random variable denoting the grid in which the agent is currently situated\" -- is the space discretized? And if so why and what happens if it isn't. I like the setting presented in this paper but I have several criticism/questions:\n\n(1) What are the failure model of this work? As richness of behaviors get complex, I expected this approach to have issues with the diversity of skills that could be discovered.\n\n(2) Looking at Sec 5.3 -- \" let X be a random variable denoting the grid in which the agent is currently situated\" -- is the space discretized? And if so why and what happens if it isn't. \n\n(3) Expanding on the first point, does the approach work with more complicated embodiment? Say a 5-link swimmer instead of 2? I think this is important to assess the generality of this approach\n\n(4) Authors claim that \"Recently, Heess et al. (2016) have independently proposed to learn a range of skills in a pre-training environment that will be useful for the downstream tasks, which is similar to our framework. However, their pre-training setup requires a set of goals to be specified. In comparison, we use intrinsic rewards as the only signal to the agent during the pre-training phase, the construction of which only requires very minimal domain knowledge.\"\n\nI don't entirely agree with this. The rewards that this paper proposes are also quite hand-crafted and specific to a seemingly limited set of control tasks. ", "title": "diversity and failure modes", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJvLtKxNg": {"type": "review", "replyto": "B1oK8aoxe", "review": "The setting in this paper is very interesting but I would have liked the authors to discuss and report failure models. Especially as the richness of behaviors gets more complex, I expect this approach to have diversity issues with the skills learnt. Did the authors try the approach with an agent more complicated than the 3-link swimmer? I guess part of what I am wondering is whether the idea in this paper has been pushed to its limits. If it has, it would be good to know where it fails. \n\nAlso looking at Sec 5.3 -- \" let X be a random variable denoting the grid in which the agent is currently situated\" -- is the space discretized? And if so why and what happens if it isn't. I like the setting presented in this paper but I have several criticism/questions:\n\n(1) What are the failure model of this work? As richness of behaviors get complex, I expected this approach to have issues with the diversity of skills that could be discovered.\n\n(2) Looking at Sec 5.3 -- \" let X be a random variable denoting the grid in which the agent is currently situated\" -- is the space discretized? And if so why and what happens if it isn't. \n\n(3) Expanding on the first point, does the approach work with more complicated embodiment? Say a 5-link swimmer instead of 2? I think this is important to assess the generality of this approach\n\n(4) Authors claim that \"Recently, Heess et al. (2016) have independently proposed to learn a range of skills in a pre-training environment that will be useful for the downstream tasks, which is similar to our framework. However, their pre-training setup requires a set of goals to be specified. In comparison, we use intrinsic rewards as the only signal to the agent during the pre-training phase, the construction of which only requires very minimal domain knowledge.\"\n\nI don't entirely agree with this. The rewards that this paper proposes are also quite hand-crafted and specific to a seemingly limited set of control tasks. ", "title": "diversity and failure modes", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}