{"paper": {"title": "An Efficient and Margin-Approaching Zero-Confidence Adversarial Attack", "authors": ["Yang Zhang", "Shiyu Chang", "Mo Yu", "Kaizhi Qian"], "authorids": ["yang.zhang2@ibm.com", "shiyu.chang@ibm.com", "yum@us.ibm.com", "kqian3@illinois.edu"], "summary": "This paper introduces MarginAttack, a stronger and faster zero-confidence adversarial attack.", "abstract": "There are two major paradigms of white-box adversarial attacks that attempt to impose input perturbations.  The first paradigm, called the fix-perturbation attack, crafts adversarial samples within a given perturbation level.  The second paradigm, called the zero-confidence attack, finds the smallest perturbation needed to cause misclassification, also known as the margin of an input feature.  While the former paradigm is well-resolved, the latter is not.  Existing zero-confidence attacks either introduce significant approximation errors, or are too time-consuming.  We therefore propose MarginAttack, a zero-confidence attack framework that is able to compute the margin with improved accuracy and efficiency.  Our experiments show that MarginAttack is able to compute a smaller margin than the state-of-the-art zero-confidence attacks, and matches the state-of-the-art fix-perturbation attacks.  In addition, it runs significantly faster than the Carlini-Wagner attack, currently the most accurate zero-confidence attack algorithm.", "keywords": ["adversarial attack", "zero-confidence attack"]}, "meta": {"decision": "Reject", "comment": "The paper proposes a new method for adversarial attacks, MarginAttack, which finds adversarial examples with small distortion and runs faster than the CW baseline, but slower than other methods. The authors provide theoretical guarantees and a broad set of experiments. \n\nIn the discussion, a consistent concern has been that, experimentally, the method does not perform noticeably better than previous approaches. The authors mention that the lines are too thick to reveal the difference. It has been pointed out that this might be related to the way the experiments are conducted, but the proposed method still does better than other methods. AnonReviewer1 mentions that the assumptions needed for the theoretical part might be too strong, meaning that the main contribution of the paper is in the experimental side. \n\nThe comparisons with other methods and the assumptions made in the theorems seem to have caused quite some confusion and there was a fair amount of discussion. Following the discussion session, AnonReviewer1 updated his rating from 5 to 6 with high confidence. \n\nThe referees all rate the paper as not very strong, with one marginally above acceptance threshold and two marginally below the acceptance threshold.  \n\nAlthough the paper seems to propose valuable ideas, and it appears that the discussion has clarified many questions from the initial submission, the paper has not provided a clear, convincing, selling point at this time. "}, "review": {"S1xcRXhFnQ": {"type": "review", "replyto": "B1gHjoRqYQ", "review": "i have change my rating from 5 to 6 after reading the numerous and thorough rebuttals from the authors. I hope they will incorporate these clarifications and additional experiments into the final version of the paper if accepted.\n\nThe purpose of this paper is presumably to approximate the margin of a sample as accurately as possible. This is clearly an intractable problem. Thus all attacks make some kind of approximation, including this paper. I am still a bit confused about the difference between \"zero-confidence attacks\" and those that don't fall into that category such as PGD. Since all of these are approximations, and we cannot know how far we are from the true margin, I don't see why these categories help. The authors spend two paragraphs in the introduction trying to draw a distinction but I am still not convinced. \n\nThe proofs provided by the authors assume that convexity and many assumptions, which makes it not very useful for the real world case. What would have been helpful is to show the accuracy of their margin for simple binary toy 2D problems, where the true margin and their approximation can be visualized. This was not done. This reduces the paper to an empirical exercise rather than a true understanding of their method's advantages and limitations.\n\nFinally, the experimental results do not show any significant advantage over PGD, either in running time (they are slower) or norm perturbation. Thus their novelty rests on the definition of zero confidence attack, and of the importance of such a attack. So clarifying the above question will help to judge the paper's novelty.\n\n", "title": "Unclear problem statement; mixed results", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SylK5GMm1V": {"type": "rebuttal", "replyto": "H1gqfh0GkV", "comment": "Thank you for following up. Regarding your two questions\n\n1. The updated results in this sub-thread are on CIFAR-10 and MNIST. The Dec 3rd comment is on ImageNet. Previously there have been discussions on the ImageNet results in the thread entitled 'Minor comments'. The Dec 3rd is the general response to these discussions.\n\n2. Here are more details on the runtime settings of both algorithms. CW comes with an option 'abort_early' and it has been turned on. This option will abort the iterations when the algorithm converges. This option will accelerate the algorithm without hurting the performance. On the other hand, we didn't implement a similar mechanism in MarginAttack, so MarginAttack will run all the way through the end even if it already converges. This places an advantage on CW. In spite of this, as shown in the results in this thread, MarginAttack still runs faster and more accurate than most of the settings in CW.\n\nIf the above discussion is not cogent enough, we have some results where the number of iteration is cut down to 200 (1/10 of the original number of iterations; binary search step set to 10):\n\nMNIST:\n\nPerturb. Lev.\tMargin2000\t        CW2000  \tMargin200    CW200\t      \n1\t\t        25.69\t\t\t24.86\t\t25.17\t\t23.87\n1.41\t\t        66.34\t\t\t63.23\t\t64.99\t\t60.60\n1.73\t\t        88.40\t\t\t85.94\t\t87.28\t\t84.25\n2\t\t        97.11\t\t\t95.42\t\t96.51\t\t94.11\n\nCIFAR10:\n\nPerturb. Lev.\tMargin2000   \tCW2000  \tMargin200     CW200\n8\t\t        24.27\t\t\t24.04\t\t24.31\t\t23.86 \n15\t\t        46.37\t\t\t45.56\t\t45.63\t\t44.98\n25\t\t        73.82\t\t\t71.80\t\t71.91\t\t70.95\n40\t\t        93.29\t\t\t92.10\t\t91.71\t\t91.02\n\nAs can be seen, even Margin200 can outperform CW2000 in most of the scenarios (except for CIFAR10 perturbation level 40). Hope this is cogent enough to show the improved accuracy-efficiency tradeoff of MarginAttack.\n", "title": "Regarding updated results and running time comparison"}, "ryeWe50MkV": {"type": "rebuttal", "replyto": "B1gHjoRqYQ", "comment": "With the help of the useful discussions in https://github.com/tensorflow/cleverhans/issues/813, we are able to get the CW ImageNet results right. We would like to update the results as follows:\n\nPerturb. Lev.\tMarginAttack\tCW bin5\n10\t\t        40.42\t\t\t40.36\n32\t\t        60.59\t\t\t58.71\n50\t\t        74.89\t\t\t70.99\n80\t\t        89.43\t\t\t85.64\n\nThis table and a continuous curve will replace the original results in the paper.", "title": "Updated CW results on ImageNet"}, "rklgf6ffyN": {"type": "rebuttal", "replyto": "HJlxfhp1J4", "comment": "Yes. Each binary search step setting comes with a separate step size tuning.", "title": "Yes"}, "S1loX4IlA7": {"type": "rebuttal", "replyto": "B1xOut6DpQ", "comment": "Regarding your first concern on the comparison with CW: In short, MarginAttack is able to achieve a higher attack success rate than CW AND a shorter running time. The paper may not make this point obvious enough probably because the curves are too thick to reveal the difference. To show this point clearly, we would like to refer you to the results in our response to reviewer 3, where we scanned through the number of binary search steps and measure the success rate and running time.\n\nAs can be seen, MarginAttack has a higher success rate than all the versions of CW. There is a success-rate-efficiency tradeoff in CW, as a smaller binary search step number leads to a lower success rate. However, even with 10 binary search steps, CW is still unable to outperform MarginAttack in terms of success rate. On the other hand, with very small numbers of binary search steps, CW still runs slower than MarginAttack. Hope these results will clarify your major concern.\n\nRegarding your minor concern:\n\nIn the theorem, we did not assume convexity. The assumption with the name 'convexity' is saying that the constraint set should not be 'too concave'. Please check the following figure where we listed what decision boundaries are permitted by our theorem and what not.\n\nhttps://docs.google.com/viewer?url=https://raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure2.pdf\n\nAs can be seen, the convexity assumption permits a wide variety of decision boundaries. Among the few cases that it does not permit is the case where the decision boundary bends more than the L2 ball does. In this case, the critical point becomes a local maximum rather than a local minimum.\n\n", "title": "Comparison with CW & We did not assume convexity"}, "SyxZ5bIxRX": {"type": "rebuttal", "replyto": "ByxzpzkyT7", "comment": "Thanks to the hyperparameter tuning suggestions, we are able to achieve a set of better results on CiFAR10. The updated results are as follows:\n\nAttack success rate:\n\nPerturb. Lev.\tMarginAttack\tCW bin10\tCW bin5\t        CW bin3\t       CW bin1\n8\t\t        24.27\t\t\t24.04\t\t23.99\t\t23.89             15.14\n15\t\t        46.37\t\t\t45.56\t\t45.39\t\t45.21             20.53\n25\t\t        73.82\t\t\t71.80\t\t71.66\t\t71.56             20.57\n40\t\t        93.29\t\t\t92.10\t\t91.86\t\t91.49             20.57\n\nRunning time:\nMarginAttack\tCW bin10\tCW bin5\t        CW bin3\t        CW bin1\n51.03\t\t\t350.10\t\t168.88\t\t100.10             24.30\n\nCompared to the previous results posted, the attack success rate of CWbin10 is almost the same, but the results of CW with fewer binary steps are improved. The basic conclusions do not change though. Notice that increasing the number of binary search steps does help to improve the success rate, but even compared with 10 binary search steps, MarginAttack still maintains a higher success rate at all levels. In the meantime, MarginAttack has a much lower running time, and thus strikes a better success-rate-balance-tradeoff.", "title": "Updated results on CiFAR10"}, "HJlyTzMqpQ": {"type": "rebuttal", "replyto": "ByxzpzkyT7", "comment": "Thank you for bringing up the binary search issue. We would like to clarify that the binary search is an integral part of the CW attack and that it cannot be replaced with hyperparameter tuning beforehand. This is because the purpose of the binary search is to find the Lagrange multiplier for the Lagrangian, which is specific to *each individual token*. In other words, each different input sample comes with a different optimal Lagrange multiplier. Therefore, it is impossible to tune a universal Lagrange multiplier and remove the binary search. The CW algorithm can be regarded as a two-way optimization problem. For each sample, it first optimizes over the Lagrange multiplier via binary search, and then optimizes over the adversarial sample via gradient descent. In short, the Lagrange multiplier is technically not a hyperparameter, but an optimization variable just like the adversarial sample itself.\n\nThe CW implementation does come with a set of hyperparameters that it asks the users to tune, including the initial Lagrange multiplier guess and the initial step size, both of which are already tuned to its best performance.\n\nNevertheless, although the binary search cannot be removed, we are interested to see what will happen is it is reduced. For this we perform an additional experiment where the number of binary search steps is reduced to 5 (named CW bin5), 3 (named CW bin3) and 1(named CW bin1) on MNIST and CIFAR10. Below are the attack success rates under different perturbation levels.\n\nMNIST:\n\nPerturb. Lev.\tMarginAttack\tCW bin10\tCW bin5\t        CW bin3\t       CW bin1\n1\t\t        25.69\t\t\t24.86\t\t24.82\t\t24.63\t\t9.94\n1.41\t\t        66.34\t\t\t63.23\t\t63.11\t\t62.72\t\t9.98\n1.73\t\t        88.40\t\t\t85.94\t\t85.90\t\t85.66\t\t9.99\n2\t\t        97.11\t\t\t95.42\t\t95.36\t\t95.25\t\t9.99\n\nCIFAR10:\n\nPerturb. Lev.\tMarginAttack\tCW bin10\tCW bin5\t        CW bin3\n8\t\t        24.27\t\t\t24.11\t\t24.02\t\t9.93\n15\t\t        46.37\t\t\t45.52\t\t45.22\t\t13.95\n25\t\t        73.82\t\t\t71.76\t\t70.92\t\t22.26\n40\t\t        93.29\t\t\t91.61\t\t90.65\t\t34.48\n\nBelow is the attack time.\n\nMNIST:\n\nMarginAttack\tCW bin10\tCW bin5\tCW bin3\tCW bin1\n3.01\t\t\t        16.02\t\t8.99\t\t5.77\t\t1.37\n\nCIFAR10:\n\nMarginAttack\tCW bin10\tCW bin5\t        CW bin3\n51.03\t\t\t234.75\t\t102.68\t\t33.98\n\nAs can be seen, the performance does drop as the number binary search steps decreases. In particular, the algorithm completely fails when the binary search step number drops below a certain threshold (1 for MNIST and 3 for CIFAR10). Upon failure threshold, there is an unproportional drop in running time, which is probably due to the early stop mechanism in CW. We conjecture that the threshold is higher when the dataset has greater variations. These results provide more complete evidence on how MarginAttack is able to achieve a much better accuracy-efficiency tradeoff than CW. We will add these results to the paper.\n\nHope this clarification helps.", "title": "Regarding comparison with CW attack"}, "ryl9jbfcpm": {"type": "rebuttal", "replyto": "ByxzpzkyT7", "comment": "First we would like to clarify that the learning rate of PGD is tuned the same way as for CW. We somehow missed this statement in the paper. We will add this statement back to the paper.\n\nSecond, yes it is entirely possible to convert PGD to a zero-confident attack. In our response to another reviewer, we estimated the computational overhead. We will copy the analysis here. Consider, for example, the CIFAR-10 dataset. Since for our model, most margins fall within 10, so let\u2019s assume the binary search range is 10 (for adversarially trained models this number will be much higher). If we want to achieve an accuracy of 0.1, then we need at least 7 binary search steps. In other words, the computation complexity increases by 7 times. The above discussion is not saying that it is impossible to convert PGD to a zero-confidence attack efficiently, but it at least provides a perspective on why the complexity reduction as well as accuracy improvement of MarginAttack is valuable.\n\nFinally, we would like to point out that while PGD is a state-of-the-art in L-infinity attack. It is not in L2 attack. One of the reasons is that PGD alternatively projects onto the constraint box and L2 ball, which is not equivalent to projecting onto the intersect of both. In L-infinity attack they are equivalent. The following link is a figure that provides an illustration on this.\n\nhttps://docs.google.com/viewer?url=https://raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure3.pdf\n\nThat is the reason why we did not incorporate PGD L2 in our comparison. However, we would like to provide the results here.\n\nMNIST:\n\nPerturb. Lev.\tMarginAttack\tPGD L2\n1\t\t        25.69\t\t\t12.53\n1.41\t\t        66.34\t\t\t37.11\n1.73\t\t        88.40\t\t\t64.13\n2\t\t        97.11\t\t\t81.68\n\nCIFAR10:\n\nPerturb. Lev.\tMarginAttack\tPGD L2\n8\t\t        24.27\t\t\t12.94\n15\t\t        46.37\t\t\t25.30\n25\t\t        73.82\t\t\t46.90\n40\t\t        93.29\t\t\t57.13\n\nIMAGENET:\n\nPerturb. Lev.\tMarginAttack\tPGD L2\n10\t\t        40.42\t\t\t29.45\n32\t\t        60.59\t\t\t40.73\n50\t\t        74.89\t\t\t50.80\n80\t\t        89.73\t\t\t66.35\n\nHope the above clarifications help.", "title": "Regarding the comparison with PGD"}, "SkxZXWf9pm": {"type": "rebuttal", "replyto": "ByxzpzkyT7", "comment": "First, regarding the white box attack definition. Yes, the white box attack is understood as having access to all network information, including structure and parameters. So it is possible to compute gradient information. Black box attacks only have access to logits or only decision, so it is not possible to accurately compute the gradient information. We will make this distinction clearer.\n\nSecond, the PGD convergence guarantee we meant is only about local convergence. Under mild assumptions, PGD is able to converge to a critical point of the PGD loss function, where no feasible direction can increase the loss function. We will clarify this in our updated version.\n\nThird, by a \u2018more realistic attack\u2019, we meant that under a true attack setting, an attacker would not confine himself to a fix perturbation, but is more likely to keep attacking until success, while minimizing perturbation.\n\nFourth, we will correct our statement about the earliest work that incorporates gradient information into adversarial attack.\n\nFinally, we will change the norm notation.", "title": "Regarding your other reviews"}, "B1ljclMcpm": {"type": "rebuttal", "replyto": "S1xcRXhFnQ", "comment": "Although this is not the major focus of your comment, we would like to revisit the theorem assumptions. While there are nine assumptions, these assumptions are in fact more realistic than expected. Take the convexity assumption, which you mentioned in your review, as an example. This assumption does not say that the constraint has to be convex. It only says that the constraint should not be \u2018too concave\u2019. In particular, the curvature of the of the decision should not exceed that of the L2 or L-infinity ball. For better illustration, we plotted some decision boundaries that are allowed by the assumption, and some that are not.\n\nPlease check the following link:\n\nhttps://docs.google.com/viewer?url=https://raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure2.pdf\n\nAs can be seen, the convexity assumption permits a wide variety of decision boundaries. Among the few cases that it does not permit is the case where the decision boundary bends more than the L2 ball does. In this case, the critical point becomes a local maximum rather than a local minimum.\n\nThe other assumptions are also more realistic than their names sound. The differentiability assumption does not stipulate that the constraint has to be differentiable. It actually permits countably infinite jump discontinuities. The Lipchitz continuous assumption does not assume Lipchitz continuity everywhere, but only at x*. We are not saying that the assumptions are very loose, but they are realistic enough to shed some light on the actual convergence property of MarginAttack. Nevertheless, we are considering adding a 2D toy example as you suggested. We will post further responses if there are further updates.", "title": "Regarding the theorem assumptions"}, "r1eYIxfqT7": {"type": "rebuttal", "replyto": "S1xcRXhFnQ", "comment": "The following link is a figure that explains the difference between zero-confidence attack and fix-perturbation attack.\n\nhttps://docs.google.com/viewer?url=https://raw.githubusercontent.com/anon181018/iclr2019_rebuttal/master/figure1.pdf\n\nAs can be seen, the zero-confidence attack finds the closest point on the decision boundary; while fix perturbation-attack finds adversarial samples within a fix perturbation. Both attacks are equivalent if we only want to compute the attack success rate under a given perturbation level. However, we will be better off with zero-confidence attacks if we want to\n\n1) Compute the margin of each individual example; and\n2) Probe and study the decision boundary of a classifier\n\nOf course, we can also measure the margin of each example using a fix-perturbation attack, for example PGD, by binary searching over the perturbation levels. However, the computation cost will significantly increase. Consider, for example, the CIFAR-10 dataset. Since for our model, most margins fall within 10, so let\u2019s assume the binary search range is 10 (for adversarially trained models this number will be much higher). If we want to achieve a accuracy of 0.1, then we need at least 7 binary search steps. In other words, the computation complexity increases by 7 times. In fact, CW applies a similar binary search idea to achieve zero-confidence attack, and that is why its computation cost is high. The above discussion is not saying that it is impossible to convert PGD to a zero-confidence attack efficiently, but it at least provides a perspective on why zero-confidence attack is challenging, and why the complexity reduction as well as accuracy improvement of MarginAttack is valuable.", "title": "Zero-Confidence vs Fix-Perturbation"}, "B1xOut6DpQ": {"type": "review", "replyto": "B1gHjoRqYQ", "review": "This paper proposes an efficient zero-confidence attack algorithm, MARGINATTACK, which uses the modified Rosen's algorithm to optimize the same objective as CW attack. Under a set of conditions, the authors proved convergence of the proposed attack algorithm. My main concern about this paper is why this algorithm has a better performance than CW attack? I would suggest comparing with CW attack under different sets of hyper-parameters.\n\nMinor comment:\nThe theoretical proof depends on the convexity assumption, I would also suggest comparing the proposed attack with CW and other benchmarks on some simple models that satisfy the assumptions.", "title": "I cannot see why the proposed method is better than CW attack", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ByxzpzkyT7": {"type": "review", "replyto": "B1gHjoRqYQ", "review": "The authors propose a new method for constructing adversarial examples called MarginAttack. The method is inspired by Rosen's algorithm, a classical algorithm in constrained optimization. At its core, Rosen's algorithm (instantiated for adversarial examples) alternates between moving towards the set of misclassified points and moving towards the original data point (while ensuring that we do not move too far away from the set of misclassified points). The authors provide theoretical guarantees (local convergence) and a broad set of experiments. The experiments show that MarginAttack finds adversarial examples with small distortion (as good as the baselines or slightly better), and that the algorithm runs faster than the Carlini-Wagner (CW) baseline (but slower than other methods).\n\nThe authors make a distinction between \"fixed perturbation\" attacks and \"zero confidence\" attacks. The former finds the strongest attack within a given constrained set, while the latter finds the smallest perturbation that leads to a misclassification. Method such as projected gradient descent fall into the \"fixed perturbation\" category, while MarginAttack and CW belong to the \"zero confidence\" category. The authors claim that zero confidence attacks pose a harder problem and hence mainly compare their experimental results to the CW attack. Indeed, their results show that MarginAttack is 3x - 5x faster than CW and sometimes achieves smaller perturbations.\n\nFirst of all, I would like to emphasize that the authors conducted a thorough experimental study on multiple datasets using multiple baseline algorithms. Unfortunately, the comparison to CW and PGD still leaves some questions in my opinion:\n\n- The authors state that CW does an internal binary search over the Lagrangian multiplier, and that this search goes for up to 10 steps. As a result, it is not clear whether the running time benchmarks are a fair comparison since MarginAttack does not automatically tune its parameters. To the best of my knowledge, the CW implementation in Cleverhans is specifically set up so that the user does not need to tune a large number of hyperparameters (the implementation accepts a running time overhead to achieve this). Since MarginAttack also contains multiple hyperparameters (see Table 4), it would be interesting to see how the running time of MarginAttack compares to that of a tuned CW implementation without the binary search.\n\n- The authors explicitly state that the step sizes for CW were tuned for best performance, but do not mention this for PGD. For a fair comparison, the step sizes used for PGD should also be (approximately) tuned. Moreover, it is not clear why PGD is only used for an l_inf comparison and not a l_2 comparison.\n\n- In the introduction, the authors emphasize the distinction between fixed perturbation attacks and zero confidence attacks. However, from an optimization point of view, these two notions are clearly related and a fixed perturbation attack can be converted to a small perturbation / zero confidence attack via a binary search over the perturbation size. While one would indeed expect an overhead due to the binary search, it is not clear a priori how large this overhead needs to be to achieve a competitive zero confidence attack with PGD (especially with a tuned step size for PGD, see above).\n\nI would be grateful if the authors could provide their view on these points. Until then, I will assign a rating of 5 since tuning the parameters of optimization algorithms is crucial for a fair comparison.\n\n\nAdditional comments:\n\n- In the introduction, the authors equate white-box attacks with access to gradient information. But generally a white-box attack is understood as an attack that has arbitrary access to the target network. It may be helpful for the reader to clarify this.\n\n- In the second paragraph of the introduction, the authors claim that fixed perturbation attacks and zero confidence attacks differ significantly. But as pointed out above, it is possible to convert a fixed perturbation attack to a zero confidence attack via a binary search. So it is not clear that there is a large gap in difficulty. Moreover, the authors state that fixed perturbation attacks often come with theoretical guarantees. But to the best of my knowledge, there is no comprehensive theory that describes when a fixed perturbation attack should be expected to succeed in attacking a commonly used neural network.\n\n- On top of Page 2, the authors claim that zero-confidence attacks are a more realistic attack setting. Why is that?\n\n- The authors state that JSMA (Papernot et al., 2016) is one of the earliest works that use gradient information for constructing adversarial examples. However, L-BFGS as employed by Szegedy et al., 2013 also uses gradient information. Moreover, the authors may want to cite the work of Biggio et al. from 2013 (see the survey https://arxiv.org/abs/1712.03141).\n\n- Since all distances referred to by d(x, y) seem to be norms (and the paper relies on the existence of dual norms), it may be more clear for the reader to use the norm notation || . || from the beginning.", "title": "Claims to be significantly faster than the CW attack, but I have some questions about the experiments", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1lSn2GUjm": {"type": "rebuttal", "replyto": "r1x5mtzSiX", "comment": "Just posted the code in the issue.", "title": "Code posted"}, "BkgN7aTzjQ": {"type": "rebuttal", "replyto": "Bkgn9VnC5Q", "comment": "Thanks for the reminder! Upon checking we confirm that we used the pre-softmax logits.\n\nWe will keep on our efforts to ensure the results are right. If you would like to share your configuration details at any time, it is always welcomed. Thanks again for initiating the discussion!", "title": "We used pre-softmax logits"}, "S1gn4AtA5Q": {"type": "rebuttal", "replyto": "SJxqX98Ccm", "comment": "No worries. We appreciate your comments and would like to ensure our results are right.\n\nBefore we discuss the CW settings, we would like to first clarify that the '2.2e-7' I mentioned is not our results. It is the results reported in the decision attack paper (https://openreview.net/forum?id=SyZI0GWCZ from ICLR last year) that you previously mentioned. They also find CW performs worse than DeepFool. According to my understanding and guess, this should be the per-pixel squared distance and the pixel range is [0, 1]. Therefore converting it to the regular L2 distortion should yield around 50. Again, this is only our interpretation of their results. We would need to consult the authors of that paper to confirm it.\n\nWe are interested in your CW results and would like to know more about how you configured the attack. Did you use CleverHans? If yes, how did you set the configuration parameters?\n\nTo be transparent, here are our CleverHans settings for CW:\n\ncw_params = {'binary_search_steps': 10,\n                         'y': l,\n                         'max_iterations': 2000,\n                         'learning_rate': 0.01 (also tried 0.001, 0.05 and 0.1),\n                         'batch_size': 100,\n                         'initial_const': 0.1 (also tried 0.01),\n                         'clip_min': 0,\n                         'clip_max': 255,\n                         'abort_early': False (also tried True)}\n\nWe scanned over the candidate settings and the results we reported were the best. Please let us know if you find anything problematic. We will be happy to make it right.", "title": "CW settings"}, "SkgEYamC5m": {"type": "rebuttal", "replyto": "SJxJd84T9m", "comment": "Thank you for your interests in our work! Regarding your comments:\n\n1. Yes, the idea of 'crawling along the decision boundary' is related to the L2 version of MarginAttack (Eq. (8)), and serves as a good reference if a black-box version of MarginAttack is to be developed. So we will add this paper to our reference. Partly because it is a white-box attack, MarginAttack does not have to wait until it reaches the decision boundary before it moves along the decision boundary, which is shown to significantly improve convergence both empirically and theoretically. Also, MarginAttack encompasses much richer attack schemes, because the L-infinity version of MarginAttack (Eq. (10)), as well as other valid settings of a_k and b_k, follows a different projection move direction from along the decision boundary. Nevertheless, we appreciate that you point out this relevant paper, and we will update our reference list accordingly.\n\n2. In fact, the decision attack paper also finds that CW performs worse than DeepFool on ImageNet. In the table at the bottom of page 6, CW gets larger median perturbation norms than DeepFool does for all of the three architectures on ImageNet. In particular, for the ResNet50 architecture (which we also used), the median perturbation norm of CW is 2.2e-7, and that of DeepFool is 7.5e-8.\n\nThe original paper of CW attack may shed some light on this. According to the paper, CW does perform better than DeepFool on ImageNet (Table V), but that is for the best case only, which refers to choosing 100 randomly chosen adversarial classes to perform the targeted attack, and then finding the easiest case. We can also try this for CW, but considering the computation cost of CW is so high already, multiplying it by 100 would really make this accuracy-efficiency tradeoff not worthwhile. On the other hand, as our paper intends to show, MarginAttack does a much better accuracy-efficiency tradeoff.\n\n3. Thank you for pointing out the ordering issue in table 3. It is not meant to create any false perceptions -- the differences in the numbers are quite distinct. But you are right, for better formality, we will adjust it in our updated version.\n\nThank you again for your comments!", "title": "Regarding your comments"}}}