{"paper": {"title": "L2-Nonexpansive Neural Networks", "authors": ["Haifeng Qian", "Mark N. Wegman"], "authorids": ["qianhaifeng@us.ibm.com", "wegman@us.ibm.com"], "summary": "", "abstract": "This paper proposes a class of well-conditioned neural networks in which a unit amount of change in the inputs causes at most a unit amount of change in the outputs or any of the internal layers. We develop the known methodology of controlling Lipschitz constants to realize its full potential in maximizing robustness, with a new regularization scheme for linear layers, new ways to adapt nonlinearities and a new loss function. With MNIST and CIFAR-10 classifiers, we demonstrate a number of advantages. Without needing any adversarial training, the proposed classifiers exceed the state of the art in robustness against white-box L2-bounded adversarial attacks. They generalize better than ordinary networks from noisy data with partially random labels. Their outputs are quantitatively meaningful and indicate levels of confidence and generalization, among other desirable properties.", "keywords": ["adversarial defense", "regularization", "robustness", "generalization"]}, "meta": {"decision": "Accept (Poster)", "comment": "\n* Strengths\n\nThis paper studies adversarial robustness to perturbations that are bounded in the L2 norm. It is motivated by a theoretical sufficient condition (non-expansiveness) but rather than trying to formally verify robustness, it uses this condition as inspiration, modifying standard network architectures in several ways to encourage non-expansiveness while mostly preserving computational efficiency and accuracy. This \u201ctheory-inspired practically-focused\u201d hybrid is a rare perspective in this area and could fruitfully inspire further improvements. Finally, the paper came under substantial scrutiny during the review period (there are 65 comments on the page) and the authors have convincingly answered a number of technical criticisms.\n\n* Weaknesses\n\nOne reviewer and some commenters were concerned that the L2 norm is not a realistic norm to measure adversarial attacks in. There were also concerns that the empirical level of robustness of the network was too weak to be meaningful. In addition, while some parts of the experiments were thorough and some parts of the paper were well-presented, the quality was not uniform throughout. Finally, while the proposed changes improve adversarial robustness, they also decrease the accuracy of the network on clean examples (this is to be expected but may be an issue in practice).\n\n* Discussion\n\nThere was substantial disagreement on whether to accept the paper. On the one hand, there has been limited progress on robustness to adversarial examples (even under simple norms such as the L2 norm) and most methods that do work are based on formal verification and therefore quite computationally expensive. On the other hand, simple norms such as the L2 norm are somewhat contrived and mainly chosen for convenience (although doing well in the L2 norm is a necessary condition for being robust to more general attacks). Moreover, the empirical results are currently too weak to confer meaningful robustness even under the L2 norm.\n\n* Decision\n\nWhile I agree with the reviewers and commenters who are skeptical of the L2 norm model (and would very much like to see approaches that consider more realistic threat models), I decided to accept the paper for two reasons: first, doing well in L2 is a necessary condition to doing well in more general models, and the ideas and approach here are simple enough that they might provide inspiration in these more general models as well. Additionally, this was one of the strongest adversarial defense papers at ICLR this year in terms of credibility of the claims (certainly the strongest in my pile) and contains several useful ideas as well as novel empirical findings (such as the increased success of attacks up to 1 million iterations)."}, "review": {"SJehfLMzlE": {"type": "rebuttal", "replyto": "BkeP1htbxN", "comment": "Regarding the specific points:\n-- We chose the best available baselines, i.e. classifiers from Madry et al. (2017), which happen to be trained with L_inf adversary. There seem to be no available models that are trained with L2 adversary, and the only paper that talked about such models, i.e. https://arxiv.org/pdf/1805.12152.pdf, reported L2 robustness that is weaker than L2 robustness achieved by training with L_inf adversary. Again more details are in the discussion with Aurko.\n-- Our Model 4's were trained with L_inf PGD with default hyperparameters from Madry et al.'s GitHub. We did not tune the hyperparameters, and you are right that there is probably room for improvement.\n\nOf course one may question everything on this forum. Luckily numbers never lie, and that is why we publish our models so that anyone can verify our results.\n", "title": "always happy to answer questions on our paper"}, "H1ezI47-gV": {"type": "rebuttal", "replyto": "BJg5F5RxgV", "comment": "Please see the dropbox link at the first paragraph of section 3.\n\nOne commenter has kindly tested our robustness numbers. Please see the comment titled \"Very well done evaluation\".\n\nIn the paper we stated that our model 4's were trained with linf adversary, but we made no claim about training with linf adversary vs training with l2 adversary. This only came up in the discussion with Aurko as an empirical observation.\n\nWe disagree with the statement that MNIST is only a sanity check. From robustness perspective, MNIST is far from a solved problem.", "title": "our models are at the Dropbox link on page 4"}, "HygKL1jgg4": {"type": "rebuttal", "replyto": "HyeXrBUggN", "comment": "91.7% is from our Table 1, and it is for Madry's MNIST model with l2 epsilon of 3 and after 100 CW iterations. The number you were looking for is 13.9% in our Table 2.\n\nIt is true that Madry et al's two papers are not reporting on the same cifar models. However the contrast between below-10% robust under l2 epsilon of 100/256 and 39.76% robust under l2 epsilon of 320/256 is so large that one of the two is likely incorrect. Our measurement of their model suggests that Fig 6(d) in Madry et al. (2017) is likely the incorrect one.\n\nPlease also see our discussion with Aurko Roy, particularly the last few rounds regarding training with l2 adversary vs training with linf adversary.", "title": "it seems that you read the wrong table"}, "Syx6Z5egeN": {"type": "rebuttal", "replyto": "BJgb-yNJgN", "comment": "We too were confused about Fig 6(d) in Madry et al. (2017), until we see \nhttps://arxiv.org/pdf/1805.12152.pdf\nwhich is from the same authors. There is evidence that they made mistakes in computing x coordinates when plotting Fig 6(d) in Madry et al. (2017), and underreported the robustness.\n\nThe last two rows in Table 4 in\nhttps://arxiv.org/pdf/1805.12152.pdf\nshow substantial robustness under l2 epsilon of 80/256 and 320/256. These results suggest that 100/256 would be a low bar, and the weak curve of Fig 6(d) in Madry et al. (2017) is highly unlikely.\n\nNow consider our measurements of Madry model in our Table 2. The numbers are in line with numbers in\nhttps://arxiv.org/pdf/1805.12152.pdf\n\nSo the most plausible explanation is that the authors used incorrect x coordinates when plotting Fig 6(d) in Madry et al. (2017). If we were to venture a guess, maybe they only added up deltas in the red channel rather than all three RBG channels.\n", "title": "likely a mistake in Fig 6(d) in Madry et al. (2017)"}, "HJx15w00RQ": {"type": "rebuttal", "replyto": "ryxW1EcRRQ", "comment": "We would like to thank the reviewer for the upgraded rating and for the kind comments. And we thank the reviewer again for the valuable suggestions that help us improve this paper and future work. We'd be happy to answer further questions on the loss function.\n", "title": "thank you very much"}, "Syg_IW86hm": {"type": "review", "replyto": "ByxGSsR9FQ", "review": "This paper presents a combination of methods that, together, yield neural networks that are robust to small changes in L2 distance. The main idea is to ensure that changing the input by a bounded L2 distance never changes the output by more than the same L2 distance. Then, the difference between the highest-scoring class and the second-highest scoring class provides a bound on how much the input must change. The trivial way to do this is to rescale the final output layer so that all of the magnitudes are very small; however, this would give no additional robustness at all. To counteract this, the paper introduces several additional heuristics for increasing the gap between the highest-scoring class and the second-highest scoring one. Adversarial training can be used to make the models even more robust. \n\nExperimental results on MNIST and CIFAR look impressive, although most are in terms of L2 distance, while most previous work optimizes L_infinity distance.\n\nThe methods described by this paper are similar to max-margin training, which is already known to be optimally robust to L2 perturbations for linear models (e.g., Xu et al. (2009)). This paper would be stronger with more discussion and analysis of this connection, although that might be work for a future paper.\n\nAlthough the method relies heavily on heuristics, the empirical results are promising. The analysis of the contribution of the heuristics is fairly thorough as well. The MNIST results are strong. The CIFAR results show improved robustness, though at reduced accuracy on natural images. A combination of robust and non-robust classifiers improves the accuracy somewhat.\n\nOverall, this is interesting work with promising empirical results. The biggest weaknesses are:\n\n- Limited theory. The loss function is particularly strange. \n\n- The majority of the comparisons focus on L2-robustness, but are comparing to a model optimized for L_infinity-robustness. (Thankfully, the authors also do some comparisons on L_infinity-robustness.)\n\n- Robustness comes at a cost in accuracy, though this is not uncommon for adversarial training.\n\nThe biggest strengths are:\n\n- Strong empirical robustness\n\n- Analysis of combinations of methods and their interactions: different loss function, different architecture, different weight constraints, and adversarial training are all evaluated together and separately.\n\n- Wide variety of experiments, including generalization on training data with noisy labels and analysis of the confidence gaps.\n\n\n\nQuestions for the authors:\n\n- For equation (4) in the loss function, why would rescaling the layers in the middle of the network be equivalent to a linear transformation (u1, u2, ..., u_K) of the output?\n\n- In equation (6), what is the average averaging over?\n\n- The connection between confidence gap and robustness is discussed empirically, as a correlation, rather than theoretically, as a bound.  Doesn't the confidence gap give a lower bound on the minimum perturbation to change the predicted class?\n\n---------\n\nEDIT: After the author response, I remain positive about this paper. In addition to addressing my concerns, I admire the authors' patience in answering the concerns of other reviewers and commenters. I think that this is a solid paper that makes a good contribution to the literature on adversarial machine learning.", "title": "A variety of methods combine to give L2-robustness", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Bkly6cc60Q": {"type": "rebuttal", "replyto": "HJgJxX5TRX", "comment": "Hi Chris,\n\nWe are sorry that we will stop responding to your questions. We feel that most readers of our paper and this page do not share your confusions, and further discussion would not help the purpose of this forum. We feel that our two rounds of comments are sufficiently clear.\n\nAs you requested, we will put a pointer on https://openreview.net/forum?id=HkxAisC9FQ so that you or anybody else can continue the discussion there.\n", "title": "our response"}, "r1e3-5Hh0m": {"type": "rebuttal", "replyto": "B1xLfvQnRm", "comment": "Hi Chris,\n\nConsider two classifiers: the first is f(x) and the second is sigmoid(f(x)).\nSuppose we have an adversarial example for the first classifier: f(x) classify x0 correctly but x0+delta incorrectly. In other words: argmax(f(x0+delta))!=argmax(f(x0)).\nBecause sigmoid is monotonically increasing, we have argmax(sigmoid(f(x0+delta)))==argmax(f(x0+delta)) and argmax(sigmoid(f(x0)))==argmax(f(x0)). Therefore argmax(sigmoid(f(x0+delta)))!=argmax(sigmoid(f(x0))). In other words, x0+delta is also an adversarial example for the second classifier.\nThat's why adding a final sigmoid layer should have zero effect on a network's robustness, and that's why their finding of \"inserting a sigmoid activation function improved model robustness\" is nothing but gradient obfuscation.\nAccording to Table 3 in their paper, their best model goes from 78.42% error to 100% error when tanh is removed.\n\nYour comments on L2 PGD vs CW describe exactly the phenomenon of gradient obfuscation. 10 PGD iterations being stronger than 1000 CW iterations suggests a situation of vanishing gradient. If CW is used properly 1000 iterations of it will be a much stronger attack than 10 PGD iterations.\n10 iterations of any attack are far too few for a meaningful robustness evaluation.\n\nWith gradient obfuscation and 10 PGD iterations, all measurements in https://openreview.net/forum?id=HkxAisC9FQ are meaningless.\n\nAs we said in the last comments, the proper way of evaluation is to use CW before sigmoid or softmax and call it with high iteration counts. That is what we do.\n\nYour statement of \"They have achieved better robustness results (in L2) than yours on CIFAR-10, by over 10% at L2 distance 1.5.\" is simply cherry picking data because it ignores our best model.\nIt's a pointless comparison anyway given that their numbers come from gradient obfuscation and 10 PGD iterations.\n\nIt seems that you have misread the Gouk et al. paper. Please see the last paragraph of Section 3.1 and first paragraph of Section 4.1 in their paper:\nhttps://arxiv.org/pdf/1804.04368.pdf\n\nIt seems that you are still confusing WTW with W, and your statement of \"what you are really trying to control is ||W||_\\infty\" makes no sense: our models are L2 nonexpansive, not L_inf nonexpansive. As we said in last comments, please see the text around equation (2) including footnotes. Measurements show that L2NNN's 2-Lipschitz constant is not far below 1, please see our response dated 11/16 to an earlier comment titled \"Estimates of the Lipschitz constant\" and our Figures 3 and 4.\n\nWe addressed batch norm in the appendix, and one simply needs to divide the scaling factors by the max one.\nThe trade-off between robustness and nominal accuracy has been discussed extensively in multiple discussions throughout this page, please read. This trade-off has been observed by previous works including adversarial training and adversarial polytope works, and it remains an open question whether such trade-off is a necessary part of life. However, one thing we know for sure is that gradient obfuscation is not the answer.\n", "title": "more on gradient obfuscation"}, "HkxISreiCm": {"type": "rebuttal", "replyto": "Bklz239KCX", "comment": "Hi Chris,\n\nThere are a number of issues with \nhttps://openreview.net/forum?id=HkxAisC9FQ\n\n1) Questionable evaluation: Inserting sigmoid layer before attack.\nThe authors stated that \"Prior to the final softmax layer, we found inserting a sigmoid activation function improved model robustness. In this case, the sigmoid layer comprised of first batch normalization (without learnable parameters), followed by the activation function t*tanh(x/t), where t is a single learnable parameter, common across all layer inputs.\"\nThere is something wrong: Adding a final sigmoid layer should have zero effect on a network's robustness, because an adversarial example before would still be an adversarial example after.\nThese statements suggest that the authors applied attacks on the sigmoid outputs, or even worse, that they may have applied attacks on the softmax outputs. Such evaluation setup is a form of gradient obfuscation, and it is well known to artificially slow down gradient-based attacks and create a false sense of security.\nThe proper way of evaluating robustness, as measured by white-box defense, is to apply attacks on the logits themselves, i.e. the direct outputs of ReLU network (or any final computing layer). That's what we do.\n\n2) Questionable evaluation: Attack setup.\nFor evaluation, the authors use attacks implemented in Foolbox:\nhttps://arxiv.org/pdf/1707.04131.pdf\nhttps://github.com/bethgelab/foolbox\nwhich contains a modified version of CW attack. The authors stated that \"Hyperparameters were set to Foolbox defaults.\" The Foolbox version of CW attack uses a default of 1000 iterations. Among all Foolbox attacks, the authors concluded that L2 PGD is the strongest, stronger than Foolbox CW, and therefore used L2 PGD in all reportings.\nFirst, the authors should have used the original CW code at https://github.com/carlini/nn_robust_attacks.\nSecond, 1000 iterations are not enough to evaluate robustness, in light of the L2 robustness numbers from Madry et al. (2017) and Tables 1 and 2 in our paper. Neither PGD with low iteration count nor CW with low iteration count reveals the true robustness of a model.\nPoint 1) above makes the situation even worse. With gradient obfuscation, 1000 CW iterations may become equivalent to 100 iterations or less.\n\nConsidering 1) and 2), one has to take their numbers with a grain of salt.\n\n3) Reliance on training data coverage.\nIn our Section 4, we review related works as two big groups. The first group fortify a network around training data points: this includes both adversarial training like Madry et al. (2017) and gradient regularization like Ross & Doshi-Velez (2017). The second group bounds a network's responses to input perturbations over the entire input space. Our work belongs to the second group.\nTheir proposed method is fairly similar to Ross & Doshi-Velez (2017), and belongs to the first group. The common weakness of the first group is the reliance on training data coverage. While works in this group are able to fortify parts of the input space, specifically flattening gradients around training data points, there exists little control over parts not covered by training data.\n\nYour statement of \"...by over 10% at L2 distance 1.5\" is cherry-picking data: even if accepting their numbers as they are (big question mark by themselves), the difference would be only 1.2%.\n\nRegarding the Gouk et al. paper, please see our response dated 11/12 to an earlier comment titled \"related work\".\n\nRegarding your comments on matrix norm. It seems that you were confusing WTW with W, please see the text around equation (2) including footnotes. Measurements show that L2NNN's 2-Lipschitz constant is not far below 1, please see our response dated 11/16 to an earlier comment titled \"Estimates of the Lipschitz constant\" and our Figures 3 and 4.\n", "title": "our answer to the question"}, "H1eKGidtnm": {"type": "review", "replyto": "ByxGSsR9FQ", "review": "Summary:\nThe paper presents techniques for training a non expansive network, which keeps the Lipchitz constant of all layers lower than 1. While being non-expansive, means are taken to preserve distance information better than standard networks. The architectural changes required w.r.t standard networks are minor, and the most interesting changes are made to the loss minimized. The main claim of the paper is that the method is robust against adversarial attacks of a certain kind. However, the results presented show that a) such robustness comes at a high cost of accuracy for standard examples, and b) even though the network is preferable to a previous alternative in combating adversarial examples, the accuracy obtained in the face of adversarial attacks is too low to be of practical value. Other properties of the networks, explored empirically, are that the confidence of the prediction is indicative of robustness (to adversarial attacks) and that the networks learn better in the presence of high label noise. \nIn short, this paper may be of interest to a sub-community interested in defense against certain types of adversarial attacks, even when the defense level is much too low to be practical. I am not part of this community, hence did not find this part very interesting. I believe the regularization results are of wider interest. However, to present this as the main contribution of L2NNN more work is required to find configuration which are resilient to overfit yet enable high training accuracy, and more diverse experiments are required.\nPros:\n+ the idea of non expansive network is interesting and important\n+ results indicate some advantages in fighting adversarial examples and label noise\nCons:\n- the results for fighting adversarial examples are not significant from a practical perspective\n- the results for copying with label noise are preliminary and require expansion with more experiments.\n- the method has costs in accuracy, which is lower than standard networks and this issue is not faced with enough attention\n- presentation clarity is medium: proofs for claims are missing, as well as relevant background on the relevant adversarial attacks. The choice to place the related work at the end also reduces presentation clarity.\n\nMore detailed comments:\nPages 1-3: In many places, small proofs are left to the reader as \u2018straightforward\u2019. Examples are: the claim in the introduction, in eq. 2, in section 2.2, section 2.3\u2019 last line of page 3, etc.. While the claim are true (in the cases I tried to verify them long enough), this makes reading difficult and not fluent. For some of these claims I do not see the argument behind them. In general, I think proofs should be brought for claims, and short proofs (preferably) should be brought for small claims. Leaving every proof to the reader as an exercise is not a convenient strategy. \nPage 4: The loss is complex and its terms utility require empirical evidence. The third term is shown to be clearly useful, enabling a trade off between train accuracy and margin. However, the utility of terms 4) and 5) is not verified. Do we really need both these terms? Cannot we just stay with one?\nThe main claim is robustness w.r.t \u201cwhite-box non targeted L2-bounded attacks\u201d. This seems to be a very specific attack type, and it is not explained at all in the text. Hence it is hard to judge the value of this robustness. Explanation of adversarial attack kinds, and specifically of \u201cwhite-box non targeted\nL2-bounded attacks\u201d is required for this paper to be a stand alone readable paper. Similarly \u2018L_\\infty\u2019-bounded attacks, for which results are shown, should be explained.\nTable 1,2: First, the model architecture used in these experiments is not stated. Second, the accuracy of the \u2018natural\u2019 baseline classifier, at least in the MNist case, is somewhat low \u2013 much better results can be obtained with CNN on MNist. Third, the accuracies of the suggested robust models are very low compared to what can be obtained on these datatsets. Forth, while the accuracies under attack of the proposed method are better than those of Madri et al., both are quite poor and indicate that the classifier is not useful under attack (from a practical perspective).\nPage 6: The classifiers which share the work between an L2NNN network and a regular more accurate network may be interesting, as the accuracies reported for them are significantly higher than the L2NNN networks. However, the robustness scores are not reported for these classifiers, so it is not possible to judge if they lead to a practical and effective strategy.\nPage 7: For me, the results with partially random labels are the most interesting in the paper. The resistance of L2NNN to overfit and its ability to learn with very noisy data are considerably better than the suggested alternatives.\nRelevant work not mentioned \u201cSpectral Norm Regularization for Improving the Generalizability of Deep Learning\u201d - Yuichi Yoshida and Takeru Miyato, Arxiv, 2017.\n\nI have read the rebuttal.\nThe discussion was interesting, but I do not see a need to change my assessment.\nThe example of ad-blocking in indeed a case (the first I encounter) where l2- perturbated adversarial examples can be useful for cyber attack. The other ones are less relevant (the attacks are not based on adversarial attacks in the sense used in the paper: images crated with small gradient-direction perturbations). Anyway talking about 'attacks on a self-driving car' are still not neaningful to me: I do not understand what adversarial examples have to do with this.\nI do not find the analogy of 'rocket improvements and moon landing' convincing: in 69 rocket improvements were of high interest in multiple applications, and moon landing was visible over the corner. \n\n", "title": "The contribution of the method for combating adversarial examples does not look practically significant to me. Other contributions, like the ability to learn in the presence of label noise are more interesting, but require further development and experiments.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HylOZ5n3pX": {"type": "rebuttal", "replyto": "S1xf3ovhpm", "comment": "Let us answer your later question first. Pixels are normalized to [0,1] for all runs in our paper, for both MNIST and CIFAR.\n\nNow, as promised, the following are average L2 norm of Jacobians of logits with respect to inputs, averaged over the first 1000 images in MNIST test set.\n\nModel 2 (Madry et al. (2017)): 10.818453\nModel 3 (L2NNN with no adversarial training): 1.054181\nModel 4 (L2NNN with adversarial training): 0.8331261\n\nA few things to note:\n-- This is a surrogate for local Lipschitz constants, in particular it is measured at the nominal point and not over a neighborhood.\n-- L2 norm of Jacobian for Models 3 and 4 can be larger than 1, because we built them as multi-L2NNN classifiers, please see the first paragraph of Section 2.4.\n-- The comparison between Model 3 and Model 4 is consistent with our hypothesis.\n-- We would not take the Model 2 number at face value, because one could argue that Model 2 should be scaled down by a constant before making this measurement. This scaling is a tricky issue, please see our response to an earlier comment titled \"Very well done evaluation\".\n", "title": "measurements"}, "rJgv81jhpm": {"type": "rebuttal", "replyto": "ByxGSsR9FQ", "comment": "We would like to thank the three reviewers for the many helpful suggestions, and we are grateful for the extensive comments from others as well. We have made our best effort in revising the paper within the page limit for the main text and only enlarging the appendix. There are a number of places where we would have liked to elaborate more and we hope we will have a chance to do so using more space if this paper is accepted.", "title": "a revision is posted"}, "Skl94a53p7": {"type": "rebuttal", "replyto": "BJx0pEqh67", "comment": "Hi Aurko,\n\nWhen comparing two models, one has to decide on a common setup.\n\nIf one chooses Madry et al.'s setup of L2 defense evaluation, then the comparison is as we stated:\nTraining with L_inf adversary produces 90% against epsilon of 4.\nTraining with L2 adversary produces 63.73% against epsilon of 2.5.\n\nIf one chooses our setup of CW attack with high iteration count, then both the above numbers will reduce.\nTraining with L_inf adversary produces 7.6% against epsilon of 3.\nTraining with L2 adversary produces ?.\n\nUnfortunately we do not have access to their L2-adversary-trained model to fill in the question mark above. If one believes that Madry et al.'s setup of L2 defense evaluation extrapolates, that question mark is likely a very small number.\n", "title": "equal-attack is the basis of comparison"}, "HylXLYOn6X": {"type": "rebuttal", "replyto": "BkxAVy9sTm", "comment": "We do not mind at all that this debate happens at our paper's page, and if people wish to continue the discussion please do.\nHowever we the authors will stop responding to this particular subject, heeding the sage words of the Area Chair.", "title": "let's agree to disagree"}, "S1lq-Lwn67": {"type": "rebuttal", "replyto": "SyxZxI6op7", "comment": "Hi Aurko,\n\nThank you and that is precisely the kind of runs we were looking for. A closer look at their numbers actually reinforces our argument.\n\nAccording to Table 4 in https://arxiv.org/pdf/1805.12152.pdf, the best MNIST L2 robustness by training with L2 adversary is 63.73% robust accuracy against epsilon of 2.5.\nAccording to Figure 6 in Madry et al. (2017), the L2 robustness by training with L_inf adversary is about 90% against epsilon of 4.\nNote that both papers are from the same authors.\nIn other words, by their own assessment, training with L_inf adversary produces stronger L2 defense than training with L2 adversary.\nThis exactly supports our argument and is consistent with our own experience as stated in our last response.\n\nHaving said the above, there is clearly something that Madry et al. know while we do not, which is how to convert PGD to an efficient L2 adversary, and we will try and find out.\n", "title": "that data support our argument"}, "SJgHFyIip7": {"type": "rebuttal", "replyto": "B1xJuFmipm", "comment": "Thank you for the interest and we're happy to clarify.\n\nFor the first point, please see our answer to Aurko Roy's comment.\n\nFor the second point, we want to clarify a few things. We have a guarantee that the Lipschitz constant of an L2NNN is strictly no great than 1. Our hypothesis on local Lipschitz constant is regarding the effect of adversarial training on L2NNNs. Initially we expected that the gap between Model 3 (L2NNN with no adversarial training) and Model 4 (L2NNN with adversarial training) will fade away as CW attacker uses more iterations. Tables 1 and 2 suggest the opposite, i.e., that the benefit of adversarial training is permanent on L2NNN and is not just making examples difficult to find. To explain this phenomenon, we cite Hein & Andriushchenko (2017) and hypothesize that adversarial training on L2NNNs reduces local Lipschitz constants and thereby enlarges the actual robustness ball.\n\nWe will measure L2 norm of Jacobians, albeit only as a surrogate for local Lipschitz constants, and report back here. Please give us a day or two, we want to finish a revision first.\n", "title": "our answer to the question"}, "rJgJYHXjTX": {"type": "rebuttal", "replyto": "HygXxYFcT7", "comment": "Hi Aurko,\n\nThank you for the interest.\n\nAs we stated in the paper, Model 2's in Tables 1 and 2 were downloaded from Madry et al.'s GitHub pages (links in footnote on page 4). To be more specific, they were fetched under name \"secret\": these were released after they closed the black-box leaderboards and match what were reported in their paper.\nIt is true that Model 2's were trained with L_inf adversary. However, let us quote from Madry et al. (2017): \"our MNIST model retains significant resistance to L2-norm-bounded perturbations too -- it has quite good accuracy in this regime even for epsilon=4.5.\" and \"our networks are very robust, achieving high accuracy for a wide range of powerful adversaries ...\" In other words, Madry et al. do not see the use of L_inf attacker in training as a limiting factor to L2 defense.\nWe are not aware of any published defense results that beat Madry et al. (2017) as measured by any norm. Please see also Athalye et al. (2018) for a comparison between Madry et al. (2017) and a set of other defense works. We are also not aware of any published MNIST or CIFAR models that were trained with L2 adversary and achieved sizable white-box defense.\nAnother fact to consider is that our Model 4's were trained with the same L_inf attacker (PGD with default hyperparameters from Madry et al.'s GitHub) and that improved L2 robustness as reported in Tables 1 and 2.\n\nIt is unclear that your suggestion would work in practice. The first question to ask is should one clip after all PGD iterations or clip per iteration.\nIf one chooses to clip after all PGD iterations, then this new adversary is not much different from PGD with a smaller L_inf epsilon, and it's more likely to weaken the effect of adversarial training than help it.\nIf one chooses to clip per iteration, then for each PGD iteration, we need to solve for the crossing point between a sphere and a line, where the line does not cross the center of sphere except for the very first iteration, and where the sphere has been modified by value range of each input entry. This is a quadratically constrained quadratic programming problem, and solving it per iteration would make PGD adversarial training much more expensive if not prohibitive, and it is difficult to implement on GPU.\nBut by all means, we'd encourage you to do so, improve on Madry et al. (2017)'s L2 defense, and publish if it succeeds.\n\nAgain, we are not aware of any published models from successful adversarial training with L2 adversary. We ourselves have made an unsuccessful attempt to use CW L2 attack in training, and it did not work because L2 attacks with low iteration counts do not seem to help our models yet we cannot afford L2 attacks with high iteration counts in the training loop. As a result, we decided to use the original PGD to build our Model 4's in Tables 1 and 2, and that gave them a nice boost in L2 robustness.\n\nWe would be very interested if someone demonstrates successful adversarial training with L2 adversary, as we want to learn from him/her to improve our Model 4's and we would be happy to include more competitors in Tables 1 and 2.\n", "title": "our answer to the question"}, "H1xEPUkq6Q": {"type": "rebuttal", "replyto": "ryxxv85_Tm", "comment": "Hi Justin,\n\nOn the first example of ad-blocking.\nAn ad publisher has a content that he/she wants to deliver. That content, if without perturbation, would be correctly handled, i.e. blocked, by perceptual ad-blocking. Hence he/she has a motivation to deliver that content by adding small perturbations.\nHumans are good at filtering out ads, and a spammer who uses the lower image in Figure 2 of https://arxiv.org/pdf/1712.03141.pdf would be an unsuccessful spammer, because nobody would pay attention to contents in such an image. A successful spammer is one that can take the upper image, add small perturbations, and deliver it to people's inboxes. For phishing and spam attacks it's even more important that people can not recognize them as such, and imperceptible perturbations are useful.\n\nOn the second example of face recognition.\nNowadays many people use their faces to unlock cellphones. If someone can print eyeglass frames to gain access to other people's devices, it's a big security gap for millions of people.\nBy the way, the relative magnitude of an eyeglass frame with respect to a face is roughly on par with the relative magnitude of L2 norm of 3 with respect to an MNIST image.\n\nOn the third example of stop sign.\nIf a stop sign gets knocked over or covered, a policeman will correct the situation. If a stop sign has four stickers on it, nobody would bother until an accident happens.\nBy the way, the relative magnitude of those four stickers with respect to the whole stop sign is roughly on par with the relative magnitude of L2 norm of 3 with respect to an MNIST image.\n\nSome of Justin's arguments seem to be that L2 and L_inf norm metrics do not constitute a sufficient condition for a robust classifier. That is absolutely true. For example, if one has a MNIST classifier that is 90% robust against L2 norm of 3 and L_inf norm of 0.3, it might still break down if an input image is rotated by an angle, while a truly robust classifier like a human would not change its decision. By the way, the \"lines\" attack and common corruption benchmark do not constitute a sufficient condition either, and the lines-attack pictures on Madry et al. (2017) seem more excusable than our Figure 5 (nonetheless we'll be happy to evaluate our models under those conditions).\n\nThe flip-side question is whether robustness as measured by L2 and/or L_inf norm metrics is a necessary condition for a robust classifier. The answer is a big yes. We must learn to walk before we can run, and frankly the status quo of neural-network robustness research is at crawling stage. While we are crawling, we do not have the luxury to look down upon people who are working to meet necessary conditions, and rejecting defense papers based on an argument of not having a sufficient condition would only hinder progress and make those more hefty goals the more difficult to reach.\n\nWe agree with many points Justin made in this paper\nhttps://arxiv.org/pdf/1807.06732.pdf\nHowever, if the conclusion is to suppress the \"perturbation defense\" literature, that would be wrong. In our opinion, advances in white-box defense, a.k.a. robustness, a.k.a. generalization, of neural networks as measured by L2 norm and/or L_inf norm metrics are not only valid but also essential topics for the deep learning community, and our paper represents a big step forward. Given the size of improvement, L2NNN is likely a key ingredient in future truly robust solutions, which people have little clue yet. When people do find those solutions, there would be profound impacts across the board on both security and generalization.\n", "title": "we must learn to walk before we can run. (with apologies to Area Chair)"}, "B1gBAL19aX": {"type": "rebuttal", "replyto": "SJxA1llFTQ", "comment": "We strongly agree with AnonReviewer2's arguments.", "title": "thank you very much"}, "SJlTkLwDTm": {"type": "rebuttal", "replyto": "r1lUDfqmam", "comment": "We are happy to comment on relations to these works.\n\nThe Miyato et al. paper has a different way of approximating the spectral radius of a weight matrix. In place of the strict bound of (2) in our paper, they approximates the current spectral radius based on a companion vector, which is intended to approximate the top singular vector at the moment and which is updated through power iterations. The up side of their approach is that it can be computationally cheap, in fact they do just one power iteration to the companion vector after each training batch. The down side of their approach is that their spectral radius is a coarse approximation: for example, consider a scenario where two top singular values are close in magnitude, and the companion vector represents v1, one of the two corresponding singular vectors v1 and v2; their regularization would suppress the first singular value, and after a few batches the second singular value becomes dominant; at this point, it would take many power iterations to move the companion vector from v1 to v2, and one power iteration per batch certainly would not make it. When there are more singular values with similar magnitudes, the situation gets even worse. The end result is likely that their models are expansive. The empirical results suggest that they improve GANs, it's unclear how they would perform under adversarial attacks.\n\nThe Tsuzuku et al. paper differs from us in a number of ways. To estimate Lipschitz constants, they follow Miyato et al. and use the same companion-vector approach. As we explained earlier, this approach has its limitations. They have a different way of modifying the loss function than ours. There does not seem to be anything that competes with our architecture changes. Their MNIST results do not seem strong; we cannot comment further on their empirical results as they only reported CW attacks with 100 iterations.\nWe would like to bring it to AnonReviewer1's attention that this is an adversarial defense work that gets accepted into NIPS with weaker results than ours.\n\nThe Scaman and Virmaux paper is on analysis of Lipschitz constants and not on optimization. In terms of analysis, the big difference between their AutoLip and ours is that they calculate Lipschitz constants of linear layers through power methods rather than we using bound of (2). Note that there is no companion vector here as it's one-shot analysis. Power methods would give a tighter estimation than our bound of (2), however they are too expensive to use in training a neural network and hence do not have practical implications for building robust models. Their SeqLip algorithm gives tighter bound, but is even more expensive, than AutoLip, and similarly would be difficult to use in training models.\n\nThe Gouk et al. paper also uses power method to estimate L2 Lipschitz constants of linear layers, and this is the main different from us in terms of regularization. We have not found an explicit statement on whether they use a companion vector or start from a random vector each time. Some statements suggest that they do use a companion vector like Miyato et al., for example, they mentioned for one experiment they only do one power iteration. As discussed earlier for Miyato et al., this approach has its limitations. In fact, the authors acknowledged multiple times in the text that they are underestimating the L2 Lipschitz constants of linear layers. There are no robustness results.\n\nThe Sedghi et al. paper is an interesting paper and they invented a way to compute the Lipschitz constant of a convolution layer. If their proof is correct, it would produce a tighter bound than ours, with a complexity that is lower than power methods. It seems that the computation cost is still fairly high, and hence they use it once every 100 iterations to regularize the convolution layers, and that resulted in improved nominal accuracy on CIFAR-10. It is unclear whether this is applicable in training robust models, especially if we can only afford to do it once in a while, -- it might be and is worth looking into and we thank the commenter for the reference. There are no robustness results in their paper. Note also that this is for convolution layers only.\n", "title": "our answer to the question"}, "HJgX7SEUT7": {"type": "rebuttal", "replyto": "r1e8nj0STm", "comment": "Please consider the following three examples.\n\nThe first example is from last week:\nhttps://arxiv.org/abs/1811.03194\nIt demonstrated that ad-blocking based on neural networks is vulnerable and easily defeated by adversarial examples. Furthermore, if ad-blocking based on neural networks is deployed, it \"would engender a new arms race that overwhelmingly favors publishers and ad-networks.\" Also because ad-blocking based on neural networks needs to run with a high privilege level inside a browser, it would \"introduce new vulnerabilities that let an attacker bypass web security boundaries and mount DDoS attacks.\"\n\nThe second example is:\nhttps://www.cs.cmu.edu/~sbhagava/papers/face-rec-ccs16.pdf\nIt demonstrated that by printing eyeglass frames, one person can impersonate another person (a specific choice) in front of a state-of-the-art face-recognition algorithm. Please see their figure 4, the authors can pretend to be Milla Jovovich and Carson Daly by just wearing glasses.\n\nThe third example is from CVPR 2018:\nhttps://arxiv.org/pdf/1707.08945.pdf\nIt demonstrated that by putting just four stickers on a stop sign, a neural network would recognize it as a \"speed limit 45\" sign, and it would make the same mistake consistently from different distances and angles. The authors also performed field test in a moving vehicle.\n\nAs we mentioned, currently the attack-side research has an upper hand over the defense side, and they are only getting better and entering the physical world more and more. It is the more reason to encourage research on adversarial defense.\n\nPeople in the security field are concerned enough that a security conference accepted Carlini & Wagner (2017a) and gave it best student paper award, and it has received more than 500 citations by today. ICLR 2018 accepted Madry et al. (2017) which has received more than 250 citations.\n\nRegarding the reviewer's second point. Allow us use an analogy: before 1969, the scientific community should not reject paper on rocket improvements on the ground that nobody had landed on the moon.\n", "title": "three examples"}, "rkgG-58VpQ": {"type": "rebuttal", "replyto": "B1lLWoxqnQ", "comment": "We thank the reviewer for the thoughtful review and many helpful suggestions. We are updating the paper to incorporate some of the suggestions and will post a revision soon. In the responses below, related points are grouped together and ordered roughly by significance.\n\n1) Regarding the third condition (preserving distance).\nWe thank the reviewer for pointing out presentation issues related to the third condition.\nIn short, the second and third conditions are two aspects of enlarging confidence gaps: the second condition does so by modifying the loss function, while the third condition does so by modifying the network architecture. The practical embodiments of the third condition include two-sided ReLU, norm-pooling, and a few more in appendix A.\nIt is true that we did not derive two-sided ReLU or norm-pooling from a mathematical formulation of the third condition. Rather we started from a heuristic notion, as the reviewer put it, of preserving distance, and hypothesized that popular non-linear functions like ReLU and max-pooling unnecessarily restrict confidence gaps (see Sections 2.2 and 2.3), and hypothesized about two-sided ReLU and norm-pooling as improvements on preserving distance, and empirically verified their effects in enlarging confidence gaps and improving robustness.\nThe argument that \"a network that maximizes confidence gaps well must be one that preserves distance well\" was meant to say that preserving-distance-well is a necessary property for an L2NNN with large confidence gaps, and hence motivate architecture changes. Again the second condition is about the loss function and the third condition is about architecture.\nResults in Table 3 suggest that architecture choices are important for nonexpansive networks, specifically that some non-linear functions that are not in common practice work better than the more standard ones. These unusual functions, two-sided ReLU and norm-pooling, have the property that they do a better job at preserving distance and let the parameter training, rather than architecture, determine what information is thrown away. Empirical results support that these functions are best practice when used in nonexpansive networks. In contrast, preserving distance is less important in ordinary networks because parameter training can choose weights that amplify distances arbitrarily. There are likely more architecture choices for nonexpansive networks which may improve future results, and architecture exploration is one of our future directions.\n\n2) Regarding comparison against Kolter & Wong (2017).\nWe did not realize that the Kolter & Wong (2017) models are available for download. The mention of scalability in Section 4 was part of literature review and not intended as an excuse. We can certainly try and put the Kolter & Wong (2017) models through the same L2-defense comparisons as in Tables 1 and 2. This may take some time as we need to port the models to be compatible with the CW attack code, and we will report back here.\nFor L_inf defense, we did report a comparison. Table 4 shows our L_inf defense results under the same epsilon 0.1 as used in Kolter & Wong (2017). The results suggest that the measured L_inf defense is roughly on par.\nWe will also add reference to their follow-up paper: https://arxiv.org/abs/1805.12514.\n\n3) Regarding using only the confidence-gap loss.\nUnfortunately the confidence-gap loss (6) alone would not work. The problem is that (6) is too weak in penalizing mistakes. Consider a hypothetical MNIST neural network that always outputs logit value 1000 for label 0, and logit value 0 for the other nine labels. It would be a useless classifier, yet its (6) loss would be approximately -100 (-1000*0.1+0*0.9), which is lower than a useful classifier.\nThe reviewer might be interested to see what happens if we put more weight on the confidence-gap loss. We reported additional accuracy-robustness trade-off points in the second to last paragraph of Section 3.1. That trade-off curve continues and here is another point with heavier weight on (6): nominal accuracy drops to 97.9% and the robust accuracy (1000-iteration attacks) increases to 24.7%. By the way, in the second to last paragraph of Section 3.3 we stated a hypothesis that this tradeoff is due to fundamentally the same mechanism as the tradeoff shown in Table 6.\n", "title": "our responses, part 1 of 2"}, "ryeApYUEpX": {"type": "rebuttal", "replyto": "B1lLWoxqnQ", "comment": "\n4) Regarding presentation issues.\nThank you very much for the suggestions and we agree with most. We are making some of the changes and will post a revision soon, and will do more if this paper is accepted and more space is allowed in final version.\nOne thing we want to point out that that L2NNN's 93.1% performance from 75%-random training labels is significantly higher than the best of ordinary networks, see Tables 5,7,8.\nOur measurements of L_inf defense of Madry et al. (2017) are close to those reported in their paper. Because we use the same L_inf epsilon values, the numbers in Table 4 can be directly compared against those reported in Madry et al. (2017), Kolter & Wong (2017) and Raghunathan et al. (2018). As we acknowledged at end of section 3.1, Madry et al.'s MNIST L_inf result is still the best, while for CIFAR-10 we are on par.\n\n5) We agree with the reviewer that pointing out that MNIST is not a solved problem is important to the field as well.\nAnd we thank the reviewer for appreciating that L2NNNs have an easily accessible measure of robustness.\n\nPlease let us know if we have missed anything and we'd be happy to continue the discussion.\n", "title": "our responses, part 2 of 2 "}, "BkxNSYINam": {"type": "rebuttal", "replyto": "H1eKGidtnm", "comment": "We thank the reviewer for the thoughtful review and helpful suggestions, especially for appreciating the scrambled-label experiments. We are updating the paper to incorporate some of the suggestions and will post a revision soon. In the responses below, related points are grouped together and ordered roughly by significance.\n\nBefore going into the list, we wish to emphasize that this paper sets a new state of the art in adversarial defense. Currently in the field, the attack side has an upper hand over the defense side, and indeed there has not been a defense that is practically significant, as the reviewer put it, from the perspective of real-life applications. However, if and when that happens, there will be wide implications across most deep-learning applications in terms of both security and generalization. That is more reason to look for advances in defense, and our paper represents a big step forward.\n\n1) Regarding the significance of our defense results and types of attacks.\nLet us put our defense results in context. The white-box non-targeted scenario is the easiest for attacker and the most difficult for defense. White-box means that the attacker has complete information of a classifier, i.e. its architecture and parameters. By definition, if a classifier achieves a certain degree of white-box defense, its defense in black-box or transfer-attack scenarios can only be higher. Non-targeted means that any misclassification is considered a successful attack, while targeted attacks must reach a certain label. If a classifier achieves a certain degree of defense against non-targeted attacks, its defense in targeted scenario can only be higher. Therefore, white-box non-targeted defense is the holy grail of defense research, as it subsumes other types, and that's what we focus on.\nThen there is the choice of how to quantify noise, and the consensus in the field seems to be L2 norm or L_inf norm, preferably both. This choice leads to two measurements, defense against L2-bounded attacks and defense against L_inf-bounded attacks.\nWhite-box defense has been an elusive goal and numerous defense proposals have failed. Before our work, adversarial training has been considered the mainstream approach and Madry et al. (2017) has been the state of the art.\nThis paper sets a new state of the art for defense against white-box non-targeted L2-bounded attacks. The reviewer commented that it is a very specific attack type, and we want to point out this type subsumes all other L2-bounded types. At the same time, L2NNNs also exhibit, in Table 4, near-state-of-the-art defense against white-box non-targeted L_inf-bounded attacks, which subsumes all other L_inf-bounded types. It is absolutely true that we cannot put a 24.4%-robust classifier in a self-driving car and declare mission accomplished. However, L2NNNs produce better defense than all other methods that are known to the field, and our results point to a different direction than what people thought as the mainstream approach.\nThe degree of interest in our results can be felt by the number of non-reviewer comments we get, and one commenter has kindly tested our models, please see the comment titled \"Very well done evaluation\". We argue that that is side evidence that our results represents meaningful development.\nWe agree with the reviewer that a better introduction would make this paper more accessible to readers outside the subfield of adversarial attack and defense, and we will improve on that if this paper gets accepted and more space is allowed in final version.\n\n2) Regarding L2NNN as a general regularization technique beyond adversarial defense.\nWe thank the reviewer for the appreciation, and we ourselves are proud of our scrambled-label results. The results also provide a partial answer to the questions posed by Zhang et al. (2017) (best paper award ICLR 2017) which reported that no traditional regularization techniques seem to stop neural networks from memorizing random labels. Our results suggest that L2NNN is one regularization technique that can suppress memorization in exchange for stronger generalization.\nWe agree strongly with the reviewer that L2NNN as regularization has wider potentials outside adversarial defense. This indeed warrants a comprehensive study, which we will pursue in future works. For this paper, adversarial defense is our main results, and we want to scratch the surface for L2NNN's other properties.\n", "title": "our responses, part 1 of 2"}, "HkgYkYLVaQ": {"type": "rebuttal", "replyto": "Syg_IW86hm", "comment": "We thank the reviewer for the thoughtful review and helpful suggestions. We are updating the paper to incorporate some of the suggestions and will post a revision soon. In the responses below, related points are grouped together and ordered roughly by significance.\n\nBefore going into the list, we wish to emphasize that this paper sets a new state of the art in adversarial defense. For security and for generalization, robustness in terms of L2 norm and L_inf norm are both important. As the reviewer pointed out, notable defense progresses in the field so far have been mostly against L_inf-bounded attacks, except for some results in Madry et al. (2017). L2 defense is a less understood, and perhaps more difficult, problem than L_inf. Since both attack types are equally valid and there has been less advances on L2 defense, that makes any work in that area more important, and our paper represents a big step forward.\n\n1) Regarding the loss function.\nThe reason that (4) can express cross-entropy loss of an ordinary network is the following. Given any ordinary ReLU network without weight regularization, pick one layer, if we divide the weight matrix of this layer by a constant c, and divide the bias vectors of this layer and all subsequent layers by the same c, and we multiply the final logits by the same c, then there would no change in the end-to-end behavior of this network. The only change from the above is that the internal activations from that layer on are all scaled by 1/c. If we do the above for all layers and choose c=sqrt(b(W)) for each layer, where b(W) is from equation(2), we can convert the initial ordinary network to a nonexpansive network, only now with extra multipliers on the logits. After considering split layers (even when there is no split layers, we treat the last linear layer as a split layer, see first paragraph of Section 2.4), the multipliers on each logit become different. Therefore, the cross-entropy loss of the initial ordinary network is equal to term (4) of the nonexpansive network with proper u_1,...u_K values.\nThe average in equation (6) is averaging over a batch.\n\n2) Regarding L2 robustness and L_inf robustness.\nAs the reviewer kindly pointed out, we report defense results against both L2-bounded attacks and L_inf-bounded attacks. For L2, L2NNNs set a new state of the art in Tables 1 and 2. At the same time, L2NNNs exhibit, in Table 4, near-state-of-the-art defense against L_inf-bounded attacks.\nIt is true that Model 2's, from Madry et al. (2017), were trained with an L_inf adversary. However, let us quote from Madry et al. (2017): \"our MNIST model retains significant resistance to L2-norm-bounded perturbations too -- it has quite good accuracy in this regime even for epsilon=4.5.\" and \"our networks are very robust, achieving high accuracy for a wide range of powerful adversaries ...\" In other words, Madry et al. do not see the use of L_inf attacker in training as a limiting factor to L2 defense.\nWe are not aware of any published defense results that beat Madry et al. (2017) as measured by any norm. Please see also Athalye et al. (2018) for a comparison between Madry et al. (2017) and a set of other defense works. We are also not aware of any published MNIST or CIFAR models that were trained with an L2 adversary and achieved sizable white-box defense, and we ourselves have not found an efficient way to train with an L2 adversary.\nAnother fact to consider is that our Model 4's were trained with the same L_inf attacker and that improved L2 robustness as reported in Tables 1 and 2.\n\n3) Regarding confidence gap and robustness bound.\nThe reviewer is correct that we could have chosen to report provable robustness rather than measured robustness, by using the noise bound guarantee provided by confidence gaps. If we had chosen a smaller L2 epsilon, say 2 for MNIST, our Model 3 has a provable robustness of 17.0%; if we chose 1.5, our Model 3 has a provable robustness of 46.5%.\nThe reason that we chose measured robustness can be seen on examples in Figure 3. For each of the images, the guaranteed bound on noise L2-norm is half of the gap value, yet in reality the noise magnitude needed is much larger, 1.5X to 2X larger, than the guarantee. The reason is that the true noise bound is a function of local Lipschitz constants, as pointed out by Hein & Andriushchenko (2017), and local Lipschitz constants can be substantially below 1 in our models. The true bound is prohibitive to compute except for very small networks.\nTherefore, in order to demonstrate our defense with a more meaningful L2 epsilon of 3 and also compete with Madry et al. (2017) on the larger epsilon, we chose to report measured robustness.\n", "title": "our responses, part 1 of 2"}, "B1gzYu84p7": {"type": "rebuttal", "replyto": "Syg_IW86hm", "comment": "4) Regarding the robustness-accuracy tradeoff.\nThere is indeed a robustness versus nominal accuracy trade-off. We reported the trade-off in the second to last paragraph of Section 3.1, and we revisited the topic in the second to last paragraph of Section 3.3 to state a hypothesis.\nAs the reviewer pointed out, we are not the only defense work that face this trade-off. It remains an open question whether such trade-off is a necessary part of life.\nOur hypothesis on L2NNN's trade-off is stated at end of Section 3.3: by having a second goal of enlarging confidence gaps, L2NNN's parameter training automatically and selectively misclassify certain training data in exchange for larger gaps. In the context of original training labels, this implies that some original labels are ignored and that leads to lower nominal accuracy. Although by looking at examples in Figure 4, one could argue that some of the original labels are better ignored. If this hypothesis is true, this trade-off mechanism is a double-edged sword, as it both costs us nominal accuracy in Tables 1 and 2 and helps us in dealing with noisy data in Table 5. This is only a hypothesis, and we may have a better answer in future work.\n\n5) Regarding max-margin training.\nThank you very much for the suggestion. Do you mean this paper http://www.jmlr.org/papers/volume10/xu09b/xu09b.pdf? Please advise. We will study the connection for future work, and we also want to see if it is appropriate to cite in this paper.\n\nPlease let us know if we have missed anything and we'd be happy to continue the discussion.\n", "title": "our responses, part 2 of 2"}, "Syg9SDL4a7": {"type": "rebuttal", "replyto": "H1eKGidtnm", "comment": "3) Regarding the robustness-accuracy trade-off.\nThere is indeed a robustness versus nominal accuracy trade-off. We reported the trade-off in the second to last paragraph of Section 3.1, and we revisited the topic in the second to last paragraph of Section 3.3 to state a hypothesis.\nWe are not the only defense work that face this trade-off. As AnonReviewer2 pointed out, adversarial training has a similar trade-off. It also can be seen in the adversarial polytope work of https://arxiv.org/abs/1805.12514. It remains an open question whether such trade-off is a necessary part of life.\nOur hypothesis on L2NNN's trade-off is stated at end of Section 3.3: by having a second goal of enlarging confidence gap, L2NNN's parameter training automatically and selectively misclassify certain training data in exchange for larger gaps. In the context of original training labels, this implies that some original labels are ignored and that leads to lower nominal accuracy. Although by looking at examples in Figure 4, one could argue that some of the original labels are better ignored. This is only a hypothesis. We agree with the reviewer that this trade-off is an important subject to study, we may have a better answer in future work.\n\n4) Regarding omitted proofs.\nWe thank the reviewer for the suggestions and we are updating the appendix to add proofs, and will post the revision soon.\n\n5) Regarding loss terms (4) and (5).\nRemoving one of these two terms would not result in as much degradation as in Table 3. If to choose one of the two, it makes sense to use (5) and the end result would be slight degradation in nominal accuracy compared with the current results in Table 1 and 2.\n\n6) Regarding architecture.\nOur models 3 and 4 in Table 1 and 2 all use convolution layers followed by fully connected layers, some of which are split layers with stacks dedicated to individual logits. These are conventional architecture choices, and our unconventional elements are two-sided ReLU and norm-pooling. By the way, they are all available for download at the dropbox link on page 4. \nFor the scrambled-label experiments, as detailed in Tables 7 and 8, we want to be fair and build ordinary networks with two different architectures, one shallow and one deep. Then the ordinary-network section of Table 5 is entry-wise max of Tables 7 and 8. The L2NNNs use the same architecture for MNIST throughout this paper.\n\n7) Regarding hybrid models reported at end of Section 3.2.\nThe following are measurements of the said hybrid models under same settings in Tables 1 and 2, after 1000 iterations: MNIST 62.9%, CIFAR-10 6.4%. Please note that the base for comparison is Models 3 in Tables 1 and 2. The CIFAR-10 number is in line with expectation, and 6.4% is a degradation from 10.1%. The MNIST number, however, is an artifact of that the CW attack code was not designed for ensemble models and Carlini & Wagner can likely do much better if they know and take advantage of the hybrid mechanism. The real MNIST number ought to be somewhat below 20.1%.\n\n8) Regarding missing reference.\nThank you and we will added the reference.\n\nPlease let us know if we have missed anything and we'd be happy to continue the discussion.\n", "title": "our responses, part 2 of 2"}, "B1lLWoxqnQ": {"type": "review", "replyto": "ByxGSsR9FQ", "review": "I read this paper with some excitement. The authors propose a very sensible idea: simultaneously maximizing the confidence gap and constraining the Lipschitz constant of the network, thus achieving a guarantee that no L2-bounded perturbation can alter the prediction so long as the perturbation is bounded by some function of the confidence gap. \n\nThe main idea consists of three parts:\n 1) smooth networks (fixed, low Lipschitz constant)\n (2) loss function that explicitly maximizes the confidence gap (distance between largest and second-largest logits). \n (3) \u201cthe network architecture restricts confidence gaps as little as possible. We will elaborate.\u201d   \n\nThe first two conditions make plain sense. The third condition and subsequent elaborations are far too vague. What precisely is the property of restricting confidence gaps? At first glance this seems akin to the smoothness sought in property one. Even in the bulleted list, the authors owe the reader a clearer explanation.\n\nThe proposed model, denoted L2-nonexpansive neural networks (L2NNNs) and consists of a sensible form of Lipschitz-constant-enforcing weight regularization, a loss function that penalizes the confidence gap.\n\nTo address the third condition, the authors say only \u201cwe adapt various layers in new ways for the third condition, for example norm-pooling and two-sided ReLU, which will be presented later\u201d which is far too vacuous. At this point the reader is exposed to the third condition for the second time and yet it remains shrouded in mystery. The authors should elaborate here and describe what precisely, if anything, this third condition consists of. If it is not rigorously defined but only a heuristic notion, that would be fine, but this should be communicated clearly to the reader. \n\nA following paragraph introduces the notion of \u201cpreserving distance\u201d. However, what follows is too informal a discussion, and the rigorous definition never materializes. The authors say in one place \u201ca network that maximizes confidence gaps well must be one that preserves distance well\u201d. In this case, why do we need the third condition at all if the second condition appears to be sufficient?\n\nIn the next sections the authors describe the methods in greater detail and summarize their results. I have placed some more specific nittier comments in the ***small issues*** section below. But comment hear on the empirical findings.\n\nOne undersold finding here is that the existing methods (including the widely-believed-to-be-robust method due to MAdry 2017) that appear robust under FGSM attacks break badly under iterated attacks, and that the attacks go stronger up to 1M iterations, bringing accuracy below 10%. \n\nIn contrast the proposed method reaches 24% accuracy, which isn\u2019t magnificent, but does appear to outperform the model due to Madry. A comparison against the method due to Kolter & Wong seems in order. The authors do not implement methods based on the adversarial polytope due to their present un-scalability, but that argument would be better supported if the authors were addressing larger models on harder datasets (vs MNIST and CIFAR10).\n\nIn short, I like the main ideas in this paper although some more empirical elbow grease is in order, the third condition needs to be discussed more rigorously or discarded. Additionally the choice of loss function should be better justified. Why do we need the original cross-entropy objective at all. Why not directly optimize the confidence gap? Did the authors try this? Did it work? Apologies if I missed this detail. Overall, I am interested in the development of this paper and would like to give it a higher vote but believe the authors have a bit more work to do to make this an easier decision. Looking forward to reading the rebuttal.\n\n\n***Small issues***\nPage 1 \u201cnonexpansive neural networks (L2NNN)\u201d for agreement on pluralization, should be \u201cL2NNNs\u201d\n\n\u201cThey generalize better from noisy training labels than ordinary networks: for example, when 75% of MNIST training labels are randomized, an L2NNN still achieves 93.1% accuracy on the test set\u201d\nWhen you make a claim about accuracy of a proposed model, it must be made in reference to a standard model, even in the intro. It\u2019s well-known in general that DNNs perform well even under large amounts of label noise. Hard to say without reference if 93.1% represents a significant improvement.\n\nRepeated phrase on page 2:\n\u201cHow to adapt subtleties like recursion and splitting-reconvergence is included in the appendix.\u201d\n\u201cDiscussions on splitting-reconvergence, recursion and normalization are in the appendix.\u201d\n\nInputs to softmax cross-entropy should be both a set of logits and the label -- here the way the function is used in the notation does not match the proper function signature\n\nFigure --- do not put \u201cModel1, Model2, Model3, Model4\u201d. This is unreadable. Put some shortname and then define it in the caption. Once one knows the abbreviations, they should be able to look at the figure and understand it without constantly referencing the caption. \n\nTable 1-4 should be at the top of the page and arranged in a grid.  This wrapfigure floating in the middle of the page, while purely a cosmetic issue that should not bear on our deliberations, tortures the template unnecessarily, turning the middle 80% of page 5 into a one-column page unnecessarily.\n\nTable 4 should show comparison to Madry model. Also this is why you need a shortname in the legend. In order to understand table 4, the reader has to consult the caption for tables 1 and 2. \n\n\u201cIt is an important property that an L2NNN has an easily accessible measurement on how robust its decisions are\u201d\nI AGREE!", "title": "Nice idea, a few cool results, and a couple missing steps ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rklwt0punX": {"type": "rebuttal", "replyto": "HygkWS4I3m", "comment": "It turns out that comparing logit-to-image gradients across classifiers is harder than we thought. The issue is that logits need to be scaled properly to have a meaningful comparison of gradient magnitudes, and there does not seem to be a rigorous way to do so. However, we do think that the hypothesis stated above is plausible.", "title": "more"}, "HygkWS4I3m": {"type": "rebuttal", "replyto": "H1xLD-frhX", "comment": "Thank you very much for the comments and for trying out our models.\n\nIt is an excellent question and one we've wondered about. Our speculation is indeed that adversarial training makes gradient descent harder, as it has an effect of flattening gradients around training data points. Madry et al. (2017) is the best we are aware of, and for the MNIST classifier, their adversarial training was so successful that we suspect that gradients are near zero in some parts of the input space, and hence it takes more iterations for an attacker to make progress. In other words, we suspect that, within many linear sections of their ReLU network, the logits have nearly flat values. The results suggest that adversarial training alone does not achieve full coverage around original-image points, and linear sections with large gradients still exist and hence bad points do exist nearby. It becomes a question of after how many steps does an attacker guided by gradient descent stumble close enough to one bad point. By the trend in Table 1, it would not be surprising if 10 million iterations would knock the accuracy down further.\n\nActually we are intrigued and will do some gradient measurements which might put some numbers behind the speculation.\n", "title": "our answer to the question"}, "BJeSl4S0q7": {"type": "rebuttal", "replyto": "SJgOaOm0qX", "comment": "Hi Robin,\n\nThank you very much for the reference and we will cite it in Section 2.2.\n\nIt is interesting that what we call two-sided ReLU has shown values outside of the scope of adversarial robustness. Perhaps between our paper and the one you pointed out, it will become more accepted. There are a couple of differences which add to the synergy. We use two-sided ReLU for a different purpose (preserving distance for better robustness) and hence do not limit it to just convolution layers. We also propose a generalized scheme in Section 2.2 which can convert nonlinearities other than ReLU to two-sided forms which are nonexpansive and preserve distance better than the original nonlinearities.\n", "title": "thank you and we will add the reference"}, "rJlHciFfqX": {"type": "rebuttal", "replyto": "S1ekOTSbc7", "comment": "Thanks for the comment and we're happy to clarify.\n\nThe commenter's thought experiment is correct. However, the conclusion is not about Lipschitz smoothness, but rather about all classifiers, including humans. Indeed it is impossible for a human to have 100% accuracy on clean MNIST images and at the same time 100% after-attack accuracy with noise L2 norm limit of more than 2.83/2 = 1.42. Because there exists a point in the input space that is 1.42 distance away from an image of 4 and also 1.42 distance away from an image of 9.\n\nTherefore, we would like to rephrase the commenter's question to the following. What is a reasonable goal for a robust classifier? The answer, in our opinion, is one that mimics a reasonable human. A human can have at or near 100% accuracy on clean MNIST images, but would have a different degree of confidence for each individual image. For the said two images of 4 and 9, his/her confidence should be low, -- using our terminology in the paper, confidence gap would be less than 1.42*sqrt(2), -- while for the vast majority of MNIST images, his/her confidence should be high. Consequently his/her after-attack accuracy is less than 100%, which is perfectly fine. It's not the fault of the classifier but the fault of certain ambiguously written digits.\n\nIn this paper, we evaluate MNIST classifiers with noise L2 norm limit of 3. This implies that an attacker can modify nine pixels from pure white to pure black or vice versa, and it can modify more pixels with less swings. We believe that a human would have enough confidence to defend against this noise magnitude for the vast majority of MNIST images. If we were to speculate, a human would have after-attack accuracy of over 95%. That's the goal in our opinion and not 100% after-attack accuracy.\n\nBefore our work, the state of the art is 7.6% or less, as shown in Table 1. We advance that to 24.4%. Although this is still a far cry from 95%, it is a big step up from 7.6%, and is better than all existing techniques as far as we know.\n\nBy the way, the commenter's thought experiment is much related to why L2NNNs generalize well from noisy data with partially random labels. Please see Section 3.3, and in particular Table 6.\n\nPlease also see the second paragraph on page 2 about preserving distance. The noise with norm 4.8 that the commenter mentioned is an example of a distance that is likely lost, while the distance of 2.83 mentioned is an example of a distance that we want to preserve through an L2NNN.\n", "title": "our answer to the question"}, "SkggPmD-q7": {"type": "rebuttal", "replyto": "BJgI-hH-9Q", "comment": "Please see Table 4 on page 6. For L_inf defense, we are on par with Madry et al. (2017) for CIFAR-10, and Madry et al. (2017) is better on MNIST.\n\nIt seems that the commenter may have misread Madry et al. (2017). They use L_inf epsilon of 0.3 for MNIST and 8/256 for CIFAR-10, not 0.3 for both.\n\nWe disagree with how the commenter translates L_inf epsilon to L2 epsilon. As show in Table 1, Model 2 can barely defend against L2 epsilon of 3, not to mention 8.4. For individual examples, please see Figures 1 and 5.", "title": "our answer to the question "}, "SkenYzvZ57": {"type": "rebuttal", "replyto": "rkgisqr-9Q", "comment": "Thanks for the comment and we're happy to clarify.\n\nThe difference is not in architecture, but rather in how to regularize the last linear layer: we recommend treating the last layer as K independent filters rather than regularizing it as a single matrix. In a single-L2NNN classifier, the K logits become related to each other: for example, if one activation in the second-last layer increases by 1, with all other activations staying the same, the K logits as a vector can only change up to L2 norm of 1. In contrast, in a multi-L2NNN approach, each individual logit can increase or decrease up to 1.\n\nWe empirically observe that the multi-L2NNN approach results in better robustness than viewing the whole classifier as a single L2NNN.\n\nWe also empirically observe that having split layers, i.e., final separate stacks of layers where each stack computes a single logit, helps improve the performance. In the multi-L2NNN approach, each such stack is covered by one of the K L2NNNs.", "title": "our answer to the question"}, "SkxbKZwWc7": {"type": "rebuttal", "replyto": "BJl1rFr-9X", "comment": "Thanks for the comment and we're happy to clarify.\n\nModel 2's in Tables 1 and 2 were downloaded from Madry et al.'s GitHub pages (links in footnote on page 4) and were fetched under name \"secret\": these were released after they closed the black-box leaderboards and match what were reported in their paper.\n\nIt is true that Model 2's were trained with L_inf adversary. However, let us quote from Madry et al. (2017): \"our MNIST model retains significant resistance to L2-norm-bounded perturbations too -- it has quite good accuracy in this regime even for epsilon=4.5.\" and \"our networks are very robust, achieving high accuracy for a wide range of powerful adversaries ...\" In other words, Madry et al. do not see the use of L_inf attacker in training as a limiting factor to L2 defense.\n\nWe are not aware of any published defense results that beat Madry et al. (2017) as measured by any norm. Please see also Athalye et al. (2018) for a comparison between Madry et al. (2017) and a set of other defense works. We are also not aware of any published MNIST or CIFAR models that were trained with L2 adversary and achieved sizable white-box defense.\n\nAnother fact to consider is that our Model 4's were trained with the same L_inf attacker and that improved L2 robustness as reported in Tables 1 and 2.", "title": "our answer to the question"}}}