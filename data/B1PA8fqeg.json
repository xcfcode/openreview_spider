{"paper": {"title": "Multiagent System for Layer Free Network", "authors": ["Hiroki Kurotaki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["kurotaki@weblab.t.u-tokyo.ac.jp", "nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"], "summary": "We propose a multiagent system that have feed-forward networks as its subset but free from layer scheme.", "abstract": "We propose a multiagent system that have feedforward networks as its subset \nwhile free from layer structure with matrix-vector scheme.\nDeep networks are often compared to the brain neocortex or visual perception system.\nOne of the largest difference from human brain is the use of matrix-vector multiplication based on layer architecture.\nIt would help understanding the way human brain works\nif we manage to develop good deep network model without the layer architecture while preserving their performance.\nThe brain neocortex works as an aggregation of the local level interactions between neurons, \nwhich is rather similar to multiagent system consists of autonomous partially observing agents\nthan units aligned in column vectors and manipulated by global level algorithm.\nTherefore we suppose that it is an effective approach for developing more biologically plausible model while preserving compatibility with deep networks to alternate units with multiple agents.\nOur method also has advantage in scalability and memory efficiency.\nWe reimplemented Stacked Denoising Autoencoder(SDAE) as a concrete instance with our multiagent system and verified its equivalence with the standard SDAE from both theoritical and empirical perspectives.\nAdditionary, we also proposed a variant of our multiagent SDAE named \"Sparse Connect SDAE\",\nand showed its computational advantage with the MNIST dataset.", "keywords": []}, "meta": {"decision": "Reject", "comment": "The reviewers were consistent in their review that they thought this was a strong rejection.\n Two of the reviewers expressed strong confidence in their reviews.\n The main arguments made by the reviewers against acceptance were:\n Lack of novelty (R2, R3)\n Lack of knowledge of literature and development history; particularly with respect to biological inspiration of ANNs (R3)\n Inappropriate baseline comparison (R2)\n Not clear (R1)\n \n The authors did not provide a response to the official reviews. Therefore I have decided to follow the consensus towards rejection."}, "review": {"SJ-YF5I7l": {"type": "rebuttal", "replyto": "HJw9DQb7l", "comment": "> What novelty does your work contribute precisely beyond reframing existing approaches in slightly different terminology?\n\nOur novelty is to reinterpret the famous feedforward neural network as a multiagent system. This means our model avoids using global information as much as possible. For example, in Algorithm 2, our method doesn't depend on the number of the units.\n\n> You claim that your method is more biologically plausible can you justify that point?\n\nIn our method, the layer restriction imposed on the standard feedforward network is removed. This property is similar to biological neural network which has no layer structure.\n\n> You show the equivalence of SDAE with your reimplementation, but can you clarify what are the advantages of your methods, either theoretically or through experiments by comparing the computation time of both methods.\n\nOur multiagent model is free from global level manipulation and processing, but we can omit these advantage and limit the localty property of our model to make it match with the standard SDAE which has global operations. In Algorithm 3, we limit the order of processing messages using the global information, the number of hidden units. In this case, our model no longer has advantage over standard SDAE because the both models do exactly same computation as proved in Section 3, which shows that our model has a strong relationship with the slightest modification.\n\n> For your sparse connect SDAE. Please clarify: Are connections dropped on a previously trained model? Using what precise rule? Do you retrain it with the dropped connections? Or do you stochastically drop connections during training (as in dropconnet)? \n\nConnections are dropped before the training to ensure computational advantage during the training.\n\n> Can you give more details about your experimentations, architecture of the network and hardware used ?\n\nWe append the detailed description of experiments in Appendix B.\n", "title": "Our model is free from global information including layer structure"}, "HJR-wXGQe": {"type": "rebuttal", "replyto": "ry8U3CCGg", "comment": "> From Sec 3.2 this seems to include any computation graph. \n> Unfortunately, these are only ever vaguely described in relation to MLPs and SDAs, which is problematic.\n\nWe defined the properties the nodes in our model can contain and update in Table 1. We also defined The initialization processes in Algorithm 2. These definitions differentiate our proposed model from general computation graphs and describe the relationship to MLPs and SDAs.\n\n> The authors should be able to define these concisely and in isolation, before comparing them to existing methods.\n\nAlgorithm 2 and Table 1 shows our proposed model in isolated and independent form. ", "title": "We defined it in Algorithm 2 and Table 1"}, "HJw9DQb7l": {"type": "review", "replyto": "B1PA8fqeg", "review": "What novelty does your work contribute precisely beyond reframing existing approaches in slightly different terminology?\n\nYou claim that your method is more biologically plausible can you justify that point ?\n\nYou show the equivalence of SDAE with your reimplementation, but can you clarify what are the advantages of your methods, either theoretically or through experiments by comparing the computation time of both methods.\n\nFor your sparse connect SDAE. Please clarify: Are connections dropped on a previously trained model? Using what precise rule? Do you retrain it with the dropped connections? Or do you stochastically drop connections during training (as in dropconnet)? \n\nCan you give more details about your experimentations, architecture of the network and hardware used ?\nThe paper reframes feed forward neural networks as a multi-agent system.\n\nIt seems to start from the wrong premise that multi-layer neural networks were created expressed as full matrix multiplications. This ignores the decades-long history of development of artificial neural networks, inspired by biological neurons, which thus started from units with arbitrarily sparse connectivity envisioned as computing in parallel. The matrix formulation is primarily a notational convenience; note also that when working with sparse matrix operations (or convolutions) zeros are neither stored not multiplied by.\n\nBesides the change in terminology, essentially renaming neurons agents, I find the paper brings nothing new and interesting to the table.\n\nPulling in useful insights from a different communitiy such as multi-agent systems would be most welcome. But for this to be compelling, it would have to be largely unheard-of elements in neural net research, with clear supporting empirical evidence that they significantly improve accuracy or efficiency. This is not achieved in the present paper.", "title": "Clarifications", "rating": "1: Trivial or wrong", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SJ-ORWDEl": {"type": "review", "replyto": "B1PA8fqeg", "review": "What novelty does your work contribute precisely beyond reframing existing approaches in slightly different terminology?\n\nYou claim that your method is more biologically plausible can you justify that point ?\n\nYou show the equivalence of SDAE with your reimplementation, but can you clarify what are the advantages of your methods, either theoretically or through experiments by comparing the computation time of both methods.\n\nFor your sparse connect SDAE. Please clarify: Are connections dropped on a previously trained model? Using what precise rule? Do you retrain it with the dropped connections? Or do you stochastically drop connections during training (as in dropconnet)? \n\nCan you give more details about your experimentations, architecture of the network and hardware used ?\nThe paper reframes feed forward neural networks as a multi-agent system.\n\nIt seems to start from the wrong premise that multi-layer neural networks were created expressed as full matrix multiplications. This ignores the decades-long history of development of artificial neural networks, inspired by biological neurons, which thus started from units with arbitrarily sparse connectivity envisioned as computing in parallel. The matrix formulation is primarily a notational convenience; note also that when working with sparse matrix operations (or convolutions) zeros are neither stored not multiplied by.\n\nBesides the change in terminology, essentially renaming neurons agents, I find the paper brings nothing new and interesting to the table.\n\nPulling in useful insights from a different communitiy such as multi-agent systems would be most welcome. But for this to be compelling, it would have to be largely unheard-of elements in neural net research, with clear supporting empirical evidence that they significantly improve accuracy or efficiency. This is not achieved in the present paper.", "title": "Clarifications", "rating": "1: Trivial or wrong", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "ry8U3CCGg": {"type": "review", "replyto": "B1PA8fqeg", "review": "From Sec 3.2 this seems to include any computation graph. Unfortunately, these are only ever vaguely described in relation to MLPs and SDAs, which is problematic. The authors should be able to define these concisely and in isolation, before comparing them to existing methods.Unfortunately, the paper is not clear enough for me to understand what is being proposed. At a high-level the authors seem to propose a generalization of the standard layered neural architecture (of which MLPs are a special case), based on arbitrary nodes which communicate via messages. The paper then goes on to show that their layer-free architecture can perform the same computation as a standard MLP. This logic appears circular. The low level details of the method are also confusing: while the authors seem to be wanting to move away from layers based on matrix-vector products, Algorithm 4 nevertheless resorts to matrix-vector products for the forward and backwards pass. Although the implementation relies on asynchronously communicating nodes, the \u201clocking\u201d nature of the computation makes the two entirely equivalent.", "title": "What is a multiagent network ?", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Sk9jkkINe": {"type": "review", "replyto": "B1PA8fqeg", "review": "From Sec 3.2 this seems to include any computation graph. Unfortunately, these are only ever vaguely described in relation to MLPs and SDAs, which is problematic. The authors should be able to define these concisely and in isolation, before comparing them to existing methods.Unfortunately, the paper is not clear enough for me to understand what is being proposed. At a high-level the authors seem to propose a generalization of the standard layered neural architecture (of which MLPs are a special case), based on arbitrary nodes which communicate via messages. The paper then goes on to show that their layer-free architecture can perform the same computation as a standard MLP. This logic appears circular. The low level details of the method are also confusing: while the authors seem to be wanting to move away from layers based on matrix-vector products, Algorithm 4 nevertheless resorts to matrix-vector products for the forward and backwards pass. Although the implementation relies on asynchronously communicating nodes, the \u201clocking\u201d nature of the computation makes the two entirely equivalent.", "title": "What is a multiagent network ?", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}