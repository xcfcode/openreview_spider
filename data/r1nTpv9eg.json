{"paper": {"title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "authors": ["Misha Denil", "Pulkit Agrawal", "Tejas D Kulkarni", "Tom Erez", "Peter Battaglia", "Nando de Freitas"], "authorids": ["mdenil@google.com", "pulkitag@berkeley.edu", "tkulkarni@google.com", "etom@google.com", "peterbattaglia@google.com", "nandodefreitas@google.com"], "summary": "We train agents to conduct experiments in interactive simulated physical environments.", "abstract": "When encountering novel objects, humans are able to infer a wide range of physical properties such as mass, friction and deformability by interacting with them in a goal driven way. This process of active interaction is in the same spirit as a scientist performing experiments to discover hidden facts. Recent advances in artificial intelligence have yielded machines that can achieve superhuman performance in Go, Atari, natural language processing, and complex control problems; however, it is not clear that these systems can rival the scientific intuition of even a young child. In this work we introduce a basic set of tasks that require agents to estimate properties such as mass and cohesion of objects in an interactive simulated environment where they can manipulate the objects and observe the consequences. We found that deep reinforcement learning methods can learn to perform the experiments necessary to discover such hidden properties. By systematically manipulating the problem difficulty and the cost incurred by the agent for performing experiments, we found that agents learn different strategies that balance the cost of gathering information against the cost of making mistakes in different situations.  We also compare our learned experimentation policies to randomized baselines and show that the learned policies lead to better predictions.", "keywords": ["Deep learning", "Reinforcement Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The following statement best summarizes the contribution: \"This paper shows that model free RL methods can learn how to gather information about physical properties of objects, even when this information is not available to a passive observer, and use this information to make decisions.\" So this is not a paper about new theory or algorithms, but rather about solving the problem of acquiring knowledge about the physics of the world around us, which is important for many problems and helps explain human performance in many tasks. There are still some concerns about the depth-of-analysis of the paper, but on balance, it is seen as an unconventional but interesting paper. As per AnonReviewer6, the final version could still aim to better address \"What should other researchers focus on if they are trying to build agents that can understand physics intuitively (building off this work)?\"\n -- Area chair"}, "review": {"B1YPso9Ug": {"type": "rebuttal", "replyto": "r1nTpv9eg", "comment": "The machine learning field has profited from hundreds of yearly publications that propose a new algorithm, which outperforms baseline algorithms and has a theoretical proof for some idealized form of the algorithm. This paper, however, is different. It does not propose a new algorithm. It incidentally uses an RL algorithm, but many other RL algorithms could have been used just as well. This paper is not about new models or algorithms.\n\nSo what is this paper about? It is an initial research step toward understanding objects and intuitive reasoning in physical worlds. Why is this important? Our best AI agents currently fail on simple control tasks and simple games, such as Montezuma\u2019s Revenge, because when they look at a screen that has a ladder, a key and a skull they don\u2019t immediately know that keys open doors, that skulls are probably hazardous and best avoided, that ladders allow us to defy gravity, etc. The understanding of physics, relations and objects enables children to solve seemingly simple problems that our best existing AI agents do not come close to begin to solve.   \n\nEndowing our agents with knowledge of objects would help enormously with planning, reasoning and exploration. Yet, this is far from trivial. First, what is an object? It turns out this is not an easy question to answer and has baffled psychologists and philosophers. To simplify matters, in this paper we restrict our attention to rigid bodies. Staring at a thing in one\u2019s hands is not enough to understand what it is. Vision or open loop perception in general is not enough. Trying to pull the thing apart to see whether it is a single entity might help. (Children do indeed love to tear things like paper apart to understand them.) Touching the thing to see what happens also helps --- perhaps it lights up and starts beeping. Further interaction might reveal that it enables us to remotely talk to someone else, to tweet, etc. Much of the knowledge gained is the result of interaction, that is perception by action. \n\nThis paper is also about designing tasks to understand how agents acquire intuitive reasoning and experimentation strategies in physical worlds. An abundant body of evidence in psychology --- see eg the works of Gerd Gigerenzer https://scholar.google.co.uk/citations?user=iw7cepUAAAAJ --- shows that people use intuitive rules to reason about physics, eg a baseball player catching a ball. How do agents acquire such intuition? In this paper, we show for the first time how this is possible with RL and the right type of task.  \n\nFor example, in the proposed Which is Heavier task we show that our agents learn a valuable intuitive reasoning ability. If the agent probes three blocks blocks and notices that one of them remains much closer to the ground than the other two it doesn\u2019t even bother to try the forth block. It simply points at the block that didn\u2019t lift and chooses to terminate. Through experience, the agent has learned a valuable rule of thumb that enables it to solve the task efficiently, terminating at an optimal stopping time. Our experiments show clearly that our agents have learned to act in this reasonable manner.\n\nThis paper shows that model free RL methods can learn how to gather information about physical properties of objects, even when this information is not available to a passive observer, and use this information to make decisions. \n\nPrevious approaches to this problem have relied on either explicit knowledge of the underlying structure of the environment (eg hard-wired physical laws) or on exploiting correlations between material appearance and physical properties, see eg the many convnets for intuitive physics research, Galileo, physics 101, learning to poke by poking, etc. (Incidentally, please note that the main author of the work on learning to poke by poking is also author here.)\n\nOne of the contributions of this paper is to show that our agents can still learn about properties of objects, even when the connection between material appearance and physical properties is broken. This setting allows us to show that our agents are not merely learning that blocks are heavy; they are learning how to check if blocks are heavy.\n\nNone of the previous approaches give a complete account of how agents could come to understand the physical properties of the world around them.  Specifying a model manually is difficult to scale, generalize and to ground in perception.  Making predictions from only visual properties will fail to distinguish between objects that look similar, and certainly it will be insufficient to answer whether a closed thermos is full or empty.  Our approach is a novel contribution to the field of learning intuitive physics that addresses an important gap that had not been filled by previous approaches.  ", "title": "Why This Paper is Important"}, "S1Kroi5Le": {"type": "rebuttal", "replyto": "r1nTpv9eg", "comment": "We would like to highlight to all reviewers that the experiments have been updated substantially since the previous version of the paper.  In particular, we now include the missing results for Fist Pixels as requested by Reviewer 3, as well as additional experiments comparing our models to random baseline policies in both environments.\n\nIn the process of making these updates we have have made several minor changes to the environments themselves. These changes have made the tower environment harder to solve, but do not alter any conclusions drawn from the experiments.", "title": "Updated Environments"}, "BJS1jjcLx": {"type": "rebuttal", "replyto": "B1YP4Db4l", "comment": "Thank you for your balanced feedback. \n\nIn addition to the specific issues addressed below, please also see our general reply which addresses the importance and positioning of this paper in spite of not not offering an algorithmic contribution.\n\nTo address your concern over the lack of the Fist Pixels setting for the Towers environment, we have conducted the experiment you requested. We have added the corresponding plots to the paper which show that the agents are also able to learn in this setting.  We have also modified the \"Waiting for information\" experiment to use the Fist Pixels agent (replacing the Fist Features agent we used for this experiment in the original version of the paper). Thank you for this suggestion to improve the paper.\n\nYour suggestion to compare against baselines that use a fixed (randomized) policy is a good one.  We have added experiments on both environments comparing the performance of our agents using learned vs randomized interaction policies (see the updated paper for full details).  These experiments show that when using the learned interaction policies agents are more accurate and often take less time to produce correct outputs as compared to randomized interactions.\n", "title": "Thank you for your feedback. "}, "ryJacsq8l": {"type": "rebuttal", "replyto": "rJOpuGQVl", "comment": "Thank you for your careful feedback.\n\nI think there is some confusion in comparing this paper to Agrawal et al. (2016), since the approach taken here is very different.  In that paper the model being learned is a model of the environment dynamics from vision.  Their model must learn to map static visual observations to dynamic properties of the objects by relying on correlations between appearance and material properties.  In this work we have carefully constructed our environments to eliminate these correlations between static appearance and physical properties, and because of this the model of Agrawal et al. (2016) would be unable to complete our tasks.\n\nThe relationship between appearance and physical properties is quite rich, but relying on vision alone gives an incomplete physical story. For example, the approach of Agrawal et al. (2016) would have trouble distinguishing between a full thermos and an empty thermos, or between a bag full of rocks and a bag full of tennis balls.\n\nThere are two different questions at play here:\n1. Given the appearance of an object, how will it move when I poke it?\n2. If I want to know how heavy an object is (or if it comes apart, etc), how should I poke it? \n\nBoth of these are questions are important. The work of Agrawal et al. (2016) targets the first question, whereas this work is focused on the second.\n\nThe purpose of analyzing the which is heavier experiments as a function of the mass gap is to show that the solutions found by the agents behave similarly to what we would expect from solving the underlying structured problem directly.  Indeed it does not seem important to have agents make fine distinctions between extremely similar masses, which is why we did not focus on ensuring that our agents obtain perfect solutions to the more difficult settings for this task.\n\nHowever, probing this axis of the problem allows us to better understand the solutions we obtain.  In particular, the fact that episode length is correlated with instance level difficulty shows that our agents learn an adaptive strategy for gathering information, that behaves as though the agents are reasoning using an implicitly learned model.  The fact that this effect persists at the instance level suggests the agents have learned more than simply to identify the generating distribution of problems.\n\nWe did not compare to other RL baselines because the choice of RL algorithm is not central to the paper. We chose to use A3C because we have an implementation of this algorithm that is robust and scalable, but we have no reason to think that there is anything unique about A3C that would preclude other algorithms from obtaining similar results, and introducing a comparison between different learning algorithms here would only distract from the message of the paper.\n", "title": "Thank you for your careful feedback."}, "Sy3KcocUx": {"type": "rebuttal", "replyto": "H1_VdJf4g", "comment": "Thank you for your suggestions to improve the precision of the paper.  In particular, our intent when describing the the Which is Heavier environment as a \"latent bandit\" seems to have caused quite a bit of confusion and this is connected to our imprecise formulation of our \"question answering\" framework which you identified earlier.  We will attempt to make these ideas more precise as we continue to update the paper.", "title": "Thank you for your suggestions to improve the precision of the paper."}, "S1-89s5Ll": {"type": "rebuttal", "replyto": "SJgSV_rBg", "comment": "Thank you for your feedback.\n\nWith regards to the desire for more analysis of the learned representations, we have added some experiments comparing the learned policies to a randomized baselines.  These experiments show that when using the learned interaction policies agents are more accurate and often take less time to produce correct outputs as compared to randomized interactions.\n\nWe would also draw your attention, particularly for the which is heavier environment, to the experiments where we examine episode length as a function of difficulty.  These are intended to show that the learned representations include information both about the prior distribution of masses (the population level experiment) as well as information the agent's state of knowledge about the mass gap for individual instances.  We assess this by measuring the behavior of the agent, but the behavior is wholly determined by the representations.\n\n> \"why does one \"must\" interact with objects in order to learn about the properties? Can't we also learn through observation?\"\n\nThis is intended as a descriptive statement about our particular environments, not as a principle of how learning must happen in a general setting.  Our environments are designed so that mere passive observation is not sufficient to perform better than chance, and therefore agents \"must\" interact with the environment in order to learn.  In real world settings appearance will often provide strong clues as to physical properties, but for this paper we have deliberately removed such clues from our environments.\n\n> I am a bit confused why having a small mass gap makes the task harder (unless it's really close to 0). Shouldn't a machine be possible to distinguish even a pixel difference of speed? If not, isn't this just because of the network architecture?\n\n\nThe mass gap is frequently quite small in the more difficult settings (see Figure 1 (right)), when this is not the case the task is reliably solved perfectly. It is possible a different architecture or more training would allow the agents to achieve perfect performance on the more difficult settings as well; however, as we argue in our reply to Reviewer 6, achieving perfect performance on this task is not really the point, and the value of doing so is questionable.\n\n> \"For example, how does an agent really know when the instance is any more difficult? Doesn't this really depend on the empirically learned distribution of training samples (i.e. P(m_3 | m_1, m_2), where m_i indicates masses of object 1, 2, and 3)?\"\n\nThe agent is not told explicitly about the difficulty of any instance. It must come to know which instances are difficult through interaction, and the purpose of the experiments on the which is heavier environment are intended to show that the agent does in fact come to know this.  The reason for including both population and instance level experiments is to show that what is learned goes beyond simply the conditional distributions of masses: in the instance level experiments the conditional distributions you mention are fixed and we still see behavior that adapts to the difficulty of the individual instances.\n\n> Any baseline approach?\n\nWe have added baseline comparisons for both environments.  Please see the updated version of the paper.", "title": "Review reply"}, "HykI2MeNe": {"type": "rebuttal", "replyto": "SksLfo17x", "comment": "1.1.\n\nThere are many approaches to analyzing how agents do and should make tradeoffs between acting in ignorance and the cost of acquiring more information, as you note.  In this paper the tradeoff that needs to be made is deliberately very simple.  The agent can either pay a fixed cost for gathering more information or can commit to a choice given their current knowledge.  \n\nHowever, much of the power of these approaches comes from giving a framework for balancing between several options with different costs.  For example, when the cost of actions are fixed the \"expected value of control\" framework reduces to simply linearly shifting the reward by the (fixed) cost of action.\n\nI think a more detailed development of the connection of cost of information to these other frameworks is best left for future work where we require agents to make more sophisticated information tradeoffs, where the tools they offer will have more power.\n\n1.2.\n\nThank you for pointing out the connection to Sutton's work. I agree this connection is one we should make explicit in the paper.  In Sutton's language our Section 2 might be better titled \"Encouraging Agents to Ask Questions\", and what we call the \"implicit question posed by the environment\" would be better called \"the question the environment encourages the agent to ask\".\n\nWe will add some discussion about how our setup is related to Sutton's framework to the final version of the paper.\n\n2.\n\nThe equivalence of which is heavier to a bandit problem is at the \"question\" level  (in our sense, not in Sutton's sense), rather than the behavioral level.  The underlying problem is a bandit in the sense that the return (mass) of each arm (block) does not change as a function of actions taken by the agent.  The instantiation of this problem in a physical environment (the physical lens) means that the agent needs to deal with the environment as an MDP, but the underlying structure of the problem to be solved is simpler.\n\n3. \n\nThe agent controls the length of the episode by choosing when to produce a label.  Taking a labeling action always immediately ends the episode.\n\nIf we interpret the discount factor as a probability of termination then it effectively controls the cost of information by forcing the agent to behave as if the episode might terminate prematurely at any step even though this event cannot actually occur.  I agree that the semantics of this seem sort of strange, but stochastic termination is not the only way to interpret the discount factor.  We can also interpret the discount factor deterministically as explicitly de-valuing rewards that take longer to obtain, and the semantics of this interpretation are less strange in our setting.\n\nThe cost of information must be extrinsic.  It can be imposed explicitly (as we do) or implicitly (through metabolic costs, time sensitive decisions, etc), but the only way for information to have a cost is for the cost to be imposed through properties of the environment.", "title": "reply"}, "HJgUsflNx": {"type": "rebuttal", "replyto": "SklkQz1mg", "comment": "We have trained agents that include no-op actions in the \"direct\" action space (although they are not discussed in the paper)  We have also trained agents that use a completely different \"point and throw\" action space where there are (effectively) no-op actions available.  In both cases this does not affect the ability of agents to solve the task.\n\nThere are a few confounding factors that I think contribute to the no-op actions not being helpful:\n\nThere is always time pressure from the discount factor and waiting for a block to settle after poking it takes several frames.\nThe fact that small distinctions in mass are relevant is not explicitly made available to the learning algorithm, so it needs to learn to make these distinctions.  This can manifest as a capacity issue (learning to make more fine distinctions requires more representational power) or a learning issue (discovering that fine distinctions are relevant is hard).\n\nIt is true that in principle this task is perfectly solvable by measuring small differences in displacement of the blocks after poking them, but in practice these differences can be quite small in episodes where the mass gap is small. \n", "title": "which is heavier difficulty"}, "HJEMizeNl": {"type": "rebuttal", "replyto": "SySZztkQg", "comment": "The contributions of this paper are to show\n\nHow to construct environments that ask questions about latent physical properties by constructing environments with latent structure\nThat RL agents can perform well on these tasks without explicit access to this latent structure\nHow to analyze the behavior of these agents by relating their behavior to behaviors we would expect from solutions that exploit the latent structure\nThat RL agents exhibit these properties\n\nWe can add more pictures of two environments to the paper to visualize inputs, and we can also make some videos available that show trained agents behaving in the environments.\n", "title": "."}, "SksLfo17x": {"type": "review", "replyto": "r1nTpv9eg", "review": "1. [Related work] I would appreciate if you could comment on the following :\n\n1.1 I think that there is a great opportunity to develop the connection of the \"cost of information\" with ideas from Bounded Rationality and the \"expected value of control\" (Botvinick).  There is also a great deal of related work on the value of information in economics and OR (Howard, Delage, etc). \n\n1.2 The partition between question/answer and its role in representation learning is an important concept of Sutton's \"Experience-oriented\" perspective on AI. This view underlies early work on PSRs, TD networks and more recently GVFs. \n\n2. [Bandit formulation] Analyzing your problem under the lens of Bandit theory could be relevant, but I think that the equivalence to a \"latent bandit\" that you put forward in section 4.1 seems incorrect. The way that the problem is posed from the very beginning is under the RL setting, thus assuming an MDP. The use of deep learning is to make state, and we're therefore not in the bandit setting. \n\n3. [Discount factor] The way that you use the discount factor is really much like a regularizer for the \"cost of information\". Since the discounted setting in MDPs is equivalent to a finite horizon MDP with random termination, I can see why this can encourage the agent to \"answer as quickly as possible\". However, I have the feeling that this termination event should be explicit, and not implicit through the discount factor. In other words: the agent should have direct control over this termination decision. In your setting, it seems that the cost of information is purely \"extrinsic\". What do you think is the right balance between intrinsic vs extrinsic ? Is it purely environmental or self-motivated ? \n\nThis paper investigates the question of gathering information (answering question)\nthrough direct interaction with the environment. In that sense, it is closely\nrelated to \"active learning\" in supervised learning, or to the fundamental\nproblem of exploration-exploitation in RL. The authors consider a specific \ninstance of this problem in a physics domain and learn\ninformation-seeking policies using recent deep RL methods.\n\nThe paper is mostly empirical and explores the effect of changing the\ncost of information (via the discount factor) on the structure of the learned\npolicies. It also shows that general-purpose deep policy gradient methods are\nsufficient powerful to learn such tasks. The proposed environment is, to my knowledge,\nnovel as well the task formulation in section 2. (And it would be very valuable to the\nthe community if the environment would be open-sourced)\n\nThe expression \"latent structure/dynamics\" is used throughout the text and the connection\nwith bandits is mentioned in section 4. It therefore seems that authors aspire\nfor more generality with their approach but the paper doesn't quite fully ground\nthe proposed approach formally in any existing framework nor does it provide a\nnew one completely.\n\nFor example: how does your approach formalize the concept of \"questions\" and \"answers\" ?\nWhat makes a question \"difficult\" ? How do you quantify \"difficulty\" ?\nHow do you define the \"cost of information\"? What are its units (bits, scalar reward), its semantics ?\nDo you you have an MDP or a POMDP ? What kind of MDP do you consider ?\nHow do you define your discounted MDP ? What is the state and action spaces ?\nSome important problem structure under the \"interaction/labeling/reward\"\nparagraph of section 2 would be worth expressing directly in your definition\nof the MDP: labeling actions can only occur during the \"labeling phase\" and that the transition\nand reward functions have a specific structure (positive/negative, lead to absorbing state).\nThe notion of \"phase\" could perhaps be implemented by considering an augmented state space : $\\tilde s = (s, phase)$\n", "title": "Related work, bandit, discount factor", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H1_VdJf4g": {"type": "review", "replyto": "r1nTpv9eg", "review": "1. [Related work] I would appreciate if you could comment on the following :\n\n1.1 I think that there is a great opportunity to develop the connection of the \"cost of information\" with ideas from Bounded Rationality and the \"expected value of control\" (Botvinick).  There is also a great deal of related work on the value of information in economics and OR (Howard, Delage, etc). \n\n1.2 The partition between question/answer and its role in representation learning is an important concept of Sutton's \"Experience-oriented\" perspective on AI. This view underlies early work on PSRs, TD networks and more recently GVFs. \n\n2. [Bandit formulation] Analyzing your problem under the lens of Bandit theory could be relevant, but I think that the equivalence to a \"latent bandit\" that you put forward in section 4.1 seems incorrect. The way that the problem is posed from the very beginning is under the RL setting, thus assuming an MDP. The use of deep learning is to make state, and we're therefore not in the bandit setting. \n\n3. [Discount factor] The way that you use the discount factor is really much like a regularizer for the \"cost of information\". Since the discounted setting in MDPs is equivalent to a finite horizon MDP with random termination, I can see why this can encourage the agent to \"answer as quickly as possible\". However, I have the feeling that this termination event should be explicit, and not implicit through the discount factor. In other words: the agent should have direct control over this termination decision. In your setting, it seems that the cost of information is purely \"extrinsic\". What do you think is the right balance between intrinsic vs extrinsic ? Is it purely environmental or self-motivated ? \n\nThis paper investigates the question of gathering information (answering question)\nthrough direct interaction with the environment. In that sense, it is closely\nrelated to \"active learning\" in supervised learning, or to the fundamental\nproblem of exploration-exploitation in RL. The authors consider a specific \ninstance of this problem in a physics domain and learn\ninformation-seeking policies using recent deep RL methods.\n\nThe paper is mostly empirical and explores the effect of changing the\ncost of information (via the discount factor) on the structure of the learned\npolicies. It also shows that general-purpose deep policy gradient methods are\nsufficient powerful to learn such tasks. The proposed environment is, to my knowledge,\nnovel as well the task formulation in section 2. (And it would be very valuable to the\nthe community if the environment would be open-sourced)\n\nThe expression \"latent structure/dynamics\" is used throughout the text and the connection\nwith bandits is mentioned in section 4. It therefore seems that authors aspire\nfor more generality with their approach but the paper doesn't quite fully ground\nthe proposed approach formally in any existing framework nor does it provide a\nnew one completely.\n\nFor example: how does your approach formalize the concept of \"questions\" and \"answers\" ?\nWhat makes a question \"difficult\" ? How do you quantify \"difficulty\" ?\nHow do you define the \"cost of information\"? What are its units (bits, scalar reward), its semantics ?\nDo you you have an MDP or a POMDP ? What kind of MDP do you consider ?\nHow do you define your discounted MDP ? What is the state and action spaces ?\nSome important problem structure under the \"interaction/labeling/reward\"\nparagraph of section 2 would be worth expressing directly in your definition\nof the MDP: labeling actions can only occur during the \"labeling phase\" and that the transition\nand reward functions have a specific structure (positive/negative, lead to absorbing state).\nThe notion of \"phase\" could perhaps be implemented by considering an augmented state space : $\\tilde s = (s, phase)$\n", "title": "Related work, bandit, discount factor", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SySZztkQg": {"type": "review", "replyto": "r1nTpv9eg", "review": "It will be great if the authors can clarify the technical contributions of the paper. Is the main contribution of the paper on applying \"LSTMs with convolutional layers using Asynchronous Advantage Actor Critic\" for the problem of physical property inference?\n\nAlso, having a figure visualizing exact inputs and outputs of LSTMs and how reinforcement signals influence them might help the understanding.\nThis paper presents interesting experimental findings that state-of-the-art deep reinforcement learning methods enable agent learning of latent (physical) properties in its environment. The paper formulates the problem of an agent labeling environmental properties after interacting with the environment based on its actions, and applies the deep reinforcement learning model to evaluate whether such learning is possible. The approach jointly learns the convolutional layers for pixel-based perception and its later layers for learning actions based on reinforcement signals.\n\nWe have a mixed opinion about this paper. The paper is written clearly and presents interesting experimental findings. It introduces and formulates a problem potentially important for many robotics applications. Simultaneously, the paper suffers from lacking algorithmic contributions and missing (some of) crucial experiments to confirm its true benefits.\n\nPros:\n\n+ This paper introduces a new problem of learning latent properties in the agent's environment.\n\n+ The paper presents a framework to appropriately combine existing tools to address the formulated problem.\n\n+ The paper tries reinforcement learning with image inputs and fist-like actuator actions. This will lead to its direct application to robots.\n\nCons:\n\n- Lacking algorithmic contribution: this paper applies existing tools/methods to solve the problem rather than developing something new or extending them. The approach essentially is training LSTMs with convolutional layers using the previous Asynchronous Advantage Actor Critic.\n\n- In the Towers experiment, the results of probably the most important setting, \"Fist Pixels\", are missing. This setting receiving pixel inputs and using the Fist actuator in a continuous space is the setting closest to real-world robots, and thus is very important to confirm whether the proposed approach will be directly applicable to real-world robots. However, Figure 5 is missing the results with this setting. Is there any reason behind this?\n\n- The paper lacks its comparison to any baseline methods. Without explicit baselines, it is difficult to see what the agent is really learning and what aspect of the proposed approach is benefitting the task. For instance, in the Towers task, how would an agent randomly pushing/hitting the tower (using 'Fist') a number of times and then passively observing its consequence to produce a label perform compared to this approach? That is, how would an approach with a fixed action policy (but with everything else) perform compared to the full deep reinforcement learning version?", "title": "clarification of technical contributions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1YP4Db4l": {"type": "review", "replyto": "r1nTpv9eg", "review": "It will be great if the authors can clarify the technical contributions of the paper. Is the main contribution of the paper on applying \"LSTMs with convolutional layers using Asynchronous Advantage Actor Critic\" for the problem of physical property inference?\n\nAlso, having a figure visualizing exact inputs and outputs of LSTMs and how reinforcement signals influence them might help the understanding.\nThis paper presents interesting experimental findings that state-of-the-art deep reinforcement learning methods enable agent learning of latent (physical) properties in its environment. The paper formulates the problem of an agent labeling environmental properties after interacting with the environment based on its actions, and applies the deep reinforcement learning model to evaluate whether such learning is possible. The approach jointly learns the convolutional layers for pixel-based perception and its later layers for learning actions based on reinforcement signals.\n\nWe have a mixed opinion about this paper. The paper is written clearly and presents interesting experimental findings. It introduces and formulates a problem potentially important for many robotics applications. Simultaneously, the paper suffers from lacking algorithmic contributions and missing (some of) crucial experiments to confirm its true benefits.\n\nPros:\n\n+ This paper introduces a new problem of learning latent properties in the agent's environment.\n\n+ The paper presents a framework to appropriately combine existing tools to address the formulated problem.\n\n+ The paper tries reinforcement learning with image inputs and fist-like actuator actions. This will lead to its direct application to robots.\n\nCons:\n\n- Lacking algorithmic contribution: this paper applies existing tools/methods to solve the problem rather than developing something new or extending them. The approach essentially is training LSTMs with convolutional layers using the previous Asynchronous Advantage Actor Critic.\n\n- In the Towers experiment, the results of probably the most important setting, \"Fist Pixels\", are missing. This setting receiving pixel inputs and using the Fist actuator in a continuous space is the setting closest to real-world robots, and thus is very important to confirm whether the proposed approach will be directly applicable to real-world robots. However, Figure 5 is missing the results with this setting. Is there any reason behind this?\n\n- The paper lacks its comparison to any baseline methods. Without explicit baselines, it is difficult to see what the agent is really learning and what aspect of the proposed approach is benefitting the task. For instance, in the Towers task, how would an agent randomly pushing/hitting the tower (using 'Fist') a number of times and then passively observing its consequence to produce a label perform compared to this approach? That is, how would an approach with a fixed action policy (but with everything else) perform compared to the full deep reinforcement learning version?", "title": "clarification of technical contributions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SklkQz1mg": {"type": "review", "replyto": "r1nTpv9eg", "review": "I'm a bit confused as to why the agent is not able to obtain a perfect score in the 'which is heavier' experiment, when it is using the z coordinate features of each of the 4 blocks. It seems like the best strategy is simply to move each of the objects once and observe their maximum z-value, and report the smallest displacement as the heaviest block; I would have thought this would be learnable with the A3C architecture (even for low values of Beta). Is there stochasticity in the agent's force or in the z displacement? If not, once this rule is learned the game plays out similarly to a deterministic bandit, which is easily solvable. Is there something I'm missing, or am I underestimating the difficulty of the task? \n\nThanks!\n\nEDIT:\nAfter re-reading the section, it seems like simply moving a block and then waiting to observe its trajectory is harder than I guessed since there is no 'no-op' action (i.e. the agent must move a block at every time step). Could you provide your justification for requiring the agent to move a block at each time step? To me, having a 'no-op' action seems more natural as young humans (which you mention in order to motivate your paper) will often move things in their environment and observe the effects. This paper purports to investigate the ability of RL agents to perform \u2018physics experiments\u2019 in an environment, to infer physical properties about the objects in that environment. The problem is very well motivated; indeed, inferring the physical properties of objects is a crucial skill for intelligent agents, and there has been relatively little work in this direction, particularly in deep RL. The paper is also well-written.\n\nAs there are no architectural or theoretical contributions of the paper (and none are claimed), the main novelty comes in the task application \u2013 using a recurrent A3C model for two tasks that simulate an agent interacting with an environment to infer physical properties of objects. More specifically, two tasks are considered \u2013 moving blocks to determine their mass, and poking towers such that they fall to determine the number of rigid bodies they are composed of. These of course represent a very limited cross-section of the prerequisite abilities for an agent to understand physics. This in itself is not a bad thing, but since there is no comparison of different (simpler) RL agents on the tasks, it is difficult to determine if the tasks selected are challenging. As mentioned in the pre-review question, the \u2018Which is Heavier\u2019 task seems quite easy due to the actuator set-up, and the fact that the model simply must learn to take the difference between successive block positions (which are directly encoded as features in most experiments).  Thus, it is not particularly surprising that the RL agent can solve the proposed tasks. \n\nThe main claim beyond solving two proposed tasks related to physics simulation is that \u201cthe agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes\u201d. The \u2018cost of gathering information\u2019 is implemented by multiplying the reward with a value of gamma < 1. This is somewhat interesting behaviour, but is hardly surprising given the problem setup.\n\nOne item the authors highlight is that their approach of learning about physical object properties through interaction is different from many previous approaches, which use visual cues. However, the authors also note that this in itself is not novel, and has been explored in other work (e.g. Agrawal et al. (2016)). I think it\u2019s crucial for the authors to discuss these approaches in more detail (potentially along with removing some other, less relevant information from the related work section), and specifically highlight why the proposed tasks in this paper are interesting compared to, for example, learning to move objects towards certain end positions by poking them.\n\nTo discern the level of contribution of the paper, one must ask the following questions: \n\n1)\thow much do these two tasks contribute (above previous work) to the goal of having agents learn the properties of objects by interaction; and\n2)\thow much do the results of the RL agent on these tasks contribute to our understanding of agents that interact with their environment to learn physical properties of objects? \n\nIt is difficult to know exactly, but due to the concerns outlined above, I am not convinced that the answers to (1) or (2) are \u201cto a significant extent\u201d. In particular, for (1), since the proposed agent is able to essentially solve both tasks, it is not clear that the tasks can be used to benchmark more advanced agents (e.g. it can\u2019t be used as a set of bAbI-like tasks). \n\nAnother possible concern, as pointed out by Reviewer 3, is that the description of the model is extremely concise. It would be nice to have, for example, a diagram illustrating the inputs and outputs to the model at each time step, to ease replication.\n\nOverall, it is important to make progress towards agents that can learn to discover physical properties of their environment, and the paper contributes in this direction. However, the technical contributions of this paper are rather limited \u2013 thus, it is not clear to what extent the paper pushes forward research in this direction beyond previous work that is mentioned. It would be nice, for example, to have some discussion about the future of agents that learn physics from interaction (speculation on more difficult versions of the tasks in this paper), and how the proposed approach fits into that picture.  \n\n---------------\nEDIT: score updated, see comments below\n", "title": "which is heavier experiment", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rkuxGkM4l": {"type": "review", "replyto": "r1nTpv9eg", "review": "I'm a bit confused as to why the agent is not able to obtain a perfect score in the 'which is heavier' experiment, when it is using the z coordinate features of each of the 4 blocks. It seems like the best strategy is simply to move each of the objects once and observe their maximum z-value, and report the smallest displacement as the heaviest block; I would have thought this would be learnable with the A3C architecture (even for low values of Beta). Is there stochasticity in the agent's force or in the z displacement? If not, once this rule is learned the game plays out similarly to a deterministic bandit, which is easily solvable. Is there something I'm missing, or am I underestimating the difficulty of the task? \n\nThanks!\n\nEDIT:\nAfter re-reading the section, it seems like simply moving a block and then waiting to observe its trajectory is harder than I guessed since there is no 'no-op' action (i.e. the agent must move a block at every time step). Could you provide your justification for requiring the agent to move a block at each time step? To me, having a 'no-op' action seems more natural as young humans (which you mention in order to motivate your paper) will often move things in their environment and observe the effects. This paper purports to investigate the ability of RL agents to perform \u2018physics experiments\u2019 in an environment, to infer physical properties about the objects in that environment. The problem is very well motivated; indeed, inferring the physical properties of objects is a crucial skill for intelligent agents, and there has been relatively little work in this direction, particularly in deep RL. The paper is also well-written.\n\nAs there are no architectural or theoretical contributions of the paper (and none are claimed), the main novelty comes in the task application \u2013 using a recurrent A3C model for two tasks that simulate an agent interacting with an environment to infer physical properties of objects. More specifically, two tasks are considered \u2013 moving blocks to determine their mass, and poking towers such that they fall to determine the number of rigid bodies they are composed of. These of course represent a very limited cross-section of the prerequisite abilities for an agent to understand physics. This in itself is not a bad thing, but since there is no comparison of different (simpler) RL agents on the tasks, it is difficult to determine if the tasks selected are challenging. As mentioned in the pre-review question, the \u2018Which is Heavier\u2019 task seems quite easy due to the actuator set-up, and the fact that the model simply must learn to take the difference between successive block positions (which are directly encoded as features in most experiments).  Thus, it is not particularly surprising that the RL agent can solve the proposed tasks. \n\nThe main claim beyond solving two proposed tasks related to physics simulation is that \u201cthe agents learn different strategies for these tasks that balance the cost of gathering information against the cost of making mistakes\u201d. The \u2018cost of gathering information\u2019 is implemented by multiplying the reward with a value of gamma < 1. This is somewhat interesting behaviour, but is hardly surprising given the problem setup.\n\nOne item the authors highlight is that their approach of learning about physical object properties through interaction is different from many previous approaches, which use visual cues. However, the authors also note that this in itself is not novel, and has been explored in other work (e.g. Agrawal et al. (2016)). I think it\u2019s crucial for the authors to discuss these approaches in more detail (potentially along with removing some other, less relevant information from the related work section), and specifically highlight why the proposed tasks in this paper are interesting compared to, for example, learning to move objects towards certain end positions by poking them.\n\nTo discern the level of contribution of the paper, one must ask the following questions: \n\n1)\thow much do these two tasks contribute (above previous work) to the goal of having agents learn the properties of objects by interaction; and\n2)\thow much do the results of the RL agent on these tasks contribute to our understanding of agents that interact with their environment to learn physical properties of objects? \n\nIt is difficult to know exactly, but due to the concerns outlined above, I am not convinced that the answers to (1) or (2) are \u201cto a significant extent\u201d. In particular, for (1), since the proposed agent is able to essentially solve both tasks, it is not clear that the tasks can be used to benchmark more advanced agents (e.g. it can\u2019t be used as a set of bAbI-like tasks). \n\nAnother possible concern, as pointed out by Reviewer 3, is that the description of the model is extremely concise. It would be nice to have, for example, a diagram illustrating the inputs and outputs to the model at each time step, to ease replication.\n\nOverall, it is important to make progress towards agents that can learn to discover physical properties of their environment, and the paper contributes in this direction. However, the technical contributions of this paper are rather limited \u2013 thus, it is not clear to what extent the paper pushes forward research in this direction beyond previous work that is mentioned. It would be nice, for example, to have some discussion about the future of agents that learn physics from interaction (speculation on more difficult versions of the tasks in this paper), and how the proposed approach fits into that picture.  \n\n---------------\nEDIT: score updated, see comments below\n", "title": "which is heavier experiment", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}