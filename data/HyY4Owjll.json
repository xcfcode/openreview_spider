{"paper": {"title": "Boosted Generative Models", "authors": ["Aditya Grover", "Stefano Ermon"], "authorids": ["adityag@cs.stanford.edu", "ermon@cs.stanford.edu"], "summary": "", "abstract": "We propose a new approach for using boosting to create an ensemble of generative models, where models are trained in sequence to correct earlier mistakes. Our algorithm can leverage  many existing base learners, including recent latent variable models. Further, our approach allows the ensemble to leverage discriminative models trained to distinguish real data from model generated data. We show theoretical conditions under which incorporating a new model to the ensemble will improve the fit and empirically demonstrate the effectiveness of boosting on density estimation and sample generation on real and synthetic datasets.", "keywords": ["Theory", "Deep learning", "Unsupervised Learning"]}, "meta": {"decision": "Reject", "comment": "The idea of boosting has recently seen a revival, and the ideas presented here are stimulating. After discussion, the reviewers agreed that the latest updates and clarifications have improved the paper, but overall they still felt that the paper is not quite ready, especially in making the case for when taking this approach is desirable, which was the common thread of concern for everyone. For this reason, this paper is not yet ready for acceptance at this year's conference."}, "review": {"H1UZ2vwUx": {"type": "rebuttal", "replyto": "SkYwc0HEx", "comment": "Thanks for your helpful comments. Please refer to the common rebuttal posted for response to questions regarding MNIST experiments. Additional comments:\n1. We have followed up on the reviewer\u2019s suggestion of removing experiments that mix different base learners. We hope this makes the presentation clean. \n2. We have restructured the related works section entirely for improved clarity, and also included a discussion on the differences of our framework with \u201cself-supervised boosting\u201d  -- a reference we had missed out earlier and are duly thankful to the reviewer for highlighting. We have also included references to some very recent relevant preprints that came after our work for a more complete and up-to-date discussion.\n3. Please read the response to Reviewer 4 below for an argument on why we feel that unnormalized models such as BGMs while certainly being limited are not a deal-breaker.", "title": "Response"}, "SklwiPvLe": {"type": "rebuttal", "replyto": "HyY4Owjll", "comment": "The bulk of the reviewer (R) comments pertain to the experiments on MNIST that we address collectively here to facilitate a richer, holistic discussion. Any remaining questions raised by the reviewers are addressed individually as follow-ups to the reviews.\n\nWe have rewritten the entire experimental section based on fresh experimentation, focusing on aspects that we believe are crucial to resolving the concerns raised by the reviewers. For brevity, we have also removed some portions from the earlier draft that we do not believe are central to the discussion. In the spirit of the constructive feedback loop that ICLR facilitates, we hope the reviewers will be able to read the revised experiments section. Our response to the reviewer comments:\n\n1. A bigger VAE vs. BGMs containing smaller models (R1)\nOur earlier submission demonstrated improved performance of BGMs for sampling with gains in computational requirements, but did not directly address the reviewer\u2019s question about model capacity. Our latest submission considers larger baseline models with equal or more capacity than BGM sequences of simpler models (Sec. 3.2.2, Table 2), and shows the superior performance of BGMs for this task (Figure 3) . As before, we also illustrate that the performance gains of BGM sequences are not at the cost of increased training time (Sec 3.2.3, Table 2).\n\n2. Samples from MNIST look weak (R4)\nOur experiments on MNIST do not (or even intended to) make any state-of-the-art claims, but instead demonstrate the improvements in computational and statistical performance that simpler models can attain for the task of generating samples as a result of the boosting procedure. Having said that, we respectfully disagree that the samples produced by BGM sequences look weak keeping in mind that the dataset under consideration is the harder \u2018binarized\u2019 version of MNIST. In fact, although we understand that it is somewhat subjective, we believe the BGM samples look better than NADE samples on the same dataset (Figure 2 (Left) in [2]). Note that we display actual samples produced by our models (without any cherry-picking), and hence, these samples should not be compared with the probabilities from sampling pixels (Figure 2 (Middle) in [2]). \n\n3. Strategies for setting weights (\\alpha\u2019s) for intermediate models (R3)\nIn our earlier submission, we discussed model weights in the theoretical section and listed heuristic strategies for setting them in practice as a future work. Our latest submission includes experimental results and discussion of some heuristic strategies that we propose for both density estimation and sampling. See Sec. 3.1.3 and Appendix A.2.1.\n\n4. Procedure for drawing samples from BGM sequences (R1, R3)\nSee Appendix A.2.3 for details.  In contrast to results in the previous submission where the independent chains were run from the same start seeds across BGM sequences and samples arranged for easy comparison, the latest results show samples produced from chains that are run independently, initialized with different random start seeds, and have an increased burn-in time of 100,000 samples (10x the previous) to mitigate the possibility of any mixing-related issues. We also observed visually that the samples from any given chain vary over time transforming smoothly across multiple digits within the burn-in time, a further indication of successful mixing. \n\n5. Quantitative estimation of log-likelihoods for sampling (R1, R3, R4)\nFor evaluating samples, log-likelihoods can be problematic. See [1], Sec. 3.2 for a detailed discussion on why log-likelihoods are not a good proxy for sample quality. Additionally, generative models such as VAEs, RBMs have intractable likelihoods and hence, resort to their own approximations for generating density estimates. These approximations are known to exhibit different behaviors; for instance, RBMs estimated using AIS are typically believed to overestimate the log-likelihoods, whereas VAEs make variational approximations that only provide lower bounds -- it is difficult to make definitive statements based on such approximations. We tried AIS on the BGM sequences considered in the paper with about 15,000 intermediate annealing distributions (step-size of 1e-3 between for the first 500 distributions, 1e-4 for the next 4000, and 1e-5 for the remaining where every transition from one distribution to another was implemented using 10 steps of Metropolis-Hastings), but got unrealistically optimistic log-likelihood evaluations that we do not feel confident reporting for the purpose of a meaningful comparison with the lower bounds provided by VAEs.  \n\nReferences:\n[1] Lucas Theis, Aaron van den Oord, and Matthias Bethge. \u201cA note on the evaluation of generative models.\u201d In ICLR, 2016.\n[2] Hugo Larochelle and Iain Murray. \"The Neural Autoregressive Distribution Estimator.\"  In AISTATS, 2011.", "title": "Common rebuttal response to questions regarding MNIST experiments"}, "HkGAaPwLl": {"type": "rebuttal", "replyto": "SkmDS6lEg", "comment": "Thanks for your helpful comments. Please refer to the common rebuttal posted for response to questions regarding MNIST experiments (points 2 and 3). \n1. Yes, the bagging procedure described is correct. The purpose of including bagging as a baseline was to demonstrate that naively increasing model capacity by adding new models to the ensemble is not very effective in correcting for model misspecification, unlike boosting which involves a reweighting step in the GenBGM version or classifier training for the DiscBGM version. A similar approach (called bagging) competes with boosting for the supervised case. For brevity, we have removed this baseline from the latest version.\n4. The experiments on semi-supervised classification have been removed in the latest version since the gain in accuracy with the current algorithm is unconvincing for a few cases. For completeness, we would like to clarify the purpose of including them earlier was to demonstrate a) the computational gains that boosted generative models can provide on this task (for example, the training time of RBM->RBM was slightly more than half of the baseline RBM model yet it was able to match the performance of the baseline RBM model) b) improving models that are weaker at this task due to different inductive biases (for example, VAE->RBM gives better accuracy than the baseline VAE). Based on our preliminary experiments, we believe this task is an important use case of boosted generative models. However, in the absence of sufficient empirical evidence, we defer improved algorithms for semi-supervised classification based on the boosting framework to future work.\n", "title": "Response"}, "BJ192wv8x": {"type": "rebuttal", "replyto": "rJVSY2ZNe", "comment": "Thanks for your helpful comments. Please refer to the common rebuttal posted for response to questions regarding MNIST experiments.  Regarding the concerns with the unnormalized final density, we highlight that probabilistic models frequently make a trade-off between expressiveness and tractability. Simple models such as NADE and mixture of Bernoullis make assumptions that lend tractability but are not very flexible in modeling complex structure in the data. On the other hand, latent variable models such as VAEs and RBMs have great expressive power but need to approximate intractable integrals. Boosted generative models also have a computationally intractable partition function  with the trade-off made in expressiveness just like VAEs, RBMs, GANs (where even the unnormalized log-likelihood cannot be directly computed), etc. We argue that intractability is however not a deal-breaker since a). many use cases of generative models (such as sampling, unsupervised feature learning) do not require the partition function, b). wherever required, there are some generic techniques available for estimating the partition function.", "title": "Response"}, "SyHf7iBme": {"type": "rebuttal", "replyto": "S1QbDykml", "comment": "Theorem 2 is a direct consequence of the unsupervised-as-supervised learning approach proposed by Friedman et al. (2001) [Sec. 14.2.4 in the book] which has been used by many subsequent works including Tu (2007). The theoretical analysis and algorithm proposed in Tu (2007) differ from the one we propose in the following ways: \n1. Tu (2007) does not account for the more practical case where we have imperfections in learning intermediate models. We account for these imperfections by assigning weights (i.e., alpha's) to intermediate models. Furthermore, Theorem 1 in our work is for the more general case where alpha's lie between 0 and 1 (both inclusive) and the analysis of Tu (2007) falls out as a special case (all alpha's = 1).\n2. Theorem 1 in Tu (2007) does not clearly state the hypothesis/conditions under which the claimed statement holds. If we assume the assumptions to be \u201cZ_k<=1 and  \\int p+(x) log odds >=1\u201d, as mentioned after the proof, these together are only a sufficient condition. In our notation, this sufficient condition translates to E_{q_{t-1}}[h] <=1 and E_p[log h] >=0. In the setting considered in Tu (2007) where all \\alpha\u2019s=1, our sufficient condition (Theorem 1 in our work, condition (1)) is also necessary, and strictly stronger as the following simple example demonstrates. Let p and q_{t-1} be degenerate distributions assigning all probability to a single point, let\u2019s say x_p and x_q i.e., p(x_p)=1 and 0 elsewhere, q_{t-1}(x_q)=1 and 0 elsewhere. Set h(x_p) = (0.5+\\epsilon_p)/(0.5-\\epsilon_p) and h(x_q)= (0.5+\\epsilon_q)/(0.5-\\epsilon_q) for any 0< \\epsilon_q <= \\epsilon_p < 0.5.  The sufficient condition of Tu (2007) is unsatisfied, because E_{q_{t-1}}[h] >1. However, our sufficient and necessary condition (in our Theorem 1) is satisfied.\n3. Algorithmically, the intermediate models in our case can be generators/discriminators whereas Tu (2007) considers only discriminators. Even for the discriminator version, we have a discussion on the effect of different type of classifiers such as random and adversarial classifiers (Corollary 1) which is absent in Tu (2007).\n\nExperiments\n1. Different base learners have different inductive biases, and these biases capture different patterns in the observed data. For instance, discriminators such as CNNs can account for translational invariance in images, latent variable models such as VAEs/RBMs are good for learning compact continuous/discrete latent representations. Furthermore, mixing models helps in task generalization as evident in our experiments; while the VAE->VAE and RBM->RBM combinations are among the best performing models for sample generation and unsupervised feature learning respectively, they turn out to be among the weakest models for unsupervised feature learning and sample generation respectively. The hybrid models allow us to interpolate between extremes and derive generative models that perform well across different tasks.\n2. In line with the above comment, it is hard to make definitive statements about either of GenBGM or DiscBGM being better than the other without considering the specific task and dataset. However, we reiterate some observations that we expect to generally hold in practice. First, DiscBGM consists of a computationally expensive of sampling from the previous BGM distribution unlike GenBGM which requires simply a reweighting of the data distribution. Second, the synthetic experiments for density estimation show that while both can algorithms can correct for model misspecification, GenBGM leans slightly towards underfitting (Fig. 1(h)) whereas DiscBGM can be hard to generalize in the absence of appropriate regularization (Fig. 1(k)).\n3. Note that the samples obtained by baseline models could be poor due to model misspecification and/or computational budget for optimization. The state-of-the-art MNIST samples from literature are generated using highly expressive models trained for a long duration. We use simple baseline models to leave room for improvement for clear qualitative evaluation. Our experiments show that the boosting approach can be used to build highly expressive models without incurring any significant computation overheads. The baseline models were indeed run up to convergence with regularization imposed by early stopping based on monitoring of cross-validation loss (which is routinely the procedure followed in literature) whereas the individual models in BGM sequences were stopped much earlier before convergence (to balance overall computation time) putting them at a disadvantage. Furthermore, we didn\u2019t observe the sample quality from boosted sequences to match the BGM sample qualities even with a more expressive baseline model with twice the hidden layers trained up to convergence. Hence, naively increasing the computational budget and/or model capacity by training highly expressive models is empirically observed to be not as effective as boosting.\n", "title": "Clarifications"}, "S1QbDykml": {"type": "review", "replyto": "HyY4Owjll", "review": "Could the authors provide a more detailed comparison to the work of Tu (2007) ? There seems to be a significant overlap with Sec. 2.2 (including Theorem 2 and its proof), which is not fairly represented in the related works section IMO. I believe more credit and transparency in this regard would benefit the paper, without adversely impacting novelty.\n\nAs for the experimental section, what is the added benefit of mixing various base-learners ? Even though the method allows for it, there is no clear rationale for doing so. Unfortunately, this seems to come at the expense of careful evaluation of each approach. As it stands, the experimental section does not convincingly answer the following questions: which approach of DiscGBM or GenGBM is better ? In which condition does boosting outperform the ML baselines ? Table 1 seems to imply there may a win in terms of training time, but this seems unconvincing without likelihoods estimates, convergence curves, etc. In particular, the samples obtained by the baseline models on MNIST seem poor compared to what can be found in the literature.\nThis paper extends boosting to the task of learning generative models of data. The strong learner is obtained as a geometric average of \u201cweak learners\u201d, which can themselves be normalized (e.g. VAE) or un-normalized (e.g. RBMs) generative models (genBGM), or a classifier trained to discriminate between the strong learner at iteration T-1 and the true data distribution (discBGM). This latter method is closely related to Noise Contrastive Estimation, GANs, etc.\n\nThe approach benefits from strong theoretical guarantees, with strict conditions under which each boosting iteration is guaranteed to improve the log-likelihood. The downside of the method appears to be the lack of normalization constant for the resulting strong learner and the use of heuristics to weight each weak learner (which seems to matter in practice, from Sec. 3.2). The discriminative approach further suffers from an expensive training procedure: each round of boosting first requires generating a \u201ctraining set\u201d worth of samples from the previous strong learner, where samples are obtained via MCMC.\n\nThe experimental section is clearly the weak point of the paper. The method is evaluated on a synthetic dataset, and a single real-world dataset, MNIST: both for generation and as a feature extraction mechanism for classification. Of these, the synthetic experiments were the clearest in showcasing the method. On MNIST, the baseline models are much too weak for the results to be convincing. A modestly sized VAE can obtain 90 nats within hours on a single GPU, clearly an achievable goal. Furthermore, despite arguments to the contrary, I firmly believe that mixing base learners is an academic exercise, if only because of the burden of implementing K different models & training algorithms. This section fails to answer a more fundamental question: is it better to train a large VAE by maximizing the elbow, or e.g. train 10 iterations of boosting, using VAEs 1/10th the size of the baseline model ? Experimental details are also lacking, especially with respect to the sampling procedure used to draw samples from the BGM. The paper would also benefit from likelihood estimates obtained via AIS.\n\nWith regards to novelty and prior work, there is also a missing reference to \u201cSelf Supervised Boosting\u201d by Welling et al [R1]. After a cursory read through, there seems to be strong similarities to the GenBGM approach which ought to be discussed.\n\nOverall, I am on the fence. The idea of boosting generative models is intriguing, seems well motivated and has potential for impact. For this reason, and given the theoretical contributions, I am willing to overlook some of the issues highlighted above, and hope the authors can address some of them in time for the rebuttal.\n\n[R1] https://papers.nips.cc/paper/2275-self-supervised-boosting.pdf\n\nPROS:\nNovel and intriguing idea\nStrong theoretical guarantees\n\nCONS:\nResulting boosted model is un-normalized\nDiscriminator based boosting is expensive, due to sampling via MCMC\nWeak experimental section\n", "title": "Request for clarifications", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SkYwc0HEx": {"type": "review", "replyto": "HyY4Owjll", "review": "Could the authors provide a more detailed comparison to the work of Tu (2007) ? There seems to be a significant overlap with Sec. 2.2 (including Theorem 2 and its proof), which is not fairly represented in the related works section IMO. I believe more credit and transparency in this regard would benefit the paper, without adversely impacting novelty.\n\nAs for the experimental section, what is the added benefit of mixing various base-learners ? Even though the method allows for it, there is no clear rationale for doing so. Unfortunately, this seems to come at the expense of careful evaluation of each approach. As it stands, the experimental section does not convincingly answer the following questions: which approach of DiscGBM or GenGBM is better ? In which condition does boosting outperform the ML baselines ? Table 1 seems to imply there may a win in terms of training time, but this seems unconvincing without likelihoods estimates, convergence curves, etc. In particular, the samples obtained by the baseline models on MNIST seem poor compared to what can be found in the literature.\nThis paper extends boosting to the task of learning generative models of data. The strong learner is obtained as a geometric average of \u201cweak learners\u201d, which can themselves be normalized (e.g. VAE) or un-normalized (e.g. RBMs) generative models (genBGM), or a classifier trained to discriminate between the strong learner at iteration T-1 and the true data distribution (discBGM). This latter method is closely related to Noise Contrastive Estimation, GANs, etc.\n\nThe approach benefits from strong theoretical guarantees, with strict conditions under which each boosting iteration is guaranteed to improve the log-likelihood. The downside of the method appears to be the lack of normalization constant for the resulting strong learner and the use of heuristics to weight each weak learner (which seems to matter in practice, from Sec. 3.2). The discriminative approach further suffers from an expensive training procedure: each round of boosting first requires generating a \u201ctraining set\u201d worth of samples from the previous strong learner, where samples are obtained via MCMC.\n\nThe experimental section is clearly the weak point of the paper. The method is evaluated on a synthetic dataset, and a single real-world dataset, MNIST: both for generation and as a feature extraction mechanism for classification. Of these, the synthetic experiments were the clearest in showcasing the method. On MNIST, the baseline models are much too weak for the results to be convincing. A modestly sized VAE can obtain 90 nats within hours on a single GPU, clearly an achievable goal. Furthermore, despite arguments to the contrary, I firmly believe that mixing base learners is an academic exercise, if only because of the burden of implementing K different models & training algorithms. This section fails to answer a more fundamental question: is it better to train a large VAE by maximizing the elbow, or e.g. train 10 iterations of boosting, using VAEs 1/10th the size of the baseline model ? Experimental details are also lacking, especially with respect to the sampling procedure used to draw samples from the BGM. The paper would also benefit from likelihood estimates obtained via AIS.\n\nWith regards to novelty and prior work, there is also a missing reference to \u201cSelf Supervised Boosting\u201d by Welling et al [R1]. After a cursory read through, there seems to be strong similarities to the GenBGM approach which ought to be discussed.\n\nOverall, I am on the fence. The idea of boosting generative models is intriguing, seems well motivated and has potential for impact. For this reason, and given the theoretical contributions, I am willing to overlook some of the issues highlighted above, and hope the authors can address some of them in time for the rebuttal.\n\n[R1] https://papers.nips.cc/paper/2275-self-supervised-boosting.pdf\n\nPROS:\nNovel and intriguing idea\nStrong theoretical guarantees\n\nCONS:\nResulting boosted model is un-normalized\nDiscriminator based boosting is expensive, due to sampling via MCMC\nWeak experimental section\n", "title": "Request for clarifications", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}