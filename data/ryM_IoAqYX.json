{"paper": {"title": "Analysis of Quantized Models", "authors": ["Lu Hou", "Ruiliang Zhang", "James T. Kwok"], "authorids": ["lhouab@cse.ust.hk", "ruiliang.zhang@tusimple.ai", "jamesk@cse.ust.hk"], "summary": "In this paper, we studied efficient training of loss-aware weight-quantized  networks with  quantized gradient  in a distributed environment, both theoretically and empirically.", "abstract": "Deep neural networks are usually huge, which significantly limits the deployment on low-end devices. In recent years, many\nweight-quantized models have  been proposed. They have small storage and fast inference, but training can still be time-consuming. This can be improved with distributed learning. To reduce the high communication cost due to worker-server synchronization, recently gradient quantization has also been proposed to train deep networks with full-precision weights. \nIn this paper, we theoretically study how the combination of both weight and gradient quantization affects convergence.\nWe show  that (i) weight-quantized models converge to an error related to the weight quantization resolution and weight dimension; (ii) quantizing gradients slows convergence by a factor related to the gradient quantization resolution and dimension; and (iii) clipping the gradient before quantization renders this factor dimension-free, thus allowing the use of fewer bits for gradient quantization. Empirical experiments confirm the theoretical convergence results, and demonstrate that quantized networks can speed up training and have comparable performance as full-precision networks.", "keywords": ["weight quantization", "gradient quantization", "distributed learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper provides the first convergence analysis for convex model distributed training with quantized weights and gradients. It is well written and organized. Extensive experiments are carried out beyond the assumption of convex models in the theoretical study.\n\nAnalysis with weight and gradient quantization has been separately studied, and this paper provides a combined analysis, which renders the contribution incremental. \n\nAs pointed out by R2 and R3, it is somewhat unclear under which problem setting, the proposed quantized training would help improve the convergence. The authors provide clarification in the feedback. It is important to include those, together with other explanations in the feedback, in the future revision.\n\nAnother limitation pointed out by R3 is that the theoretical analysis applies to convex models only. Nevertheless, it is nice to show in experiments that deep networks training is benefitted from the gradient quantization empirically."}, "review": {"BJgWl8V_RQ": {"type": "rebuttal", "replyto": "ByxL06zL0Q", "comment": "Thanks for your insightful comment. \n\nGradient quantization (Seide et al., 2014; Wen et al., 2017; Alistarh et al., 2017; Bernstein et al., 2018) and gradient sparsification (Aji & Heafield, 2017; Wangni et al., 2017) are most useful when the communication cost is larger than the computational cost. This is the case, for example, in\n(1) networks such as AlexNet, VGG and LSTM (Alistarh et al., 2017), in which most of the parameters are in the fully-connected layers (e.g., ~95% parameters for AlexNet, ~90% parameters for VGG-16, and almost all parameters for LSTM); or\n(2) when the communication bandwidth is limited, e.g., on mobile devices.\n\nAssume that one single worker requires T training iterations, and each iteration takes t_comp time. The total training time is t_comp*T. With N workers, each processing 1/N of training samples (weak scaling), the number of training iterations required is T/N. When communication overlaps with computation (as suggested by the reviewer), the communication time is further reduced when gradient quantization is used. In the case where communication completely overlaps with computation, and computation takes longer than communication, the total training time is t_comp*(T/N). Here, we ignore the communication time for bottom layer gradients. The speedup compared to the single worker case is thus N, which is linear with the number of workers.\n", "title": "Reply to the \"computation and communication overlap\" problem"}, "HJx0CdY1hX": {"type": "review", "replyto": "ryM_IoAqYX", "review": "In a distributed learning system where a parameter server maintains a full resolution copy of the parameters, communication costs can be reduced by (a) discretizing the weights that the server broadcasts to the workers, and (b) discretizing the gradients that the workers return to the parameter server. Following existing literature, the authors propose to discretize the parameters in a manner that limits its impact on the loss function by means of a diagonal approximation of the Hessian. This also means that one can bound the difference between the gradient for the full precision parameter and the gradient for the discretized parameter.  In contrast, they discretize the gradients stochastically so that the discretized version is an unbiased estimator of the full precision stochastic gradient. Since the stochastic gradient is itself an unbiased estimator of the gradient, this means we are dealing with an estimator whose variance has increased in a manner we can bound as well. The theoretical analysis consists in pushing these two bounds through classical analyses of the stochastic gradient algorithm, in this case, a regret-based version in the style of Zinkevich or Duchi.  Although i did not check the minute details of the proof, the argument feels correct and familiar.  They also give an interesting result in favor of clipping gradients, worth developing.\n\nAlthough the title promises an analysis that holds for deep networks, this analysis strictly applies only to convex models. The author argue that the predictions made by this analysis also apply to deep networks, and support this argument with extensive experiments (which certainly represent a fair amount of work).  This result is believable but should not be construed as an analysis. Nevertheless, both results (the theoretical result for convex model and the empirical result for deep networks) are interesting and worth sharing.\n\nThe main caveat comes from the style the parallel learning algorithm they are considering.  In the data-parallel case (which they consider), parameter servers approaches have been displaced by setups where all workers update their copy of the weights using the allReduced gradients.  One could also use discretized gradients to speedup the allReduce operation (this is less of a win because latencies dominate) but this would only result in an increased variance and a much simpler analysis.\n\nFinally I am not completely up-to-date with this line of work and cannot evaluate the novelty with confidence. This was not known to me, which is only a piece of evidence.\n\n-- bumping down my score because the misleading title was not addressed by the author response.\n-- bumping it up again because the authors have reacted.\n", "title": "The title is misleading but paper contains good material.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ByxL06zL0Q": {"type": "rebuttal", "replyto": "rJenGm0BAm", "comment": "Thanks very much for your review and your feedback. Your review on the misleading title is very important, and we have discussed among the authors and changed it to \"Analysis of Quantized Models\" in our first revision. I am very sorry that I forgot to update manuscript immediately after the response,  and forgot to mention this in our initial response, and caused the misunderstanding. \n\nWe are also thinking about the \"computation and communication overlap\" problem and will respond to you as soon as possible. Thanks again for your effort in improving our paper.  If you have any further questions or suggestions, please do not hesitate to let us know. We are willing to incorporate further suggestions or requests to improve our paper.\n", "title": "Reply to misleading title"}, "HkeHExsB0Q": {"type": "rebuttal", "replyto": "rJexvR-c37", "comment": "Thanks for your review and suggestions.\n\n1.(a)\"comparison between stochastic weight quantization and loss-aware deterministic weight quantization\"\n- Comparison of stochastic weight quantization (Theorems 4-6 in Appendix D) and loss-aware deterministic weight quantization (Theorems 1-3):\n     (1) Convergence speed: Similar to loss-aware weight quantization,\n         (i) Stochastic weight quantization converges with a O(d/\\sqrt{T}) rate to the error (but with a different scaling, see \n              (17) and (25));\n         (ii) Gradient quantization slows convergence (relative to using full-precision gradients) by a factor related to \n               gradient quantization resolution \\Delta_g and d; and \n         (iii) Gradient clipping makes the speed degradation dimension-free.\n    (2) Error: Stochastic weight quantization also has an error term LD\\sqrt{dD_{\\infty}^2\\Delta_w^2/4}, which is related \n          to the weight quantization resolution and dimension. Moreover, this term can potentially be larger than the one \n          (LD\\sqrt{D^2+ d\\alpha^2\\Delta_w^2/4}) induced by loss-aware weight quantization, as D_{\\infty} can be much \n          larger than \\alpha and dD_{\\infty}^2 be much larger than D^2.\n- We now add an experiment on stochastic weight quantization (W-SQ4) on CIFAR-10 with two workers. The setting is the same as the CIFAR-10 experiment in Table 1. The numbers are test set accuracies (%). Compared to LAQ4 in Table 1, stochastic weight quantization has worse accuracies than the full-precision baseline, even with 4-bit weights.\n\n-----------------------------------------\nG\t\t                        W(SQ4)\n-----------------------------------------\nFP\t\t                        83.29\nSQ2(no clipping)\t        81.31\nSQ2(clip, c=3)\t\t82.80\nSQ3(no clipping\t\t82.82\nSQ3(clip, c=3)\t\t82.99\nSQ4(no clipping)\t        82.92\nSQ4(clip, c=3)\t\t82.90\n----------------------------------------\n\n(b) \"The bias introduced by the latter seems the culprit in the reduction of predictive performance. What if we applied non-biased weight quantization, with stochastic quantized gradient?\"\n- If non-biased stochastic weight quantization with stochastic quantized gradient is applied, the resultant gradient w.r.t. the stochastically quantized weight is unbiased only for the linear model (Zhang et al., 2017). \n- For nonlinear models, stochastic weight quantization makes the gradient biased, and there is an induced error. Detailed bounds for linear stochastic weight quantization with full-precision gradient, quantized gradient with/without clipping can be found in Appendix D.\n\n2.(a) \"there is no report of training time on ImageNet, profiling as been made in a communication model and not in a real setting.\"\n- Profiling is based on a performance model which is commonly used in the gradient compression literature (e.g., Wen et al, 2017, \"Deep gradient compression: Reducing the communication bandwidth for distributed training\", ICLR-2018)\n\n(b) \"It would be great to see the best training time that you achieve by weight/gradient quantization (say on 4 bits).\"\n- To measure the actual computation time, dedicated hardware for low-bit operations are needed. This will be investigated in the future. \n\n3. \"it appears that even with 4 bit quantization, the test accuracy of the trained model is significantly reduced. Why not increase the size to say 6 or 8 bits?\"\n- As suggested by the reviewer, we now add the results for 6-bit weight quantization (LAQ6) on the ImageNet dataset. Compared with Table 3, using 6 bits has comparable or slightly better accuracy than 4-bit. In particular, LAQ6 using quantized clipped gradients has less than 1% absolute top-1 accuracy drop compared to using full-precision weights.\n\n-----------------------------------------------------------------------------------------------------------------------------\nWeight      Gradient                   N=2         \t\t             N=4                               N=8\n-----------------------------------------------------------------------------------------------------------------------------\n\t\t\t\t                      top1/top5  acc (%)      top1/top5  acc (%)      top1/top5 acc (%)\n-----------------------------------------------------------------------------------------------------------------------------\n                  FP                               54.23/77.54\t\t     54.31/77.46                  54.75/78.18\nLAQ6        SQ3(no clipping)      52.64/76.08 \t\t     53.00/76.38                  53.08/73.34\n                  SQ3(clip, c=3)           54.21/77.32 \t\t     54.53/77.85                  54.61/78.10\n-----------------------------------------------------------------------------------------------------------------------------\n\n", "title": "RE: AnonReviewer1's review[1/2]"}, "SkxZe09HC7": {"type": "rebuttal", "replyto": "S1lX2i9S0Q", "comment": "5. \"Theorem 1 is an analysis for training with quantized weights and full-precision gradients, which is essentially the same setting as BinaryConnect. Similar analysis has been done in Li et al, 2017. What is the difference or connection with their bound?\"\n- We differ from the setting in BinaryConnect as:\n    (1) in BinaryConnect, there is no scaling parameter \\alpha; whereas in loss-aware weight quantization, there is \n          a \\alpha learned from the data;\n    (2) BinaryConnect considers only binarization, while we consider m-bit quantization which is more general.\n- We differ from the bound in Li et al., 2017 as:\n    (1) Li et al, 2017 studies simple SGD; whereas loss-aware quantization considers the proximal step with \n          preconditioning (Section 2.2) like RMSprop or Adam. \n    (2) If we do not derive the last step in (12) (proof for Theorem 1 in Appendix B.1), then the convergence speed \n          for loss-aware weight quantization is controlled by the two terms \\frac{D_{\\infty}^2}{2 \\eta}\\sum_{i=1}^{d} \n         \\sqrt{T\\hat{v}_{T,i}} and \\frac{\\eta G_{\\infty} }{\\sqrt{(1-\\beta)}} \\sum_{i=1}^{d} ||\\hat\\g_{1:T,i}||_2. When \n         the data features are sparse, these two terms can be much smaller than the O(d\\sqrt{T}) bound (Duchi et \n         al., 2011; Kingma & Ba, 2015) in Theorem 3 (Note that D^2 and G^2 are upper bounded by dD_{\\infty}^2 and \n         d G_{\\infty}^2) in Li et al, 2017. This indicates faster convergence of loss-aware weight quantization than \n         BinaryConnect in Li et al, 2017.\n    (3) BinaryConnect in Li et al, 2017 uses the simple sign function for binarization; while loss-aware weight \n          quantization minimizes a weighted distance with the full-precision weight. \n- Their error bounds are special cases of ours. Specifically, when (i) \\alpha is fixed at \\Delta; (ii) \\H'_t=LI, where L is the Lipschitz constant in Assumption 2 and I is the identity matrix; and (iii) one does not consider that the updated full-precision weight is outside the representable range (equation (14) in the submission), then (S_w)^d=\\{-1, +1\\}^d, \\Delta_w=1, and the error LD\\sqrt{D^2+d \\alpha^2 \\Delta_w^2/4} in our Theorem 1 reduces to \\sqrt{d}LD\\Delta_w/4, which is tighter than \\sqrt{d}LD\\Delta in Theorem 3 in Li et al., 2017. \n\n6. \"not clear how gradients are calculated w.r.t. quantized weights on worker, is straight through estimator (STE) used for backpropagation through Q_w?\" \n- As quantized weights are used in forward propagation, we can directly obtain the gradients w.r.t. these quantized weights. Hence, no STE is needed.\n\n7. \"why is \\tilde{g}_t stochastically quantized gradient? How about statically quantized gradients?\n- Recent gradient quantization papers (Wen et al., 2017; Alistarh et al., 2017, Zhang et al., 2017) all require the quantized gradient to be unbiased and all use stochastically quantized gradients.  Deterministic gradient quantization makes the quantized gradient biased. \n\n8.(a) \"Why do the authors use linear model in section 4.1?\"\n- Please see our reply to Q3(c) above.\n\n(b) \"Why are the solid lines in Figure 3 finished earlier than dashed lines?\"\n- We use early stopping, and training is terminated when the average training loss does not decrease for 5000 iterations.\n\n(c) \"Would it possible to verify the theorem on deep nets of different dimension?\"\n- As suggested by the reviewer, we added this experiment on neural networks, please see our reply to Q3(c) above. \n\n9.(a) \"Why does the number of worker affect the performance?\"\n- There are two scaling schemes in distributed training with data parallelism: strong scaling and weak scaling (\"A framework for performance modeling and prediction\", ACM/IEEE conference on Supercomputing, 2002). Here, we consider weak scaling, which is more popular in deep network training. \n- In weak scaling, the same data set size is used for each worker. The gradients are averaged over the N workers as g_t = \\frac{1}{N} \\sum_{n=1}^{N} g_t^{(n)} (Here, with a slight abuse of notation, g_t ^{(n)} can be the full-precision gradient or quantized gradient with/without clipping). If the gradients before averaging are independent random variables with zero mean, and ||\\g_t^{(n)}||^2 is bounded by G^2, then |\\g_t||^2 is bounded by G^2/N. From Theorems 1-3, the number of iterations for convergence is subsequently reduced by a factor of 1/N as compared to using a single worker. \n\n(b) \"influence of batch sizes rather than the number of workers\"\n- When weak scaling is used, the total batch size (i.e., the total number of samples processed by all workers in each iteration) is proportional to the number of workers. The influence of the number of workers on performance is discussed in our reply to 9(a) and empirically in Section 4.2.4 and Section 4.3.\n ", "title": "RE: AnonReviewer2's review[2/3] "}, "S1gLcxsHCX": {"type": "rebuttal", "replyto": "HJx0CdY1hX", "comment": "1. \"analysis strictly applies only to convex models\"\n- Please see our reply to Q3(b) for Reviewer 2 above.\n\n2. \"In the data-parallel case (which they consider), parameter server approaches have been displaced by setups where all workers update their copy of the weights using the allReduced gradients. One could also use discretized gradients to speedup the allReduce operation (this is less of a win because latencies dominate) but this would only result in an increased variance and a much simpler analysis.\"\n- For the all-reduce communication model with N=2^k workers, we need k steps to gather the gradients. Assuming that the gradients are directly averaged at each all-reduce step, our theory is independent of how the gradients are aggregated, and can be directly applied. The only difference with the parameter server model is that the number of bits used for gradients increases by one at every aggregation step in the worst case. However, even considering this effect, gradient quantization still significantly reduces the network communication time, as shown in Figure 4. \n", "title": "RE: AnonReviewer3's review"}, "r1l0SeoHR7": {"type": "rebuttal", "replyto": "HkeHExsB0Q", "comment": "4. \"can the communication quantization be used jointly with a forward/backward quantized evaluation?\"\n- Our communication quantization is already used jointly with a forward quantized evaluation (as the quantized weights are used for forward propagation). However, currently, we do not consider quantizing the gradients in the backward propagation.\n", "title": "RE: AnonReviewer1's review[2/2]"}, "r1eE80qH07": {"type": "rebuttal", "replyto": "SkxZe09HC7", "comment": "10.(a) \"Why is zero weight decay used for CIFAR-10 experiment but non-zero weight decay for imagenet experiment?\"\n- Weight quantization can be viewed as regularization (Courbariaux et al., 2015; Hou et al., 2017). For the small CIFAR-10, we do not need additional regularization such as weight decay (as also in Courbariaux et al., 2015; Hou et al., 2017; Hou et al, 2018).\n- For ImageNet, weight decay is also used in other weight-quantized networks (Li & Liu, 2017, Zhu et al., 2017, Leng et al., 2018). \n\n(b) \"How was weight decay applied in Adam for quantized weights?\"\n- The weight decay is used on the quantized weights during forward propagation.\n\n11. \"The notation of full-precision gradient w.r.t quantized weights in Figure 1 should be \\hat{g}_t, however, g_t is used.\"\n- Thanks for pointing out this typo, and we have corrected it in the manuscript.\n", "title": "RE: AnonReviewer2's review[3/3]"}, "S1lX2i9S0Q": {"type": "rebuttal", "replyto": "HJxJpq863X", "comment": "Thanks for your review and suggestions.\n\n1.(a) \"unclear what problems the authors try to solve.\"\n- The problem is about how the gradient precision affects convergence of weight-quantized nets in a distributed environment.\n\n(b) \"the author does not make it clear the questions they are asking\"\n- The question we want to study is: What are the convergence properties of networks with quantized weights and quantized gradients?\n\n(c) \"how the theoretical results can guide the practical algorithm design\"\n- The theoretical results show that gradient clipping should be used in training weight-quantized models with quantized gradients. Specifically, \n    (1). directly quantizing gradients (Section 3.3) slows convergence (compared to the original full-precision baseline in \n          Section 3.2) by a factor related to gradient quantization resolution \\Delta_g and dimension d. This is problematic \n          as  (i) deep networks typically have a large d; and (ii) distributed learning often uses a small number of bits for \n          the gradients, and thus a large \\Delta_g.\n    (2). gradient clipping makes speed degradation negligible (Section 3.4).\n\n\n2. \"The authors mentioned that quantized gradient slows convergence (relative to using full-precision gradient) in contribution 2 while also claims that quantizing gradients can significantly speed up training of quantized weights in contribution 4, which is contradictory to each other.\"\n- In contribution 2, convergence speed is measured by how fast the regret is reduced w.r.t. the number of gradient evaluations. Quantized gradient loses information and so requires more gradient evaluations. This can be alleviated by gradient clipping.\n- In contribution 4, we mean the total training time (computation time plus communication time) in a distributed learning setting. Quantizing gradient reduces the communication cost, and thus speeds up training. \n- Combining with the above two, training weight-quantized networks with clipped quantized gradients is fast (as can be seen from Figure 4).\n\n\n3.(a) \"not clear what relaxation was made on the assumptions of f_t in section 3.1.\"\n- Existing analysis assumes square loss on linear model (f_t) and unbiased gradient (Zhang et al., 2017), stochastic weight quantization (Li et al., 2017; De Sa et al., 2018) or simple deterministic weight quantization using the sign (Li et al., 2017). These limitations are relaxed in this paper.\n\n(b) \"The assumptions and theoretical results may not hold for non-convex deep nets\"\n- As mentioned in Section 3.1, the convexity assumption does not hold for nonconvex deep nets. \n- However, this assumption facilitates analysis of deep learning models, and has also been used in various papers (Kingma & Ba, 2015; Reddi et al., 2018; Li et al., 2017; De Sa et al., 2018). \n- Moreover, as can be seen from Section 4, it helps to explain the empirical behavior.\n\n(c) \"the author does not validate the theorems results on d with neural networks but only with linear models in section 4.1.\"\n- As mentioned in section 4.1, popular deep networks usually have hand-crafted architectures, and thus we used the linear model (which is also used in Zhang et al., 2017) in the submission.\n- As suggested by the reviewer, we added an experiment that varies the dimension of deep networks on CIFAR-10 dataset. The results can be checked at \n  https://www.dropbox.com/s/bcwwzu35fu496yv/iclr19_rebuttal.pdf?dl=0\nSimilar to the linear model results, a larger d leads to larger convergence speed degradation.\n\n4. \"no comparison is made with other related works (e.g., Wen et al, 2017)\"\n- Indeed, we have compared with (Wen et al, 2017) and (Alistarh et al., 2017). Note that in Table 1, SQ2 corresponds to Terngrad (proposed in Wen et al, 2017) with 2-bit stochastic quantization, and SQm corresponds to QSGD (proposed in Alistarh et al., 2017) with m-bit quantization. \n\n", "title": "RE: AnonReviewer2's review[1/3]"}, "HJxJpq863X": {"type": "review", "replyto": "ryM_IoAqYX", "review": "\nSummary:\n\nThis paper studies the convergence properties of loss-aware weight quantization with different gradient precisions in the distributed environment, in which servers keeps the full-precision weights and workers keeps quantized weights. The authors provided convergence analysis for weight quantization with full-precision, quantized and quantized clipped gradients. Specifically, they find that: 1) the regret of loss-aware weight quantization with full-precision gradient converge to an error related to the weight quantization resolution and dimension d. 2) gradient quantization slows the convergence by a factor related to gradient quantization resolution and dimension d. 3) gradient clipping renders the speed degradation dimension-free. \n\nComments:\n\nPros:\n\n- The paper is generally well written and organized. The notation is clean and consistent. Detailed proofs can be found in the appendix, the reader can appreciate the main results without getting lost in details.\n\n- The paper provides theoretical analysis for the convergence properties of loss-aware weight quantization with full-precision gradients, quantized gradient and clipped quantized gradient, which extends existing analysis beyond full-precision gradients, which could be useful for distributed training with limited bandwidth. \n\nCons:\n\n- It is unclear what problems the authors try to solve. The problem is about gradient compression, or how the gradient precision will affect the convergence for training quantized nets in the distributed environment, in which workers have limited computation power and the network bandwidth is limited. It is an interesting setting, however, the author does not make it clear the questions they are asking and how the theoretical results can guide the practical algorithm design. \n\n- The authors mentioned that quantized gradient slows convergence (relative to using full-precision gradient) in contribution 2 while also claims that quantizing gradients can significantly speed up training of quantized weights in contribution 4, which is contradictory to each other.\n\n- It is not clear what relaxation was made on the assumptions of f_t in section 3.1. The analysis are still based on three common assumptions: 1) f_t is convex 2) f_t is twice differentiable 3) f_t has bounded gradients. The assumptions and theoretical results may not hold for non-convex deep nets. E.g., the author does not valides the theorems results on d with neural networks but only with linear models in section 4.1.\n\n- The author demonstrate training quantized nets in the distributed environment with quantized gradients, however, no comparison is made with other related works (e.g., Wen et al, 2017). \n\nQuestions: \n\n- Theorem 1 is an analysis for training with quantized weights and full-precision gradients, which is essentially the same setting as BinaryConnect. Similar analysis has been done in Li et al, 2017. What is the difference or connection with their bound?\n\n- It is not clear how gradienta are calculated w.r.t. quantized weights on worker, is straight through estimator (STE) used for backpropagation through Q_w?\n\n- In section 3.3, why is \\tilde{g}_t stochastically quantized gradient? How about statiscally quantized gradients?\n\n- Why do the authors use linear model in section 4.1? Why are the solid lines in Figure 3 finished earlier than dashed lines? For neural networks, a common observation is that the larger the dimension d, the better the generalization performance. However, Figure 3 and Theorem 1 seem to be contradictory to this common belief. Would it possible to verify the theorem on deep nets of different dimension?\n\n- Why does the number of worker affect the performance? I failed to see why the number of workers affect the performance of training if it is a synchronized distributed training with the same total batch size. After checking appendix C, I think it is better to discuss the influence of batch sizes rather than the number of workers.\n\n- Why is zero weight decay used for CIFAR-10 experiment but non-zero weight decay for imagenet experiment? How was weight decay applied in Adam for quantized weights? \n\nMinor issues: \n- The notation of full-precision gradient w.r.t quantized weights in Figure 1 should be \\hat{g}_t, however, g_t is used.\n", "title": "interesting problem setting and analysis, unclear conclusion from the analysis and experiments", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJexvR-c37": {"type": "review", "replyto": "ryM_IoAqYX", "review": "Summary\n------\n\nThe authors proposes an analysis of the effect of simultaneously quantizing the weights and gradients in training a parametrized model in a fully-synchronized distributed environment, using RMSProp training updates.\n\nThe authors provide a theoretical analysis in term of regret bound, when the objective functions are smooth, convex and gradient-bounded wrt the parameter. They also assume that the parameters remains in a compact space. Their conclusions are as follow (thm 1, 2 and 3):\n\n- weight quantization, which is deterministic and therefore introduces a bias in the objective functions, introduces a non-vanishing term in the average reget, that depens on the quantization error, where the vanishing term decreases in O(d /sqrt(T)).\n\n- gradient quantization, which is performed in a stochastic, unbiased way (wrt to the full-precision gradient) do not introduce a further non-vanishing term, but augments the constant factor in the vanishing term.\n\n- gradient clipping onto gradient quantization reduced this constant factor, at the cost of ntroducing a further non-vanishing term in the average regret.\n\nAn experimental setting is performed to assess how much the theoretical conclusions derived ina simpe setting apply to predictive functions parametrized with neural-network. The experiments are three folded:\n- a first toy experiment with convex objective validates the theoretical findings\n- a second experiment performed on CIFAR assess the performance on a grid of weight/gradient quantization with or without gradient clipping\n- a third experiement, that is profiled (synthetically) assesses the performance of wieght/gradient quantization when training a model on imagenet.\n\nIn conclusion, the authors observe that quantizing weight/gradients systematically lead to a slight decrease in performance but provides promising improvement in term of training speed\n\nReview\n------\n\nThe paper is well written, documented and well-sectioned, with well written theoretical guarantees and thorough experiments, including one on a large dataset. The theoretical guarantees are relatively non-surprising and their proofs are indeed little involved. The authors are yet the first to analyse the effect of biased weight quantization on one hand, and of gradient clipping on the other hand.\n\nThe reviewer would have appreciated further comparison with existing analysis, in particular a comparison between stochastic weight quantization and loss-aware deterministic weight quantization. The bias introduced by the latter seems the culprit in the reduction of predictive performance. What if we applied non-biased weight quantization, with stochastic quantized gradient ?\n\nThe experiments as presented are a little underwhelming: first of all, there is no report of training time on ImageNet, and I believe that the profiling as been made in a communication model and not in a real setting. It would be great to see the best training time that you achieve by weight/gradient quantization (say on 4 bits).\n\nMoreover, it appears that even with 4 bit quantization, the test accuracy of the trained model is significantly reduced. Why not increase the size to say 6 or 8 bits ? \n\nOn a related aspect, can the communication quantization be used jointly with a forward/backward quantized evalution ?\n\nOverall, although this paper is relatively incremental and has underwhelming experiments, it is a thorough work that is worthy of being presented at ICLR 2019, in the reviewer's opinion.\n\nMinor\n-----\n\np 2: the notation w_i is overloaded\n\nEq 1: S_w^d should read (S_w)^d (cartesian product)\n\nThm 3: the notation R() is overloaded\n\nFigure 1 is very hard to read: increase the font size\n\nFigure 3 4 6: increase the legend size, ensure that the color used vary in lightness for printing\n\nTable 1: use bold font to indicate the best performing FP/FP model, and your best performing model\n\nFig 7 c: training curve\n", "title": "Pedagogical though incremental contribution", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}