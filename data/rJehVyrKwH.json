{"paper": {"title": "And the Bit Goes Down: Revisiting the Quantization of Neural Networks", "authors": ["Pierre Stock", "Armand Joulin", "R\u00e9mi Gribonval", "Benjamin Graham", "Herv\u00e9 J\u00e9gou"], "authorids": ["pstock@fb.com", "ajoulin@fb.com", "remi.gribonval@inria.fr", "benjamingraham@fb.com", "rvj@fb.com"], "summary": "Using a structured quantization technique aiming at better in-domain reconstruction to compress convolutional neural networks", "abstract": "In this paper, we address the problem of reducing the memory footprint of convolutional network architectures. We introduce a vector quantization method that aims at preserving the quality of the reconstruction of the network outputs rather than its weights. The principle of our approach is that it minimizes the loss reconstruction error for in-domain inputs. Our method only requires a set of unlabelled data at quantization time and allows for efficient inference on CPU by using byte-aligned codebooks to store the compressed weights. We validate our approach by quantizing a high performing ResNet-50 model to a memory size of 5MB (20x compression factor) while preserving a top-1 accuracy of 76.1% on ImageNet object classification and by compressing a Mask R-CNN with a 26x factor.", "keywords": ["compression", "quantization"]}, "meta": {"decision": "Accept (Spotlight)", "comment": "This paper addresses to compress the network weights by quantizing their values to some fixed codeword vectors. The paper is well written, and is overall easy to follow. The proposed algorithm is well-motivated, and easy to apply. The method can be expected to perform well empirically, which the experiments verify, and to have potential impact. On the other hand, the novelty is not very high, though this paper uses these existing techniques in a different setting."}, "review": {"HJWNvyUr-P": {"type": "rebuttal", "replyto": "nfzoubyRgVz", "comment": "Good morning Eunhui Kim,\n\nThanks for your interest in our work! To facilitate the collaboration and the debugging, could you please fill an issue here: https://github.com/facebookresearch/kill-the-bits/issues/new by copy-pasting what you wrote above? Also, could you indicate in the issue:\n\n- Relevant output logs?\n- Clarify the following: do you refer to the validation accuracy as the one given by running inference.py on the compressed model obtained by running quantize.py? In this case, resolved issue #9 (https://github.com/facebookresearch/kill-the-bits/issues/9)  should help you (you have to patch two lines of code in inference.py).\n\nThanks again for reaching out,\n\nThe authors ", "title": "Answer"}, "bky56UZZCC": {"type": "rebuttal", "replyto": "rJehVyrKwH", "comment": "Dear AC, Dear reviewers, \n\nThank you again for your constructive comments and feedback. We uploaded the final version of our manuscript. \n\nSee you in Ethiopia!", "title": "Final version"}, "OgcAdDx7Y9": {"type": "rebuttal", "replyto": "jjnNUFHmw1", "comment": "Thanks for pointing out the reference! The authors propose an interesting theoretical viewpoint on quantization, and also consider the activations to derive theoretical bounds on the MSE error when using scalar quantization. We will include this work in our final version. ", "title": "Answer"}, "B1ezIjyQiH": {"type": "rebuttal", "replyto": "H1gAi6GdtH", "comment": "We thank Reviewer 3 for raising important questions. We answer them below.\n\nUsing \\tilde x in the E- and M-steps. \nWe agree with Reviewer 3 that \u201cthe error arising from quantizing v into c is only affected by a subset of rows of \\tilde x\u201d. However, we solve Equation (2) with this proxy algorithm for two reasons. First, using the full \\tilde x matrix allows to factor the computation of the pseudo-inverse of \\tilde x and thus allows for a much faster algorithm, see answer to Reviewer 2 and the details of the M-step in the paper (as well as footnote 2). Second, early (and slow) experiments suggested that the gains were not significant when using the right subsets of \\tilde x in this particular context. \n\nMinimizing the reconstruction error\nOur method results in both better reconstruction error and better training loss than na\u00efve PQ *before* any finetuning. As we state in the paper, applying naive PQ without any finetuning to a ResNet-18 leads to accuracies below 18% for all operating points, whereas our method (without any finetuning) gives accuracy around 50% (not reported in the paper, we will add it in the next version of our paper). \n\nChoosing the optimal number of centroids/blocks size\nThere is some rationale for the block size, related to the way the information is structured and redundant in the weight matrices (see in particular point 1 of answer to Reviewer 1). For instance, for convolutional weight filters with a kernel size of 3x3, the natural block size is 9, as we wish to exploit the spatial redundancy in the convolutional filters. For the fully-connected classifier matrices and 1x1 convolutions however, the only constraint on the block size if to be a divisor of the column size. Early experiments when trying to quantize such matrices in the row or column direction gave similar results. Regarding the number of centroids, we expect byte-aligned schemes (256 centroids indexed over 1 byte) to be more friendly for an efficient implementation of the forward in the compressed domain. Otherwise, as can be seen in Figure 3, doubling the number of centroids results in better performance, even if the curve tends to saturate around k=2048 centroids. As a side note, there exists some strategies that automatically adjust for those two parameters (see HAQ for example). \n\nComparison with pruning and low-rank approximation\nWe argue that both pruning and low-rank approximation are orthogonal and complementary approaches to our method, akin to what happens in image compression where the transform stage (e.g., DCT or wavelet) is complementary with quantization. See \u201cDeep neural network compression by in-parallel pruning-quantization\u201d, Tung and Mori for some works investigating this direction. \t\n", "title": "Answer"}, "SJlzQjkXjB": {"type": "rebuttal", "replyto": "ryegpfeaYS", "comment": "We thank Reviewer 2 for their support and questions. We answer them below.\n\nQuantization time\nAs we state in our paper, quantizing a ResNet-50 (quantization + finetuning steps) takes about one day on one Volta V100 GPU. The time of quantization is around 1 to 2 hours, the rest being dedicated to finetuning. Thus, the time dedicated to quantization is relatively short, especially compared with the fine-tuning and even more with the initial network training. This is because we optimized our EM implementation in at least two ways as detailed below. \n-\tThe E-step is performed on the GPU (see file src/quantization/distance.py, lines 61-75) with automatic chunking. This means that the code chunks the centroids and the weight matrices into blocks, performs the distance computation on those blocks and aggregates the results. This falls within the map/reduce paradigm. Note that the blocks are automatically calculated to be the largest that fit into the GPU, such that the utilization of the GPU is maximized, so as to minimize the compute time. \n-\tThe M-step involves calculating a solution of a least squares problem (see footnote 2 in our paper). The bottleneck for this is to calculate the pseudo-inverse of the activations x. However, we fix x when iterating our EM algorithm, therefore we can factor the computation of the pseudo inverse of x before alternating between the E and the M steps (see file src/quantization/solver.py and in particular the docstring). \n\nWe provided pointers to the files in the code anonymously shared on OpenReview. To our knowledge, these implementation strategies are novel in this context and were key in the development of our method to be able to iterate rapidly. Both strategies are documented in the code so that they can benefit to the community. \n\nIncorporating the non-linearity\nAs the Reviewer rightfully stated, optimally we should take the non-linearity in Equation (4) into account. One could hope for a higher compression ratio. Indeed, the approximation constraint on the positive outputs would stay the same (they have to be close to the original outputs). On the other hand, the only constraint lying on the negative outputs is that they should remain negative (with a possible margin), but not necessarily close to the original negative outputs.  However, our early experiments with this method resulted in a rather unstable EM algorithm. This direction may deserve further investigation. \n", "title": "Answer"}, "rJlnnKJQiH": {"type": "rebuttal", "replyto": "S1e82d0HqB", "comment": "We thank Reviewer 4 for stating that \u201cthe proposed method has a good compression ratio while maintaining competitive accuracy\u201d. We provide clarification for the two main questions of the Reviewer below.\n\nNovelty of the paper\nAs we state in our introduction, using codebooks to compress networks is not new, as well as using a weighted k-means technique. However, as we state in the paper: \u201cThe closest work we are aware of is the one by Choi et al. (2016), but the authors use a different objective (their weighted term is derived from second-order information) along with a different quantization technique (scalar quantization). Our method targets a better in-domain reconstruction, as depicted by Figure 1\u201d. \n\nNote that we already cite two of the suggested references by Reviewer 4, namely \u201cTowards the limit of network quantization\u201d and \u201cThiNet: A filter level pruning method for deep neural network compression\u201d in our work. We will further clarify our positioning in an updated version of the paper. \n\nCompression ratio\nWe provide an example of the computation of compression ratio in Section 4.1, paragraph \u201cMetrics\u201d. Let us detail it further here. The memory footprint of a compressed layer is split between the indexing cost (one index per block indicating the centroid used to encode the block) and the cost of storing the centroids. Say we quantize a layer of size 128 \u00d7 128 \u00d7 3 \u00d7 3 with 256 centroids and a block size of 9. Then, each block of size 9 is indexed by an integer between 0 and 255: such integer can be stored using 8 bits or 1 byte (as 2^8 = 256). Thus, as we have 128 x 128 blocks, the indexing cost is 128 x 128 x 1 byte = 16,384 bytes = 16 kB. Finally, we have to store 256 centroids of dimension 9 in fp16, which represents 256 x 9 floats (fp16) = 256 x 9 x 2 = 4,608 bits = 4.5 kB. The size of the compressed model is the sum of the sizes of the compressed layers. Finally, we deduce the overall compression ratio which is the size of the compressed model divided by the size of the non-compressed model.\n", "title": "Answer"}, "rkxdLt17jr": {"type": "rebuttal", "replyto": "rklaWryJoH", "comment": "We thank Reviewer 1 for their insightful questions and suggestions. We agree that Product Quantization (PQ) is key to get \u201cimpressive compression ratio\u201d while maintaining competitive accuracy, provided that there is some special structure and redundancy in the weights and the way we quantize them. \n\nWhich kind of redundancy does our method capture?  \nAs rightfully stated by Reviewer 1, choosing which elementary blocks to quantize in the weight matrices is crucial for the success of the method (what the Reviewer calls \u201chorizontal/vertical/other\u201d correlation). In what follows, let us focus on the case of convolutional weights (of size C_out x C_in x K x K). As we state in our paper: \u201cThere are many ways to split a 4D matrix in a set of vectors and we are aiming for one that maximizes the correlation between the vectors since vector quantization-based methods work the best when the vectors are highly correlated\u201d. We build on previous work that have documented the *spatial redundancy* in the convolutional filters [1], hence we use blocks of size K x K. Therefore, we rely on the particular nature of convolutional filters to exploit their spatial redundancy. We have tried other ways to split the 4D weights into a set of vectors to in preliminary experiments, but none was on par with the proposed choice.  We agree with Reviewer 1 that the method would probably not yield as good a performance for arbitrary matrices. \n\nUsing row permutations to improve the compressibility?\nThis is a very good remark. Indeed, redundancy can be artificially created by finding the *right* permutation of rows (when we quantize using column blocks for a 2D matrix). Yet in our preliminary experiments, we observed that PQ performs systematically worse both in terms of reconstruction error and accuracy of the network that when applying a random permutation to a convolutional filter. This confirms that our method captures the spatial redundancy of the convolutional filters as stated in the first point. \n\n[1] Exploiting linear structure within convolutional networks for efficient evaluation, Denton et al.", "title": "Answer"}, "H1gAi6GdtH": {"type": "review", "replyto": "rJehVyrKwH", "review": "This paper addresses to compress the network weights by quantizing their values to some fixed codeword vectors. The authors aim to reduce the distortion of each layer rather than the weight distortion. The proposed algorithm first selects the candidate codeword vectors using k-means clustering and fine-tune them via knowledge distillation. The authors verify the proposed algorithm by comparing it with existing algorithms for ResNet-18 and ResNet-50.\n\nOverall, I think that the proposed algorithm is easy to apply and the draft is relatively well written. Some questions and doubts are listed below.\n\n-In k-means clustering (E-step and M-step), is it correct to multiply \\tilde x to (c-v)? I think that the error arising from quantizing v into c is only affected by a subset of rows of \\tilde x. For example, if v is the first subvector of w_j, then I think that only 1-st, m+1-th, 2m+1-th, \u2026 rows of \\tilde x affect to the error.\n\n-Does minimizing reconstruction error minimizes the training loss (before any further fine-tuning) compared to na\u00efve PQ? If not, \n\n-Is there any guideline for choosing the optimal number of centroids and the optimal block size given a target compression rate?\n\n-Is there any reason not comparing the proposed algorithm with other compression schemes? (e.g., network pruning and low-rank approximation) \n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}, "ryegpfeaYS": {"type": "review", "replyto": "rJehVyrKwH", "review": "This paper suggests a quantization approach for neural networks, based on the Product Quantization (PQ) algorithm which has been successful in quantization for similarity search. The basic idea is to quantize the weights of a neuron/single layer with a variant of PQ, which is modified to optimize the quantization error of inner products of sample inputs with the weights, rather than the weights themselves. This is cast as a weighted variant of k-means. The inner product is more directly related to the network output (though still does not account for non-linear neuron activations) and thus is expected to yield better downstream performance, and only requires introducing unlabeled input samples into the quantization process. This approach is built into a pipeline that gradually quantizes the entire network.\n\nOverall, I support the paper and recommend acceptance. PQ is known to be successful for quantization in other contexts, and the specialization suggested here for neural networks is natural and well-motivated. The method can be expected to perform well empirically, which the experiments verify, and to have potential impact.\n\nQuestions:\n1. Can you comment on the quantization time of the suggested method? Repeatedly solving the EM steps can add up to quite an overhead. Does it pose a difficulty? How does it compare to other methods?\n2. Can you elaborate on the issue of non-linearity? It is mentioned only briefly in the conclusion. What is the difficulty in incorporating it? Is it in solving equation (4)? And perhaps, how do you expect it to effect the results?", "title": "Official Blind Review #2", "rating": "8: Accept", "confidence": 2}, "S1e82d0HqB": {"type": "review", "replyto": "rJehVyrKwH", "review": "This paper proposes to use codes and codebooks to compress the weights. The authors also try minimizing the layer reconstruction error instead of weight approximation error for better quantization results.\nDistillation loss is also used for fine-tuning the quantized weight. Empirical results on resnets show that the proposed method has a good compression ratio while maintaining competitive accuracy.\n\nThis paper is overall easy to follow. My main concern comes from the novelty of this paper. The two main contributions of the paper: \n(1) using codes and codebooks to compress weights; and \n(2) minimizing layer reconstruction error instead of weight approximation error\nare both not new. For instance, using codes and codebooks to compress the weights has already been used in [1,2].  A weighted k-means solver is also used in [2], though the \"weighted\" in [2] comes from second-order information instead of minimizing reconstruction error. In addition, minimizing reconstruction error has already been used in low-rank approximation[3] and network pruning[4]. \nClarification of the connections/differences, and comparison with these related methods should be made to show the efficacy of the proposed method.\n\nIt is not clear how the compression ratio in table 1 is obtained. Say for block size d=4, an index is required for each block, and the resulting compression ratio is at most 4 (correct me if I understand it wrong).\nCan the authors provide an example to explain how to compute the compression ratio? \n\n[1]. Model compression as constrained optimization, with application to neural nets. part ii: quantization. \n[2]. Towards the limit of network quantization.\n[3]. Efficient and Accurate Approximations of Nonlinear Convolutional Networks.\n[4]. ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression. \n\n", "title": "Official Blind Review #4", "rating": "6: Weak Accept", "confidence": 3}, "rklaWryJoH": {"type": "review", "replyto": "rJehVyrKwH", "review": "The suggested method proposes a technique to compress neural networks bases on PQ quantization. The algorithm quantizes matrices of linear operations, and, by generalization, also works on convolutional networks. Rather than trying to compress weights (i.e. to minimize distance between original and quantized weights), the algorithm considers a distribution of unlabeled inputs and looks for such quantization which would affect output activations as little as possible over that distribution of data. The algorithm works by splitting each column of W_ij into m equal subvectors, learning a codebook for those subvectors, and encoding each of those subvectors as one of the words from the codebook.\n\nThe method provides impressive compression ratios (in the order of x20-30) but at the cost of a lower performance. Whether this is a valuable trade-off is highly application dependent.\n\nOverall I find the paper interesting and enjoyable. However, as I am not an expert in the research area, I can not assess how state of the art the suggested method is.\n\nThere are a few other questions that I think would be nice to answer. I will try to describe them below:\n\nSuppose we have a matric W_{ij} with dimensions NxM where changing i for a given j defines a column. By definition, linear operation is defined \ny_i = sum_j W_ij x_j . Now say each column of matrix W is quantized into m subvectors. We can express W_ij in the following way:\nW_ij = (V^1_ij + V^2_ij + ... V^m_ij)x_j where V^m_ij is zero everywhere except for the rows covering a given quantized vector.\nFor example, if W had dimensions of 8x16 and m=4, \nV^2_{3,j}=0, for all j, V^2_{4,j}=non_zero, V^2_{7,j}=non_zero, V^2_{8,j}=0, V^2_{i=4:8,j}=one_of_the_quantized_vectors.\n\ny_i = sum_j W_ij x_j = sum_k sum_j (V^k_ij) x_j =def= sum_k z^k_i where z^k are partial products: z^k_i=0 for i<k*N/m and i>(k+1)N/m\n\nThus, the suggested solution effectively splits the output vector y_i into m sections, defines sparse matrices V^k_{ij} 1<=k<=m, and performs column-wise vector quantization for these matrices separately.\n\nGenerally, it is not ovious or given that the current method would be able to compress general matrices well, as it implicitly assumes that weight W_{ij} has a high \"correlation\" with weights W_{i+kN/m,j} (which I call \"vertical\" correlation), W_{i,k+some_number} (which I call \"horizontal\" correlation) and W_{i+kN/m,k+some_number} (which I call \"other\" correlation). It is not given that those kind of redundancies would exist in arbitrary weight matrices.\n\nNaturally, the method will work well when weight matrices have a lot of structure and then quantized vectors can be reused. Matrices can have either \"horizontal\" or \"vertical\" redundancy (or \"other\" or neither). It would be very interesting to see which kind of redundancy their method managed to caprture.\n\nIn the 'horizontal' case, it should work well when inputs have a lot of redundancy (say x_j' and x_j'' are highly correlated making it possible to reuse code-words horizontally within any given V^k: V^k_ij'=V^k_ij''). However, if thise was the case, it would make more sense to simply remove redundancy by prunning input vector x_j by removing either x_j' or x_j'' from it. This can be dome by removing one of the outputs from the previous layer. This can be a symptom of a redundant input.\n\nAnother option is exploiting \"vertical\" redundancy: this happens when output y_i' is correlated with output y_{i'+N/m}. This allows the same code-word to be reused vertically. This can be a symptom of a redundant output. It could also be the case that compressibility could be further subtantially improved by trying different matrix row permutations. Also, if one notices that y_i' ir correlated with y_i'', it might make sense to permute matrix rows in such a way that both rows would end up a multiple N/m apart. It would be interesting to see how this would affect compressibility.\n\nThe third case is when code words are reused in arbitrary cases. \n\nGenerally, I think that answering the following questions would be interesting and could guide further research:\n1. It would be very interesting to know what kind of code-word reusa patterns the algorithm was able to capture, as this may guide further research.\n2. How invariance copressibility is under random permutations of matrix rows (thus also output vectors)?\n", "title": "Official Blind Review #1", "rating": "8: Accept", "confidence": 1}, "S1ezZlil5B": {"type": "rebuttal", "replyto": "Bkg1AQulcH", "comment": "Thanks for pointing out this reference! It is definitely relevant to our work, and therefore we will add it in our paper. We would like to point out that our method goes beyond this prior work on several aspects:\n- We use Product Quantization (i.e quantizing chunks of columns) whereas in the cited work the authors use Vector Quantization (i.e. the authors quantize the columns). Our choice takes better advantage of the spatial redundancy of information in the convolutional filters as VQ is less likely to discover the mutual dependency except if using very large amount of data for learning.\n- The cited work does not quantize the layers sequentially and does not finetune the learned centroids -- it finetunes the dense (non-compressed) weights of the classifier.\n- The cited work does not use distillation to take advantage of the teacher, non-compressed network to help compressing the student network.\n- The speed-up reported in the cited work assumes that the scalar products between the input activations of all layers and the centroids are already pre-computed, which is not the case in a real inference scenario.", "title": "Answer"}}}