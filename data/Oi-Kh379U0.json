{"paper": {"title": "Generalizing and Tensorizing Subgraph Search in the Supernet", "authors": ["Hansi Yang", "quanming yao"], "authorids": ["~Hansi_Yang1", "~quanming_yao1"], "summary": "We broaden the horizon of existing supernet-based NAS methods. We generalize supernet to other deep learning tasks that have graph-like structures and propose to solve them in a unified framework of supernet based on tensor network.", "abstract": "Recently, a special kind of graph, i.e., supernet, which allows two nodes connected by multi-choice edges, has exhibited its power in neural architecture search (NAS) by searching better architectures for computer vision (CV) and natural language processing (NLP) tasks. In this paper, we discover that the design of such discrete architectures also appears in many other important learning tasks, e.g., logical chain inference in knowledge graphs (KGs) and meta-path discovery in heterogeneous information networks (HINs). Thus, we are motivated to generalize the supernet search problem on a broader horizon. However, none of the existing works are effective since the supernet's topology is highly task-dependent and diverse. To address this issue, we propose to tensorize the supernet, i.e. unify the subgraph search problems by a tensor formulation and encode the topology inside the supernet by a tensor network. We further propose an efficient algorithm that admits both stochastic and deterministic objectives to solve the search problem. Finally, we perform extensive experiments on diverse learning tasks, i.e., architecture design for CV, logic inference for KG, and meta-path discovery for HIN. Empirical results demonstrate that our method leads to better performance and architectures.\n", "keywords": ["deep learning", "neural architecture search", "tensor decomposition"]}, "meta": {"decision": "Reject", "comment": "This paper proposes a new and general formulation for supernet, which encodes supernet with tensor network(TN). The idea is interesting and motivated.  However, the paper is well presented and the clarify needs to be further improved.  The effectiveness of algorithm is not well justified and experimental results are less convincing even after additional results provided in the revision. Most importantly, it is not clear that the 'TENSORIZING' method can solve the current NAS's ineffectiveness problem.  It is confirmed that the reference to ICLR-2021 paper is not used for the decision of paper. "}, "review": {"UeExtMBTFGb": {"type": "review", "replyto": "Oi-Kh379U0", "review": "Summary:\n\nThis paper proposes a new and general formulation for supernet, which encodes supernet with tensor network(TN). Based on TN, the topology of supernet can be encoded. Besides, this paper proposes a corresponding algorithm to solve the search problem. \n\nReasons for score:\n\nOverall, I vote for accepting marginally. My major concern is about the clarity of the paper and some additional experiments (see cons below). Hopefully the authors can address my concern in the rebuttal period.\n\nPros:\n\n1.In this paper, the authors introduce a new and general formulation for supernet. Due to its generalization, supernet-based NAS methods with it can be applied to many deep learning tasks. In addition, this formulation can encode the topology of supernet, which benefits the network search.\n\n2.Based on this formulation, this paper proposes a corresponding search algorithm. This algorithm can solve supernet search problem for both deterministic formulation and stochastic formulation. \n\n3.Experiments are well thought out and highlight the key advantages of the method over other NAS methods. \n\nCons:\n1.For recent NAS methods, they all validate their performances on ImageNet. In the experiments, there is only a experiment on ImageNet-16-120. Could you provide an experiment in weight-sharing setting on ImageNet dataset?\n\n2.For Table 3, it is a must to compare with more NAS methods, such as PDARTS and DropNAS. In this way, the effective of the proposed method can be validated fully.\n\n3.In section 3.2, $R^{n}$ in $\\mathcal{T}_{i-}$ is used without any definition. Are there some hints for it? If not, it would lead to some misunderstandings.\n\nQuestions during rebuttal period:\n\nPlease address and clarify the cons above\n\nSome typos:\n\nIntroduction, Notations, last line: a vector $\\textbf{o}_{i} \\in \\mathbb{R}^{n} $ -> $\\textbf{o} \\in \\mathbb{R}^{n} $", "title": "Official Blind Review #4 ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "4xG41W6lWj2": {"type": "review", "replyto": "Oi-Kh379U0", "review": "\n+ This paper generalizes the supernet search problem on a broader horizon. Specifically, some of the current NAS methods use supernet to co-training different neural architectures for further architecture search. This paper does not just consider supernet as a tool for NAS, but also consider supernet as a graphical model and extend supernet to several general tasks in the form of graph data. (+)\n\n+ This paper unifies the above tasks by a tensor formulation and encodes the topology inside the supernet by a tensor network. Different from NAS, the topological structure of the supernet is utilized in this paper. (+)\n\n+ The paper further proposes an efficient algorithm that admits both stochastic and deterministic objectives to solve the search problem. (+)\n\n+ A wide range of machine learning tasks besides computer vision are used to evaluate the proposed method's effectiveness, such as logic chain inference on knowledge graphs and meta-path discovery for HIN. (+)\n\n- After reading the related work section, we notice that the authors have known some methods used in the NAS problem, but they are not aware of the current NAS's ineffectiveness problem. Specifically, the NAS's effectiveness is open to question, with its architecture rating is often inaccurate, especially in DARTS and some single-path one-shot NAS method. Two ICLR 2020 papers suggest that many NAS methods are not better than random architecture search. This paper borrows the concept from NAS but ignore the ineffectiveness/inefficiency of NAS. Therefore, the proposed method's effectiveness in this paper is questionable unless the authors provide an architecture rating analysis. (-)\n\nIn the context of chaotic phenomena in NAS nowadays, analyzing the architecture rating problem is of most importance. As there are many NAS papers published every year and their ineffectiveness may still be not widely recognized by the reviewers and the public, merely borrowing NAS methods to other domains (e.g., in this paper) is dangerous. This may cause more chaos in the new field. I think a NAS method's effectiveness should be sufficiently investigated before NAS can be generalized to a new domain.\n\n\n- As defined, Ti in [0, 1] represents how \"\"good\"\" P can be. Then, there must be a connection between T and M(f(w, P), D), which represents the performance of learning model f(w, P) on dataset D? My understanding is that the authors can use the CORRELATION BETWEEN EDGE IMPORTANCE AND MODEL ACCURACY in the following paper [reference] to measure the connection between T and M. In other words, as is formulated by the authors, \"Ti \u2208 [0, 1] represent how \"good\" P can be,\" can the authors provide pieces of evidence showing the proposed method can actually have this effect? This is exactly related to the inaccurate architecture rating problem in NAS. Only can the authors answer this question will I be able to consider an acceptance of this paper. (-)\n\n\n[reference]\n@inproceedings{\nanonymous2021dots,\ntitle={{\\{}DOTS{\\}}: Decoupling Operation and Topology in Differentiable Architecture Search},\nauthor={Anonymous},\nbooktitle={Submitted to International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=y6IlNbrKcwG},\nnote={under review}\n}\n\n\n+ The way to use routine (Algorithm 1) to tensorize the supernet is reasonable and correct. \uff08+\uff09\n\n\n- This paper is very difficult to understand. I must read very carefully (especially for the subscript notation) twice until I can understand the paper. I think the main idea of this paper is simple. But the authors use a complicated way to describe the method, which makes the article unreadable and thus mysterious. (-)\n\nOverall, this paper proposes a method that generalizes the idea from NAS to general tasks. However, it is open to question whether this generalization is appropriate as the supernet-based NAS's effectiveness is an active research topic.\n", "title": "A method that generalizes the idea from NAS to general tasks", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "6lkd0Jspne_": {"type": "rebuttal", "replyto": "lYqSf9sLnA", "comment": "Please first check the reply on the presentation and novelty in \"common questions\".\n\nQ1. This paper has not clearly described the difference with previous works. \n\tSpecifically, although this paper claims to give a unified method, \n\tit is difficult to distinguish what has been unified. \n\tAnd in the 10th line of the Abstract, what is the concrete description of \"the above problems\"?\n\nWe have unified different objects (neural architecture, logic rule, meta-path) \nthat originally arise in different areas as a ````\"generalized architecture\" in supernet. \nBased on this unified treatment, we propose a novel algorithm to solve \nthis ``\"generalized architecture search\" problem. \nWe have rewritten the abstract and methodology part to emphasize our contribution \non a unified framework. \n\nQ2. In Figure 2, whether is $\\alpha^{(1)}$ the $\\theta^1$? \n\tIf yes, in Figure 2(b), \n\tit would be better to use $\\theta^1$ to $\\alpha^{(1)}$ replace directly. \n\tAnd in the caption of Figure 2, \n\twhat does \"$N_1(t)/N_2(t)$\" mean, \n\ta division or ``\"$N_1(t)$ or $N_2(t)$\"?\n\nWe have changed ``$\\theta$ to $\\alpha$ and \"$N_1(t)/N_2(t)$\" to \"`$N_1(t)$ or $N_2(t)$\" to avoid ambiguity. \n\nQ3. What is the meaning of $R_{N_1(t)}$? And how to choose the value of $R_{N_1(t)}$?\n\n$R_{N_1(t)}$ is a hyper-parameter in our method which corresponds to the rank of the tensor network. \nWe have thoroughly rewrite the methodology part (common question 2) \nand marked important revisions with blue color. \nAnd we have done an ablation study on the rank in Section 5.3.2, \nwhich demonstrates that the optimal rank should be set based on experiments. \n\nQ4. In Section PROPOSED METHOD, how to transform a supernet to a tensor graph? It is highly suggested that adding more details about the transforming process. \n\nThank you for your suggestion. \nWe have re-written Section 3.2.\nSpecifically, we introduce a third-order tensor $\\alpha^t$ for each edge, \nwhich is based on previous methods (e.g., DARTS and SNAS), \nbut uses tensors instead of vectors to allow more flexibility. \nAnd we use index summation to reflect the topological structure (common nodes) \nbetween different edges. \nThe transformation from this computation process to a ``tensor graph'' \nfollows the common practice in tensor decomposition literature, \nplease refer to [A, B] for more detail. \n\nBesides, we have added more details to make our methods more clear (a graphical illustration is added to Appendix.G). \n\n[A] Tensor networks for dimensionality reduction and large-scale optimization: Part 1 low-rank tensor decompositions. Foundations and Trends in Machine Learning, 2016.\n\n[B] Tensor networks for dimensionality reduction and large-scale optimization: Part 2 applications and future perspectives. Foundations and Trends in Machine Learning, 2017.\n\nQ5. The experimental results are not good enough. There are a number of competitive methods that obtained better results than the proposed one on Cifar10.\n\nThank you for your suggestion. \nWe have added more results of recent NAS methods on CIFAR-10 in Table 3. \nPlease also refer to common question 1, \nwhere we discuss the relation between TRACE and other NAS methods. \nAnd we can achieve better performances than recent NAS methods\nby combining TRACE with PC-DARTS, as is also shown in Table 3. \n\nQ6. The writing requires significant proofreading. There are plenty of grammar errors...\n\nWe have thoroughly revised all possible errors. Thank you for your tolerance.", "title": "Reply to Reviewer 1"}, "_0n8TtbapGe": {"type": "rebuttal", "replyto": "eB5aHlKM-09", "comment": "Please first check the reply on the presentation and novelty in \"common questions\".\n\nQ1. The gradient-based NAS finds an optimal path in the global search space. However, for other tasks mentioned in this paper, is 'architecture search' appropriate to describe them? Could it be named as 'path search' or some others?\n\nWe have revised the title and placed our submission in a better position. Please also refer to common question 1. Thanks for your great suggestion! \n\nQ2. The notations are not clear. \n- The $\\mathcal{J}$, the $r_{N_1(t)}$, the $\\theta$ should all be described. However, I could not find them either in the 'annotation' paragraph or the first-mentioned sentences. \n- Move notations to the beginning of Section 3. \n- What is $R_{N_1(t)}$ in Alg. 1? The $R$ has not been explained. However, I think this is important for the reader to understand the algorithm. \n- In Fig. 2, it's better to add a toy example based on Eq. 4 and add more explanations.\n\nThank you for your suggestions. We have made thorough revisions (marked with blue) to our paper. \n- We have revised the notation part and move it to the beginning of Section 3\n- $R_{N_1(t)}$ and $R_{N_2(t)}$ are two hyper-parameters corresponding to the rank of tensor network. \n- Fig. 2 in section 3.2 is used to illustrate our encoding method, while Eq. 4 shows how to use softmax trick to satisfy the normalization constraint. \nPlease refer to our main text for more detail. \n\nQ3. The results on CIFAR-10 aim to show that 'TRACE' could accelerate searching without loss of accuracy?\n\nPlease have a double-check over Table 3. Our results do not support that TRACE can accelerate searching,\nit is NASP, PC-DARTS, and ASNG-NAS that support such a claim.\n\nQ4. What will happen if searching for longer epochs?\n\nThanks for such an insightful question. Since what exactly will happen if searching a long time is still an open issue [A, B, C], we consider both stand-alone and one-shot setting. For a stand-alone setting, please see Figure 3(a),  which demonstrates that searching for longer epochs will have limited improvements; for the weight-sharing setting, our experimental setting follows the common setting in NAS literature, i.e. same training epochs. \n\n[A] Understanding and Robustifying Differentiable Architecture Search. ICLR 2020\n\n[B] Weight-Sharing Neural Architecture Search: A Battle to Shrink the Optimization Gap. arXiv preprint 2008.01475\n\n[C] Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS. NIPS 2020\n\nQ5. According to 'Motivated by the success of previous methods (e.g., DARTS, SNAS, and ENAS), our TN should include their encoding methods as special cases.', what is the number of parameters used for modeling the architectures?\n\nThanks for your question. We have added a table (Table 8 in Appendix) to compare different modeling methods. Our TN uses a similar number of parameters with better performances. \n\nQ6. 'their decomposition method does not consider the topological structure of supernets,'. For the cell-based NAS in Table. 1. Why the topological structure has not been considered?\n\nPrevious results apply the same encoding method (CP decomposition) for different supernets, which does not consider their topological structures. We have revised our paper to make our logic more clear. Please refer to the revision (marked with blue) in Section 3. \n\nQ7. Please report the cost in Table 4, 5. For example, the params/search cost/flops in Table 3.\n\nThanks for your suggestion. Table 4 and 5 follow existing works [Sadeghian et al., 2019, Yun et al., 2019] and it is not a common practice in KG/HIN area to report these numbers. The reason for that is these numbers are quite similar for all compared methods. And we also add tables (Table 8 and 9 in Appendix B) to show the similar computational cost of compared methods. \n\nQ8. Lack of experiments on large-scale datasets, like ImageNet.\n\nThanks for your suggestion. We have added the experiments on ImageNet in Table 4. Our experimental results demonstrate that TRACE+PC can achieve better performances than previous NAS methods. \n\nQ9. The HIN task searches for the rank-1 CP (Table 1), and the CV task also searches for the rank-1 solution. So this HIN task is just adopting the standard solution on another dataset? \n\nThanks for admitting our conceptual novelty. We conceptually unify these applications and show they can be represented by a tensor formulation. And we construct a tensor network to replace the CP decomposition, which can exploit the topological structure of supernets for better performances. This novel discovery by us leads to such a misunderstanding. \n\nQ10. What if the rank goes higher in Figure 3 (b)?\n\nThanks for your suggestion. We have made the rank go higher in experiments and the results are added to Figure 3(b). From the figure, we can see that when the rank goes higher, the performance becomes worse due to over-parametrization and difficult optimization. ", "title": "Response to Reviewer 2"}, "PVhsykz5aCc": {"type": "rebuttal", "replyto": "4xG41W6lWj2", "comment": "Please first check the reply on the presentation and novelty in \"common questions\".\n\nQ1. The authors are not aware of the current NAS's ineffectiveness problem. Specifically, the NAS's effectiveness is open to question, with its architecture rating is often inaccurate, especially in DARTS and some single-path one-shot NAS method. Two ICLR 2020 papers and many ICLR 2021 submissions suggest that many NAS methods are not better than random architecture search. This paper borrows the concept from NAS but ignore the ineffectiveness/inefficiency of NAS. Therefore, the proposed method's effectiveness in this paper is questionable...\n\nThanks for mentioning such concerns,\nthe validity of TRACE is as follows:\n- First,\nplease check common question 1,\nthis submission is an algorithmic paper that targets at the supernet search problem instead of NAS.\n- Second,\nwe are indeed deeply aware of the problems you mentioned in NAS.\nThus,\nwe perform experiments on both the stand-alone setting\nand one-shot setting.\nYour concerns \nfall into the one-shot setting,\nand are eliminated in the stand-alone one.\nHowever,\nthe effectiveness of TRACE is supported by both settings.\n- Third,\nthe superiority of TRACE\nis further cross-validated by other applications from HIN and KG \ncompared with random search. \n\nQ2. ... unless the authors provide an architecture rating analysis. As defined, $\\mathcal{T}$ represents how \"good\" $\\mathcal{P}$ can be. Then, there must be a connection between $\\mathcal{T}$ and $\\mathcal{M}(f(w, \\mathcal{P}), \\mathcal{D})$, which represents the performance of learning model $f(w, \\mathcal{P})$ on dataset $\\mathcal{D}$? My understanding is that the authors can use the CORRELATION BETWEEN EDGE IMPORTANCE AND MODEL ACCURACY in the following paper [reference] to measure the connection between $\\mathcal{T}$ and $\\mathcal{M}$. In other words, as is formulated by the authors, ``$\\mathcal{T}_{i-} \\in (0,1)$ represent how \"good\" P can be, can the authors provide pieces of evidence showing the proposed method can actually have this effect? This is exactly related to the inaccurate architecture rating problem in NAS.\n\nThanks for your suggestion. \nIndeed,\ncorrelation is a good criterion to show the rationality of\none-shot architecture search methods [Bender et al., 2018; Liu et al., 2018; Yu et al., 2020; Guo et al., 2020].\nHowever,\nit is only a sufficient, not necessary condition.\nSpecifically,\nthe goal of tensor $\\mathcal{T}$ here is to capture good \nsubgraph in the whole supernet,\nthus we expect the possibilities of $\\mathcal{P}_i$ will concentrate on some top \nsubgraphs,\nwhich is shown in below Figure 4 (Appendix F.1).\nThus,\nTRACE can accurately capture top subgraphs in the supernet.\n\nQ3. This paper is very difficult to understand...\n\nThanks for your suggestion. \nWe have rewritten our methodology part to make it more clear. \nPlease refer to the blue-marked text in our paper for revision. ", "title": "Reply to Reviewer 3"}, "pyVuateiBdW": {"type": "rebuttal", "replyto": "UeExtMBTFGb", "comment": "Please first check the reply on the presentation and novelty in \"common questions\".\n\nQ1. For recent NAS methods, they all validate their performances on ImageNet. In the experiments, there is only an experiment on ImageNet-16-120. Could you provide an experiment in weight-sharing setting on ImageNet dataset?\n\nThank you for your suggestion. \nWe have added the experiments on ImageNet in Table 4.\nResults demonstrate that TRACE+PC can achieve better \nperformances than previous NAS methods. \n\nQ2. For Table 3, it is a must to compare with more NAS methods, such as PDARTS and DropNAS. In this way, the effectiveness of the proposed method can be validated fully.\n\nThank you for your suggestion. \nPlease first see the position of our paper in common question 1.\nThe word \"compare\" here is not that accurate \nas PDARTS and DropNAS solve the NAS problem \nin an orthogonal way to ours. \nHowever, \nwe also add the performance of recent NAS methods and TRACE combined with \nPC-DARTS in Table 3, \nwhich demonstrates that our method can be combined with recent NAS \nto achieve better performances. \n\nQ3. In Section 3.2, $R_n$ in $\\mathcal{T}_{i-}$ is used without any definition. Are there some hints for it? If not, it would lead to some misunderstandings.\n\nThe $R_n$ here is the rank of our proposed tensor network. \nTo avoid ambiguity, we have also rewritten the methodology part. \nPlease see our response to common question 2 \nand our revision (marked with blue) in our paper. ", "title": "Reply to Reviewer 4"}, "lYqSf9sLnA": {"type": "review", "replyto": "Oi-Kh379U0", "review": "The idea of employing a tensor method to generalize NAS seems to be interesting. However, this paper suffers from a few problems: \n1. The presentation of this paper requires improvement. There are some confusion:\n\n- This paper has not clearly described the difference with previous works. Specifically, although this paper claims to give a unified method, it is difficult to distinguish what have been unified. And in the 10-th line of the Abstract, what is the concrete description of \"the above problems\"?\n- In Figure 2,  whether $\\alpha^{(1)}$ is the $\\theta^{1}$? If yes, in Figure 2(b), it would be better to use $\\theta^1$ to replace $\\alpha^{(1)}$ directly. And in the caption of Figure 2, what does \"$N_1(t)/N_2(t)$\" mean, a division or \"$N_1(t)$ or $ N_2(t)$\"?\n- In Algorithm 1, what is the meaning of $R_{N_1(t)}$? And how to choose the value of $R_{N_1(t)}$?\n- In Section **PROPOSED METHOD**, how to transform a supernet to a tensor graph?  It is highly suggested that adding more details about the transforming process.\n\n2. The experimental results are not good enough. For example,  in results on Cifar10, there are a number of competitive methods which obtained better results of the proposed one. \n\n| Model                                           | Err.      | #Params | Cost(GPU days) |\n| ----------------------------------------------- | --------- | ------- | -------------- |\n| TRACE(This paper)                               | 2.78\u00b10.12 | 3.3     | 0.6            |\n| P-DARTS + cutout(Xin Chen et. al.. ICCV 2019.)  | 2.5       | 3.4     | 0.3            |\n| PC-DARTS + cutout(Yuhui Xu et. al.. ICLR 2020.) | 2.57\u00b10.07 | 3.6     | 0.1            |\n\n3. The writing requires significant proofreading. There are plenty of grammar errors\uff1a\n\n- In the 2-nd line of Section 2.2, \"Tensor methods have found wide applications\" -> \"... have been  found in...\"\n- In the 5-th line of Section 3.1, \"Note that a subnet P can be distinguished by its choices on each edge, we propose....\" -> \"... each edge, and we propose....\"\n- In page 2,  \"it also emerge in\"\n- etc..", "title": "This paper proposed a new way to unify the searching strategy for NAS", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "eB5aHlKM-09": {"type": "review", "replyto": "Oi-Kh379U0", "review": "This paper tries to use tensorize the supernet. The supernet itself is a graph and NAS aims to search the operations on the edges. The 'TRACE' finds that path-discovery appears in a number of tasks and NAS is one of them. \n\nThis is an interesting topic and the experiments are conducted on vision/logic chain/node classification tasks. \n\n+) This paper is clear and easy to follow.\n\n+) The topic is interesting. It is charming to design a universal method that could solve a number of problems.\n\nHowever, there are still some concerns:\n\n-) Is the title appropriate? The gradient-based NAS finds an optimal path in the global search space. However, for other tasks mentioned in this paper, is 'architecture search' appropriate to describe them? Could it be named as 'path search' or some others? Are there any other works support the 'architecture search' in other tasks?\n\n-) The notations are not clear. The $\\mathcal{J}$, the $r_{N_1(t)}$, the $\\theta$ should all be described. However, I could not find them either in the 'annotation' paragraph or the first-mentioned sentences.\n\n-) Move notations to the beginning of section 3.\n\n-) What is $R_{N_1(t)}$ in Alg. 1? The $R$ has not been explained. However, I think this is important for the reader to understand the algorithm.\n\n-) The results on CIFAR-10 aim to show that 'TRACE' could accelerate searching without loss of accuracy? What will happen if searching for longer epochs? According to 'Motivated by the success of previous methods (e.g., DARTS, SNAS and ENAS), our TN should include their encoding methods as special cases.', what is the number of parameters used for modeling the architectures? \n\n-) ' their decomposition method does not consider the topological structure of supernets,'. For the cell-based NAS in Table. 1. Why the topological structure has not been considered?\n\n-) In Fig. 2, it's better to add a toy example based on Eq. 4 and add more explanations.\n\n-) Please report the cost in Table 4, 5. For example, the params/search cost/flops in Table 3.\n\n-) (minor) Lack of experiments on large-scale datasets, like ImageNet.\n\n-) The HIN task searches for the rank-1 CP (Table 1), and the CV task also searches for the rank-1 solution. So this HIN task is just adopting the standard solution on another dataset? Are there any other differences?\n\n-) What if the rank goes higher in Figure 3 (b)?\n\nBased on the quality of this paper, I don't think this paper creates something new. In my opinion, this paper describes the solutions from a novel/universal perspective. Some of the existing methods are special cases. So I selected 5 as the initial score. It would be nice if the authors emphasize the unique contribution more clear and address the concerns. ", "title": "Review", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}