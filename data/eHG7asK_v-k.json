{"paper": {"title": "Multi-Agent Trust Region Learning", "authors": ["Ying Wen", "Hui Chen", "Yaodong Yang", "Zheng Tian", "Minne Li", "Xu Chen", "Jun Wang"], "authorids": ["~Ying_Wen1", "hcch.work.18@gmail.com", "~Yaodong_Yang1", "zheng.tian.11@ucl.ac.uk", "~Minne_Li1", "successcx@gmail.com", "~Jun_Wang2"], "summary": "A Multi-Agent Trust Region Learning (MATRL)  algorithm that augments the single-agent trust region policy optimization with a weak stable fixed point approximated by the policy-space meta-game.", "abstract": "Trust-region methods are widely used in single-agent reinforcement learning.  One advantage is that they guarantee a lower bound of monotonic payoff improvement for policy optimization at each iteration. Nonetheless, when applied in multi-agent settings, such guarantee is lost because an agent's payoff is also determined by other agents' adaptive behaviors.  In fact, measuring agents' payoff improvements in multi-agent reinforcement learning (MARL) scenarios is still challenging. Although game-theoretical solution concepts such as Nash equilibrium can be applied, the algorithm (e.g., Nash-Q learning) suffers from poor scalability beyond two-player discrete games. To mitigate the above measurability and tractability issues, in this paper, we propose Multi-Agent Trust Region Learning (MATRL) method. MATRL augments the single-agent trust-region optimization process with the multi-agent solution concept of stable fixed point that is computed at the policy-space meta-game level. When multiple agents learn simultaneously, stable fixed points at the meta-game level can effectively measure agents' payoff improvements, and, importantly, a meta-game representation enjoys better scalability for multi-player games.  We derive the lower bound of agents' payoff improvements for MATRL methods, and also prove the convergence of our method on the meta-game fixed points. We evaluate the MATRL method on both discrete and continuous multi-player general-sum games; results suggest that MATRL significantly outperforms strong MARL baselines on grid worlds, multi-agent MuJoCo, and Atari games. ", "keywords": ["multi-agent learning", "reinforcement learning", "empirical game-theoretic analysis"]}, "meta": {"decision": "Reject", "comment": "There was some slight disagreement on the paper, but the majority of reviewers agree that although some answers of the authors on questions brought good clarification, other issues still remain problematic. Some of the assumptions remain unclear (w.r.t CDTE), and reviewers still have doubts about the global convergence and weak stable fixed point concept, that lack clear math details.\nThe experiments are also still a bit too immature, more comparison is needed, as well as an evaluation on other domains."}, "review": {"ZTl5LVYYn6u": {"type": "rebuttal", "replyto": "FdUNZqrdXdY", "comment": "Dear reviewers, as promised, the experiments with five more random seeds have been updated in Fig. 5, the latest revision paper.", "title": "More random seeds for the experiments."}, "FEcsUi9QRdr": {"type": "rebuttal", "replyto": "vkx03Glvsck", "comment": "Thanks for your kind suggestion.\n\nWe have added 5 more random seeds for all the models in the experiments, and new learning curves with 10 seeds are updated in Fig. 5, the latest revision paper.\n", "title": "10 random seeds for the experiments."}, "OjAZn4DQOTc": {"type": "rebuttal", "replyto": "vkx03Glvsck", "comment": "Thanks for your further feedback!\n\n**Question**: I still am not sure I understand how weak stable points can be stronger than local Nash.\n\n**Response**: Sorry for the confusion still. We didn\u2019t mean that weak stable is stronger than local Nash; we think the stable fixed points and local Nash are not equivalent concepts in all the cases. In different cases (like Fig. 1 in [1]), a local Nash can be either a stable fixed point or saddle point. Similarly, a weak stable fixed point can be a stable or saddle point as well. Therefore, we think which concept would be better will depend on the specific objectives or settings, and we have updated the revision accordingly to justify this point.\n\n[1] Mazumdar E, Ratliff L J, Sastry S S. On Gradient-Based Learning in Continuous Games[J]. SIAM Journal on Mathematics of Data Science, 2020, 2(1): 103-131.", "title": "Further Response to Reviewer 3"}, "J0a9yaF6IS": {"type": "rebuttal", "replyto": "NRpVRcJDN3r", "comment": "Thanks for your further feedback!\n\n**Question**: Why is this a reasonable assumption in the more general (possibly competitive) case?\n\n**Response**: We agree that it is unrealistic to assume knowledge about other agents' policies in many real-world competitive scenarios. But in some cases, it might only require decentralized execution; like generative adversarial networks (GAN), it is ok to expose the generator and discriminator's parameters/policies. Furthermore, we want to show that the proposed method can suit the general settings. For example, even in zero-sum games, it can still stabilize the simultaneous learning processes, like some other research (e.g., SGA [1], MADDPG[2]). \n\n[1] Balduzzi D, Racaniere S, Martens J, et al. The mechanics of n-player differentiable games[J]. ICML, 2018.\n\n[2] Lowe R, Wu Y I, Tamar A, et al. Multi-agent actor-critic for mixed cooperative-competitive environments[J]. Advances in neural information processing systems, 2017, 30: 6379-6390.\n\n**Question**: I'm still not convinced about the global convergence theory. In particular, there is a connection between solving the meta-game that will limit possible solutions. Citation [2] is helpful here, but the details should be more clear and self-contained in the paper. Furthermore, the theory seems quite weak, so discussing how useful it is would be beneficial.\n\n**Response**: The meta-game analysis enhances the classic LookAhead method with variable step size scaling (e.g., WoLF-IGA [3], GA-SPP[4]) or two time-scale update rule [5]. In our case, the step size is varying at each TPR, which is controlled by the restricted meta-game Nash. We have added this clarification in the new revision and will try to have a formal mathematical and more precise discussion in the final version.\n\n[3] Bowling M, Veloso M. Multiagent learning using a variable learning rate[J]. Artificial Intelligence, 2002, 136(2): 215-250.\n\n[4] Song X, Wang T, Zhang C. Convergence of multi-agent learning with a finite step size in general-sum games[J]. AAMAS, 2019.\n\n[5] Heusel M, Ramsauer H, Unterthiner T, et al. Gans trained by a two time-scale update rule converge to a local nash equilibrium[C]//Advances in neural information processing systems. 2017: 6626-6637.\n\n**Question**:  The paper should still contain more common domains and comparisons with more state-of-the-art methods.\n\n**Response**: We understand that the more different/complex domains and baselines make the results more convincing, and we are working on more affordable experiments like multi-agent atari games, poker, etc.\n", "title": "Further Response to Reviewer 1 "}, "FdUNZqrdXdY": {"type": "rebuttal", "replyto": "eHG7asK_v-k", "comment": "We thank all the reviewers, ACs, and PCs for your efforts and constructive feedback.\n\nAs suggested, we have refined the revision paper, including a discussion about the convergence of global underlying game, model assumptions, the reasonability of the weak stable fixed point in the local restricted underlying game, etc. Besides, we have added an extra baseline for the experiments and an empirical comparison of the extra cost from the proposed method compared to independent learners with various agent numbers.\n\nWe are currently running several additional experiments and more random seeds as suggested, but it takes more time. We will post an update as soon as new results are available.\n", "title": "General Response to Reviewers' Comments "}, "WCSLuYhJs2m": {"type": "rebuttal", "replyto": "coFUd2X0s9", "comment": "We thank the reviewer for the detailed feedback.\n\n**Question**: the notion of \u201cweak stable fixed point\u201d is extremely weak. For example , Mazumdar 2020 (who is cited) and many others consider much stronger criteria like local Nash or differential Nash, or even quasi-Nash.\n\n**Response**: At first, it is worth pointing out that local Nash is not a stronger notion than stable points. In fact, it is the other way round. A local Nash is only a stable fixed point in zero-sum games (see Lemma 8 in [1]). In potential games, a local Nash can easily be unstable; think about the case of x^2+y^2+2xy, local nash of (0,0) is not a stable point. In general, stable points is a set of solution concepts for gradient-based methods that try to describe the cases when the gradient of policy change is zero, while local nash requires the best response from each agent\u2019s perspective. \nAlso, we agree that \u201cweak stable fixed point\u201d is a weak requirement, but it is reasonable to avoid strict maxima/minima. We make weak stable requirements for two reasons: 1. We want the method suits for general game settings, as shown in Eq. 16 and its following text, in different game settings: cooperative, competitive, and general-sum, the fixed-point found by the meta-game analysis can be either stable or saddle points. If we make some additional game class assumptions, we can easily obtain a stronger requirement. 2. The motivation of this MATRL is to find a low-cost way to search for a point better than an independent trust region learner. So, in the MATRL case, we only consider the stable points within each trust region gradient-descent steps. And in fact, one can only know that in a multi-agent setting because when each agent is updating using a trust region method in one gradient step, it is impossible (unless additional constraints to force the hessian definite in Eq. 16 to be PSD) to guarantee all of them will converge to an even fully stable point or local Nash. Therefore, finding other better stable points in a restricted step could be a good idea, it may suit for next step\u2019s work, but it comes with a cost, requiring additional computation or assumptions which may break the most general settings.\n\n[1] Balduzzi D, Racaniere S, Martens J, et al. The mechanics of n-player differentiable games[J].ICML, 2018.\n\n\n**Question**: a more complete discussion of convergence,\n\n**Response**: Thanks for the kind suggestions, it is a good point to clarify the method\u2019s contribution and effectiveness.   We have added more global convergence discussion in section 3 and the convergence analysis in experiments. In short, from the global convergence side, MATRL is a variant of LookAhead methods (e.g., extra-gradient, LOLA, SOS) via best response to the trust stable region (one-step look-ahead), which enjoys local convergence and avoids strict saddles in all differentiable games, see [2]. \n\n[2] Letcher A, Foerster J, Balduzzi D, et al. Stable opponent shaping in differentiable games. ICLR, 2019.\n\n\nResponses to other questions:\n\nAccording to your comments, we have refined the revision paper to make the it is easier to follow:\n\n1. Changed figure 1 and added additional explanations in captions to understand figures 1 and 2 better.\n\n2. For Figures 4, 5, 6, and table 1, added additional experiment settings and descriptions of results to make it easier to understand the results.\n\n3. We have simplified theorem 1 and had an additional discussion to show that multi-agent independent improvement with worse lower bound than the single-agent case.\n\n4. Added the mixed nash descriptions for $rho$.\n\nWorking on changes:\n\n1. Adding more runs for experiments that need much more time. We plan to increase the number of random seeds for each model to 10 to obtain more confident results.\n\n2. Thank you for pointing out these typos. To keep the minimum changes in the revision, we did not do the deep editing on the current revision.  But we definitely will look for a professional copy editor to fix all typos and syntax/semantic errors and make the presentation more fluent in the final version.\n", "title": "Response to Reviewer 3"}, "oMlpI0JK_ms": {"type": "rebuttal", "replyto": "L0ZRF9Ubise", "comment": "We thank the reviewer for the detailed feedback.\n\n**Question**: it would be interesting to investigate how the performance of MATRL and IL varies with a range of policy update sizes, and it would also be useful to clarify how the hyperparameters for the various experiments were selected.\n\n**Response**: It is a good suggestion to reveal further how and when MATRL works. As we assume the predicted policies have monotonic improvements, it is better to use trust-region-based ILs and small step size to make the meta-game easily estimated and solved. \nAs for the hyperparameters selection, the most important hyper-parameters are learning rate/step size, the number of update steps, batch size and value, policy loss coefficient. Appropriate learning rate and update steps plus larger batch size give a more stable learning curve. And for different environments, policy and value network loss coefficients that keep two losses at the same scale are essential in improving the learning result and speed. Also, for meta-game construction and best response update where we use the importance ratio to do estimation, a clipping factor of the ratio is vital to achieving a stable and monotonic improving result. The followings are the detailed parameter settings for each task. We have included this discussion in Appendix E.\n", "title": "Response to Reviewer 4"}, "x-WzOWcFRjl": {"type": "rebuttal", "replyto": "2_u4bPkjy5", "comment": "We thank the reviewer for the detailed feedback.\n\n**Question**:  It still isn't clear exactly what is assumed to be known about other agent policies.\n\n**Response**:  We are sorry for the confusion. To clarify the model assumptions,  we have made the \u201cto estimate and solve the meta-game, the knowledge about other agents\u2019 policies to collect the joint experience/trajectories is required\u201d assumption more obvious at the beginning of section 3 (and section 3.2) and added more detailed comparisons with other CTDE methods in related work.\n\n**Question**: It also isn't clear how useful the theory is since it only applies to the restricted underlying game rather than the full game.\n\n**Response**:  From the global convergence side, MATRL is a variant of LookAhead methods (e.g., extra-gradient, LOLA, SOS) via best response to the trust stable region (one-step look-ahead), which enjoys local convergence and avoids strict saddles in all differentiable games, see [1]. \nOn the other side, in a local restricted underlying game (during each meta-game analysis step), the meta-game equilibrium is an approximated equilibrium of the true underlying game, see [2]. \n\n[1] Letcher A, Foerster J, Balduzzi D, et al. Stable opponent shaping in differentiable games. ICLR, 2019.\n[2] Tuyls K, Perolat J, Lanctot M, et al. Bounds and dynamics for empirical game theoretic analysis[J]. Autonomous Agents and Multi-Agent Systems, 2020, 34(1): 7.\n\n**Question**: The paper should discuss why a subset of domains is chosen (i.e., why these particular domains).\n\n**Response**: We chose these environments (random matrix games, grid-worlds, multi-agent mujuco and atari pong) to test the method in various general settings, including cooperative/competitive and continuous/discrete games. VDN/QMIX etc is for discrete cooperative games, MADDPG is for continuous action games, and PPO based IL works for all the cases. We have added a clarification in the experiment sections.\n\n**Question**: Why use only VND and QMIX (which are very different than the proposed method)?\n\n**Response**: We used VND and QMIX as baselines on discrete grid-worlds because they empirically work well compared to COMA and MAAC. To do more comprehensive comparisons, we have included QTRAN as a baseline in grid-worlds, and we are also working on having more runs to make the results more confident.\n\n**Question**: Why not use any other method (besides independent learners) in multi-agent pong?\n\n**Response**: We used PPO based ILs in the Pong game (discrete action, competitive game, so we cannot apply VND/QMIX/MADDPG); because our MATRL can have the same policy models and parameters as PPO based ILs, then we can examine the effectiveness of the designed centralized trust region learning procedures. We agree that including more baselines to have comparisons would be much better, and we are working on adding more baselines like independent DQN, MAAC. But this game takes more time to train; we will update the results once it has been done.\n\n**Question**: how it was adapted to run in the partially observable case.\n\n**Response**: Thanks for indicating that! You are right, the grid-worlds are partially observable, and it is better to have an additional clarification to make it clearer. We now have added a description of the gap between the grid-worlds and formulations. \n", "title": "Response to Reviewer 1"}, "EKsxTqfBb5Z": {"type": "rebuttal", "replyto": "xPFXEeKDrzz", "comment": "We thank the reviewer for the detailed feedback.\n\n**Question**: some comparison with techniques like LOLA, symplectic gradient adjustments, etc\n\n**Response**: We have MATRL-PP as a substitute of LOLA (uses Taylor expansion to t approximate the best response to the predicted policies), in which MATRL-PP takes true best response gradient steps to the predicted. We have made this more obvious in the experiment settings.\n\n**Question**: Alternative (higher-order) methods don't necessarily converge to the equilibria of the original game -- does MATRL do so? Are MATRL solutions still a 'true' Nash equilibrium of the underlying game?\n\n**Response**: From the global convergence side, MATRL is a variant of LookAhead methods (e.g., extra-gradient, LOLA, SOS) via best response to the trust stable region (one-step look-ahead), which enjoys local convergence and avoids strict saddles in all differentiable games, see [1]. \nOn the other side, in a local restricted underlying game (during each meta-game analysis step), the meta-game equilibrium is an approximated equilibrium of the true underlying game, see [2]. \n\n[1] Letcher A, Foerster J, Balduzzi D, et al. Stable opponent shaping in differentiable games. ICLR, 2019.\n[2] Tuyls K, Perolat J, Lanctot M, et al. Bounds and dynamics for empirical game-theoretic analysis[J]. Autonomous Agents and Multi-Agent Systems, 2020, 34(1): 7.\n\n**Question**: I would have liked to see more reporting on the runtime of solving the meta-game.\n\n**Response**: Thanks for this suggestion. In the revision, we analyzed the cost of time of MATRL, MATRL without Best Response (MATRL w/o BR), Independent Learner with Policy Prediction (IL-PP) and Independent learner (IL) on 2 agents Checker, 3 agents Hopper, and 4 agents Switch game with 20K sampling steps. For all methods, they run the same number of 20,000 env steps and 50 gradient steps. As shown in Fig. 7, revision paper, we can see that the time consumption of each variation has no significant difference, MATRL takes around  10%-20% more time than the IL agents. \n\n**Question**: Eq 2 has a typo? D(pi_i, pi_i)\n\n**Response**: Thanks for pointing out this typo. It should be  $D(\\pi_i, \\hat{\\pi}_i)$, and we have corrected it in the updated version.\n", "title": "Response to Reviewer 2"}, "coFUd2X0s9": {"type": "review", "replyto": "eHG7asK_v-k", "review": "Summary:\nThis paper proposes a modification of the Independent Learners trust region policy optimization method in general sum games. The modification consists of first forming a \u201cmeta game\u201d\u2014ie. the matrix game in which each agent\u2019s options are his previous policy and his independent trust region optimized policy\u2014-and then interpolating between the two for each agent according to a Nash equilibrium of the meta game. The paper shows that this algorithm results in each step generating a \u201cweak stable fixed point\u201d. The paper concludes by showing a number of experimental results indicating the convergence and overall performance of the method as compared with relevant baselines in a number of different games.\n\nStrengths:\n* good motivation of why independent learner algorithms may not converge due to missed coupling\n* clever construction of meta game\n* good choice of variety of experiments\n\nWeaknesses:\n* the notion of \u201cweak stable fixed point\u201d is extremely weak. For example , Mazumdar 2020 (who is cited) and many others consider much stronger criteria like local Nash or differential Nash, or even quasi-Nash. As I understand it, \u201cweak stability\u201d is just requiring that iterates don\u2019t ever find a local minimum in the coupled strategy space of all agents. Not only is this artificially introducing a coupling requirement between the agents, but it is basically just saying that the agents shouldn\u2019t ever find themselves all wishing to change collectively.\n* the preliminaries are sometimes stated in a way that indicates only two players and other times N players\n* figures 1 and 2 are confusing\n* I find theorem 1 to be hard to parse (much notation is not defined clearly), based on very strong assumptions (alpha coupling, discrete spaces), essentially pointing out a fact about prior work on independent methods (not the current proposed one), and overall unnecessary in the present paper. I would advocate removing it.\n* it should be clarified that the rho are really distributions corresponding to mixed Nash strategies in the meta game, rather than deterministic strategies (which may not even exist)\n* in the experiments, it is not statistically useful to show results for only 5 seeds. I understand that these things are expensive and I have this comment for the vast majority of work in RL I have ever read, but showing plots like figure 5 is misleading, esp. without an immediate caution in the caption and remain text\n* I found figure 6 and the related text very confusing and not really useful in understanding what is going on\n* only validating convergence in matrix games is very limited. I was missing a more complete discussion of convergence, if only experimental\n\nNitpicks:\n* there were numerous typos and syntax/semantic errors throughout the paper. I would recommend employing the services of a copy editor in future\n\nOverall:\nI like the main concept of this paper, but I really can not recommend it for publication at this time, I have listed a number of directions in which I feel this paper could be improved in rebuttal or even in a later submission if it goes that route. My main concerns are: (1) the weakness of the theoretical property of \u201cweak stability\u201d, (2) lack of a more complete evaluation of convergence, (3) general confusion throughout reading the paper.\n", "title": "Interesting idea but has a number of holes", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "L0ZRF9Ubise": {"type": "review", "replyto": "eHG7asK_v-k", "review": "---- Summary ----\nThis paper proposes a multi-agent learning algorithm where after each policy improvement step, a small meta-game is analysed to propose a corrected step. The paper performs experiments in various MARL environments to test the approach.\n\n---- Reasons for score ----\nI recommend accepting this paper. The paper addresses the important problem of the non-convergence of independent learning in MARL. The algorithm proposed is novel and well justified, and the experiments show that it yields an improvement over independent learners.\n\n---- Pros ----\n1. The fundamental idea of the paper - taking independent policy improvement steps, and then analysing a local metagame to decide how to update policies - is interesting and novel.\n2. The practical algorithm this leads to helps with the problems of independent learning in MARL, without a large increase in algorithm complexity.\n3. The experiments demonstrate the advantages of the algorithm, in a variety of domains and with a fair comparison to the IL methods MATRL is being based on and other relevant baselines.\n4. The paper is situated well with respect to the existing literature.\n\n---- Cons ----\nIt seems to me that the effectiveness of the proposed method is likely to be heavily influenced by the underlying IL algorithm employed. In particular, an IL algorithm making very large policy updates is likely to need more correcting. For this reason, it would be interesting to investigate how the performance of MATRL and IL varies with a range of policy update sizes, and it would also be useful to clarify how the hyperparameters for the various experiments were selected.\n\n---- Typos and other minor comments ----\n1. Please clarify the meaning of the error bars in Figure 5.\n2. In the sentence before section 3.1, steps->step and details->detail.\n3. I didn't understand this in section 3.1: \"We set agent i\u2019s to make a monotonic improvement of its policy.\"\n4. Near the bottom of page 3, \"conflict interests\" -> \"conflicting interests\"", "title": "Novel algorithm addressing an important problem.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "2_u4bPkjy5": {"type": "review", "replyto": "eHG7asK_v-k", "review": "This paper presents a new trust-region method for multi-agent reinforcement learning (MARL). This approach extends ideas from single-agent trust-region methods to construct a smaller meta-game representing possible policy changes for each agent. The meta-game can then be solve to provide policy updates for the agents. Theory is provided for the meta-game (and corresponding restricted underlying game) and experimental results are shown with a number of baselines. \n\nThe idea of extending trust region methods to the multi-agent case is an interesting idea. As the paper points out, the multi-agent case is more complicated since the other agents are also learning, making the environment appear non-stationary from a single agent's perspective. The idea of using a meta-game with limited (i.e., 2) policy update choices at each step greatly simplifies learning, while at least somewhat taking other agent changes into account. Also, the method is based on theory for converging to a (weak stable) fixed point. \n\nThere are a lot of interesting ideas in the paper and the experiments are promising, but many of the details in the paper are not clear.\n\nFor example, the general assumptions and algorithm become clear in Figure 3 and Algorithm 1, but these should be explicitly stated much earlier (e.g., at the beginning of 3). Even so, it still isn't clear exactly what is assumed to be known about other agent policies. It appears that the process is fully centralized, where all agents know all other agents policies at all times. If this is indeed the case, why is it reasonable to use a centralized approach in a self-interested setting? In MARL, even assuming other agent actions are observed is a strong assumption, so assuming other agents policies are known is very strong. Furthermore, the paper says \"MATRL also provides fully decentralized execution and only requires a centralized mechanism to adjust the step size rather than a centralized critic or communication.\" but it appears that the training phase would need much more than that (to solve the meta-game) while the execution could be decentralized. A more detailed description of the algorithm should be given along with the assumptions needed. \n\nIt also isn't clear how useful the theory is since it only applies to the restricted underlying game rather than the full game. Since it appears to assume the other agents are fixed, it would be useful to at least speculate what could be said about the full case (e.g., when other agents are not fixed and when it can't be assumed the other other agent policy updates are known). Also, the game values in 5 would just be estimates in the MARL case (due to sampling and other agent changes) so it doesn't seem like the theory would apply in that case. These issues should be clarified in the paper. \n\nThe experimental results are impressive and show the proposed method works well. The method (MATRL) converges faster or to better values in all the domains. Nevertheless, it is not clear how the domains were chosen and many of the baselines are weak. In terms of the domains, the paper should discuss why a subset of domains is chosen (i.e., why these particular domains). In terms of baselines, there are stronger methods in each case. For example, many other MARL methods would apply in the Checker, Switch and MuJoCo domains (e.g., COMA and MAAC). Why use only VND and QMIX (which are very different than the proposed method)?  Why not use any other method (besides independent learners) in multi-agent pong? Lastly, several of the domains are partially observable, but the method is only defined in terms of full observability and nothing is said about how it was adapted to run in the partially observable case. It is assumed that they pretend the observation is state, but this should be made clear. ", "title": "Blind review", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "xPFXEeKDrzz": {"type": "review", "replyto": "eHG7asK_v-k", "review": "MULTI-AGENT TRUST REGION LEARNING\n\nSummary\n\nThe authors establish a trust-region method for multi-agent RL. The idea is to apply TRPO to each agent separately, but with a trust region whose properties for agent i, depend on those of agent not-i. The authors propose, at iteration k, to find a weak stable fixed point for a game M_k, defined by a matrix of payoffs given by the (expectation value of) advantages. They show that solving for the equilibrium of M_k yields policies that monotonically improve performance, i.e., each agent's payoff at least does not decrease. \n\nThe authors addresss the extra cost introduced by having to solve a game at each iteration, which does not scale well as the number of agents N grows. \n\nThey compare with independent learners (IL), and evaluate 3 synthetic experiments, multi-agent variations of Mujoco sims, and Atari pong. For games have fully cooperative rewards, but feature adversarial dynamics/rewards, the authors compare convergence rates (faster convergence in synthetic games), pair-wise win-rate (MATRL beats IL agents on pong), and overall reward-growth (MATRL improves reward faster in cooperative games). \n\nStrengths\n\n- The paper is written clearly, and the technique is relatively straightforward. \n- The empirical evaluation seems sufficient in terms of showing MATRL improves over IL. \n\nWeaknesses\n\n- I would have liked to also see some comparison with techniques like LOLA, symplectic gradient adjustments, etc, as it's unclear what equilibrium MATRL converges to. Alternative (higher-order) methods don't necessarily converge to the equilibria of the original game -- does MATRL do so? Are MATRL solutions still a 'true' Nash equilibrium of the underlying game? \n- I would have liked to see more reporting on the runtime of solving the meta-game. As the authors mention, this is a crucial issue in scaling this up, and it would be good to see some quantitative benchmark of run-time vs number of agents. \n\nQuestions\n\n- Eq 2 has a typo? D(pi_i, pi_i)", "title": "Clearly written, sufficient number of experiments, but some possible points of improvement", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}