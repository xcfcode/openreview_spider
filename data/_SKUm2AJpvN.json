{"paper": {"title": "Decoupling Representation Learning from Reinforcement Learning", "authors": ["Adam Stooke", "Kimin Lee", "Pieter Abbeel", "Michael Laskin"], "authorids": ["~Adam_Stooke2", "~Kimin_Lee1", "~Pieter_Abbeel2", "mlaskin@berkeley.edu"], "summary": "We introduce a new unsupervised learning task tailored for RL that, for the first time, supports representation learning fully decoupled from policy learning, as demonstrated across a range of visually diverse RL benchmarks.", "abstract": "In an effort to overcome limitations of reward-driven feature learning in deep reinforcement learning (RL) from images, we propose decoupling representation learning from policy learning.  To this end, we introduce a new unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC), which trains a convolutional encoder to associate pairs of observations separated by a short time difference, under image augmentations and using a contrastive loss.  In online RL experiments, we show that training the encoder exclusively using ATC matches or outperforms end-to-end RL in most environments.  Additionally, we benchmark several leading UL algorithms by pre-training encoders on expert demonstrations and using them, with weights frozen, in RL agents; we find that agents using ATC-trained encoders outperform all others.  We also train multi-task encoders on data from multiple environments and show generalization to different downstream RL tasks.  Finally, we ablate components of ATC, and introduce a new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when RL requires augmentation.  Our experiments span visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and our complete code is available at \\url{hidden url}.", "keywords": ["reinforcement learning", "representation learning", "unsupervised learning"]}, "meta": {"decision": "Reject", "comment": "This paper introduces ATC, which is a contrastive learning on observations separated in time, to learn representations that do not need to take rewards into consideration. These learned representations allow, for the first time, a real disentanglement between representation learning and control, as the agent can simply load such a representation, \u201cfreeze it\u201d, and still recover performance of end-to-end deep reinforcement learning agents.\n\nOverall, all reviewers agree this is a promising direction. Nevertheless, there has been extensive discussion (with the authors and privately) about the significance of the reported results due to the small number of seeds. On one hand, there\u2019s the argument that there is a wide range of experiments and that should compensate for a small number of seeds in individual experiments. On the other hand, there are experiments with as little as two seeds (e.g., DMControl multi-env) and this can be seen at most as anecdotal evidence. There\u2019s also the argument that we, as a community, should be striving for more reliable and meaningful experiments in reinforcement learning. Moreover, there have been concerns about how \u201cvariance\u201d is being reported (max and min performance) and, although the authors replied to that, an alternative plotting was never shown.\n\nImportantly, at this point it is not clear how many seeds were used in each experiment (Figures 6, 7, 9, 11, 12, 13 do not report the number of seeds used). It is said that each curve represents a minimum of 3 random seeds, but that is very informal and not that useful. Exactly stating the number of seeds would be the right thing to do, not to mention that in the rebuttal it is said that 8-game pretraining for Atari multi-env uses 2 seeds, contradicting the original claim. Also, sometimes, different methods, in the same experiment, are  \u201caveraged\u201d across different numbers of seeds (\u201cDMLab offline -- ATC is 4 seeds, PC and CPC are 2 seeds each\u201d). This is particularly problematic because of the small number of seeds and potentially high variance. Reporting the max over 4 numbers drawn from a Gaussian distribution is very likely to lead to a larger number than when reporting the max over 2 numbers drawn from the same Gaussian distribution. \n\nI do acknowledge the effort to increase the number of seeds during the rebuttal phase, but it is hard to accept a paper with unknown results. We have very little evidence to believe that going from 2 seeds to 5 seeds is not going to change the results. The reviewers couldn\u2019t agree on the variance of this process as well. Some say the variance of PPO is low between runs when using the same hyper parameters while others mention papers (e.g., Deep RL that matters) that show how much variance one can have across these methods. Thus, I cannot accept this paper conditioned on more seeds being added to the final version because we don\u2019t know what the results will look like. Since this paper is mostly an empirical study, it should have thorough experiments and a careful analysis of the results, but the small number of seeds prevents that in my opinion. Thus, as difficult as it is given the promising direction of the paper, I\u2019m recommending its rejection. I strongly encourage the authors to increase the number of runs in their experiments and to use a more standard measure of variability (e.g., standard error, standard deviation) when reporting their results. This will then be a very strong submission for a future conference.\n"}, "review": {"67WeylJ5OSx": {"type": "review", "replyto": "_SKUm2AJpvN", "review": "**Summary:**\nThis paper presents a new unsupervised learning method for learning latent representations for visual RL control domains. The method, Augmented Temporal Contrast (ATC), can be used alone to learn a representation to be combined with an RL algorithm, or as an auxiliary task in an end-to-end system. ATC matches or outperforms comparable end-to-end systems in several environments. The paper provides an extensive experimental study to support its claims.\n\n**Strengths:**\nThe paper is clearly written, and all of the main points are well articulated. ATC appears sufficiently novel, and is applicable to a wide variety of domains, and can be deployed in various configurations (e.g. auxiliary task, unsupervised pre-training, etc\u2026). Included is a thorough experimental study that effectively demonstrates the performance of the method.\n\n**Weaknesses:**\nAlthough the experimental study seem thorough. I could not find the actual number of independent runs (seeds) for each domain listed anywhere. This information should be included so that the reader can better evaluate the variance of each method, and make more confident conclusions. \n\n**Recommendation:**\nOverall I vote to accept. The method presented in the paper is not revolutionary, but it appears to be novel and significant enough to be of interest to deep RL practitioners.\n\n**Questions:**\nHow many independent runs (seeds) were used in each of the domains? Can this information be included in the main text?\n\n**After Author Response and Discussion:**\nThanks to the authors for their responses. After reading the other reviews and the author responses, I am lowering my score to 5. I think that the number of independent runs used (especially on the smaller domains), and the way the results are presented with the min-max extent makes me less convinced of the results than I was in the initial review. Adding many more independent runs (seeds), especially on the smaller domains, would improve my confidence a lot. Overall I think the paper is of interest to the community, but the experiments and their analysis could be improved.", "title": "A new unsupervised learning method for learning latent representations for visual control domains.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "dYg8nU0sOFm": {"type": "review", "replyto": "_SKUm2AJpvN", "review": "The paper introduces an unsupervised task called Augmented Temporal Contrast which associates pairs of observations separated in time using a contrastive loss. The paper uses the task in several training regimes (in online RL, pretraining and multi-task RL).\n\nPros:\n- Well written and structured paper.\n- Interesting, general and simple to implement task.\n- Evaluated in several training regimes and on several environments.\n- Improves sample efficiency on most of the environments and setups, and improves over prior methods.\n- The attention maps in the paper and appendix are great and although they may be hand picked(?) examples, it highlights the issue with many approaches that can't model a goal rarely seen.\n\nCons:\n- You write you use multiple seeds but I don't see anywhere details on this? Consider adding it to the tables in appendix.\n- In Figure 3 one agent step is 4x environment frames? I suggest to make it clear in plot or in caption. In Figure 2, environment frames is used.\n\nComments/questions:\n- Wrt. 4.1, to what extent is the \"small\" replay for DMLab necessary over just using observations in batches/unrolls? Looking at Table 3 I can't see how large the replay is? 10k as in SAC or smaller?\n\nUpdate: Not all the experiments are particular thorough and the novelty less than expected.", "title": "A good improvement over prior unsupervised RL tasks in many setups", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "AYBhxVXFT3": {"type": "review", "replyto": "_SKUm2AJpvN", "review": "This paper proposes an auxiliary task for learning better representations for reinforcement learning. The idea is interesting and is a very active area of research at the moment.\n\nMy main concern is that the paper is mostly an empirical evaluation, as the novel algorithm is mostly an extra contrastive loss. Further, it seems their method is almost exclusively meant for pixel-based environments (see point 1 below), so the authors should make this point more explicit. Given this empirical emphasis, I feel the authors could have performed a deeper exploration into understanding _why_ their proposed algorithm performs the way it does in the different environments considered. In particular, some very specific design decisions were made in evaluation (see, for instance, points 2, 5, 9, 11, 12, 13, 14, 15, 16 below). \nThe clarity of exposition could also use some improvement, as I detail below.\n\nMain questions/concerns:\n1. In Section 3, the authors write \"This task encourages the learned encoder to extract meaningful elements of the structure of the MDP from observations.\". This assumes some type of continuity in pixel space, relative to MDP dynamics, which is in general not true.\n2. In Figure 1, why doesn't the momentum encoder go through a residual predictor?\n3. It seems $\\theta$, $\\phi$, $\\bar{\\theta}$, and $\\bar{\\phi}$ are all updated independently, is this the case? What is the actual training regimen? Are the ATC and regular RL networks trained concurrently?\n4. It would help if you include a proper algorithm in the paper.\n5. Above equation (2), the authors say \"In our implementation, the positives from all other elements...\". It's not clear what \"positives from  all other elements\" means.\n6. In section 4.1 the authors say \"multiple seeds were run\", please specify how many.\n7. In section 4.2 the authors say they are capable of \"training the encoder online, fully detached from the RL agent\", but how is it fully detached if they share the conv layers?\n8. It's not clear what the difference between ATC and UL training is. In some experiments the authors use ATC, in others UL. Are they the same thing? For example, in Figure 3, which ones are ATC? Also in Figure 14 vs Figures 15 and 16?\n9. In Figure 3, why does one environment compare with pri and the other with 2x, but not both in both environments?\n10. In the **Atari** subsection on the comment of detached training, the authors point out subpar performance on Breakout and SpaceInvaders. On SpaceInvaders it's possible the screen changes could cause issues, but what do the authors think cause the subpar performance in Breakout?\n11. In point (iii) of section 4.3, what's the network architecture used for training the RL part?\n12. In section 4.3 the authrs say they \"drew expert demonstrations from partially-trained RL agents\". Were these all drawn from the same checkpoint?\n13. In the **DMControl** section, the VAE is trying to reproduce a frame $T$ steps in the future? What is the value of $T$ used? Did you try different values?\n14. Similar questin for the **Atari** subsection. Also for this section, does your VAE try to predict individual frames or stacked frames (as frame stacking is common in Atari experiments)?\n15. In Figure 8 top, are these after pre-training the encoder? If that is the case, regular RL would have used fewer frames in comparison, no? Where would RL be if left to train for longer?\n16. In section 4.5 please clarify what \"random shift augmentations\" are.\n17. In the **Encoder analysis** subsection, what do you mean by \"attention\"?\n18. In Figures 14, 15, and 16 it's not at all clear what we're supposed to be looking for, nor how they show that ATC/UL is focusing on the score/enemy and the others are not.\n\nMinor comments:\n1. At the bottom of page 2, the term \"POMDP\" has not been introduced yet.", "title": "Official Blind Review", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "lEFf4CHjUeh": {"type": "rebuttal", "replyto": "_MBBvjgMK40", "comment": "We've increased the number of seeds for online RL vs ATC experiments from 6 to 10 in DMControl and from 4 to 8 in Atari, without significant change in outcome. (new plots are in the paper Fig 2 and Fig 4) DMLab experiments are underway now, altho they take longer to run.\n\nBest,\nAuthors", "title": "Updated draft with more seeds"}, "GJqDiWxniDr": {"type": "rebuttal", "replyto": "6CCSpQkwqHj", "comment": "We've increased the number of seeds for online RL vs ATC experiments from 6 to 10 in DMControl and from 4 to 8 in Atari, without significant change in outcome.  (new plots are in the paper Fig 2 and Fig 4)  DMLab experiments are underway now, altho they take longer to run.  ", "title": "more seeds run"}, "VSz4znrAJ6": {"type": "rebuttal", "replyto": "6CCSpQkwqHj", "comment": "Yes, good point to have more seeds.  We are launching more experiments to improve the confidence levels.\n\nIn the meantime, here is a more full accounting of the number of seeds already in the paper; hopefully it comes across as a large total number of random seeds given the many env/algo combinations (apologies for the confusion that we conservatively reported as 3 seeds some experiments which were actually more, notably DMControl online, our mistake!).  We'll  also note that we ran many experiments leading up to the final ones in the paper, which gives us confidence about the results (of course doesn't count in the paper).\n\n\nIn order in the paper:\nDMControl online -- 6 seeds  \nAtari online -- 4 seeds\nDMLab online -- 3 seeds (except lasertag_small -- 5 seeds)\n\nDMcontrol offline -- 4 seeds\nAtari offline -- minimum 3 seeds\nDMLab offline -- ATC is 4 seeds, PC and CPC are 2 seeds each\n\nDMControl multi-env -- 4 seeds ATC (6 seeds RL)\nAtari multi-env -- 3 seeds (except 8-game pretraining -- 2 seeds)\n\n\nWe will give it a try to plot each individual run, which would be the best raw presentation!  Just afraid this will get cluttered with the DMLab and Atari experiments which have so many different algos on the same plot.", "title": "yes, will provide more seeds"}, "Xa5EVZBXVzJ": {"type": "rebuttal", "replyto": "GlCmz3Rg27E", "comment": "Thank you for taking the time to provide feedback on our manuscript.\n\nAnd thank you for raising your work [2] to our attention, we have added a citation in our draft to acknowledge it and strengthen our related work discussion. (blue text in the paper)\n\n---\n**Q1. ATC task is similar to [2]**\n\n**A1.** We agree that there are similarities between our ATC task and your CEB auxiliary task [2], but ATC is more easily understood in relation to the other works we already provide--clearly this is an active area of research we are building upon.  The main motivation for and distinction of the auxiliary task in [2] seems to be compressive ability of the conditional entropy bottleneck (CEB), but we make no use of this mechanism in our loss function (and avoid its additional hyperparameter).  Furthermore, unlike [2], the main point of our paper is to fully decouple UL from RL and compare against end-to-end RL, see Q3/A3.\n\n---\n**Q2. ATC task is similar to [1]**\n\n**A2.** Although the developments in [1] emphasized spatially local losses, they do include a final experiment which uses a global-only loss similar to ATC (although still not identical), and they provide nice analysis of the MI learning objective, as we have described (and have discussed this reference with the authors).  [1] does not decouple UL from RL.  Although there are certainly closely shared components, it seems to us unlikely that someone reading and reproducing [1] or [2] would arrive at ATC directly (nor its simplicity).\n\n---\n**Q3. Other methods have decoupled encoder training**\n\n**A3.** It's correct that other methods have tried decoupling UL from RL in their ablations, but they have always taken a performance hit relative to end-to-end---this is true of CURL (Laskin 2020) and appears to be the case for PI-SAC (Fig 2 vs Fig 15 in v2 of [2]). You're also right that world models like Dreamer use a detached encoder. However, Dreamer and similar methods don't show results with the reward gradient backpropagated end-to-end. In our work, we make an apples to apples comparison between the decoupled and end-to-end method, and show that ATC succeeds in decoupling UL from RL without losing performance. We agree that this is nuanced and have clarified this further in the text (blue text) since you're right that other methods have included detached encoder results.\n\n---\n**Q4. Learning the structure of MDPs from observations**\n\n**A4.** True, we have refined the statement in the introduction to be more precise, that the part of the structure of the MDP being learned is to do with the observations and transitions only.\n\n---\n**Q5. On not using actions.**\n\n**A5.**  Thank you for pointing out the merits of training ATC without actions.  We were surprised, if a little disappointed, not to discover strong benefits from including the actions either as an input to the ATC model (forward model) nor predicting them as an output (inverse model) in our experiments.  In Section E of [2] (in v2, 26 Oct only), it does not appear to us a clear-cut claim that action-conditioning had a strong effect, as most of the shaded regions in the figure overlap almost entirely.  We found similar, very slight differences (much less than the difference between ATC and other UL methods) and opted for the simplicity of doing without actions.  While it seems intuitive that they should help, perhaps a more clever/meaningful way to introduce them into the model is required, and hopefully by mentioning our attempts in the paper, we give at least a starting clue for others.\n\n---\n**Q6. Novel empirical contributions**\n\n**A6.** Thank you for acknowledging our experiments, we hope they are a useful reference!\n", "title": "comparing decoupled UL to end-to-end RL"}, "mIoHxaCcbUV": {"type": "rebuttal", "replyto": "UlS6LjvrlW4", "comment": "Q1. solid lines average?\nA1. Yes, exactly, we have updated the draft to state this.\n\nQ2. same number of seeds?\nA2. Within an experiment, each method being compared had the same number of seeds.  Some experiments had more or less seeds than others, mainly based on different time/computation requirements.\n\nQ3. Reporting best/worst vs other summary statistic?\nA3. Some papers report the average with a bold line, and then with faint lines draw all the individual random seeds, but this became too cluttered when we were comparing so many UL methods.  Conveying the maximum extent of the raw results still seemed useful when reporting on a modest number of seeds (3 or 4), especially in the DMControl experiments, some of which can vary widely from run to run (i.e. so if someone is reproducing this and they get some of the high/low runs, it\u2019s straightforward to see if they fall in the range we observed).  Does this seem reasonable?\n", "title": "paper updated with number of seeds"}, "bKpVmm5Br8w": {"type": "rebuttal", "replyto": "AYBhxVXFT3", "comment": "We appreciate your detailed questions, comments, and suggestions!  We clarify each of these below, and have revised the manuscript to incorporate your feedback.  Please see the blue text in the new draft for these revisions.\n\nWe agree the the paper is mainly empirical, but we view this as a strength. We (i) show the first successful decoupling of unsupervised representation learning and reinforcement learning that does not suffer in performance relative to end-to-end RL, and (ii) identify the simplest possible algorithm and architecture that achieves strong performance across a diverse range of common pixel-based benchmarks.  We hope that it is rather a strength to include such extensive evaluations (as other reviews have noted), each demonstrating a different setting/aspect for the algorithm performing in reasonable ways, for a fairly comprehensive first assessment.\n\nDeeper probing of why our approach works is an interesting question, and to good extent is included in the ablations and encoder analysis--we\u2019ll try to clarify several points below. \n\nQ1: continuity in pixels relative to MDP dynamics\n\nA1: We think of this as a rather widely-held assumption, that the (subsequent) screen observations contain relevant information to the MDP state and dynamics, as is widely considered for POMDPs.  We can discuss more thoroughly if maybe there is a specific kind of continuity in mind?\n\nQ2: no momentum predictor\n\nA2: As in Grill 2020, the predictor is intended to be a transform from \u201ccurrent\u201d timestep of the anchor to \u201cfuture\u201d timestep of the positive, and hence is not needed on the positive branch.\n\nQ3: parameter update scheme\n\nA3: The momentum encoder parameters are updated every time the encoder parameters are updated (Table 4, Appendix 3), although a less frequent schedule could probably work.  When the main encoder parameters are updated is flexible and differs by how ATC is used, e.g. online vs offline, so we describe that in the respective experiment sections.\n\nQ4: include a proper algorithm\n\nA4:  Following your suggestion, we included a block Algorithm in Appendix A.1.\n\nQ5: clarity of \u201cpositives of all other elements\u201d\n\nA5: Thanks, we have re-ordered the phrasing in the paper.\n\nQ6: number of seeds\n\nA6:  At least 3 seeds for each curve,  added a detailed description to the paper.\n\nQ7: meaning of \u201cfully detached\u201d\n\nA7: We have clarified in the paper that this means no training gradient from the RL loss is used to update the encoder (this term \u201cdetached\u201d followed recent previous literature Laskin 2020b).\n\nQ8: ATC vs UL\n\nA8: Thanks, we have unified the text/figures to refer to ATC explicitly in all cases.\n\nQ9: prioritized vs 2x UL training in different environments\n\nA9: This is discussed in the paper--in one environment the agent needs to identify a rarely-seen object associated with reward (so use prioritized), in the other environment it simply benefits from doing more representation learning (it was apparently a bottleneck, so use 2x).\n\nQ10: why subpar on breakout\n\nA10: An interesting question for future research, although we did address it in the offline setting--with certain ablations the ATC encoder focused on the paddle in cases where it needed to represent the ball (Figure 10).\n\nQ11: architecture for policy after offline UL\n\nA11: Same architecture as for online UL, now written in the paper.\n\nQ12: checkpoint for expert data\n\nA12: Each different UL algorithm accessed the exact same data set, now made clear in the paper.\n\nQ13&14: values of T for VAE\n\nA13&14:  These are listed in the appendix: for DMControl we tried 0 and 1, with 1 best; for Atari 0, 1, and 3, with 3 best.\n\nQ15: number of pretraining frames\n\nA15: Yes, also listed in appendix, we used 5e4 transitions from each environment for pre-training.\n\nQ16: meaning of \u201crandom shift\u201d\n\nA16: Now defined in the paper.\n\nQ17: meaning of \u201cattention\u201d\n\nA17: Similar to [1], we compute the spatial attention map by mean-pooling the absolute values of the activations along the channel dimension and follow with a 2-dimensional spatial softmax. We clarified this in the revised draft. \n\n[1] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In ICLR, 2017.\n\nQ18:  figures 14, 15, 16\n\nA18: Basically, given input observations, spatial attention highlights the where the encoder focuses on. Therefore, Figure 14, 15, and 16 implies that the encoder pre-trained via ATC focuses on the meaningful features, such as the score/enemy, which also explains why ATC encoder is effective for RL. \n\nWe hope that these clarifications along with the comprehensiveness of our results will merit your further consideration.  We strove to bring value by showing how widespread are the possible use cases for our method--which all reviewers agree addresses an important area of research--hopefully making it likely to be adopted and to prompt further studies, both empirical and theoretical.\n", "title": "Many suggestions/clarifications incorporated // empirical emphasis rather as a strength"}, "l_aWebqsz9v": {"type": "rebuttal", "replyto": "e6miFpBSNCB", "comment": "\nWe appreciate your thoughtful consideration of our paper. As other reviewers (R2, R3, R4) mentioned, we introduce a novel unsupervised learning method for visual RL with extensive experiments on various RL benchmarks. We hope that our work will bring novel ideas/perspectives and highly effective techniques towards improving visual RL to the ICLR community.  \n\n---\n**Q1. Goal of the paper**\n\n**A1.** The goal of the paper is to show that unsupervised representation learning can be decoupled from reinforcement learning without degrading the algorithm\u2019s performance relative to end-to-end RL. To the best of our knowledge, we are the first to show that this is possible. We then investigate the properties of unsupervised pre-training through a series of detailed ablations. We note that not only do we present pre-training experiments, we first show ATC can successfully learn representations in an online fashion which starts from scratch with the RL agent.\n\nThe questions of emphasis in the paper are interesting and worth discussing.  We would like to argue that the paper very specifically and intentionally addresses a capability that has been hinted at in a long series of related works, but never fully achieved.  For this reason it is a worthwhile contribution to explore a broad range of (standard) environments, for example to identify any corner cases which don\u2019t quite work.  In the end, we discovered only a few of these (some of the Atari games).  Establishing and reporting all of this is already a substantial amount of new material and is the main point of the paper.  We can make this emphasis more explicit in the writing.\n\n---\n**Q2. Analysis on generality of representation**\n\n**A2:** Agreed it would feel rather unsatisfying not to include at least some coverage of the immediate benefits of our approach outside of the usual RL workflow (i.e. not just ATC as an auxiliary task or detached encoder training).   We show that our method makes it quite easy to leverage multi-domain training in DMControl, but the same is not fully true for Atari.  This result clearly points to the fact that one environment can share visual features while the other cannot--a useful reference point for future research into multi-domain methods.\n\n**_Your suggestion to deepen the multi-domain studies could lead to a very interesting addition:_** to include encoder attention analysis for these cases (as we did for the ablations).  Thanks very much for pushing this direction, we will set about generating these in time for final review!\n\n---\n**Q3. Several ideas in the paper**\n\n**A3.** Furthermore we were able to show that image augmentation helps to regulate the policy, not only the convolutional encoder, in DMControl, a previously unknown fact. And we open the door to doing augmentation in the latent image space, improving computational efficiency with a pre-trained encoder.  Of course we also included several ablations of our method to help establish the minimal working algorithm.\n\nThis is certainly a spread of ideas, curated with intention.  We aimed to firmly establish that ATC is widely applicable and not dependent on some quirk or gimmick from one type of environment.  And thereafter we point toward some follow-on benefits arising from the flexibility that decoupled representation learning brings to RL, hopefully setting the stage for further investigations by the community.\n \n\nDoes this sound reasonable and a subjectively sound contribution?\n", "title": "main goal -- show decoupled UL works as well as end-to-end RL //  will add more analysis in multi-domain case"}, "9r-addPM-eO": {"type": "rebuttal", "replyto": "dYg8nU0sOFm", "comment": " We appreciate your positive assessment of our paper. As you and other reviewers (R2 and R3) mentioned, we introduce a novel unsupervised learning method for visual RL with extensive experiments on various RL benchmarks. We hope that our work will bring novel ideas/perspectives and highly effective techniques towards improving visual RL to the ICLR community.\n\nPlease see the following clarifications in blue text in the new draft:\n\n---\n**Q1. Random seeds**\n\nA1. Good point!  We ran at least 3 for each curve, and have added this to the paper.\n\n---\n**Q2. Frame repeat**\n\nA2. Thanks, we have clarified this relationship in the captions in the draft (the conventions in previous literature in DMControl is to report environment steps, and Atari/PPO more often agent steps).\n\n---\n**Q3. Replay Buffer size**\n\nA3. Thanks we have now listed this in the appendix for Online ATC settings.  We used 100k in all environments (as standard in SAC, and much smaller than the 1M standard in DQN).  This would be an interesting ablation to decrease the size or try to do away with it altogether for PPO.  DMLab slightly preferred not to have time-correlated anchors in each training batch but it wasn\u2019t a huge effect, and Atari was fine with it.  But keeping a replay buffer does provide more flexibility for techniques like prioritized replay or simply doing more representation learning updates, both of which were helpful in DMLab.\n", "title": "Appreciate the positive assessment!"}, "rSUfn2B9450": {"type": "rebuttal", "replyto": "67WeylJ5OSx", "comment": "We appreciate your positive assessment of our paper. As you and other reviewers (R2 and R4) mentioned, we introduce a novel unsupervised learning method for visual RL with extensive experiments on various RL benchmarks. We hope that our work will bring novel ideas/perspectives and highly effective techniques towards improving visual RL to the ICLR community.  \n\n---\n**Q. Random seeds**\n\nA.  Good point!  We ran at least 3 for each curve, and have added this to the paper. (blue text in the updated draft)\n", "title": "Appreciate the positive assessment!"}, "e6miFpBSNCB": {"type": "review", "replyto": "_SKUm2AJpvN", "review": "Summary of the paper:\nThe paper aims to define an unsupervised pretraining architecture that can be used to pretrain representations for a reinforcement learning agent. The proposed solution is called \"Augmented Temporal Contrast\" (ATC). It consists of an encoder-compressor-predictor architecture for an (augmented) observation that is trained in a small latent space by minimizing the InfoNCE loss between that prediction, and the encoding of some future state.\nThe representation used (the pretrained encoder) is then frozen, and a policy network is trained consuming these representations. The authors then evaluate their approach on several different tasks: a control domain (DMcontrol), Mazenagivation (DMLab) and Atari. They show that their approach - without finetuning the representations - achieves comparable results to end-to-end reinforcement learning on most of the different domains. They also show that adding their defined loss as a regularizer always helps learning. They finally show that their representations can generalize out-of-domain by running several multi-task experiments, while only having pretrained on one domain.\n\nCommentary on the goal of the paper:\n\nThe goal of the paper is extremely important: separating learning representations from learning policies would enable better transfer, possible more sample efficiency and lower variance in outcome. \n\nStrengths:\n- The authors propose a well-designed solution that combines existing approaches in a well thought-out way. \n- The paper is extremely well written\n- The paper has an extensive empirical section. \n- Results are generally very good.\n\nWeaknesses:\n- The goal of the paper is a bit vague. As I said above, I agree and understand the desire for the separation of representation learning and reinforcement learning. However, the paper would have been stronger if it had concentrated on a single benefit of this separation, and evaluated their approach on that. While they say that their goal is to investigate \"how to learn representations which are agnostic to rewards\", this is too general as well. (One example would be to say that the decoupling makes for better generalizes to new MDPs - but this is not the focus of the analysis, just an aspect. The paper is, unfortunately, less convincing for it.)\n- Along a similar line of thought, the results, while strong, are not as convincing as they could be, because the paper does not focus on the benefits of reward-agnostic representations learned by ATC. The results that consider the generalization advantage (the Multi-Task learning experiments) are weaker, without the paper offering an analysis as to why.\n\nIn total, I would argue that this is a well-written paper with interesting analysis that could be a lot stronger by narrowing the scope of the contained argument. I argue for rejection, because I can see an updated version of this paper to be a great paper.", "title": "A well-written paper with sound reasoning and mostly good results that should sharpen and narrow its focus", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}