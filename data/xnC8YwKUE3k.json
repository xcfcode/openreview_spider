{"paper": {"title": "Clairvoyance: A Pipeline Toolkit for Medical Time Series", "authors": ["Daniel Jarrett", "Jinsung Yoon", "Ioana Bica", "Zhaozhi Qian", "Ari Ercole", "Mihaela van der Schaar"], "authorids": ["~Daniel_Jarrett1", "~Jinsung_Yoon1", "~Ioana_Bica1", "~Zhaozhi_Qian1", "ae105@cam.ac.uk", "~Mihaela_van_der_Schaar2"], "summary": "We develop and present Clairvoyance: a pipeline toolkit for medical time series.", "abstract": "Time-series learning is the bread and butter of data-driven *clinical decision support*, and the recent explosion in ML research has demonstrated great potential in various healthcare settings. At the same time, medical time-series problems in the wild are challenging due to their highly *composite* nature: They entail design choices and interactions among components that preprocess data, impute missing values, select features, issue predictions, estimate uncertainty, and interpret models. Despite exponential growth in electronic patient data, there is a remarkable gap between the potential and realized utilization of ML for clinical research and decision support. In particular, orchestrating a real-world project lifecycle poses challenges in engineering (i.e. hard to build), evaluation (i.e. hard to assess), and efficiency (i.e. hard to optimize). Designed to address these issues simultaneously, Clairvoyance proposes a unified, end-to-end, autoML-friendly pipeline that serves as a (i) software toolkit, (ii) empirical standard, and (iii) interface for optimization. Our ultimate goal lies in facilitating transparent and reproducible experimentation with complex inference workflows, providing integrated pathways for (1) personalized prediction, (2) treatment-effect estimation, and (3) information acquisition. Through illustrative examples on real-world data in outpatient, general wards, and intensive-care settings, we illustrate the applicability of the pipeline paradigm on core tasks in the healthcare journey. To the best of our knowledge, Clairvoyance is the first to demonstrate viability of a comprehensive and automatable pipeline for clinical time-series ML.", "keywords": ["reproducibility", "healthcare", "medical time series", "pipeline toolkit", "software"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper addresses a pressing problem for applications involving clinical time series and introduce a pipeline that handle many of the issues pertaining to data preprocessing.\n\nAn important contribution is the software that makes the processing more seamless, which will, without a doubt, be useful to the community given the need for reproducibility.\n\nThe authors have responded suitably to reviewer comments with the main 'leftover criticism' being that such a paper may not be the best fit for ICLR. This isn't a typical paper. However, something that introduces this level of automation and flexibility in handling time series has not been presented at this conference (or other ML conferences) to the best of my knowledge. It seems it could work in conjunction (as opposed to competing) with any new time series models/techniques that may be introduced."}, "review": {"-cKlp1jlinV": {"type": "rebuttal", "replyto": "RvLeelbIlfT", "comment": "---\n\nWe are sincerely grateful for your time and energy in the review process.\n\nIn light of our responses (Nov 19) and revisions (Nov 19), we would appreciate if the reviewer kindly let us know of any leftover concerns in the very limited time remaining. We would be happy to do our utmost to address them.\n\nThank you!\n\nPaper3220 Authors", "title": "Dear Reviewer #1"}, "3trvxEJ7swX": {"type": "rebuttal", "replyto": "mC5gsJsIW4o", "comment": "---\n\nWe are sincerely grateful for your time and energy in the review process. Please let us know if there are any leftover concerns--- We would be happy to do our utmost to address them.\n\nThank you!\n\nPaper3220 Authors", "title": "Dear Reviewer #3"}, "X6wc3TWOQPH": {"type": "rebuttal", "replyto": "EmxX_YVHkCJ", "comment": "---\n\nWe are sincerely grateful for your time and energy in the review process.\n\nIn light of our responses (Nov 19) and revisions (Nov 19), we would appreciate if the reviewer kindly let us know of any leftover concerns in the very limited time remaining. We would be happy to do our utmost to address them.\n\nThank you!\n\nPaper3220 Authors", "title": "Dear Reviewer #2"}, "deN1sFyiGu9": {"type": "rebuttal", "replyto": "xnC8YwKUE3k", "comment": "---\n\nWe thank the reviewers again for their thoughtful comments.\n\nWe have updated the paper to more clearly and accurately reflect our specific positioning and contributions. Broadly, this includes (a.) fully-worked examples with code, (b.) additional background on experiments, (c.) specifics regarding software design principles, and (d.) miscellaneous clarifications. We are grateful for all questions and suggestions, which improve the clarity and thoroughness of our positioning and presentation. All additions to the manuscript are made in *blue*.\n\n---\n\n**(a.) Fully-worked Examples with Code**\n\n[Reviewers 1, 2, 3, and 4]:\n\n- **New Appendix E** (\u201cWorked Example: Using the Full Pipeline\u201d): We now provide a worked example of using the *Clairvoyance pipeline* to train and use a model (for this, we use the predictions pathway). This takes the form of a detailed, step-by-step walk-through of the entire pipeline and its components, with fully-executable code, comments, and accompanying descriptions where appropriate.\n\n[Reviewers 1, 2, and 3]:\n\n- **New Appendix F** (\u201cWorked Example: Using the AutoML Interface\u201d): We now provide a worked example of using the *optimization interface* to perform stepwise model selection (for this, we use the treatment effects pathway for variety). This also takes the form of executable, commented code, organized as in a standard experiment---using a \u201cmain\u201d function wrapper with top-level arguments.\n\n[Reviewers 1, 2, and 3]:\n\n- **New Appendix G** (\u201cExtensibility: Example Wrapper Class\u201d): We now provide an example of how a generic *wrapper class* should be written for the purpose of integrating an external model/algorithm that is not already implemented in the current version of Clairvoyance. Specifically, we show an example of how a classical time-series prediction model (ARIMA) can be easily integrated.\n\n[Reviewers 1, 2, and 3]:\n\n- **New Focus Box** (\u201cWorked Examples\u201d): In Section 4 of the revised manuscript, pointers to all of these worked examples are now included in a dedicated \u201cfocus box\u201d. Furthermore, this also includes references to the *interactive tutorials* (using Jupyter notebooks) and *top-level API code* with examples of pathways and optimizations, that are part of the software repository.\n\n---\n\n**(b.) Additional Background on Experiments**\n\n[Reviewers 1 and 3]:\n\n- **New Appendix B** (\u201cBackground References for Experiments\u201d). We now provide a complete tabulation of exemplary references to original papers for the following details on each modeling question examined in the Section 4 experiments (e.g. \u201cclinical deterioration of ward patients\u201d), as well as---where appropriate---pointers to the relevant sections in those papers that we follow. These references include original works describing and justifying the modeling task; original description/documentation for the dataset used, including details on data collection, coverage, ethics, etc.; the procedure for initial data cleaning/selection that we followed; as well as theoretical background on the Clairvoyance pathway involved in the modeling question. Note that this tabulation of details is *in addition* to the medical motivation provided in Section 4, the low-level experiment details in Appendix D, as well as the worked-out code examples now provided in Appendices E, F, and G.", "title": "Revised Paper"}, "P5r4dZCu_T": {"type": "rebuttal", "replyto": "deN1sFyiGu9", "comment": "---\n\n**(c.) Specifics Regarding Software Design Principles**\n\n[Reviewer 2]:\n\n- **New Focus Box** (\u201cKey Design Principles\u201d): In Section 2 of the revised manuscript, the framework\u2019s design philosophy is now explained in detail, including: (a) Pipeline First, Models Second (emphasis is on *reproducibility*), and how it concretely manifests in the framework\u2019s design. (b) Be Minimal and Unintrusive (emphasis is on *standardization*), and how it concretely manifests in the framework\u2019s design. (c) Encourage Extension (emphasis is on *extensibility*), and how it concretely manifests in the framework\u2019s design.\n\n[Reviewer 2]:\n\n- **New Focus Box** (\u201cFlexibility vs. Standardization\u201d): In Section 2 of the revised manuscript, the intended usage of Clairvoyance is clarified in a discussion about flexibility and standardization. In particular, we reiterate our focus on the *research-and-development* workflow for medical time-series (in which flexibility and standardization go hand-in-hand), which contrasts with the downstream goal of *productionizing* mature architectures for deployment (in which a tradeoff exists, but which is firmly beyond the scope of our mission).\n\n[Reviewers 1, 2, and 3]:\n\n- **New Focus Box** (\u201cImplementations and Extensions\u201d): In Section 2 of the revised manuscript, a pointer is now included to a (new) Appendix C (\u201cChoice of Built-in Techniques\u201d), where we provide a discussion and justification of the initial choice of algorithms included in the current version of the framework. In addition, a pointer is now included to the (new) Appendix G (\u201cExtensibility: Example Wrapper Class\u201d, mentioned above), where we provide an example of how an external model can be easily integrated into the pipeline.\n\n---\n\n**(d.) Miscellaneous Clarifications**\n\n[Reviewer 1]:\n\n- **New Appendix I** (\u201cGlossary of Acronyms\u201d). We now provide a list of all potentially lesser-known acronyms. Where appropriate, we also provide the original citation to the paper in which the acronym is first defined.\n\n[Reviewers 1, 3]:\n\n- **Package Details**: We have now updated the cover page to include \u201cPython\u201d in front of the (placeholder) link to the repository. In the camera-ready version (where anonymity is no longer an issue), this shall be the actual link to the software repository and license. That way, the official \u201creadme\u201d and \u201crequirements.txt\u201d (containing information about package versions and dependencies) will be one click away. (Since Clairvoyance will remain under active development and community engagement, these details may need to be updated rapidly, so a direct link to the software repository is easier to maintain than a static Appendix entry).\n\n[Reviewer 4]:\n\n- **Grammar**: We have now revised the problematic sentence to resolve the grammatical issue.\n\n---\n\nWith our clarifications and revisions, we hope that we have addressed the reviewers' concerns. Thank you for your kind consideration.", "title": "Revised Paper (continued)"}, "rIRDVgsxucG": {"type": "rebuttal", "replyto": "mC5gsJsIW4o", "comment": "---\n\nThank you for your thoughtful comments and suggestions. We give answers to each in turn, as well as pointing out corresponding updates to the revised manuscript (additions are made in *blue*).\n\n---\n\n**(1) Background and References for Experiments**\n\n(1.1) Describing and Justifying Experiments:\n\nWe agree that---in describing the illustrative examples---the paper would benefit from a more thorough presentation of background information and justification.\n\nIn the revised manuscript, we have now included a (new) Appendix B: \u201cBackground References for Experiments\u201d, which tabulates exemplary references to original papers for the following details on each modeling question (e.g. \u201cclinical deterioration of ward patients\u201d), as well as---where appropriate---pointers to the relevant sections in those papers that we follow.\n\n- original works describing and justifying the modeling task,\n\n- original description/documentation for the dataset used, including\n\n- details on data collection, coverage, ethics, etc.,\n\n- the procedure for initial data cleaning/selection that we followed, as well as\n\n- theoretical background on the Clairvoyance pathway involved in the modeling question.\n\n(1.2) Details on Processing, Evaluation, etc:\n\nThank you for pointing out the benefit of additional clarity with respect to how our experiments are conducted---even more so since reproducibility and transparency are central to our thesis.\n\nTo this end, we now provide *fully worked examples* of Clairvoyance in actual usage, which directly demonstrates how an experiment is exactly run, as well as further highlighting the simplicity and standardization of coding with the pipeline abstraction.\n\n(a) In the (new) Appendix E in the revised manuscript, we now provide a worked example of using the *Clairvoyance pipeline* to train and use a model (for this, we use the predictions pathway). This takes the form of a detailed, step-by-step walk-through of the entire pipeline and its components, with fully-executable code, comments, and accompanying descriptions where appropriate.\n\n(b) In the (new) Appendix F in the revised manuscript, we now provide a worked example of using the *optimization interface* to perform stepwise model selection (for this, we use the treatment effects pathway for variety). This also takes the form of executable, commented code, organized as in a standard experiment---using a \u201cmain\u201d function wrapper with top-level arguments.\n\n(c) In the (new) Appendix G in the revised manuscript, we now provide an example of how a generic *wrapper class* should be written for the purpose of integrating an external model/algorithm that is not already implemented in the current version of Clairvoyance. Specifically, we show an example of how a classical time-series prediction model (ARIMA) can be easily integrated.\n\nIn the main text of the revised manuscript, pointers to all of these worked examples are now included in a dedicated \u201cfocus box\u201d at the end of Section 4 with its own subtitle: \u201cWorked Examples\u201d. Furthermore, this also includes references to the interactive tutorials (using Jupyter notebooks) and top-level API code with examples of pathways and optimizations, that are part of the software repository.\n\nPutting it all together, we believe that the combination of the following elements now adequately demonstrates the motivation, contribution, and ease of adoption of the Clairvoyance pipeline: Section 4 (high-level motivations and medically relevant use cases), Appendix B (background and original references), Appendix D (low-level experiment details), and Appendices E, F, G (fully-worked/executable examples).", "title": "Response to Reviewer #3 [Part 1/4]"}, "LII0mVi0wVs": {"type": "rebuttal", "replyto": "mC5gsJsIW4o", "comment": "---\n\n**(2) Release Details and Community Engagement**\n\n(2.1) Community Engagement:\n\nThank you for pointing out the importance of release details and encouraging user adoption. We completely agree that this an important step in the development cycle of any software framework.\n\nTo be clear, we do have a fully-developed agenda for *community engagement* in execution. While we are necessarily brief on the following details (due to the requirement for anonymity during the review process), this should give an adequate idea of outreach:\n\n- Our lab introduced this framework/initiative in May of this year in an official announcement, accompanying technical description, and publicly available source code.\n\n- In a keynote address at a workshop in a prominent ML conference in July, this framework was introduced, and participants were encouraged to download, try out, and give feedback on our software.\n\n- In August, a piece of long-form content was published by the team, highlighting the framework and calling for readers to visit the software repository.\n\nMoreover, from September to the present (and continuously going forward), we introduced the framework through a series of our lab\u2019s hour-long online \u201cengagement sessions\u201d for machine learning and healthcare researchers and students. Specifically:\n\n- In September, a general introduction to the pipeline abstraction was given, while participants were encouraged to visit our software repository and try the framework in their own projects.\n\n- In October, our engagement session featured a 10-minute section specifically dedicated to explaining the mission of the initiative, as well as the goals of the framework.\n\n- Most recently, in November our engagement session focused extensively on the framework--including 16 minutes of tutorials on how to use the software, and how to add new algorithms to the framework, as well as fielding numerous questions on the pipeline during the subsequent Q&A session.\n\nIn time, these initiatives will lead us towards more mature community engagement, whereupon we will be equipped to measure and report user interviews, feedback, and adoption patterns. Finally, note that in the camera-ready version, the link to the software repository and license will feature on the first page of the paper. (That said, note that a development version of the code is available in the supplementary material for perusal).\n\n(2.2) Release Details:\n\nWe have now updated the cover page to include a (placeholder) link to the repository. In the camera-ready version (where anonymity is no longer an issue), this shall be the actual link to the software repository and license. That way, the official \u201creadme\u201d and \u201crequirements.txt\u201d (containing information about package versions and dependencies) will be one click away. (Since Clairvoyance will remain under active development and community engagement, these details may need to be updated rapidly, so a direct link to the software repository is easier to maintain than a static Appendix entry).", "title": "Response to Reviewer #3 [Part 2/4]"}, "jpnCPZU_5-": {"type": "rebuttal", "replyto": "QAIL2GPu7tL", "comment": "---\n\nThank you for your thoughtful comments and suggestions.\n\n---\n\n**(1) Separation of Training and Testing Sets**\n\nWe completely agree that preprocessing, imputation, and feature selection should be based on the training data alone, such that there is no \u201cdata leakage\u201d going on. Actually, the pipeline is designed such that we *do* always adhere to follow this (very important) principle. Kindly allow us to clarify:\n\nIn each of these \u201cprocessing\u201d modules, there are two distinct methods provided (and similarly, any new user-created module would need to expose these two methods):\n\n- \"fit_transform\". This is applied to the training data. When this is called on the training data, the \u201cprocessing\u201d is *based on* the training data, and then *applied to* the training data.\n\n- \"transform\". This is applied to the testing data. When this is called on the testing data, the \u201cprocessing\u201d is still just the one based *solely* on the training data; we are just *applying* it to the testing data.\n\nIn this manner, no overfitting due to \u201cdata leakage\u201d can occur. Importantly, note that this \u201cfit_transform / transform\u201d paradigm that we adopt is exactly as is standard in practice (see e.g. sklearn\u2019s interface).\n\nFurthermore, in order to better illustrate this important principle in a step-by-step manner (and prevent inadvertent misunderstanding), we have now included in the revised manuscript a (new) Appendix E, which provides a fully-worked example of using the pipeline to train and predict with a model (for this, we use the predictions pathway). This takes the form of a detailed, step-by-step walk-through of the entire pipeline and its components, with fully-executable code, comments, and accompanying descriptions where appropriate.\n\n---\n\n**(2) Grammatical Error**\n\nThank you for pointing out the grammatical error---we agree, and have revised the sentence as follows:\n\n\u201c[...] and privacy and heterogeneity are important, [...]\u201d\n\n---\n\nWith our clarifications and revisions, we hope that we have addressed your concerns. Thank you for your kind consideration.", "title": "Response to Reviewer #4"}, "R3Usw-zyE8": {"type": "rebuttal", "replyto": "RvLeelbIlfT", "comment": "---\n\nThank you for your thoughtful comments and suggestions. We give answers to each in turn, as well as pointing out corresponding updates to the revised manuscript (additions are made in *blue*). Sections (A) and (B) below are aimed at the \u201crationale for score\u201d, and Section (C) is aimed at the miscellaneous questions.\n\n---\n\n**(A) Software Platform and ICLR Venue**\n\nThank you for bringing up the question of venue. We would like to take care in pointing out the following, in order to gently clarify the appropriateness of the submission for ICLR and our intended audience:\n\n(a) Software platforms are indeed a core part of ML conferences (main conference), although usually not as conspicuous or numerous as theory-/methods-based papers.\n\n- ICLR: The \u201cCall for Papers\u201d at ICLR directly seeks submissions for \u201cSoftware Platforms\u201d and \u201cImplementation Issues\u201d (this is found under the \u201cList of Relevant Topics\u201d, and has been part of this list for every year since the very first ICLR conference in 2013).\n\n- NeurIPS and ICML: Likewise for other similar conferences, where the \u201cCall for Papers\u201d directly seeks submissions for \u201cImplementations and Software\u201d and \u201cSoftware Toolkits\u201d (Item #3) at NeurIPS and \u201cApplications\u201d (Item #7) for ICML (also since the past).\n\n(b) While not as prominent, there have been many such contributions. For a random selection of similar initiatives published at ML conferences (main conference track) in the past two years, we refer to [99--101] for ICLR, [102--104] for ICML, and [105--107] for NeurIPS. Note that although the specific ML subfields are different from ours, these all similarly propose high-level API software packages and/or standardized environments to enable rapid prototyping and ease of development, as well as to encourage encourage systematic and reproducible validation. Crucially, while some of these are highly specialized (e.g. molecular physics in [105], cross-lingual transfer in [103], etc.), their focus---like ours---is on what the *ML-side* of the cross-disciplinary community can do to further the initiative.\n\n(c) Finally, at risk of repetition, we reiterate that Clairvoyance is *not* designed as a preset collection of algorithms to be repeatedly applied by end-users. Indeed, purely implementation-focused packages such as [73--79] may not find their place in this venue, versus a more specialized journal. But quite to the contrary, Clairvoyance is a *framework* developed as part of our mission to tackle the paramount issues of reproducibility, standardization, and transparency in (cross-disciplinary, collaborative) ML research. This paper is therefore part software description, and part \u201ccall-to-arms\u201d for the *ML/DL research community* itself to strive for these ideals in the context of medical time series, much as [103] and [105] do for theirs, for instance. In order to reach the intended audience, we therefore firmly believe that a venue such as ICLR is much more appropriate than a medical journal.", "title": "Response to Reviewer #1 [Part 1/5]"}, "10PnH69B1xQ": {"type": "rebuttal", "replyto": "RvLeelbIlfT", "comment": "---\n\n**(B) Elaboration and Explanation**\n\nThank you for pointing out the importance of balancing between \u201cadvertising\u201d and more thorough \u201celaboration and explanation\u201d in the paper. We agree that the paper's exposition would benefit from more *concrete* details. To this end, in the revised manuscript we have made a number of important additions to provide a complete picture of our mission, contribution, and actual usage of the platform. \n\n(B.1) Fully-Worked Code Examples:\n\nFurther speaking to our ideals of reproducibility and transparency, we now provide *fully worked examples* of Clairvoyance in actual usage, which directly demonstrates how an experiment is exactly run, as well as further highlighting the simplicity and standardization of coding with the pipeline abstraction.\n\n(a) In the (new) Appendix E in the revised manuscript, we now provide a worked example of using the *Clairvoyance pipeline* to train and use a model (for this, we use the predictions pathway). This takes the form of a detailed, step-by-step walk-through of the entire pipeline and its components, with fully-executable code, comments, and accompanying descriptions where appropriate.\n\n(b) In the (new) Appendix F in the revised manuscript, we now provide a worked example of using the *optimization interface* to perform stepwise model selection (for this, we use the treatment effects pathway for variety). This also takes the form of executable, commented code, organized as in a standard experiment---using a \u201cmain\u201d function wrapper with top-level arguments.\n\n(c) In the (new) Appendix G in the revised manuscript, we now provide an example of how a generic *wrapper class* should be written for the purpose of integrating an external model/algorithm that is not already implemented in the current version of Clairvoyance. Specifically, we show an example of how a classical time-series prediction model (ARIMA) can be easily integrated.\n\nIn the main text of the revised manuscript, pointers to all of these worked examples are now included in a dedicated \u201cfocus box\u201d at the end of Section 4 with its own subtitle: \u201cWorked Examples\u201d. Furthermore, this also includes references to the interactive tutorials (using Jupyter notebooks) and top-level API code with examples of pathways and optimizations, that are part of the software repository.\n\n(B.2) Background on Illustrative Examples:\n\nIn the revised manuscript, we have now included a (new) Appendix B: \u201cBackground References for Experiments\u201d, which tabulates exemplary references to original papers for the following details on each modeling question (e.g. \u201cclinical deterioration of ward patients\u201d), as well as---where appropriate---pointers to the relevant sections in those papers that we follow.\n\n- original works describing and justifying the modeling task,\n\n- original description/documentation for the dataset used, including\n\n- details on data collection, coverage, ethics, etc.,\n\n- the procedure for initial data cleaning/selection that we followed, as well as\n\n- theoretical background on the Clairvoyance pathway involved in the modeling question.\n\nPutting it all together, we believe that the combination of the following elements now adequately demonstrates the motivation, contribution, and ease of adoption of the Clairvoyance pipeline: Section 4 (high-level motivations and medically relevant use cases), Appendix B (background and original references), Appendix D (low-level experiment details), and Appendices E, F, G (fully-worked/executable examples).", "title": "Response to Reviewer #1 [Part 2/5]"}, "Au_rk4aSJBp": {"type": "rebuttal", "replyto": "mC5gsJsIW4o", "comment": "---\n\n**(3) Software Platform and ICLR Venue**\n\nThank you for bringing up the question of venue. We would like to take care in pointing out the following, in order to gently clarify the appropriateness of the submission for ICLR and our intended audience:\n\n(a) Software platforms are indeed a core part of ML conferences (main conference), although usually not as conspicuous or numerous as theory-/methods-based papers.\n\n- ICLR: The \u201cCall for Papers\u201d at ICLR directly seeks submissions for \u201cSoftware Platforms\u201d and \u201cImplementation Issues\u201d (this is found under the \u201cList of Relevant Topics\u201d, and has been part of this list for every year since the very first ICLR conference in 2013).\n\n- NeurIPS and ICML: Likewise for other similar conferences, where the \u201cCall for Papers\u201d directly seeks submissions for \u201cImplementations and Software\u201d and \u201cSoftware Toolkits\u201d (Item #3) at NeurIPS and \u201cApplications\u201d (Item #7) for ICML (also since the past).\n\n(b) While not as prominent, there have been many such contributions. For a random selection of similar initiatives published at ML conferences (main conference track) in the past two years, we refer to [99--101] for ICLR, [102--104] for ICML, and [105--107] for NeurIPS. Note that although the specific ML subfields are different from ours, these all similarly propose high-level API software packages and/or standardized environments to enable rapid prototyping and ease of development, as well as to encourage encourage systematic and reproducible validation. Crucially, while some of these are highly specialized (e.g. molecular physics in [105], cross-lingual transfer in [103], etc.), their focus---like ours---is on what the *ML-side* of the cross-disciplinary community can do to further the initiative.\n\n(c) Finally, at risk of repetition, we reiterate that Clairvoyance is *not* designed as a preset collection of algorithms to be repeatedly applied by end-users. Indeed, purely implementation-focused packages such as [73--79] may not find their place in this venue, versus a more specialized journal. But quite to the contrary, Clairvoyance is a *framework* developed as part of our mission to tackle the paramount issues of reproducibility, standardization, and transparency in (cross-disciplinary, collaborative) ML research. This paper is therefore part software description, and part \u201ccall-to-arms\u201d for the *ML/DL research community* itself to strive for these ideals in the context of medical time series, much as [103] and [105] do for theirs, for instance. In order to reach the intended audience, we therefore firmly believe that a venue such as ICLR is much more appropriate than a medical journal.\n\n---\n\nWith our clarifications and revisions, we hope that we have addressed your concerns. Thank you for your kind consideration.", "title": "Response to Reviewer #3 [Part 3/4]"}, "zBVVpWWs4w": {"type": "rebuttal", "replyto": "mC5gsJsIW4o", "comment": "---\n\n**References** (numbering as continued from revised manuscript)\n\n[99] ICLR 2020: Novak et al., \"Neural Tangents: Fast and Easy Infinite Neural Networks in Python\".\n\n[100] ICLR 2020: Osband et al., \"BSuite: Behavior Suite for Reinforcement Learning\".\n\n[101] ICLR 2019: Schneider et al., \"DeepOBS: A Deep Learning Optimizer Benchmark Suite\".\n\n[102] ICML 2020: Goyal et al., \"PackIt: A Virtual Environment for Geometric Planning\".\n\n[103] ICML 2020: Hu et al., \"EXTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization\".\n\n[104] ICML 2019: Bansal et al., \"HOList: An Environment for Machine Learning of Higher-Order Theorem Proving\".\n\n[105] NeurIPS 2020: Schoenholz et al., \"JAX, M.D.: A Framework for Differentiable Physics\".\n\n[106] NeurIPS 2019: Tran et al., \"Bayesian Layers: A Module for Neural Network Uncertainty\".\n\n[107] NeurIPS 2019: Park et al., \"Park: An Open Platform for Learning-Augmented Computer Systems\".", "title": "Response to Reviewer #3 [Part 4/4]"}, "GTCw7Fw0jpY": {"type": "rebuttal", "replyto": "RvLeelbIlfT", "comment": "---\n\n**(C) Miscellaneous Questions**\n\n(C.1) Evaluations against existing packages:\n\nKindly allow us to reiterate that the proposed framework is the first of its kind, and there are no other frameworks that provide an end-to-end, unified structure to the *entire* time-series workflow (see Section 4: \u201cRelated Work\u201d, and Table 1: \u201cClairvoyance and Comparable Software\u201d). While there is existing software providing *implementations* of individual models and optimization algorithms [73--79], there is no comparable *framework-level* software that makes sense to \u201cjuxtapose\u201d with (especially in the medical setting)---in fact, this is precisely what motivated developing/disseminating Clairvoyance in the first place.\n\nMoreover, it goes without saying that implementation-based packages such as [73--79] exist in *complement* to the Clairvoyance pipeline (rather than in competition)---especially as the worked example in Appendix G now shows. Likewise, this is also true of existing research for model selection: For instance, as the worked example in Appendix F now shows, it is precisely the pipeline abstraction and proposed optimization interface that allows us to \u201cplug in\u201d the existing technique of deep kernel learning (DKL) in tackling the stepwise model selection (SMS) problem.\n\n(C.2) Fully worked example with code:\n\nWe agree that worked examples do greatly benefit clarity of exposition, and we have indeed now included a number of such coded examples in the revised Appendices. Please refer our much more detailed response (B.1) above: We now include a complete worked example of using the pipeline (Appendix E), an example of using the stepwise optimization interface (Appendix F), and example of how to extend component modules to include a new technique (Appendix G). In addition, please also refer to our response (B.2) above: We now tabulate more background and references to original studies that provide more concrete information about the medical and theoretical relevance (i.e. as ML modeling problems) of our illustrative examples.\n\n(C.3) Basic software details:\n\nIndeed, Clairvoyance is a *python* package. We completely agree that this should be stated up front. We have now updated the cover page to include \u201cPython\u201d in front of the (placeholder) link to the repository.\n\nWe also agree that versioning and dependencies should be more accessible. In the camera-ready version (where anonymity is no longer an issue), we shall include the link to the software repository and license on the very first page. That way, the official \u201creadme\u201d and \u201crequirements.txt\u201d (containing information about package versions and dependencies) will be one click away. (Since Clairvoyance will remain under active development and community engagement, these details may need to be updated rapidly, so a direct link to the software repository is easier to maintain than a static Appendix entry. See also point (C.5) below).\n\nFinally, allow us to clarify that as a *high-level* API framework, support for GPU acceleration simply depends on the underlying implementation details of the component modules being used. For the deep learning models that are already built into the initial version of Clairvoyance, this is certainly supported. (However, if an external model is integrated via a wrapper class, for instance, then this depends entirely on how that external model is implemented).\n\n(C.4) Definitions of acronyms:\n\nThank you for pointing out that some acronyms are not properly defined. We agree that---for better accessibility---these should be identifiable within the paper itself.\n\nFor optimal referencing, in a (new) Appendix I in the revised manuscript, we now provide a list of all potentially lesser-known acronyms. Where appropriate, we also provide the original citation to the paper in which the acronym is first defined. For instance, for the two examples mentioned:\n\nCRN: Counterfactual recurrent network, first defined in [31].\n\nR-MSN: Recurrent marginal structural network, first defined in [30].\n\netc.", "title": "Response to Reviewer #1 [Part 3/5]"}, "hUCBGgd3vys": {"type": "rebuttal", "replyto": "RvLeelbIlfT", "comment": "---\n\n(C.5) Completeness of development:\n\nAs far as the overall *structure* of the pipeline as proposed and presented in the paper, development is fairly complete (save internal refactoring or slight changes in naming conventions, etc).\n\nAs for the *scope* of the project (i.e. extending components to incorporate newer/more powerful individual techniques, or even adding additional components based on community feedback), we hope to remain in active development and community engagement on an ongoing basis into the future.\n\nIn this latter sense, as an open project, development is ideally never \u201ccomplete\u201d. In fact, we do have a fully-developed agenda for community engagement and feedback in execution. While we must necessarily be brief on the following details (due to the requirement for anonymity during the review process), this should give an adequate idea of outreach:\n\n- Our lab introduced this framework/initiative in May of this year in an official announcement, accompanying technical description, and publicly available source code.\n\n- In a keynote address at a workshop in a prominent ML conference in July, this framework was introduced, and participants were encouraged to download, try out, and give feedback on our software.\n\n- In August, a piece of long-form content was published by the team, highlighting the framework and calling for readers to visit the software repository.\n\nMoreover, from September to the present (and continuously going forward), we introduced the framework through a series of our lab\u2019s hour-long online \u201cengagement sessions\u201d for machine learning and healthcare researchers and students. Specifically:\n\n- In September, a general introduction to the pipeline abstraction was given, while participants were encouraged to visit our software repository and try the framework in their own projects.\n\n- In October, our engagement session featured a 10-minute section specifically dedicated to explaining the mission of the initiative, as well as the goals of the framework.\n\n- Most recently, in November our engagement session focused extensively on the framework--including 16 minutes of tutorials on how to use the software, and how to add new algorithms to the framework, as well as fielding numerous questions on the pipeline during the subsequent Q&A session.\n\nIn time, these initiatives will lead us towards more mature community-driven development, whereupon we will be equipped to measure and report user interviews, feedback, and adoption patterns.\n\n---\n\nWith our clarifications and revisions, we hope that we have addressed your concerns. Thank you for your kind consideration.", "title": "Response to Reviewer #1 [Part 4/5]"}, "mo14K5ls5k": {"type": "rebuttal", "replyto": "RvLeelbIlfT", "comment": "---\n\n**References** (numbering as continued from revised manuscript)\n\n[99] ICLR 2020: Novak et al., \"Neural Tangents: Fast and Easy Infinite Neural Networks in Python\".\n\n[100] ICLR 2020: Osband et al., \"BSuite: Behavior Suite for Reinforcement Learning\".\n\n[101] ICLR 2019: Schneider et al., \"DeepOBS: A Deep Learning Optimizer Benchmark Suite\".\n\n[102] ICML 2020: Goyal et al., \"PackIt: A Virtual Environment for Geometric Planning\".\n\n[103] ICML 2020: Hu et al., \"EXTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization\".\n\n[104] ICML 2019: Bansal et al., \"HOList: An Environment for Machine Learning of Higher-Order Theorem Proving\".\n\n[105] NeurIPS 2020: Schoenholz et al., \"JAX, M.D.: A Framework for Differentiable Physics\".\n\n[106] NeurIPS 2019: Tran et al., \"Bayesian Layers: A Module for Neural Network Uncertainty\".\n\n[107] NeurIPS 2019: Park et al., \"Park: An Open Platform for Learning-Augmented Computer Systems\".", "title": "Response to Reviewer #1 [Part 5/5]"}, "mEzB2nPGJfp": {"type": "rebuttal", "replyto": "EmxX_YVHkCJ", "comment": "---\n\nThank you for your thoughtful comments and suggestions. We give answers to each in turn, as well as pointing out corresponding updates to the revised manuscript (additions are made in *blue*).\n\n---\n\n**(1) Pipeline Design and Tradeoffs**\n\n(1.1) Considerations in Design of API:\n\nWe agree that---in presenting a software framework---the paper would benefit from an explicit description of the considerations driving the API\u2019s design. Our design philosophy is based on the authors\u2019 experience in prototyping and developing real-world collaborative research projects in clinical time-series settings:\n\n(a) Pipeline First, Models Second: Our first emphasis is on *reproducibility*: The process of engineering and evaluating complete medical time-series workflows needs to be clear and transparent. Concretely, this manifests in the strict \u201cseparation of concerns\u201d enforced by the high-level API of each component module along the pipeline (see e.g. Figure 3 for illustration). With the ProblemMaker and PipelineComposer as first-class objects, the central abstraction here is the *pipeline* itself; the intricacies and configurations of individual models (e.g. a specific deep learning temporal imputation method) are limited to within each component module.\n\n(b) Be Minimal and Unintrusive: Our second emphasis is on *standardization*: While workflow development needs to be unified and systematic, learning to use the framework should be intuitive. Concretely, this manifests in the API\u2019s adherence to the existing and popular \u201cfit-transform-predict\u201d paradigm (see e.g. sklearn) in all component modules---both *along* the pipeline steps, as well as *across* the pathways that define the patient\u2019s healthcare lifecycle. This enables easy adoption and rapid prototyping---qualities that are paramount given the degree of collaborative research and cross-disciplinary code-sharing required in healthcare-related research.\n\n(c) Encourage Extension: Our third emphasis is on *extensibility*: Given that novel methods are proposed in the ML community every day, the pipeline components should be easily extensible to incorporate new algorithms. Concretely, this manifests in the *encapsulated* design for models within each component module: Specifically, in order to integrate a new component method (e.g. from another researcher\u2019s code, or from an external package) into the framework, all that is required is a simple wrapper class that implements the \u201cfit\u201d, \u201cpredict\u201d, and \u201cget_hyperparameter_space\u201d methods; likewise, for an optimization agent, all that is required is to expose an \u201coptimize\u201d method.\n\nIn the revised manuscript, these points pertaining to design philosophy are now explained in a dedicated \u201cfocus box\u201d under Section 2 with its own subtitle: \u201cKey Design Principles\u201d.", "title": "Response to Reviewer #2 [Part 1/5]"}, "OHANJpZrB36": {"type": "rebuttal", "replyto": "EmxX_YVHkCJ", "comment": "---\n\n(1.2) Tradeoff between Flexibility and Standardization:\n\nThank you for pointing out the importance of considering this tradeoff. We agree that contextualizing our mission along this tradeoff would better clarify our motivation and contribution.\n\nFirst, we note that the modularity of the pipeline no doubt enforces a specific form of \u201cstandardization\u201d of workflows. However, there are two possible senses of \u201cflexibility\u201d: (a) in the freedom to customize pipelines within the proposed abstraction, and (b) in the generality of the pipeline abstraction itself. Briefly, with respect to (a), it is easy to see that our design emphasis on encapsulation and extensibility means that pipelines are flexibly customized as required---both by mixing and matching component models as needed, as well as by easily incorporating novel methods through minimal wrapper classes. In this sense, then, Clairvoyance seeks to maximize standardization and flexibility *in parallel*: Unless there is a compelling reason for entangling two or more steps in the pipeline, there is little disadvantage to adopting the Clairvoyance workflow instead of starting fresh.\n\nAs for (b), first allow us to clarify that our central thesis revolves around bettering the *research-and-development* workflow for medical time-series. At slight risk of generalization, this---importantly---contrasts with the downstream goal of *productionizing* mature architectures for deployment (which is firmly beyond the scope of our mission). In the former, our focus on rapid prototyping and reproducible experimentation means that \u201cstandardization\u201d is paramount, and the proposed pipeline abstraction is firmly positioned as such. In the latter, of course, this second notion of \u201cflexibility\u201d becomes much more important to balance against: Various application-/preference-specific considerations arise from the practical needs of deployment and integration, such as handling data streams, patient privacy, algorithmic fairness, or compliance with government regulations. In this sense, a development team looking to address such concerns would need to more carefully weigh the *tradeoff* between flexibility and standardization.\n\nIn the revised manuscript, this explanation of the tradeoff is now included in a dedicated \u201cfocus box\u201d under Section 2 with its own subtitle: \u201cFlexibility vs. Standardization\u201d.\n\n(1.3) Choice of Built-in Techniques:\n\nWe agree a discussion of our choice of built-in techniques would better position the focus of our work.\n\nBy way of preface, allow us to reiterate our \u201cPipeline First\u201d focus (see point 1.1(a) above), especially in the context of medical settings: Rather than (re-)implementing every time-series model in existence, our primary contribution is in unifying all three key pathways in a patient\u2019s healthcare lifecycle (i.e. predictions, treatments, and monitoring tasks; see Section 2: \u201cThe Patient Journey\u201d) through a single end-to-end pipeline abstraction---for which Clairvoyance is the first (see Table 1: \u201cClairvoyance and Comparable Software\u201d).\n\nFor the predictions pathway, while there is a virtually infinite variety of time-series models in the wild, we choose to include standard and popular classes of *deep learning* models, given their ability to handle large amounts and dimensions of data, as well as the explosion of their usage in medical time-series studies (see e.g. virtually any of the paper references in Section 1). For both the treatment effects and active sensing pathways, there is much less existing work available; for these, we provide state-of-the-art models (e.g. CRN, R-MSN, ASAC, DeepSensing) implemented exactly as given in their original research papers.\n\nWith that said, as noted throughout, recall that all component modules (including the various other pipeline components) are easily *extensible*: For instance, if more traditional time-series baselines from classical literature were desired for comparison purposes, existing algorithms from [73--79] can be integrated into Clairvoyance by using simple wrapper classes, with little hassle (for an explicit demonstration of this, see also our response to point (2.1) below for reference to the newly included example in Appendix G).\n\nIn the revised manuscript, this discussion on built-in implementations is now included in a (newly added) Appendix C. In addition, in the main text, pointers to Appendix C and Appendix G are now included in a small box (due to space constraints) at the end of Section 2 with subtitle: \u201cImplementations and Extensions\u201d.", "title": "Response to Reviewer #2 [Part 2/5]"}, "PUWwJHc3f_n": {"type": "rebuttal", "replyto": "EmxX_YVHkCJ", "comment": "---\n\n**(2) Benefits of Using Clairvoyance**\n\n(2.1) Time to Setup:\n\nFurther to the quantitative results in Section 4, we agree that additional indication regarding the *ease-of-use* of Clairvoyance would better clarify its advantage---over starting from scratch.\n\nNow, it is difficult (if not impossible) to accurately quantify any notion of \u201ctime savings\u201d involved in adopting Clairvoyance for use during research and development of any medical time-series project. However, we do believe that providing *fully worked examples* of Clairvoyance in actual usage shall more than adequately demonstrate the significant benefits of simplicity and standardization in any actual coding work required.\n\n(a) In the (new) Appendix E in the revised manuscript, we now provide a worked example of using the *Clairvoyance pipeline* to train and use a model (for this, we use the predictions pathway). This takes the form of a detailed, step-by-step walk-through of the entire pipeline and its components, with fully-executable code, comments, and accompanying descriptions where appropriate.\n\n(b) In the (new) Appendix F in the revised manuscript, we now provide a worked example of using the *optimization interface* to perform stepwise model selection (for this, we use the treatment effects pathway for variety). This also takes the form of executable, commented code, organized as in a standard experiment---using a \u201cmain\u201d function wrapper with top-level arguments.\n\n(c) In the (new) Appendix G in the revised manuscript, we now provide an example of how a generic *wrapper class* should be written for the purpose of integrating an external model/algorithm that is not already implemented in the current version of Clairvoyance. Specifically, we show an example of how a classical time-series prediction model (ARIMA) can be easily integrated.\n\nIn the main text of the revised manuscript, pointers to all of these worked examples are now included in a dedicated \u201cfocus box\u201d at the end of Section 4 with its own subtitle: \u201cWorked Examples\u201d. Furthermore, this also includes references to the interactive tutorials (using Jupyter notebooks) and top-level API code with examples of pathways and optimizations, that are part of the software repository.\n\nFrom these detailed examples, the benefits with respect to easy adoption, rapid prototyping, and extensibility in incorporating new techniques should become apparent. Note that these worked examples are also relevant in support of points 1.1(a,b,c) above).\n\nAs a final clarification, allow us to reiterate that the proposed framework is the first of its kind, and there are no other frameworks that provide an end-to-end, unified structure to the *entire* time-series workflow (see Section 4: \u201cRelated Work\u201d, and Table 1: \u201cClairvoyance and Comparable Software\u201d). While there is existing software providing *implementations* of individual models and optimization algorithms, there is no comparable *framework-level* software that makes sense to \u201cjuxtapose\u201d with (especially in the medical setting)---in fact, this is precisely what motivated developing/disseminating Clairvoyance in the first place.\n\n(2.2) Time to Train:\n\nAt risk of belaboring the point, Clairvoyance is a pipeline abstraction (i.e. framework), not a specific algorithm (i.e. implementation). Therefore the \u201ctime to train\u201d depends entirely on (a) which individual techniques are ultimately selected by the user, and (b) the characteristics of the dataset being used. This is also true of the optimization interface; i.e. the time to train depends on the specific optimizer selected. Moreover, all of this depends on (c) the hardware on which the computation is performed.\n\nImportantly, however, we can certainly provide an upper bound on the \u201ctypical\u201d time to train as pertains the types of datasets we used (see characteristics and statistics in Table 2) in our experiments, including those performing Bayesian optimization for stepwise model selection on top of the pipeline. Our computations were performed with a single NVIDIA GeForce GTX 1080 Ti GPU, and each experiment took approximately ~24--72 hours. Of course, this duration may be shortened through the use of multiple GPUs in parallel.\n\nIn the revised manuscript, this brief note on \u201cTime to Train\u201d is now included at the end of the (new) Appendix C.", "title": "Response to Reviewer #2 [Part 3/5]"}, "oUGNcScR2n-": {"type": "rebuttal", "replyto": "EmxX_YVHkCJ", "comment": "---\n\n**(3) User Adoption and Feedback**\n\nThank you for pointing out the value in measuring engagement and feedback from the community. We completely agree that this an important step in the development cycle of any software framework.\n\nFirst, however, we would like to point out that the development cycle (as pertains presentation/writing) generally proceeds in two broad stages. (a) In the first instance, the newly developed platform/framework is presented in a \u201cproposal\u201d paper that is part software description and part call-to-arms for the overarching goal. (b) Then, after development matures and community feedback is incorporated over a period of time, a \u201ccapstone\u201d paper presents the final design philosophy and description of user engagement/adoption. In this present paper, we are striving to accomplish \u201cStage (a)\u201d as part of our effort to disseminate and encourage community participation.\n\nFor a random selection of similar \u201cStage (a)\u201d efforts in the machine learning (main conference) community in the past two years, we refer to [99--101] for ICLR, [102--104] for ICML, and [105--107] for NeurIPS. Note that although the specific ML subfields are different from ours, these all similarly propose high-level API software packages and/or standardized environments to enable rapid prototyping and ease of development, as well as to encourage encourage systematic and reproducible validation. (For a classic example of this \u201cproposal + capstone\u201d pattern of development and presentation, see [108--109]; note that the study on user adoption is reported in the capstone paper).\n\nSecond, we do have a fully-developed agenda for *community engagement* in execution, which would lead us towards \u201cStage (b)\u201d in time. While we are necessarily brief on the following details (due to the requirement for anonymity during the review process), this should give an adequate idea of outreach:\n\n- Our lab introduced this framework/initiative in May of this year in an official announcement, accompanying technical description, and publicly available source code.\n\n- In a keynote address at a workshop in a prominent ML conference in July, this framework was introduced, and participants were encouraged to download, try out, and give feedback on our software.\n\n- In August, a piece of long-form content was published by the team, highlighting the framework and calling for readers to visit the software repository.\n\nMoreover, from September to the present (and continuously going forward), we introduced the framework through a series of our lab\u2019s hour-long online \u201cengagement sessions\u201d for machine learning and healthcare researchers and students. Specifically:\n\n- In September, a general introduction to the pipeline abstraction was given, while participants were encouraged to visit our software repository and try the framework in their own projects.\n\n- In October, our engagement session featured a 10-minute section specifically dedicated to explaining the mission of the initiative, as well as the goals of the framework.\n\n- Most recently, in November our engagement session focused extensively on the framework--including 16 minutes of tutorials on how to use the software, and how to add new algorithms to the framework, as well as fielding numerous questions on the pipeline during the subsequent Q&A session.\n\nIn time, these initiatives will lead us towards more mature community engagement, whereupon we will be equipped to measure and report user interviews, feedback, and adoption patterns (i.e. \u201cStage (b)\u201d). Finally, we note that this project was precisely born out of collaborations between a team of ML researchers and medical professionals: With university clinicians as developers/authors, our central design goal is always to provide for easy adoption in realistic usage scenarios.\n\n---\n\nWith our clarifications and revisions, we hope that we have addressed your concerns. Thank you for your kind consideration.", "title": "Response to Reviewer #2 [Part 4/5]"}, "8R1Nxs7sg-1": {"type": "rebuttal", "replyto": "EmxX_YVHkCJ", "comment": "---\n\n**References** (numbering as continued from revised manuscript)\n\n[99] ICLR 2020: Novak et al., \"Neural Tangents: Fast and Easy Infinite Neural Networks in Python\".\n\n[100] ICLR 2020: Osband et al., \"BSuite: Behavior Suite for Reinforcement Learning\".\n\n[101] ICLR 2019: Schneider et al., \"DeepOBS: A Deep Learning Optimizer Benchmark Suite\".\n\n[102] ICML 2020: Goyal et al., \"PackIt: A Virtual Environment for Geometric Planning\".\n\n[103] ICML 2020: Hu et al., \"EXTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization\".\n\n[104] ICML 2019: Bansal et al., \"HOList: An Environment for Machine Learning of Higher-Order Theorem Proving\".\n\n[105] NeurIPS 2020: Schoenholz et al., \"JAX, M.D.: A Framework for Differentiable Physics\".\n\n[106] NeurIPS 2019: Tran et al., \"Bayesian Layers: A Module for Neural Network Uncertainty\".\n\n[107] NeurIPS 2019: Park et al., \"Park: An Open Platform for Learning-Augmented Computer Systems\".\n\n[108] NeurIPS 2019: Paszke et al., \"PyTorch: An Imperative Style, High-Performance Deep Learning Library\".\n\n[109] NeurIPS 2017: Paszke et al., \"Automatic differentiation in PyTorch\".", "title": "Response to Reviewer #2 [Part 5/5]"}, "QAIL2GPu7tL": {"type": "review", "replyto": "xnC8YwKUE3k", "review": "Very interesting and timely project. \n\nMajor concern:\nFor a well-planned model development with large enough dataset, it is recommended to separate the case and controls from the very beginning and apply the pre-processing and imputation on the training dataset only. Feature selection should also be based on the training data alone, which is not the case in this pipeline from my understanding. The way, the pipeline is described, the processing, including imputation and feature selection are performed on the full dataset, which is then passed to the modeling phase. During the modeling phase, where model training and validation and testing will be performed.  \n\nThis can be cause of over-fitting since the testing dataset was to some level \"seen\" before the model testing. This has to be stated in the limitation of the study design. \n\nHaving a team science approach with clinician scientists as part of the team is integral part of the study, which seems that this paper is all about.  \n\n\n\nMinor:\nSome grammatical /stylistic error. ex: \u201cWhile issues such as data cleaning, algorithmic fairness, and privacy and heterogeneity have import, they are beyond the scope of our software.\u201d \uf0e0 revise the sentence \u201chave import\u201d.\n\n\nFinally, I am not a software engineer and will leave that level of evaluation to my colleagues.\n", "title": "Timely project for healthcare application - some concern about the study design", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "RvLeelbIlfT": {"type": "review", "replyto": "xnC8YwKUE3k", "review": "##########################################################################\n\nSummary:\n\nThe authors present a new package aimed at improving the design and validation of pipelines using medical time series data. The pipeline covers many aspects of time series pipelines including pre-processing, prediction, treatment effect estimation, calibration, etc. The package, as depicted in the paper, appears to be very comprehensive and well motivated.\n\n##########################################################################\n\nRationale for score: \n\nThe primary reason for my recommendation of reject is this paper is ill-suited for a venue such as ICLR. The paper is a descriptive paper for a new software package and it is aimed at the healthcare/informatics sub-community. While I appreciate the considerable effort the authors have clearly put into this software package, I think the paper would have a better chance of reaching it's intended audience in a more focused venue such as the Journal of the American Medical Informatics Association (JAMIA), the Machine Learning for Healthcare Conference (MLHC), or the Journal of Statistical Software (JSS). I also believe that the ICLR format does a disservice to the authors, as 8 pages is not enough room to fully elaborate on their work and the current version is very compressed which makes for difficult reading. Given the space constraints, the paper feels more like an advertisement for the package vs. an elaboration and explanation document. I think there is an impressive amount of work on display here, but I think that ultimately a paper such as this would be better served in a different venue. \n \n##########################################################################\n\nPros: \n\n1. The pipeline, as it is proposed in the paper is quite impressive. Moreover, the authors do an excellent job at motivating the need for such a pipeline not only as a tool but as a means of standardization and benchmarking, something that is sorely needed in many healthcare applications of machine learning. \n\n2. The figures and table 1 do a good job at summarizing the proposed approach and existing alternatives.\n\n##########################################################################\n\nCons: \n\n1. There are no empirical evaluations against alternative methods in this paper. At minimum there should be some kind of head to head evaluations against the existing packages.\n \n2. I found the vignettes too condensed to be helpful and it's unclear how the proposed pipeline was used to produce the results in the tables. The authors do a good job at setting up the clinical problem, but (likely due to space constraints) it is unclear how the problem reduces to a series of pipeline steps. Having one fully worked example with code (even in the appendix) would greatly help to understand how the proposed pipeline works.\n\n3. Several basic details are missing from the paper. For example, is this a python package? From the code snippets, I assume it is but this is never stated in the paper. If it is a python package, what versions of python is it compatible with, what are the dependencies, is GPU acceleration supported, etc? I was able to find some of this by digging through the included code but these details should be included in the paper.\n\n4. Many acronyms are used but never defined in the text, e.g. CRN and R-MSN from the examples. If, as the authors claim, they would like their package to be used by clinicians and ML practitioners alike, the should define these acronyms in the text to aid the reader. \n\n5. Are all of these modules complete or are some still in the alpha phase of development?", "title": "This paper provides a new software package to standardize many common tasks associated with clinical time series data, along with some illustrative examples", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "mC5gsJsIW4o": {"type": "review", "replyto": "xnC8YwKUE3k", "review": "The manuscript introduces and illustrates an end-to-end software pipeline, called Clairvoyance, for medical machine learning on time-series data. The authors must be congratulated for having designed and developed this wonderful resource to accelerate the adoption of these computational techniques in clinical practice as a way to support people\u2019s judgement and decision-making. The manuscript excels in describing and relating its contributions with related work. It has also included a convincing set of experimentation on datasets from three medical environments that are supplementary to each other. My only concerns with this paper are (i) describing and justifying these experiments, their materials (also research ethics), and their processing, evaluation, and statistical significance testing methods to an extent that allows the reader to comprehend these original studies that are now a part of the pipeline release paper (perhaps an appendix or references to separate original studies would do), (ii) release details of the pipeline seem to be missing (e.g., where to get the code, what is the licence of the release, and how are the authors facilitating people adopting the toolkit), and (iii) I am uncertain if ICLR is the right venue for the manuscript to obtain envisioned impacts of the conclusion section of the paper \u2014 I would have seen this contribution published in a medical journal instead, and the lacking details related to the item (ii) before make it even harder to assess this paper. However, the submission is excellent, and the program committee should discuss this case further.", "title": "Review of the submission called Clairvoyance: A Pipeline Toolkit for Medical Time Series", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "EmxX_YVHkCJ": {"type": "review", "replyto": "xnC8YwKUE3k", "review": "In this paper, the authors showcase a pipeline intended to standardize and industrialize AI model development and testing for medical time series.\n\nIt is very clear that the authors have put in a tremendous amount of work in building their pipeline. As someone who works on ML for healthcare, I appreciate it and look forward to using such pipelines.\n\nAs far as the *paper* is concerned, however, I\u2019m not sure if the paper (as it is written) and the venue (ICLR) are a good fit. For papers describing such frameworks, I\u2019d prefer to read about:\n1. pipeline design and tradeoffs: why did the team make the decisions they did . For e.g. why did they decide to design the API interface the way they did, why did they decide to offer some ML techniques over others, what are the tradeoffs between standardization and flexibility for using such a pipeline, when would a user use Clairvoyance over starting fresh.\n2. benefits of using this pipeline over other pipelines or no pipeline: the authors benchmarked their pipeline\u2019s performance over off-the-shelf ML models for some tasks, which is great! It would also be good to see the benefits of using Clairvoyance in terms of time to setup, time to train, etc.\n3. user interviews, feedback, and adoption to demonstrate how quickly new users can learn this framework and the benefits they observe while using it\n\nI look forward to hearing back from the authors and I\u2019m open to changing my score.", "title": "Exciting and useful pipeline; concerned that the paper doesn't do justice to the authors' work", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}}