{"paper": {"title": "Combating Deep Reinforcement Learning's Sisyphean Curse with Intrinsic Fear", "authors": ["Zachary C. Lipton", "Jianfeng Gao", "Lihong Li", "Jianshu Chen", "Li Deng"], "authorids": ["zlipton@cs.ucsd.edu", "jfgao@microsoft.com", "lihongli.cs@gmail.com", "jianshuc@microsoft.com", "deng@microsoft.com"], "summary": "Owing to function approximation, DRL agents eventually forget about dangerous transitions once they learn to avoid them, putting them at risk of perpetually repeating mistakes. We propose techniques to avert catastrophic outcomes.", "abstract": "To use deep reinforcement learning in the wild, we might hope for an agent that can avoid catastrophic mistakes. Unfortunately, even in simple environments, the popular deep Q-network (DQN) algorithm is doomed by a Sisyphean curse. Owing to the use of function approximation, these agents eventually forget experiences as they become exceedingly unlikely under a new policy. Consequently, for as long as they continue to train, DQNs may periodically relive catastrophic mistakes. Many real-world environments where people might be injured exhibit a special structure. We know a priori that catastrophes are not only bad, but that agents need not ever get near to a catastrophe state. In this paper, we exploit this structure to learn a reward-shaping that accelerates learning and guards oscillating policies against repeated catastrophes. First, we demonstrate unacceptable performance of DQNs on two toy problems. We then introduce intrinsic fear, a new method that mitigates these problems by avoiding dangerous states. Our approach incorporates a second model trained via supervised learning to predict the probability of catastrophe within a short number of steps. This score then acts to penalize the Q-learning objective, shaping the reward function away from catastrophic states.", "keywords": ["Deep learning", "Reinforcement Learning", "Applications"]}, "meta": {"decision": "Reject", "comment": "This paper presents a few interesting ideas, namely the idea of keeping around a set of \"danger states\" and treating these states with some special consideration in reply to make sure that their impact is not neglected after collecting a lot of additional data.\n \n However, there are two main problems: 1) the actual implementation here seems fairly ad-hoc, and it's not at all clear to me that this particular algorithm (building a classifier with equal numbers of good and danger states, and then injecting an additional reward into the Q-learning task based upon this classifier), is the right way to go about this. The presentation is also difficult to follow, and the final results imply aren't that compelling (though this is improving after the revisions, but still has a way to go. We therefore encourage the authors to resubmit their work at a future conference venue.\n \n Pros:\n + Interesting idea of keeping around danger states and injecting them into training\n \n Cons:\n - Algorithm doesn't seem that well motivated\n - Presentation is a bit unclear, takes until page 6 to actually present the basic approach.\n - Experiments aren't that convincing (better after revisions, but still need work)"}, "review": {"S16CajZSe": {"type": "rebuttal", "replyto": "Skw-_de4x", "comment": "Dear reviewer,\n\nThanks for your patience. We'd like to alert you that we've updated the draft, improving several aspects in response to your comments. \n\nAs relates to your concerns, please note:\n\n1) We have added plots on adventure seeker and on Cart-Pole comparing and expected-SARSA, on-policy variant of DQN to the standard DQN. We also did more extensive tuning to make each baseline model as strong as possible. On Adventure Seeker, expected SARSA does better than DQN but still suffers the problem that the model perpetually encounters catastrophes. On cart-pole expected SARSA does marginally worse, and again continues to perpetually die. On both domains, the fear model quickly solves the game. After this point, it gets unbounded total rewards (so we can't plot the reward) and zero catastrophes. \n\n2) We added preliminary experiments comparing the intrinsic feat model to DQN on Atari game. Results:\nFor high settings of the fear factor and wide fear radius, the agent is able to avoid death but at the expense of getting points. Interestingly, for a lower setting of the fear factor and radius (putting the fear penalty on same scale as extrinsic rewards), the agent gets a significantly high total reward than the standard DQN but a near-identical catastrophe rate. This suggests an interesting interplay between the minimizing catastrophes and maximizing rewards.\n\nEven this one game required the use of 6 GPUS (for multiple runs) over an extended period of time. We hope to extend the results but given resource constraints this may take time. \n\n3) We added some analysis demonstrating why the approach may not be as ad-hoc as it seems. Namely, if we have prior knowledge that the optimal policy avoids the set of danger states at radius k, and if the danger model is sufficiently accurate, then the optimal policy implied by the new objective should remain unchanged. \n\n4) We fixed the bibliographic errors. Thanks for pointing these out and we apologize for our hasty assembly of the initial bibliography. We've added the years and proper venues of publication and fixed the odd error of the misplaced \"et al.\".\n\n", "title": "Added experiments on Atari, comparisons to expected SARSA, fixed bibliographic errors"}, "SJW8Ls-He": {"type": "rebuttal", "replyto": "Sybmx1uEe", "comment": "Dear Reviewer,\n\nThanks for your feedback. We're glad that you enjoyed our poetry and hope you find the latest updates to the paper interesting. \n\nSummary:\nWe ran extensive preliminary experiments at getting intrinsic fear working with a deep convolutional Q-network for the Atari game Seaquest. We choose Seaquest because it trains reasonably fast compared to some other Atari games and because of the well-defined catastrophes: the swimmer is eaten by a fish and loses its last life. Because the openAI gym environment does not inform the agent of intermediate deaths (loss of 1st, 2nd life) we don't take these into account but in the future might if we could hack the game environment to expose this information.\n\nResults:\nFor high settings of the fear factor and wide fear radius, the agent is able to avoid death but at the expense of getting points. Interestingly, for a lower setting of the fear factor and radius (putting the fear penalty on same scale as extrinsic rewards), the agent gets a significantly high total reward than the standard DQN but a near-identical catastrophe rate. This suggests an interesting interplay between the minimizing catastrophes and maximizing rewards.\n\nAdditionally, we\n1) Extended the discussion of prior work\n2) Compared on Adventure Seeker and Cart-Pole to expected SARSA, showing that the problems we expose do not stem from on-policy vs off-policy learning.\n3) Fixed the bibliographic errors\n4) Extended analysis of our method. We consider: under what circumstances does or does not our objective function perturbation influence the optimal policy? As long as the optimal policy never enters the danger zone, and the danger model becomes arbitrarily accurate, the optimal policy should be unchanged.\n\nWe hope that you find these improvements compelling and see fit to revise the score. We'll continue to revise the draft and are grateful for any additional suggested improvements. ", "title": "Preliminary results on Atari Seaquest"}, "H1jqZBJHl": {"type": "rebuttal", "replyto": "S1hrLN1Hg", "comment": "Thanks for pointing this out! Next update I'll explicitly use the \"revisions\" link. Last time, I believe I updated by using the \"edit\" link and updating the pdf. Sorry for the confusion and thanks for the note.", "title": "Thanks! "}, "SyAtZWkre": {"type": "rebuttal", "replyto": "Skw-_de4x", "comment": "Dear reviewer,\n\nThank you for your extensive and insightful comments. Please pardon the delay in response. We have been working to show some preliminary results of applying these techniques on the Atari arcade environment. Unfortunately each of these experiments can take 10 days to run and require many GPUs. Because there is some variability among multiple runs, we want to run each method multiple times before reporting even preliminary results. We will notify you when we update the paper and hope you check back as we expect to update it considerably. \n\nIn the meantime we didn't want to leave your comments hanging without a reply.\n\n1) We agree that the previous work can be explored in both more detail and more directions. We are aware of the work of Shie Mannor and are looking into these connections. We're not sure which is the latest version of the paper that you've accessed. In the latest update (just a day or so before your review was posted), we amended the related work considerably. Following the lead of Garcia et al's extensive review on the literature we looked at two lines of work (i) methods that optimize a modified objective and (ii) methods that seek to ensure safe exploration by incorporating expert knowledge via constraints or interactive learning into the exploration process. \n\n2) Consider the case of self-driving car which may hit pedestrians (but never should). This represents a special notion of catastrophe insufficiently addressed in previous literature. Not only is the state bad in the sense of low reward, it's also a state that we know should never be proximal. In other words, we have prior knowledge that an optimal policy should never be within k steps of possibly killing a pedestrian. But how can we know in advance which states (in the state space) are within this radius? \n\nIf we knew precisely how to recognize these states a priori, then we wouldn't need to learn the danger model, we could specify it in advance. In this case, our problem would simply reduce to reward shaping. But for many real-world problems with large continuous input spaces it seems unlikely that we can know this in advance. The problem setup may appear complex, but we argue it is only to reflect the complexity of reality.\n\n3) We present these toy cases because they indicate just how unsettling the problems with deep reinforcement learning may be. We believe the simplicity of the problems is precisely what makes them disturbing. Surely, any of these dynamics could be embedded in a much harder problem. Think of the XOR failure case for linear models. It's the simplicity of case that is noteworthy. \n\nIdeally we will show results on the toy examples and also that our solutions enable improved performance on harder problems. Accordingly, we have preliminary experiments running on the Atari environment where many games have obvious catastrophes (e.g. Seaquest sprite eaten by a fish or PacMan is eaten by a Ghost). We hope to augment the paper with some preliminary results before the review period is over. \n\nThanks again for your comments and we promise to update you as we improve the draft. ", "title": "Thanks for insightful comments (and a few responses)"}, "rJZ_9xyBe": {"type": "rebuttal", "replyto": "r1tHvHKge", "comment": "Thanks for an interesting take on an important question :) \nSome high level comments/questions about the issue of \"safety\" in RL:\n\n1. What is the justification that there is any hope of behaving safely when a model is unavailable? Some amount of exploration is needed anyway, and without exploration you are doomed to not find the optimal policy. Essentially you either get stuck with a sub-optimal policy or accept some amount of failures. Whether such failures are acceptable or not is highly application specific, and hence I would encourage the authors to consider a concrete application that has some use (as opposed to toy problems) and provide a reasonable solution tailored to the same. A general solution is unlikely to exist, or be useful in practice.\n\n2. If a model is available, then much of the motivations raised become irrelevant. A simulated car can hit simulated pedestrians many times, and learn from the mistake in simulation -- it does not pose any threat. Further, measures like risk sensitive RL or robust RL can be put in place to ensure sim2real transfer.\n\n3. In light of (2), I would actually encourage the authors to think of their danger model as a form of reward shaping. Can we think of fear as a way to guide the exploration space when a model is made available? That way, the question shifts away from \"safety\" to sample efficiency and transfer. I think the danger model could be a good approach for these considerations (e.g. similar to SARSA). Though these have been mentioned in the paper, I look forward to an expanded discussion along these lines.", "title": "Some high-level comments"}, "HyCCjWgQx": {"type": "rebuttal", "replyto": "r197Kmyml", "comment": "Dear Reviewer,\n\nThanks for notifying us of your shared questions with reviewer 1. Given the overlap, we've composed a single response: \n\u00a0\nPer your suggestion, we implemented expected SARSA and ran experiments on the Adventure Seeker problem. The change in target did not achieve better results. Like the standard DQN, expected SARSA (with neural network function approximation) suffers from catastrophic forgetting under distributional shift of training data. We are happy to add this as a fair baseline per your request.\n\u00a0\nWe believe that expected SARSA fails for the following reasons: The oscillating behavior that we observe owes to distributional shift in combination with function approximation. This problem cannot be explained by the use of off-policy learning. It persists even for on-policy training. For example, we observed the problem for Q-learners even after epsilon is attenuated to 0 (so long as we continue to update the model).\u00a0\n\u00a0\nThe reason is that each policy under discussion is only updated on the distribution of states that it encounters. Once any policy learns to avoid the catastrophic states, these states cease to be represented in the training data. Even when using an experience replay buffer, these states will eventually be flushed. And even if the experience replay buffer is never flushed, these states will come to form an arbitrarily small fraction of the stored experiences, having negligible influences on the gradient during training.\n\u00a0\nOur work addresses the susceptibility of neural networks to forget old patterns due to catastrophic interference. While this problem is well described in general, such as the well-known chattering behavior of SARSA, the connections to deep reinforcement learning are under-explored and the connection to AI safety has not, to our knowledge, been examined.\u00a0", "title": "Copying reply to R1 to address the shared question of R3"}, "rJ2lj-gXl": {"type": "rebuttal", "replyto": "ryeIcMy7l", "comment": "Dear Reviewer,\n\u00a0\nThanks for your comments. Per your suggestion, we implemented expected SARSA and ran experiments on the Adventure Seeker problem. The change in target did not achieve better results. Like the standard DQN, expected SARSA (with neural network function approximation) suffers from catastrophic forgetting under distributional shift of training data. We are happy to add this as a fair baseline per your request.\n\u00a0\nWe believe that expected SARSA fails for the following reasons: The oscillating behavior that we observe owes to distributional shift in combination with function approximation. This problem cannot be explained by the use of off-policy learning. It persists even for on-policy training. For example, we observed the problem for Q-learners even after epsilon is attenuated to 0 (so long as we continue to update the model).\u00a0\n\u00a0\nThe reason is that each policy under discussion is only updated on the distribution of states that it encounters. Once any policy learns to avoid the catastrophic states, these states cease to be represented in the training data. Even when using an experience replay buffer, these states will eventually be flushed. And even if the experience replay buffer is never flushed, these states will come to form an arbitrarily small fraction of the stored experiences, having negligible influences on the gradient during training.\n\u00a0\nOur work addresses the susceptibility of neural networks to forget old patterns due to catastrophic interference. While this problem is well described in general, such as the well-known chattering behavior of SARSA, the connections to deep reinforcement learning are under-explored and the connection to AI safety has not, to our knowledge, been examined.\u00a0\n", "title": "The problems we observe stem from distributional shift, not off-policy learning"}, "r197Kmyml": {"type": "review", "replyto": "r1tHvHKge", "review": "I would like to echo R1 that Q-learning suffers as expected and past work has addressed such issues. I would like the authors to position their research contributions within the broader literature. - The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. After the pre-review comments, authors do mention that they compared against expected SARSA but I would really like to see these and other extensive baselines before accepting this paper. \n\n- There is also an increasing amount of literature of using reward replay buffers in deep RL agents (c.f. Jaderberg, Max, et al. \"Reinforcement learning with unsupervised auxiliary tasks.\", Blundell, Charles, et al. \"Model-free episodic control.\" , Narasimhan et al. \"Language understanding for text-based games using deep reinforcement learning\"), which could perhaps reinforce the agent to avoid revisiting catastrophic states. \n\n- Overall, the approach presented is not very principled. For instance, why isn't catastrophe directly provided as a signal to the learner instead of a separate model? ", "title": "baselines", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ByojUYgVl": {"type": "review", "replyto": "r1tHvHKge", "review": "I would like to echo R1 that Q-learning suffers as expected and past work has addressed such issues. I would like the authors to position their research contributions within the broader literature. - The topic of keeping around highly rewarding or dangerous states is important and has been studied extensively in the RL literature. After the pre-review comments, authors do mention that they compared against expected SARSA but I would really like to see these and other extensive baselines before accepting this paper. \n\n- There is also an increasing amount of literature of using reward replay buffers in deep RL agents (c.f. Jaderberg, Max, et al. \"Reinforcement learning with unsupervised auxiliary tasks.\", Blundell, Charles, et al. \"Model-free episodic control.\" , Narasimhan et al. \"Language understanding for text-based games using deep reinforcement learning\"), which could perhaps reinforce the agent to avoid revisiting catastrophic states. \n\n- Overall, the approach presented is not very principled. For instance, why isn't catastrophe directly provided as a signal to the learner instead of a separate model? ", "title": "baselines", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ryeIcMy7l": {"type": "review", "replyto": "r1tHvHKge", "review": "Q-learning is learning off-policy about the greedy policy, so by construction it cannot learn about the consequences of the exploration epsilon in its behavior policy. There is well-established \"cliff\" style domains that study this effect when exploration can be catastrophic, and there are numerous well-established algorithmic solutions, namely Expected SARSA -- which can trivially replace the Q-learning update inside the DQN setup. I'd strongly encourage the authors to report those results as the (fair) baseline.This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea).\n\n1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of \u201crobust RL\u201d. A good place to start might be with the work of Shie Mannor.\n\n2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as \u201ccatastrophic\u201d if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I\u2019d want to see plots for \u201caverage reward\u201d in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for entering these otherwise invisible \u201cdanger states\u201d?\n\n3) a somewhat ad-hoc solution, that introduces new domain-specific hyperparameters (k_r, k_lambda and lambda) a second deep network and and two additional replay memories. In terms of results, I\u2019m also unsure whether I can trust the results, given the long-standing track-record of cart-pole being fully solved by many methods: is DQN an outlier here? Or is the convnet not an appropriate function-approximator? Actually: which exact variant \u201cstate-of-the-art\u201d variant of DQN are you using?\n\nThe good idea that I encourage the authors to pursue further is D_d, this set of rare but dangerous states, that should be kept around in some form. I see it as an ingredient for continual learning that most typical methods lack -- it is also one of the big differences between RL and supervised learning, where such states would generally be discarded as outliers.\n\nGiven my comment a couple of weeks ago, and the prompt response (\u201cwe implemented expected SARSA\u201d), I would have expected that the paper had been revised with the new results by now? In any case, I\u2019m open to discussing all these points and revising my opinion based on an updated version of the paper.\n\nMinor comment: the bibliography is done sloppily, with missing years, conference venues and missing/misspelled author lists, e.g. \u201cSergey et al. Levine\u201d. I also think it is good form to cite the actual conference publications instead of arXiv where applicable.", "title": "SARSE", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Skw-_de4x": {"type": "review", "replyto": "r1tHvHKge", "review": "Q-learning is learning off-policy about the greedy policy, so by construction it cannot learn about the consequences of the exploration epsilon in its behavior policy. There is well-established \"cliff\" style domains that study this effect when exploration can be catastrophic, and there are numerous well-established algorithmic solutions, namely Expected SARSA -- which can trivially replace the Q-learning update inside the DQN setup. I'd strongly encourage the authors to report those results as the (fair) baseline.This paper addresses an important and timely topic in a creative way. I consider it to have three flaws (and one good idea).\n\n1) insufficient context of what is known and had been studied before (in shallow RL), for example within the field of \u201crobust RL\u201d. A good place to start might be with the work of Shie Mannor.\n\n2) an ill-defined general problem setup. Does it make sense to do post-hoc labeling of certain actions as \u201ccatastrophic\u201d if the agent is not informed about that metric during learning? Training a system to do one thing (maximize reward), but then evaluating it with a different metric is misleading. On the training metric, it could even be that the baseline outperforms the new algorithm? So I\u2019d want to see plots for \u201caverage reward\u201d in fig 3 as well. Also, what would the baseline learn if it was given large negative rewards for entering these otherwise invisible \u201cdanger states\u201d?\n\n3) a somewhat ad-hoc solution, that introduces new domain-specific hyperparameters (k_r, k_lambda and lambda) a second deep network and and two additional replay memories. In terms of results, I\u2019m also unsure whether I can trust the results, given the long-standing track-record of cart-pole being fully solved by many methods: is DQN an outlier here? Or is the convnet not an appropriate function-approximator? Actually: which exact variant \u201cstate-of-the-art\u201d variant of DQN are you using?\n\nThe good idea that I encourage the authors to pursue further is D_d, this set of rare but dangerous states, that should be kept around in some form. I see it as an ingredient for continual learning that most typical methods lack -- it is also one of the big differences between RL and supervised learning, where such states would generally be discarded as outliers.\n\nGiven my comment a couple of weeks ago, and the prompt response (\u201cwe implemented expected SARSA\u201d), I would have expected that the paper had been revised with the new results by now? In any case, I\u2019m open to discussing all these points and revising my opinion based on an updated version of the paper.\n\nMinor comment: the bibliography is done sloppily, with missing years, conference venues and missing/misspelled author lists, e.g. \u201cSergey et al. Levine\u201d. I also think it is good form to cite the actual conference publications instead of arXiv where applicable.", "title": "SARSE", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}