{"paper": {"title": "Unsupervised Pretraining for Sequence to Sequence Learning", "authors": ["Prajit Ramachandran", "Peter J. Liu", "Quoc V. Le"], "authorids": ["prajitram@gmail.com", "peterjliu@google.com", "qvl@google.com"], "summary": "Pretraining seq2seq models gives large gains in both generalization and optimization on a variety of tasks.", "abstract": "This work presents a general unsupervised learning method to improve\nthe accuracy of sequence to sequence (seq2seq) models. In our method, the\nweights of the encoder and decoder of a seq2seq model are initialized\nwith the pretrained weights of two language models and then \nfine-tuned with labeled data. We apply this method to\nchallenging benchmarks in machine translation and abstractive\nsummarization and find that it significantly improves the subsequent\nsupervised models.  Our main result is that the pretraining\naccelerates training and improves generalization of seq2seq models,\nachieving state-of-the-art results on the WMT\nEnglish->German task, surpassing a range of methods using\nboth phrase-based machine translation and neural machine\ntranslation. Our method achieves an improvement of 1.3 BLEU from the\nprevious best models on both WMT'14 and WMT'15\nEnglish->German. On summarization, our method beats\nthe supervised learning baseline.", "keywords": ["Natural language processing", "Deep learning", "Semi-Supervised Learning", "Transfer Learning"]}, "meta": {"decision": "Reject", "comment": "This paper effectively demonstrates that the use of pretraining can improve the performance of seq2seq models for MT and summarization tasks. However, despite these empirical gains, the reviewers were not convinced enough by the novelty of the work itself and did not feel like there were technical contributions to make this a fit for ICLR.\n \n Pros:\n - All reviewers agree that the empirical gains in this paper are convincing and lead to BLEU improvements on a large scale translation and translation like tasks. Reviewer 4 also praises the detailed analysis that demonstrates that these gains come from the pretraining process itself. \n - From an impact perspective, the reviewers found the approach clear and implementable. \n \n Cons:\n - Novelty criticisms are that the method is a \"compilation\" of past approaches (although at a larger scale) and therefore primarily experimental, and that the objectives given are \"highly empirical\" and not yet motivated by theory. The authors did respond, but the reviewer did not change their score.\n - There are suggestions that this type of work would perhaps be more widely impactful in an NLP venue, where a BLEU improvement of this regard is a strong supporting piece on its own."}, "review": {"HJpdIkwwl": {"type": "rebuttal", "replyto": "S1iDqXoVl", "comment": "We would just like to follow up and see if the latest version of our draft addresses your main concern regarding the empirical nature of the objective. In short, we updated the paper to connect our method with language modeling, which gives the method a good foundation. Additionally, we discuss and draw several connections with the papers you listed, giving our paper a theoretical justification.\n\nThe key novelty of our work is we develop pretraining for seq2seq, along with modifications in the network architecture and training algorithm to support it: monolingual language modeling losses and using residual connections to connect the pretrained components. In combination they achieve large improvements over strong baselines on competitive tasks. That said, we do agree that the main selling point of our paper is the empirical evaluation of the method, which we believe will be valuable for subsequent studies on this topic.", "title": "Followup on the latest draft"}, "ryGrtTKHg": {"type": "rebuttal", "replyto": "HyfU5MFSg", "comment": "Thank you for the review. We would like to highlight some important differences between our work and the related works that were mentioned:\n\n* Dai and Le, 2015 focus on text classification, not harder seq2seq tasks.\n* It has been well documented in the machine learning literature that pretraining improves performance on tasks with very little data. Zoph et al. 2016, and a few other seq2seq works, demonstrate similar findings on resource poor language pairs.\n* However, the conventional wisdom is that pretraining is unnecessary, or even harmful, for tasks with medium or large datasets. We believe we are the first to show the contrary by achieving strong results on competitive tasks with medium sized datasets.\n\n\nWe were surprised to see the score of 6 given the positive review, because a paper with an averaged score of 6 may not eventually get accepted (taking into account last year\u2019s threshold and this year\u2019s increased number of submissions).", "title": "Re: Reviewer4's review"}, "HysqtRC4g": {"type": "rebuttal", "replyto": "r1L2IyIVe", "comment": "Thank you for the review.\n\n* Even though our paper does look like a compilation of previous techniques, it is important to see that it really works. After years of research, it is extremely rare to see unsupervised learning help supervised learning, and thus our paper is an important achievement.\n* In recent literature, pretraining has been dismissed as being effective only on small datasets. We show that pretraining seq2seq models gives significant improvements even on challenging medium sized datasets.\n* Furthermore, as noted in the review, we\u2019re one of the only works that try to deeply understand pretraining, which we tackle with our comprehensive ablation study.\n* We in fact have an experiment with a randomly initialized model trained with the monolingual objective in our paper. Pretraining increases performance by 2.0 BLEU points over this baseline.\n* We have added additional references to papers published in the 80s and 90s, and have also corrected the typo in the latest revision of our paper.\n", "title": "Re: Reviewer3's review"}, "r1rhd0CEl": {"type": "rebuttal", "replyto": "S1iDqXoVl", "comment": "Thank you for the review. The method of Chen et al., 2016 and our method are indeed similar in spirit, and the differences are in the details:\n\n* They do not apply their method to seq2seq. They experiment on toy datasets as a proof-of-concept. We demonstrate strong results on real world datasets.\n* If applied to seq2seq, we can interpret the first term of their objective as making the output sequence more likely, as scored by a pretrained language model. In our method, the language model is folded into the decoder. We believe that using the pretrained language model for scoring is less efficient than using all the pretrained weights.\n* They have an artificial objective to ensure output sequences depend on input sequences. We rely on labeled examples.\n\nWe believe that the connection between our algorithm and Chen et al\u2019s will strengthen our work and make our algorithm more theoretically justified. \n\nThe method of Dahl et al., 2012 and our method are also similar in that we both use unsupervised pretraining to help supervised learning, but they are different in many other ways:\n* They focus on feedfoward networks with DBNs, which are not a natural model for sequences. We focus on seq2seq learning, using easily trained language models.\n* Pretraining acoustic models is now considered unnecessary, probably because the reconstruction objective used by DBNs and autoencoders is too easy. We show that pretraining by next step prediction improves seq2seq models significantly on realistic and challenging datasets.\n* In addition to the differences in the algorithms, we perform comprehensive ablation studies to verify that pretraining helps generalization and optimization. They make this argument without substantial supporting evidence from their experiments.\n\nWe have rewritten the related works section in the latest revision of our paper to include discussions about these two interesting papers.", "title": "Re: Reviewer1's review"}, "SJgg6GB7x": {"type": "rebuttal", "replyto": "SJAhIUyXl", "comment": "(Dahl et al. 2012), which is the paper you link, uses layer-wise pretrained DBNs to initialize their DNNs. In contrast, we just pretrain language models on a ton of unlabeled data. It is no longer necessary to train using the layer-wise fashion since we can train large models end-to-end with modern techniques.\n\nHowever, their reasons for pretraining are actually similar to ours, so we can extend their reasoning:\n\n(1) Pretraining is a strong, data-dependent regularizer / prior.\n\nSay our unsupervised task uses the dataset D_U and the supervised task uses the dataset D_S. Then we can use Bayes Rule to compute the conditional probability of parameters (\\theta) after supervised training:\n\nlog p(\\theta | D_S) = log p(D_S | \\theta) + log p(\\theta | D_U) - log p(D_S)\n\nThe likelihood is standard. However, the prior here depends on the unsupervised pretraining, as opposed to something simple like a Gaussian prior (which corresponds to L2 regularization). Since our language models capture so much about language, this is an extremely informative prior, enabling a strong model to be trained.\n\nHowever, it is important that this prior is good. As seen in the ablation study for machine translation, if you pretrain on the parallel corpus (that is, the exact same data used for supervised training), the results are relatively poor compared with pretraining on large monolingual corpora. This is because the parallel corpus prior does not add much information to what is normally learned in supervised training, whereas a large monolingual corpora is required to learn the intricacies of language. This is a good thing -- it means that more unlabeled data gives a better prior, which matches our intuition.\n\nHaving the pretraining prior also makes learning on low-resource tasks much better. Without it, the parameter space is relatively unconstrained, and SGD will find parameters that do not generalize past D_S. Here, the pretraining prior is crucial for learning a good representation.\n \n(2) Pretraining is important for the optimization of networks that are very deep.\n\nFor their specific implementation, modern techniques make pretraining an unnecessary step. However, we still do not know how to effectively train RNNs that can capture very long term dependencies (over hundreds of steps). If initialized randomly, training an LSTM to effectively represent hundreds of inputs is incredibly difficult because the ill conditioned matrices lead to the vanishing gradient problem. Pretraining makes optimization easier from the very start -- the matrices are much better conditioned, which allows gradient to flow back much farther in time.\n\nThis will become an increasingly important going forward as more tasks will require reading long sequences (e.g. character-level NMT is becoming popular, but the input and output sequences are significantly longer than their word-level counterparts).\n\nHere's a great quote from (Dahl et al. 2012) that summarizes what I've said: \"The generative model learned during pre-training helps prevent overfitting, even when using models with very high capacity and can aid in the subsequent optimization of the recognition weights\".", "title": "Re: DNN pretraining"}, "SJAhIUyXl": {"type": "review", "replyto": "H1Gq5Q9el", "review": "Can you compare unsupervised Pretraining for Sequence to Sequence proposed in this paper with its counterpart for DNN (Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition, IEEE Transactions on Audio, Speech, and Language Processing, 2012)? what is the theoretical insight you may have out of this comparison?strengths:\n\nA method is proposed in this paper to initialize the encoder and decoder of the seq2seq model using the trained weights of language models with no parallel data. After such pretraining, all weights are jointly fine-tuned with parallel labeled data with an additional language modeling loss.\n\nIt is shown that pretraining accelerates training and improves generalization of seq2seq models.\n\nThe main value of the proposed method is to leverage separate source and target corpora, contrasting the common methods of using large amounts of parallel training corpora.\n\n\nweaknesses:\n\nThe objective function shown in the middle of pg 3 is highly empirical, not directly linked to how non-parallel data helps to improve the final prediction results. The paper should compare with and discuss the objective function based on expectation of cross entropy which is directly linked to improving prediction results as proposed in arXiv:1606.04646, Chen et al.:  Unsupervised Learning of Predictors from Unpaired Input-Output Samples, 2016.\n\nThe pre-training procedure proposed in this paper is also closely connected with the DNN pretraining method presented in Dahl et al. 2011, 2012. Comparisons should be made in the paper, highlighting why the proposed one is conceptually superior if the authors believe so. \n", "title": "Unsupervised Pretraining ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "S1iDqXoVl": {"type": "review", "replyto": "H1Gq5Q9el", "review": "Can you compare unsupervised Pretraining for Sequence to Sequence proposed in this paper with its counterpart for DNN (Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition, IEEE Transactions on Audio, Speech, and Language Processing, 2012)? what is the theoretical insight you may have out of this comparison?strengths:\n\nA method is proposed in this paper to initialize the encoder and decoder of the seq2seq model using the trained weights of language models with no parallel data. After such pretraining, all weights are jointly fine-tuned with parallel labeled data with an additional language modeling loss.\n\nIt is shown that pretraining accelerates training and improves generalization of seq2seq models.\n\nThe main value of the proposed method is to leverage separate source and target corpora, contrasting the common methods of using large amounts of parallel training corpora.\n\n\nweaknesses:\n\nThe objective function shown in the middle of pg 3 is highly empirical, not directly linked to how non-parallel data helps to improve the final prediction results. The paper should compare with and discuss the objective function based on expectation of cross entropy which is directly linked to improving prediction results as proposed in arXiv:1606.04646, Chen et al.:  Unsupervised Learning of Predictors from Unpaired Input-Output Samples, 2016.\n\nThe pre-training procedure proposed in this paper is also closely connected with the DNN pretraining method presented in Dahl et al. 2011, 2012. Comparisons should be made in the paper, highlighting why the proposed one is conceptually superior if the authors believe so. \n", "title": "Unsupervised Pretraining ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HkNeGo5Zg": {"type": "rebuttal", "replyto": "Bkh_wAKZg", "comment": "The model without pretraining (but with the LM objective, as in the ablation study) has a detokenized, truecased BLEU of 21.3 for newstest2014 and 24.3 for newstest2015. As expected, these are above the numbers of Jean et al. (2015), with a +1.9 BLEU point improvement for newstest2015. So we get a +2.7 BLEU point improvement on newstest2015 with pretraining. Note that the model without pretraining uses the LM objective, and we expect its performance to drop without it. The next revision of our paper will include these numbers in Table 1.", "title": "Re: baseline"}, "Bkh_wAKZg": {"type": "rebuttal", "replyto": "H1Gq5Q9el", "comment": "what's your own baseline for NMT without pre-training? Jean et al. (2015) uses a more shallow architecture than this paper, so I presume your baseline would be higher (this is also corrobated in figure 3).\n\nyou should make it clear how much of your improvement over related work comes from pretraining, and how much from having a deeper architecture.", "title": "baseline"}}}