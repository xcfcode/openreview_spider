{"paper": {"title": "Wandering within a world: Online contextualized few-shot learning", "authors": ["Mengye Ren", "Michael Louis Iuzzolino", "Michael Curtis Mozer", "Richard Zemel"], "authorids": ["~Mengye_Ren1", "~Michael_Louis_Iuzzolino1", "~Michael_Curtis_Mozer1", "~Richard_Zemel1"], "summary": "We propose a new continual few-shot learning paradigm and a new model.", "abstract": "We aim to bridge the gap between typical human and machine-learning environments by extending the standard framework of few-shot learning to an online, continual setting. In this setting, episodes do not have separate training and testing phases, and instead models are evaluated online while learning novel classes. As in the real world, where the presence of spatiotemporal context helps us retrieve learned skills in the past, our online few-shot learning setting also features an underlying context that changes throughout time. Object classes are correlated within a context and inferring the correct context can lead to better performance. Building upon this setting, we propose a new few-shot learning dataset based on large scale indoor imagery that mimics the visual\nexperience of an agent wandering within a world. Furthermore, we convert popular few-shot learning approaches into online versions and we also propose a new model that can make use of spatiotemporal contextual information from the recent past.", "keywords": ["Few-shot learning", "continual learning", "lifelong learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes a new online contextualized few-shot learning setting, with two associated datasets (notably, including one obtained from trajectories within the real-world Matterport3D reconstructions). A simple recurrent contextualized extension of Prototypical Networks is also proposed as a stronger baseline, demonstrating the need for incorporating such context. The reviewers all agreed that this is an interesting setting combining continual and few-shot learning, offering a more realistic problem that mirrors those that might be encountered by embodied agents. The authors provided very detailed rebuttals, answering some of the questions and concerns raised by the reviewers. In the end, all reviewers agreed that this paper would contribute a significant novel setting, and so I recommend acceptance. I encourage the others to include modifications related to some of the comments, such as strengthening/clarifying the setting including metrics, details of the method, etc."}, "review": {"o10wFYO7j0o": {"type": "review", "replyto": "oZIvHV04XgC", "review": "The paper proposes a new learning paradigm that combines both few-shot learning(FSL)  and continual learning (CL) to provide a more realistic learning environment rather than the traditional train-test-retrain approach in FSL. Two environments are proposed, along with a novel dataset. The evaluation seems to be thorough, with strong baselines (conventional approaches adapted to the proposed setting). A novel approach is proposed based on augmenting ProtoNets with contextual memory and is shown to have consistently strong performance compared to the baselines on both tasks.\n\nStrengths:\n+ The paper is very well written and reads very nicely. I particularly liked the motivation for the task.\n+ The use of contextual memory (incorporating both spatial and temporal context) is very interesting and is a promising approach for both FSL as well as a general learning architecture for visual event perception.\nConcerns:\n- While very excited by the potential of the proposed learning environment, I am a bit confused about how the actual implementation/evaluation maps to the motivation. For example, from what I can see, all baselines (including the proposed model) have access to the number of classes that are present in the data (k). A softmax-based decision function forces the model to choose one of these classes, either based on some online learning-based features or just through storing examples. Now, the premise (that of knowing when and what to learn) is not quite satisfied here and thus the metric (Average Precision) doesn't quite capture the entire picture. I think a better metric, IMHO, would be the harmonic mean of novel class detection and known class prediction. This would allow us to actually ascertain whether the models are learning novel instances and not just getting \"lucky\".\n- The baselines that are chosen are classic approaches to few-shot learning. Not many continual learning approaches are tested and I think that would make for a better comparison. Again, the actual learning setting is quite a bit more simplified than what is claimed.\n\nOverall, I think it is a very nicely written paper with some issues with evaluation settings that might be exaggerating the performance of baselines.\n\n==== Post discussion Update ====\n\nI am updating my score to accept after the discussion.", "title": "Interesting continual learning paradigm with few shot learning ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Q2PzJLe6hL4": {"type": "rebuttal", "replyto": "5-eWx1pJo2x", "comment": "**Sec 3**\n\n> Figure hyperrefs point to captions (below the figure).\n\nThanks for pointing it out. Unfortunately we don\u2019t know how to fix this to make it point to above the figure. If you know, please let us know and we\u2019d appreciate it!\n\n> At this point, the problem formulation should clearly state how training will be carried out. This section does not mention the fact that the agent will first encounter various evaluation sequences and use those to generalize to unseen evaluation sequences. It mentions multiple few-shot sequences and train/val/test splits, but it is largely unclear at this point how these various sequences are supposed to be used.\n\nThanks for pointing this out. During training, models are trained with training sequences. And at test time, models are evaluated with test sequences, which contains no overlap of object classes with the training sequences. We will revise the paper and make this clearer.\n\n> The way the problem setting is presented, it makes it hard to differentiate it substantially from few-shot or continual learning, since it mainly focus on evaluation being done continually. It seems like a crucial part of the distinction is that the agent is evaluated even on unseen classes.\n\nWe target this work as an extension of few-shot learning to make it an online sequential decision problem, similar to continual learning. However, in contrast to classic benchmarks in continual learning, the agent needs to quickly learn new classes with one or a few examples, and it also needs to detect unknown classes (as shown in Figure 1). We believe that being able to say something is unknown is essential for incremental class learning / continual learning but unfortunately the common definitions of continual learning currently do not capture this. We will follow your suggestion and be more explicit about detecting unknown classes.\n\n> This seems to be closely connected to work on open world and open set learning. Could the authors provide some guidance into how it compares to that setting? [1]\n\nThank you for the reference and we will cite more open world literature in our next version. The main difference is that in our online few-shot setting, the unknown class will become known in the next immediate iteration, if the label information is given. This will require models to recognize unknown classes in a 1-shot manner.\n\n**Sec 4**\n\n> When doing online average, the method seems to assume that the current prototype p_{t-1} is fixed. But, if the model parameters change over time, which I expect they do in a continual learning setting, this prototype would be different if recomputed. How is this taken into account?\n\nCurrently the CNN is frozen during the sequence and therefore it won\u2019t have this issue of updating prototypes. Freezing the CNN is an approximation to the assumption that the time scale of a sequence is brief relative to the agent's lifetime. Thus, there is no representation learning within the perceptual system. However, we are currently studying the effect of changing the CNN parameters over time within the sequence.\n\n**Sec 5**\n\n> I would've liked to see the ablation tests on RoamingRooms as well.\n\nThank you for the suggestion. We will run these ablation tests.\n\n> My only concern is that the evaluations are only on two different data sets.\n\nWhile we could run RoamingImageNet-like experiments, we would like to emphasize that our main interest is to study realistic online sequences and this is showcased in our RoamingRooms dataset.\n\n> In terms of the baselines, it is unclear which can leverage previous sequences to learn the new ones. I believe this is only possible with OML and variants, whereas all the other methods can only leverage data from the current sequence.\n\nAll of the baselines leverage data only from the current sequence. During meta-learning, all models can learn the weights of the CNN and/or the RNN by rolling out the sequence and back-propagating through time.", "title": "Response to Reviewer 1 (Part 2)"}, "xx3gdmCvL8e": {"type": "rebuttal", "replyto": "35oPnOr_ZIO", "comment": "Thank you for your comments and suggestions. Below are our responses.\n\n> (AP and N-Shot accuracy), those metrics lack some details and are not well justified. My understanding is that both metrics are accumulated from the starting to the current step and across all sequences. AP is different from standard average precision (the TP/TN/FP/FN definitions are different).\n\nWe realize that our description of these metrics is not clear in the paper. We will update the paper.\n\nThese metrics are justified by the domain. We chose to use AP (average precision or area under the precision-recall curve) as a way of integrating two aspects of performance: \n\n1) the binary accuracy of whether an instance belongs to a known or unknown class (KU-Assign for short), and\n2) the accuracy of assigning an instance the correct class label given it is from a known class (Class-Assign for short).\n\nThe procedure to calculate AP is as follows. We first sort all the {KU-Assign, Class-Assign} predictions across all sequences in descending order based on KU-Assign probability, where the high ranked predictions should be known (not novel) classes. For the N top ranked instances in the sorted list, we compute:\n\n* precision@N = correct(Class-Assign)@N / N\n* recall@N = correct(Class-Assign)@N / K,\n\nwhere K is the true number of known instances and correct(Class-Assign)@N is the count of the number of correct class assignments among the top N. (The class assignment for an unknown instance is always incorrect.) To obtain the AP, we compute the integral of the function (y=precision@N, x=recall@N) across all N\u2019s.\n\nN-shot accuracy: We define N-shot accuracy as the number of times an instance that has been seen N times thus far in the sequence is classified correctly. We compute the mean and standard error of this over all sequences.\n\n> How do these metrics capture catastrophic forgetting? For example, the accuracy / AP vs. the time interval between the current label and the last time the same label was observed.\n\nIt is true that these metrics do not explicitly capture catastrophic forgetting (CF). In this setting it is difficult to pull out CF explicitly because of the increase in the number of classes over time. So a decrease in accuracy could be due to both CF and the increasing difficulty based on the number of classes. We provide two different measurements, and both are unfortunately coupled with the total number of classes.\n\nFirst, we would like to point the reviewer to Figure 5, where we show instantaneous accuracy over time, which drops as the number of classes increases. \n\nSecond, we also plot a 2D table here in our rebuttal. On the y-axis is the number of times we have seen item X\u2019s label for K times (K-shot), and on the x-axis we look at the time interval since we last saw such item X. We are comparing CPM with Online ProtoNet (OPN)\n\n**RoamingOmniglot (Last seen interval vs. K-shot)**\n\n|     **Interval** | **1 ~ 2** | **3 ~ 5** | **6 ~ 10** | **11 ~ 20** | **21 ~ 50** | **51 ~ 100** |\n|--------------|-------|-------|--------|---------|---------|----------|\n| OPN 1-shot   | 88.8  | 86.9  | 85.2   | 84.7    | 83.6    | 81.1     |\n| CPM 1-shot   | **96.06** | **94.01** | **92.95**  | **91.56**   | **88.21**   | **84.58**    |\n| OPN 3-shot   | 97.2  | 97.1  | 96.6   | 96.7    | **96.5**    | 95.3     |\n| CPM 3-shot   | **98.48** | **98.16** | **97.46**  | **97.17**   | 95.37   | **95.53**    |\n\n**RoamingOmniglot-Semi-Sup**\n\n|     **Interval** | **1 ~ 2** | **3 ~ 5** | **6 ~ 10** | **11 ~ 20** | **21 ~ 50** | **51 ~ 100** |\n|------------|-------|-------|--------|---------|---------|----------|\n| OPN 1-shot | 90.05 | 88.93 | 88.41  | 87.60   | 87.31   | 85.12    |\n| CPM 1-shot | **95.86** | **93.84** | **92.81**  | **91.78**   | **89.36**   | **85.68**    |\n| OPN 3-shot | 97.77 | 97.33 | 97.11  | **97.75**   | **97.69**   | **96.78**    |\n| CPM 3-shot | **98.71** | **97.53** | **97.54**  | 96.51   | 96.34   | 92.93    |\n\n**RoamingRooms**\n\n|     **Interval** | **1 ~ 2** | **3 ~ 5** | **6 ~ 10** | **11 ~ 20** | **21 ~ 50** | **51 ~ 100** |\n|------------|-------|-------|--------|---------|---------|----------|\n| OPN        | 93.47 | 89.30 | 79.35  | 67.19   | 60.27   | 60.06    |\n| CPM 1-shot | **95.70** | **92.16** | **85.68**  | **75.15**   | **70.04**   | **66.36**    |\n| OPN 3-shot | 95.11 | 91.80 | 85.55  | 78.19   | 74.64   | 73.84    |\n| CPM 3-shot | **96.14** | **93.78** | **87.73**  | **81.40**   | **79.09**   | **78.20**    |\n", "title": "Response to Reviewer 4 (Part 1)"}, "TXpLpx4Ijk3": {"type": "rebuttal", "replyto": "o10wFYO7j0o", "comment": "Thank you for your insightful comments. Below are our responses:\n\n> I am a bit confused about how the actual implementation/evaluation maps to the motivation. For example, from what I can see, all baselines (including the proposed model) have access to the number of classes that are present in the data (k). \n\nAll models have access to the number of classes *that have been learned so far up until the current step*, but they are not given the total number of classes that will appear in the sequence.\n\n> A softmax-based decision function forces the model to choose one of these classes, either based on some online learning-based features or just through storing examples. Now, the premise (that of knowing when and what to learn) is not quite satisfied here and thus the metric (Average Precision) doesn't quite capture the entire picture. I think a better metric, IMHO, would be the harmonic mean of novel class detection and known class prediction. This would allow us to actually ascertain whether the models are learning novel instances and not just getting \"lucky\".\n\nTo clarify, our model has two-headed outputs. It separately outputs 1) a sigmoid probability of whether it belongs to a new class, and 2) a softmax probability of which previously known class it belongs to. \n\nWe chose to use AP (average precision or area under the precision-recall curve) as a way of integrating two aspects of performance: \n\n1) the binary accuracy of whether an instance belongs to a known or unknown class (KU-Assign for short), and \n\n2) the accuracy of assigning an instance the correct class label given it is from a known class (Class-Assign for short).\n\nThe procedure to calculate AP is as follows. We first sort all the {KU-Assign, Class-Assign} predictions across all sequences in descending order based on KU-Assign probability, where the high ranked predictions should be known (not novel) items. For the N top ranked items in the sorted list, we compute:\n\n* precision@N = correct(Class-Assign)@N / N\n* recall@N = correct(Class-Assign)@N / K,\n\nwhere K is the true number of known instances and correct(Class-Assign)@N is the count of the number of correct class assignments among the top N. (The class assignment for an unknown item is always incorrect.) To obtain the AP, we compute the integral of the function (y=precision@N, x=recall@N) across all N\u2019s.\n\nTherefore, just like you suggested, it is a combination of novel class detection and known class prediction: precision takes in the accuracy of known class prediction, and for novel class detection, you need to have a perfect sigmoid score ranking. A harmonic mean will also show similar trends, but would be a single point on the precision-recall curve.\n\n> The baselines that are chosen are classic approaches to few-shot learning. Not many continual learning approaches are tested and I think that would make for a better comparison. Again, the actual learning setting is quite a bit more simplified than what is claimed.\n\nThis is a good point. We consider our setup primarily an extension of few-shot learning, so we feel it is the most appropriate to compare with few-shot learning baselines. However it is somewhere between few-shot and continual learning. So we do in fact compare to both. OML was recently proposed as a meta-continual learning model. And we have adapted it to output unknown classes (OML-U and OML-U++) and compared them. While we would love to compare to more continual learning methods, we need to do some non-trivial amount of adaptation to each, to have a novelty detection output. The length of each online sequence in our setting is also shorter than most continual learning setups.", "title": "Response to Reviewer 2"}, "5-eWx1pJo2x": {"type": "rebuttal", "replyto": "S219z92c1i7", "comment": "We are grateful for R1\u2019s detailed reading and constructive comments. Below are our responses.\n\n> It appears that the authors' proposed method requires a pre-training phase, where the agent encounters many evaluation scenarios like this one, in order to perform well in the final evaluation scenario. This is never explicitly stated or explained in detail, but if it is the case, it is highly unrealistic, as it requires the agent to essentially live multiple lives, before being able to perform well in a \"test\" life.\n\nWe are not modeling a de-novo learner, but rather, our model is more like an adolescent who comes to new environments with some previous experience (which are encoded in the long-term memory of the CNN and RNN weights). After each episode in meta-training, the short-term memory is reset but long-term memory in the form of the CNN and RNN weights are updated. So if our work is 'lifelong learning', it's lifelong from adolescence forward. Standard FSL also has the same assumption of a fair amount of past experience prior to the first few shot-episode. Therefore, making the agent entirely starting from scratch is not our goal here.\n\n> The techniques used to create the Roaming version of Omniglot could potentially be  applied to different existing data sets, but this is not described in the paper, limiting the usefulness of it.\n\nDue to space limitations, we included the sampler details in the Appendix A.2. We have also released the code base.\n\n> Since the authors do not create additional benchmarks following this procedure, the \n> evaluation is only on two data sets, which limits the reader's ability to assess the \n> benefits of the proposed approach. Would it be possible to create \"Roaming\" versions \n> of other synthetic benchmarks? What are the limitations for this?\n\nWe thank you for the suggestion of creating other versions such as a Roaming-ImageNet benchmark. There is no technical limitation of doing this. We would like to emphasize that our main interest is to study realistic online sequences and this is showcased in our RoamingRooms dataset, which resembles a more continuous stream of objects that could be encountered during a real-world sequence.\n\n> The idea of using the recurrent net to output the thresholds used to make decisions of \n> when to detect instances as novel is quite interesting, but I was left lacking an intuitive \n> description of what this should do and how.\n\nThe recurrent net will encode a window of the previous items and use that to decide what kind of threshold should it give for known vs. unknown. For example, if the agent just entered an unfamiliar room (spatial and temporal context), then there is a higher chance that objects are new/unknown here. Also, the RNN can also control the prior belief of things being new at a given time step, since towards the end of the sequence, a greater fraction of the items are known.\n\n**Intro**\n\n> What is a trial?\n\nA trial refers to a single item in the input sequence. We will clarify the terminology.\n\n> This section states that comparisons will be made against few-shot baselines, which made me think that no comparisons would be made against continual learning baselines. Please clarify that you indeed compare against continual learning baselines.\n\nThis is a good point. We consider our setup primarily an extension of few-shot learning, so we feel it is the most appropriate to compare with few-shot learning baselines. However it is somewhere between few-shot and continual learning. So we do in fact compare to both. OML was recently proposed as a meta-continual learning model. And we have adapted it to output unknown classes (OML-U and OML-U++) and compared them. While we would love to compare to more continual learning methods, we need to do some non-trivial amount of adaptation to each, to have a novelty detection output. The length of each online sequence in our setting is also shorter than most continual learning setups.\n\n**Sec 2**\n\n> Last paragraph of FSL has an incomplete sentence, and it remains unclear how Caccia et al. (2020) compares to the submission. Since this is clearly the most closely related work (as evidenced in Table 6), it is very important to make this comparison as complete as possible.\n\nBoth our paper and Caccia et al. (2020) remove the notion of support and query split in each episode/sequence. However, there are still notable differences that prevent us from directly comparing them. First, their setting is not incremental class learning, and instead of detecting new classes, they detect new environments based on the current training loss. Furthermore, since in our setting, models must make  a class prediction before seeing the label; therefore it cannot use training loss to decide whether it is a new class or not. We will make Table 6 clearer w.r.t. these distinctions.", "title": "Response to Reviewer 1 (Part 1)"}, "KZ1dgnAqrNO": {"type": "rebuttal", "replyto": "W4Ku2cMDVPZ", "comment": "Thank you for your comments and suggestions. Below are our responses.\n\n> Numbers on CORe50, OpenLORIS and synthetic task sequences of Omniglot and Tiered-ImageNet.\n\nThank you for the suggestions. CORe50 contains only 50 object instances in total, so the maximum number of classes is 50 before train/val/test split; we worry that it is not enough for few-shot/meta-learning based approaches. As for OpenLORIS, it is not an incremental class learning benchmark. The continual variations across time are: Illumination, Occlusion, Object size, Camera-to-object angles/distances, Clutter. However, we will consider your suggestion of running on Tiered-ImageNet. Conceptually it will be similar to RoamingOmniglot, since in both cases, sequences are sampled from static images.\n\n> [1] proposed a model for a setting very similar to Online Contextualized Few-Shot Learning. Even if this method detects new classes by thresholding the probabilities for novel class detection, it should be used as a baseline method.\n\nWe have studied [1] in detail and we think it cannot be applied in our setting. When we say similar, we mean both papers remove the notion of support+query split in each episode. However, there are still notable differences that prevent us from directly comparing with them. First, their setting is not incremental class learning, and instead of detecting when a class is novel, they detect when an environment is new based on the current training loss. Furthermore, in our setting, a model makes a class prediction before seeing the label; therefore it cannot use training loss to decide whether or not the current instance is a new class. Conceptually, the method proposed in [1] is a simple gradient based meta-learning method, and therefore we would like to refer the reviewer to the OML-U and OML-U++ results in our paper for a comparison for gradient-based meta-learners.\n\n> Clever fine-tuning is competitive, if not the state-of-the-art, for continual learning [2] and few-shot learning [3]. How does this perform as a baseline?\n\nWe thank the reviewer for the suggestion. In our CPM model, fine-tuning the CNN could potentially corrupt old memory entries/prototypes and could introduce catastrophic forgetting. We can also try a pre-trained CNN + store all historical examples + fine-tuning. But we will still need to design a way for the model to predict known vs. unknown at each step, and this makes fine-tuning a less than straightforward baseline.", "title": "Response to Reviewer 3"}, "rqXrDg_MAnd": {"type": "rebuttal", "replyto": "xx3gdmCvL8e", "comment": "**RoamingRooms-Semi-Sup**\n\n|     **Interval** | **1 ~ 2** | **3 ~ 5** | **6 ~ 10** | **11 ~ 20** | **21 ~ 50** | **51 ~ 100** |\n|------------|-------|-------|--------|---------|---------|----------|\n| OPN 1-shot | 86.50 | 83.59 | 76.33  | 68.39   | 64.69   | 61.48    |\n| CPM 1-shot | **91.01** | **88.73** | **82.89**  | **76.99**   | **72.16**   | **66.50**    |\n| OPN 3-shot | 92.60 | 88.03 | 85.08  | 81.07   | 80.64   | 76.67    |\n| CPM 3-shot | **94.82** | **91.04** | **86.89**  | **83.09**   | **82.72**   | **79.17**    |\n\nAs shown above, there is a drop in accuracy if we haven\u2019t seen an item for a long time. However, the time interval is again coupled with the increase of total number of classes. The amount of drop in accuracy is smaller for 3-shot accuracy than 1-shot. In RoamingRooms the drop seems to be larger than RoamingOmniglot, but this might also be due to 1) the training sequences are shorter (100 vs. 150), and 2) the context switching happens less often during training than RoamingOmniglot. In general, we don\u2019t think there is severe catastrophic forgetting in our online few-shot learning experiments. Catastrophic forgetting usually happens when a network adapts the weights and representation. In our model, CPM, the CNN is frozen during test episodes. In theory, CPM could still lead to forgetting due to RNN activation and Gated Averaging Unit, but this is rarely the case as shown in the results above, where it only loses to Online ProtoNet a few times, and wins most of the time due to its ability to model temporal contexts. Therefore, we believe that the drop in accuracy was mostly due to the increase in the total number of classes. \n\nWe will include these additional results in our next version of the paper. \n\n> Table 1 and 2: Why the std of AP metric is not included?\n\nAs explained above, the AP metric is calculated by aggregating all the predictions from all sequences together, so that we can rank all the scores together. And therefore, there is no std for the AP metric. But since we are aggregating 2000 sequences together, and each sequence has around 100 items, the score is very stable and does not fluctuate much.\n\n> Figure 5 caption does not match the layout. Is this 1-shot accuracy? I did not find the description in the paper.\n\nThe standard k-shot N-way few-shot accuracy does not fit neatly into this online setting, and therefore we measure the instantaneous accuracy at each time step, aggregated over 2000 sequences.", "title": "Response to Reviewer 4 (Part 2)"}, "S219z92c1i7": {"type": "review", "replyto": "oZIvHV04XgC", "review": "############## Summary ##############\n\nThis paper presents a new definition of the continual learning problem which attempts to bridge the gap between artificial settings typically used to evaluate continual learning and the way the real world requires us humans to perform. Concretely, this setting involves changing environments, where the agent is expected to autonomously detect when new classes are encountered. The changes in the environment include both spatial and temporal cues to enable the agent to differentiate between environments. Additionally, this work creates a new data set and adapts an existing one to this novel setting, and provides a method that can deal with it, as validated empirically.\n\n############## Strengths ##############\n\n1. The provided evaluation setting is close to what we would want in a continual learning setting.\n2. The provided data sets are potentially useful for evaluating future algorithms.\n3. The presented method is effective in handling this challenging evaluation setting.\n\n############## Weaknesses ##############\n\n1. While the evaluation setting is closer to a human setting, it appears that there is a pre-training stage where the agent prepares for that evaluation setting, which is not described in detail.\n2. The techniques used to create the Roaming version of Omniglot could potentially be applied to different existing data sets, but this is not described in the paper, limiting the usefulness of it.\n3. Since the authors do not create additional benchmarks following this procedure, the evaluation is only on two data sets, which limits the reader's ability to assess the benefits of the proposed approach.\n\n############## Recommendation ##############\n\nI recommend this paper for acceptance. I believe that problem settings that become increasingly realistic is a valuable direction of work, and although this manuscript perhaps oversells how realistic their proposed setting is, I believe it is still a step in the right direction. The introduction of a novel data set on Matterport3D could be impactful to future researchers working in this field, and provides a more realistic benchmark than many existing ones. Finally, the proposed approach is novel and effective.\n\n\n############## Arguments ##############\n\nThe main contribution of this work is to propose a novel problem formulation for continual few-shot learning, which is closer to realistic continual learning. I believe this in itself is interesting. The evaluation setting requires the agent to automatically detect when new classes are encountered, and simultaneously classify objects into existing classes. However, from the experimental section, it appears that the authors' proposed method requires a pre-training phase, where the agent encounters many evaluation scenarios like this one, in order to perform well in the final evaluation scenario. This is never explicitly stated or explained in detail, but if it is the case, it is highly unrealistic, as it requires the agent to essentially live multiple lives, before being able to perform well in a \"test\" life. I would encourage the authors to explain this in more detail, and more honestly assess how realistic their problem formulation is. Even if the pre-training phase is unrealistic, the evaluation phase is still realistic, which is valuable. This should be explicitly stated to make the contributions of the paper clearer. \n\nI very much liked the proposed RoamingRooms data set, and I also believe that to be a valuable contribution. The fact that a similar data set can be created from Omniglot interesting. Would it be possible to create \"Roaming\" versions of other synthetic benchmarks? What are the limitations for this? Since we typically expect continual learning methods to be evaluated in various benchmarks, it would be valuable to see diverse benchmarks for this new setting, or at least the details for how to create them. This could also enable a more comprehensive evaluation of the proposed approach in this submission.\n\nThe idea of using the recurrent net to output the thresholds used to make decisions of when to detect instances as novel is quite interesting, but I was left lacking an intuitive description of what this should do and how.\n\nIn terms of the empirical results, I believe them to demonstrate a substantial margin of improvement with respect to baselines, and the set of baselines to be sufficient. The ablative tests show the usefulness of each part of the proposed approach.\n\n############## Additional feedback ##############\n\nThe following points are provided as feedback to hopefully help better shape the submitted manuscript, but did not impact my recommendation in a major way.\n\nIntro\n- What is a trial?\n- Hyperref to Figure 4 leads to Section 4.\n- This section states that comparisons will be made against few-shot baslines, which made me think that no comparisons would be made against continual learning baselines. Please clarify that you indeed compare against continual learning baselines.\n\nSec 2\n- Last paragraph of FSL has an incomplete sentence, and it remains unclear how Caccia et al. (2020) compares to the submission. Since this is clearly the most closely related work (as evidenced in Table 6), it is very important to make this comparison as complete as possible.\n- Omniglotor --> Omniglot or\n- Very comprehensive and well-written related work section, clearly outlining various lines of related work, including recent (and even concurrent) efforts that handle very similar settings to this one.\n\nSec 3\n- Figure hyperrefs point to captions (below the figure).\n- At this point, the problem formulation should clearly state how training will be carried out. This section does not mention the fact that the agent will first encounter various evaluation sequences and use those to generalize to unseen evaluation sequences. It mentions multiple few-shot sequences and train/val/test splits, but it is largely unclear at this point how these various sequences are supposed to be used.\n- The way the problem setting is presented, it makes it hard to differentiate it substantially from few-shot or continual learning, since it mainly focus on evaluation being done continually. It seems like a crucial part of the distinction is that the agent is evaluated even on unseen classes.\n    - This seems to be closely connected to work on open world and open set learning. Could the authors provide some guidance into how it compares to that setting? [1]\n\nSec 4\n- When doing online average, the method seems to assume that the current prototype p_{t-1} is fixed. But, if the model parameters change over time, which I expect they do in a continual learning setting, this prototype would be different if recomputed. How is this taken this into account?\n\nSec 5\n- I would've liked to see the ablation tests on RoamingRooms as well.\n- My only concern is that the evaluations are only on two different data sets. \n- In terms of the baselines, it is unclear which can leverage previous sequences to learn the new ones. I believe this is only possible with OML and variants, whereas all the other methods can only leverage data from the current sequence. \n\n\nAppendices\n- Thanks for providing lots of details on the experimental setting for baselines.\n- The additional visualizations are useful, especially the one of the learned thresholds over time (Figure 9).\n- These appendices should be referred to in the main text so the reader knows to look for them. Same for the provided videos.\n\n\n[1] Geng, C., Huang, S. J., & Chen, S. (2020). Recent advances in open set recognition: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence.", "title": "A novel problem formulation with a more realistic evaluation setting, a novel data set, and a new method that works well in this setting", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "35oPnOr_ZIO": {"type": "review", "replyto": "oZIvHV04XgC", "review": "#################################\n\nSummary:\n\nThe paper presented a new setting of online contextualized few shot learning to mimic human learning. This setting combines continual learning and few shot learning, and additionally considers context switch. Specifically, a learning method is presented with a sequence of samples that might come with labels. The method is then tasked to classify the current input into known categories, or recognize the input as belonging to a \u201cnew\u201d category, while at the same time updating the model for known and new categories. Two new datasets (hand-written characters and indoor images) were constructed to support the learning setting. An extension of Prototypical Network (Snell et al.) was explored for this new setting. The results were compared against several baselines and were quite promising. \n\n#################################\n\nPros:\n\n* A novel setting of continual few-shot learning that considers context switching. The motivation is well articulated (naturalistic human learning). The setting has great potential to address some of the key challenges in AI (e.g., embodied vision, robotics, etc).\n* New datasets to support the proposed setting. Those dataset might facilitate future research in this direction.\n* The paper explored several baselines for the proposed setting, including an interesting extension of ProtoNet. The experiments are solid and the results are promising.  \n\n#################################\n\nCons:\n\n* The evaluation metric will need some thoughts\n\nProper evaluation metric is a critical component for the proposed learning setting. While the authors did provide a short description of the evaluation metrics (AP and N-Shot accuracy), those metrics lack some details and are not well justified. My understanding is that both metrics are accumulated from the starting to the current step and across all sequences. This was not particularly clear from the text. Also the definition of AP is different from standard average precision (the TP/TN/FP/FN definitions are different). Some more description is needed in the text. \n\nHow do these metrics capture catastrophic forgetting? For example, the accuracy / AP vs. the time interval between the current label and the last time the same label was observed.  \n\n\n#################################\n\nMinor comments:\n\nPage 3 paragraph 2: \u201cfocuses on more flexible ...\u201d not a sentence. \n\nTable 1 and 2: Why the std of AP metric is not included?\n\nFigure 5 caption does not match the layout. Is this 1-shot accuracy? I did not find the description in the paper. \n\n#################################\n\nJustification for score:\n\nA good paper proposing an interesting learning paradigm. I\u2019d expect some more discussion of the evaluation metric. Otherwise, I am happy to accept the paper. \n", "title": "An interesting and new learning setting", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "W4Ku2cMDVPZ": {"type": "review", "replyto": "oZIvHV04XgC", "review": "Summary:\n\nThis work aims to make a realistic learning setting by combining few-shot learning and continual learning in the online setting. Similar to few-shot learning, the model needs to adapt to new classes with a few samples (at least in the beginning). Similar to continual learning, the model needs to learn new classes over time while being tested on the older classes as well. When encountering a new class, the model is expected to recognize that. Similar to the online setting, model evaluation happens on each trial, after which the model can be updated with that data (labeled or unlabeled). This new paradigm is called Online Contextualize Few-Shot Learning.\n\nThe authors recognize the importance of spatio-temporal context in human learning. Building on this, they propose a new dataset, RoamingRooms, that incorporates such context. The authors propose a new method, Contextual Prototypical Memory, to tackle this problem. It makes use of an RNN to encode contextual information and a prototype memory to remember previously learned classes.\n\nPros:\n1. The new setting proposed, Online Contextualized Few-Shot Learning, is a very realistic setting. Few-shot learning misses that classes are repeated over time. Continual learning misses that new classes are not processed and learnt in groups. This new paradigm combines the two settings and improves on their shortcomings to make it more realistic. Additionally, this is all done in an online setting.\n2. The use of spatio-temporal context in creating the dataset and the model is realistic and interesting.\n3. The proposed model is simple, with components added specifically to make use of the additional information in the dataset (the RNN) or to output additional information required for the task (the new class detection branch).\n\nCons:\n1. Authors mention that there exist some datasets under a very similar setting, namely CORe50, OpenLORIS, and synthetic task sequences of Omniglot and Tiered-ImageNet. If something similar does exist, the authors should report numbers on these datasets rather than RoamingOmniglot.\n2. Authors mention that [1] proposed a model for a setting very similar to Online Contextualized Few-Shot Learning. Even if this method detects new classes by thresholding the probabilities for novel class detection, it should be used as a baseline method.\n3. Clever fine-tuning is competitive, if not the state-of-the-art, for continual learning [2] and few-shot learning [3]. How does this perform as a baseline?\n4. Average precision is used as the metric for this setting. What about the maximum F-1 scores?\n\nNotes:\n1. There is a lot going on in this paper. The writing can be made less redundant and more to the point to incorporate more details.\n\n[1] Massimo Caccia et al. Online Fast Adaptation and Knowledge Accumulation: A New Approach to Continual Learning.\n[2] Hang Qi et al. Low-Shot Learning with Imprinted Weights.\n[3] Guneet S. Dhillon et al. A Baseline for Few-Shot Image Classification.", "title": "New realistic learning paradigm", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}