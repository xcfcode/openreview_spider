{"paper": {"title": "Symbol-Shift Equivariant Neural Networks", "authors": ["David Salinas", "Hady Elsahar"], "authorids": ["~David_Salinas1", "~Hady_Elsahar2"], "summary": "We define a class of model whose outputs are equivariant to entity permutations without having to specify or detect such entities in a pre-processing step.", "abstract": "Neural networks have been shown to have poor compositionality abilities: while they can produce sophisticated output given sufficient data, they perform patchy generalization and fail to generalize to new symbols (e.g. switching a name in a sentence by a less frequent one or one not seen yet). In this paper, we define a class of models whose outputs are equivariant to entity permutations (an analog being convolution networks whose outputs are invariant through translation) without requiring to specify or detect entities in a pre-processing step. We then show how two question-answering models can be made robust to entity permutation using a novel differentiable hybrid semantic-symbolic representation. The benefits of this approach are demonstrated on a set of synthetic NLP tasks where sample complexity and generalization are significantly improved even allowing models to generalize to words that are never seen in the training set. When using only 1K training examples for bAbi, we obtain a test error of 1.8% and fail only one task while the best results reported so far obtained an error of 9.9% and failed 7 tasks.\n", "keywords": ["compositionality", "Symbolic", "Equivariance", "question answering", "Language Processing"]}, "meta": {"decision": "Reject", "comment": "This paper proposed a  new type of models that are invariant to entities by exploring the symbolic property of entities. This problem is important in language modeling since it gives intrinsically more proper representation of sentences, which can better generalize to new entities.  However I still suggest to reject this paper for the following reasons \n1. The description of model is not clear enough which can certainly use a serious round of revision.\n2. The experiments on bAbi is not convincing enough since it is an overly simple and toyish data-set with many ways to hack\n3. Similar entity-invariant idea has been explored long time ago by  (https://arxiv.org/pdf/1508.05508.pdf) which attempted to represent entities as \u201cvariables\u201d\n\n"}, "review": {"XNt1--9TU1M": {"type": "rebuttal", "replyto": "4JhpCp8C6Wg", "comment": "We have uploaded a new version of the manuscript with some of your recommendations to enhance the clarity of Section 4", "title": "Updated manuscript based on AnonReviewer2 recommendations"}, "jzebDRRnSL2": {"type": "rebuttal", "replyto": "4JhpCp8C6Wg", "comment": "Thank you for the useful suggestions.\n\nTo answer to your specific points:\n\n* we would use your suggestion and refer more to Figure 3 to give the intuition of the projection matrix $B_\\varphi$\n* we would like to point that $B_\\varphi$ is not squared,  $B_\\varphi \\in \\mathbb{R}^{n\\times m}$ (its dimension is indicated in \"Mapping words into and from symbolic representations\")\n* we appreciate your suggestion regarding the presentation of the symbolic embedding (we would have to rename it as $e_x$ is already used in the semantic embedding $A e_x$.)\n* Yes, $\\alpha_x$ is the output of a sigmoid unit, we will add this mention in the paper. ", "title": "answer"}, "Q2U0TQS3-ox": {"type": "rebuttal", "replyto": "zEtb74vbLS", "comment": "Thank you for following-up. We updated the manuscript version with feedback from Reviewer 2 and some of your feedback. We agree that there is a clear consensus of the reviewers in some aspect of our presentation that we will work on clarifying.\n\n* We regret that the assumptions of the theorem were not clear and will make it clearer in the context. While the theorem states that \"SMN and STPR are symbol-shift equivariant.\", where symbol-shift equivariant definition implies that $A_{\\tau_{(i)}} = A_i$, it is clear to us now that this should be better highlighted and explained.\n\n> \"There's a claim made in the introduction -- \"we rely solely on differentiation to determine whether a word acts like an entity\". I have not seen any follow up on this claim. \"\n\nThis refers to the parameter $\\alpha_x$ that is learned per word, for instance in Section 4: \"where $\\alpha_x$ is ... a learnable parameter, $0< \\alpha_x<1 $, that indicates how much each word should behave as a symbol\".\n\n> \"$A_{\\tau_{(i)}} = A_i$ is a huge assumption. In fact, the model is equivariant without even requiring the symbolic part at all.\".\n\nRegarding the assumption, we believe the problem comes mainly from our presentation and we will make this point clearer. The assumption $A_{\\tau_{(i)}} = A_i$ is not needed for our model (as shown in the experiments where this property obviously does not hold), it is needed only in our proof and symbol-shift definition.\n\nThe reason why we use this strong assumption is to be able to restrict the class of possible word permutations without knowing entities groups: clearly, there must be some restriction as we cannot expect the meaning of sentences to be preserved if we allow permuting \"John\" and \"the\" for instance. The classic definition allows permutation only between words of the same group but this imposes knowing the group while our definition of symbol-shift avoids this need.\n\nRegarding the fact that a model with $A_{\\tau_{(i)}} = A_i$ with no symbolic will be equivariant: this is true but such a model would not be able to distinguish entities in the same group.\n\nTo summarize, the assumption $A_{\\tau_{(i)}} = A_i$ is only required to investigate theoretical properties of the model. We will add experiments with another dataset to show that the approach proposed can also work out of bAbI in real-world settings.\n\n", "title": "Updated the Manuscript to improve the writing"}, "ie9TFHqrZC": {"type": "rebuttal", "replyto": "dsmZWVGCyAS", "comment": "We agree vocab-id and position-id are standard (in fact the two baselines we consider use vocab-id and position-id). However, here we use the *order of appearance* in the context which is not standard (in particular the way we propose to project such representation to the vocabulary). \n\nNote that positional encoding are discussed in related work and in particular the difference with the approach we propose (\"While  positional  encoding  (Graves  et  al.,  2014;  Vaswani  et  al.,  2017)  may give some compositionality ...\").\n\nWe will add an illustration of the word embedding (e.g. a matrix with $n \\times d$ where the $i$-th row represents John and has coordinates [1, -1]).\n\nRegarding, the illustration of restricting valid permutation, we refer to Figure 2 which is illustrating the concept. \n\n", "title": "Answer"}, "l1HAr7rkbW6": {"type": "rebuttal", "replyto": "rtFrRWxONlX", "comment": "First of all, thank you for your comment and following-up.\n\nYou are right that if a word is not seen during training, the model cannot infer properties such as gender or POS. We would like to point out that the description of the method does not claim that it can restrict possible permutations. The definition of possible permutations (symbol-shift in the paper) is only required to define the class of symbol-shift equivariant models: e.g. the class of model whose outputs is equivariant to possible permutations of symbols. While the definition is restrictive (having semantic vectors equals for different groups of entities is a stringent condition), it is helpful in our view as it allows to restrict to permutations that does not alter syntax (e.g. preventing permuting \"John\" by \"Who\"), it is also helpful as we can prove that some models are indeed equivariant under those conditions.\n\nThe semantic word properties (gender, POS, ...) are learned in a standard way (as noted to R1 if $\\alpha_x = 0$ for all words, the model symbolic component vanishes). For instance, if the model sees \u201cJohn\u201d sufficiently many times, the model learns the word properties with its semantic vector. If the word is never seen during training, our experiments on zero-shot show that the model can still generalize by leveraging the symbolic representation (for words that does not appear in the training, semantic vectors are set to zero).\n\nRegarding the terminology, we agree that using \u201csymbolic\u201d is not standard but given that we are introducing a new type of equivariant representation, it is unclear to us if there would be a standard terminology (we would appreciate any suggestion). We choose to call \"symbolic representation\" parts of the model that are equivariant when permuting symbols (e.g. when applying symbol shift). Note that this does not contain *just* the one-hot of the order of appearance in the context but also how this one-hot representation can be projected back to the vocabulary (again preserving the equivariance property) and how this representation can be transformed.\n", "title": "answer to comment"}, "5VZxZOSYEgQ": {"type": "rebuttal", "replyto": "5Y8sB-Z26HA", "comment": "Thank you for your review.\n\nTo answer your questions:\n1. The only free-parameter of the symbolic representation $\\alpha_x e_\\varphi(x)$ is $\\alpha_x \\in [0, 1]$ given that $e_\\varphi(x) \\in \\mathbb{R}^m$ is set to the one-hot vector of the order of appearance of $x$. With the parameter $\\alpha_x$, the model can learn if a word $x$ should behave as a symbol or not.  \n2. In our definition of symbol-shift, permutation of symbols must happen without changing semantic embeddings so that \u201cbanana\u201d and \u201capple\u201d can be changed only if they share the same semantic word vector. This is a theoretical requirement which allows us to prove formally that the models we introduce are equivariant under symbol shift.\nHowever, in practice as you mention we cannot expect the semantic vector of two entities of the same group to be exactly the same, they would be close but different. Our claim is that the added inductive bias helps in practice, even if the theoretical requirement does not hold, which we aimed to demonstrate in our experiments as the models can generalize to a larger number of entities (for instance in the experiment of Fig 1, the semantic vectors of all entities are close but different), to unseen one and also converges faster than models that do not have the symbolic representation we propose.\n\nAbout your other points on notations, we will do our best to simplify them and we are grateful for your feedback. We agree that the symbolic vector can be described in simple words, however it is a bit more difficult to express the projection of a symbolic vector to the vocabulary in words without equations.\n\nPlease let us know of any other questions you may have.\n", "title": "Answer to Reviewer 2"}, "u0OEM78INz": {"type": "rebuttal", "replyto": "vR4_t_lmP5S", "comment": "Thank you for your review. \n\nThere seems to be an important misunderstanding that we would like to clarify. We do not assume that we have \u201cdomain knowledge of word/entity type equivalences\u201d. This is in fact the main motivation and contribution of this paper: having compositionality and symbol abstraction **without** having to specify/detect entities in advance. \n\nWe hope you can adapt your review as the point you raised is the main motivation and contribution of the paper (as stated in the abstract \u201cwe define a class of models whose outputs are equivariant to entity permutations *without requiring to specify or detect entities* in a pre-processing step\u201d or in the introduction \u201cThe main advantage and novelty of our approach is that *entities are not required to be identified in advance* as we rely solely on differentiation to determine whether a word acts like an entity\u201d). \n\nIn case there is one sentence that is misleading and indicates that we are assuming entities equivalences are given, we would really appreciate it if you could point us to it. \n\nWe can add the references you mentioned but we would like to point-out that we already highlighted the fact that replacing entities by token helps compositionality in our related work section (\u201ccompositionality becomes much easier if symbols (or entities) are detected before-hand. For instance, [Li2015] showed that replacing entities by dedicated token placeholders leads to significant improvement in question answering.\u201d).\n", "title": "Clarification on the fact that no domain knowledge is assumed"}, "jE56DZiUXsL": {"type": "rebuttal", "replyto": "nJ1dB0KSpcK", "comment": "Thank you for the review.\n\n\u201cI don't understand how the model is able to determine that \"apple\" and \"orange\" have the same embedding while \"apple\" and \"John\" have different embeddings.\u201d\n\nThe model learns the semantic embeddings in the same way as a standard model (in fact, if $\\alpha_x = 0$ for all word $x$, the model will reduce to a \u201csemantic\u201d model). Thus the model is able to learn similar embeddings to \u201corange\u201d and \u201capple\u201d that would be close but different (for instance if semantic embedding encodes color) the same way than any word embedding model relying on differentiation. While we assume that vectors of entities in the same group are equal, this is only required to prove formally that models are equivariant. In practice as noted by R2, the semantic vectors are only *close* but not equal between entities of the same group. However, our claim is that this additional inductive bias still allows the model to generalize as seen in our experiments where \u201csymbolic\u201d models are able to learn with large number of entities or unseen ones even if the condition of the theorem does not strictly apply.\n\n In regards to your specific points:\n* $n$ is the number of words in the vocabulary, it is introduced in Section 3 paragraph 2, we will recall its definition in Section 4 to ease readability.\n* We did a typo when indicating the dimension of  $B_\\varphi$ which is $B_\\varphi \\in R^{n \\times m}$\n* $\\alpha_x$ is the $x$-th component of $\\alpha \\in [0, 1]^n$ and hence $\\alpha_x \\in \\mathbb{R}$. $e_{\\varphi(x)} \\in R^m$ as it is the one-hot vector of the order of appearance of $x$ in the context (which has $m$ words). \n\nWe hope this clarifies the points raised, please let us know of other questions you may have.\n", "title": "Answer to Reviewer 1"}, "cy9owemQt8U": {"type": "rebuttal", "replyto": "whAxkamuuCU", "comment": "We would like to thank all the reviewers for their work and for highlighting the merits of our paper in particular:\n- having compositionality without specifying entities in advance in Neural Network which is a well motivated and a challenging problem (R1, R2)\n- Significant performance gains of the symbolic models and especially in the 1K setting  (R1,R2)\n\nWe would like to point out that the criticism of R3 (that we need to detect entities in advance which is not novel) is a misunderstanding. We do not assume entities or any domain knowledge to be specified in advance. This is in fact the main contribution of our approach: to provide some compositionality without requiring such information.\n\nWith respect to proposed enhancements, two reviewers ask for model clarifications that we answered in our comments, we will update our manuscript in the next days to reflect those points. In addition, we share the code privately (that we intend to release upon publication) in the hope it can also bring some clarification.\n", "title": "General answer to reviewers."}, "5Y8sB-Z26HA": {"type": "review", "replyto": "whAxkamuuCU", "review": "**Summary**.\nThis paper proposes a new type of models that are equivariant to entity permutations, which is an important criterion to build language models that can easily generalize to new entities. The authors modified a Memory-Network and a Third-order tensor product RNN to make them symbolic-shit invariant. The new models were evaluated and compared on the 20 bAbi tasks. Results show that the symbolic versions of the models yield better performance than the original ones.\n\n**Positives**.\nThe topic is of great interest and it is indeed crutial that neural language models become symbol-shift invariant to allow them to better generalize. This work is clearly motivated.\n\n**Confusions**.\nThe beginning of Section4 mentions that the main idea of this work is to concatenate a regular \"semantic\" word vector with a \"symbolic\" representation essentially corresponding to a one-hot vector of the token order of appearance.\nIn the following paragraphs, the work presented lacks clarity and seems to over-complicate concepts with hard-to-follow math notations. For instance, the \u201c*Mapping words into and from symbolic representations*\u201d paragraph introduces tedious math notations to describes something simple that was clear before, namely, the mapping from tokens to their respective symbolic vector, which is simply defined as the one-hot vector position appearance of this token in the context.\nSimilarly, the \"*Hybrid semantic-symbolic embeddings*\" paragraph uses again tedious math notations to describe how semantic and symbolic embedding are concatenated.\n\nGiven the confusion presented in Section4, it is currently not clear how adding a one-hot vector to the input embedding can make a neural model symbol-shift equivariant.\nIn particular, below are the two things I could not understand:\n1) The paper mentions that \"*all parameters are differentiable*\". It is not clear if that also includes the symbolic representation or not? If so, then the initial one-hot vector may not be a one-hot vector after the gradient updates performed during training, which would result in a non-symbolic representation? if it is kept fix during training, then it is not clear how it is used by the network.\n2) In addition, assuming that the symbolic representation of all tokens stays the same during training, I don't see how \"_permuted symbols share the same latent representations_\" if the latent representations are made of both on-hot vectors **and** regular word vectors. I understand that the symbolic representation does not change for a permuted word since it will appear at the same place as the original word. But the semantic representation will be different. For instance, the semantic word vector of \u201cbanana\u201d is similar but still different than the word vector of \u201capple\u201d.\n\n**Conclusion**.\nI would suggest the authors to simplify their mathematical notation and make their paper easier to read. As of now, I could not fully understand the paper and unfortunately for that reason could only put a score of 4 with a low confidence of 2.", "title": "Confusing", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "vR4_t_lmP5S": {"type": "review", "replyto": "whAxkamuuCU", "review": "This work proposes to improve the generalizability of bAbi models through [entity permutations].\nMore specifically the approach assumes domain knowledge of word/entity type equivalences, which helps restricting possible permutations between word POS (e.g., \u201cJohn\u201d vs \u201cwhy\u201d) or gender (e.g., \u201cJohn\u201d vs \u201cMary\u201d). Each word type has its own embedding param and is concatenated with normal word embeddings to form the final word representation. Experiment with memory networks and Third-order Tensor Product RNN shows that the proposed approach indeed enables the models (especially TPR) to handle artificial data sets with large number of entity names. \n\nOverall I find the proposed research not very well motivated. Leveraging word type knowledge to improve the generalizability of NLP models has been a popular and effective approach. Commonly used strategy is to replace named entities in sentences with their word type tokens . e.g., from [how old is Obama] to [how old is PERSON]\nhttps://arxiv.org/abs/1601.01280\nhttps://arxiv.org/abs/1611.00020\nThe proposed approach seems to achieve a similar effect, but is a lot more complex.\n", "title": "not very well motivated", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nJ1dB0KSpcK": {"type": "review", "replyto": "whAxkamuuCU", "review": "The authors propose a network that is equivariant to entity permutations without requiring the pre-specification of the set of entities. To this end, the authors propose a hybrid semantic-symbolic embedding which they integrate into two QA models. Finally, the authors show significant gains on the bAbi tasks, with especially impressive gains in the 1K setting. \n\nThe problem is quite interesting and challenging in the setting where entities are not prespecified.\nHowever, given the model description it is not clear at all how the model is able to learn a symbol-shift equivariant embedding. \nI don't understand how the model is able to determine that \"apple\" and \"orange\" have the same embedding while \"apple\" and \"John\" have different embeddings. \nWhat is the loss/model architecture/data augmentation guiding this? How is the model able to figure out that \"John\" and \"Sasha\" share embedding?\n\nApart from the high level details, I don't understand the following notations and operations:\n* In Section 4, what is $n$? Is it total number of words in the sequence?\n* If $B_\\varphi \\in R^{m \\times n}$  and $e^m_j \\in R^m$, the multiplication $B_\\varphi e^m_j$ doesn't make sense.\n* How exactly is $\\alpha_x e_{\\varphi(x)} \\in R^m$?  What exactly is $\\alpha_x$ and what is it's shape? \n\nThe notation and the working of the model is not clear to me, hence, I am giving a low rating for now.\nApart from this I also doubt the proposed method's generalizability beyond toy settings.\n\n\n\n\n \n", "title": "Equivariant Networks for NLP", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}