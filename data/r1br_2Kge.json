{"paper": {"title": "Short and Deep: Sketching and Neural Networks", "authors": ["Amit Daniely", "Nevena Lazic", "Yoram Singer", "Kunal Talwar"], "authorids": ["amitdaniely@google.com", "nevena@google.com", "singer@google.com", "kunal@google.com"], "summary": "For sparse boolean inputs, Neural Networks operating on very short sketches can provably and empirically represent a large class of functions.", "abstract": "Data-independent methods for dimensionality reduction such as random projections, sketches, and feature hashing have become increasingly popular in recent years.  These methods often seek to reduce dimensionality while preserving the hypothesis class, resulting in inherent lower bounds on the size of projected data.  For example, preserving linear separability requires $\\Omega(1/\\gamma^2)$ dimensions, where $\\gamma$ is the margin, and in the case of polynomial functions, the number of required dimensions has an exponential dependence on the polynomial degree.\n                                                                                                                                                            \nDespite these limitations, we show that the dimensionality can be reduced further while maintaining performance guarantees, using improper learning with a slightly larger hypothesis class. In particular, we show that any sparse polynomial function of a sparse binary vector can be computed from a compact sketch by a single-layer neural network, where the sketch size has a logarithmic dependence on the polynomial degree.\n                                                                                                                                                           \nA practical consequence is that networks trained on sketched data are compact, and therefore suitable for settings with memory and power constraints. We empirically show that our approach leads to networks with fewer parameters than related methods such as feature hashing, at equal or better performance.", "keywords": ["Theory"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "The reviewers present a detailed set of concerns regarding the paper. In particular, the paper lacks comparison to other sketching works. The sketches used in the paper are rudimentary and in practice, there are more sophisticated sketches employed.\n \n Sketching has a different goal from function approximation. Sketching aims to reconstruct (with a certain error). The paper uses sketching in a naive manner, in the sense that the error due to the sketch is incorporated into the overall error. But presumably, networks can be compressed without achieving reconstruction on each instance, while maintaining a guarantee on the overall accuracy. I would find such a framework to be more interesting and practically relevant.\n \n On the other hand, it is important for the ICLR community to be exposed to sketching frameworks. Hence, I would like to invite the paper to be presented in the workshop, in the hope that it can spur further ideas."}, "review": {"HkNX9xPVl": {"type": "rebuttal", "replyto": "HyP8A4VEe", "comment": "The reviewer seems to be pointing to the improper learning aspect of our work. Recall that improper learning is the idea of learning a function f from a hypothesis class H by using a function g from a larger hypothesis class H\u2019. This approach is often used to make learning more efficient computationally. From a learning point of view, the primary motivation for restricting f to be from H in the first place is to get good generalization, and using an H\u2019 that is not \u201cmuch larger\u201d suffices for this purpose. It is very rarely the case that g not being in H is an issue. Most previous applications of improper learning allow us to get around NP-hardness results and get polynomial time learning. In our setting, we use it to get faster running times.\n\nIn our experiments, we could have added more constraints to make our H\u2019 even smaller as the reviewer suggests. We view the fact that we did not have to do that as a feature: the simplicity allows us to deploy our sketching approach with little changes to modern learning infrastructures. Our theorems hold for exactly the setting that we use in the experiments: the theorems show that H\u2019 contains (good approximations to) every f in H. They do not require that H\u2019 does not contain anything else.", "title": "reply to AnonReviewer2 comment"}, "HkPbtmzNg": {"type": "rebuttal", "replyto": "B1IqQJhzx", "comment": "We performed additional experiments comparing sketching the input to using as a MinHash as an input layer. We generated synthetic datasets for sparse linear and polynomial regression using settings similar to those described in Section 6. Denote by k be the number of non-zero features per example. We reduced dimensionality using the following settings:\nSketches: 2^3 blocks of size 2^3 * k\nMinHash: 2^3 * k blocks of size 2^3 (3 bits)\nThis resulted in the two representations having equal size (2^6*k) and equal number of nonzero entries (2^3*k).  We used an efficient implementation of MinHash using hash functions, as described in https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/a13-li.pdf. We used 4-universal hash functions for both MinHash and Sketches. Note that MinHash is more expensive to compute, as it scales as k^2, whereas Sketches are linear in k.  We found that Sketches substantially outperform MinHash. \nOn the linear regression task (k=50, s1=50, |I|=50, 25 \u201cnoise\u201d features per example, d=10K), MinHash mean squared error (MSE) was 1.39 (0.35), whereas the MSE with Sketches was 0.23 (0.05), over 50 generated datasets. \nOn the polynomial regression task (k=100, s2=25, s3=25, |I|=50, 50 noise features per example), the MinHash MSE was 0.28 (0.42) and the Sketches MSE was 0.056 (0.014). \nWe plan to report a more exhaustive comparison in the next version of the paper. ", "title": "Some experiments comparing Sketches and MinHash"}, "rk_iu7GVe": {"type": "rebuttal", "replyto": "BksOoLpXg", "comment": "The theoretical contribution of our paper is to show that neural networks can learn a sparse linear or polynomial functions from smaller sketches than Lasso can. For this reason we sometimes refer to our approach as improper learning. While the data-generating model may attain the best performance on the original training data, it is not necessarily the best choice for sketched inputs, as we also demonstrate empirically in Fig 2 (center). As the reviewer points out, the existence of a network does not convey anything about how difficult it is to learn such a network. Our empirical results are positive, but we provide no theoretical guarantees beyond the formal reconstruction guarantees. We do not understand the comment on generalization as the results of, for instance Bartlett, are applicable in our setting as well. \n\nOur experiments do include an NLP task with a very large vocabulary (entity type tagging), and we do in fact compare our approach to using the original features (mapped to 32bit integers and hashed). We demonstrate that sketches can achieve comparable accuracy, with significant savings in the number of parameters (see Figure 3 right). Training sketches may take slightly longer than original data, since the number of non-zero features per example is increased by a factor of #blocks.", "title": "Reply to AnonReviewer3 review"}, "HydN_QfEe": {"type": "rebuttal", "replyto": "r1EWX_eNl", "comment": "We would like to emphasize that our main results pertain to existence of neural networks that can implement a \u201csketch decoding\u201d function. The particular network we describe is just one way of implementing such a function, and there exist other possibilities, including networks with negative weights (and even different non-linearities). Thus we did not consider it necessary to explicitly enforce non-negativity and block-sparsity. This setting is called improper learning as first described by Valiant over 20 years ago in his seminal on PAC learnability. We hope this clarifies our experimental setting.", "title": "Reply to AnonReviewer2 review"}, "HkMxh0WXl": {"type": "rebuttal", "replyto": "By_kXmRfx", "comment": "We thank the reviewer for the constructive comments and for the references to the papers on fast random projections and spherical random features. We will discuss these works in the updated version, and also clarify the terms proper and improper learning\n\nAll our results, much like all results on projections in learning that we cite, are for any single fixed w. Corollary 3.2 talks about a single x at a time. Thm 4.1 and 4.2 need the \u201cuniformity\u201d in the sense that the same neural network parameters work simultaneously for (almost) all x. The reviewer is correct that uniform guarantees should be possible with k log d sized sketches. For our requirement, we need not just a small sketch, but also a decoding step that can be implemented by a low depth neural network. To our knowledge, the known algorithms in the compressed sensing literature that give for all guarantees with sparse sketching matrices, such as EMP, SMP and l_1 minimization, require larger depth, as they are iterative. In appendix C in our paper, we present a deterministic sketch that gives a for all guarantee. For the simpler sketch, a for-all type guarantee is not true unless we increase the sketch size to about k^2.\n\nIn both the sparse linear functions case and the sparse polynomial functions case, we learn all parameters including the matrix V. This is necessary because we do not a priori know the support of w, and hence do not know which bits need to be decoded. Decoding all bits would negate the advantages of projection, which is why we need to learn V.", "title": "Response on uniformity"}, "S1bBf0-Qg": {"type": "rebuttal", "replyto": "B1IqQJhzx", "comment": "We thank the reviewer for the constructive comments and for the references to the min-hashing papers. We plan to discuss these points and related work in the updated version. \n\nMin-hashing is known to work well for similarity detection when the similarity function of interest is the Jaccard distance. The b-bit min hashing used in citation (1) above shows that they work also for linear regression problems. As pointed out by the authors of (1), it does not work well as a dimension reduction technique since the dimension of the final projection is 2^b k. Since b needs to be about 8 to 10 in order to get good performance, the experiments in (1) end up using, e.g. a projected dimension of 51k for the webspam dataset.\n\nAdditionally, as the reviewer points out, it is not clear whether minwise hashing allows for reconstruction or for polynomial approximation. The limited results in (5) on the polynomial case have an exponential dependence on the degree just like other previous work. We view one of the main contributions of our work to be the drastic provable reduction in sketch size in the polynomial case, that results from the fact that we use improper learning.\n\nIn light of the above, we believe our approach has significant benefits over b-bit min-hash. In the next revision, we will also add an experimental comparison to it.", "title": "Thanks for the suggestions."}, "By_kXmRfx": {"type": "review", "replyto": "r1br_2Kge", "review": "The paper presents some nice ideas on learning over sketched data.\n\n- Corr3.2 is stated for a fixed x and w. Th.4.1 and Th 4.2 are stated uniformly for all x , I would expect a dependency of m and t on \n k log(d/k) to be uniform on B_{d,k}. same thing for w . or the statement in Th 4.1 etc  should be like corr 3.2 , meaning not for all x and all w?\n\n- Sparse linear function: If I understand correctly matrix V is a binary matrix with fixed weights and is not learned, then there is a weight w , that is learned, under a regression or classification cost.  so it seems the paper learns a linear function and not a neural network. N_{Relu} is a deterministic implementation of D^{AND}? or is V learned also in the experiments by back propagation?\n\nMany citations on work on random projections and polynomial kernel approximations are missing and worth the comparison theoretically and practically for instance:\n\n*Random Projections for Linear Support Vector Machines \nhttps://arxiv.org/pdf/1211.6085v5.pdf\n\n* Spherical Random Features for Polynomial Kernels\nhttps://papers.nips.cc/paper/5943-spherical-random-features-for-polynomial-kernels.pdf\n\n- Experiments:  would be interesting to try other datasets than AG news , such as Amazon reviews, to see how m and t depends on the training size and vocabulary size (dimension) in an NLP application. \nalso might be interesting to have some bounds  for m and t taking in account training size and vocabulary size.\n\n- Minor comments:\n proper learning and improper learning are not defined in a clear way in the paper.\n\nSummary of the paper:\n\nThe paper introduces sketches that approximates linear and sparse polynomials on binary data. The paper shows that such sketches can be represented as a one layer neural network. Experiments are conducted on some language processing tasks. \n\nNovelty:\n\nApproximation for linear functions and polynomial have been studied in previous work, hashing here is learned as a neural network which is novel. Although I have some reserves on the connexion between the theory presented and the learning part in the paper (Please see comments below).\n\nThe main concern is that the  single layer neural network that is  learned  is not restricted to the class of  hash functions studied, hence it is hard to know if we are really testing the hashing class or just a one layer neural network.\n\nClarity: \n\nThe paper is  written in a clear way.\n\nComments :\n\n- The link between neural network and the hashing stems from the implementation of the 'and operation' with a relu and a binary weight V and a bias 1-t. V encodes the support of the linear weight for instance in the linear model. \nWhen Learning V it is regularized with l_1 norm to encourage sparsity, nevertheless the weights are not restricted to be positive to be faithful to the theory introduced in the paper. When learning V by back propagation when can try projected gradient to restrict V values to be in [0,1]. On a synthetic example where the support of w is known it would be interesting to see if V recovers the support of w. \n\n- Similarly  for polynomial seems group sparsity is a good regularizer, difficulty being here that the groups are unknown. Many works have tackled that issue see for instance http://www.di.ens.fr/~fbach/IEEE_TSP_shervashidze_bach_2015.pdf\n\n- The experiments are limited to small sets.\n\n", "title": "Uniformity of the results in Corr 3.2 Theorem 4.1,4.2 and 5.1", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1EWX_eNl": {"type": "review", "replyto": "r1br_2Kge", "review": "The paper presents some nice ideas on learning over sketched data.\n\n- Corr3.2 is stated for a fixed x and w. Th.4.1 and Th 4.2 are stated uniformly for all x , I would expect a dependency of m and t on \n k log(d/k) to be uniform on B_{d,k}. same thing for w . or the statement in Th 4.1 etc  should be like corr 3.2 , meaning not for all x and all w?\n\n- Sparse linear function: If I understand correctly matrix V is a binary matrix with fixed weights and is not learned, then there is a weight w , that is learned, under a regression or classification cost.  so it seems the paper learns a linear function and not a neural network. N_{Relu} is a deterministic implementation of D^{AND}? or is V learned also in the experiments by back propagation?\n\nMany citations on work on random projections and polynomial kernel approximations are missing and worth the comparison theoretically and practically for instance:\n\n*Random Projections for Linear Support Vector Machines \nhttps://arxiv.org/pdf/1211.6085v5.pdf\n\n* Spherical Random Features for Polynomial Kernels\nhttps://papers.nips.cc/paper/5943-spherical-random-features-for-polynomial-kernels.pdf\n\n- Experiments:  would be interesting to try other datasets than AG news , such as Amazon reviews, to see how m and t depends on the training size and vocabulary size (dimension) in an NLP application. \nalso might be interesting to have some bounds  for m and t taking in account training size and vocabulary size.\n\n- Minor comments:\n proper learning and improper learning are not defined in a clear way in the paper.\n\nSummary of the paper:\n\nThe paper introduces sketches that approximates linear and sparse polynomials on binary data. The paper shows that such sketches can be represented as a one layer neural network. Experiments are conducted on some language processing tasks. \n\nNovelty:\n\nApproximation for linear functions and polynomial have been studied in previous work, hashing here is learned as a neural network which is novel. Although I have some reserves on the connexion between the theory presented and the learning part in the paper (Please see comments below).\n\nThe main concern is that the  single layer neural network that is  learned  is not restricted to the class of  hash functions studied, hence it is hard to know if we are really testing the hashing class or just a one layer neural network.\n\nClarity: \n\nThe paper is  written in a clear way.\n\nComments :\n\n- The link between neural network and the hashing stems from the implementation of the 'and operation' with a relu and a binary weight V and a bias 1-t. V encodes the support of the linear weight for instance in the linear model. \nWhen Learning V it is regularized with l_1 norm to encourage sparsity, nevertheless the weights are not restricted to be positive to be faithful to the theory introduced in the paper. When learning V by back propagation when can try projected gradient to restrict V values to be in [0,1]. On a synthetic example where the support of w is known it would be interesting to see if V recovers the support of w. \n\n- Similarly  for polynomial seems group sparsity is a good regularizer, difficulty being here that the groups are unknown. Many works have tackled that issue see for instance http://www.di.ens.fr/~fbach/IEEE_TSP_shervashidze_bach_2015.pdf\n\n- The experiments are limited to small sets.\n\n", "title": "Uniformity of the results in Corr 3.2 Theorem 4.1,4.2 and 5.1", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJgF0uizl": {"type": "rebuttal", "replyto": "BJXtnlsGx", "comment": "Thanks Olivier!\nYou are right that as stated, our definition of pairwise independent hash functions could be clearer. The probability in the definition (and in the usage) is taken over sampling a random hash function from the set, and we will make this explicit in the next version. Thanks also for pointing out the other typos. We will fix them as well.\n", "title": "Thanks"}, "BJXtnlsGx": {"type": "rebuttal", "replyto": "rkTkPv5zg", "comment": "Sorry, I believe I just had misunderstood the definition of a pairwise independent distribution. I would just suggest to make it a bit clearer, for instance something like this: \"a distribution of hash functions from [d] to [m] is pairwise independent if (..), where the probability is taken over h sampled from this distribution\".\n\nAnd while I'm here, just a couple typos I noticed:\n- At end of proof of th. 3.1 there is the expression Pr[Y_hj(i) != xi], which is missing a j in the subscript (see eq.1)\n- In th. 4.2 the probability should be over G, not h_1:t", "title": "Never mind!"}, "rkTkPv5zg": {"type": "rebuttal", "replyto": "r1br_2Kge", "comment": "Hi,\n\nIt seems to me something's missing in the definition of a \"pairwise independent set of hash functions\" in order to prove Theorem 3.1. The definition above the theorem says nothing about the relationship between two sampled hash functions, but if they are correlated then the conclusion may not hold (the extreme case would be if they are all equal, which is not forbidden as far as I can tell). Could you please clarify?\n\nThanks!", "title": "Question on Theorem 3.1"}}}