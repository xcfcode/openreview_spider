{"paper": {"title": "Point Cloud GAN", "authors": ["Chun-Liang Li", "Manzil Zaheer", "Yang Zhang", "Barnab\u00e1s P\u00f3czos", "Ruslan Salakhutdinov"], "authorids": ["chunlial@cs.cmu.edu", "manzilz@cs.cmu.edu", "yz6@andrew.cmu.edu", "bapoczos@cs.cmu.edu", "rsalakhu@cs.cmu.edu"], "summary": "We propose a GAN variant which learns to generate point clouds. Different studies have been explores, including tighter Wasserstein distance estimate,  conditional generation, generalization to unseen point clouds and image to point cloud.", "abstract": "Generative Adversarial Networks (GAN) can achieve promising performance on learning complex data distributions on different types of data. In this paper, we first show that a straightforward extension of an existing GAN algorithm is not applicable to point clouds, because the constraint required for discriminators is undefined for set data. We propose a two fold modification to a GAN algorithm to be able to generate point clouds (PC-GAN). First, we combine ideas from hierarchical Bayesian modeling and implicit generative models by learning a hierarchical and interpretable sampling process. A key component of our method is that we train a posterior inference network for the hidden variables. Second, PC-GAN defines a generic framework that can incorporate many existing GAN algorithms. We further propose a sandwiching objective, which results in a tighter Wasserstein distance estimate than the commonly used dual form in WGAN. We validate our claims on the ModelNet40 benchmark dataset and observe that PC- GAN trained by the sandwiching objective achieves better results on test data than existing methods. We also conduct studies on several tasks, including generalization on unseen point clouds, latent space interpolation, classification, and image to point clouds transformation, to demonstrate the versatility of the proposed PC-GAN algorithm.", "keywords": ["Point Cloud", "GAN"]}, "meta": {"decision": "Reject", "comment": "Reviewers mostly recommended to reject after engaging with the authors, however since not all author answers have been acknowledged by reviewers, I am not sure if there are any remaining issues with the submission. I thus lean to recommend to reject and resubmit. Please take reviewers' comments into consideration to improve your submission should you decide to resubmit.\n"}, "review": {"r1eiKIzRC7": {"type": "rebuttal", "replyto": "rJlR-Pg90X", "comment": "1. Yes, since the training takes time, we reported the preliminary results above, but we believe it should be enough to observe the trade-off when we increase/decrease the ratio toward the extremes (upper only and lower bound only). We will make the study more complete and revise the results in the revision. \n\n2. We do not have any quantitative results, but we do have some qualitative picture proofs. We can add that in appendix to the camera ready.  \n\n3. Yes, we agree with reviewer that our framework is related to Neural Statistician and would discuss it  in the camera ready. \n\n4. We do have some results in the paper. For example, in Table 2, we can observe increasing number of points brings marginal improvements. Same situation is also observed in ShapeNet55 experiments, where increasing number of points can lead to <0.5% improvements. \n\n5. The results depends on the pooling function. For max pooling, it\u2019s more robust to non-uniform decimation while the mean pooling suffers more. If the decimation results in significant change of the objects (e.g. remove half of the objects), both PC-GAN and AAE  (Achlioptas et al., 2017) cannot results in valid reconstruction since it is very different from training data. We will make this study more thorough in the revision.\n", "title": "Re: Thanks for your responses so far "}, "H1e8c5rL3Q": {"type": "review", "replyto": "ByxAcjCqt7", "review": "Summary:\nThis paper introduces a generative model for 3D point clouds. Authors aim at theoretically showing the difficulties of using existing generative models to learn distributions of point clouds, and propose a variant that supposedly solves the issues.\n\nPros:\n+ The problem of designing generative models for 3D data is important.\n\nCons: \n- Paper is often hard to follow, and contains a significant number of typos. \n- Authors claim to identify a fundamental problem with the existing generative models for point clouds, yet Section 2 tries to show that a _specific version_ that uses DeepSet does not satisfy theoretical guarantees. What if we use e.g. a recurrent network instead? As is, the counter example proof itself is quite confusing: it would really help if the proof was more formal.\n- Jointly learning an inference network (Q) has certainly been done before, and I am not sure authors provide an elaborate enough explanation of what is the difference with adversarially learned inference /  adversarial feature learning.\n- It is not clear why authors did not follow the evaluation protocol of [Achlioptas\u201917] or [Wu\u201916] more closely. In particular, evaluation for the classification task should be compatible with the proposed model, which would give a much better picture of the learned representations.\n", "title": "The paper may contain interesting ideas, but lacks clarity, and might have issues with experimental evaluation.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1eAuTlLCX": {"type": "rebuttal", "replyto": "H1eZXehVnX", "comment": "We thank reviewer for his insightful comments.\n\n1. We agree with the reviewer that sinkhorn iteration is a way to obtain an upper bound on Wasserstein distance. However, based on the original paper, they solve the Sinkhorn divergen with T iterations, later when they solve the generator based on the estimated distance, the gradient has to backpropagate through those T iterations, which is expensive and infeasible. We also note that there is new work, IPOT (Xie et al., 2018), which can get rid of backpropagating through the T iterations as what we adopted (Bertsekas, 1985) in the paper. Combining PC-GAN with IPOP or other future works could be an interesting future work. \n\n2.  The variance of the sandwiched estimator can be higher, but we are more concerned about bias in this work, which can be treated as a bias-variance trade-off. \n\n3. The 20:1 mixture used in practice do not directly correspond to s in theory, because the distances we compute are not scaled. For example, if the f_\\phi, the discriminator of GAN, is k-Lipschitz, the lower bound estimate should be divided by k. However, k is unknown in practice. Therefore, we just numerically did a coarse grid search and find the best mixture ratio.  Also, we try different ratios as we replied to R2 above. \n\nRatio                   D2F (Distance to Face)       Coverage\n1:0                               6.03E+00                        3.36E-01\n40:1                              6.06E+00                       3.41E-01\n20:1                              5.77E+00                       3.47E-01\n10:1                              6.85E+00                       3.56E-01\n0 :1                               9.19E+00                       3.67E-01\n\n4. We do not consider W_s to be very close from W_U. As can be seen from Figure 6, for the aeroplane examples, W_U fails to capture aeroplane tires while W_s can. Similarly for Chair example, W_s recovers better legs than W_U. Quantitatively, we highlight that W_s outperforms W_U consistently as shown in Table 1. Thus, we consider both W_U and W_L is needed to generate good quality point clouds. \n\n\n", "title": "Re: A variant of WGAN, suppressing bias but leading to worse stability when estimating Wasserstein distance"}, "S1x_7qgLAX": {"type": "rebuttal", "replyto": "SyeS7CQ83m", "comment": "1. We apologize for typos and if any term is not defined at the appropriate places. We  fixed all the typos and define the abbreviation for IPM at the first occurrence. Please check the revision. P and G, the elements of the divergence D(P||G) that appears in the first paragraph of section 3, is defined in the subsequent two sentences in the same paragraph. By the notation G_theta(u) \\sim p(theta), we mean that we want to train the generator G_theta such that when fed a random variable u \\sim p(u), the distribution of G_theta(u) matches that of p(theta). Sorry if it is confusing, but G_theta is not parameterized by theta, it just indicates that its the generator for theta. (Like G_x indicates that it is the generator for x).\n\n2. The training of G_theta is described in the subsection titled \u201cHierarchical Sampling\u201d. As correctly pointed out by the reviewer, that G_theta does not appear in the objective function (4). Using (4), we train G_x and Q networks. After training G_x and Q, we use trained Q to collect inferred Q(X), for each point cloud X. Then we train the generator G_theta using ordinary WGAN formulation to produce samples from same distribution as that of the samples Q(X) for each point cloud X. In addition to such two step training, a joint training also works, but is slower computationally, thus we report only the two step training in the paper.\n\n3. Quantitative evaluation of generative modeling performance is unfortunately very hard for real world problems like point clouds, which is the probable cause for it being missing from much of GAN literature. Thus, to provide some quantitative results for generation, we resorted to the toy problem. In the toy problem, we can accurately gauge the generation capabilities as can be seen from Figure 5. (We did not explicitly provide numbers like KL divergence, as it is evident from the Figure that PC-GAN would be significantly better than AAEs if we evaluate the numbers.) The same protocol can be extended for measuring the quality of the final hierarchical sampling. \n\n4. To showcase the effect of varying s, we chose the reasonable sized ModelNet10 dataset and ran for s=0, s=1, and three values s_1<s_2<s_3 in between. The results are as follows:\n\n\n                   D2F (Distance to Face)       Coverage\ns=0                        6.03E+00                     3.36E-01\ns1                          6.06E+00                     3.41E-01\ns2                          5.77E+00                     3.47E-01\ns3                          6.85E+00                     3.56E-01\ns=1                        9.19E+00                     3.67E-01\n\n4. Yes the model nicely captures simple topological features of the object, like presence of holes versus being one solid object. Even in the latent space, objects with hole group together.\n\n", "title": "Re: Generative model for point clouds, based on local-global latent variables. "}, "HJg61YeLRX": {"type": "rebuttal", "replyto": "H1e8c5rL3Q", "comment": "1. The purpose of the counterexample is only to show that there exists some spurious solutions to GANs with general DeepSets-style discriminator for point clouds. We agree that setup we selected is destined to fail, but it was done on purpose to illustrate the presence of spurious solutions. A good generator and discriminator would definitely be a solution as well. However, solutions during optimization might not always correspond to such good solutions and can also correspond to the demonstrated spurious solutions. We found empirically that GAN with simple DeepSet-like discriminator most of the times fails to learn to generate point clouds even after converging, however, it does sometimes results in reasonable generations (although worse than proposed PC-GAN). So, we do not consider the argument to be unrealistic as we often observe the degeneracy. So the message here is that we need additional constraints for GANs with simple DeepSet-like discriminator to exclude such bad solutions and lead to a more stable training. Other architectures like RNN might work, but they are not permutation invariant, which is a desirable property for set data like point clouds. More comparisons between using RNN and DeepSets for other tasks on set data can refer to Zaheer et al., (2017).\n\n2. As we discussed in the end of Section 3, ALI and BiGan\u2019s goal is to match (z, G(z)) and (Q(X), X), which aims to infer the random noise z and enforce the latent code to follow noise  distribution (e.g. Gaussian). On the other hand, we do not enforce Q(X) to follow from Gaussian. Instead, we train the other G_theta(u) to match Q(X), which is more similar to AAE-like works (Engel et al., 2017; Kim et al., 2017; Achlioptas et al. 2017). The difference of the interpretation between PC-GAN and  those AAE-like work is also explained in the second paragraph of Sec 4.\n\n3. We followed the same protocol that we trained on ShapeNet55 and tested on ModelNet40 testing set. Please check Table 3 in the revision. PC-GAN achieves 86.9% accuracy which is better than AAE (84.5%),  3D-GAN (83.3%) and other unsupervised learning approach.\n", "title": "Re: The paper may contain interesting ideas, but lacks clarity, and might have issues with experimental evaluation. "}, "H1er4dxIRX": {"type": "rebuttal", "replyto": "ByxAcjCqt7", "comment": "Dear reviewers, \n\nWe thank for your comments. We polished the paper to fix typos. Also, we added addition experiments for classification with Shapenet55. Please check Table 3 in the revision.  We will respond to each reviewer\u2019s comment below the review. ", "title": "Paper Revision "}, "SyeS7CQ83m": {"type": "review", "replyto": "ByxAcjCqt7", "review": "Summary:\nThis paper proposes a generative point cloud model based on adversarial learning and definitti\u2019s representation theorem of exchangeable variables.\nThe main focus in experiments and the exposition is on 3D point clouds representing object shapes (seems the surface, but could also be the interior of objects, please clarify). \nThe main idea is to represent a point cloud using a global latent variable that captures the overall shape, and a collection of local latent variables that code for the position of a point on the shape.\nThe model consists of thee components:\n(i) an \u201cencoder\u201d that takes a point cloud as input and maps it to a (point estimate of) the global latent variable of the shape represented by the input cloud, a point-net architecture is used here\n(ii) a \u201cdecoder\u201d that takes the estimated global latent variable, and a local latent variable, and maps it to an \u201coutput\u201d point in the cloud to be produced by the model. \n(iii) a \u201cdiscriminator\u201d network that aims to distinguish points from a *given* shape, and the points produced by pipe-lining the encoder and decoder. Critically different from conventional GANs, the discriminator is optimized *per shape*, ie each point cloud is considered as a *distribution* over R^3 specific to that shape. \n(iv) a \u201cshape prior\u201d that, once the encoder-decoder model from above is trained, is used to model the distribution over the global latent variables. This model is trained, presumably, in a conventional GAN style using the global latent variable representations inferred across the different training point clouds.\n\nAs compared to prior work by Achiloptas et al (2017), the proposed approach has the advantage to allow for sampling an arbitrary number of points from the target shape, rather than a fixed pre-defined number. \n\nIn addition, the authors propose to minimize a weighted average of a lower bound and upper bound on the Wasserstein distance between the distributions of points corresponding to given shapes. \nThis approach translates to improved quantitative evaluation measures, \n\nExperiments are conducted on a simple toy data set, as  a proof of concept, and on data from ModelNet10 and ModelNet40. \nTwo performance metrics are introduced to assess the auto-encoding ability of the model: to what extent does the encoder-decoder pipeline result in point clouds similar to the shape from which the input point-cloud is generated. \n\nOverall I find the idea of the paper interesting and worth publishing, but the exposition of the paper is less than ideal and needs further work. \nThe experimental validation of the proposed approach can also be further improved, see more specific comments below. \n\nSpecific comments:\n\n- The counter example at the bottom of page 2 is limited, in the sense that the oracle assumption seems highly non-realistic, casting doubt on the relevance of the argument.\n\n- The notation in section 3 (before 3.1) is rather sloppy. \nFor example, \n- please define P and G, the elements of the divergence D(P||G) that appears in the first paragraph of section 3.\n- it is not defined in which space theta lives, it is not clear what the authors intend with the notation G_theta(u) \\sim p(theta). \n- what prior distributions p(z) and p(u) are used? What is the choice based on?\n\n- abbreviation IPM is referred several times in the paper, but remains undefined in the paper until end of page 4, please define earlier. \n\n- The model G_theta does not appear in the training objective function (4), how is this module trained precisely?\n\n- Lack of clarity in the following passage: \u201cIn our setting, each point xi in the point cloud can be considered to correspond to single images when we train GANs over images\u201d\n\n- The notion of divergence D(P|G) is not made concrete in section 3 and 3.1, which makes the notation of rather little use.\n\n- The following paper merits a discussion in the related work section: \n\u201cTOWARDS A NEURAL STATISTICIAN\u201d, ICLR\u201917, https://openreview.net/pdf?id=HJDBUF5le\n\n- The manuscript contains many typos. For example\n \u201cvedio\u201d op page 4, \u201ccircile\u201d on page 5, \u201ccondct\u201d on page 8, etc.\nPlease proof read your paper and fix these.\nThe refenence to  Bengio 2018 is incomplete: what do you refer to precisely?\n\n- There seems to be no mention of the dimension of the \u201clocal\u201d latent variables z_i. \nPlease comment on the choice, and its impact on the behavior of the model.\n\n- The quantitative evaluation in table 1 is interesting and useful. \nIt is limited, however, in the sense that it (only) measures auto-encoding capabilities: to what extent can the shape be reproduced given a sample point cloud from the given shape. \nQuantitative evaluation of generative modeling performance is unfortunately missing from this paper, as it is in much of the GAN literature. \nCould you please comment on how this can/will be fixed?\n\n- The toy data set experiments could be dropped  to make room for experiments suggested below.\n\n- An experimental study of the effect of the mixing parameter \u201cs\u201d would be useful to include. \nFor example, by taking s on a grid from 0 to 1, one could plot the coverage and distance-to-face measures.\n\n- Experimental evaluation of auto-encoding using a variable number of input points is interesting to add: ie how do the two evaluation measures evolve as a function of the number of points in the input point cloud?\n\n- Similar, it is interesting to evaluate how auto encoding performs when non-uniform decimation of the input cloud is performed, eg what happens if we \u201cchop off\u201d part of the input point cloud (eg the legs of the chair), does the model recover and add the removed parts? This is potentially useful to practitioners which have to deal with incomplete point clouds acquired by range scanners. \n\n- Analysis of shapes with different genus and dimensions would be interesting. \nDoes the model manage to capture that some shapes have holes, or consists of a closed 2D surface (ball) vs an open surface (disk),  despite a simple prior on the local latent variables z?\n\n", "title": "Generative model for point clouds, based on local-global latent variables.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1eZXehVnX": {"type": "review", "replyto": "ByxAcjCqt7", "review": "Authors provide a variant of WGAN, called PC-GAN, to generate 3D point clouds. The drawback of a vanilla GAN with a DeepSet classifier is analyzed. The rationality that decoupling the point generator with the object generator is also discussed. \nA sandwiching objective function is proposed to achieve a better estimation of Wasserstein distance. \nCompared with AAE and the simplified variants of the proposed PC-GAN, the proposed PC-GAN achieves incremental results on point cloud generation.\n\nComments:\n1. Authors calculate W_U in a primal form via solving an assignment programming problem. Have authors ever tried Sinkhorn iteration? To my knowledge, sinkhorn iteration is a very popular method to solve OT problem effectively. It would be nice if authors can provide some reasons and comparisons for their choice on the optimizer of W_U. \n\n2. Authors proved that the sandwiching object W_s is closer to the real Wasserstein distance, but it increases the variance of the loss function. Specifically, the dynamics of W_U, and W_L, according to lemma1, is (epsilon2-epsilon1)*w(P, G) while the dynamics of W_s is 2*epsilon1 * w(P, G), and 2epsilon1 > epsilon2 - epsilon1 (according to the assumption in lemma 1). Does it mean that the W_s is not as stable as W_L or W_U during training?  Additionally, authors combined W_U with W_L with a mixture 20:1, i.e., the s in Eqs(6, 13, 14) is smaller than 0.05. In such a situation, both the value and the dynamics of W_s will be very close to that of W_U. Does it mean that W_L is not so important as W_U? Authors should analyze the stability of their method in details.\n\nEssentially, the proposed method is a variant of WGAN, which estimates Wasserstein distance with lower bias but may suffer from worse stability. In the experiments, both the setting and the experimental results show that the proposed W_s will be very close to W_U. As a result, the improvement caused by the proposed method is incremental compared with its variants. \n\nTypos:\n- The end of the 2nd line of lemma 1: P, G should be \\mathbb{P}, \\mathbb{G}\n- The 3rd line of lemma 1: epsilon1 -> epsilon_1\n- Page 14, Eq(14), \\lambda should be s\n- Page 14, Eqs(13, 14), w(\\mathbb{P}, \\mathbb{G}) should appear on the right.\n", "title": "A variant of WGAN, suppressing bias but leading to worse stability when estimating Wasserstein distance", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Bye2CkeKqQ": {"type": "rebuttal", "replyto": "BygRsWmBcQ", "comment": "We thank you for raising an excellent question about counter example. The purpose of the counter example is only to show that there exists some spurious solutions to GANs with general DeepSets-style discriminator for point clouds. We agree that setup we selected is destined to fail, but it was done on purpose to illustrate the presence of spurious solutions. A good generator and discriminator would definitely be a solution as well. However, solutions during optimization might not always correspond to such good solutions and can also correspond to the demonstrated spurious solutions. This is precisely the point made by counter example argument, that simple DeepSets classifier does not ensure proper generative model. Moreover, we found empirically that GAN with simple DeepSet-like discriminator most of the times fails to learn to generate point clouds even after converging. However, sometimes it does results in reasonable generations (although worse than proposed PC-GAN), which may correspond to the case you describe. So the message here is that we need additional constraints for GANs with simple DeepSet-like discriminator to exclude such bad solutions and lead to a more stable training.\n", "title": "Great question"}, "BylnKVcMqQ": {"type": "rebuttal", "replyto": "ryeUxneAY7", "comment": "We thank you for pointing out some of the recent results of applying deep learning on point cloud for classification and segmentation by leveraging graph convolution. Main focus of our work is to design a generative model for point cloud and thus many of these papers were purposefully not included in related works for being concise. However, it might be a good idea to provide a more exhaustive background of on geometric deep learning and we would definitely discuss the related papers you mention above in next revision of the paper.\n\nMoreover, some of the architectures presented in the mentioned papers, like the point cloud autoencoder with graph convolution may be an alternative choice of Q in addition to DeepSets in the proposed PC-GAN framework, but again is not the focus of our paper. We want to develop a generic framework for generation of point clouds or set data in general, thus employed the most simplistic components like DeepSets. Using the more advanced architectures (e.g. Wang et al., (2018) or PointNet++) should bring in improvements and can be an interesting future direction. We will discuss these aspects in the next revision of the paper. \n", "title": "Thanks for your comments"}}}