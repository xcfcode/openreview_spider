{"paper": {"title": "Continual learning using hash-routed convolutional neural networks", "authors": ["Ahmad Berjaoui"], "authorids": ["~Ahmad_Berjaoui1"], "summary": "We present a scalable continual learning framework composed of individual units, selected using feature hashing.", "abstract": "Continual learning could shift the machine learning paradigm from data centric to model centric. A continual learning model needs to scale efficiently to handle semantically different datasets, while avoiding unnecessary growth. We introduce hash-routed convolutional neural networks: a group of convolutional units where data flows dynamically. Feature maps are compared using feature hashing and similar data is routed to the same units. A hash-routed network provides excellent plasticity thanks to its routed nature, while generating stable features through the use of orthogonal feature hashing. Each unit evolves separately and new units can be added (to be used only when necessary). Hash-routed networks achieve excellent performance across a variety of typical continual learning benchmarks without storing raw data and train using only gradient descent. Besides providing a continual learning framework for supervised tasks with encouraging results, our model can be used for unsupervised or reinforcement learning.", "keywords": ["Lifelong learning", "continual learning", "feature hashing"]}, "meta": {"decision": "Reject", "comment": "Reviewers raised several concerns about the paper guided by unfounded heuristics as well as the artificiality of the tasks involved.  Rebuttal only answered a few of them and did not convince the reviewers which has been clearly stated in the response. We hope that the authors will improve the paper for future submission based on the reviews. "}, "review": {"wkxFB22pXzf": {"type": "rebuttal", "replyto": "D_HrFrGCAQX", "comment": "- Incremental-Cifar100 dataset construction: R1 is correct in saying that splitting a 100 classes dataset into 10 10 classes dataset has no practical application. This is merely an \"easy\" way to generate distinct learning tasks in order to assess catasptrophic forgetting and learning capacity for a continual learning model. The same idea has been applied for the Pairwise-MNIST dataset. This kind of trick has indeed no real application but is very commonly used in continual learning benchmarks, as there are very few datasets that were specifically developped to test continual learning models.\nWe have explored more realistic uses of continual learning when we combined learning tasks from MNIST and Fashion-MNIST, and SVHN/Cifar100.\n- Fairness w.r.t model sizes: It is true that for each experiment, HRN contains more trainable parameters and convolution layers than other methods. However, adding extra convolution layers to other methods would also not be fair, as this would increase the feature map size before classification. Nevertheless, HRN have a limited depth and at each forward pass, only a given combination of units is involved, resulting in a limited feature map size (even before feature hashing).", "title": "R1 specific comments"}, "kDHeWNTx_rV": {"type": "rebuttal", "replyto": "EwFr3lgDjKn", "comment": "- A more detailed ablation study was added to the Appendix, showing the impact that each threshold has on accuracy, catastrophic forgetting and runtime. Typical trade-offs are performance vs runtime and top accuracy vs catastrophic forgetting mitigation.\n- We have added plots for global accuracy, accuracy drop and top task accuracy, removing the need for Table 1. R3 is correct in saying that better catastrophic forgetting mitigation comes at the cost of lower per-task accuracy. However, this can be compensated by increasing training epochs or by adding extra units.\n- Sections 2 and 3.1 have been modified to better explain the intution behind the choice of feature hashing and its relevance to our work. Also note that section 3.2.2 already discusses key design choices.\n- As explained to R2, we do not claim to surpass state-of-the-art continual learning algorithms with HRN. We merely present a novel and scalable techique. This is why we have compared our approach only with the most relevant approaches and typical benchmarks.\n- Stable features: HRN's output is a feature vector. We have coupled it to MLP classifiers for supervised tasks but we could have used the generated feature vector to estimate a V-value and a policy, in a policy gradient reinforcement learning setup. We also could have reconstructed the input using the generated feature vector, through the use of an upsampling transpose-convolution block, in an unsupervised learning setup. This is why HRN generate stable features that could be used for unsupervised or reinforcement learning.\n- Section 3.2.2: R3 says: \"Does this mean that, since hashes are similar, there is not much variance across projections and so the residual contains more information?\" This is the correct interpretation.\n- Empirically, as shown in the ablation study, residuals sparsity increases catastrophic forgetting mitigation.\n- Section 3.4 now explains how to automate the addition of new units.", "title": "R3 specific comments"}, "VX1aO17sLpc": {"type": "rebuttal", "replyto": "a1yc6r29AJ", "comment": "1- Runtime performance: Please see the updated Appendix for runtime performance.\n2- New classes management: HRN generate stable feature vectors and classification is performed using a task specific MLP classifier. There is therefore no need to adapt the softmax layer. Please refer to section 5.1 for further detail.\n3- Training a single model on a combination of the old and new datasets will lead to good performance on both datasets. However, in a continual learning context, data is only available sequentially hence, such a combination is not usually possible.", "title": "R4 specific comments"}, "fWPkArpWiwN": {"type": "rebuttal", "replyto": "PoNifWam6K", "comment": "- Similarity: It is true that similarity in the raw pixel space might not always be meaningful and that this might lead to performance degradation. This is particularly true for large images (e.g. 64x64 pixels or more) but we found that performance was acceptable for small patches (e.g. 32x32 or less). Using a first stage feature generator such as a pre-trained convolution layer with frozen weights is actually something we had considered. It is an interesting suggestion but we chose not to discuss it in this paper for two main reasons. First, it does not change the nature of our algorithm as the output of the primary layer can be seen as a raw image (even though it has more structure). Second, it limits our network's scalability as the primary layer is not necessarily suited to handle all types of images. Otherwise, it would need to be a large pre-trained network that would significantly increase the computational cost of the HRN.\n- Figure 2: Figure 2 plots have been updated and additional plots were added to show top task accuracy and accuracy drop.\n- Precise architecture and hyperparameters: Please see the updated Appendix. The supplementary material contains the full code and configuration files to reproduce all experiments.\n- ANML paper: we have now mentioned this paper in the related work section but we do not consider the chosen approach to be of interest for comparison to our approach (see the discussion in the updated version of the paper). Indeed, we do not claim to surpass state-of-the-art continual learning algorithms with HRN. We only present a new scalable approach with good results on typical benchmarks.\n- ICLR 2018 paper: we have now mentioned this very interesting paper in the related work section (dynamic networks).\n- Obtaining fixed-length representations: R2's suggestion is again something we had tried at first. This did not lead to acceptable performance and this is what prompted the search for fixed length representations that eventually led to the use of feature hashing.\n- Basis update: The basis update algorithm only updates the least stable part of the basis, keeping as much stability as possible in the output representation.\n- Figure 4: The figure is correct, there is no missing arrow that should be coming out of U_4. Indeed, when the final unit is selected (maximum depth has been reached), there is no need to perform the final convolution. Only the projection onto the final unit's basis is needed, to compute the last residue vector.", "title": "R2 specific comments"}, "0osZ_13CzKC": {"type": "rebuttal", "replyto": "2G9u-wu2tXP", "comment": "We would like to thank the reviewers for their positive feedback and valuable comments. We hope the following answers will address the reviewers' concerns. We have tried to address as many questions and issues as possible, through an updated version of the paper and the supplementary material.\nIn brief, here are the main changes:\n\nPaper: \nTypos and other suggested formatting corrections were made. Several clarifications were added. Related work section was updated. Global accuracy plots were added to existing plots. Two additional plots were added to SVHN/incremental-Cifar100 scenario for a better analysis, removing the need for the large accuracy scores table. The appendix now includes many implementation details, detailed ablation and runtime performance.\n\n\nSupplementary material:\nCode has been updated to latest pytorch version and several optimizations were implemented, improving overall runtime. Ablation experiments configuration files are more explicit and have been put into a dedicated folder. EWC now has a dedicated classifier per task (as suggested by R3) and results were updated accordingly. \n\nSpecific concerns and questions will be adressed separately to each reviewer.", "title": "Global comment"}, "D_HrFrGCAQX": {"type": "review", "replyto": "2G9u-wu2tXP", "review": "##########################################################################\n\nSummary:\n\nThis paper studies the problem of continual learning and proposes a new learning framework named hash-routed convolutional neural networks (HRN). HRN has a set of convolutional units and hashes similar data to the same unit. With this design, the paper claims three key contributions. (1) HRN provides excellent plasticity and more stable features. (2) HRN achieves excellent performance on a variety of benchmarks. (3) HRN can be used for unsupervised or reinforcement learning. \n\n\n##########################################################################\n\nReasons for score: \n\nOverall, I like the scope of this paper that studies continual learning. The experiments in Figure 2 verify that the proposed HRN outperforms a number of baselines on incremental-Cifar100 dataset. However, I am not sure whether the dataset makes sense for benchmarking continual learning. It seems the authors split the 100 classes into 10 groups, with each group having 10 classes. The 10 groups and corresponding labels serve as 10 distinct tasks for evaluating continual learning. Such formulation is odd that leads to decreasing accuracy scores as shown in Figure 2. I cannot find a real-world application that can benefit from such formulation due to decreased accuracy scores. It will be good if the paper can clarity the above points to strengthen its motivation. \n\n##########################################################################\n\nPros:\n\n1. The paper proposes a novel neural network HRN for continual learning. HRN leverages multiple units of CNN and hashes similar data to the same unit for training.  \n\n2. The proposed HRN achieves impressive performance when compared with a selected set of baselines. HRN consistently shows more robust accuracy scores on three datasets, as shown in Figure 2 and Table 1. \n\n\n##########################################################################\n\nCons: \n\nThe most important point relates to the motivation of problem formulation. The paper splits a 100-classes dataset into 10 10-classes datasets. Any two datasets have no overlapping in the classes. Under such formulation, a model suffers from decreased accuracy score as shown in Figure 2. It is necessary to explain why decreased scores are appealing in practice given they are the base of the concerned continual learning. \n \nAccording to Figure 2 and Table 1, all models suffer from decreased accuracy scores. Besides, the accuracy scores decrease monotonously regarding the task ids. Task id should not be a factor of performance. It will be good if the paper can provide some explanations. All existing machine learning researchers seek better accuracy scores so the decreased accuracy scores seem unusual. \n \nIt may not be fair to compare baselines with HRN that exhibits a larger model size. HRN leverages multiple (e.g. 6 in figure 2) units of CNN. To be fair, the paper should use the same number of units (or equivalently ensembles) for a baseline method. It will be good if the paper can run some experiments or show a comparison on model sizes. \n\n\n##########################################################################\n\nQuestions during rebuttal: \n\nIt will be nice if the authors can add experiments or discussions related to decreased accuracy scores. It will be perfect if the authors can show real applications of the proposed continual learning with decreased accuracy scores. \n\n", "title": "The paper has values in the exploration of data hashing for neural networks. At the same time it needs more evidence make sense the motivation.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "EwFr3lgDjKn": {"type": "review", "replyto": "2G9u-wu2tXP", "review": "############## Summary ##############\n\nThis submission addresses the question of how to avoid catastrophic forgetting while being scalable and adaptable to multiple tasks. The paper introduces the hash-routed network (HRN), a new routing mechanism for neural networks whereby feature hashing (Weinberger et al.) is used to determine the similarity between data points; similar data points are then routed to the same units. HRNs  work by keeping various convolutional units, each coupled with an orthonormal projection basis. The parameters of the convolutional layers are trained via vanilla back-propagation and the orthonormal basis is created following the Gram Schmidt procedure. An aging mechanism enables updating basis vectors that have become obsolete over time. The proposed method is evaluated in three standard continual learning settings, and is demonstrated to avoid catastrophic forgetting.\n\n############## Strengths ##############\n\n1. The idea to use hashing to determine routing paths is novel and could be interesting to the general deep learning community.\n2. This is to my knowledge the first mechanism that enables learning routing networks for lifelong learning, which is certainly relevant.\n3. Code is included for reproducibility.\n\n############## Weaknesses ##############\n\n1. The obtained results show that, while catastrophic forgetting is avoided, this comes at the cost of a model that is too inflexible to handle new tasks.\n2. The proposed algorithm hinges too heavily on heuristics, including counters, thresholds, and manually deciding when new units should be added. While this is not necessarily wrong, it would require substantial experimentation and validation that each component is necessary to achieving the desired behavior.\n3. The paper is hard to follow and so it makes it tough to assess exactly how the different parts of the proposed method are implemented.\n\n############## Recommendation ##############\n\nUnfortunately, at this time I have to recommend this paper for rejection. I think a future iteration of this work could be interesting to the community if either much of the heuristics were replaced by more well-founded techniques, or each of the heuristics was appropriately analyzed in terms of its empirical effect. Moreover, the paper should provide other views of the obtained results, including final overall accuracy, which is omitted.\n\n############## Arguments ##############\n\nThe key idea studied in this work is how to leverage hashing to decide different routes in a routing net in a continual learning setting. I believe this to be a powerful and interesting idea, but I am of the opinion that the proposed approach uses too many heuristic tricks, and therefore makes it hard to assess the benefits of the key contribution. In particular, the method uses a threshold (1) on the projection residual magnitude to decide when to stop chaining, a threshold (2) on the projection magnitude to decide when to pick a random, un-initialized unit, another threshold (3) on the projection magnitude to decide when to add a new basis vector to the chosen unit, and a final threshold (4) on the age of a basis to choose when to update it. While it is possible that such a heuristic method is the right way to go, it warrants a much more comprehensive evaluation that enables us to understand what the contribution of each of the aspects is to overall performance.\n\nOn the other hand, it seems that the obtained results are not very strong, and so I didn't find enough evidence in the paper to convince me that the proposed method is appropriate for continual learning. In particular, it seems that the cost of avoiding forgetting is to almost entirely prevent the model from adapting to new tasks (e.g., Table 1-top). It would be much easier to evaluate these results if the authors provided other views into their results. For example, the authors could show accuracy evaluation of all tasks throughout the training process, average final accuracy of all tasks, average forgetting across all tasks. \n\nThe other major point I have is the lack of clarity in the paper. Section 2 is supposed to provide background for feature hashing. Ideally, the reader should finish reading this section and come out with an understanding of what the point of hashing is in general, how it's computed, and how it will be useful for the provided approach. Instead, this section gives a few mathematical properties of feature hashing, with no intuition or high-level description of it or how it will be used. It seems like the rest of the paper similarly doesn't really give an intuition for why hashing might be useful, other than the fact that it gives a constant-size dimensionality to the hashed features.  The manuscript places emphasis on the fact that inner product is maintained under hashing, so I suppose the point is that distance metrics can be computed in the hashed space, which is used to compute similarity. I believe this fact is never explicitly stated, even though it is a key point of the proposed approach.\n\nI'm also surprised that the paper did not provide a comparison to stronger lifelong baselines, like those based on experience replay, which have been shown to vastly outperform regularization-based methods in recent work (e.g., Lopez-Paz & Ranzato). \n\n############## Additional feedback ##############\n\nThe following points are provided as feedback to hopefully help better shape the submitted manuscript, but did not impact my recommendation in a major way.\n\nAbstract\n- The abstract mentions that the method is potentially useful for unsupervised or reinforcement learning unlike prior approaches, but this is never tested. Why should the reader believe this to be the case?\n\nIntro\n- Some paragraphs are spaced and some aren't. The rest of the paper has un-spaced paragraphs only. Please be sure to use consistent formatting.\n- What are \"stable features\", and how are they useful for unsupervised learning? The manuscript states that the proposed approach would find such stable features, but I didn't see any evidence of this in the evaluation.\n- The paper initially says that the method will be trained only with gradient descent, but then it mentions regularization techniques. Would it be fair to say that EWC is also trained only with gradient descent, since it only adds a regularization term and then uses the penalized objective for gradient descent?\n- If the three benchmarks are MNIST, Fashion/MNIST, and SVHN/incremental-Cifar100, then the text shouldn't say that benchmarks \"include\" those three, but rather that those three \"are\" the benchmarks used.\n\nSec 2\n- I recommend using `` '' for quotation marks\n- This section focuses on providing some theoretical properties of feature hashing, but no good intuition about what it's doing, how it's computed, or how it will be useful for the approach.\n    - What's N in the summation? In Weinberger et al., there's no N. It seems to be j=1^N where h(j)=i.\n    - Need to define \\sigma.\n\nSec 3.1\n- Elements of the proposed architecture are introduced, but we're still given no intuition of what they'll be useful for. This section should guide the reader by explaining from the beginning what the purpose of each piece of the architecture will be.\n\nSec 3.2.1\n- Why are feature maps of vanilla CNNs growing in size?\n- What is the i index? It seems to be the selected unit. Oddly, in Sec 3.1 k was the selected unit, but here it seems that k is the operation number in the chain of operations.\n- Does the set subtraction in Eq. 4 mean that the units used so far cannot be used for the subsequent operations? This should be stated in text.\n- Why is there no mention of how the bases are initialized yet?\n\nSec 3.2.2\n- Where is there an inner product between hashes computed for similarity? Is this the inner product with the basis? We can't know that at this point, since we have no information about how the bases are created.\n- Overall, this section could clarify a lot of things like why is orthogonality important in this context, why the residuals are chosen. It attempts to do so, but leaves me somewhat confused still.\n    - This statement: \"the orthogonal subspace\u2019s contribution to total variance is much more important than that of B_k\" could be explained in more detail. Does this mean that, since hashes are similar, there is not much variance across projections and so the residual contains more information? \n- Why are residuals encouraged to be sparse?\n\nSec 3.3.1\n- basis --> bases (plurals)\n- Using two separate thresholds on the projection magnitude seems to introduce too many hyperparameters. It seems odd to use projection magnitude simultaneously for 1) choosing the current unit, 2) choosing whether to initialize a new unit, and 3) choosing when to expand a unit's basis projection. Why do this?\n\nSec 3.4\n- Scalability depends on manually adding units. Would it be possible to come up with an automatic way to do this? This section states that it should be possible, but I think a significant contribution of this work could come from designing a technique for it.\n- This manual addition seems to go against the stated goal of keeping data scientist involvement to a minimum.\n\nSec 5\n- Is EWC also allowed a task-specific classifier? A fair evaluation would give EWC a task-specific classifier like other methods get.\n- The description of the experimental setup is fairly complete, including details about when units were added, and which data sets were \"semantically different\"\n- Table 1 uses \",\" instead of \".\" for decimals. The \"%\" sign is missing.\n- The fact that catastrophic forgetting is avoided at the cost of lower overall performance suggests that the stability-plasticity trade-off was not well chosen: there is too much stability at the cost of very little flexibility.\n- We don't get to see average performance to accurately assess performance in Table 1.\n    - Manually computing it for SVHN/CIFAR-100 gives 55.46 (HRN) - 58.93 (ELL).\n- How were parameters of baselines chosen? It's odd that EWC neither retains performance on earlier tasks nor adapts to new tasks. This suggests a very poor choice of \\lambda.\n- It's nice to see unit usage plots, but these don't really show substantial differences in usage across tasks: all bars except the first look about the same.\n- The hyperparameters and ablation evaluation does not seem to add much value. Here, I would like to see a much more comprehensive ablation study that goes in depth into the effect of the different parts of the proposed algorithm.\n", "title": "Hashing is an interesting idea for routing networks, but I would like to see a more complete evaluation and a clearer description of the method", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "a1yc6r29AJ": {"type": "review", "replyto": "2G9u-wu2tXP", "review": "Summary:\nFor a learning model to learn continuously, it needs to handle new datasets without catastrophic forgetting or requiring the model to grow larger. This paper proposes a hash-routed convolution neural network where a different set of convolution filters are used depending on the data. New convolution filters can be added to the network without increasing the computational cost of the network.\n\nQuestions:\n1) What is the running time performance for hash-routed networks (HRN) during training and inference?\n\n2) How are new classes added to the Softmax layer during training?\n\n3) Couldn't you train a single model on the old and new datasets such that it performs well on both? If so, does the continual learning model train faster than building a data-centric model?\n", "title": "Official Blind Review #4", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "PoNifWam6K": {"type": "review", "replyto": "2G9u-wu2tXP", "review": "\n### Summary\n\u00a0\nOk, but not good enough. The authors present Hash-Routed Convolutional Neural Networks (HRNs), intended to enable learning of stable representations for continual learning, i.e. representations that change little for previously-learned tasks as more tasks are learned. The authors benchmark HRNs against several baselines in a number of tasks. Overall, the proposed method does not appear as conceptually and empirically convincing as those of other papers at ICLR and similar conferences. \n\n\n### Reasons for score\n\n- The presented algorithm does not appear as conceptually compelling as existing work on the subject (see below for papers that the authors could have cited and evaluated against, but did not). \n- Overall, the architecture, and in particular the use of hash routing as it is used here, does not seem natural to me. (See explanation below)\n- The results from the evaluation section are mixed, reinforcing my intuition that the proposed architecture is not as suitable as existing solutions. \n- The paper omits information about hyper-parameters, hyper-parameter tuning and choices, making the paper\u2019s results impossible to reproduce. Overall, there is so little information that it is impossible to tell if the experimental evaluation was fair to existing algorithms. \n\n\n### Pros\n\n- The paper addresses an important question: how can we train models in a continual-learning set-up, build representations that are meaningfully shared across successive tasks, and avoid performance degradation on previously learned tasks as new tasks are learned? The authors attempt to solve this problem using feature hashing, which had not previously been used in this particular way before. The proposed architecture is novel. \n\n### Cons\n\n- Conceptually, I find the HRN algorithm presented here somewhat unnatural. The authors write \u201cSimilar feature maps get to be processed by the same units, as a consequence of using feature hashing for routing. \u201d That\u2019s technically true, but since the similarity is measured right in the raw feature space, the similarity will often not be meaningful. For example, a black dog in front of a green background would have little measured pixel-level similarity with a beige dog in front of a blue background, even though conceptually both images contain dogs. I would expect that first computing higher-level representations and then using those to route to an appropriate CNN would be more useful than measuring similarity on raw inputs. I do understand that the HRN inference proceeds through multiple iterations, and that the output of the first iteration becomes an input to the second iteration. Still, I imagine that hashing raw inputs right at the start to decide where to route is  harmful. Note: while Weinberger et al (https://alex.smola.org/papers/2009/Weinbergeretal09.pdf) did hash their input features, but they were using bag of words features, not pixel features. \n- Figure 2 (top) is misleading and cherry-picks data that make the author\u2019s proposed algorithm look better. The graph suggests (green line) that the authors\u2019 HRN algorithm\u2019s performance on Task 0 stays nearly constant at a high level as more tasks are added. However, a closer look at Table 1 (top) indicates that HRN performs significantly worse than baselines on several tasks (see e.g. task T1, T2 and T3), both in max and min accuracy.  \n\n- The paper does not provide (enough/any) information on the precise architectures and hyperparameters that were used for the \u201cExperiments\u201d section. This makes the paper very challenging to reproduce. Even for the HRN algorithm presented in this paper, I could not find what value was used for the maximum basis size parameter \u201cm\u201d was used. I could not find much information on whether the authors selected or tuned the hyper parameters for the algorithms that they benchmarked against in the \u201cExperiments\u201d section. Based on the information provided, I cannot tell if the experiments made were fair to the baselines. \n\n- Some relevant existing work has not been cited and compared to: the authors did not cite the ANML algorithm (https://arxiv.org/pdf/2002.09571.pdf), which has achieved state-of-the-art performance on tasks similar to the one that the papers of the paper reviewed here evaluate on. I would suggest that the authors cite this paper, and benchmark againnnst ANML unless there is a good reason not to do so. \n\n- The authors should also have cited \u201cRouting Networks: Adaptive Selection of Non-Linear Functions for Multi-Task Learning\u201d (ICLR 2018, https://openreview.net/forum?id=ry8dvM-R-) . \n\n- The authors write ' *The output of a typical CNN is a feature map with a dimension that depends on the number of output channels used in each convolutional layer. In a HRN, this would lead to a variable dimension output as the final feature map depends on the routing.* ' Why would there necessarily be a variable-length output dimension? One could fix the output width of multiple CNNs, and sum the outputs element-wise to get a fixed-length representation. \n\n- the authors write \u201cAs the network trains, hashed features will also change and routing might need adjustment. If nothing is done to update full basis, the network might get \u201dstuck\u201d in a bad configuration. \u201d Would these basis updates not lead to changing output representations, thus breaking one of the key properties that the authors were looking for in this architecture? In fact, in Task T1 Table 1 (top), it appears that the model\u2019s performance degraded quite significantly as tasks were learned. \n\n### Questions during rebuttal period:\n\n- I would be grateful if the authors could address as many of the cons listed above as possible. \n- In Figure 4, on the right, there are arrows going into U_4, but no arrows coming out of U_4. Is this an error? \n\u00a0\n### Some suggestions:\n\n- There are possessive apostrophes missing here and there, e.g. \u201cunits\u201d vs \u201cunit\u2019s\u201d. I suggest that the authors review their use of possessive apostrophes one more time. \n\n- \u201c0 This is a 11 tasks\u201d should be \u201c0 This is an 11 tasks\u201d\n\n- \u201cThe following scenarios were considered.\u201d might better be followed by a colon than a period.  ", "title": "Ok but not good enough. Conceptually and empirically not entirely convincing. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}