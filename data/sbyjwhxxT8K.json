{"paper": {"title": "Near-Black-Box Adversarial Attacks on Graph Neural Networks as An Influence Maximization Problem", "authors": ["Jiaqi Ma", "Junwei Deng", "Qiaozhu Mei"], "authorids": ["~Jiaqi_Ma1", "junweid@umich.edu", "~Qiaozhu_Mei1"], "summary": "We establish a connection between adversarial attack on graph neural networks and the influence maximization problem and propose a group of effective black-box attack strategies based on this connection.", "abstract": "Graph neural networks (GNNs) have attracted increasing interests. With broad deployments of GNNs in real-world applications, there is an urgent need for understanding the robustness of GNNs under adversarial attacks, especially in realistic setups. In this work, we study the problem of attacking GNNs in a restricted near-black-box setup, by perturbing the features of a small set of nodes,  with no access to model parameters and model predictions. Our formal analysis draws a connection between this type of attacks and an influence maximization problem on the graph. This connection not only enhances our understanding on the problem of adversarial attack on GNNs, but also allows us to propose a group of effective near-black-box attack strategies. Our experiments verify that the proposed strategies significantly degrade the performance of three popular GNN models and outperform baseline adversarial attack strategies. ", "keywords": ["Graph Neural Network", "Adversarial Attack", "Influence Maximization"]}, "meta": {"decision": "Reject", "comment": "This paper relates the problem of influence maximization and adversarial attacks on GCNs. \nThe paper, and its formulation and assumptions stirred up quite a discussion among the reviewers and the authors. I do appreciate the thorough rebuttal that the authors provided, and the reviewers did take it into account (and revised their scores). \nHowever, all in all, I am afraid that there are just a few too many concerns with this paper. \nIf the authors take the reviews to heart, they should be able to improve the manuscript and submit a stronger and improved version to the next conference. "}, "review": {"SQ2GEyyPzmN": {"type": "rebuttal", "replyto": "XVQQbSInyEh", "comment": "Thanks for updating the evaluation. That is encouraging!\n\n---\n\nBeyond the theoretical justifications, this assumption also helps us derive practically effective attack strategies. We note that the derivation of Eq. (3) relies on this assumption, which is a critical foundation for the development of the following concrete and practical attack strategies. \n\nThere are multiple existing works in the GNN literature where the developments of practical techniques rely on assumption 1. For example, [1] developed a skip-connection structure on top of GCN with theoretical justifications relying on assumption 1; [2] derived a principled active learning algorithm for GNNs with assumption 1 as a critical foundation; [3] derived adversarial attack strategies also under assumption 1.\n\nDespite the approximation made by assumption 1, the new GNN architectures [1], active learning algorithms [2], and adversarial attack strategies [3 and ours], all demonstrate **effective empirical performances**, which are strong empirical evidence that assumption 1 aligns well with the **practical behavior** of GCN. \n\n---\n\nWe will change the plot of Figure 3 in the final version of this draft. Thanks for the thorough consideration! \n\n---\n\nReferences\n\n[1] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, Stefanie Jegelka. Representation Learning on Graphs with Jumping Knowledge Networks. ICML 2018.\n\n[2] Yuexin Wu, Yichong Xu, Aarti Singh, Yiming Yang, Artur Dubrawski. Active Learning Graph Neural Networks via Node Feature Propagation. ArXiv 2019.\n\n[3] Jiaqi Ma, Shuangrui Ding, Qiaozhu Mei. Black-box adversarial attacks on graph neural networks with limited node access. NeurIPS 2020.", "title": "More elaborations on Assumption 1"}, "IE1LpHqMwzT": {"type": "review", "replyto": "sbyjwhxxT8K", "review": "##########################################################################\nSummary:\nThis paper studies the problem of designing adversarial attacks (on GNN models) that perturb the feature to maximize the misclassified instances. Assuming that the activations are activated independently at random, the paper shows that the attack design can be reduced to the influence maximization problem under the threshold model. The paper identifies several conditions on the threshold that can make the influence maximization problem submodular, thereby making it easy to optimize.  Experiments have been shown that the proposed attack method has higher performance compared to the existing ones.\n\n##########################################################################\nI find the paper interesting but lean towards rejection at this point. The main reason is that the assumptions are not realistic, and the contributions are incremental.\n##########################################################################\n\nStrength:\n\nThe technical proofs seem to be sound, and the relationship between the attack design and influence maximization is an interesting observation. \n\nWeakness:\n\nThe entire analysis is based on Assumption 1, but the paper does not provide a formal description of the data flow in the network under such an assumption, which makes it hard to follow the subsequent analysis. In addition, this assumption is very restrictive in the sense that it makes all the activation functions (\\sigma) output random numbers, ignoring what has been received from the last layer. For theoretical analysis on general neural networks, such an assumption is somehow minimal (though not realistic, as pointed out in Kawaguchi 2016) to obtain theoretical results, but it is overly strong for designing a practical attack. In particular, assuming Assumption 1 means that we have ignored the structure of GCN, and therefore, it is not appropriate to target this paper for GCN model. In general, I would not think a meaningful attack could be designed without having certain types of prior knowledge. \n\nAnother concern is that the technical analysis largely follows Ma 2020 - the proofs and ideas are very similar. From the introduction, the main difference is to adopt Assumption 1 and get theoretical results, which is somehow incremental.\n\nThe amount of perturbation is not extensively discussed in the experiments; to me, the strength of perturbation is critical to the attack performance. \n\nMinor issues:\n\nIt would better to introduce the goal of the attack along with the attack setup.\nPlease define \\E_path[H(S)] before using it.\nPlease introduce RWCS and GC_RWCS before using these terms.\n\n\n\n", "title": "The paper studies an interesting problem but the settings are not very realistic.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "6DEHktX1-kd": {"type": "rebuttal", "replyto": "pz_h8e1ztHR", "comment": "Thank you for the follow-up and for the suggestions to further improve our clarity!\n\nNow in our updated draft, we have restated Assumption 1 as Assumption 2 in Appendix A.2 and made it precise in math. \n\nAt a high level, this assumption does make the **activation** (but NOT the **output**) of ReLU ignore the input. We would like to highlight that this assumption does NOT ignore the structure of GCN, which is why, under this assumption, the derived attack strategies are still effective. ", "title": "Thanks for the follow-up and we updated the draft accordingly!"}, "PYmwdf6fbnz": {"type": "rebuttal", "replyto": "vvm2PTl7n51", "comment": "We appreciate the detailed suggestions and we've updated the draft accordingly.\n\n---\n\n(1)\n\nWe changed the term from \"black-box\" to \"near-black-box\" per the suggestion by the reviewer. \n\nIn addition, we also *added the suggested experiment setup* (restricting to the training partition) and the results are now shown in Table 4 in Appendix A.3 of the updated draft. The results are very similar to those in Table 1, which is not surprising since we only use coarse gradient information.\n\n---\n\n(2)\n\nWe further updated the draft to clarify in Sec. 5.1 that in practice, \"the hyper-parameter L does not have to be the same as the number of layers of the  GNN being attacked\", and to clearly state in Sec. 5.2 that our experiments used 2-layer GC, 2-layer GAT, and 7-layer JKNet.\n\n---\n\n(3)\n\nWe added more details of the synthetic experiments in our updated draft. Further for your reference, we used the code by [Ma et al. 2020], which is publicly available here: https://github.com/Mark12Ding/GNN-Practical-Attack/blob/main/utils.py#L11\n\n", "title": "Thanks for the suggestions and we've updated the draft accordingly!"}, "npAglBvXep3": {"type": "rebuttal", "replyto": "xsrpHNpp33w", "comment": "Thank you again for the follow-up! \n\n---\n\n(a) \n\nFor the distribution parameter $a$, it should be implicitly dependent on $\\epsilon$ but unknown.\n\nFor the hyper-parameter $a$, if we were to conduct very careful hyper-parameter search on $a$ given $\\epsilon$, then for different $\\epsilon$, the *optimal hyper-parameter* $a$ should also differ; and hence in this sense, the *optimal* hyper-parameter $a$ is relevant to $\\epsilon$. \n\nIn our experiment, we find that not carefully searching the hyper-parameter $a$ also works fine in practice. So we just set it as a fixed value. In this sense, the *fixed hyper-parameter* we set is indeed irrelevant to $\\epsilon$. \n\n---\n\n(b) \n\nYes, your description is correct. That is the way we simulate the domain knowledge following Ma et al. (2020).\n\nTo further address the potential concern regarding the use of the term \"black-box\" in a strict sense, we are changing the term from \"black-box\" to \"near-black-box\" in our draft (to be updated soon), per the suggestion by AnonReviewer4.", "title": "More explanations."}, "aeTKN5DI2OP": {"type": "review", "replyto": "sbyjwhxxT8K", "review": "Summary of the paper:\n--------------\n\nThe authors propose an adversarial attack strategy for graph neural networks based on influence maximization. The attack is (claimed to be) black-box (does not have direct access to the model), evasion-based, and limited to perturbations of the node attributes (i.e. cannot insert / delete edges). The authors make simplifying assumptions about the GCN model (all ReLU paths are equally likely) and the distribution of the individual nodes' thresholds, theta_j. The attack is evaluated experimentally on three well-known datasets and three different models. The proposed attack outperforms the centrality-based baselines and the baselines of [Ma et al. 2020].\n\nStrengths:\n---------\n\n* The paper is well-written and generally easy to follow. \n* The connection to influence maximization is interesting. \n* The problem of GNN robustness under (realistic) adversarial attacks is important.\n\nWeaknesses:\n-----------\n\n* The description of the method and experimental set-up leave a number of open questions (see detailed feedback).\n* Despite claiming black-box attacks, the method actually uses the target model's gradients, even for the test set, for the attack.\n* The experimental evaluation is rather slim. For example, there is no analysis of the node selection algorithm, e.g. what kind of nodes are selected, no analysis of the sharp differences in performance on the different models, and no analysis of the hyperparameters a and sigma.\n\nDetailed comments:\n------------------\n\nI have several concerns about both the method as it is described in the paper as well as the experimental evaluation.\n\n\nMajor points:\n\nMethod:\n* The authors emphasize the black-box nature of their attack, however the perturbation vector requires access to the respective model's gradient, which violates the black-box setting. This is especially critical since Eq. (10) involves the gradient of the loss of ALL nodes, including the validation and test nodes. So effectively, the attack utilizes the gradient of the loss of the test nodes. Also, it is not stated which value of lambda is used for the perturbation, i.e. what magnitude the perturbation has. The authors mention that this is done because the input features lack semantic meaning; however, they do not mention how potential semantic meaning of the features could be used instead of using the gradients.\n* The neighbor weight of alpha_{ij} = 1/|N_i| is not the weight proposed by [Kipf and Welling 2017]. Instead, they propose alpha_{ij} = (d_i+1)^{-0.5} * (d_j+1)^{-0.5}, where d_i is the degree of node i.\n* Why is the attack target the misclassification rate on the whole dataset (Eqs. (3,5,6,7)) and not only on the test set? Does this mean that the results in Table 1 are also reported on the whole dataset? If not, why is there a mismatch between the reported misclassification rate and the optimization objective?\n* Eq. (4) is unclear/not well defined. As stated above the equation, hat{k}_j is the predicted class of node j after perturbing the set S. However, if S fails to change the predicted class of node j, the numerator and the denominator both become 0, i.e. ill-defined. \n* Sec. 4.4: the authors mention that the first approximation leads to the problem becoming \"likely to be submodular\". Why is it only likely to be submodular and not guaranteed? \n* Also Sec. 4.4: According to the authors, the first approximation \"integrates out the randomness in data [...]\". Where is there any randomness in the data?\n* Section 4 shows how the attack can be instantiated for GCN. How is the attack adapted to the other models, i.e. GAT and JKNet?\n* The paper just briefly mentions that Eqs. (6) and (7) are used to select the target nodes via a greedy influence maximization approach. A few more sentences on how this will be exactly done would be readers not familiar with inf. max.\n* The authors should give more details (proof and/or reference to existing work) regarding the derivation of Corollary 1 and 2.\n\n\nExperimental evaluation:\n* Unrealistic set-up: 60% train data. For semi-supervised node classification, typically we have 10% or less training samples.\n* The authors use L=4 layers for the GNNs. However, [Kipf and Welling 2017] report substantially worse performance for L=4 compared to, e.g., L=2. What are the results of the attack when the L used for attacking is different than the L of the GNN?\n* The node attributes in the datasets have special constraints, e.g. for Citeseer and Cora the features are binary, and for Pubmed the features are nonnegative. How is this accounted for by the perturbation model?\n* How do the choice of sigma and a influence the results?\n* Is there any insight into why JKNet and GAT seem to be much more fragile to the attacks? Even for the otherwise very weak random attack GAT's and JKNet's performances drop sharply. Why does this happen?\n* There is no analysis into what kind of nodes the proposed algorithm selects; since node selection is the main contribution of this paper, some insight into this would greatly benefit the experimental evaluation.\n\n\nMinor points:\n* p. 1 original from => original form\n* p. 6 first paragraph: non-decreasing => non-increasing\n* The paper does not cite the correct sources for the dataset.", "title": "Official Blind Review", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Iiv8NjsjPp0": {"type": "rebuttal", "replyto": "guAkHqxA-DH", "comment": "Thank you very much for the follow-up and for providing us the opportunity to make further clarifications! \n\n---\n\n(1) Connection between $\\theta$ and $\\epsilon$. \n\nWe guess the confusion is possibly due to the subtle distinction between the $a$ (or similarly for $\\sigma$) as the **distribution parameter** of $\\theta$ and the $a$ as the **hyper-parameter** of the derived attack strategy. \n\nThis distinction is perhaps easier explained through an analogy to fitting data from Gaussian mixtures. Suppose we are given a dataset generated by a mixture of $k=3$ Gaussian distributions. However, the ground-truth number of mixtures $k$ is not revealed. Therefore we have to treat $k$ as a hyper-parameter and, in practice, we may find fitting a Gaussian mixture model with $k$ set to 5 still achieves good performance. \n\nIn our case, the $a$ as the distribution parameter is relevant to $\\epsilon$, analogous to the ground-truth $k=3$. However, the exact dependence $a(\\epsilon)$ is unknown due to the limited attacker knowledge. In our attack strategy, we treat $a$ as a hyper-parameter (analogous to the hyper-parameter $k=5$) and set it as a fixed value. \n\nWe note that **it is possible (though non-trivial) to further optimize the hyper-parameter $a$ according to the choice of $\\epsilon$**, which should lead to even better attack performance. However, our experiment results demonstrate that even without careful optimization of the hyper-parameter, the proposed attack strategies already achieve decent attack performance. Moreover, we have conducted sensitivity analysis in Figure 3 and Figure 4 in the Appendix A.3 to show that the proposed strategies are fairly robust to the hyper-parameter choice. \n\nWe also updated the \"Implementation of InfMax-Unif and InfMax-Norm\" part in Sec. 5.1 to reflect the clarifications.\n\n---\n\n(2) Regarding the gradient.\n\nWe clarify that the use of gradients is more about the experiment setup than the attack strategy. \n\nIn many real-world scenarios, we often know a couple of critical node attributes based on our knowledge of the prediction task. For example, the attribute of \"age\" might be a critical feature when predicting \"income\". In these cases, we can construct a perturbation vector $\\epsilon$ based on the semantic meaning of the features, without the need to query the model. \n\nFor the three benchmark datasets (Cora, Citeseer, and Pubmed), however, we have lost track of the semantic meaning of the features. To construct a meaningful perturbation vector $\\epsilon$ for the purpose of experimental evaluation, we use the gradients to *simulate* the domain knowledge of the task. Nevertheless, we would like to highlight that\n\n- **very mild gradient information** is used to construct $\\epsilon$: we only use the coarse population-level (rather than node-level) average gradients to identify which few entries of $\\epsilon$ to be non-zero, and the magnitudes of these non-zero entries are irrelevant to the gradients;\n- our paper focuses on the node selection step, and all the attack strategies (including ours and all the baselines) use **exactly the same $\\epsilon$** so the comparison is fair;\n- we follow exactly the same setup as [Ma et al. 2020], which is the reason we leave the detailed description of this experiment setup in Appendix A.2.\n\nWe believe such mild information, i.e., a few important features for the prediction task, is usually available from domain knowledge and thus our simulation is a reasonable proxy of domain knowledge. To verify this, we also further have **an additional synthetic data experiment**. In particular, we generate synthetic node classification data where a couple of important features for the task are known as (model-agnostic) domain knowledge, so that we can construct a constant perturbation vector in the pure black-box setting **without any query to the model**. And we demonstrate that the proposed strategies still outperform the baselines (see Table 3 in Appendix A.3).\n", "title": "Thank you for the follow-up and here are more clarifications."}, "egHLd4L1Gj": {"type": "rebuttal", "replyto": "v4tlgRbGopE", "comment": "#### Missing details\n\n(1) Connection between $\\theta$ and $\\epsilon$.\n\nThis is a great question! First we clarify that $\\epsilon$ is the perturbation vector to be added to the node features. So it is always needed to complete the attack process in the experiments. \n\nRegarding the distribution of $\\theta$, it is indeed affected by $\\epsilon$. Even with approximations we made in Corollary 1 (uniform) and 2 (normal), they are still relevant to $\\epsilon$ through the parameters $a$ or $\\sigma$. Ideally, we could optimize the choice of $a$ or $\\sigma$ according to $\\epsilon$ as the attacker knows the perturbation vector $\\epsilon$. However, in this work, we find even simple choices of $a$ or $\\sigma$ would already result in effective attack performance, leaving room for further optimizations in future work. \n\nAn additional evidence that the distribution of $\\theta$ is affected by $\\epsilon$ lies in the additional experiment (Figure 3 in the Appendix A.3) of different perturbation strength. Recall that the baseline RWCS is a special case of InfMax-Unif with $a=\\infty$. Intuitively, when the perturbation strength is small, the parameter $a$ or $\\sigma$ should be larger. And, in Figure 3, indeed we find that RWCS outperforms InfMax-Unif (with $a=0.01$) in a few settings and when the perturbation strength is small. \n\n(2) The gradient.\n\nAs explained before Eq. (10), in practice, the construction of the perturbation vectors should be done with the domain knowledge of the task and the semantic meaning of the node features. In our experiments, as there lacks semantic meaning of the features, we are following [Ma et al. 2020] to use very mild information from the average gradient to simulate the domain knowledge of the task to construct the perturbation vector. To verify our methods in a pure black-box setting, we further added a synthetic data experiment in Table 3 in the updated draft, where we are able to know the important features without access to the model.\n\nWe also added in the updated draft that $\\lambda$ is fixed as 1.\n\n(3) Selection of the feature indexes j.\n\nWe clarify that the top feature indexes j are selected in terms of the original average gradients. And the selected few indexes are assigned to a constant $\\lambda$ multiplying the sign of the gradient to form a sparse perturbation vector $\\epsilon$. \n\n(4) Improvement over baselines.\n\nWe first note that GC-RWCS is a strong baseline. As discussed in the Appendix A.2, it can be viewed as a special variant of the proposed method. We also highlight that the advantages of the proposed strategies over GC-RWCS are three-fold: 1) performance gain, 2) fewer hyper-parameters, and 3) better interpretations.\n\nRegarding the distribution of $\\theta$, we did plot some real distributions assuming W is known in Figure 2 as well as Figure 5. And many examples of the real distribution of $\\theta$ present bell shapes that are close to normal distributions. \n\n(5) The efficiency\nIn practice, both the proposed strategies and RWCS/GC-RWCS are very fast. We call them \"efficient\" in the sense that the solutions are efficient compared to solving the original combinatorial optimization problem. \n\nA more detailed analysis is shown here. Both the proposed strategies and RWCS/GC-RWCS need to first compute an L-step random walk probability matrix B through sparse matrix multiplication, which is O(NE), where N is the number of nodes and E is the number of edges. For RWCS, one only needs to have a column sum of B, then take argmax over the sums, which in total is O(N^2). For the proposed strategies and GC-RWCS, we need to do some minor updates on B and do the column sum for each node to be selected. And the operation in each step is still O(N^2). To greedily select r nodes, we would have a time complexity of O(rN^2). Note r << N. Further, in the case when we only target at the test set of, say, S nodes, all the O(N^2) will be reduced to O(NS). We also note that the most of the O(N^2) operations can be made parallel and exploit the sparsity of the matrix B.\n\n---\n\n#### Missing references\n\nThanks for the pointers! We have added these references in the updated draft.\n", "title": "Response to AnonReviewer3 (Part 2)"}, "v4tlgRbGopE": {"type": "rebuttal", "replyto": "nUUJy9ylFl3", "comment": "We appreciate the detailed questions and comments by the reviewer and address them below. In particular, we would like to highlight that we provide responses clarifying why the current thread model is meaningful and showing the assumptions on $\\theta$ lead to practical attack strategies. We also address the missing details with thorough discussions as well as additional experiments added in the updated draft.\n\n---\n\n#### Concerns regarding the threat model.\n\nWe first explain the motivation of the restricted black-box setup where querying model predictions is limited. A relevant real-world scenario is attacking a GNN-based recommender system on a social network. While the attackers may be able to query the model predictions for a handful of accounts, it is impossible for them to query the model predictions for a massive number of accounts. \n\n[Ma et al. 2020] proposed this setup and had a detailed discussion of the motivations. So in our submission we refer the motivations to that literature as the setup is not our contribution. However, we agree that elaborating the motivations is beneficial for the completeness of this paper and for readers who are not familiar with the literature, and thanks for raising up this question! We added a brief discussion at the end of the first paragraph of Section 1 in our updated draft.\n\nRegarding the perturbation type, we agree that structure perturbation attack is a more interesting problem. However, such an attack is also technically more challenging under this realistic but extremely restricted black-box setup. So we leave this problem as a future work. We also updated our conclusion to include this future direction.\n\nFinally, we would like to highlight that, under the restricted black-box setup, even the problem of node feature perturbation raises difficult technical challenges left open by [Ma et al. 2020]. And this work fills the gap through non-trivial analysis of the problem. \n\n---\n\n#### Assumptions on $\\theta$ distribution\n\nWe acknowledge that the uniform and normal assumptions are somewhat oversimplified assumptions. However, we make the following three remarks.\n\n(1) The main goal of the analysis is to develop practically effective adversarial attack strategies. Due to the restricted information available to the attackers, it is inevitable to make certain approximations of the model to derive attack strategies. We highlight that, despite the approximations brought by our assumptions, the proposed strategies are still empirically effective. \n\n(2) For the expected objective to be submodular, we only require the distribution of $\\theta$ to be non-increasing on the positive region. The real distribution of $\\theta$ seems to align well with this condition according to our visualizations in Figure 2 in Section 5.3 and Figure 5 in Appendix A.4 (Figure 3 in Appendix A.3 in the original draft). Further, many examples of the real distribution of $\\theta$ present bell shapes that are close to normal distributions. \n\n(3) While we have made simplifying assumptions, our proposed strategies are still more interpretable and theoretically justified compared to baseline methods. In Section 5.1 and Appendix A.2, we discussed how the baselines RWCS and GC-RWCS can be viewed as special variants of the proposed InfMax-Unif strategy.\n\n---\n\n(To be continued)", "title": "Response to AnonReviewer3 (Part 1)"}, "NtW2X-j04-T": {"type": "rebuttal", "replyto": "IE1LpHqMwzT", "comment": "We appreciate the reviewer for the detailed comments and we address your specific concerns in this response. In particular, we would like to highlight that there seems to be some misalignment between our understandings of Assumption 1 and the work of [Ma et al. 2020], as we clarify below. We also added *an additional sensitivity analysis* for the perturbation strength in our updated draft.\n\n---\n\n#### Concerns regarding assumption 1\n\nAssumption 1 is indeed an oversimplified assumption for neural networks and we agree that one should be careful about the context to properly use this assumption. However, we think the use of this assumption is proper in our case for the following reasons.\n\n(1) We would like to clarify that assumption 1 does NOT ignore the (graph) structure of GCN. It only makes approximations on the ReLU functions. In fact, it has been reported that assumption 1 aligns particularly well with GCN in practice. For evidence, this assumption is used in quite a few existing GNN literatures [1,2,3]. This is also explicitly stated by Keyulu Xu (the author of [1]) in a public comment in this openreview post: https://openreview.net/forum?id=S1ldO2EFPr&noteId=SklIn_C3uB\n\n(2) Even under the simplified assumption 1, the proposed attack strategies already present practical and superior attack effects in the empirical experiments. Relaxing the assumption might lead to even better attack strategies, which would be an interesting future direction to explore. \n\n(3) An attacker usually has limited information about the model and simplifying assumptions are often involved when designing an adversarial attack strategy. In this sense, the task of designing adversarial attacks is even more tolerant to simplified assumptions compared to that of improving classification models. \n\n---\n\n#### Comparison with [Ma et al. 2020]\n\nFirst, we would like to clarify that Assumption 1 is also used in [Ma et al. 2020] so this is not the difference between our work and theirs. \n\nNext, while we follow the black-box attack setup and the experiment setup in [Ma et al. 2020], the technical analyses, including the proofs and the proposed attack strategies, are significantly different. And we highlight the main differences below.\n\n(1) Our analysis targets on directly maximizing the mis-classification rate, which is a discrete function; [Ma et al. 2020] focus on the analysis of the smooth classification loss. Our analysis on the discrete function is more challenging and the proofs are very different. \n\n(2) As a result of (1), the attack strategies derived from our theoretical analysis are directly effective on increasing the mis-classification rate. In [Ma et al. 2020], however, the attack strategy (RWCS) derived from their analysis needs to be combined with heuristic tricks (GC-RWCS) to work well in practice. Our proposed attack strategies have less hyper-parameters and are more interpretable than that of [Ma et al. 2020]. In Section 5.1 and Appendix A.2, we also discussed how RWCS and GC-RWCS can be viewed as special variants of the proposed InfMax-Unif. \n\n(3) Our analysis establishes a novel connection between the adversarial attack on the graph neural networks and the well-studied influence maximization problem, which not only obtains stronger results but also opens up many opportunities for follow-up explorations.  \n\n---\n\n#### The strength of the perturbation\n\nYes, the strength of perturbation does affect the attack performance but we find the relative rank between different attack strategies is pretty stable. We further added a sensitivity analysis with respect to the strength perturbation (see Figure 3 in the Appendix) in the updated draft. \n\n---\n\n#### Minor Issues\n\nThanks for pointing out the unclear parts! We updated the draft according to the suggestions.\n\n---\n\n#### References\n\n[1] Keyulu Xu, , Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, Stefanie Jegelka. Representation Learning on Graphs with Jumping Knowledge Networks. ICML 2018.\n\n[2] Yuexin Wu, Yichong Xu, Aarti Singh, Yiming Yang, Artur Dubrawski. Active Learning Graph Neural Networks via Node Feature Propagation. ArXiv 2019.\n\n[3] Jiaqi Ma, Shuangrui Ding, Qiaozhu Mei. Black-box adversarial attacks on graph neural networks with limited node access. NeurIPS 2020.\n", "title": "Response to AnonReviewer1"}, "1wI2ra_ph_": {"type": "rebuttal", "replyto": "aeTKN5DI2OP", "comment": "We are very grateful for the detailed review, which helps improve our draft a lot in the updated version. We provide both in-depth discussions and multiple additional experiments in the updated draft to address all the concerns raised by the reviewer. In particular, we would like to highlight that 1) we clarify that the use of gradient information is very mild, and further add a synthetic data experiment with a pure black-box setup; 2) we add sensitivity analysis experiments; 3) an additional experiment on the proposed methods that targets on the test set further improves the attack performance on the test accuracy, thanks to the suggestion by the reviewer.\n\n---\n\n#### Method\n\n(1) Black-box evaluation\n\nWe acknowledge that the current evaluation setup is not a perfect black-box setup. However, we first note that we follow the same evaluation setup as [1] and highlight that the current use of model information is very mild: determining a few number of important features and the binary direction to perturb for each selected feature, only at the **global level** (the same constant perturbation is added to each node to be perturbed). Such mild information should be available if we have more domain knowledge about the task and the features, and then we can move to a full black-box setup. \n\nWe added in the updated draft that $\\lambda$ is set as 1.\n\nTo verify this and further address the concern, we also add *an additional synthetic data experiment* following the recently updated version of [1]. In particular, we generate synthetic node classification data where a couple of important features for the task are known as (model-agnostic) domain knowledge, so that we can construct a constant perturbation vector in the *pure black-box setting*. And we demonstrate that the proposed strategies still outperform the baselines (see Table 3 in the updated draft).\n\n(2) Neighbor weight\n\nThanks for spotting this mismatch. The D^{-1}A variant of GCN was used in [2] and also works well in practice. We changed the citation at the end of Section 3.1 to [2] in the updated draft to make it precise. \n\nIt is also worth noting that, despite the nuance between the analyzed model and the empirically evaluated model (GCN, GAT, JKNet), the derived attack strategies show effective performance in the experiments, which suggests the nuance is not critical for the purpose of developing attack strategies. \n\n(3) Target on test set\n\nFirst, we clarify that the results in Table 1 are the accuracy obtained on the test set (now explicitly stated in the updated draft). Second, we note that the objectives can be easily adapted to sum over the test set by changing the summation terms. \n\nWe agree with the reviewer that targeting on the test set should further improve our proposed strategies in terms of the drop of accuracy on the test set. This is indeed verified by *an additional experiment* we added in the updated draft. By modifying the proposed strategies to target on the test set, the attack performances are consistently improved in all settings (see Table 2 in the Appendix).\n\n(4) Edge case of Eq. (4)\n\nThanks for spotting it. We have now defined the $\\theta_j$ to be $\\infty$ when $\\hat{k}_j = y_j$ in the updated draft.\n\n(5) Likely to be submodular\n\nThe submodularity of the expected objective h(S) relies on the marginal CDFs to be concave. So it is only \"likely\". \n\n(6) Randomness in the data\n\nThe \\theta depends on both the model parameters and the data, which we do not have full access to (we do not know the model parameters and the node labels). So we instead treat them as random, and taking expectation over \\theta essentially integrates out the randomness. \n\nWe elaborated a little bit on this paragraph in the updated draft to address 5 and 6.\n\n(7) Adapt to GAT and JKNet\n\nWe note that the derived attack strategies themselves do not rely on the specific GNN models so they can be directly applied to most GNN models, though without theoretical justifications. Nevertheless, the goal of the experiments on GAT and JKNet is to demonstrate that the strategies theoretically derived for GCN can be empirically generalized to various other types of GNN models. \n\n(8) Explanation of the greedy strategy\n\nAdded the following explanation in the updated draft: each strategy iteratively selects nodes into the set to be perturbed up to a given size. At each iteration, the node, combining with the existing set, that maximizes Eq. 5 or Eq. 6 will be selected.\n\n(9) Derivation of Corollary 1 and 2\n\nAdded the explanation: Corollary 1 and 2 directly follow Proposition~\\ref{prop:submodular} given the cumulative distribution functions of the uniform distribution and the normal distribution as well as the fact that they are concave at the positive region.\n\n---\n\n(To be continued)", "title": "Response to AnonReviewer4 (Part 1)"}, "mABo_4IVos8": {"type": "rebuttal", "replyto": "1wI2ra_ph_", "comment": "#### Experimental evaluation\n\n(1) Training split\n\nWe first note that we are following the training split setup of [1], which itself follows the setup of JKNet [3]. We also note that, as the goal of the experiments is to demonstrate the effectiveness of the attack strategies rather than improving the classification performance, we consider the training split as a nuisance parameter. \n\n(2) The parameter L\n\nWe clarify that the parameter L only refers to the number of random walks used in the attack strategies but does not refer to the number of GNN layers. We used the common choice of number of layers for each model, i.e., 2 layers for GCN and GAT, and 7 layers for JKNet.\n\nThe fact that the number of random walks used in the attack strategies is different from the number of layers in the GNN models suggests that the proposed strategies do not need to know the exact number of layers of the GNN models to be effective. This phenomenon is also reported in [1] for the baseline GC-RWCS. \n\n(3) Feature constraints\n\nWe used the datasets preprocessed by the Deep Graph Library [4], where the features are already made continuous. So we didn't consider the feature constraints in our experiments.\n\nThis is indeed an interesting question. However, we consider it out of the scope of the main goal of this work. We added it as future work in our conclusion. \n\n(4) Choice of $\\sigma$ and $a$\n\nWe first note that the $\\sigma$ and $a$ are fixed as 0.01 in all our experiments and they generalize well across datasets and models. The results are also fairly robust for a range of choices and we added a sensitivity analysis in Figure 4. \n\n\n(5) Performance on GAT and JKNet\n\nUnfortunately we have no solid clue so far for this phenomenon. But one guess is that more complicated models tend to overfit the decision boundaries more, and thus are more sensitive to the adversarial attack. \n\n(6) Analysis on the nodes being selected\n\nThis is a good and also hard question! It is not yet clear to us what characteristics at the node level will provide the most effective attack, as otherwise we can directly select nodes based on these characteristics. One insight we do have is that, not surprisingly, the selected nodes by our methods do not have the highest centrality scores, which indicates most existing centrality measures are not good indicators in terms of adversarial attack. We plan to further investigate along this direction in the future work, which may potentially lead to simpler attack strategies.\n\n---\n\n#### Minor points\n\nWe have corrected them in the updated draft. Thanks for catching them!\n\n---\n\n#### References\n\n[1] Jiaqi Ma, Shuangrui Ding, Qiaozhu Mei. Black-box adversarial attacks on graph neural networks with limited node access. NeurIPS 2020.\n\n[2] Will Hamilton, Zhitao Ying, Jure Leskovec. \"Inductive representation learning on large graphs.\" NeurIPS 2017.\n\n[3] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. ICML 2018. \n\n[4] https://www.dgl.ai/\n\n", "title": "Response to AnonReviewer4 (Part 2)"}, "n0lJ2CJKKVa": {"type": "rebuttal", "replyto": "s3W8yQqftnf", "comment": "We thank the reviewer for the recognition as well as the questions. And we answer your specific questions below.\n\n---\n\n(1) Analogy to IM.\n\nUnder the assumptions in Corollary 1, the problem reduces to the classic IM problem under LT where $\\theta$ has uniform distributions. In this case, we believe those properties of the general IM problem under LT do extend to our specific problem. We added this comment under the Corollary 1 and 2 in the updated draft. \n\n---\n\n(2) Complexity.\n\nIn practice, both the proposed strategies and most baselines are very fast. We call them \"efficient\" in the sense that the solutions are efficient compared to solving the original combinatorial optimization problem. \n\nA more detailed analysis is shown here. Both the proposed strategies and RWCS/GC-RWCS need to first compute an L-step (L=4 in this paper) random walk probability matrix B through sparse matrix multiplication, which is O(NE), where N is the number of nodes and E is the number of edges. This step is efficient for sparse real-world graphs. For RWCS, one only needs to have a column sum of B, then take argmax over the sums, which in total is O(N^2). For the proposed strategies and GC-RWCS, we need to do some minor updates on B and do the column sum for each node to be selected. And the operation in each step is still O(N^2). To greedily select r nodes, we would have a time complexity of O(rN^2). Further, in the case when we only target at the test set of, say, S nodes, all the O(N^2) will be reduced to O(NS). We also note that the most of the O(N^2) operations can be made parallel and exploit the sparsity of the matrix B.\n", "title": "Response to AnonReivewer2"}, "nUUJy9ylFl3": {"type": "review", "replyto": "sbyjwhxxT8K", "review": "Paper summary: The paper studies the problem of attacking GNNs in a restricted black-box setup, i.e., by perturbing the features of a small set of nodes, with no access to model parameters and model predictions. The authors draw a connection between the restricted attack problem and the influence maximization problem, and then propose several approximation techniques to solve the reformulated attack problem. Experimental results on attacking three GNN models demonstrate the effectiveness of the proposed attack. \n\n\nStrengths\n+The paper is well-written and well organized\n+Solving the restricted black-box based on influence maximization is novel and interesting\n\nWeaknesses\n-Threat model is limited\n-Assumptions are not satisfied\n-Some details are unclear \n-Missing important references\n\n\n\nDetailed comments:\n\n-Threat model is limited.\n\nMy first concern is that the proposed attack only focuses on node feature perturbation, while not mentioning structure perturbation. For attacks to graph neural networks, structure perturbation attack is more meaningful and effective. (see Zugner et al., 2018, Dai et al., 2018). \n\nMy second concern is that why restricting that the attacker does not know model predictions? What\u2019s the motivation and in what scenarios? From my understanding, even in the black-box attack, an attacker can obtain the model  predictions, e.g., via querying the model using the input nodes. \n\n-Assumptions are not satisfied\n\nThe authors assume that theta is from certain simple distributions, in order to make the objective function submodular. However, from the experimental results, such simple distributions are inappropriate. \n\n-Some details are missing\n\nIn Equation (4), there is a connection between theta and epsilon.  But in your experimental setup, this connection is lost. Is epsilon necessary in your experiments, as theta is assumed to satisfy some distribution that is irrelevant to epsilon? \n\nHow do you obtain the gradient (\\partial L / \\partial X) in Equation (10) in the black-box setting? What\u2019s the lambda value? \n\nAs you use the sign of the gradient and a fixed lambda, all the positive gradients should generate the same value and thus the feature perturbations. In this case, how do you select the feature indexes j as there are many ties in epsilon_j?  \n \nIn all experimental results, the improvement of the proposed attack over RWCS/GC-RWCS is marginal (less than 3 percent). I think one reason could be that the selected simple distribution for theta largely deviates from the true distribution. Can you plot the true theta distribution, assuming that W is known in advance?   \n\nWhat\u2019s the efficiency of the proposed attack? Is it better than RWCS/GC-RWCS? \n\n-Missing important references\nThe authors miss the following important works on attacks to graph neural networks:\n\nWang and Gong, \u201cAttacking Graph-based Classification via Manipulating the Graph Structure\u201d, CCS\u201919\nWu et al., \u201cAdversarial Examples for Graph Data: Deep Insights into Attack and Defense\u201d, IJCAI\u201919\nEntezari et al., \u201cAll You Need Is Low (Rank): Defending Against Adversarial Attacks on Graphs \u201d, WSDM\u201920\u2028\n  \n", "title": "The paper is interesting and the proposed attack method is novel. However, some important details are missing and the threat model is limited.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "s3W8yQqftnf": {"type": "review", "replyto": "sbyjwhxxT8K", "review": "This paper introduces a novel connection between adversarial attack on graph neural networks in a restricted black-box setup via node feature perturbation, on the one hand, and the influence maximization problem under the linear threshold model on the same graph, on the other hand. An analysis shows that the objective function of the corresponding IM problem is submodular under assumption, hence the problem admits greedy approximation algorithms as effective black-box attack strategies. Experiments show such attacks are effective compared to baselines in degrading the performance of GNNs in terms of mis-classification rate.\n\nA question that remains unaddressed is whether the analogy to the IM problem under the LT model extends to other properties of the problem: under the uniform distribution of threshold \u03b8, the diffusion of influence under the LT is equivalent to a diffusion whereby each node picks at most incoming edge to be active with probability corresponding to the incident edge's weight. This paper would be stronger if it addressed the question of whether this property also extends to the analogy, apart from the submodularity property.\n\nUnder the current analysis, two greedy algorithms are proposed for two different objectives, yet there is no analysis of their complexity and no runtime results on their efficiency. The algorithms are called efficient, but no evidence is provided to that effect. That is a weak point of the paper.", "title": "Interesting connection revealed, efficiency claim unsubstantiated", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}