{"paper": {"title": "Selfish Emergent Communication", "authors": ["Michael Noukhovitch", "Travis LaCroix", "Aaron Courville"], "authorids": ["michael.noukhovitch@umontreal.ca", "tlacroix@uci.edu", "aaron.courville@gmail.com"], "summary": "We manage to emerge communication with selfish agents, contrary to the current view in ML", "abstract": "Current literature in machine learning holds that unaligned, self-interested agents do not learn to use an emergent communication channel. We introduce a new sender-receiver game to study emergent communication for this spectrum of partially-competitive scenarios and put special care into evaluation. We find that communication can indeed emerge in partially-competitive scenarios, and we discover three things that are tied to improving it. First, that selfish communication is proportional to cooperation, and it naturally occurs for situations that are more cooperative than competitive. Second, that stability and performance are improved by using LOLA (Foerster et al, 2018), especially in more competitive scenarios. And third, that discrete protocols lend themselves better to learning cooperative communication than continuous ones. ", "keywords": ["multi agent reinforcement learning", "emergent communication", "game theory"]}, "meta": {"decision": "Reject", "comment": "There has been a long discussion on the paper, especially between the authors and the 2nd reviewer. While the authors' comments and paper modifications have improved the paper, the overall opinion on this paper is that it is below par in its current form. The main issue is that the significance of the results is insufficiently clear.  While the sender-receiver game introduced is interesting, a more thorough investigation would improve the paper a lot (for example, by looking if theoretical statements can be made)."}, "review": {"HkleHtCd9S": {"type": "review", "replyto": "B1liIlBKvS", "review": "This paper looks at the question of emergent communication amongst self-interested learning agents. The paper finds that \"selfish\" (ie. self-interested) agents can learn to communicate using a cheap talk channel as long as the objective is partially cooperative. \nThe paper makes states that this is is a novel finding that contradicts the previous understanding of emergent communication in the literature (side point: at least some of the papers referenced for this claim did not at all make the claim). \n\nI believe there is a major miss-understanding here: As noted in the paper, self-interested agents can learn to communicate in settings in which the reward function is cooperative. Furthermore, it is also known that in 2 player zero-sum there is no incentive to learn a communication protocol. \nThis clearly shows that talking about whether or not \"selfish\" agents can learn to communicate only ever makes sense within the context of a specific game / reward structure. \n\nWith this in mind, the main finding, agents learn to somewhat communicate with each other in a simple toy setting, with more communication happening when the payouts are more cooperative, is not very interesting. \n\nThis doesn't mean that there isn't a good paper to be written here, in principle. Finding simple settings in which SOTA multi-agent learning \"fails\", ie. doesn't find Nash policies, understanding why it fails and then finding ways to mend things is generally a good research direction. However, this would require a few things which are currently lacking from the paper: (1) clear understanding of the Nash policies for the different reward settings (2) Implementation of SOTA methods for MARL which are appropriate for this setting (3) In depth analysis of learning successes and failures, ideally in settings which have previously been studied in literature (given how task-specific this analysis necessarily is).\n\nRegarding 2: General sum games will generally have mixed-strategies as Nash equilibria (just think 'rock-paper-scissors'). With this in mind, using a deterministic policy for the receiver is inappropriate for making any claims about learning in general sum games. \nFurthermore, it is well known that independent gradient descent (IGD) is not generally going to converge in general sum games (consider the loss functions X * Y and - X *Y or matching pennies). So looking at the outcome of IGD without checking for convergence means the results could be just about anything. Indeed, we don't have to go all the way to writing about emergent communication or complex \"sequential social dilemma\" to study this, those issues can easily be found in (iterated) matrix games. \n\nThis gets us to the second major point of the paper. To the authors' credit,  LOLA [1] has been shown to help with convergence in general sum settings and to lead to the emergence of cooperation and reciprocity in iterated games. \n\nHowever, the key point for the \u2018cooperation\u2019 part is iterated. In a single shot setting (which is explored in this paper), there is simply no way for the agents to reciprocate with each other. So in short, I do not believe the authors' interpretation that agents learn to cooperate with each other because of LOLA, but I do believe that LOLA can help with the learning of mixed strategies (at least for the sender, given that the receiver is deterministic) and with stabilizing convergence. Lastly, the part of the experimental section is dominated by large error bars and graphs that are difficult to interpret.\n\n\nOther points:\n-\"..but train agents to emerge their own.\" (and many other instances). AFAIK \"to emerge something\" is grammatically wrong (and also sounds really odd). \n-\"Since the loss is differentiable with respect to the receiver, it is trained directly with gradient descent, so we are training in the style of a stochastic computation graph (Schulman et al., 2015).\". This is a weird statement. You don't need SCGs for training a supervised objective. Also, note that the loss is also differentiable with respect to the action of the 1st agent. It is trivial in this setting to compute the true expected return, if that is what you are after. Note my point above about deterministic policies\n-\"We perform a hyperparameter search to over both agents\u2019\" -> spurious \"to\"\n-\"We investigate a similar scenario but concern ourselves with learning agents as opposed to fully-rational agents that have full knowledge of the structure of the game, and we do not assume that agents use an existing language, but train agents to emerge their own\" .This would be interesting, if the game was complex.\n- L_1 vs L - these symbols are used inconsistently, with the subscript _1 sometimes being applied and sometimes not.\n-\"we can look to extant results\" - s/extant/extent?\n-\"We use the L2 metric only on hyperparameter search and keep L1 as our game\u2019s loss to maintain a constant-sum game for the fully competitive case.\" - A few points: (a) the game is not in general constant sum (b) By doing this hyperparameter search the evaluation is strongly biased towards 'fair' attributions. This seems highly problematic. \n-\"We report our results in Figure ??\" -> Broken reference. \n-\"We do not test b = 180\u25e6 because the game is constant-sum and therefore trivially Ls1 + Lr1 = 180\u25e6.\" -> So? It would still be interesting to see what learning agents do in this setting. \n\n[1]: \"Learning with Opponent Learning Awareness\", Foerster et al. \n\n[update: I have updated the score based on the discussion with the authors]. While the paper lacks execution and conceptual clarity, I believe the game itself is interesting and could serve as a starting point for more thorough investigation.\n", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 4}, "SJgddF_hoS": {"type": "rebuttal", "replyto": "Hklb-YSPir", "comment": "Interesting Reciprocity\nWe agree it is different and reciprocity is indeed interesting. We think that learning to agreeing to a protocol under competition is also interesting\n\nDifferentiability \nI think we may have slightly different meanings for \u201cdifferentiable\u201d. In RL terminology, gradient estimators are used specifically when something is not differentiable. If it were differentiable, we wouldn\u2019t need RL and would just use the exact gradient. And \n\nDoes Marginalizing Across Messages Help?\nMarginalizing across the messages is possible and it is just about allowing the sender to backpropogate through the receiver. We coded the marginalizing sender (https://controlc.com/4b0f1f50 ) but after running found no significant difference to the regular reinforce setup (https://pasteboard.co/IGPUlSC.png ) which implies that REINFORCE is a good enough gradient estimator and the instabilities are not from gradient estimation. Simply calculating the return as a function of the two agents isn't sufficient to get to equilibria, you must, at minimum, also take into account the learning dynamics (e.g. LOLA) \n\n\nLearnability\nI think we\u2019re very much on the same page about learnability, that is what we\u2019re most interested in. The question is whether \u201clearning works properly\u201d as you state. Previous works have not managed to make it \u201cwork properly\u201d and we wanted to demonstrate that it could! \n\nCao et al argue that you need prosocial agents, we show that you don't.  Jaques et al argue that you need their SOTA learning rule for selfish communication, but we show that you regular RL is sufficient given the right setup. We think a toy task is just the scenario to show these details and carefully investigate why previous approaches may have been unsuccessful. We believe it is necessary so future work doesn\u2019t automatically assume you need SOTA or fully cooperative agents to emerge communication.\n\nNotation\nWe think that notation is consistent. We explain generally that agents get some loss between their target and the action ($L$) and then we specify that we choose this loss to be an $L_1$ loss on the circumference of the circle.\n\nHyperparameters Tuning\nWe think that hyperparameter searches are generally done in all of machine learning and especially deep learning which is very sensitive to it. Our situation even more strongly demands hyperparameter search as we are not just looking to find the \u201cbest\u201d model but make arguments about whether something is feasible or not. If we do not do an exhaustive search, we cannot argue that something is infeasible (e.g. communication in high competition)\n\nFinally, the whole question is how to resolve the issue of communication vs manipulation (we use $L_2$). Your suggestion to \u201cuse exact gradients\u201d does not address that because our situation is not that simple (see below)\n\nIs Our Setup Too Simple?\nLooking at similar situations (e.g. https://github.com/facebookresearch/EGG/tree/master/egg/zoo/language_bottleneck/guess_number ) and the architectures present there, our game is more complex but our architectures are comparable. \n\nGraphs\nThe shading is indeed the standard error of the mean. This is in the description for Figure 2 but we will add it to the other descriptions as well to make it more clear. \n\nError Bars\nGiven that even fully cooperative emergent communication does not converge occasionally, and our situation is complicated by divergent interests, we don\u2019t think our graphs are too high variance. Each point is a different experiment where 5 random seeds run for a particular bias, so the graphs definitely look higher variance than usual RL graphs charting reward over time. We are honest with our random seeds and make no attempt at tuning them. At the very least, our results are significant despite the variance.\n", "title": "Marginalizing and Simplicity Experiments"}, "HJliF6cniH": {"type": "rebuttal", "replyto": "B1liIlBKvS", "comment": "We\u2019ve updated the paper based on the discussions here:\n\n- Fixed Figure reference\n- Changed papers cited for claim that previous work didn\u2019t find selfish communication\n- Rewritten our section on Crawford and Sobel to reflect this discussion\n- Added results for bias = 180 in the appendix for completeness\n\nWe want to thank all the reviewers for their comments and responses, especially Reviewer 2 with whom we\u2019ve had an in-depth and productive discussion. Though there are still disagreements, we are deeply thankful for their responsiveness and willingness to interact with our work. \n\nThank you\n", "title": "Paper Update and Thanks"}, "SklHlnc2ir": {"type": "rebuttal", "replyto": "Skla0QSwsB", "comment": "We don't believe Nash equilibria are essential to our specific research but we do think they are useful and future work with a Nash analysis could extend and improve this current work\n\nKnowing nash equilibria does not give full clarity on learning dynamics\n- A Nash equilibrium is just a local attractor which does not guarantee clarity about general learning dynamics. Neural networks are famous for guarantees at possible minima while explanations of learning dynamics are still an active research area \n- Even if we found a learning algorithm that converged towards nash equilibria it could trivially find the non-communication equilibrium, which is the lower bound on performance anyways\n- \u201cNash equilibrium strategy has no prescriptive force. At best the equilibrium identifies conditions under which learning can or should stop (more on this below) ,but it does not purport to say anything prior to that\u201d (Shoham et al, 2003)\n \nNash equilibria can be useful for playing against new opponents but this is not possible without an alternative paradigm (e.g. meta-learning) for emergent communication\n- Agents co-learn a protocol and two agents trained separately cannot communicate with each other at test time\n- Even if an agent learned a Nash strategy, it would not be useful against another a new opponent who would not understand their language\n\nFinding and defining Nash equilibria are not necessary for any of our findings\n- Empirically showing that communication with selfish agents is possible and conditional on the cooperative nature of the game does not require knowing Nash equilibria\n- We know the upper bound on the effectiveness of communication (total error = bias) without needing to know whether there is a Nash equilibria there \n- Showing that the regular emergent communication setup fails to learn in competitive scenarios does not require Nash equilibria. Proving or disproving possible equilibria in the competitive failure cases would not definitively prove or disprove whether agents theoretically could learn to communicate (as they could learn a non-equilibrium or an unstable equilibrium)\n- LOLA is shown to improve cooperation and communication statistically significantly without needing to show whether it converges to equilibria\n- Our analysis of discrete/continuous communication is a purely practical and deals with learning dynamics\n\nWhat knowing Nash equilibria could help with\n- Setting tighter upper bounds on the achievable stable communication. Perhaps the communication found by LOLA is even closer to optimal\n- Stronger arguments on whether communication is feasible to achieve in highly competitive scenarios. A lack of Nash equilibria does not guarantee infeasibility but it is a possible indicator\n- Proof of stable competitive communication protocols \n\nDeterministic Receiver \nWe noted your review but we point out the futility of making the receiver stochastic.  \n    - If we implement a stochastic receiver as described, it would be functionally equivalent to a deterministic receiver. The difference is only adding noise during training.  \nRock paper scissors is a normal form game, whereas our game is extended form and therefore the second player is conditioned on the first and can be deterministic\n \n \nSOTA MARL\nThe convergence guarantees for those two algorithms are only for differentiable games. Emergent communication with discrete messages is not a differentiable game\nSOS is an extension of LOLA that only improves performance in situations such as the tandem game.  \n- In IPD, SOS was shown to be nearly identical to LOLA.  \n- If our situation resembled the tandem game, LOLA would not improve upon regular RL\n- Since situation does not resemble the tandem game, we should not expect SOS to do any better than LOLA\nSGA  \n- requires knowing the loss and jacobian of other players\n- converges to a Nash equilibria, which could be trivially non-communication\n \nAnalysis of Failures\nBest case learning is clearly defined: a sum of players $L_1$ errors = the bias and one agent is not manipulating the other. This is not the best **stable** configuration but why does it need to be stable? Agents can maintain a reward with a communication protocol in flux\nWe're not actually at equilibria previously so the issue with more competitive scenarios must be that learning dynamics break down (which is why LOLA, with better dynamics, can help)\n\n", "title": "Author Response"}, "BJgC_BOnir": {"type": "rebuttal", "replyto": "H1lAfjIosH", "comment": "You\u2019re right, to emerge communication in the negotiation game we should carefully construct the sampling of agent preferences (weights) and sampling of items to be more cooperative. This would indeed be a nice addition to the paper and we would be happy to add it to the camera-ready if we manage to overcome the learning dynamic issues of the game.\n\nBy \u201cunderstand the nature of the game\u201d we mean that RL agents must discover the equilibria but given the game setup and learning dynamics, one agent learns to dominate early on. The dominating agent can then prevent the other from discovering better options and the learning dynamics of RL can make it infeasible for a badly losing agent to recover in this game.\n\nThe game is indeed not zero-sum, we meant to write \"fully competitive\" (unless one agent has weight 0 for an item, agents are competing on all items, there is no common reward they can optimize together, they must always compromise)\n\n@Concision: Agreed.\n", "title": "Negotiation Game Followup"}, "r1eSxdPjsr": {"type": "rebuttal", "replyto": "SkgaB38jsH", "comment": "Sorry, we meant \"fully competitive\" not \"zero-sum\", the argument is based on that assumption\n\nIt seems we're agreed that a carefully constructed game is necessary (and one of the important ways we improve upon previous emergent communication work). The distinction between reward sharing vs selfish is secondary and we think selfish agents just make understanding the game/reward clearer, though functionally we could change the structure to reward-sharing.", "title": "Author Response"}, "BJlPGfe9iH": {"type": "rebuttal", "replyto": "H1eW6F5J9S", "comment": "Thank you for your review and comments, we appreciate your view and we hope that the paper was readable and enjoyable to someone who isn\u2019t an expert in emergent communication.\n\nWe will fix the figure reference and would like to know which acronyms you found unfamiliar that we can define them when they are first used.\n\nYou make an important point about the generalizability of our results, our setup is indeed a simplified game that spans only the range of 2-player cooperative/competitive games and our learning is simplified by a simple state/action space. The reasoning for this is to be able to carefully control the learning dynamics:\n- tunable bias that perfectly reflects level of competition\n- no communication through a non-linguistic action space\n- clear way to differentiate between manipulation and communication\n- quantified optimal cooperative performance (maximizing cooperative reward)\n\nMore complex 2-player environments should only differ in the complexity of learning the state, action, and game dynamics. But the feasibility of learning to communicate should still be relevant and our hope is to encourage strong baselines on selfish emergent communication. We also want researchers who don\u2019t successfully achieve communication to investigate the underlying reasons, from competitiveness to issues with learning dynamics.\n\nFor 3+ players, we expect game dynamics to be different and indeed it could be trivially reformulated to be zero-sum (Balduzzi et al, 2019) so that concept is not meaningful. There are competitive situations not covered by 2 players, e.g. even if 3 agents are fully competitive with each other, two of them could be incentivized to cooperate with one another in order to defeat the third. Still, we think our fundamental claim is generalizable from the simple 2-player cooperative/competitive nature to the general idea that communication should emerge naturally if cooperation is always preferred to non-cooperation (which we quantify). \n\nOur second two points should also be valid. LOLA should still be an essential tool to model opponents and allow communication even in cases where cooperation can be exploited. And discrete communication may still be preferable to continuous.\n\nStill, we would look forward to expanding on the research question of selfish communication to 3+ agents and more complex scenarios (e.g. ad-hoc communication using meta-learning) in future work\n", "title": "Response to Reviewer 3"}, "SkgUaRgFoS": {"type": "rebuttal", "replyto": "r1gZEZSwoB", "comment": "Thank you for getting back so promptly! Sorry about the long response, we didn\u2019t initially realize the 5000 character limit and were just trying to be thorough. Thank you for going through it all and responding, we really appreciate it.\n\nMisunderstanding\n- We agree that self-interested agents should only be discussed within the context of a game\n- We don\u2019t think Cao et al\u2019s comments were meant to be just within the context of their game\n    - Jaques et al (2019) is from a similar set of authors and cites Cao et al for their point that Jaquest et al\u2019s self-interested agents should not learn in a completely different game!\n- We believe our experiments clarify some issues in Cao et al (2018)\n    - Cao et al conjecture that the issue is that the game is not iterated. We show in our experiments that iteration is not necessary to emerge communication under competition which could be counter to their narrative\n    - The real reason communication does not emerge is likely two-fold 1. The game dynamics allow one agent to dominate under non-communication 2. The game is likely too competitive (their setup does not control the level of competition but our experiments found it to be generally high)\n    - We\u2019ve investigated the negotiation game and gone into more detail in our reply above\n\nCooperative Reward Function vs Specific Game Structure\n- There is a fundamental difference, specifically in MARL, between a game structured for cooperation and reward sharing: Reward sharing doesn\u2019t guarantee cooperation on it\u2019s own. \n- Take Cao et al and assume their negotiation game is zero-sum (it isn't always, based on the description). What does it mean for two agents playing a zero-sum game to be 30% competitive? \n- Let\u2019s think about being competitive over 30% of the possible reward. Can we define 30% competitive with reward-sharing?\n- We can give both agents a reward $R  = 0.7 * R_{you} + 0.3 * R_{them}$ but what does that do? Given your weight for an item $W_you$ and your opponent's weight for it $W_{other}$ there's three cases:\n1. both agree you should take the item ($W_{you} * 0.43 > W_{them}$ )\n2. both agree they should have the item ($W_{them} * 0.43 > W_{you}$)\n3. you compete over the item ($0.43 * W_{you} < W_{them} < 2.3 * W_{you}$)\n- Is this 30% competitive? It depends on the weights given to a player and the distributions of items being negotiated over. \n- Just doing reward sharing does not guarantee a specific level of competition. Even a partially cooperative reward function may not guarantee cooperation as the optimal strategy\n- Our point is that reward-sharing still needs a carefully constructed game to specify the level of competition and encourage cooperation, so we make a distinction between reward-sharing and actually guaranteeing the level of competition by carefully creating a game with cooperative/competitive dynamics. \n", "title": "Response To Misunderstanding + Reward Sharing"}, "Bklu1pgFiB": {"type": "rebuttal", "replyto": "SyxjGoHDoS", "comment": "It's interesting you mention the negotiation game because figuring out why communication didn't emerge there was the starting point of our research\n\n- We could not reproduce all the curves for selfish agents seen in Cao et al (2018)\n    - We contacted the authors and together still did not manage to reproduce their curve for \u201cProposal\u201d \u201cLinguistic\u201d and \u201cBoth\u201d \n    - We could not figure out why their agents performed more fairly in the presence of a linguistic channel (\u201cLinguistic\u201d) despite one agent dominating in the non-communication case and therefore having no incentive to communicate\n    - We explained our results and verified all differences we could think of but the authors could not suggest a reason for the difference\n    - Deepmind did not publicly release their code\n\n- Instead, we found the first player to be regularly dominating the second player because of learning dynamics\n    - The first player could not make an accept action on their first move which made it more likely that the second player was the first to learn the accept action\n    - The second player would see two possible outcomes if the accept action could succeed: 1. Gain whatever reward was associated with accepting the deal 2. Continue negotiating and likely go past the final round and end up with no reward\n    - Once the second player was constantly accepting the deal given to it, the first player would learn to give it a deal with as little reward as possible \n    - We found this to be a stackelberg game where the second player has little recourse if they do not understand the nature of the game. The first player would give the second one a deal and the second player would just accept it\n\n- The main obstacle to communication was the domination of one agent as well as the highly competitive nature of the game\n    - Since the first agent dominated the other under non-communication, it was never in their interest to communicate\n    - We allowed agents to mask their communication and found that the dominating agent always masked their communication\n    - We achieved some communication when we did partial reward sharing (Peysakhovich and Lerer, 2017) but found that we still needed to tune hyperparameters to disadvantage the first agent \n    - If the reward sharing was high enough, it became the first agent\u2019s best interests to communicate and communication emerged.\n\n- We eventually decided that the negotiation game was not a good test bed for these experiments because the learning dynamics allowed one agent to dominate\n    - A simple idea, inspired by Lowe et al (2019), is that communication should only emerge if the possible reward under communication is greater than under non-communication for both agents\n    - Since the negotiation game has one agent dominating and the game is often zero-sum, we do not think any algorithm can lead to communication\n    - We decided to come up with a different game to specifically look at the role of cooperation/communication unfettered by common issues in emergent communication games: communication through the action space, unquantified game dynamics, unquantified optimal possible play, and badly tuned baselines.\n\nSorry for going over the 5000 character limit with these two posts but we wanted to give more detail on the results of the experiment you were asking about.\n", "title": "Negotiation Game Experiment"}, "SygY0oetiB": {"type": "rebuttal", "replyto": "SyxjGoHDoS", "comment": "Tabular Exact Gradient\n- We can create exact gradients by having the message be continuous and allow the sender to backpropagate directly through the receiver.\n    - This is functionally similar to your suggestion about \u201cmarginalizing across messages\u201d but cleaner to implement\n    - This is also just the continuous game with a more powerful sender that essentially has one-sided access to the receiver\u2019s parameters\n    - We had a couple experiments but did not pursue them deeply because we preferred a more symmetric setup. We will take a closer look\n- Because our state space is continuous, we cannot do tabular RL but instead choose to use policy gradient. Would you want to see a discrete state space instead of a continuous one? \n    - This should not improve learning stability and may actually harm it because discrete spaces are not ordered by default\n    - Our agents would need to learn the ordering of the space (e.g. input 1 < input 2) as well as how to use it\n    - Preliminary experiments we made with discrete action spaces when designing the game were not promising\n\nIteration\n- We did actually implement an iterative game in exploratory experiments.\n- It is more difficult to design and train effectively because we had to add a notion of statefulness \n    - One option is to use an RNN to maintain state but this complicates learning dynamics \n    - Another option is to condition on a single previous state (as in LOLA) but the two-step nature of our game making it difficult to specify an unbiased initial state. Agents need to distinguish the first state as having no history but it is not straightforward to have a null previous state for our continuous state space\n- Training was less stable and learning dynamics were more complicated. \n    - Ultimately, it was much more computationally intensive which made our extensive hyperparameter searches much less feasible to run.\n    - Sadly, we could not achieve baselines we felt to be reasonable \n- The idea of alternating lies and truth over rounds is interesting but \u201clying\u201d is not as straightforward as it seems\n    - Because emergent communication is not separately learning a meaning and use but learning the two simultaneously\n    - It can be hard to learn what is \u201clying\u201d because it relies on there being an existing meaning and subverting that meaning\n    - Learning agents would not be able to distinguish between lying and misinterpreting a signal and would just adjust their distribution of meanings (\"without somewhat agreeing to meanings, agents cannot use those meanings to compete (Searcy & Nowicki, 2005; Skyrms & Barrett, 2018).\")\n- We chose to focus on the initial problem of learning to communicate honestly but noisily", "title": "2/3 Interesting Experiments Response"}, "S1eiWjvIir": {"type": "rebuttal", "replyto": "r1xb36iCFS", "comment": "\nSemantic Meaning Of Communication\n\n- For emergent communication to consistently improve performance over non-communication, it must be semantically meaningful. \n    - So our protocol is meaningful, but we are more concerned with how effective the meanings are as opposed to what they are exactly.\n- And communicative efficacy is better measured with rewards rather than qualitative methods (see Lowe et al 2019)\n    - Qualitative methods can be tricked by spurious mutual information metrics caused by certain network architectures\n    - Looking at reward with communication vs non-communication is a clear and foolproof way of measuring the efficacy\n\n- Could you clarify what you mean by \u201cgrounded\u201d? \n    - We use \u201cgrounded\u201d to refer to a symbol having a semantic meaning (essentially a mapping of symbols -> meanings). Any effective emergent communication can be said to be at least partially \u201cgrounded\u201d since there must be semantic meaning conveyed by the agents in order to be effective.\n    - Kottur et al say that \u201ccompositional language is one of the optimal policies\u201d  and point to the compositionality of grounded meanings as necessary for generalization\n    - For now, this remains a difficult term to encapsulate and we think the community has many meanings for it. Sidenote: Chris Manning had a great little speech on this exact point at last year\u2019s ViGIL workshop (https://bluejeans.com/playback/s/jftkhICjhUnEbcglGD4qWWpHsvunBNISIZNdGdUo2AD7vD9nAq5aI2yXus70immP in Chapter 2 starting at 1:05:27)\n\nReferences\n\nResnik, David B. \u201cHow-Possibly Explanations in Biology\u201d. Acta Biotheoretica (1991) ,39(2):141\u2013149.", "title": "Response to Reviewer 1 Part 2"}, "HJlic5wLoB": {"type": "rebuttal", "replyto": "r1xb36iCFS", "comment": "Thank you for your comments and corrections! We\u2019ve taken quite some time to mull everything over and address your concerns point by point below. We would be happy to discuss further and more in depth.\n\nCrawford and Sobel\n\n- We put a discussion about the differences between our paper and Crawford and Sobel (1986) in a post above.\n- On your point about our phrase \u201can existing language\u201d, we agree and are also happy to revise the wording. Our point was more subtle that they look at specific equilibria where there exists a fixed mutual understanding as opposed to looking at learning dynamics where the language is emerged and in flux. Would it be more appropriate to write they study \u201cfixed languages at equilibria\u201d?\n\nSection 4.2 Deterministic Mappings\n\n- In section 4.2, we do not make an assumption of deterministic mappings and indeed, during training, the sender is stochastic, choosing a symbol based on categorical distribution over a vocabulary (as is standard in emergent communication).\n- Our main point was that given a random initialization of a non-learning sender (learning rate close to 0) and a learning receiver with a regular learning rate, it is highly likely that the learning agent would dominate. \n    - This does not necessitate a deterministic sender since a stochastic sender\u2019s mappings can be mostly learned (and therefore dominated) in nearly all cases.\n    - The only case where a non-learning sender cannot be dominated by a learning receiver would be a sender with a Nash policy (e.g. all states are mapped to the same symbol and communication is uninformative). But initializing a sender to a Nash policy is very unlikely given the random initialization methods of neural networks.\n    - So in the vast majority of cases, the learning agent would indeed do significantly better than its non-learning opponent.\n- We can revise the example to make it more clear that the situation is highly likely but not guaranteed.\n\nJustifying The Circular Game\n\n- We would like to clarify that this game is not a benchmark but closer to a diagnostic tool.\n    - We wanted to run experiments on the full range of 2 player cooperative/competitive games and empirically show that selfish emergent communication can be feasibly achieved \n    - We also wanted to demonstrate that it is indeed the bias of the game that influences the level of communication achieved and can explain why previous literature was mistaken.\n- We believe the game is really an extension of Crawford and Sobel, made to be smoothly tuneable from fully cooperative to fully competitive. \n    - To our knowledge, there does not exist such a game in game theory literature (Crawford and Sobel\u2019s original game is not as easily made fully-competitive). \n    - To our knowledge, no existing games in emergent communication literature have fine-grained control of the level of cooperation/competition in the game.\n\nAlgorithmic Game Description\n\n- We added the extra line about updates to make it more clear when the episode ends and agents can update their weights. \n- This is indeed not explicitly part of the game and we can remove it if it seems superfluous to understanding how the game is played by our agents.\n\nRealism of Selfish Communication\n\n- We would like to stress that emergent communication is a \u201chow-possibly\u201d explanation (Resnick, 1991) of language emergence. \n    - In this way, we think that reward-sharing and full cooperation is not as realistic of a model as two selfish agents that emergent communication for selfish reasons given an environment that requires cooperation. \n    - Current literature in emergent communication has usually assumed reward-sharing and therefore is less realistic than our setting. \n\n- On the topic of \u201ctoy setting\u201d, this is indeed exactly a toy setting to see if communication emerges. \n    - Since literature in the field of emergent communication has implied communication does not emerge, we created this toy task to see if it could and whether Crawford and Sobel\u2019s equilibria could be feasibly achieved in the modern setting of emergent communication. \n    - We do not make any presumptions about all games, nor do we think our game should be a benchmark. What we have shown is that research in emergent communication under competition should make use of strong baselines with selfish agents and take into account the quantifiable cooperation/competitive nature of the games studied, which it sometimes does not.\n \n\n--- continued below ---", "title": "Response to Reviewer 1 Part 1"}, "BkezmBvUoB": {"type": "rebuttal", "replyto": "HkleHtCd9S", "comment": "Thank you for the in-depth comments, corrections, and suggestions. We\u2019ve tried to address all your concerns point by point below and would be happy to discuss further and more in-depth.  \n\nPapers Claiming Selfish Communication Doesn\u2019t Work\n\n- After reviewing all the papers we agree that (Foerster et al 2016) and (Lazaridou et al 2018) are bad citations for previous literature making this claim, many thanks for bringing this to our attention. \n- We still believe that the view of emergent communication being possible only in cooperative settings is prevalent in the literature and believe this is an important misunderstanding to address.\n- For Cao et al (2018), a main claim is that selfish agents cannot learn to effectively emerge communication whereas agents that share a reward function do.\n    - \u201cSelfish agents do not appear to ground cheap talk\u201d \n    - They conjecture in section 3.2 that this is because the game is not iterated but we show this is not necessary (more on this lower down in our comment)\n    - Instead, the game is likely too competitive and it is not necessary to share a reward function in order to communicate\n- Jaques et al (2019) reiterate the claim of Cao et al (2018) and claim that their learning rule allows for communication between competitive agents whereas regular methods do not, without explicitly quantifying the cooperative/competitive nature of their games.\n    - \u201cThe IC metrics demonstrate that baseline agents show almost no signs of coordinating behavior with communication, i.e. speakers saying A and listeners doing B consistently. This result is aligned with both theoretical results in cheap-talk literature (Crawford & Sobel, 1982), and recent empirical results in MARL (e.g. Foerster et al. (2016);Lazaridou et al. (2018); Cao et al. (2018)).\u201d\n- We also found that Lanctot et al (2017) imply that emergent communication is a purely cooperative task (in the sense that they take communication to be a paradigm of cooperation):\n    - \u201cIn MARL, several agents interact and learn in an environment simultaneously, either competitively such as in Go [92] and Poker [39,106,73], cooperatively such as when learning to communicate [23, 94, 36], or some mix of the two [59, 96, 35].\u201d \n\n\"Reward Function Is Cooperative\"\n\n- We would like to make a small distinction between the reward function being cooperative and the game encouraging cooperation. \n    - In previous work in emergent communication, there have been papers that have simply given the same reward to both agents or given a part of one agent\u2019s reward to the other explicitly (Lerer and Peysakovich 2018) which is a cooperative reward function. \n    - The reward function for our agents is purely their own \u2014 selfish. They do not, a priori, have cooperative intentions. It is only through discovering the nature of the current game\u2019s setup that they should realize cooperation is advantageous.\n- Cao et al (2018) study a cooperative \u201cprosocial\u201d reward function in a game that is quite competitive whereas we study purely selfish reward functions but in a game whose competitive nature can be tuned by the bias.\n    - Regardless of the game \u201cprosocial\u201d agents are going to be cooperative\n    - We take the view of Shoham et al (2003) that if agents are not being controlled by a central designer then the interesting scenario is when \u201clearning takes place by self-interested agents\u201d, as opposed to prosocially-interested agents\n\nMisunderstanding\n\n- We could not figure out what is the exact misunderstanding you are pointing to. Could you rephrase it perhaps?\n\n\"Uninteresting\"\n\n- Though the result may seem uninteresting from the perspective of static analysis where Crawford and Sobel\u2019s result is clear, we perform a dynamic analysis (this is explained in more detail in our Crawford and Sobel discussion).\n- We believe it is, at minimum, interesting for the emergent communication community that seems to hold an opposing belief.\n    - Since there are two well-cited publications from top conferences (ICLR, ICML) that clearly and unambiguously state that selfish agents do not learn to communicate and one implying emergent communication is purely cooperative, we believe that there is indeed a misconception of selfish emergent communication in the field that deserves to be clarified\n    - We believe that our toy task is sufficient to show that selfish emergent communication should be feasible to achieve with modern deep RL methods, overturning that belief\n\n-- continued below --", "title": "Reviewer 2 Response Part 1 "}, "BygopOPLjr": {"type": "rebuttal", "replyto": "HkleHtCd9S", "comment": "Broken Reference\n\n- We thank the reviewer for pointing this out and will fix this to read Figure 2\n\nExperiments for Bias = 180\n\n- Since we are plotting the sum of rewards, the curve would be trivially at 180.\n- We could plot each agent\u2019s individual reward. A couple test runs of our basic setup were found to have non-communication (90/90) error split\n- If you have specific experiments and plots you would like us to make, we would be happy to investigate\n\nReferences:\n\nKharitonov et al. \u201cEGG: a toolkit for research on Emergence of lanGuage in Games\u201d Arxiv 2019. https://github.com/facebookresearch/EGG/\n\nKingma, Diederik P. and Max Welling. \u201cAuto-Encoding Variational Bayes.\u201d ICLR (2014)\n\nLanctot, Marc et al. \u201cA Unified Game-Theoretic Approach to Multiagent Reinforcement Learning.\u201d NIPS (2017).\n\nResnik, David B. \u201cHow-Possibly Explanations in Biology\u201d. Acta Biotheoretica (1991) ,39(2):141\u2013149.\n\nSingh, Satinder P. et al. \u201cNash Convergence of Gradient Dynamics in General-Sum Games.\u201d UAI (2000).\n\nShoham, Yoav et al. \u201cMulti-Agent Reinforcement Learning: A Critical Survey.\u201d (2003).\n", "title": "Reviewer 2 Response Part 4"}, "rJl-EdvIir": {"type": "rebuttal", "replyto": "HkleHtCd9S", "comment": "\nIterated vs One-shot Game\n\n- This is a very interesting point and we are happy you brought this up\n- Though it seems like a simple one-shot game, two things seem to make cooperation possible here\n    1. A two stage game (1. Sender sends message 2. Receiver takes action). We did experiments on iterated prisoner\u2019s dilemma (a one-stage game) and found that LOLA could not emerge cooperation in the one-round case though it does in the iterated game.\n    2. \u201cIteration in the parameter space\u201d. Though the game itself is not iterated, the fact an agent plays with the same opponent throughout training allows them to learn conventions with their opponent e.g. it is trivial to learn a simple coordination game between RL agents that are trained together\n- LOLA is indeed the reason for improved cooperation and communication efficacy\n    - Communication at bias = 90 is clearly better with LOLA agents than it is with our basic setup\n    - It is specifically LOLA that is helping cooperation as making that one change is sufficient to get our results\n    - You can review our code, the only difference between the LOLA scenario and REINFORCE scenario is that specific agent\u2019s loss function reflects LOLA updates, all other code is unchanged\n\nConfusing Graphs and Error Bars\n\n- Could you please specify which graphs are confusing and which error bars you feel are too large? We deeply care about the clarity and statistical significance of our paper and would be happy to improve it\n\nSender's Differentiable Loss?\n\n- In \"note that the loss is also differentiable with respect to the action of the 1st agent\" we took \"1st agent\" to refer to the sender\n- The loss is not differentiable wrt to the sender\n    - Sender maps its state to a categorical distribution over symbols from a vocabulary and stochastically chooses a single symbol based on the distribution. This symbol is the message\n    - Receiver takes the message and deterministically chooses the target\n    - The loss is differentiable wrt to the message but not wrt to the parameters of the sender because of the stochastic choice\n- For this reason we need to use a gradient estimator \n    - For the basic setup we use REINFORCE with a mean baseline for variance reduction. This is standard in emergent communication (see EGG by Kharitonov et al, 2019)\n    - For the LOLA setup we use DiCE because it allows us to do higher order gradient estimation \n- This setup of a differentiable receiver and gradient estimated sender is an SCG\n    - It differs by having two different objectives, one for the sender and one for the receiver\n\n\"This would be interesting if the game was complex\"\n\n- We did not aim to find a difficult existing game and show that communication under competition could be achieved because \n    - Jaques et al (2019) already emerge communication in a more complex game, albeit with a complex learning rule\n    - And if we did do this for an even more complex game without being able to easily control the exact levels of competition/cooperation, it would be more difficult to show when communication is feasible as well as precisely how it can be achieved. \n    - A complex game would likely have dynamics that are much harder to control (e.g. communication through a visual action space instead of utterances). This would make our arguments and conclusions weaker\n- We believe our results are still interesting despite the simplicity of our game\n    - Previous research has looked at more complex games (e.g. Jaques et al, 2019) but has not quantified the level of cooperation/competition. \n    - We needed to create a game that not only had a quantifiable level of cooperation/competition but also allowed for it to easily tunable, spanning the range of fully cooperative to fully competitive \n    - We believe we have shown the feasibility of achieving communication under competition, setting an example for future research to use better baselines and better quantify the level of competition/cooperation\n\nLoss Function Notation\n\n- We use standard ML notation with $L$ referring to a generic loss function and $L_1$ specifically referring to the absolute distance loss function. Please let us know if this was not clear.\n\nL2 as a hyperparameter metric\n\n- The game with bias = 180 is indeed constant sum (see our proof in the appendix). \n    - Is there a difference between \u201cgeneral constant sum\u201d and \u201cconstant sum\u201d?\n- We are indeed implicitly biasing towards fairness by using the $L_2$ metric\n    - We believe this is reasonable to recover the difference between \u201ccommunication\u201d and \u201cmanipulation\u201d because fair communication cannot be \u201cmanipulation\u201d. We are open to other ways of achieving that if you have suggestions.\n    - We also show both agents' $L_1$ losses for all hyperparameter search runs in Figure 4 to allow readers to understand the distribution of results on top of just the best hyperparameters we picked\n\n--- continued below ---", "title": "Reviewer 2 Response Part 3"}, "H1gm5SDUiH": {"type": "rebuttal", "replyto": "HkleHtCd9S", "comment": "Nash Equilibria and MARL\n\n- Though studying equilibria and Nash may be useful, we believe that it is not something our paper on multi-agent learning should focus on. Our view is strongly influenced by Shoham et al (2003) which has informed modern deep MARL and we give a brief summary of related points here\n    - We are studying how agents \u201cshould\u201d learn and therefore how agents \u201cshould\u201d act\n    - One possible agenda is \u201cequilibrium\u201d and asks whether a vector of learning strategies forms an equilibrium\n    - Another possible agenda is \u201cAI\u201d and asks what is the best learning strategy given a fixed class of possible opponents\n    - The main difference between the two agendas is \u201cbounded rationality\u201d\n    - \u201cEquilibrium\u201d assumes perfect reasoning and infinite mutual modelling\n    - \u201cAI\u201d starts from a base of bounded rationality and only adds mutual modelling when necessary\n    - These two agendas are not necessarily mutually exclusive but there is distinct philosophical difference \n- Why our situation is better represented by the \u201cAI\u201d agenda and therefore should not be focused on convergence or Nash equilibria\n    - We believe the \u201cbounded rationality\u201d assumption to be more appropriate for language emergence which could be said to give a \u201chow-possibly\u201d model (Resnick, 1991) of the emergence of human language \n    - We already model our fixed class of opponents as being SGD learning models with similar loss structures. This class of opponents is an assumption made by LOLA and we believe it is reasonable for deep MARL.\n\n\nSOTA MARL\n\n- We believe that, though simple, our methods are indeed state of the art for the specific problem they are tackling. \n    - We do not know of better complex gradient estimators being used in such low-dimensional input situations\n    - We believe that our results are quite good and relatively close to the theoretical optimum \n- Do you have specific MARL learning algorithms you believe would perform better for our setup? We would be glad to implement and test them\n\nAnalysis of Failures\n\n- We believe we do a fair analysis of successes and failures. \n    - We find the issue that likely was underlying why previous works did not emerge communication with selfish agents (competitiveness) and do a careful analysis to show how it could be possible\n    - We look at two different popular ways of emergent communication (continuous vs discrete) and analyse how they affect our situation and the achievability of good selfish emergent communication\n\nDeterministic Receiver\n\n- Deterministic receivers are standard in basic emergent communication (see examples in EGG by Kharitonov et al) \n- We could change our receiver to be stochastic, for example, by making its output a gaussian distribution over actions and then sampling from that. This would not make things too different.\n    - Since we train with gradient descent, we could use the reparametrization trick to get the gradient for the receiver (Kingma and Welling, 2014)\n    - Doing this would make our stochastic receiver no different from a deterministic receiver that has an added gaussian noise in its output\n    - This would essentially only be adding variance to the learning of our receiver and simply be worse from an optimization perspective without being too functionally different\n\nIGD and Convergence\n\n- We would like to clarify that we are not making theoretical arguments about all general-sum games\n    - Theoretically, Singh et al (2013) have shown that for 2-player 2-action general-sum games, independent gradient ascent with an infinitesimally small step size will lead to either convergence at a Nash equilibrium or an expected payoff equivalent to the payoff at some Nash equilibrium\n- Please see our point about Nash and MARL for a discussion on why we are not specifically looking for equilibria or convergence\n\n--- continued below ---", "title": "Reviewer 2 Response Part 2"}, "Bkxo5-v8sB": {"type": "rebuttal", "replyto": "B1liIlBKvS", "comment": "Since both Reviewer 1 and Reviewer 2 have brought up issues related to how our work differs from Crawford and Sobel (1986), we thought we would address them together and try to clarify the differences.\n\nCrawford and Sobel do a static analysis at equilibria and give existence proofs and guarantees. In contrast, we do a dynamic analysis and focus on showing empirical feasibility of competitive selfish communication in the modern ML paradigm of emergent communication.\n\n\nStatic Analysis vs Dynamic Analysis\n\n- Crawford and Sobel study possible equilibria and the fixed communication protocols at those equilibria. \n    - They prove the existence of equilibria and their properties but do not show how to achieve those equilibria nor whether certain learning rules could lead to and maintain those equilibria\n- We specifically study the standard setup used in emergent communication: learning by gradient descent on the receiver and REINFORCE (or variants) on the sender. We show that even in our basic scenario, it is feasible to achieve communication when the game is more cooperative than competitive.\n    - Our agents are also not seeking equilibria when they achieve communication, they are simply seeking selfish reward without taking the opponent into mind\n\n- Crawford and Sobel show that communication is not possible after a degree of divergence in interests\n- We empirically demonstrate that emergent communication is feasible with regular agents in the modern paradigm, overturning a previous misconception. \n    - We quantify the exact circumstances (level of competition) that would cause this misconception (when the game is more competitive than cooperative)\n    - We make a strong case for all future papers in competitive emergent communication to precisely quantify the level of competitiveness in the game (something that is not currently done e.g. Leibo et al (2017)). This would give a better perspective on the efficacy of achieved and achievable communication.\n\nKnowledge of the Game and Opponent\n\n- Crawford and Sobel suppose that agents are \u201cperfectly rational\u201d and are given perfect knowledge (see Shoham et al 2003) \n    - Both players are fully aware of the nature of the game, and have knowledge of their and the other player\u2019s reward for all situations, and always take the rational best response. \n    - Messages are modelled as states of the world with added noise\n- We start from the assumption of \u201cbounded rationality\u201d and only add modelling of the opponent and game as necessary \n    - We suppose nothing about an agent\u2019s knowledge of the other player, the rules of the game, or how they should act. \n    - Our agents use RL to discover all knowledge through trial and error, with the goal of optimizing their own reward. Our agents do not see the other player\u2019s reward and are solely optimizing their own.\n    - We only add opponent modelling as necessary for LOLA. Agents use a model of their opponent and built into LOLA is the assumption that opponents learn with gradient descent and have similar loss objectives.\n    - Messages are simply mappings of the world to symbols and do not necessarily need to be ordered or completely cover all states of the world\n\n\nIn short, our work seeks to offer a fundamental contribution to the field of emergent communication and machine learning. We are leaning on the work of Crawford and Sobel as a guide for possible equilibria but fundamentally we wish to show feasibility with learning dynamics not possibility and theoretical guarantees. Our hope with this work is to correct a misconception about selfish emergent communication prevalent in the field, to bring the theoretical contributions of Crawford and Sobel into the fold of emergent communication literature, and to give guides about how to correctly measure and possibly improve emergent communication under competition.\n\n\nReferences:\n\nLeibo, Joel Z. et al. \u201cMulti-agent Reinforcement Learning in Sequential Social Dilemmas.\u201d AAMAS (2017).\n\nShoham, Yoav et al. \u201cMulti-Agent Reinforcement Learning:a critical survey.\u201d (2003).\n\n\n", "title": "Crawford and Sobel Response"}, "r1xb36iCFS": {"type": "review", "replyto": "B1liIlBKvS", "review": "Summary:\nThis paper introduces a new sender-receiver game to study emergent communication in partially-competitive scenarios. The authors find that communication can also emerge in partially-competitive scenarios and demonstrate how to encourage communication: 1) selfish communication is proportional to cooperation, and it naturally occurs for situations that are more cooperative than competitive, 2) stability and performance are improved by using LOLA, and 3) discrete protocols are better than continuous ones.\n\nStrengths:\n- This is an interesting paper that is well written and motivated.\n- They have justified a new sender-receiver game that can be tuned for various levels of competition which then allows them to analyze the effects of various levels of cooperation and competition.\n- They perform sufficient experimental analysis to show that LOLA outperforms standard methods like REINFORCE in these settings and that discrete communication lends to cooperative communication.\n- Evaluation is good in the sense that they repeat their experiments multiple times across different random seeds.\n\nWeaknesses:\n- Given that cheap talk is an extremely well-studied topic in economics, I feel that the authors should have devoted more time to explain the difference in setting between their work in classic pieces like those of Sobel and Crawford. The authors should properly define what they mean by learning agents versus fully rational agents, and the key differences between the two. Furthermore, nowhere do the authors in the cheap talk paper assert that agents use an existing language: the equilibrium itself assigns meaning to each sender\u2019s message; this is not part of the problem definition per se. In fact, the work of Sobel and Crawford does not even constrain the size of the vocabulary (as was done in this paper): one of its key contributions is to show that in strictly non-cooperative settings, all equilibria must be partition equilibria, with only a finite number of messages used.\n- Section 4.2: \u201cinitial random mapping of targets to messages.\u201d The authors made the assumption that this mapping has to be deterministic. Absent a proof or a citation, I find this difficult to accept. This is especially so since mixed strategies are a crucial component of games of imperfect information.\n- The introduction of the circular game is suspect. There already exist numerous games involving cheap talk, one of them from the Sobel and Crawford paper. Why is there a need for this new benchmark?\n-  The description of the game is given as an algorithm in the appendix. This comes across as counterintuitive: why are the gradient steps being included as part of the game description? A game\u2019s specification and the algorithm which is being used to solve it are two different things.\n- It is difficult for me to assess the significance of these results since the authors have not presented real-world scenarios and experiments that demonstrate the importance of selfish communication. For cooperative communication we see it a lot in examples like grounded language learning, visual dialog, multi-agent communication etc. But I am concerned that the new setting proposed in this paper seems like a 'toy setting' to investigate if emergent communication would happen.\n- Are the communicated symbols (discrete or continuous) semantically meaningful? It was shown in Kottur et al. (2017) that for emergent communication to occur and generalize to unseen test instances, it was crucial that the communication protocol was grounded i.e. one symbol learning to represent the color, one representing the shape, one representing the size. What is the final communication protocol learned in this case, and it is useful/interpretable in a similar sense?\n- Typo: 'Figure ??' in line 3 of section 5.1", "title": "Official Blind Review #1", "rating": "1: Reject", "confidence": 2}, "H1eW6F5J9S": {"type": "review", "replyto": "B1liIlBKvS", "review": "This ICLR submission deals with a problem of whether selfish agents can learn to use an emergent communication channel, using a sender-receiver game as a case study. It is found that communication can emerge in partially-competitive scenarios, and conditions in which this can happen are investigated.\nThis review is delivered with the caveat that I am not an expert in this particulat field.\nThe investigation seems relevant and the paper is well written and structured, being within the scope of the conference. Proofs in the appendix are sound to the best of my understanding.\nThe literature review is up to date and seems overall relevant.\nThis study should be understood as a proof of concept, given that the setting seems rather restrictive, so I am unsure that he results could be generalized.\nThey seem anyhow promising and partially challenge the current understanding of the problem.\nMinor issues:\nAll acronyms in the text should be defined the first time they appear in the text.\nLaTex problem with Fig. reference at the beginning of section 5.1.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 1}}}