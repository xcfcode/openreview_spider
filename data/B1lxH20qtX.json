{"paper": {"title": "Learning to control self-assembling morphologies: a study of generalization via modularity", "authors": ["Deepak Pathak", "Chris Lu", "Trevor Darrell", "Philip Isola", "Alexei A. Efros"], "authorids": ["pathak@berkeley.edu", "chris.lu@berkeley.edu", "trevor@eecs.berkeley.edu", "phillip.isola@gmail.com", "efros@eecs.berkeley.edu"], "summary": "Learning to control self-assembling agents via dynamic graph networks", "abstract": "Much of contemporary sensorimotor learning assumes that one is already given a complex agent (e.g., a robotic arm) and the goal is to learn to control it. In contrast, this paper investigates a modular co-evolution strategy: a collection of primitive agents learns to self-assemble into increasingly complex collectives in order to solve control tasks. Each primitive agent consists of a limb and a neural controller. Limbs may choose to link up to form collectives, with linking being treated as a dynamic action. When two limbs link, a joint is added between them, actuated by the 'parent' limb's controller. This forms a new 'single' agent, which may further link with other agents. In this way, complex morphologies can emerge, controlled by a policy whose architecture is in explicit correspondence with the morphology. In experiments, we demonstrate that agents with these modular and dynamic topologies generalize better to test-time environments compared to static and monolithic baselines. Project videos are available at https://doubleblindICLR19.github.io/self-assembly/", "keywords": ["modularity", "compostionality", "graphs", "dynamics", "network"]}, "meta": {"decision": "Reject", "comment": "Strengths: A co-evolution of body connectivity and its topology mimicing control policy is presented.\n\nWeaknesses: Reviewers found the paper to be lacking in detail. The importance of message passing in achieving the given results is clear on one example but not some others. Some reviewers had questions regarding the baseline comparisons.\nThe authors provided lengthy details in responses on the discussion board, but reviewers likely had limited time to fully reread the many changes that were listed.\nAC:  The physics in the motions shown in the video require signficant further explanation. It looks like the ball joints can directly attach themselves to the ground, and make that link stand up. Thus it seems that the robots are not underactuated and can effectively grab arbitrary points in the environment. Also it is strange to see the robot parts dynamically fly together as if attracted by a magnet.  The physics needs significant further explanation.\n\nPoints of Contention: The R2 review is positive on the paper (7), with a moderate confidence (3).\nR1 contributed additional questions during the discussion, but R2 and R3 were silent.\n\nThe AC further examined  the submission (paper and video). \nThe reviewers and the AC are in consensus regarding\nthe many details that are behind the system that are still not understood.  The AC is also skeptical\nof the non-physical nature of the motion, or the unspecified behavior of fully-actuated contacts\nwith the ground.\n"}, "review": {"BklxiCctkN": {"type": "rebuttal", "replyto": "rJgsB2uYkE", "comment": "Thank you for the suggestion of adding a pseudo-code of the overall algorithm. We provide the pseudo-code of our DGN algorithm below exactly as it is implemented in code line-by-line. Note that all these equations and parameters are already defined in Section 3.3. We will add this pseudo-code to the final version of the paper as open-review allows update. Hope this addresses your concern. We will also make our code publicly available.\n\nLooking forward to your reply!\n\n----------------------------------------------------\n[Notation Summary as already defined in Section 3.3]\n----------------------------------------------------\nFor each node i:\n    First compute \\pi_{\\theta_1}^i (s_t^i, m_t^{C_i}) = m_t^i\n    Then compute \\pi_{\\theta_2}^i (m_t^i, m_t^{p_i}) = [a_t^i, \\hat{m}_t^i]\nwhere:\n    s_t^i: observation state of agent limb i\n    a_t^i: action output of agent limb i: {3 torques, attach, detach}\n    m_t^{C_i}: aggregated message from children nodes input to agent i (bottom-up-1)\n    m_t^i: output message that agent i passes to its parent  (bottom-up-2)\n    m_t^{p_i}: message from parent node to agent i (top-down-1)\n    \\hat{m}_t^i: message from agent i to its children\n    => \\theta: {\\theta_1, \\theta_2}\n    => messages are 32 length floating point vectors.\n\n----------------------------------------------------\n[Pseudo-code: Bottom-up, Top-down DGN]\n----------------------------------------------------\n1. Initialize parameters {\\theta_1, \\theta_2} randomly.\n   Initialize all message vectors {m_t^{C_i}, m_t^i, m_t^{p_i}, \\hat{m}_t^i} to be zero.\n\n2. Represent graph connectivity $G$ as [collection of nodes i, edges between nodes i]\n   Note: In the beginning, all edges are zeros, i.e., non-existent\n\n3. Begin loop {for each time step t}\n4.     Each limb agent i observes its own state vector s_t^i\n5.     Begin loop {for each agent i}\n6.         # Compute incoming child messages\n            m_t^{C_i} = 0\n            for each child node c of agent i in $G$:\n                m_t^{C_i} += m_t^c\n\n7.         # Compute message to parent p of agent i in $G$:\n            m_t^i := \\pi_{\\theta_1}^i (s_t^i, m_t^{C_i})\n\n8.         # Compute action and messages to children of agent i:\n            a_t^i, \\hat{m}_t^i := \\pi_{\\theta_2}^i (m_t^i, m_t^{p_i})\n\n9.         # Execute morphology change as per a_t^i\n            if a_t^i[3]==attach:\n                find closest agent j within distance d from agent i, otherwise j=NULL\n                add edge (i,j) in $G$\n                also make physical joint between (i,j)\n            if a_t^i[4]==detach:\n                delete edge (i, parent of i) in $G$\n                also delete physical joint between (i,j)\n\n10.       # Execute torques from a_t^i\n             Apply torques a_t^i[0], a_t^i[1], a_t^i[2]\n11.    End loop\n\n12.    # Update message variables\n          Begin loop {for each agent i}\n              let p be parent of agent i in $G$\n              if p is NULL:\n                  set m_t^{p_i} to be zero\n              else:\n                  m_t^{p_i} = \\hat{m}_t^i\n         End loop\n\n13.    # Update graph and agent morphology\n         Find all connected components in $G$\n         Begin loop {for each connected component}\n              Begin loop {for each agent i in the connected component}\n                  reward r_t^i = reward of corresponding connected component (e.g. max height)\n              End loop\n         End loop\n\n14. End loop\n\n15. Update \\theta = {\\theta_1,\\theta_2} to maximize joint discounted reward:\n        \\max_{\\theta} \\mathbb{E} \\sum_{agent i} [\\sum_t r_t^i]\n    This is exactly same as an ordinary reinforcement learning objective.\n\n    => We optimize it by using off-the-shelf PPO to update \\theta, i.e., {\\theta_1,\\theta_2} as follows:\n        let \\vec{a}_t = [a_t^1, a_t^2.. a_t^n]\n            \\vec{s}_t = [s_t^1, s_t^2.. s_t^n]\n            \\hat{A}_t = advantage of discounted rewards, r_t = \\sum_{agent i} r_t^i\n        PPO: \\max_{\\theta} \\mathbb{E} [\n                            \\hat{A}_t\\frac{ \\pi_\\theta(\\vec{a}_t|\\vec{s}_t) }{ \\pi_\\thetaOld(\\vec{a}_t|\\vec{s}_t) }\n                          - \\beta KL(\\pi_\\thetaOld(.|\\vec{s}_t) || \\pi_\\theta(.|\\vec{s}_t)) ]\n        Hyper-parameters: Section 4, Para 1\n\n16. Repeat Step-3 to Step-15 until training converges\n\n\n----------------------------------------------------\n[Pseudo-code: Bottom-up DGN]\n----------------------------------------------------\n=> Force set m_t^{p_i}=0 by removing Step-12.\n\n\n----------------------------------------------------\n[Pseudo-code: Bottom-up DGN]\n----------------------------------------------------\n=> Force m_t^{C_i}=0 by removing Step-6.\n\n\n----------------------------------------------------\n[Pseudo-code: No-message DGN]\n----------------------------------------------------\n=> Force m_t^{C_i}=0 by removing Step-6.\n=> Force set m_t^{p_i}=0 by removing Step-12.", "title": "Pseudo Code of the DGN Algorithm"}, "HJxIz-sYkV": {"type": "rebuttal", "replyto": "Skx_oPizAm", "comment": "Dear Reviewers:\n\nR1 suggested us to provide a pseudo-code of the overall algorithm so as to improve the understanding of the algorithm. We have provided the pseudo-code of our DGN algorithm below in reply to R1. We will add this pseudo-code to the paper as soon as open-review permits update. We will also make our code publicly available.\n\nLooking forward to your comments!\n\n\nPsuedo-Code in reply to R1: https://openreview.net/forum?id=B1lxH20qtX&noteId=BklxiCctkN", "title": "Added Pseudo-code of the algorithm in reply to R1"}, "HJlv_HUI2Q": {"type": "review", "replyto": "B1lxH20qtX", "review": "Summary:\n--------------\nThe paper considers the problem of constructing compositional robotic morphologies that can solve different continuous control tasks in a (multi-agent) reinforcement learning setting. The authors created an environment where the actor consists of a number of primitive components which interface with each other via \"linking\" and construct a morphology of a robot. To learn in such an environment, the authors proposed a graph neural network policy architecture and showed that it is better than the baselines on the proposed tasks.\n\nI find the idea of learning in environments with modular morphologies as well as the proposed tasks interesting. However, the major drawback of the paper is the lack of any reasonable details on the methods and experiments. It's hard to comment on the novelty of the architecture or the soundness of the method when such details are simply unavailable.\n\nMore comments and questions are below. I would not recommend publishing the paper in the current form.\n\n\nComments:\n----------------\n- If I understand it correctly, each component (\"limb\") represents an agent. Can you define precisely (ie mathematically) what the observations and actions of each agent are?\n\n- Page 4, paragraph 2: in the inline equation, you write that a sum over actions equals policy applied to a sum over states. What does it mean? My understanding of monolithic agents is that observations and actions must be stacked together. Otherwise, the information would be lost.\n\n- Page 4, paragraphs 3-(end of section): if I understand it correctly, the proposed method looks similar to the problem of \"learning to communicate\" in a cooperative multi-agent setting. This raises the question, how exactly the proposed architecture is trained? Is it joint learning and joint execution (ie there's a shared policy network, observation and action spaces are shared, etc), or not? All the details on how to apply RL to the proposed setup are completely omitted.\n\n- Is the topology of the sub-agents restricted to a tree? Why so? How is it selected (in cases when it is not hand-specified)?\n\n- From the videos, it looks like certain behaviors are very unphysical or unrealistic (eg parts jumping around and linking to each other). I'm wondering which kind of simulator was used? How was linking defined (on the simulator level)? It would be nice if such environments with modular morphologies were built using the standard simulators, such as MuJoCo, Bullet, etc.\n\n\nAll in all, despite potentially interesting ideas and setup, the paper is sloppily written, has mistakes, and lacks crucial details.", "title": "Interesting ideas and the setup, but virtually no details provided", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJxzS5iGRQ": {"type": "rebuttal", "replyto": "HkgwSoPq2Q", "comment": "We thank the reviewer for the constructive feedback and are glad that the reviewer found the results \"interesting\" and \"quite an encouraging\" demonstration of the proof of concept. Here we address your specific concerns. Please also see the \"common response\" posted separately.\n\nR2: Why does message passing help the standing task and not locomotion?\nR2: \"Is grouping behavior typical in the locomotion task or it is an infrequent event?\"\n=> It is possible to do well on our locomotion task with a large variety of morphologies, unlike the task of standing up where a linear high tower is strongly preferrable. In locomotion, any morphology with sufficient height and forward velocity is able to make competitive progress (as also apparent in videos) hence the context provided by the messages is not as useful. So yes, linking-up is much less frequent in locomotion compared to standing up.\n\nHowever, no-message-passing merely implies the absence of context information provided to the limbs. The DGN aggregated policy is still modular and jointly learned with the morphology, two characteristics where we differ from more conventional agents, which usually have monolithic policies and fixed morphologies. We have clarified this in both Section 3.3 and Section 5.3.\n\nR2: \"Would it be correct to assume that even without message passing and given enough training time the \"assemblies\" will learn to perform as well?\"\n=> Generally no. Each of our limb agent only receives its own local sensory information (clarified in Section 2 of updated draft) and does not have access to the global information about other limbs. Hence, in scenarios where the space of desired morphological structures is small, the role of message passing will be crucial. \n\nIt might be possible that, in some scenarios, the self-assembling agents learn to \"overfit\" the training environment in a way that these contextual messages do not provide added benefit. But in such cases, the agent might have trouble generalizing to novel scenarios, such as, presence of random pushes-and-pulls (i.e., wind), adding more limbs etc.   One such example is shown in Table 2 where no-message-passing DGN works better than top-down DGN at training (row 1). However, it does not generalize as well and performs worse in novel environments with respect to the top-down DGN (row 3, 4).  A possible reason is that the presence of distractors (e.g. wind) may need the dynamic agent to change its morphology due to which the limb controllers can benefit from their context with respect to the remaining graph which is passed in messages. \n\n\nR2: \"The videos demonstrate balancing in the standing task and it is unclear why the bottom-up and bidirectional messages perform equally well.\"\n=> This is perhaps because we do not have any cycles in the morphology and hence one pass of message passing (either top down or bottom up) is sufficient to capture the information. This property is analogous to belief propagation in Bayesian Trees [Jordan et.al. 2003] where only directional pass is enough to obtain joint distribution; we draw this connection in Section 3.3 of the updated draft. \n\nR2: \"I would disagree with your comment about lack of information for balancing in the top-down messages. The result is not intuitive.\"\n=> We apologize the comment wasn't clear.  We think one of the reasons that top-down does not perform as well on the standing task could be because the controller policy for the limbs situated at the bottom of the tower has to be very precise to maintain the whole balance in comparison to the ones at the top of the tower. However, this is still a speculation and we would try to empirically investigate it in the final version of paper.\n\nR2: \"does message passing lead to a faster training?  ... add an experimental evidence\"\n=> Our empirical observation suggests that the message passing is helpful in scenarios where the space of morphologies that perform well at a task is small. In such cases (e.g. standing), message passing indeed leads to faster training (as shown in Figures 3(a), 4(a) in the updated draft). However, message passing does not seem to have any effect on the training speed when many morphological structures can perform well at the same time (e.g., locomotion), as shown in Figure 3(b) of updated draft.\n\nFurthermore, we have significantly improved the presentation quality of the overall paper, and would like to request the reviewer to take a second look at it. Thank you!", "title": "[Authors' Response to R2] Discussing message-passing and it's value"}, "Syldx5oMC7": {"type": "rebuttal", "replyto": "HJlv_HUI2Q", "comment": "We thank you for the constructive feedback and are glad you found the modular morphologies and the proposed idea interesting.  Here we address your specific concerns. Please also see the \"common response\" posted separately.   \n\nR1: \"the major drawback of the paper is the lack of any reasonable details ...\"\n=>  We apologize for the lack of detail. We have updated the paper with all the details about the method. The overall presentation of the paper has also been significantly improved. \n\nR1: \" \u2026 define precisely (ie mathematically) what the observations and actions of each agent\"\n=> The output action space of each primitive agent contains the continuous torque values that are to be applied to the motor connected to the agent, and are denoted by $\\{\\tau_\\alpha, \\tau_\\beta, \\tau_\\gamma\\}$ for three degrees of rotation. In addition, the agent also outputs two binary actions $\\{\\sigma_{link}, \\sigma_{unlink}\\}$ which denote whether to connect another limb at the parent-end or disconnect the child-end from the other already attached limb.\n\nEach agent limb only has access to its local sensory information and does not know about other limbs. The sensory input of each agent includes its own dynamics, i.e., the location of the limb in 3-D euclidean coordinates, its velocity, angular rotation and angular velocity. Each end of the limb also has a trinary touch sensor to detect whether the end of the cylinder is touching 1) the floor, 2) another limb, or 3) nothing. Additionally, we also provide our limbs with a very simple point depth sensor that captures the surface height on a 9x9 grid around the projection of the center of limb on the surface.\n\nThis text has been added to Section 2 of the updated draft.\n\n\nR1: \"In the inline equation [Page 4, parag 2] \u2026 states, actions must be stacked together\"\n=> Yes, indeed it was a typo in the notation. Thank you. We have fixed it and explained it in Section 3.2 (first parag) of the updated draft.\n\nR1: \"looks similar to the problem of \"learning to communicate\" in a cooperative multi-agent\"\n=> Our setup bears similarity to a multi-agent setup in the sense that each limb makes its own decisions in response to the previous actions of the other agents. However, our setup differs in that our agents may physically join up, rather than just coordinate behavior. One other key difference is that, when our agents assemble to form a collective, the resulting morphology becomes a new single agent and all limbs within the morphology maximize a joint reward function. We now mention this in Section 3.3 (last para) of updated draft.\n\nR1: ...how is it trained? joint? shared? how to apply RL?...\n=> In our case, each limb has a learned controller of its own and the controller parameters are shared across limbs. The action space of each limb contains both torques as well as the commands to connect/disconnect.  This allows our self-assembling agent to jointly co-evolve their morphology along with the controller policy. DGN optimization and how it is performed using RL is described in the Section 3.3 (see Eq. 1) of the updated draft.\n\nR1: \"Is the topology of the sub-agents restricted to a tree? Why so? How is it selected (in cases when it is not hand-specified)?\"\n=>  We do not allow the emergence of cycles in agent topology for simple computational reasons: in case of cycles, one would have to perform message-passing iteratively through the cycle until convergence (similar to loopy-belief-propagation in Bayesian graphs). The topology is not hand-specified; we simply don't allow a limb to link up with already attached limbs within the same morphology. See Section 2 of the updated draft.\n\nR1: \"It would be nice ... using the standard simulators, such as MuJoCo, Bullet, etc\"\n=> We implemented our environments in the standard Unity ML framework [Juliani et.al. 2018], which is one of the dominant platforms for designing realistic games and is efficient. We did try Mujoco briefly but found it hard to simulate lots of individual controllable limbs in parallel. \n\nR1: \"How was linking defined (on the simulator level)?\"\nR1: \"certain behaviors are very unphysical or unrealistic eg parts jumping around and linking\"\n=> We implement linking action by attaching the closest limb within a small radius around the parent-node. If no other limb is present within the threshold range, the linking action has no effect. (see Section 2 of updated draft).  The linking mechanism is difficult to implement realistically in simulation and it makes things look somewhat unrealistic. \n\n\nWe have also significantly improved the presentation quality of the overall paper, and would like to request the reviewer to take a second look at it. Thank you!", "title": "[Authors' Response to R1] Updated paper with method, agent, environment details."}, "HklWuqsM0Q": {"type": "rebuttal", "replyto": "HJerl2q3h7", "comment": "We thank the reviewer for the constructive feedback and are glad that the reviewer found the general motivation \"interesting\" and the video results \"attractive\". Here we address your specific concerns. Please also see the \"common response\" posted separately.\n\nR3: The proposed model is specific to the \"multi-limb\" setting. I don't understand the applicability to other setting. How much generality does the method (or the experiment) have?\n=> First, we would like to highlight that the multi-limb setting is quite broad: many real world robots are acyclic assemblies of rigid limbs. \nSecond, we draw connections between the proposed approach and: a) multi-agent learning scenarios (Section 3.3), b) modular robotics (Section 6, para 1), c) automated robot design (Section 6, para 2), d) automated architecture search in neural networks (Section 6, para 2), and e) Bayesian Graphs (Section 3.3). We hope this work could serve as a stepping stone for future research on a range of agents (e.g., agents with soft bodies), or different materials for each limb, or more varied kinds of actuators.\n\nR3: \"Comparison to other existing methods is not enough. There are many state-of-the-art RL algorithms, and there should be natural extension to this problem setting. I can not judge whether the proposed methods work better or not.\"\n=> We apologise that it was not clear in our submitted draft, but our contribution is *not* about designing a new reinforcement learning algorithm (at least not in the sense of trying to create an optimizer that would be comparable to or compete with PPO, Q-learning, DDPG, etc.). Indeed, for all our experiments, we used a standard off-the-shelf RL method, PPO (Schulman et al.) as the underlying optimization tool. PPO was used to optimize our model and it was also used to optimize the baselines. We may just as well have used a different RL method, but choose to stick with PPO in the same way that papers on supervised learning may choose to use Adam as their optimizer without comparing to alternatives -- because it is standard and orthogonal to our contribution. The contribution of our approach is on the problem formulation and modeling side: \n(a) Formulating morphological search as a reinforcement learning problem, where linking and unlinking are treated as actions. \n(b) Representing policy via a graph whose topology matches the agent's physical structure.\n\nTo further clarify, we have updated the legends of all the monolithic policy baselines in all the graphs, results and the tables to: (a) Monolithic Policy, Dynamic Graph and (b) Monolithic Policy, Fixed Graph. They are clarified in Sections 4, 5 in the updated draft of the paper. We have also added a concrete list of contributions to the end of Introduction (Section 1).\n\n\nR3: \"detail of the sensor inputs, action spaces, and the whole algorithm \u2026 not explained well.\"\n=>   Thank you for valuable feedback. We have added full algorithm details in Section 3.3, and implementation details in Section 4 (first paragraph) in the updated draft of the paper.  The following sensor/action details have been added to Section 2 (last 2 paragraphs):\n\nAction Space: The output action space of each primitive agent contains the 3 continuous torque values (for 3 degrees of freedom) that are to be applied to the motor connected to the agent. In addition, the agent also outputs two binary actions which denote whether to connect or disconnect.\nSensory Space: Each agent limb only has access to its local sensory information including: (a) own dynamics, i.e., the location of the limb in 3-D euclidean coordinates, its velocity, angular rotation and angular velocity; (b) a trinary touch sensor at each end to detect whether the end is touching the floor, another limb, or nothing; (c) a very simple point depth sensor that captures the surface height on a 9x9 grid around the limb.\n\n\nFurthermore, we have significantly improved the presentation quality of the overall paper, and would like to request the reviewer to take a second look at it. Thank you!", "title": "[Authors' Response to R3] Clarifying our contribution; Updated paper with details"}, "Skx_oPizAm": {"type": "rebuttal", "replyto": "B1lxH20qtX", "comment": "We thank the reviewers for their insightful and helpful feedback. We are glad reviewers found the general motivation of proposed task \"interesting\" (R1, R2, R3), the video results \"attractive\" (R3) and \"as a proof of concept... quite an encouraging demonstration\" (R2).   However, all reviewers were concerned about missing details in the method and experiment sections. We apologize for this lack of clarity.   \n\nMotivated by the reviewers' comments, we have done a major update of the paper, clarifying the experiments and hopefully addressing all the reviewers' questions and concerns.  Here we summarize the key changes we have made:\n\n1) Improved overall presentation: updated introduction, environment/agents details, method section and discussion of results.\n2) Added a list of contributions to the end of the introduction\n3) Replaced generalization graphs with tables: we realized that showing the generalization results as plots was unnecessarily confusing. These plots showed zero-shot generalization performance at each *iteration* of training. However, what is more common is to pick the single best policy from training (the one that achieves highest training reward), then test how well it generalizes to new scenarios. In our revision, we report these numbers in Table 1. (For completeness, we have moved the original plots to the supplementary material.)\n\nWe will answer individual questions that the reviewers raised in the respective replies, and look forward to their follow-up advice.", "title": "[Authors' Common Response] Major update to paper; improved presentation and details."}, "HJerl2q3h7": {"type": "review", "replyto": "B1lxH20qtX", "review": "This paper investigates a collection of primitive agents that learns to self-assemble into complex collectives to solve control tasks.\nThe motivation of the paper is interesting. The project videos are attractive. However there are some issues:\n1. The proposed model is specific to the \"multi-limb\" setting. I don't understand the applicability to other setting. How much generality does the method (or the experiment) have?\n\n2. Comparison to other existing methods is not enough. There are many state-of-the-art RL algorithms, and there should be natural extension to this problem setting. I can not judge whether the proposed methods work better or not.\n\n3. The algorithm is not described in detail. For example, detail of the sensor inputs, action spaces, and the whole algorithm including hyper-parameters are not explained well.", "title": "Collection of primitive agents is interesting, but", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HkgwSoPq2Q": {"type": "review", "replyto": "B1lxH20qtX", "review": "The paper describes training a collection of independent agents enabled with message passing to dynamically form tree-morphologies.  The results are interesting and as proof of concept this is quite an encouraging demonstration.\n\nMain issue is the value of message passing\n- Although the standing task does demonstrate that message passing may be of benefit. It is unclear in the other two tasks if it even makes a difference. Is grouping behavior typical in the locomotion task or it is an infrequent event?\n  - Would it be correct to assume that even without message passing and given enough training time the \"assemblies\" will learn to perform as well as with message passing? The graphs in the standing task seem to indicate this. Would you be able to explain and perform experiments that prove or disprove that?\n  - The videos demonstrate balancing in the standing task and it is unclear why the bottom-up and bidirectional messages perform equally well. I would disagree with your comment about lack of information for balancing in the top-down messages. The result is not intuitive.\n  - Given the above, does message passing lead to a faster training?  Would you be able to add an experimental evidence of this statement?", "title": "An interesting idea of dynamical \"self-assembly\" but unclear implications of the proposed message passing ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ryxNdJrpiQ": {"type": "rebuttal", "replyto": "HyxDkjCwjm", "comment": "We thank you for taking the time to read our draft. Our answers are as follows:\n\n1. \"Fixed-Morphology Baseline\"\n=> For the fixed-morphology baseline, we chose the morphology to be a straight line chain of 6-limbs (i.e., a linear morphology) in all the experiments including standing-up and locomotion. This linear-chain may be optimal for standing as tall as possible, but it is not necessarily optimal for learning to stand; same would hold for locomotion.\n\n=> Note that DGN also converges to linear-chain morphology to achieve the best reward in case of standing-up task (e.g., see video results on the project website). Moreover, one can confirm that the locomotion task is also solvable with linear-morphology because one of the DGN ablation methods converged to a linear-morphology while doing well at locomotion.\n\n=> The underlying PPO code is used off-the-shelf from a publicly available implementation (https://github.com/ikostrikov/pytorch-a2c-ppo-acktr) and is kept same across all methods in the graph without any change.\n\n=> That being said, we were indeed surprised at baseline not performing too well and had been working on improving it. We recently found that it is hard to train fixed-morphology baseline for 6-limbs while it works well with 4-limbs. However, in either case, it does not seem to generalize as well. We will include these latest findings in an updated draft of the paper.\n\n2. \"Role of Message-passing in DGN\"\n=> We would like to emphasize that the message-passing DGN works significantly better than the non-message passing variant in the standing-up task. For instance, there is a significant gap between blue-curve (message passing) and gray-curve (non-message passing) in Figure-1.\n\n=> For the locomotion task, in particular, the graphs do indicate that the message-passing does not improve the performance. We investigated this issue in depth and found out that it is possible to do well on the current bumpy-terrain-locomotion task without making any complicated morphology. For instance, any morphology with sufficient height and forward velocity can make comparable progress. We are running experiments by making the terrain even harder to verify indeed whether it is easiness of the task or the overhead of message-passing that makes non-message passing DGN work as well or better in this case.\n\n3. Other Clarification:\n=> Finally, we would like to clarify that the generalization curves denote the performance of different training checkpoints of a model across novel setups without any further fine-tuning on those setups (i.e., zero-shot). Hence, the checkpoint, which performs the best at training, is the one that matters the most in generalization plots instead of the whole x-axis. An alternate and cleaner way to present these generalization results would be to show scores in a table for the best training checkpoint. \n\nHope this clarifies the raised questions. We would update the submitted version as the rebuttal period starts with these clarifications and new results.", "title": "Authors' response to comments"}}}