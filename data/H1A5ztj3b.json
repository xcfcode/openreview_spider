{"paper": {"title": "Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates", "authors": ["Leslie N. Smith", "Nicholay Topin"], "authorids": ["leslie.smith@nrl.navy.mil", "ntopin1@umbc.edu"], "summary": "Empirical proof of a new phenomenon requires new theoretical insights and is relevent to the active discussions in the literature on SGD and understanding generalization.", "abstract": "In this paper, we show a phenomenon, which we named ``super-convergence'', where residual networks can be trained using an order of magnitude fewer iterations than is used with standard training methods.   The existence of super-convergence is relevant to understanding why deep networks generalize well.  One of the key elements of super-convergence is training with cyclical learning rates and a large maximum learning rate.  Furthermore, we present evidence that training with large learning rates improves performance by regularizing the network. In addition, we show that super-convergence provides a  greater boost in performance relative to standard training when the amount of labeled training data is limited.  We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate.  The architectures to replicate this work will be made available upon publication.\n", "keywords": ["Deep Learning", "machine learning"]}, "meta": {"decision": "Reject", "comment": "The paper reports unusally rapid convergence of the ResNet-56 model on CIFAR-10 when a single cycle of a cyclic learning rate schedule is used.  The effect is analyzed from several different perspectives. However, the reviewers were not convinced because the effect is only observed for one task, so they question the significance of the result. There was significant discussion of the paper by the reviewers and area chair before this decision was reached.\n\nPros:\n+ Paper illustrates a \"super-convergence\" phenomenon in which training of a ResNet-56 reaches an accuracy of 92.4% on CIFAR-10 in 10,000 iterations using a single cycle of a cyclic learning rate schedule, while a more standard piecewise-constant schedule reaches 91.2% accuracy in 80,000 iterations.\n+ There was partial, independent replication of the results on other tasks reported on OpenReview.\n\nCons:\n- In the paper, the effect is shown for only one architecture and one task.\n- In the paper, the effect is shown for only a single run.\n- There are no error bars to indicate which differences are significant.\n"}, "review": {"rJfAp3Zef": {"type": "review", "replyto": "H1A5ztj3b", "review": "The paper discusses a phenomenon where neural network training in very specific settings can profit much from a schedule including large learning rates. Unfortunately, this paper feels to be hastily written and can only be read when accompanied with several references as key parts (CLR) are not described and thus the work can not be reproduced from the paper.\n\nThe main claim of the author hinges of the fact that in some learning problems the surface of the objective function can be very flat near the optimum. In this setting, a typical schedule with a decreasing learning rate would be a bad choice as the change of curvature must be corrected as well. However, this is not a general problem in neural network training and might not be generalizable to other datasets or architectures as the authors acknowledge.\n\nIn the end, the actual gain of this paper is only in the form of a hypothesis but there is only very little enlightenment, especially as the only slightly theoretical contribution in section 5 does not predict the observed behavior. \n\nPersonally i would not use the term \"convergence\" in this setting at all as the runs are very short and thus we might not be close to any region of convergence. Most of the plots shown are actually not converged and convergence in test accuracy is not the same as convergence in training loss, which is not shown at all. The results of smaller test error with larger learning rates on small training sets might therefore just be the inability of the optimizer to get closer to the optimum as steps are too long to decrease the expected loss, thus having a similar effect as early stopping.\n\nPros:\n- Many experiments which try to study the effect\nCons:\n-The described phenomenon seems to depend strongly on the problem surface and might never \nbe encountered on any problem aside of Cifar-10\n- Only single runs are shown, considering the noise on those the results might not be reproducible.\n-Experiments are not described in detail\n-Experiment design feels \"ad-hoc\" and unstructured\n-The role and value of the many LR-plots remains unclear to me.\n\nForm:\n- The paper does not maker clear how the exact schedules work. The terms are introduced but the paper misses the most basic formulas\n- Figures are not properly described, e.g. axes in Figures 3 a) and b)\n- Explicit references to code are made which require familiarity with the used framework(if at all published).  ", "title": "Interesting Phenomenon, but no deep insights", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H1yQ04YxG": {"type": "review", "replyto": "H1A5ztj3b", "review": "In this paper, the authors analyze training of residual networks using large cyclic learning rates (CLR). The authors demonstrate (a) fast convergence with cyclic learning rates and (b) evidence of large learning rates acting as regularization which improves performance on test sets \u2013 this is called \u201csuper-convergence\u201d. However, both these effects are only shown on a specific dataset, architecture, learning algorithm and hyper parameter setting. \n\n\nSome specific comments by sections:\n\n2. Related Work: This section loosely mentions other related works on SGD, topology of loss function and adaptive learning rates. The authors mention Loshchilov & Hutter in next section but do not compare it to their work. The authors do not discuss a somewhat contradictory claim from NIPS 2017 (as pointed out in the public comment): http://papers.nips.cc/paper/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks.pdf\n\n3. Super-convergence: This is a well explained section where the authors describe the LR range test and how it can be used to understand potential for super-convergence for any architecture. The authors also provide sufficient intuition for super-convergence. Since CLRs were already proposed by Smith (2015), the originality of this work would be specifically tied to their application to residual units. It would be interesting to see a qualitative analysis on how the residual error is impacting super-convergence.\n\n4. Regularization: While Fig 4 demonstrates the regularization property, the reference to Fig 1a with better test error compared to typical training methods could simply be a result of slower convergence of typical training methods. \n5. Optimal LRs: Fig.5b shows results for 1000 iterations whereas the text says 10000 (seems like a typo in scaling the plot). Figs 1 and 5 illustrate only one cycle (one increase and one decrease) of CLR. It would be interesting to see cases where more than one cycle is required and to see what happens when the LR increases the second time.\n\n6. Experiments: This is a strong section where the authors show extensive reproducible experimentation to identify settings under which super-convergence works or does not work. However, the fact that the results only applies to CIFAR-10 dataset and could not be observed for ImageNet or other architectures is disappointing and heavily takes away from the significance of this work. \n\nOverall, the work is presented as a positive result in very specific conditions but it seems more like a negative result. It would be more appealing if the paper is presented as a negative result and strengthened by additional experimentation and theoretical backing.", "title": "Large cyclic learning rates for fast convergence, works in very narrow conditions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hyn-NPJbz": {"type": "review", "replyto": "H1A5ztj3b", "review": "This paper discusses the phenomenon of a fast convergence rate for training resnet with cyclical learning rates under a few particular setting. It tries to provide an explanation for the phenomenon and a procedure to test when it happens. However, I don't find the paper of high significance or the proposed method solid for publication at ICLR.\n\nThe paper is based on the cyclical learning rates proposed by Smith (2015, 2017). I don't understand what is offered beyond the original papers. The \"super-convergence\" occurs under special settings of hyper-parameters for resnet only and therefore I am concerned if it is of general interest for deep learning models. Also, the authors do not give a conclusive analysis under what condition it may happen.\n\nThe explanation of the cause of \"super-convergence\" from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments. I feel most content of this paper (section 3, 4, 5) is observational results, and there is lack of solid analysis or discussion behind these observations.", "title": "This paper shows an observation of \u201csuper-convergence\u201d when training resnet with cyclical learning rates but does not provide conclusive analysis or experiment results.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SyXNU83Wf": {"type": "rebuttal", "replyto": "H1A5ztj3b", "comment": "1. AnonReviewer1 comments and replies:\n\"Loshchilov & Hutter in next section but do not compare it to their work.\"\nWe stated that Loshchilov & Hutter's form for CLR (called SGDR) does not work for super-convergence.  The paper now states this more clearly in the Related Works Section.\n\n\"contradictory claim from NIPS 2017 (as pointed out in the public comment)\"\nWe do not consider the claims in Hoffer, et al. (2017) contradictory.  They show that a longer training is a form of regularization, which doesn't contradict the regularization effects of large learning rates any more than it contradicts the use of dropout for regularization.  From a practical perspective, training longer has the obstacle of an even larger computational burden, hence other forms of regularization are preferable.\n\n\"It would be interesting to see a qualitative analysis on how the residual error is impacting super-convergence.\"\nWe don't think it is the residual nature of the networks that are relevant but how batch norm stabilizes the training in the presence of large learning rates causing gradient noise.  We discuss this more clearly now.\n\n\"Fig 1a with better test error compared to typical training methods could simply be a result of slower convergence of typical training methods.\" \nHoffer, et al. (2017) implies that longer training would improve the slower convergence rate in Fig 1a.  We actually let the training for the typical training schedule go to 120,000 iterations but the test accuracy was higher at 80,000 so Fig 1a shows longer training in a better light.  \n\n\"It would be interesting to see cases where more than one cycle is required and to see what happens when the LR increases the second time.\"\nThis has been done.  For example, see \"Snapshot ensembles: Train 1, get m for free\" arXiv:1704.00109.  Most of our experiments were performed last winter, prior to this paper but we saw similar results as they described.\n\n\"Overall, the work is presented as a positive result in very specific conditions but it seems more like a negative result.\"\nThe super-convergence paper presents empirical evidence of a new phenomenon that is not yet adequately explained by the literature on SGD, as such it is a positive result.  The Discussion Section should make the impact of this work clearer.\n\nThank you for your comments and the opportunity to address your concerns.\n\n\n2. AnonReviewer2 comments and replies:\n\"the work cannot be reproduced from the paper.\"\nArchitectures and code will be available on github.com.\n\n\"convergence in training loss, which is not shown at all\"\nThe training loss is shown in Figure 4.  Furthermore, we examined the training loss for all of the figures but did not include them in most of the figures for readability and it did not provide any additional insights.  \n\n\"-The described phenomenon seems to depend strongly on the problem surface and might never be encountered on any problem aside of Cifar-10\"\n\"- Only single runs are shown, considering the noise on those the results might not be reproducible.\"\nIf the purpose of the paper was to demonstrate another new technique to obtain a half a percent improvement in results, we would have averaged over 10 runs to show that the half-percent improvement. Also, the limitation of the effect to only Cifar would heavily detract from the practical significance of this paper. However, that is tangential to the primary purpose of this paper. Instead, this super-convergence paper presents empirical evidence of a new phenomenon that is not yet adequately explained by the literature on SGD and regularization.\n\n\"-Experiments are not described in detail.\"\n\"-Experiment design feels \"ad-hoc\" and unstructured\"\n\"-The role and value of the many LR-plots remains unclear to me.\"\n\"- The paper does not maker clear how the exact schedules work. The terms are introduced but the paper misses the most basic formulas\"\nArchitectures and code will be available on github.com.\n\n\"- Figures are not properly described, e.g. axes in Figures 3 a) and b)\"\nThe caption for Figure 3 was amended.  This figure was borrowed with permission from \"Qualitatively characterizing neural network optimization problems.\" arXiv:1412.6544 (2014) and a full description is available in that paper.\n\n\"- Explicit references to code are made which require familiarity with the used framework(if at all published).\"  \nArchitectures and code will be available on github.com.\n\n3. AnonReviewer3 comments and replies:\n\"I don't understand what is offered beyond the original papers.\"\n\"I am concerned if it is of general interest for deep learning models.\"\n\"Also, the authors do not give a conclusive analysis under what condition it may happen.\"\n\"a lack of solid analysis or discussion behind these observations.\"\n\nWe believe the significance of this paper and how it is intertwined with recent discussions in the literature on SGD and generalization is made clearer by the Discussions Section.", "title": "Replies to specific reviewer\u2019s comments."}, "r1J5783bz": {"type": "rebuttal", "replyto": "H1A5ztj3b", "comment": "Thank you to all the reviewers for your time and effort in reading our paper.\n\nAlthough many papers in the deep learning literature suggest new techniques for training deep networks, we did not intend for this paper to be of this kind.  Instead, this super-convergence paper presents empirical evidence of a new phenomenon that is not yet adequately explained by the literature on SGD.  While super-convergence might be of some practical value, the primary purpose of this paper is to provide empirical support and theoretical insights to the active discussions in the literature on SGD and understanding generalization.  Based on the reviewers' comments, it is apparent that the relevance of super-convergence to ongoing discussions in the literature is unclear.  We have rewritten the Discussion Section and revised various other parts of the paper to more explicitly show how our results are relevant to ongoing discussions in the literature on SGD and generalizations.  We hope the response to super-convergence is similar to the reaction to the initial report of network memorization, which sparked an active discussion within the deep learning research community on better ways of understanding the factors in SGD leading to solutions that generalize well. \n\n", "title": "General reply to the Reviewers comments."}, "rJJuxtxeG": {"type": "rebuttal", "replyto": "H1A5ztj3b", "comment": "Jastrz{\\k{e}}bski,  et al. [1] show that the larger the ratio of the learning rate to the batch size, the greater the noise during training and the better the network generalizes. They also demonstrate that instead of increasing the learning rate via cyclical learning rates, one obtains a similar effect by decreasing the batch size.  Independently, Chaudhari, et al. [2] show that the entropy of the steady-state distribution of the weights scales linearly with the ratio of the learning rate over two times the batch size and this ratio completely determines the strength of SGD's regularization.  Although the authors don't suggest a cycle, they do recommend that this ratio be large in practice, which coincides with our empirical results.\n\n1. Jastrz\u0119bski, Stanis\u0142aw, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. \"Three Factors Influencing Minima in SGD.\" arXiv preprint arXiv:1711.04623 (2017).\n2. Chaudhari, Pratik, and Stefano Soatto. \"Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks.\" arXiv preprint arXiv:1710.11029 (2017).\n", "title": "Two new papers (after the submission deadline) independently provide theoretical support for the super-convergence phenomenon "}}}