{"paper": {"title": "Asymptotics of Wide Networks from Feynman Diagrams", "authors": ["Ethan Dyer", "Guy Gur-Ari"], "authorids": ["edyer@google.com", "guyga@google.com"], "summary": "A general method for computing the asymptotic behavior of wide networks using Feynman diagrams", "abstract": "Understanding the asymptotic behavior of wide networks is of considerable interest. In this work, we present a general method for analyzing this large width behavior. The method is an adaptation of Feynman diagrams, a standard tool for computing multivariate Gaussian integrals. We apply our method to study training dynamics, improving existing bounds and deriving new results on wide network evolution during stochastic gradient descent. Going beyond the strict large width limit, we present closed-form expressions for higher-order terms governing wide network training, and test these predictions empirically.\n", "keywords": []}, "meta": {"decision": "Accept (Spotlight)", "comment": "This submission presents bounds on the training dynamics (including gradient evolution) for deep linear (and in some cases nonlinear) networks as a function of the width of the layers or number of convolutional layers. The work also presents experimental results that provide evidence that the bounds are tight.\n\nStrengths:\nThe work provides interesting insights into these training dynamics, particularly for the wide-but-not-infinite setting, which is less studied.\nThe work also adapts cluster graphs and Feynman diagrams to derive these bounds, which could be useful tools for researchers in this field.\n\nWeaknesses:\nThe validity and applicability of some of the results for nonlinear networks was not entirely clear at first but has been clarified in the revision.\n\nThe reviewer consensus was to accept this submission.\n"}, "review": {"SyesRMTujB": {"type": "rebuttal", "replyto": "B1x_upx_sr", "comment": "We will try to clarify the relationship between activations, Feynman diagrams (FDs), and cluster graphs (CGs). We are happy to add these clarifications to the paper if they are useful!\n\nFDs and CGs are complementary tools. CGs provide a correct upper bound in all cases we prove, as well as in all cases we tested empirically. FDs can disagree with CGs: FDs give tighter bounds on deep linear networks, but cannot be used to derive bounds for general activations (e.g. tanh). This is one reason the conjecture is phrased in terms of CGs and not FDs.\n\nAnalytically, we show that Feynman diagrams provide upper bounds on the asymptotics of deep linear networks. We further show that cluster graphs provide upper bounds on some networks with non-linearities (as well as on deep linear networks). Cluster graphs are a useful concept even for deep linear networks, because they are much simpler to use than Feynman diagrams. On the other hand, Feynman diagrams are useful because they provide tight bounds for deep linear networks. They are also useful because they are used heavily in the proof of Conjecture 1 for deep linear networks.\n\n> So at least one of these things is true: Conjecture 1 with FDs substituted in does not apply to tanh networks\u2026.\n\nThis is correct. Conjecture 1 with FDs substituted does not apply to deep tanh networks. However, Conjecture 1 as written does apply to tanh networks. \n", "title": "Clarifying the relationship between Feynman diagrams and cluster graphs"}, "HklO4Zydir": {"type": "rebuttal", "replyto": "BJefC4YVqr", "comment": "The authors would like to thank the reviewer for taking the time to review our paper, and for their detailed feedback! We uploaded a revision in which we believe their concerns are addressed. Specifically:\n\n> (1) In 2.1. the authors state the def. of a deep LINEAR network, however, the activation functions are non-linear. Could you be more clear what you mean by linear in your setting? \n\nThis was a typo, the word `linear\u2019 should not appear. Fixed in the revised version.\n\n> (2) What are the vertices in Conjecture 1? v_i = T(x_i), right? I think it's clear from the context (i.e., the definition of edges), but this could be written more precisely.\n\nCorrect. This is now clarified in the Conjecture.\n\n> (3) What is the actual relevance of the ORDER of the correlation function in Conjecture 1 and the relevance of the cluster graph. Can the authors motivate this more clearly, or provide some more intuition on this construction?\n\nThe method presented in this work is a general-purpose tool that can be added to a researcher's toolbox. It allows a researcher to quickly identify ways in which network behavior and training dynamics simplify at large width, for example by deriving which quantities vanish in this limit. As we demonstrate, using this method one is able to dramatically cut down on the amount of effort required to derive several key results related to wide networks, as well as to go beyond the infinite width limit and investigate ordinary neural networks.\nAs to the cluster graph, it lets us easily read off the order of a correlation function from the structure of summed derivatives acting on the network map. \nWe added the motivation behind studying the order of correlation functions to the introduction.\n> (4) What does the notation x_1 \\leftrightarrow x_2 in Eq. 8 mean?\n\nIt means \u201cadd the same term, except we exchange x_1 and x_2 where they appear\u201d. Now clarified below eq. 8.\n\n> (5) Thm. 3 - Apparently, the expression for C only depends on the #loops in \\gamma. Is this independent of the number of connected components (especially, since Conjecture 1 explicitly hinges on #connected components)?\n\nFor networks with a single hidden layer, the number of loops in a diagram is equal to the number of connected components. For deep linear networks this is no longer the case, and the quantity that gives a meaningful bound is the number of components, as in Conjecture 1. This is now clarified in Theorem 3.\n\n> (6) Does the analysis equally hold if you consider affine maps (Ax + b ) instead of linear operations Ax? \n\nYes, the analysis also holds with bias. We added a proof in the appendix (section B.3).\n\n", "title": "Authors' response"}, "B1el0lJusr": {"type": "rebuttal", "replyto": "Byl5H39iqS", "comment": "The authors would like to thank the reviewer for their detailed and thoughtful feedback! We uploaded a revision that we believe addresses their concerns. All minor corrections and suggested improvements have been taken into account. In addition:\n\n> It is unclear to me whether cluster graphs are as \"powerful\" as FDs, i.e. whether the bound at the end of the Proof in page 8 is always saturated. Are there some cases in which you need to use FDs to get a tighter upper bound?\n\nThe answer to this depends on the activation. For linear and ReLU networks, there are cases in which Feynman diagrams provide a tighter bound than the formula of Conjecture 1. For example, see Table 1, 5th example. The formula from Conjecture 1 predicts an exponent of -1, while a full diagrammatic calculation gives an exponent of -2, which matches the empirical results. However, for tanh activation the formula of Conjecture 1 gives a tight bound in all cases we tested. We explain this in section 2.3 of the revised version.\n", "title": "Authors' response"}, "BJefC4YVqr": {"type": "review", "replyto": "S1gFvANKDS", "review": "The paper investigates the asymptotic behavior of correlation functions of wide networks. The main part of the work revolves around Conjecture 1 which assesses how the correlation function scales depending on the number of connected components of even/odd size of the constructed cluster graph. The authors then study training dynamics of wide networks under gradient flow and (stochastic) gradient descent and present empirical evidence to support their claims.\n\nWhile I am no expert in this particular area, the paper is fairly well written and the main concepts and ideas are outlined nicely in most places. I do have some comments, though, wrt. the notation and the relevance of certain results:\n\n(1) In 2.1. the authors state the def. of a deep LINEAR network, however, the activation functions are non-linear. Could you be more clear what you mean by linear in your setting? \n\n(2) What are the vertices in Conjecture 1? v_i = T(x_i), right? I think it's clear from the context (i.e., the definition of edges), but this could be written more precisely.\n\n(3) What is the actual relevance of the ORDER of the correlation function in Conjecture 1 and the relevance of the cluster graph. Can the authors motivate this more clearly, or provide some more intuition on this construction?\n\n(4) What does the notation x_1 \\leftrightarrow x_2 in Eq. 8 mean?\n\n(5) Thm. 3 - Apparently, the expression for C only depends on the #loops in \\gamma. Is this independent of the number of connected components (especially, since Conjecture 1 explicitly hinges on #connected components)?\n\n(6) Does the analysis equally hold if you consider affine maps (Ax + b ) instead of linear operations Ax? \n\nOverall, the paper presents some quite interesting results, but the authors could be more clear on the relevance of those results and its implications. Non-expert readers are left with having to figure this out on their own. In my point of view, this would make the paper much stronger.", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 1}, "SJxzoDYLcS": {"type": "review", "replyto": "S1gFvANKDS", "review": "This paper proposes a new tool based on Feynman diagrams to analyze wide networks (e.g., feed-forward networks with one or more large hidden layers or CNNs with a large number of filters).\n\nThe main contributions of the paper are:\n- a new method (using Feynman diagrams) to bound the asymptotic behavior of correlation functions (ensemble averages of the network functions and its derivatives). The method is presented as a conjecture.\n- tighter bounds for gradient flow of wide networks\n- an extended analysis of SGD for wide networks\n- a formalism for deriving finite-width corrections\n\n\nThe study of (infinitely) wide networks has been active over the last few years. A better understanding of wide networks could, amongst other things, shed light on recent empirical results related to over-parametrized networks.  As such, improving our theoretical understanding of wide networks and especially properties of finite-width networks, which is what this paper explores, seems significant and potentially very impactful.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 1}, "Byl5H39iqS": {"type": "review", "replyto": "S1gFvANKDS", "review": "This is a positive review. Feel free to skip to the feedback.\n\nSUMMARY OF PAPER\nThis paper explains how to use cluster graphs to easily compute the asymptotic behaviour of any given correlation function (Definition 1) for deep linear networks. By \"asymptotic behaviour\" I mean that it upper-bounds correlation functions by c\u00b7n^s, where s is a nonnegative integer given by the particular cluster graph, and c a \"constant\" (which I think depends on the particular input, x, to correlation function, among other things).\n\nThe authors then conjecture (Conjecture 1) that these bounds transfer to deep nonlinear networks, and that they are tight:\n- Appendix C proves that these upper bounds also hold for deep ReLU networks, and 1-hidden-layer networks with smooth nonlinearity. This is also mentioned in page 3.\n- Section 2.3 empirically shows that these bounds are pretty tight. (in terms of the exponent, none of the theory here gives a value for the constant c)\n\nThe tool provided above is the main result. The authors then use it to provide some results about wide networks  \n- They give a different proof that for large width, the Neural Tangent Kernel stays constant during training. This is because its derivative wrt. time as a function of width n is O(n^-1), and thus 0 for n->infinity.\n- Using the ease of calculation from the tool, they approximate the change in the NTK over training time for any network. They do this using its value at initialization + a term that depends on n^-1. These results are numerically verified in Figure 1.\n- They present numerical evidence for the accuracy of this approximation to the change in NTK over time. \n\nThe authors spend the last 2 pages explaining how cluster graphs derive from Feynman diagrams (FDs), and why these help compute asymptotics.\n\nWHY I AM ACCEPTING THIS PAPER\n\nThe paper adapts FDs and cluster graphs, which is a potentially very useful tool for other wide-network researchers, and could accelerate research in this whole sub-field. It also shows their power by providing a surprisingly large amount of novel theoretical results.\n\nFEEDBACK\n\nAt a very high level, there is only one thing that I think isn't made quite clear by the presentation, and it should be. If I understand correctly, Feynman diagrams (or cluster graphs) are only used here to calculate correlation functions for deep *linear* networks. Then, other results establish that the width-dependent asymptotic behaviour for linear networks holds as-is for nonlinear networks, and these results with FDs constitute Conjecture 1. There are proofs for ReLU and 1-layer smooth networks, mentioned in pg. 3; and the experiments in the paper support it for common nonlinear deep networks as well. I think that asymptotics for linear networks transfer to nonlinear ones is an interesting result, which doesn't depend on FDs.\n\nWhat follows are details.\n\nIt is unclear to me whether cluster graphs are as \"powerful\" as FDs, i.e. whether the bound at the end of the Proof in page 8 is always saturated. Are there some cases in which you need to use FDs to get a tighter upper bound?\n\nIn Table 1 you should say that the values under \"lin. ReLU tanh\" are the fitted exponent s_C. This is not explained. Perhaps you  can mark the only 2 cases (in the 5th row) where the bound is not tight. It would be nice to know how much error remains between the fitted c\u00b7n^(s_C) and the empirical values.\n\nPlease explain x1 <-> x2 in eq. 8\n\nIn figure 1b, consider adding the finite-width limit prediction for the training dynamics. You have already done so for the prediction of the NTK during training in figure 5c, you could indicate it in the same way in 5b.\n\n\nTypos:\n\nFigure 2 caption: feynman -> Feynman\n\npg 8. anlytic evicence -> analytic", "title": "Official Blind Review #4", "rating": "8: Accept", "confidence": 3}}}