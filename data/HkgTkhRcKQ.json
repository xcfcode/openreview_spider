{"paper": {"title": "AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods", "authors": ["Zhiming Zhou*", "Qingru Zhang*", "Guansong Lu", "Hongwei Wang", "Weinan Zhang", "Yong Yu"], "authorids": ["heyohai@apex.sjtu.edu.cn", "neverquit@sjtu.edu.cn", "gslu@apex.sjtu.edu.cn", "wanghongwei55@gmail.com", "wnzhang@sjtu.edu.cn", "yyu@apex.sjtu.edu.cn"], "summary": "We analysis and solve the non-convergence issue of Adam.", "abstract": "Adam is shown not being able to converge to the optimal solution in certain cases. Researchers recently propose several algorithms to avoid the issue of non-convergence of Adam, but their efficiency turns out to be unsatisfactory in practice. In this paper, we provide a new insight into the non-convergence issue of Adam as well as other adaptive learning rate methods. We argue that there exists an inappropriate correlation between gradient $g_t$ and the second moment term $v_t$ in Adam ($t$ is the timestep), which results in that a large gradient is likely to have small step size while a small gradient may have a large step size. We demonstrate that such unbalanced step sizes are the fundamental cause of non-convergence of Adam, and we further prove that decorrelating $v_t$ and $g_t$ will lead to unbiased step size for each gradient, thus solving the non-convergence problem of Adam. Finally, we propose AdaShift, a novel adaptive learning rate method that decorrelates $v_t$ and $g_t$ by temporal shifting, i.e., using temporally shifted gradient $g_{t-n}$ to calculate $v_t$. The experiment results demonstrate that AdaShift is able to address the non-convergence issue of Adam, while still maintaining a competitive performance with Adam in terms of both training speed and generalization. ", "keywords": ["optimizer", "Adam", "convergence", "decorrelation"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes a new stochastic optimization scheme similar to Adam. The authors claim that Adam can be improved upon by decorrelating the second-moment estimate v_t from gradient estimates g_t. This is done through the temporal decorrelation scheme, as well as block-wise sharing of estimates v_t.\n\nThe reviewers agree that the paper is sufficiently well-written, original and significant to be accepted for ICLR, although some unclarity remains after the reviews. A disadvantage of the method is mainly an increased computational cost (linear in 'n', however this might be negligible when sharing v_t across blocks)."}, "review": {"HJgaHFKn9V": {"type": "rebuttal", "replyto": "BJglD8rbzV", "comment": "Hi, we have published our code of AdaShift on GANs. \n\nSee https://github.com/ZhimingZhou/AdaShift-Lipschitz-GANs-MaxGP. \n\n------------\n\nThis repo includes the implementation of AdaShift and also the demonstration code that uses AdaShift to training GANs which achieves FID: 15.8800\u00b10.4921 and Inception Score: 8.0367\u00b10.0499 for unsupervised image generation of GANs in CIFAR-10.\n\nThe provided implementation of AdaShift (common/optimizer/AdaShift) is further developed version, which extends our discussion in [1], i.e., v_t can be any random variable that keeps the scale of the gradients and is independent of g_t. We use LGANs developed in [2] and MaxGP described in [3].\n\n[1] AdaShift: Decorrelation and Convergence of Adaptive Learning Rate Methods https://arxiv.org/abs/1810.00143\n\n[2] Lipschitz Generative Adversarial Nets https://arxiv.org/abs/1902.05687\n\n[3] Towards Efficient and Unbiased Implementation of Lipschitz Continuity in GANs https://arxiv.org/abs/1904.01184\n\nWe use tensorflow 1.5 with python 3.5. You can refer to setting_cuda9_cudnn7_tensorflow1.5.sh to build up your environment. Try the code via running: python3 realdata_resnet.py. synthetic_real.py and synthetic_toy.py are the code we used for the synthetic experiments in [2] and [3].", "title": "Code of AdaShift for GANs training. "}, "rygwYIIiGV": {"type": "rebuttal", "replyto": "rkgAZSb9ME", "comment": "In our experiments, we have also found that GP sometimes leads to training divergence, we suggest using MaxGP, which we have found usually can solve this problem. \n\nFixing discriminator and training generator is a well-defined optimization problem, but it is less interesting we think. Because it is merely finding the optimal x, the x that holds the maximum D(x). Or more strictly, optimizing all x towards the local optimal. This is kind of simulating the typical cause of mode collapse problem in GANs.  \n\nGiven fixed P_r and P_g and training the discriminator is estimating the given distance metric, say Wasserstein distance, between P_r and P_g, which is a sound optimization problem we believe. And we suspect that being hard to get the optimal discriminative function is one important cause of why GAN is currently not easy to train. This is also what motivates the authors to study optimizer. \n\nIf the memory is not wrong, we have achieved Inception Score around 8.0 with AdaShift (comparable with Adam). For more experiment details, we will post another response in a few weeks. \n", "title": "A quick reply. "}, "BygUdM1fGE": {"type": "rebuttal", "replyto": "BJglD8rbzV", "comment": "Thanks a lot for your interest in our paper and your reproduction of our results. \n\n* GANs is currently a mystery. When you joint training the generator and discriminator, there exists a lot of issues on the minimax conflict and generator-discriminator balance/tradeoff in GANs, even using the relatively mature WGAN. So, we choose to train the discriminator only, i.e.., fixing the generator. \n\n* WGAN requires the discriminative function to be k-Lipschitz. However, how to enforce the Lipschitz is currently a not well-addressed problem. Though WGAN-GP provides a solution which is well accepted, we believe it is not an accurate implementation of Lipschitz. We thus choose MaxGP which we believe is a more accurate implementation. See Appendix E in [1] for details. \n\n* The penalty coefficient is set to 0.1 because we empirically found that no matter for MaxGP or the original GP, it is better than 10.0 in most cases. So, we use 0.1 as the default. \n\n* Thanks for point out the discrepancy between the implementation and the description in the paper. We think we have miswritten it in the paper, though in the current form of the paper, to match the implementation, we only need reformulation the function phi to be sqrt(max x^2). We choose the square the gradient first because we'd like $v$ to reflect the gradient scale, otherwise, the max operation will ignore the negative ones. We will revise the paper to make it more clear. Thanks. \n\n[1] Understanding the Effectiveness of Lipschitz-Continuity in Generative Adversarial Nets: https://openreview.net/pdf?id=r1zOg309tX \n\nBTW: did you try different learning rate for AdaShift in the WGAN experiments? Its best learning rate is usually around ten times the learning rate of Adam. According to our experiences, they should be comparable. \n", "title": "Thanks a lot!"}, "B1gVyyYq0X": {"type": "rebuttal", "replyto": "BklQd4otAQ", "comment": "Thank you so much for the comments and useful references. We will put our effort into the convergence analysis. Hopefully, we will have some convergence analysis in our final version. ", "title": "Thank you so much. "}, "SkxDrCgdnX": {"type": "review", "replyto": "HkgTkhRcKQ", "review": "In this paper, the authors found that decorrelating $v_t$ and $g_t$ fixes the non-convergence issue of Adam. Motivated by that, AdaShift that uses a temporal decorrelation technique is proposed. Empirical results demonstrate the superior performance of AdaShift compared to Adam and AMSGrad. My detailed comments are listed as below. \n\n1) Theorem 2-4 provides interesting insights on Adam. However, the obtained theoretical results rely on specific toy problems (6) and (13). In the paper, the authors mentioned that \"... apply the net update factor to study the behaviors of Adam using Equation 6 as an example. The argument will be extended to the stochastic online optimization problem and general cases.\" What did authors mean the general cases?\n\n2) The order of presenting Algorithm 1, 2 and Eq. (17) should be changed. I suggest to first present AdaShift (i.e., Eq. (17) or Algorithm 3 with both modified adaptive learning rate and moving average), and then elaborate on temporal decorrelation and others. AdaShift should be presented as a new Algorithm 1.  In experiments, is there any result associated with the current Algorithm 1 and 2? If no, why not compare in experiments? One can think that Algorithm 1 and 2 are adaptive learning rate methods against adaptive gradient methods (e.g., Adam, AMSGrad). \n\n3) Is there any convergence rate analysis of AdamShift even in the convex setting?\n\n4) The empirical performance of AdamShift is impressive. Can authors mention more details on how to set the hyperparameters for AdamShift, AMSGrad, Adam, e.g., learning rate, \\beta 1, and \\beta 2? \n", "title": "Another fix of non-convergence of Adam -- AdaShift", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1gToxwR6X": {"type": "rebuttal", "replyto": "S1guRoZwhQ", "comment": "Thanks for your constructive feedback. \n\nQ: In my eyes, the limitations of the paper are that the example studied is a bit contrived and as a result, I am not sure how general the improvements. More generally, I am worried that the theoretical results and the intuitions backing the improvements are built only on one pathological example. Are there arguments to claim that this example is a prototype for a more general behavior? \n\n>> We mixed the general arguments for the non-convergence of Adam into these analyses of counterexamples. According to the reviewers' feedback, we realize that it is indeed confusing. We thus have reorganized the analysis section, and clearly separated the analysis on counterexamples and the general arguments on the non-convergence issue of Adam. Actually, \u2018\u2018assigning relatively small step-size to large gradient and assigning relatively large step-size to small gradient\u2019\u2019 is the general behavior of Adam and traditional adaptive learning rate methods. Sometimes it causes non-convergence, and more generally, it just hampers the convergence. Please see the reorganized arguments in Section 3.3 for details. \n\nQ: With regards to the solution proposed, temporal decorrelation, I wonder how it interacts with the mini-batch side. With only a light understanding of the problem, it seems to me that large mini-batches will decrease the variance of the gradient estimates and hence increase the correlation of successive samples, breaking the assumptions of the method. \n\n>> The argument is thought-provoking. But it seems that, though decreasing the variance makes the difference between samples smaller, it does not change the independence. Assume that the gradients are independently sampled from a standard Gaussian N(0, 1). If the Gaussian is squeezed to N(0, 0.1), gradients sampled from the squeezed Gaussian are still independent of each other. Using our argument in the paper, we still reach the same conclusion: assuming the loss function is fixed, as long as these mini-batches are independently sampled, no matter the mini-batch size is large or small, their gradients are always independent. \n\nQ: The performance gain compared to Adam seems consistent. It would have been interesting to see Nadam in the comparisons. \n\n>> We have conducted a set of experiments for Nadam. The results are presented in Appendix K. Generally, we found Nadam shows quite similar performance as Adam. Please check Appendix K for details. \n\nQ: Ali Rahimi presented a very simple example of the poor performance of the Adam optimizer in his test-of-time award speech at NIPS this year. It seems like an excellent test for any optimizer that tries to be robust to ill-conditioning (as with Adam), though I suspect that the problem solved here is a different one than the problem raised by Rahimi's example. \n\n>> It is an interesting test and we have tested our algorithm with the code they provided. Our finding is somewhat weird: as long as the training is sufficiently long, SGD, Adam, and AdaShift basically converge in this problem, though the final performance of SGD is significantly better than Adam and AdaShift. \n\n>> We tend to believe this is a general issue of adaptive learning rate method when comparing with vanilla SGD. Because these adaptive learning rate methods are generally scale-invariance, i.e., the step-size in terms of g_t/sqrt(v_t) is basically around 1, which makes it hard to converge very well in such an ill-conditioning quadratic problem. SGD, in contrast, has a step-size g_t. As the training converges, SGD would have a decreasing step-size, making it much easier to converge better. To confirm our analysis, we train the same task with a decreasing learning rate, and we found that at the end of the training, Adam and AdaShfit both converge satisfactorily. \n\n>> Levenberg-Marquardt, which minimizes $(\\delta W_1, \\delta W_2)$ by solving least-squares, shows the fastest convergence. It indicates the possibility of better alternatives to gradient descent (backpropagation) based optimization, which deserves further investigations. \n", "title": "Response to Reviewer 2 "}, "BJe4vgwCaX": {"type": "rebuttal", "replyto": "SkxDrCgdnX", "comment": "Thanks for your constructive feedback. \n\nQ: However, the obtained theoretical results rely on specific toy problems (6) and (13). In the paper, the authors mentioned that \"... apply the net update factor to study the behaviors of Adam using Equation 6 as an example. The argument will be extended to the stochastic online optimization problem and general cases.\" What did the authors mean the general cases? \n\n>> We are sorry for the confusion. We mixed the general arguments and the counterexample-specific arguments together. According to the reviewers\u2019 feedback, we have reorganized the analysis section, and now the analysis on counterexamples and the general arguments on the non-convergence of Adam are separated. We would appreciate if you could have a check on these reorganized arguments (Section 3.3). The general arguments are actually very sound. \n\nQ: The empirical performance of AdaShift is impressive. Can authors mention more details on how to set the hyperparameters for AdaShift, AMSGrad, Adam, e.g., learning rate, \\beta 1, and \\beta 2? \n\n>> In the revision, we have listed hyperparameter settings in each experiment in Appendix. We have also conducted a set of experiments on hyperparameter sensitivities of AdaShift, which are also included in Appendix. Please check these details in Appendix I of the new version of our paper. \n\nQ: I suggest to first present AdaShift (i.e., Eq. (17) or Algorithm 3 with both modified adaptive learning rate and moving average), and then elaborate on temporal decorrelation and others. AdaShift should be presented as a new Algorithm 1. \n\n>> Thanks a lot for this valuable suggestion. We have tried your suggestion and it looks much better. Please check it in the revised version. \n\nQ: Is there any convergence rate analysis of AdaShift even in the convex setting? \n\n>> Currently, we do not have convergence rate analysis for AdaShift. We will work on it and hope it will appear soon. \n", "title": "Response to Reviewer 3"}, "S1gWBeDCpQ": {"type": "rebuttal", "replyto": "r1gzGyM5hX", "comment": "Thanks for your constructive feedback. \n\nQ: Regarding content, the reviewer is quite dubious about the spatial decorrelation idea. Assuming shared moment estimation for blocks of parameters is definitely meaningful from an information perspective, and has indeed been used before, but it seems to have little to do with the 'decorrelation' idea. \n\n>> In our proposed algorithm, only the spatial elements of temporally-shifted gradient g_{t-n} are involved in the calculation of v_t. Based on the temporal independence assumption, g_{t-n} is independent of g_t, which naturally implies that all elements in g_{t-n} are independent of the elements in g_t. Thus, using the spatial elements in g_{t-n} does not break the independence assumption. We have revised the related sections and avoided the term \u2018\u2018spatial independence\u2019\u2019 that is indeed confusing. \n\nQ: Regarding presentation, the reviewer's opinion is that the paper is too long. Too much space is spent discussing an interesting yet limited counterexample, on which 5 theorems (that are simple analytical derivations) are stated. This should be summarized (and its interesting argument stated more concisely), to the benefit of the actual algorithm presentation, that should appear in the main text (Algorithm 3). The spatial decorrelation method, that remains unclear to the reviewer, should be discussed more and validated more extensively. The current size of the paper is 10 pages, which is much above the ICLR average length. \n\n>> Thanks a lot for these constructive suggestions. We have rewritten related sections accordingly. The main changes are: (i) we have renamed the analytical derivations as lemmas and removed unnecessary details; (ii) we have reorganized the analysis section to make it more concise and clear; (iii) we have removed Algorithms 1 and 2, and directly presented Algorithm 3; (iv) we have made the arguments on the validity of using spatial elements much more clear. \n\nQ: The reviewer would be curious to see a comparison with temporal-only AdaShift in the experiment, as the block/max operator \\phi, to isolate the temporal and 'spatial' effect. \n\n>> We have added experiments on temporal-only AdaShift and spatial-only AdaShift. Some experiments on temporal-only AdaShift can be found in Figure 2 and Figure 3 in the experiments Section, and more results are included in Appendix J and K. \n\n>> Temporal-only AdaShift is actually not as stable as AdaShift. It works well in simple tasks, but it suffers from explosive gradient in complex systems: a neuron recovering from a vanishing gradient state is the typical failure case, where v_t is nearly zero. AdaShift with spatial operation, in contrast, does not suffer from this problem: the gradients of an entire block is relatively stable and won\u2019t vanish. \n\n>> Spatial-only AdaShift turns out not to fit our assumption, but it is indeed a very interesting extension of Adam. Therefore, we have also conducted a set of experiments on spatial-only AdaShift. According to our initial investigations, \u2018\u2018spatial-only AdaShift\u2019\u2019 shares a similar performance to Adam. Details are presented in Appendix J and K. ", "title": "Response to Reviewer 1 "}, "r1gzGyM5hX": {"type": "review", "replyto": "HkgTkhRcKQ", "review": "Summary\n------\n\nBased on an extensive argument acoordig to which Adam potential failures are due to the positive correlation between gradient and moment estimation, the authors propose Adashift, a method in which temporal shift (and more surprisingly 'spatial' shift, ie mixing of parameters) is used to ensure that moment estimation is less correlated with gradient, ensuring convergence of Adashift in pathological cases, without the efficiency cost of simpler method such as AMSGrad. An extensive analysis of a pathological counter example, introduced in Reddi et al. 2018 is analysed, before the algorithm presentation and experimental validation. Experiments shows that the algorithm has equivalent speed as Adam and sometimes false local minima, resulting in better training error, and potentially better test error.\n\nReview\n-------\n\nThe decorrelation idea is original and well motivated by an extensive analysis of a pathological examples. The experimental validation is thorough and convincing, and the paper is overall well written. \n\nRegarding content, the reviewer is quite dubious about the spatial decorrelation idea. ASsuming shared moment estimation for blocks of parameters is definitely meaningful from an information perspective, and has indeed been used before, but it seems to have little to do with the 'decorrelation' idea. The reviewer would be curious to see a comparison with temporal-only adashift in the experiment, as the block / max operator \\phi, to isolate the temporal and 'spatial' effect.\n\nRegarding presentation, the reviewer's opinion is that the paper is too long. Too much space is spent discussing an interesting yet limited counterexample, on which 5 theorems (that are simple analytical derivations) are stated. This should be summarized (and its interesting argument stated more concisely), to the benefit of the actual algorithm presentation, that should appear in the main text (algorithm 3). The spatial decorrelation method, that remains unclear to the reviewer, should be discussed more and validated more extensively. The current size of the paper is 10 pages, which is much above the ICLR average length.\n\nHowever, due to the novelty of the algorithm, the reviewer is in favor of accepting the paper, provided the authors can address the comments above.\n", "title": "Original contribution to stochastic optimizers, with presentation to be rearranged", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1guRoZwhQ": {"type": "review", "replyto": "HkgTkhRcKQ", "review": "This manuscript contributes a new online gradient descent algorithm with adaptation to local curvature, in the style of the Adam optimizer, ie with a diagonal reweighting of the gradient that serves as an adaptive step size. First the authors identify a limitation of Adam: the adaptive step size decreases with the gradient magnitude. The paper is well written.\n\nThe strengths of the paper are a interesting theoretical analysis of convergence difficulties in ADAM, a proposal for an improvement, and nice empirical results that shows good benefits. In my eyes, the limitations of the paper are that the example studied is a bit contrived and as a results, I am not sure how general the improvements.\n\n# Specific comments and suggestions\n\nUnder the ambitious term \"theorem\", the results of theorem 2 and 3 limited to the example of failure given in eq 6. I would have been more humble, and called such analyses \"lemma\". Similarly, theorem 4 is an extension of this example to stochastic online settings. More generally, I am worried that the theoretical results and the intuitions backing the improvements are built only on one pathological example. Are there arguments to claim that this example is a prototype for a more general behavior?\n\n\nAli Rahimi presented a very simple example of poor perform of the Adam optimizer in his test-of-time award speech at NIPS this year (https://www.youtube.com/watch?v=Qi1Yry33TQE): a very ill-conditioned factorized linear model (product of two matrices that correspond to two different layers) with a square loss. It seems like an excellent test for any optimizer that tries to be robust to ill-conditioning (as with Adam), though I suspect that the problem solved here is a different one than the problem raised by Rahimi's example.\n\n\nWith regards to the solution proposed, temporal decorrelation, I wonder how it interacts with mini-batch side. With only a light understanding of the problem, it seems to me that large mini-batches will decrease the variance of the gradient estimates and hence increase the correlation of successive samples, breaking the assumptions of the method.\n\n\nUsing a shared scalar across the multiple dimensions implies that the direction of the step is now the same as that of the gradient. This is a strong departure compared to ADAM. It would be interesting to illustrate the two behaviors to optimize an ill-conditioned quadratic function, for which the gradient direction is not a very good choice.\n\n\nThe performance gain compared to ADAM seems consistent. It would have been interesting to see Nadam in the comparisons.\n\n\n\nI would like to congratulate the authors for sharing code.\n\nThere is a typo on the y label of figure 4 right.\n", "title": "Analyses and fixes one problem of ADAM that could be specific or general", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Byg39Xum9Q": {"type": "rebuttal", "replyto": "S1eEIQx-9Q", "comment": "Thanks for your interest in our paper and sorry for not releasing the code in time. The code is now accessible from the provided link. \n\nWe think publicizing the code should be done before the review process, rather than after paper acceptance. And from our perspective, releasing the code bears no relation to contribution, but the authors'  duty. ", "title": "The code is now accessible."}}}