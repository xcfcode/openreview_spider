{"paper": {"title": "Expected Information Maximization: Using the I-Projection for Mixture Density Estimation", "authors": ["Philipp Becker", "Oleg Arenz", "Gerhard Neumann"], "authorids": ["philippbecker93@googlemail.com", "oleg@robot-learning.de", "geri@robot-learning.de"], "summary": "A novel, non-adversarial, approach to learn latent variable models in general and mixture models in particular by computing the I-Projection solely based on samples.", "abstract": "Modelling highly multi-modal data is a challenging problem in machine learning. Most algorithms are based on maximizing the likelihood, which corresponds to the M(oment)-projection of the data distribution to the model distribution.\nThe M-projection forces the model to average over modes it cannot represent. In contrast, the I(nformation)-projection ignores such modes in the data and concentrates on the modes the model can represent. Such behavior is appealing whenever we deal with highly multi-modal data where modelling single modes correctly is more important than covering all the modes. Despite this advantage, the I-projection is rarely used in practice due to the lack of algorithms that can efficiently optimize it based on data. In this work, we present a new algorithm called Expected Information Maximization (EIM) for computing the I-projection solely based on samples for general latent variable models, where we focus on Gaussian mixtures models and Gaussian mixtures of experts. Our approach applies a variational upper bound to the I-projection objective which decomposes the original objective into single objectives for each mixture component as well as for the coefficients, allowing an efficient optimization. Similar to GANs, our approach employs discriminators but uses a more stable optimization procedure, using a tight upper bound. We show that our algorithm is much more effective in computing the I-projection than recent GAN approaches and we illustrate the effectiveness of our approach for modelling multi-modal behavior on two pedestrian and traffic prediction datasets.  ", "keywords": ["density estimation", "information projection", "mixture models", "generative learning", "multimodal modeling"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes a new algorithm called Expected Information Maximization (EIM) for learning latent variable models while computing the I-projection solely based on samples. The reviewers had several questions, which the authors sufficiently answered. The reviewers agree that the paper should be accepted. The authors should carefully read the reviewer questions and comments and use them to improve their final manuscript. "}, "review": {"BkxpEGGQor": {"type": "rebuttal", "replyto": "B1er3Km0KH", "comment": "We thank the reviewers for their time and valuable feedback. Besides fixing small typos, ambiguities and unclearities we elaborated on the relation and differences to previously existing GAN and VI methods (see in particular section 2).\n\n Also, we uploaded an implementation of EIM, which can be found at: https://github.com/eimAuthors/EIM\n\nWe are now going to answer your specific questions:\n\n\u201cMy main concerns focus on the novelty\u201d / \u201cFor the former, reverse KL has been exploited before, both in the marginal space [1] and the joint one [2]. Other detailed comments are listed below.\u201d / \u201cIn Sec 4.4, it seems EIM is highly overlapped with VIPS. So what're the advantages of EIM here?\u201d -\n\nOur approach is, to the best of our knowledge, the first approach allowing non-adversarial computation of the I-Projection based solely on samples of the target distribution. \n\nThe difference to VIPS and [1]  is that both assume access to the unnormalized (log) density of the target distribution, i.e. are applicable for variational inference. EIM on the other hand assumes access to samples of the target distribution, i.e. is applicable for density estimation.\n\nThe difference to [1] and [2] is that EIM is not adversarial as pointed out in section 4.3. \n\nWe reworked the related work section to make these important distinctions clearer. \n\nFurthermore, [2] introduces a bound for the symmetric KL between the joints over x and z (where x is the random variable underlying the target samples and z the latent variable). The learned discriminator thus needs both x and z as inputs. In order to infer the latent variable for the target samples an additional variational distribution q(z|x) (i.e. an encoder) needs to be learned. \nEIM does not use the symmetric KL, but the reverse KL. Furthermore it works with a bound for the KL between the marginals over x, not the joint of x and z. Thus the discriminator only needs to be given x and the latent variable z does not need to be inferred for the training data, i.e., no \u201cencoder\u201d is necessary.\n\n\u201cIn Figure 2 (b), the experimental settings for adversarial learning are not fair, as the discriminator is not fixed there. \u201c\n\nThe purpose of this figure (and section 4.3 in general) is to provide an illustrative example of the immediate effects and benefits of avoiding the adversarial forumulation \n\n\u201cIn Figure 3, how many steps for Generator and Discriminator are used for f-GAN? Does f-GAN finally converge?[...]\u201d - \n\nFigure 3: As suggested in the f-GAN paper we alternate single generator and discriminator steps. We also evaluated training the discriminator longer without notable changes to the final performance. The f-GANs do eventually converge and we report the best value achieved on a test set for each run, averaged over 20 runs. A list of all hyperparameters can be found in the appendix, we added the parameters for the f-GAN training to this. \n\n\u201cIn Eq. 9, adding the denominator q(z_i) will change the optimal solution. Why only add it to the first term?\u201d -  The notation in this equation was a bit unclear. The denominator is in fact added to both terms, which scales the optimal value but does not change the optimal solution. We apologize for the unclear notation and adapted the equation to make it clearer.  \n\n\u201cIn Section 5.3 and Figure 5, \u201cSSD\u201d might be a typo. \u201c - Indeed a typo, thanks for pointing it out, we fixed it. \n\nWe hope we could clarify and remove some of the remaining doubts in our approach. We invite you to ask additional questions and engage in further discussion if this is not the case.\n", "title": "Response to Reviewer 2"}, "rJlXs-G7sH": {"type": "rebuttal", "replyto": "rkxrfrIfcB", "comment": "We thank the reviewers for their time and valuable feedback. Besides fixing small typos, ambiguities and unclearities we elaborated on the relation and differences to previously existing GAN and VI methods (see in particular section 2).\n\n Also, we uploaded an implementation of EIM, which can be found at: https://github.com/eimAuthors/EIM\n\nWe are now going to answer your specific questions:\n\n\u201cI would have liked the authors to spend some more time on the right way to evaluate\u201d/ \u201cmore discussion about the evaluation metric\u201d\n\nWe believe that the reverse KL is the right metric for the applications that we consider, which motivates the formulation of our optimization problem.\nSadly it is not possible to compute the reverse KL if the true density underlying the data is not known (which is the case in all but the first experiment, in which we evaluated the reverse KL). This is also the key reason for why minimizing the reverse KL is much harder than minimizing the forward KL (i.e. maximizing the likelihood).\n\nThus, we had to resort to auxillary metrics. The likelihood is an obvious choice here since it is a standard evaluation criterion for generative models and easy to compute. Additionally we evaluate our models on metrics, meaningful to the task at hand. By combining the likelihood with the auxiliary metric, we can evaluate both whether the data distribution is covered and whether unrealistic samples are generated by the learned model. The same is typically done for GAN approaches which suffer from the same problem of a non-computable objective for non-toy tasks.\n\n\u201c- would have liked to see an explicit algorithm for the optimization procedure\u201d - There is pseudocode for the GMM case in the appendix.  As previously mentioned, we also released the real code by now.\n\n\u201c- small lack of clarity in the presentation of Section 4.1---notation q_t is not introduced for example\u201d - We kindly ask you to elaborate on this lack of clarity so we might clarify. We already clarified the introduction of q_t.  \n\n\u201c- linking it more to prior work\u201d - the related work section has been reworked.\n\n\nWe hope we could clarify and remove some of the remaining doubts in our approach. We invite you to ask additional questions and engage in further discussion if this is not the case.\n", "title": "Response to Reviewer 3 "}, "BJevwbfXoB": {"type": "rebuttal", "replyto": "SyxDIDyp5S", "comment": "We thank the reviewers for their time and valuable feedback. Besides fixing small typos, ambiguities and unclearities we elaborated on the relation and differences to previously existing GAN and VI methods (see in particular section 2).\n\n Also, we uploaded an implementation of EIM, which can be found at https://github.com/eimAuthors/EIM\n\nWe are now going to answer your specific questions:\n\n\u201cHow this can be applicable to more realistic and complex models where training requires millions of gradient steps?\u201d - The density ratio estimator does not need to be trained from scratch every iteration. It can be warm started using the density ratio estimator from the previous iteration. We use early-stopping for regularization which will typically end training after a few iterations. The change in density ratio during each iteration is rather small since the model updates are constraint. \n\n  \u201cI am a little confused about Sec 4.3. It seems that the latent variable z is not necessary for the proposed EIM?\u201d - In Sec 4.3 we consider only the simplest possible model, a single univariate Gaussian (i.e. a Gaussian Mixture with one component) for illustrative purposes. Like for any latent variable approach the latent variable can be \u201comitted\u201d by choosing q(z) to be a deterministic distribution.\n\n\u201cAlso, the typical practise of training GAN [...] training gets more stable than standard GAN?\u201d - We in fact update the discriminator for multiple steps with early stopping. The difference to GANs is that our approach is not adversarial, this removes a key reason for the instability of GAN training. Our derivations for this non-adversarial optimization are based on the additional KL term. Hence, we also believe that it is a major reason for the improved stability of EIM. \n\n\u201cWill the same algorithm can be applied on more general latent variable models\u201d -\nThe general approach derived in section 4 can be applied to general latent variable models. \n(For discussion on the KL term see the next bullet point)\n\n\u201c or even implicit models like GAN does?\u201d / \u201c[...] , how to compute the regularization term KL(q(x) || q_t(x)) in EIM for normal generator which is typically implicit?\u201d - \nIf the KL term can not be computed in closed form it can still be approximated using samples as long as the model density is tractable. Note that we do not need to compute the KL divergence between the marginals KL(q(x) || q_t(x)) but only the KL between the conditionals KL(q(x|z) || q_t(x|z)) and the latent distribution KL(q(z)||q_t(z)). Even if the Marginal q(x) is intractable (as for GANs) the density of the conditionals is usually not. \nNethertheless, investigating how to use EIM for typical GAN scenarios, e.g., generating image data, is an interesting direction for future work.\n\nWe hope we could clarify and remove some of the remaining doubts in our approach. We invite you to ask additional questions and engage in further discussion if this is not the case.\n", "title": "Response to Reviewer 1"}, "B1er3Km0KH": {"type": "review", "replyto": "ByglLlHFDS", "review": "The paper presents an algorithm to match two distributions with latent variables, named expected information maximization (EIM). Specifically, EIM is based on the I-Projection, which basically is equivalent to minimizing the reverse KL divergence (i.e. min KL[p_model || p_data]); to handle latent variables, an upper-bound is derived, which is the corresponding reverse KL divergence in the joint space. To minimize that joint reverse KL, a specific procedure is developed, leading to the presented EIM. EIM variants for different applications are discussed. Fancy robot-related experiments are used to evaluate the presented algorithm.\n\nOverall, the paper is in good shape wrt the logic and the writing. My main concerns focus on the novelty (compared to existing methods that are similar but not discussed) and the experiments. For the former, reverse KL has been exploited before, both in the marginal space [1] and the joint one [2]. Other detailed comments are listed below.\n\nAs Eq 4 is for matching two joint distributions, discussions/comparisons should be made to reveal the novelty of the presented EIM over existing methods such as [2], etc.\n\nIn Figure 2 (b), the experimental settings for adversarial learning are not fair, as the discriminator is not fixed there. \n \nIn Sec 4.4, it seems EIM is highly overlapped with VIPS. So what're the advantages of EIM here?\n\nIn Figure 3, how many steps for Generator and Discriminator are used for f-GAN? Does f-GAN finally converge? It would be helpful if some results are given to demonstrate the final state of each method.\n\nIn Eq. 9, adding the denominator q(z_i) will change the optimal solution. Why only add it to the first term?\n\nIn Section 5.3 and Figure 5, \u201cSSD\u201d might be a typo. \n\n[1] Adversarial Learning of a Sampler Based on an Unnormalized Distribution. AISTATS 2018.\n[2] Symmetric Variational Autoencoder and Connections to Adversarial Learning. AISTATS 2019.\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 3}, "rkxrfrIfcB": {"type": "review", "replyto": "ByglLlHFDS", "review": "This paper propose EIM an analog to EM but to perform the I-projection (i.e. reverse-KL) instead of the usual M-projection for EM. The motivation is that the reverse-KL is mode-seeking in contrast to the forward-KL which is mode-covering. The authors argue that in the case that the model is mis-specified, I-projection is sometimes desired as to avoid putting mass on very unlikely regions of the space under the target p.\n\nThe authors propose an iterative procedure that alternates between estimating likelihood ratios and proposal distribution by minimizing an upper bound on the reverse-KL. The derivations seem correct. There are some experiments, majoritarily in the robotics domain. As the author point out, likelihood shouldn't be the right metric since you are now minimizing the reverse-KL---I would have liked the authors to spend some more time on the right way to evaluate---and actually use that new metric. Finally, there has been plethora of work on different objectives and distance between distributions as well as a zoo of lower/upper bounds on how to evaluate them---it would be interesting to have more connections to prior work.\n\n[Pros]\n- clearly written\n- clear motivation\n- correct derivations\n- interesting algorithm\n\n[Cons]\n- experiments are a little weak (and focus on a single domain)\n- would have liked to see an explicit algorithm for the optimization procedure\n- small lack of clarity in the presentation of Section 4.1---notation q_t is not introduced for example\n- more discussion about the evaluation metric\n- linking it more to prior work\n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 3}, "SyxDIDyp5S": {"type": "review", "replyto": "ByglLlHFDS", "review": "In this paper, the authors proposed a new algorithm -- expected information maximization (EIM) -- for computing the I-projection of the data distribution to the model distribution, solely based on samples for general latent variable models, where the paper only focus on Gaussian mixtures models and experts. The proposed method applies a variational upper bound to the I-projection objective which is decomposable for each mixture components and the coefficients. Overall, I think the proposed technique quite sound and results are convincing. However, I do have some questions:\n\nQuestions:\n-- The proposed EIM algorithm in Sec 4.1 seems to require \u201cre-training\u201d the discriminator every time the q function is updated. How this can be applicable to more realistic and complex models where training requires millions of gradient steps? Will the same algorithm can be applied on more general latent variable models or even implicit models like GAN does? As the paper has pointed out, the vanilla f-GAN itself can be seen as optimizing some forms of the I-Projection (reverse Kullback-Leibler divergence) objective.\n-- I am a little confused about Sec 4.3. It seems that the latent variable z is not necessary for the proposed EIM? \n-- Also, the typical practise of training GAN is also iterative between the generator and the discriminator, we sometimes need to update the discriminator with more steps than the generator? Shouldn\u2019t it be the exactly same as the proposed EIM except we have an additional regularization term of KL(q(x) || q_t(x)) which might be the true reason why training gets more stable than standard GAN?\n-- Similar to the previous two questions, how to compute the regularization term KL(q(x) || q_t(x)) in EIM for normal generator which is typically implicit?\n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 1}}}