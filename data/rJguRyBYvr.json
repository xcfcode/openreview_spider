{"paper": {"title": "Improved Detection of Adversarial Attacks via Penetration Distortion Maximization", "authors": ["Shai Rozenberg", "Gal Elidan", "Ran El-Yaniv"], "authorids": ["shairoz@cs.technion.ac.il", "elidan@google.com", "elyaniv@google.com"], "summary": "Adversarial detection method based on separating class clusters in the embedding space. ", "abstract": "This paper is concerned with the defense of deep models against adversarial at-\ntacks. We develop an adversarial detection method, which is inspired by the cer-\ntificate defense approach, and captures the idea of separating class clusters in the\nembedding space so as to increase the margin. The resulting defense is intuitive,\neffective, scalable and can be integrated into any given neural classification model.\nOur method demonstrates state-of-the-art detection performance under all threat\nmodels.", "keywords": ["Adversarial Examples", "Adversarial Attacks", "Adversarial Defense", "White-Box threat models"]}, "meta": {"decision": "Reject", "comment": "A defense against of adversarial attacks is presented, which builds mostly on combining known methods in a novel way. While the novelty is somewhat limited, this would be fine if the results were unequivocally good and other parts of the problematic. However, reviewers were not entirely convinced by the results, and had a number of minor complaints with various parts of the paper.\n\nIn sum, this paper is not currently at a stage where it can be accepted."}, "review": {"S1ez9A-QKH": {"type": "review", "replyto": "rJguRyBYvr", "review": "After rebuttal: my rating remains the same.\nI have read other reviewers' comments and the response. Overall, the contribution of retraining and detection with previously explored kernel density is limited. \n\n=================\nSummary: \nThis paper proposes new regularization techniques to train DNNs, which after training, make the crafted adversarial examples more detectable. The general idea is to minimize the inter-class variance and maximize the intra-class distance, at some feature layer. This involves regularization terms: 1) SiameseLoss, an existing idea of contrastive learning known can increase inter-class margin; 2) reduce variance loss (RVL), a variance term on deep features, and 3) reverse cross entropy (RCE), a previously proposed term for detection purpose. The motivation behind seems intuitive and the empirical results demonstrate moderate improve in detection AUC, compared to one existing technique (e.g RCE).\n\nMy concerns:\n1. The proposed technique requires retraining the networks to get a few percents of detection improvement. This is a disadvantage compared to standard detection approaches such as [1] and [2] which do not need to retain the network. I am surprised that these standard detection methods were not even mentioned at all. Retraining with fixed loss becomes problematic when the networks have to be trained using their own loss functions due to application-specific reasons. Moreover, the detection performance reported in this paper is not better than the one reported in [2] (ResNet, CIFAR-10, 95.84%) which do not need retraining.\n\n2. There are already well-known margin-based loss functions, such as triplet loss [4], center loss [5], large-Margin softmax loss [6], and many others, which are not mentioned at all.\n\n3. In terms of retraining-based detection, higher AUCs have been reported in [3] for a neural fingerprinting method.\n\n4. Incorrect references to existing works. The second sentence in Intro paragraph 2: Metzen, et al, .... these are not adversarial training. Xu, et al. (feature squeezing) is not a randomization technique.\n\n5. The \"baseline\" method reported in Table 2, is confusing. RCE is also a baseline? You mean conventional cross entropy (CE) training?\n\n6. Some of the norms are not properly defined, which can be confusing in adversarial research. For example, from Equation (1) to (4). The \"Frobenius norm used here\" statement in Equation (3), don't know this F norm comes from.\n\n\n[1] Characterizing adversarial subspaces using local intrinsic dimensionality. ICLR, 2018\n[2] A simple unified framework for detecting out-of-distribution samples and adversarial attacks. NeurIPS, 2018\n[3] Detecting Adversarial Examples via Neural Fingerprinting. arXiv preprint arXiv:1803.03870, 2018\n[4] Facenet: A unified embedding for face recognition and clustering. CVPR, 2015.\n[5] A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV, 2016.\n[6] Large-Margin Softmax Loss for Convolutional Neural Networks. ICML 2016.", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 3}, "HJlIS4s1qB": {"type": "review", "replyto": "rJguRyBYvr", "review": "Update after author response:\nI would like to thank the authors for the thoughtful response, and for addressing some of the concerns raised by the reviewers. The draft appears improved but my concerns about the novelty and interpretability of the work still stand, leading me to keep my assessment unchanged.\n---------------------------\n\nIn this paper, the authors propose a general defense method against adversarial attacks by maximizing an approximate bound on the magnitude of distortion needed to force a misclassification. The authors note that this maximization can be achieved by increasing the margin between class clusters and by reducing the norm of the Jacobian of intermediate layers. Subsequently, they either directly adopt or introduce simple modifications to existing techniques to affect these two factors, showing the robustness of the combined method to several adversarial attacks on MNIST and CIFAR-10 datasets. \n\nAs neural networks get deployed for increasingly critical applications, the issue of defense against adversarial attacks becomes progressively relevant. The paper does a good job of motivating a relatively simple approach to the problem based on an approximate bound, and pulls in from different existing methods to build a robust system. The strong points of the paper:\n1. The paper is clearly written, and the approach is sensible. \n2. Fairly thorough empirical investigation under different threat models.\n3. The proposed method performs consistently above the baselines for different experiments.\n\nHere are some of my concerns:\n1. The work is somewhat incremental and the novelty mostly lies in pulling a few different methods together that seem to work well in unison.\n2. The two methods used for increasing the margin don\u2019t actually optimize that objective directly. The Siamese Loss uses cosine distance as proxy and the variance reduction doesn\u2019t guarantee increase in margin which is sensitive to outliers. Any improvement achieved thus appears to be an ill-understood side-effect. \n3. The definition of cluster distance (page 3) looks erroneous. \n4. The authors note that the proposal doesn\u2019t work very well for a specific kind of attack (BIM) but don\u2019t have clear recommendations for improvement. The tentative explanation of why this happens is also somewhat loose. \n\nIn summary, I think the paper addresses an interesting problem even though the development is arguably incremental. However, since the unified approach is simple yet novel, and the results fairly promising, I am somewhat inclined to accept this paper.", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 1}, "HkxTPm3_jS": {"type": "rebuttal", "replyto": "S1ez9A-QKH", "comment": "Thanks for your thoughtful comments. Please consider our response.\n\n1. \"The proposed technique requires retraining.\"\n\nOur method intends to be attack-agnostic and is thus compared only to such methods.\nWe agree that it would be most interesting to remain attack-agnostic without retraining. \nThe two papers you mentioned ([1] and [2]) clearly utilize adversarial optimization (i.e., using adversarial examples for optimization) and are thus not attack-agnostic. Therefore, this is not a fair comparison. To verify that [1] and [2] utilize adversarial optimization,see section 5.2 in [1] and section B2 in supplementary material in [2]\n\n2. \"There are already well-known margin-based loss functions, such as triplet loss [4], center loss [5], large-Margin softmax loss [6], and many others, which are not mentioned at all.\"\n\nThanks for the references. We plan to experiment with some of them in future work. We will mention all these margin increasing loss functions.\n\n3. In terms of retraining-based detection, higher AUCs have been reported in [3] for a neural fingerprinting method.\n\n\nWe agree that the Neural Fingerprinting paper is very interesting and presents phenomenal results. However, the threat model of that paper is completely different than in our setting. Specifically, in their gray-box threat model, the adversary has no information about the fingerprint instances whatsoever, which amounts to several thousands secret parameters. In our gray-box threat model the adversary is solely unaware of the use of the KDE-based defense mechanism. Thus, no apples-to-apples comparison can be made here.\n\nIn the white-box threat model, we measure the *distortion* required to fool our model using the CW-wb attack, while the fingerprint paper they implemented an adaptive version of FGSM, BIM and SPSA and measured *robustness* to a given set of hyper-parameters. Here again, the comparison between the two papers isn't apples-to-apples.\n\nWe note that our threat models follow the ones defined in [1][2]\n[1]Feinman, et,al. Detecting adversarial samples from artifacts\n[2]Pang et,al. Towards robust detection of adversarial examples\n\n4.  \"Incorrect references:\nFixed\n\n5. \"RCE is also a baseline?\"\nYes. To the best of our knowledge the RCE NIPS-2018 paper still presents SOTA results for adversarial detection.\n\n6. \"Some of the norms are not properly defined\"\n\nWhile we could use any Lp norm, all our results are achieved with the L2 norm for the embedding  and the Frobenius for the Jacobian. Fixed.\n", "title": "Reply to  Reviewer #3"}, "SJxRCM3OsS": {"type": "rebuttal", "replyto": "SkxDVJmVKH", "comment": "Thanks for your thoughtful comments. Please consider our response.\n\n1.  \"relies on a lot of general intuitions and unproven claims about neural networks.\" \"hard to verify given just Figure 1.\"\n\nThere was indeed a problem with the color scheme - now fixed (in the new version). \nWe followed your advice and conducted an analysis showing that these claims directly hold on the embedding space (and not relying on t-sne). The conclusion is qualitatively the same. When considering L2 distance in the embedding space 70% of the attacks on instance x targets one of the two closest classes to x. This will be added to the next version.\n\n2. Variance reduction: \"Would  this still work for a dataset with a large number of classes (e.g., ImageNet)?\"\n\nWe have checked the variance reduction on Cifar-100 (100 classes), and found that it still works.\nSpecifically, when examining the embedding clustering quality using the Davies-Bouldin index (DBI) we get an improvement of of 25% \nThese preliminary results will be included after the rebuttal.\n\n3. Evaluation section\n\n(a) \"It would still be good to provide additional explanations (white-box model)\"\n\nWe followed the procedure for the KDE spoofing attack in which one sets the hyper-parameters such that all generated adversarial examples are able to fool the targeted model. The required such hyper-parameters are specified in Appendix C.\n\n(b) \"Optimize this objective using gradient-free attacks\"\n\nWe included the SPSA gradient-free attack on Cifar-10. The results indicate better performance using our method (significantly better in the case of resiliency). For a perturbation (epsilon) of 0.05, PDM achieved a robustness (adversary fail rate) of 0.4 compared to 0.13 achieved by an RCE trained model and 0.08 achieved by a CE trained model.\n\n(c)  \"I also suggest trying it out on rotation-translation attacks\"\n\nDone. And results are consistent. Our method is still much better. \nSpecifically, PDM achieved an AUC score of 0.931 compared to 0.914 achieved by an RCE trained model and 0.89 achieved by a CE trained model.\n\n4. \"Missing citations.\"\nThanks, added. The new version now includes them all.", "title": "Reply to  Reviewer #2"}, "HklKrf3uoH": {"type": "rebuttal", "replyto": "HJlIS4s1qB", "comment": "Thanks for your thoughtful comments. Please consider our response.\n\n1. We agree that the novelty of our method is in the combination of several techniques that work well in unison. We note that the combination itself is intuitive and, more importantly, it leads to significant and consistent improvements.\n\n2. While it would be interesting to explore other metrics,\nthe use of cosine as a proxy is common many applications. Our choice followed the need to use a differentiable and bounded metric.  \n\nIn regards to variance reduction, our intention (which will be clarified in the next version) is to consider the class-wise *average* variance, which is less sensitive to outliers. When considering the average the proposed method works very well.\n\n3. Cluster distance definition is indeed wrong - now fixed (in the new revision).\n\n4. BIM deficiency: We agree it is a deficiency. We expect it however to disappear when using a different Jacobian smoothing method. On the positive side, we have made a significant step toward identifying the source of this problem (see preliminary explanation in Section 4.2),\nwhich we plan to resolve.", "title": "Reply to Review #1 "}, "SkxDVJmVKH": {"type": "review", "replyto": "rJguRyBYvr", "review": "\nSummary\n========\nThis paper proposes a defense against adversarial examples that detects perturbed inputs using kernel density estimation. The paper uses a combination of known (and often known to be broken) techniques, and does not provide a fully convincing evaluation.\nI lean towards rejection of this paper.\n\nDetailed comments\n=================\nThe idea of increasing robustness by maximizing inter-class margins and minimizing intra-class variance is fairly natural, but the author's discussion of their approach (mainly in sections 1 and 2) is very hand-wavy and relies on a lot of general intuitions and unproven claims about neural networks.\n\nFor example, in the introduction, the authors claim:\n\n\"A trained deep classification model tends to organize instances into clusters in the embedding space, according to class labels. Classes with clusters in close proximity to one another, provide excellent opportunities for attackers to fool the model. This geometry explains the tendency of untargeted attacks to alter the label of a given image to a class adjacent in the embedding space as demonstrated in Figure 1a.\"\n\nFirst, a t-SNE representation is just a 2D projection of high-dimensional data that is useful for visualization purposes, and one should be careful when extrapolating insights about the actual data from it. For example, distances in the 2D projection do not necessarily correspond directly to distances in the embedding space. \nThe claim that untargeted attacks lead to a \"nearby\" cluster are hard to verify given just Figure 1. First, the colors of the labels between 1a and 1b do not seem to match (e.g., Dog is bright green in 1b but this color does not appear in 1a). If the other colors match, then this would seem to suggest that trucks (purple) often get altered to ships (orange). Yet, the two clusters are quite far apart in 1a. It seems hard to say something qualitative here. An actual experiment comparing distances in the embedding space and the tendency of untargeted attacks to move from one class to another would be helpful.\nThe color scheme in Figure 1b is also unclear. A color bar would help here at the very least.\n\nThese observations are then used to justify increasing cluster distance while minimizing cluster variance, but it would be nice to see a more formal argument relating these concepts to the embedding distance.\n\nThe technique proposed in Section 3.2. to reduce variance loss estimates each class' variance on each batch. Would  this still work for a dataset with a large number of classes (e.g., ImageNet)? For such a dataset, each class will be present less than once in expectation in each batch, which seems problematic.\n\nThe plots in Figure 2 don't give much of a sense of how the combination of the different proposed techniques is better than any individual technique. The evaluation compares PDM to RCE, but from Figure 2 one could guess that variance reduction alone (2c) performs very similarly to PDM (2e). An ablation study showing the contribution of each of the individual techniques would be helpful.\n\nThe evaluation section could be improved significantly. FGSM, JSMA, and to some extent BIM, are not recommended attacks for evaluating robustness. The gray-box and black-box threat model evaluations are also not the most interesting here. Instead, and following the recommendations of Carlini et al. (2019), the evaluation should:\n\n- Propose an adaptive attack objective, tailored for the proposed defense in a white-box setting. The authors do this to some extent, by re-using the attack objective from Carlini & Wagner 2017, which targets KDE. It would still be good to provide additional explanations about how the hyperparameters for this attack were set.\n- Optimize this objective using both gradient-based and gradient-free attacks\n- As the proposed defense is attack-agnostic, I also suggest trying it out on rotation-translation attacks, as the worst-case attack can always be found by brute-force search\n\nOther\n=====\n- The citations for adversarial training in the 2nd paragraph of the intro are unusual. Standard references here are for sure the first two below, and maybe some of the other three as is relevant to your work\n    - Szegedy et al. 2013: \"intriguing properties of neural networks\"\n    - Goodfellow et al. 2014: \"Explaining and harnessing adversarial examples\"\n    - Kurakin et al. 2016: \"Adversarial Machine Learning at Scale\"\n    - Madry et al. 2017: \"Towards deep learning models resistant to adversarial attacks\"\n    - Tramer et al. 2017: \"Ensemble Adversarial Training\"\n- The Taylor approximation in (1) does not seem to be well defined. The Jacobian of F is a matrix, so it isn't clear what evaluating that matrix at a point x means.\n- The \"greater yet similar\" symbol (e.g., in equation (4)) should be defined formally.", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 4}}}