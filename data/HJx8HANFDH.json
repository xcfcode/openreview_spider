{"paper": {"title": "Four Things Everyone Should Know to Improve Batch Normalization", "authors": ["Cecilia Summers", "Michael J. Dinneen"], "authorids": ["ceciliasummers07@gmail.com", "mjd@cs.auckland.ac.nz"], "summary": "Four things that improve batch normalization across all batch sizes", "abstract": "A key component of most neural network architectures is the use of normalization layers, such as Batch Normalization. Despite its common use and large utility in optimizing deep architectures, it has been challenging both to generically improve upon Batch Normalization and to understand the circumstances that lend themselves to other enhancements. In this paper, we identify four improvements to the generic form of Batch Normalization and the circumstances under which they work, yielding performance gains across all batch sizes while requiring no additional computation during training. These contributions include proposing a method for reasoning about the current example in inference normalization statistics, fixing a training vs. inference discrepancy; recognizing and validating the powerful regularization effect of Ghost Batch Normalization for small and medium batch sizes; examining the effect of weight decay regularization on the scaling and shifting parameters \u03b3 and \u03b2; and identifying a new normalization algorithm for very small batch sizes by combining the strengths of Batch and Group Normalization. We validate our results empirically on six datasets: CIFAR-100, SVHN, Caltech-256, Oxford Flowers-102, CUB-2011, and ImageNet.", "keywords": ["batch normalization"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes techniques to improve training with batch normalization. The paper establishes the benefits of these techniques experimentally using ablation studies. The reviewers found the results to be promising and of interest to the community. However, this paper is borderline due in part due to the writing (notation issues) and because it does not discuss related work enough. We encourage the authors to properly address these issues before the camera ready."}, "review": {"v2vwTHjL4D": {"type": "rebuttal", "replyto": "WsOIcy29H", "comment": "Hi Saurabh,\n\nThank you for the reference, and kudos as well on having your paper accepted to ICCV.\n\nThere indeed is similarity between the method you present in your paper and one of the four methods we study (\"Inference Example Weighing\", Section 3.1). For what it's worth, we posted an earlier version of this work on arxiv over six months ago, well before the referenced paper was published in ICCV, and had actually performed initial experiments with the method several months before that.\n\nFrom what we can tell, there are a number of differences between the method we propose and the one studied in your paper:\n-'EvalNorm' uses different weighing parameters for the mean and variance, additionally using different parameters in each normalization layer (other than for a few baselines studied). Our approach uses a single parameter for both moments and all layers.\n-'EvalNorm' optimizes its weighing parameters by encouraging similarity to the training outputs of a BN layer; we treat our single weighing parameter $\\alpha$ as a hyperparameter to be optimized for a metric of interest -- indeed, as we demonstrate in Figures 1, 2, and (especially) 4, we show that optimizing for different metrics, even if they're correlated, can result in rather different optimal values of $\\alpha$.\n-We give a proof on the effect of Inference Example Weighing on the output range of a Batch Normalization layer.\n-We examine more thoroughly how model behavior changes with the weighing parameter.\n-The experimental setups and evidence are quite different -- for example, we also study the non-i.i.d. minibatch setting and show the impact of Inference Example Weighing, while the referenced paper also looks at object detection.\n\nOverall, we view 'EvalNorm' and Inference Example Weighing from our work as taking two complementary approaches towards addressing the same limitation of Batch Normalization, where both were clearly done in parallel, a relatively common occurrence when studying important problems.\n\nWe'd be happy to cite your work, and should you find yourself updating the arxiv version of your paper, would appreciate if you do the same.", "title": "Response"}, "eghGazEpgr": {"type": "rebuttal", "replyto": "r1glmJ2ptH", "comment": "(We wanted to add this response earlier but it was outside the paper rebuttal period)\n\nThank you for the clarification -- yes, in addition to the theory, this is also empirically true, and we can provide multiple concrete examples of networks exceeding our bound in Eq. 2 during testing, but respecting it during training, where the bound applies. We have measured this on seven networks, and all seven exhibited a wider output range during testing than during training. We will add these experiments to the Appendix.", "title": "Response to Clarification (3)"}, "r1glmJ2ptH": {"type": "review", "replyto": "HJx8HANFDH", "review": "The paper performs an empirical study of four batch-normalization improvements and proposes a new normalization technique for small batch sizes, based on group and batch normalizations. Among others, the authors address the inconsistency between the train and the test stages and the problem of small batch sizes. The authors conducted an empirical ablation study of the four techniques and proposed an intuition when each method should be used.\n\nConcerns:\n(1) The comparison with baselines in section 4.2 seems to be unclear. Fig.5 shows the performance of normalization methods for different batch sizes. Batch Normalization, however, has the same performance for all batch sizes. The authors refer to this baseline as \u201cidealized Batch Normalization\u201d. Additional elaboration on what does this means is required. \n(2) It would also be beneficial to see the comparison with the original Ghost Batch Normalization in the final evaluation (section 4.2), since this method, according to section 3.2, was capable of the significant improvement for Caltech-256 dataset.\n(3) In section 3.1, the authors provide an intuition of why can the discrepancy between test and train phases hurts the performance of a model. The empirical evaluation of this effect is needed to justify this intuition.\n\nOverall, the newly proposed method is a minor update, and novelty is limited. However, the thorough empirical study of existing improvement techniques would be a good addition to the conference.\n\nMinor comments:\n1. share the y-axis in Fig.4 between different ghost batch sizes.\n\nI would also recommend authors to include the following papers to the related work section:\n1. Riemannian approach to batch normalization [https://arxiv.org/abs/1709.09603]\n\n----------\n\nRespond to the rebuttal. Clarification on the concern (3):\n\nI agree that, in general, a discrepancy between training and testing can hurt a model. The paper showed that the output of a batch normalization layer is theoretically unbounded during testing. However, it would be beneficial to see numerically if it indeed the case on a real test set (the output range is wider than the one during training).\n\n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 4}, "SygALR6ior": {"type": "rebuttal", "replyto": "r1lmLcNaYB", "comment": "(Split into two responses due to character limit)\n\nThank you for your review! We appreciate that you valued the contributions of the paper (positive points 1-3).\n\nWe aim to address your remaining concerns here:\n\nNovelty: We believe that our accumulation of insights is a significant contribution and of interest to the research community. Batch Normalization is fundamental to the success of much of deep learning. We presented multiple (four) improvements to it, validated our results extensively on six datasets in multiple training scenarios (training from scratch, transfer learning, and non-i.i.d. training in our updated version), and even presented intriguing negative results (see Appendix).\n\nExperiments: In the submitted version, we validated our methods on 5 datasets (with ImageNet the only dataset that didn't receive the whole gamut of experiments due to computational constraints). The other reviewers explicitly highlighted the experiments (\"thorough empirical study\", \"support from experiments\", \"experimental evidence seems sufficient\"). In our revision, we have added a 6th dataset, adding further evidence to the transfer learning setting, and also added the suggested experiments in the non-i.i.d. setting.\n\n1. We wrote Eq. 2 as minimizing over only $x_1, \\ldots, x_{B-1}$ because the minimum is actually independent of $x_0$ -- in other words, no matter what value $x_0$ has, we can pick $x_1, \\ldots, x_{B-1}$ large enough that the bound is achieved (see proof in Appendix). Nonetheless, we appreciate that this might not be straightforward to see, and have revised the minimization to include $x_0$, with added explanation in the Appendix proof.\n\n2. While both Inference Example Weighing and Batch Renormalization involve interactions with the moving average statistics, they're actually quite different:\n-Batch Renormalization applies a scaling and offset to make the offset and scaling used during training more similar to the moving average; Inference Example Weighing uses a weighted average of the example (not batch) statistics and moving averages during inference.\n-Batch Renormalization requires re-training a network; Inference Example Weighing doesn't.\n-Batch Renormalization introduces 2 hyperparameters, $r_{max}$ and $d_{max}$, and up to 5 in its full form when using a warmup as suggested in their paper (number of steps to use vanilla Batch Normalization, number of warmup steps for $r_{max}$, and a distinct number of warmup steps for $d_{max}$); Inference Example Weighing has a single hyperparameter that can be easily tuned, even on networks that have already been trained.\n-At its core, Inference Example Weighing applies during inference and doesn't affect training; Batch Renormalization applies during training and doesn't affect inference (other than indirect effects on weights).\n\n3a. The choice of model was described in Sec. 4.1 (with a forward pointer in the first paragraph of Sec. 3): On CIFAR-100 and SVHN we used a ResNet-18, and on Caltech-256 we used Inception-v3. \n3b. While we admire the work and interesting results of those who have the resources to do so, given our computational budget it was prohibitive to do many experiments on ImageNet (besides those in Sec. 3.1) -- a single model training on ImageNet takes about as long as all of the experiments put together on another dataset. Given that hyperparameters on ImageNet might also need to be tuned, we thought it more important to focus on fully exploring experiments on other datasets.\n\nAlso, to demonstrate the real-world applicability of our methods, we performed transfer learning experiments, which are directly applicable to most real-world scenarios; they employ transfer from a large, labeled dataset (ImageNet) in order to solve problems when less data is available, as is typically the scenario for all use-cases where labeled data isn't plentiful. We have also strengthened our transfer learning experiments in the revision by adding an additional transfer learning dataset.\n\n4. Interestingly, while He et al. discuss removing regularization on \\gamma and \\beta, which they combine with any other biases or variables in the network, in their experiments (Table 4 of their paper) removing this weight decay actually led to a degradation in performance (note that each row of the table stacks the previous rows' changes together). Their experiments are consistent with our results.\n\n5. (See 3b, as the question is very similar.)\n\n6. Although this wasn't the primary goal of our paper, this is an interesting experiment, which we have performed and now added to our paper. In short, Inference Example Weighing and Ghost Batch Normalization both help tremendously in the non-i.i.d. setting (up to ~30% reduction in error) by reducing the dependence on the distribution within each batch of data. This shows that our techniques are beneficial in both i.i.d. and non-i.i.d. settings. Thank you for your suggestion!", "title": "Thank you for your review (part 1)"}, "Bke7e0Tsor": {"type": "rebuttal", "replyto": "r1lmLcNaYB", "comment": "(continued from part 1)\n\n7. Thank you for the references, we have added brief descriptions of each to our section detailing related work.\n\nMinor issues:\nWith all due respect, only one of these is actually a typo (missing the word 'and' for 2.2):\n\n1. The contributions are written as a list (note that each ends in a comma). Since contribution 4 is the last item in the list, contribution 3 ends with \"and\".\n2.1. This is correct English grammar.\n2.2. Thank you! This was indeed a typo.\n2.3. This is correct English grammar.\n\nThank you for the review!\nBest,\nPaper 1113 Authors", "title": "Thank you for your review (part 2)"}, "B1l2ospjoB": {"type": "rebuttal", "replyto": "B1lHRzu6YS", "comment": "Thank you for your review! We put care into making sure the concepts in the paper were easy to follow, and appreciate that it came through effectively.\n\nTheory: While we do have some theory in our paper (e.g. the proof in Appendix A), we have replaced the instances of this word with \"hypothesis\", which is more clear for the other parts of the paper. Thank you for your suggestion!\n\nNovelty (same comment as Reviewer 3): We appreciate and agree with the reviewer's opinion that improving Batch Normalization, such a fundamental component of modern neural networks, is of significant value to the community. We presented multiple (four) improvements to Batch Normalization, validated our results extensively, and in our latest revision we have added experiments on another transfer learning dataset, bringing the number of evaluation datasets up to 6, and added an additional experiment for training on non-i.i.d. distribution.\n\nThank you for the review!\nBest,\nPaper 1113 Authors", "title": "Thank you for your review"}, "r1xcvsToiH": {"type": "rebuttal", "replyto": "r1glmJ2ptH", "comment": "Thank you for the thoughtful review! We appreciate your thorough reading of the paper, and here are responses to your listed concerns:\n\n(1) This was meant to provide an easy comparison to Batch Normalization -- in the ideal case, Batch Normalization would scale perfectly across batch sizes, not changing in performance as the overall batch size changed. Plotting the performance of Batch Normalization this way lets us compare our approach against that ideal.\n\n(2) Thank you for the suggestion, we have now added standalone Ghost Batch Normalization to the analysis in Figure 5 of the updated paper.\n\n(3) We're a little unsure of the precise meaning of what you're asking, and would appreciate any clarification you could give. Generally speaking, introducing most types of discrepancies between training and testing should hurt a model, since models perform best when they are trained to directly optimize the task they are evaluated on. While some exceptions exist (e.g. data augmentation), training on a task or distribution slightly different from the task being evaluated on, or training in a slightly different way than the evaluation settings, should hurt performance. Resolving discrepancies introduced by Batch Normalization has been a common theme in prior work (see references in Sec. 3.1)\n\nNovelty (same comment as Reviewer 2): We appreciate and agree with the reviewer's opinion that improving Batch Normalization, such a fundamental component of modern neural networks, is of significant value to the community. We presented multiple (four) improvements to Batch Normalization, validated our results extensively, and in our latest revision we have added experiments on another transfer learning dataset, bringing the number of evaluation datasets up to 6, and added an additional experiment for training on non-i.i.d. distribution.\n\nFig. 4 y-axis: Thank you for your suggestion! We've looked into this, but found that keeping the y-axis constant across different plots makes many of the plots hard to read -- for example, for Caltech-256 the full y-axis for accuracy would be [30, 60], but Ghost Batch Sizes 4 through 16 only have values in the range [53, 60], which squeezes all of the accuracy curves down to ~25% of their original size.\n\nReference: Thank you for the reference! We have added it in our related work.\n\nThank you for the review!\nBest,\nPaper 1113 Authors", "title": "Thank you for your review"}, "H1lhSYpijB": {"type": "rebuttal", "replyto": "H1gqe_rmjr", "comment": "Yes, always happy to cite the latest and greatest research (this paper was submitted to arxiv after the ICLR submission deadline).\nThanks for the reference!", "title": "Thank you for the reference"}, "r1lmLcNaYB": {"type": "review", "replyto": "HJx8HANFDH", "review": "The authors discuss four techniques to improve Batch Normalization, including inference example weighing, medium batch size, weight decay, the combination of batch and group normalization. Equipped with the proposed techniques, the authors obtain promising results when training deep models with various batch sizes. However, the novelty of this paper seems very limited and more experiments are required.\n\nPlease see my detailed comments below.\n\nPositive points\uff1a\n1. The proposed inference example weighing method yields promising results and does not require any re-training.\n2. The combination of batch and group normalization makes it possible to train deep models with very small batch size.\n3. By combining all the techniques, the proposed method yields promising performance when training deep models with different batch sizes.\n\nNegative points:\n\n1. Some notations are very confusing. For example, the authors use B to represent the size of a minibatch. However, why do the authors only consider B-1 samples in Eq. (2), i.e., selecting the minimum possible output among x_1, \u2026, x_{B-1}?\n\n2. The proposed inference example weighing method seems very similar to Batch Renormalization. Both methods seek to use a linear function to combine the batch statistics and the moving statistics. What is the essential difference between these methods?\n\n3. What model do the authors use in the experiment of Figure 2? Why do the authors conduct experiments on different datasets in Section 3.1 (including ImageNet) and Section 3.2 (excluding ImageNet)? It would be stronger to provide ImageNet results in Section 3.2.\n\n4. The authors draw different conclusions about the usage of weight decay from a recent work (He et al, CVPR2019). The CVPR paper reports that training \\gamma and \\beta without weight decay on ResNet-50 yields significant performance improvement. However, this paper shows that training ResNet-50 with weight decay improves the performance. Please comment on the differences in the conclusions.\n\nReference: \"Bag of tricks for image classification with convolutional neural networks.\" CVPR, 2019.\n\n5. The authors only report ImageNet results of the proposed inference example weighing method. However, all the experiments in Section 4 are performed on three small datasets. It is necessary and important to provide ImageNet results to show the effectiveness of the other three techniques in Section 4. \n\n6. Note that training deep models with non-i.i.d. minibatches is a typical case to evaluate normalization methods, e.g., Batch Renormalization. Specifically, examples in a minibatch are not sampled independently. What would happen if the authors apply the proposed techniques to the non-i.i.d. case?\n\n7. Some closely related work should be discussed in the paper, such as\n\n[1] \"Decorrelated Batch Normalization.\" CVPR, 2018.\n[2] \"Double Forward Propagation for Memorized Batch Normalization.\" AAAI, 2018.\n[3] \"Differentiable Dynamic Normalization for Learning Deep Representation.\" ICML, 2019.\n[4] \"Iterative Normalization: Beyond Standardization towards Efficient Whitening.\" CVPR, 2019.\n\nMinor issues:\n1. In Section 1, the third contribution is not a complete sentence.\n\n2. There are many typos in the paper.\n(1) In Section 2, \u201cLayer Normalization, which has found use in many natural language processing tasks.\u201d Should \u201cwhich has found use\u201d be \u201cwhich has been used\u201d?\n(2) In Section 3.1, \u201cBatch Normalization has a disparity in function between training inference\u201d. \u201cbetween training inference\u201d should be \u201cbetween training and inference\u201d.\n(3) In Section 3.1, \u201cwe need only figure out \u2026\u201d should be \u201cwe only need to figure out \u2026\u201d\n\n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}, "B1lHRzu6YS": {"type": "review", "replyto": "HJx8HANFDH", "review": "The paper introduces four techniques to improve the deep network model through modifying Batch Normalization (BN). The inspirations are from the gaps between train&test and between batches in multi-gpu training, comparison to other normalization methods, and weight decay in regularizing convolution weights training. The paper studies each techniques with the support from experiments. The paper is easy to follow. The techniques seem effective.\n\nThe paper mentions \"theory\" multiple times, but lacks sufficient justification to support these \"theories\". So one suggestion is to replace \"theory\" with a soft word.\n\nExperimental evidence seems sufficient and there are some theoretical derivations, but it looks incremental that the paper presents some techniques in improving Batch Normalization only. In general, the paper is of values to the community.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 1}}}