{"paper": {"title": "DeepCoder: Learning to Write Programs", "authors": ["Matej Balog", "Alexander L. Gaunt", "Marc Brockschmidt", "Sebastian Nowozin", "Daniel Tarlow"], "authorids": ["matej.balog@gmail.com", "t-algaun@microsoft.com", "mabrocks@microsoft.com", "Sebastian.Nowozin@microsoft.com", "dtarlow@microsoft.com"], "summary": "", "abstract": "We develop a first line of attack for solving programming competition-style problems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network's predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites.", "keywords": ["Deep learning", "Supervised Learning", "Applications", "Structured prediction"]}, "meta": {"decision": "Accept (Poster)", "comment": "This is a well written paper that attempts to craft a practical program synthesis approach by training a neural net to predict code attributes and exploit these predicted attributes to efficiently search through DSL constructs (using methods developed in programming languages community). The method is sensible and appears to give consistent speedups over baselines, though its viability for longer programs remains to be seen. There is potential to improve the paper. One of the reviewers would have liked more analysis on what type of programs are difficult and how often the method fails, and how performance depends on training set size etc. The authors should improve the paper based on reviewer comments."}, "review": {"H1BuFh1vl": {"type": "rebuttal", "replyto": "HJor8rbNg", "comment": "Thank you for your review!\n \nIn our general response posted on 29 December we have commented on your first and last questions regarding the length of considered programs (our programs are solving much more complex problems compared to prior work) and the use of 5 I/O examples only (we agree a stronger specification will be needed, and it can be naturally incorporated into DeepCoder).\n \nWe agree on the importance of finding a \"best\" program. To solve this problem, we would need to develop a ranking function or -- our favoured solution -- a data generation process that generates training programs according to an intelligent prior distribution over programs. After training the neural net on programs generated from this prior, there will be a natural bias towards searching over program space in best-first order. In this case, the method presented in the paper can be used un-modified. Because of this, we felt it was reasonable to leave the ranking problem for future work.\n \nIn the second experiment we considered showing the speedups for solving a given proportion of test problems (e.g. the chosen 20%) more informative than showing the speedup for solving all problems. The reason for this is that the latter metric would only compare the running times of the most difficult problem for the baseline and for DeepCoder respectively, ignoring the speedups achieved on all other (perhaps more typical) problems.\n \nIn the 3rd revision we have added an analysis of failure modes in a new Appendix G. We have looked at a generalisation of confusion matrices to the multilabel classification setting: for each attribute in a ground truth program (rows) measure how often each other attribute (columns) is chosen as a false positive. We then re-ordered the confusion matrix to try to expose block structure in the false positives. This reveals groups of instructions that tend to be difficult to distinguish. While the results are somewhat noisy, we observe a few general things:\n- There is increased confusion amongst instructions that select out a single element from an array: HEAD, LAST, ACCESS, MINIMUM, MAXIMUM.\n- FILTER and ZIPWITH are predicted more often regardless of the ground truth and most of the lambdas associated with them are also predicted often.\n- There are some groups of lambdas that are more difficult for the network to distinguish within: + vs -; +1 vs -1; /2 vs /3 vs /4.\n- When a program uses **2, the network often thinks it's using *, presumably because both can lead to large values in the output.\nThe NNet is never making hard decisions, so there isn't a clear concept of it being wrong. It can only rank the needed operations far down in its ordering, which causes the search to take longer. However, in the Sort and add enumeration experiment reported in Section 5.1, it only happened in 3 instances that the neural network slowed down the search: the respective slowdowns were by factors of 3.1x, 1.5x and 1.3x.\n \nRegarding your question on pooling information from individual I/O examples, we have chosen arithmetic averaging as it is one of the simplest operations that combines information from all the examples and is independent of their order. Other pooling operations could be tried, and whether they are better would be an empirical question. We thought that having each I/O example predict the output independently and then combine the predicted attributes would be inferior, because then there is no opportunity for the model to integrate different pieces of evidence from different I/O examples in a non-trivial way.", "title": "Thanks!"}, "HyMXsZ58g": {"type": "rebuttal", "replyto": "ByldLrqlx", "comment": "We have now uploaded two revisions of our submission, taking the discussion and reviewer feedback here into account. Most notably, we have extended the experimental evaluation to also consider the Lambda2 program synthesis system as back-end solver, and furthermore extended the considered test set of programs to synthesize. We also updated the text in several places to include the clarifications we provided in the comments here, and to cite more related work.\n\nFinally, we are working towards another revision that will include a analysis of failure cases and hope to publish it before the end of the week.", "title": "Summary of changes in 1st and 2nd revision"}, "ryE8XE6Bx": {"type": "rebuttal", "replyto": "B1s82JXVe", "comment": "> 1. The employed test set of 100 programs seems rather small.\n> in addition the authors ensure that the test set programs are semantically disjoint\n> from the training set programs. Could the authors provide additional details about\n> the small size of the test set and how to the disjoint property is enforced?\n\nTo enforce disjointness, we computed the outputs of all programs on a small set of example inputs, and only chose programs that showed output different from all other programs considered before. Thus, every pair of programs has at least one input on which they differ. We did this as a preprocessing step for all programs, before randomly splitting off test and validation sets.\n\nFor the experiment, we subsampled our test set down to only 100 programs. We have now sampled a second set of 500 programs of length 3 and run experiments for the faster/more promising methods on this set, using the trained models we used in the paper (i.e., these results are comparable to Table. 1 from the paper, but on a larger set of examples). The results are as follows:\n\nBack-end solver \"DFS\":\n  Time to solve 20%:\n    Baseline:   0.04078s \n    DeepCoder:  0.00268s \n    Speedup: 15.2x\n\n  Time to solve 40%:\n    Baseline:   0.12561s\n    DeepCoder:  0.03261s \n    Speedup: 3.9x\n\n  Time to solve 60%:\n    Baseline:   0.31413s \n    DeepCoder:  0.11019s \n    Speedup: 2.9x\n\n\nBack-end solver \"Enumeration\":\n  Time to solve 20%:\n    Baseline:   0.08002s\n    DeepCoder:  0.00129s\n    Speedup: 62.2x\n\n  Time to solve 40%:\n    Baseline:   0.33539s\n    DeepCoder:  0.00615s\n    Speedup: 54.6x\n\n  Time to solve 60%:\n    Baseline:   0.86050s \n    DeepCoder:  0.02728s \n    Speedup: 31.5x\n\nBack-end solver \\lambda^2:\n  Time to solve 20%:\n    Baseline:   18.8710s\n    DeepCoder:  0.23483s\n    Speedup: 80.4x\n\n  Time to solve 40%:\n    Baseline:   49.6019s \n    DeepCoder:  0.52448s\n    Speedup: 94.6x\n\n  Time to solve 60%:\n    Baseline:   84.2487s \n    DeepCoder:  13.5468s\n    Speedup: 6.2x\n\n\nOur plan is to complete these experiments for the remaining methods and longer programs before uploading a new revision.\n", "title": "Test set choice / size"}, "BkjN4T-Sx": {"type": "rebuttal", "replyto": "ByldLrqlx", "comment": "Thank you to all the reviewers for their time and comments. To summarize, the reviews say this is an interesting and well-written paper. The biggest issue raised in the reviews is that the scale of programs is said to be small (we disagree), and there are questions about how the approach will scale to more complicated programs (this is a big open problem, which--as discussed in Sec 7--we agree we don't solve, but we believe that DeepCoder lays a solid foundation upon which to scale up from.)\n\nWe would like to respond to these issues in a bit more detail:\n\n1. DeepCoder is solving problems that are significantly more complex than those tackled in machine learning research (e.g., as can be found in [1-6]), and it can significantly speed up strong methods from the programming languages community. DeepCoder scales up better than any method we are aware of for solving simple programming competition style problems from I/O examples.\n\n2. Inducing large programs from weak supervision (in this case input-output examples) is a major open problem in computer science. We don't claim to solve this problem, and it would be a major breakthrough if we did. There clearly need to be many significant research contributions to get there. DeepCoder develops two ideas that are generally useful: 1. learning bottom-up cues to guide search over program space, and 2. building a system that sits on top of and improves the strongest search-based techniques. The benefit is that as new search techniques are developed, the DeepCoder framework can be added on top, and it will likely provide significant additional improvements (we have now shown large improvements over 3 different search techniques: 2 from the original paper plus the \\lambda^2 system requested in reviewer comments).\n\n3. There is a question of whether there is enough information in input-output examples to induce large programs, and we agree that there is probably not enough information using strictly the formulation in this paper (see Discussion). However, it is straightforward to extend DeepCoder: First, by using natural language as an additional input to the encoder, we can learn to extract more information about what the program is expected to do, and how it is expected to do it. Second, generating data from a trained generative model of source code will cause the system to favor generating programs that are likely under the trained model, which will help with the ranking problem. Third, the definition of attributes can be expanded to be richer (e.g., ranking the different instructions per position in the program, or combinations of instructions), to enable the neural network component to solve more of the problem (and thus rely less on the search). Together, we believe these directions represent a clear path towards scaling up program synthesis even further by leveraging the key ideas in DeepCoder.\n\n\n[1] Graves, A., Wayne, G. and Danihelka, I., 2014. Neural turing machines. arXiv preprint arXiv:1410.5401.\n\n[2] Zaremba, W., Mikolov, T., Joulin, A. and Fergus, R., 2015. Learning simple algorithms from examples. arXiv preprint arXiv:1511.07275.\n\n[3] Andrychowicz, M. and Kurach, K., 2016. Learning Efficient Algorithms with Hierarchical Attentive Memory. arXiv preprint arXiv:1602.03218.\n\n[4] Riedel, S., Bo\u0161njak, M. and Rockt\u00e4schel, T., 2016. Programming with a Differentiable Forth Interpreter. arXiv preprint arXiv:1605.06640.\n\n[5] Gong, Q, Tian, Y, Zitnick, C. L., Unsupervised Program Induction with Hierarchical Generative Convolutional Neural Networks. ICLR 2017 Submission.\n\n[6] Kurach, K., Andrychowicz, M. and Sutskever, I., 2015. Neural random-access machines. arXiv preprint arXiv:1511.06392.", "title": "General response"}, "HJVJ6KS4g": {"type": "rebuttal", "replyto": "rJfxhn0Qg", "comment": "I have now finished a first round of experiments with lambda^2 as search backend, for programs of length 3. The following data summarises the results, and will be added to Table. 1 from the paper in the next revision.\n\nTime to solve 10%:\n  Baseline (\\lambda^2):  15.2671s \n  DeepCoder (\\lambda^2):  0.19674s \n  Speedup: 77.6x\n\nTime to solve 20%:\n  Baseline (\\lambda^2):  18.8706s\n  DeepCoder (\\lambda^2):  0.26149s\n  Speedup: 72.2x\n\nTime to solve 40%:\n  Baseline (\\lambda^2):  36.3594s\n  DeepCoder (\\lambda^2):  0.58823s\n  Speedup: 61.8x\n\nTime to solve 60%:\n  Baseline (\\lambda^2):  67.3201s\n  DeepCoder (\\lambda^2): 13.8663s\n  Speedup: 4.9x\n\nTime to solve 80%:\n  Baseline (\\lambda^2):  86.7913s\n  DeepCoder (\\lambda^2): 16.3012s\n  Speedup: 5.3x\n\nWe are investigating the fall-off in speed-up at the 60% boundary, and are furthermore running experiments for length 5 (to extend the table in Fig. 3(a)).\n\nNote the total runtimes here as well, which indicate that \\lambda^2 is several orders of magnitude slower than our specialised enumeration strategy. For example, \\lambda^2 using the prior needs 18.8s (resp. 67.3s) to solve 20% (resp. 60%) of the examples, whereas our depth-first search needs 0.028s (resp. 0.24s).\n", "title": "Lambda2 results"}, "rJut_AmNe": {"type": "rebuttal", "replyto": "H1B98EQEe", "comment": "The numbers in the paper are correct for our evaluation with the Sketch synthesis system as search backend.\n\nAs requested, I have also started to build a similar evaluation (following the strategy described for the Sketch system) for the \\lambda^2 synthesis system as search backend. These results are not presented in the paper yet, and in the comment above I gave a very rough intuition of how the raw data looks like: using our strategy with \\lambda^2 seems to shows the same trend as reported for the other search backends in the paper, but the quantative details will of course differ.", "title": "Re: comparisons"}, "H1B98EQEe": {"type": "rebuttal", "replyto": "Bk0A6E-Vx", "comment": "For the experiments, I assume you are taking the Lambda2 implementation, inserting your strategy in that framework and seeing if you can get an end-to-end speed-ups? \n\nI am not sure what is meant by \"by between one and two orders of magnitude\"? Table 1 and 2 seem to be showing 2x speed-ups at 60%: 2x is not 2-order of magnitude. \n\nI am also not sure what exactly is the Lambda2 modeled on the Sketch experiments? \n\n", "title": "comparisons"}, "HJXewHZ4g": {"type": "review", "replyto": "ByldLrqlx", "review": "N/AThis paper presents an approach to learn to generate programs. Instead of directly trying to generate the program, the authors propose to train a neural net to estimate a fix set of attributes, which then condition a search procedure. This is an interesting approach, which make sense, as building a generative model of programs is a very complex task.\n\nFaster computation times are shown in the experimental section with respect to baselines including DFS, Enumeration, etc. in a setup with very small programs of length up to 5 instructions have to be found. \nIt is not clear to me how the proposed approach scales to larger programs, where perhaps many attributes will be on. Is there still an advantage?\n\nThe authors use as metric the time to find a single program, whose execution will result in the set of 5 input-output pairs given as input. However, as mentioned in the paper, one is not after a generic program but after the best program, or a rank list of all programs (or top-k programs) that result in a correct execution.\nCould the authors show experiments in this setting? would still be useful to have the proposed approach? what would the challenges be in this more realistic scenario?\n\nIn the second experiment the authors show results where the length of the program at training time is different than the length at test time. However, the results are shown when only 20% of the programs are finished. Could you show results for finding all programs? \n\nThe paper is missing an analysis of the results. What type of programs are difficult? how often is the NNet wrong? how does this affect speed? what are the failure modes of the proposed method?\n\nThe authors proposed to have a fix-length representation of the each input-output pair, and then use average pooling to get the final representation. However, why would average pooling make sense here? would it make more sense to combine the predictions at the decoder, not the encoder?\n\nLearning from only 5 executions seems very difficult to me. For programs so small it might be ok, but going to more difficult and longer programs this setting does not seem reasonable. \n\nIn summary an interesting paper. This paper tackles a problem that is outside my area of expertise so I might have miss something important. \n", "title": "N/A", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "HJor8rbNg": {"type": "review", "replyto": "ByldLrqlx", "review": "N/AThis paper presents an approach to learn to generate programs. Instead of directly trying to generate the program, the authors propose to train a neural net to estimate a fix set of attributes, which then condition a search procedure. This is an interesting approach, which make sense, as building a generative model of programs is a very complex task.\n\nFaster computation times are shown in the experimental section with respect to baselines including DFS, Enumeration, etc. in a setup with very small programs of length up to 5 instructions have to be found. \nIt is not clear to me how the proposed approach scales to larger programs, where perhaps many attributes will be on. Is there still an advantage?\n\nThe authors use as metric the time to find a single program, whose execution will result in the set of 5 input-output pairs given as input. However, as mentioned in the paper, one is not after a generic program but after the best program, or a rank list of all programs (or top-k programs) that result in a correct execution.\nCould the authors show experiments in this setting? would still be useful to have the proposed approach? what would the challenges be in this more realistic scenario?\n\nIn the second experiment the authors show results where the length of the program at training time is different than the length at test time. However, the results are shown when only 20% of the programs are finished. Could you show results for finding all programs? \n\nThe paper is missing an analysis of the results. What type of programs are difficult? how often is the NNet wrong? how does this affect speed? what are the failure modes of the proposed method?\n\nThe authors proposed to have a fix-length representation of the each input-output pair, and then use average pooling to get the final representation. However, why would average pooling make sense here? would it make more sense to combine the predictions at the decoder, not the encoder?\n\nLearning from only 5 executions seems very difficult to me. For programs so small it might be ok, but going to more difficult and longer programs this setting does not seem reasonable. \n\nIn summary an interesting paper. This paper tackles a problem that is outside my area of expertise so I might have miss something important. \n", "title": "N/A", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "Bk0A6E-Vx": {"type": "rebuttal", "replyto": "rJfxhn0Qg", "comment": "We have now set up an evaluation based on Lambda2, modeled on the Sketch experiments (that is, we use the \"Sort and add\" scheme from the paper). While we are still working on doing a full run of experiments, preliminary results show the same trend as the other experiments, i.e., that Lambda2 returns results significantly quicker (by between one and two orders of magnitude) when sorting instructions to add using our neural network architecture instead of sorting them by the counting prior.\nHowever, note that these are preliminary results, obtained by me manually picking 10 random programs and comparing the results; we hope to finish a more formal and complete evaluation next week.", "title": "Lambda2 comparisons"}, "rJfxhn0Qg": {"type": "rebuttal", "replyto": "HyQKUPoQe", "comment": "Thanks for the suggestion. We have contacted the Lambda2 author (Jack Feser) to see if we can get it running in this domain.\n\nHowever, Jack has already pointed out that he is not sure that Lambda2 can offer an improvement of the implemented enumerative search. Our depth-first search already implements pruning of the program search space based on types, and efficiently re-uses program prefixes that it has already explored. Due to the structure of the synthesis problem here, Lambda2 can use few of its additional deduction-based pruning strategies, while paying a cost for trying them anyway. Thus, we disagree with the characterization of the optimized DFS as \"very weak\". ", "title": "Baseline comparisons"}, "rkq2i3C7l": {"type": "rebuttal", "replyto": "SyGPwM8me", "comment": "Thanks for your comments!\n\nRe your questions:\n1. We experimented a bit with different integer ranges. The general trend seems to be that larger integer ranges cause the neural network to yield more of a win, because there is more information in the input-output examples. We did not try longer programs, as the runtime was growing large at T=5. However, we would expect to see similar qualitative behavior as in Figure 6 (Appendix), where some problems are solved very quickly and others take much longer.\n2. Exploring partial conditioning might be a good direction for future work.\n3. All methods are exploring the same search space, and in the worst case they must explore the full space to find a solution. So it just means that there are some small number of examples where the learning does not provide any benefit.", "title": "Possible extensions of the approach"}, "HyQKUPoQe": {"type": "rebuttal", "replyto": "ByldLrqlx", "comment": "I read the paper in some depth and the idea has some potential, however, the main claim is not substantiated well. Concretely, the 3rd contribution is problematic as the baselines here are very weak and are not what is used in state of the art synthesizers. \n\nTo substantiate point 3, you need to actually compare against a relevant synthesizer. For instance, for the type of programs you use, I would not use sketch, you can select lambda2 (PLDI'15) as a baseline: take the implementation insert your heuristic in there and see what the results would be.\n", "title": "Experimental Eval?"}, "SyGPwM8me": {"type": "rebuttal", "replyto": "ByldLrqlx", "comment": "This paper incorporates the PL/ML sides really nicely, and I think it's plausible that highly hybridized approaches such as this might become more and more popular. Great work!\n\nA few questions/comments/things I was wondering:\n1. Were you able to experiment with longer programs or with larger integer ranges, or were there fundamental limitations that prevented this? If not, how did this degrade/improve results?\n2. C_hat is only computed once per input/output pair and not recomputed as the search produces intermediate programs; was there a reason to not include any partial conditioning, or would this likely not help much?\n3. It's true that the augmented searches reach 20% accuracy on the test set much faster than baseline methods, but all methods seem to converge to 100% solving of the test set at the same time -- do you have any conjectures as to why this might be true? It seems like learning slows down enough for the baseline to catchup. ", "title": "Great! "}, "S1B-tVAgx": {"type": "rebuttal", "replyto": "SJ0onOclx", "comment": "Thanks for the feedback and the suggestions! We'll incorporate them into our next revision.", "title": "Thanks!"}, "SJ0onOclx": {"type": "rebuttal", "replyto": "ByldLrqlx", "comment": "Just had a quick scan over this paper. It's very cool to see program induction models that provide interpretable programs rather than a set of weights. Looking forward to reading this in more depth.\n\nTwo relevant pieces of work you may wish to read/refer to here (full disclosure: they are from our group so this is a shameless plug):\n\n1) Program generation from text with efficient marginalisation over multiple forms of attention:\nLatent Predictor Networks for Code Generation, Ling et al, ACL 2016.\n\n2) Learning pushdown automata and other data structures, similar to the Joulin & Mikolov (NIPS 2015) paper you cite:\nLearning to transduce with unbounded memory, Grefenstette et al. (NIPS 2015)\n", "title": "Nice"}}}