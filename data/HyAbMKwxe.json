{"paper": {"title": "Tighter bounds lead to improved classifiers", "authors": ["Nicolas Le Roux"], "authorids": ["nicolas@le-roux.name"], "summary": "", "abstract": "The standard approach to supervised classification involves the minimization of a log-loss as an upper bound to the classification error. While this is a tight bound early on in the optimization, it overemphasizes the influence of incorrectly classified examples far from the decision boundary. Updating the upper bound during the optimization leads to improved classification rates while transforming the learning into a sequence of minimization problems. In addition, in the context where the classifier is part of a larger system, this modification makes it possible to link the performance of the classifier to that of the whole system, allowing the seamless introduction of external constraints.", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "This is a well written paper that proposes the adaptation of the loss function for training during optimization, based on a simple and effective tighter bound on classification error. The paper could be improved in terms of a) review of related works; b) more convincing experiments."}, "review": {"BkeUYNfAcm": {"type": "rebuttal", "replyto": "HyAbMKwxe", "comment": "Thanks for the update. Its very helpful and i have to learn even more from it. I have been in the forex , binary and   crypto space for so long trying to figure when to buy and when not to. I ran into luck when I contacted Baileyaart1199 @ gmail dot com from the comment section of a video and he gave me his guidance. It was my first time of trading cryptocurrency and forex, and I have felt confident in my decisions. I have made 10 times on my trading capital in 3 weeks and with the market making large moves and the support and mentoring I get from Mr.Bailey and he's reachable through his mail.", "title": "forex , crpto currency and binary investments and investors for beginners"}, "SyGI9HKSg": {"type": "rebuttal", "replyto": "SJAJ3rHNl", "comment": "Per the reviewer's suggestion, the paper has been reorganized with the connection to RL delayed to the end of the paper.\n\nWe are not sure which figures the reviewer refers to regarding the lack of legend but would be happy to make further changes if it makes the paper clearer.", "title": "Reply to reviewer 2"}, "HJGLfdrNl": {"type": "rebuttal", "replyto": "SJAJ3rHNl", "comment": "Thank you for your review and comments.\n\nThe link with RL is made to show that these two communities optimize different losses, even though they care about the same objective. I will make the connection clearer.\n\nCould you elaborate on what you mean by \"pushed further\"? Do you mean evaluating its impact on a larget set of models? If so, do you have specific models in mind? Rather than performing an extended set of experiments, which will always be too limited for some, I hope to provide a compelling enough argument for everyone to try it on their log-likelihood optimized classifier of choice.\n\nI shall rewrite and improve unclear parts of the paper.", "title": "Reply to reviewer 2"}, "S1nVbOSEx": {"type": "rebuttal", "replyto": "rkkJ9LHNg", "comment": "Thank you for your review and comments. I apologize if some parts were unclear and will modify the paper accordingly.\n\n- In the experiments, the deterministic classifier is used. Interestingly, in the iterative scheme, the randomized classifier converges to the deterministic one, something which is not true in general.\n\nYou are right that, in the deterministic case (the one studied), the algorithm does not minimize an upper bound on the classification error but rather an upper bound on a smooth version of that error (replacing the step function with a sigmoid). I will make this clearer.\n\nI will also read your additional references and update the paper accordingly.\n\nI hope this clarifies any misunderstandings you might have had.", "title": "Deterministic vs. stochastic classifiers"}}}