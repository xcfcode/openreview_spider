{"paper": {"title": "Small steps and giant leaps: Minimal Newton solvers for Deep Learning", "authors": ["Joao Henriques", "Sebastien Ehrhardt", "Samuel Albanie", "Andrea Vedaldi"], "authorids": ["joao@robots.ox.ac.uk", "hyenal@robots.ox.ac.uk", "albanie@robots.ox.ac.uk", "vedali@robots.ox.ac.uk"], "summary": "A fast second-order solver for deep learning that works on ImageNet-scale problems with no hyper-parameter tuning", "abstract": "We propose a fast second-order method that can be used as a drop-in replacement for current deep learning solvers. Compared to stochastic gradient descent (SGD), it only requires two additional forward-mode automatic differentiation operations per iteration, which has a computational cost comparable to two standard forward passes and is easy to implement. Our method addresses long-standing issues with current second-order solvers, which invert an approximate Hessian matrix every iteration exactly or by conjugate-gradient methods, procedures that are much slower than a SGD step. Instead, we propose to keep a single estimate of the gradient projected by the inverse Hessian matrix, and update it once per iteration with just two passes over the network. This estimate has the same size and is similar to the momentum variable that is commonly used in SGD. No estimate of the Hessian is maintained.\nWe first validate our method, called CurveBall, on small problems with known solutions (noisy Rosenbrock function and degenerate 2-layer linear networks), where current deep learning solvers struggle. We then train several large models on CIFAR and ImageNet, including ResNet and VGG-f networks, where we demonstrate faster convergence with no hyperparameter tuning. We also show our optimiser's generality by testing on a large set of randomly-generated architectures.", "keywords": ["deep learning"]}, "meta": {"decision": "Reject", "comment": "The proposal is a scheme for using implicit matrix-vector products to exploit curvature information for neural net optimization, roughly based on the adaptive learning rate and momentum tricks from Martens and Grosse (2015). The paper is well-written, and the proposed method seems like a reasonable thing to try.\n\nI don't see any critical flaws in the methods. While there was a long discussion between R1 and the authors on many detailed points, most of the points R1 raises seem very minor, and authors' response to the conceptual points seems satisfactory.\n\nIn terms of novelty, the method is mostly a remixing of ideas that have already appeared in the neural net optimization literature. There is sufficient novelty to justify acceptance if there were strong experimental results, but in my opinion not enough for the conceptual contributions to stand on their own.\n\nThere is not much evidence of a real optimization improvement. The per-epoch improvement over SGD is fairly small, and (as the reviewers point out) probably outweighed by the factor-of-2 computational overhead, so it's likely there is no wall-clock improvement. Other details of the experimental setup seem concerning; e.g., if I understand right, the SGD training curve flatlines because the SGD parameters were tuned for validation accuracy rather than training accuracy (as is reported). The only comparison to another second-order method is to K-FAC on an MNIST MLP, even though K-FAC and other methods have been applied to much larger-scale models. \n\nI think there's a promising idea here which could make a strong paper if the theory or experiments were further developed. But I can't recommend acceptance in its current form.\n"}, "review": {"rklSUeaoYV": {"type": "rebuttal", "replyto": "rJgU3zOvyV", "comment": "We would like to address a few points which we consider inaccurate in the AC\u2019s final decision:\n\n> \u201cThe per-epoch improvement over SGD is fairly small\u201d\nWe show improvements on the order of 5% on CIFAR, and even reduce the ResNet error by a factor of 3 (from an already low 2.1% to 0.7%). We also improve on Adam by 2 to 3% in most cases. Compare this to many papers in the same venue that were published with reductions of 1% error compared to their baselines.\n\n> \u201c[Improvements] probably outweighed by the factor-of-2 computational overhead, so it's likely there is no wall-clock improvement\u201d\nWe show that there is a wall-clock improvement in Fig. 3. The improvements in optimization compensate for the small computational overhead.\n\n> \u201cSGD parameters were tuned for validation accuracy\u201d\nThis was only the case for the ImageNet experiments, which are 2 out of 7 experimental settings. However, we agree that it is fairer to use the same cross-validation protocol as for the other experiments. We found that SGD with the optimal learning rate still does not outperform Adam or our method, and will include this change in a future version.\n\n> \u201croughly based on [...] tricks from Martens and Grosse (2015).\u201d\nThe core of our method is an implicit inversion of the Hessian, while the mentioned work has an explicit model of the Hessian that is kept in memory - the methods differ substantially.\n", "title": "Response to final decision"}, "SJgp6njLJN": {"type": "rebuttal", "replyto": "SJx9di-XkV", "comment": "Thank you for pointing out this interesting connection. Quoting from the paper you mentioned:\n\n\u201cIf CG terminated after just 1 step, HF becomes equivalent to NAG, except that it uses a special formula based on the curvature matrix for the learning rate instead of a fixed constant.\u201d\n\nThe same reasoning applies to our own method, since it is a modification of the HF approach. The main difference from NAG is then the use of curvature, which Nesterov\u2019s accelerated GD (NAG) does not use, as pointed out in the quote.\n\nAs a practical matter, note that NAG is not robust to perturbations and accumulates errors in the gradient oracle linearly with iterations (as discussed in a paper by Nesterov himself and co-authors, \u201cFirst-Order Methods of Smooth Convex Optimization with Inexact Oracle\u201d).\n\nNAG is a part of standard deep learning frameworks and practitioners try it from time to time. While we cannot comment on its success in RNNs with sigmoids (which Sutskever focused on), our experience with modern CNNs shows that it is \u201chit or miss\u201d, with no clear advantage over a similarly well-tuned momentum SGD. Its success is highly dependent on the stochasticity of the problem and many other factors (which is expected given the Nesterov paper we mentioned).\n", "title": "Relationship to Nesterov accelerated GD"}, "Byxdx-dHJE": {"type": "rebuttal", "replyto": "Ske076UWJV", "comment": "Thank you for the response to our points. We have answered the initial comments that you mentioned in the appropriate thread.\n\n> \u201cKFAC has been used in various papers to train neural nets (see references below) so I still think the authors should provide a comparison on the larger networks.\u201d\n\nAs much as we would have liked to include the improved KFAC modifications published by Grosse & Martens recently, it would represent a large departure from our training regime, which uses single GPUs. The mentioned works are all intended for multi-GPU settings. Multi-GPU synchronization of updates brings a host of confounding factors and different dynamics compared to synchronous (single-GPU) training, see Mitliagkas et al. (arXiv:1605.09774) for one example. However, we recognize that this line of research is very relevant, and we will include a caveat in the paragraph on KFAC that these improved versions exist, and should perform better in these settings.\n\n> \u201cResults in Figure 2 are averaged over several runs? What does the variance look like?\u201d\n\nThe results in fig. 2 are not averaged over several runs, although the results in fig. 3 (left) are (averaged over 50 random architectures), which gives an indication of the variance. In practice, the variance across optimizers on larger benchmarks is very small -- we will add additional runs to the final version to make this more concrete.\n\n> \u201cCan you show the plots in Figure 2 in terms of training time? In Fig. 7, your method does not outperform others so I would like to see more empirical results.\u201d\n\nYes, we can include all the plots for completeness, but the conclusions are unchanged compared to the referenced plot (which is fig. 3-right in the updated paper). Our goal was to show that it is comparable to first-order methods, despite the overhead of the second-order operations, and gains the benefit of having no hyper-parameter tuning.\n\nWe would also like to bring attention to the fact that the large gap between Adam and SGD on the per-iteration plots (fig. 2) is mostly erased in the wall-clock time plot (fig. 3-right), due to the additional matrix operations that each step of Adam requires. The same phenomena happens with our method, which could be improved with better engineering of the FMAD operations.\n\n> \u201cHow sensitive is your approach to the batch size?\u201d\n\nIt is not very sensitive to batch size, as we simply used the same batch sizes as in the original papers for all of the tested architectures. It is possible that tuning the batch size brings additional benefit but we did not exploit it.\n", "title": "Response to additional questions"}, "SygVWkE4JE": {"type": "rebuttal", "replyto": "ryeMkWWQn7", "comment": "> \u201cYou claim to decay rho\u201d\n\nWe do not claim to explicitly decay rho (one of the points we focused on was on not having complicated schedules to tune), but rather it decays naturally as a consequence of the automatic hyper-parameter adaptation (fig. 5 in updated paper).\n\n> \u201cBetter than simply using CG on a trust-region model?\u201d\n\nWe tried this variant (fig. 7), but the large number of inner iterations makes it less competitive.\n\n> \u201cAnalysis is only performed on a quadratic\u201d\n\nTheorem A.2 deals specifically with non-quadratic functions.\n\n> \u201cIt appears the rate would scale with the square root of the condition number\u201d; \u201cconstant is not as good as Heavy-ball on a quadratic\u201d\n\nUnfortunately, the rates that we derived are not as directly interpretable as SGD or momentum SGD, despite our best efforts (eq. 38). However, on a convex quadratic we do not expect our rate of convergence to be better than momentum SGD with the optimal momentum parameter. Note that using momentum GD with the optimal hyper-parameters (which require knowledge of the Hessian eigenvalues of the quadratic) already provides a square-root improvement in the condition number compared to GD. We would consider any further improvement in linear convergence in this well-explored setting to be quite a breakthrough.\n\n> \u201cSub-sampling \u2026 not discussed\u201d; \u201cconsider extending the proof\u201d\n\nWe addressed this matter in our initial response.\n\n> \u201cConsider showing the gradient norms\u201d\n\nWe added this to the paper (fig. 6).\n\n> \u201cMethods have not yet converged\u201d\n\nIt is standard practice in deep learning comparisons to give methods a budget of epochs to converge over, due to the time-consuming nature of the experiments. We used the default numbers of epochs for which the SGD learning rate schedules were defined. Note that even when this is not the case, early-stopping is used as an effective regularization method (since the parameters vastly outnumber the samples).\n\n> Compare to Newton method with true Hessian\n\nWe added this to the paper; see the \u201cExact Hessian\u201d (Newton method) row in table 1 and fig. 1.\n\n> \u201cWhy is BFGS in Rosenbrock but not in NN plots?\u201d\n\nThis was addressed in another comment.\n\n> \u201cDauphin does not rely on the Gauss-Newton approximation\u201d\n\nWe cited Dauphin et al. as a reference on avoiding saddle-points with PSD surrogates for the Hessian. We did not mean to imply that they used the Gauss-Newton surrogate. We agree that this should be more clear, and corrected the text.\n\n> \u201cThe title is rather bold and not necessarily precise since the stepsize of curveball is not particularly small\u201d\n\nThe title is actually a reference to the different step sizes in different parameter-space directions, when optimizing an ill-conditioned function (mentioned in the 2nd paragraph of section 2.1).\n\n> Additional citations and other editing suggestions\nWe would like to thank the reviewer for the suggestions, which we incorporated in the paper. Note that we cited Loizou & Richtarik (2017) as an up-to-date summary of theoretical results; nevertheless we now cite the original papers.", "title": "Detailed response to AR1 (part 2/2)"}, "rJxEV074kE": {"type": "rebuttal", "replyto": "ryeMkWWQn7", "comment": "> \u201cThe derivation ... is very much driven on a set of heuristics without theoretical guarantees\u201d\n\nIt is common (in fact, necessary) to propose changes to existing methods based on empirical observation of their failures. It is only after proposing changes that we can prove theoretical guarantees. We would like to refer the reviewer to Theorems A.1-A.2 for such guarantees.\n\n> \u201cIn the early phase, the gradient norm is likely to be large and thus z will change significantly. One might also encounter regions of high curvature\u201d\n\nAlthough the analysis (theoretical and experimental) shows that the method converges anyway, we can analyze these cases in the following way.\n\nThe step z update can be rearranged as: z <- (rho*I - beta*H)*z - beta*J. Assume that the hyper-parameters and Hessian model are correct to enable convergence (H is positive-definite, rho is close to 1 and beta < 1/||H||). Then, in the high-curvature directions of H, z will be reduced the most towards 0, compared to lower-curvature directions. So in those directions, the algorithm behaves like GD. Likewise, for a large enough gradient J and low curvature, its magnitude overwhelms the first term, so the update devolves to standard GD.\n\n> \u201cThe \"warm start\" at s_{t-1} is also what yields the momentum term, what interpretation can you give to this choice?\u201d\n\nThe warm-starting is what makes the algorithm directly comparable to momentum. We establish this connection in section 3, and expand on differences and similarities (CurveBall vs momentum SGD). If there is a specific unclear aspect we\u2019ll be happy to address it.\n\n> \u201cIt is rather unclear why one iteration is an appropriate number\u201d\n\nIt is appropriate due to the interleaving of steps (Algorithm 1) -- it does not represent a single isolated step, but rather builds on the previous iteration. Likewise, one could ask: why does momentum SGD only update the step z once for each iteration of w?\n\n> \u201cAdaptive strategy where CG with a fixed accuracy\u201d\n\nThis has been done in previous works, and is very costly since it often requires dozens of steps (see fig. 7 in the appendix for a direct comparison).\n\n> \u201cGradually increasing the batch-size\u201d\n\nThis would introduce a schedule of batch sizes to tune, which would increase the complexity of implementation and usage.\n\n> \u201cThe number of outer iterations may be a lot less for the Hessian-free method than for SGD\u201d\n\nThe Hessian-free method would have to converge faster than SGD by orders of magnitude (measured in outer iterations) to compensate. While this is observed with linear models (where such methods are widely deployed), it is not necessarily so for deep networks, as verified in our experiments with CG (figure 7 in appendix). Nevertheless, we agree that this claim is overly broad, and we toned it down in the text.\n\n> Choice of GD over Krylov subspace methods, Lanczos\n\nThere are several reasons:\n- Memory. While Krylov subspace methods have better convergence rates, they require more storage.\n- Simplicity. The aim of this paper is to create a \u201cminimal\u201d solver. Gradient descent fits this criteria better than the other methods; it can be described in a single line given a gradient.\n- Robustness to noise between iterations. SGD is well-understood and works well with perturbed updates; it remains to be demonstrated whether Lanczos and other methods can be made equally robust. Very recent work in this front (De Sa et al., \u201cAccelerated Stochastic Power Iteration\u201d, arXiv 2017) shows that much larger batches than what is acceptable for deep networks (i.e. tens of thousands) may be needed.\n\nd) \u201cI\u2019m not really sure [rho] makes sense\u201d\n\nThe rho parameter allows bridging two formalisms which would not be possible otherwise. Our method can be interpreted as:\n1) A momentum GD variant: it modifies momentum GD by introducing a single term, -beta*H.\n2) A Hessian-free optimizer variant: by performing the changes that the reviewer just mentioned (section 3).\nNote that the rho parameter is crucial for the first interpretation. Nonetheless, its apparent arbitrariness when viewed under the second interpretation can be resolved by setting rho=1. We tried fixing rho=1 experimentally, but it degrades performance; we added this experiment to the paper (fig. 8).\nThe effect of rho can be interpreted in two ways. First, rho<1 gradually erases stale updates (based on old Hessian matrices) from the z buffer, which is important for a non-quadratic objective. Second, it results in the regularizer (1-rho)*||z||^2 in the quadratic model, which can be beneficial, and has small magnitude with rho close to 1.\nFinally, the automatic hyper-parameter tuning (eq. 18) requires rho to be present, which is another practical reason for its presence.\n", "title": "Detailed response to AR1 (part 1/2)"}, "S1e706lJkN": {"type": "rebuttal", "replyto": "rkxy10K2C7", "comment": "> \u201cIt seems we would all agree that the paper is lacking from a theoretical point of view.\u201d\n\nThis is not a fair characterization of our viewpoint. Our point was that we focused much more on delivering an algorithm that is easy to implement and use by practitioners, than on tuning it to obtain theoretical guarantees (e.g. by adding variance reduction techniques). The reviewer has stated a preference for the opposite approach, which we acknowledge, and it was with this concern in mind that we included Theorems A.1-A.2 in our initial submission.\n\n> \u201cWhy is BFGS in Rosenbrock but not in NN plots?\u201d\n\nBFGS was not included because it does not work in these settings, a widely known fact among practitioners, which is matched by our observations, and which explains its absence from the state-of-the-art in deep learning. Concretely, the issues are:\n- Memory. For typical problems, we simply cannot form a millions-by-millions-sized matrix. Limited-memory variants will instead require K columns, but K is still in the dozens or more. For large models, we typically cannot afford more than a factor of 2 or 3 more storage than the original parameters.\n- Stochasticity. BFGS breaks down with noisy functions (observed in our Stochastic Rosenbrock experiments). Variants that are robust to noise exist, but they have similar or worse memory and computation requirements.\n\nThese are not a concern with stochastic first-order methods, which are widely used.\n\n> \u201cSame question regarding K-FAC, why is it only showed for MNIST?\u201d\n\nK-FAC requires non-trivial amounts of hand-tuning to work, as stated by the author on the project page. For example, they state the learning rate can take values between 10^-5 and 100, and in a later paper (Ba et al., ICLR 2017) they use an exponential learning rate decay with constants c_0, zeta, and exponential averaging of parameters over time.\n\nAdditionally, its memory requirements are unusually large, necessitating distributed learning across multiple machines in the mentioned paper, which makes it unwieldy.\n\nNevertheless, we show one comparison to K-FAC that is feasible, using the original paper\u2019s code and problem setting (MLP autoencoder on MNIST), with hyper-parameters calibrated by the authors, in the interest of fairness.\n\n> \u201cKFAC seem to be reaching a lower function value in Figure 2, please use a log scale\u201d\n\nWe will consider this scale in the final version. Note that at these noise levels it will be hard to observe any meaningful difference between the algorithms. \n\n> \u201cI disagree with your claim \u201cWe believe we are the first to apply a second-order method in such a way to extremely large settings\u201d. BFGS has been used for a long time to optimize deep neural networks, see e.g. https://arxiv.org/pdf/1311.2115.pdf that had experiments on a a twelve layer neural network.\u201d\n\nWe must clarify that extremely large settings mean several layers and millions of parameters, on non-toy problems. There are several aspects that make the referenced paper non-comparable:\n- The number of parameters or model architecture is not reported, other than mentioning that it has 12 layers.\n- It is an MLP applied to 28x28 inputs, which is only applicable to the least realistic scenarios.\n- The dataset (\u201cCURVES\u201d) is entirely composed of synthetic toy data.\n\nWe would like to contrast this scale to training a VGG-f model with over 60 million parameters on ImageNet, with 224x224 images, among our other experiments. Hence our claim.\n\n> \u201cRegarding the proof of convergence of Theorem A.2, note that you either require convexity or - as you suggested -, you could rely on a trust-region approach but then the decrease is only valid for achieving a **model** decrease. You would need to implement a proper trust-region algorithm to guarantee a **function** decrease.\u201d\n\nAs the reviewer has noted, this is not a problem with the proof -- which assumes a trust region, a reasonable assumption -- but with the implementation, which uses a simple mechanism for this trust region.\n\nAs explained in section 3 (subsection on hyper-parameter lambda), we chose the simplest trust-region adaptation because it requires only 1 additional function evaluation. We could have easily chosen another mechanism, at the cost of speed, which is important in the large-scale. We remark that the same choice was made by Martens & Grosse (2015). This choice is entirely divorced from the validity of the proposed method, and represents one of the usual trade-offs made in any practical implementation.\n\n> \u201cI therefore do not think the statement in Theorem A.2 is especially relevant to the deep learning setting which you seem to be targeting in this paper.\u201c\n\nOn the contrary, the extensive experiments show that the trust-region model with automatic hyper-parameter adaptation is quite accurate, otherwise the reported high performance would not have been observed.\n", "title": "Such comparisons are not feasible at this scale; the mentioned \"theory\" problem is actually an implementation detail"}, "HyeK5Jt3R7": {"type": "rebuttal", "replyto": "ryeMkWWQn7", "comment": "We thank the reviewer for the constructive comments, especially with regards to having a more complete bibliography, which we have integrated.  However, we differ considerably with the reviewer in their assessment of our contribution.\n \nWe would like to emphasize that our specific goal and motivation for this work was the development of a practical optimization method for large-scale deep learning.     \n \nIn this respect, our contribution pushes the boundaries of what has been done previously in similar papers, with a much greater scope and stringent protocol (e.g. no tuning of hyper-parameters on each experiment). We believe we are the first to apply a second-order method in such a way to extremely large settings, such as the VGG-f on ImageNet (over 60 million parameters and 1 million samples), as well as several other datasets, models, and large numbers of randomly-generated architectures.\n \nThis is not to say that we do not place great value in theoretical guarantees, and in fact we proved convergence of our algorithm in convex quadratic functions (Theorem A.1) and guaranteed descent in general non-convex functions (Theorem A.2), a much broader result that the reviewer did not mention.  However, we consider that providing convergence proofs for the non-convex stochastic case (as suggested) is an unreasonable burden, both due to their much greater complexity, and because the guarantees they afford are usually mild. Instead, we only proved formally that our algorithm \u201cdoes the right thing\u201d (i.e. descends for reasonable functions), and the hard case (stochastic non-convex functions with millions of variables) is instead validated empirically.\n \nWe recognise that our work places greater reliance on careful empirical evidence than the theoretical analysis preferred by the reviewer, but we hope that they will nevertheless reconsider their assessment that it represents a useful contribution to the community targeted by this conference. \n \nWe will give a more detailed answer to each point in a separate comment.\n", "title": "Short summary of response to AR1"}, "SyemYpEiam": {"type": "rebuttal", "replyto": "ryeIGPDv2m", "comment": "Thank you for the very thoughtful suggestions and questions.\n\n> Comparison to the LiSSA algorithm\nThere are indeed very interesting connections between CurveBall and LiSSA. Despite their main update being derived from very different assumptions, we found that it is possible to manipulate it into a form that is directly comparable to ours, without a learning rate \\beta. Another difference is structural: LiSSA uses a nested inner loop to create a Newton-like update from scratch every iteration, like other Hessian-free methods, while our algorithm structure has no such nesting and thus has the same structure as momentum SGD (cf. Alg. 1 and Alg. 2 in the paper).\n\nWe updated the paper with a much more detailed exposition of these points, which we have only hinted at in this response to keep it short. It can be found in the last (large) paragraph of the related work (section 5, p. 9).\n\n> Page numbers on books\nThank you, we agree that this is important; we just added them to the paper.\n\n> Vectors as capital letters, e.g. J(w)\nWe share this concern, however this was used to simplify our exposition of automatic differentiation (sec. 2.2). There, the gradient J arises from the multiplication of several Jacobians, which are generally matrices, and it only happens to be a vector because of the shape of the initial projection. We could have treated Jacobians and gradients separately, but it would hamper this unifying view which we found more instructive.\n\n> Automatic hyper-parameters derivation\nAlthough this can be found in the work of Martens & Grosse (2015), to make the paper self-contained we added the derivations to the appendix (section A.1), consisting of a simple minimization problem in the \\rho and \\beta scalars.\n\n> \u201cPlot the evolution of \\beta, \\rho and \\lambda\u201d\nThis is an interesting aspect to analyze. We plot these quantities for two models (with and without batch normalization) in the (newly-added) Fig. 5.\n\nIt seems that the momentum hyper-parameter \\rho starts with a high value and decreases over time, with what appears to be geometric behavior. This is in line with the mentioned theory, although it is simply a result of the automatic tuning process.\n\nAs for the learning rate \\beta, it increases in an initial phase, only to decrease slowly afterwards. We can compare this to the practitioners\u2019 manually-tuned learning rate schedules for SGD that include \u201cburn-in\u201d periods, which follow a similar shape (He et al., 2016). Similar schedules were also obtained by previous work on gradient-based hyper-parameter tuning (Maclaurin et al., \u201cGradient-based Hyperparameter Optimization through Reversible Learning\u201d, ICML 2015).\n\nThe trust region \\lambda decreases over time, but by a minute amount. The trust region adaptation is a 1D optimization problem over \\lambda, minimizing the difference between the ratio \\gamma and 1. This 1D problem has many local minima, punctuated by singularities corresponding to Hessian eigenvalues (see Wright & Nocedal (1999) fig. 4.5). Given a large enough spread of eigenvalues, it is not surprising that a minimum close to the initial \\lambda was found by the iterative adaptation scheme.\n\n> Comment on Stochastic Line Searches; damping for \\lambda using \\gamma\nWe agree that a more satisfactory solution would be to employ the ideas of Probabilistic Line Search. However, it would involve reframing the optimization in Bayesian terms, which would be a large change and add significant complexity, which we tried to avoid.\n\nInstead, and inspired by KFAC (Martens & Grosse, 2015), we change \\lambda in *small* increments based on how close \\gamma is to 1. The argument is that, even if a particular batch gives an inaccurate estimate of \\gamma, in expectation it should be correct, and so most of the small \\lambda increments will be in the right direction (in 1D). The procedure would indeed be unstable if the increments were much less gradual.\n\n> Re-do experiments with Armijo-Wolfe line search; BFGS performance\nBFGS only needs 19 function evaluations to achieve 10^-4 error on the *deterministic* Rosenbrock function, which we considered to be a reasonable result. However, the *stochastic* Rosenbrock functions are more difficult, as expected.\n\nThe cubic line search is part of the BFGS implementation that ships with Matlab. Following this suggestion, we also tested minFunc\u2019s implementation of L-BFGS, which includes Armijo and Wolfe line searches. We tried several initialization schemes, as well as different line search variants, and found no improvement over the previous ones (Table 1).\n\n> \u201cTry (true) Newton's method\u201d\nWe considered LM as the upper baseline as the Hessian isn\u2019t necessarily definite positive, but the true Newton\u2019s method is indeed subtly different. It achieves slightly better results overall (see updated Table 1). Note that when the Hessian matrix has negative eigenvalues, we use the absolute values instead.\n\n> \u201cPlease consider rephrasing some phrases\u201d\nWe did; thank you for the suggestions.\n", "title": "Response to AR2"}, "B1xyXcQ767": {"type": "rebuttal", "replyto": "SyxRWTJxpQ", "comment": "We would like to thank the reviewer for the comments and questions.\n\n> \u201cIntroducing \\rho parameter and solving for optimal \\rho, \\beta complicates things\u201d\nIt does, but it makes the proposed solver more reliable (no tuning is necessary).\n\nIt is possible to set the hyper-parameters manually, but this requires multiple runs (similarly to learning rate tuning for SGD), which makes it much less convenient.\n\nOne reason for introducing rho, in addition to the connection to momentum SGD, is that this allowed us to use the same automatic tuning strategy as Martens & Grosse (2015). This formulation depends on the update equations having both rho and beta hyper-parameters. Another intuitive reason is to slowly forget stale updates, which is the same role played by this parameter in momentum SGD. We will clarify this further in the paper.\n\nOur analysis for the convex quadratic case (visualized in fig. 4, appendix A) shows that the algorithm converges on a relatively large region of the (rho, beta) parameter-space. However, the best performance is achieved in a relatively narrow band, which will vary depending on the Hessian eigenvalues. Automatically solving for the optimal rho and beta removes this concern.\n\n> \u201cFor ImageNet results, they show 82% accuracy after 20 epochs on full ImageNet using VGG. Is this top5 or top1 error?\u201d\nThis is top-1 training error; if it were top-1 validation error, it would indeed be unreasonably good.\n\nCounter-intuitively, SGD is well tuned -- its training error stalls, however the validation error keeps going down for a few more epochs. The learning rate annealing schedule was chosen by the authors of the VGG-f model taking this into account. This is a problem with SGD -- as it is implemented, it works both as optimizer and regularizer.\n\nWe show the training error in all plots in order to accurately measure improvements in optimization, without the added confusion of such regularization effects. We study and discuss the validation error separately (in the last subsection of the experiments).\n\nIn summary, we found that models that have an appropriate number of parameters w.r.t. the dataset size benefit from our improved optimization, while the larger models (e.g. ResNet) require additional regularization to lower the validation error.\n\nWe view this development as a two-step process: first we create algorithms that can optimize the objective function efficiently; and once we have them, we can focus on effective regularization techniques. We believe that this strategy is more promising than developing both simultaneously.\n", "title": "Response to AR3"}, "SyxRWTJxpQ": {"type": "review", "replyto": "Sygx4305KQ", "review": "Authors propose choosing direction by using a single step of gradient descent \"towards Newton step\" from an original estimate, and then taking this direction instead of original gradient. This direction is reused as a starting estimate for the next iteration of the algorithm. This can be efficiently implemented since it only relies on Hessian-vector products which are accessible in all major frameworks.\n\nBased on the fact that this is an easy to implement idea, clearly described, and that it seems to benefit some tasks using standard architectures, I would recommend this paper for acceptance.\n\nComments:\n- introducing \\rho parameter and solving for optimal \\rho, \\beta complicates things. I'm assuming \\rho was needed for practical reasons, this should be explained better in the paper. (ie, what if we leave rho at 1)\n- For  ImageNet results, they show 82% accuracy after 20 epochs on full ImageNet using VGG. Is this top5 or top1 error? I'm assuming top5 since top1 would be new world record for the number of epochs needed. For top5, it seems SGD has stopped optimizing at 60% top5. Since all the current records on ImageNet are achieved with SGD (which beats Adam), this suggests that the SGD implementation is badly tuned\n- I appreciate that CIFAR experiments were made using standard architectures, ie using networks with batch-norm which clearly benefits SGD", "title": "Well-motivated idea", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "ryeMkWWQn7": {"type": "review", "replyto": "Sygx4305KQ", "review": "This paper proposes an approximate second-order method with low computational cost. A common pitfall of second-order methods is the computation (and perhaps inversion) of the Hessian matrix. While this can be avoided by instead relying on Hessian-vector products as done in CG, it typically still requires several iterations. Instead, the authors suggest a simpler approach that relies on one single gradient step and a warm start strategy. The authors points out that the resulting algorithm resembles a momentum method. They also provide some simple convergence proofs on quadratics and benchmark their method to train deep neural networks.\n\nWhile I find the research direction interesting, the execution is rather clumsy and many details are not sufficiently motivated. Finally, there is a lot of relevant work in the optimization community that is not discussed in this paper, see detailed comments and references below.\n\n1) Method\nThe derivation of the method is very much driven on a set of heuristics without theoretical guarantees. In order to derive the update of the proposed method, the authors rely on three heuristics:\na) The first is to reuse the previous search direction z as a warm-start. The authors argue that this might be beneficial if If z does not change abruptly. In the early phase, the gradient norm is likely to be large and thus z will change significantly. One might also encounter regions of high curvature where the direction of z might change quickly from one iteration to the next.\nThe \"warm start\" at s_{t-1} is also what yields the momentum term, what interpretation can you give to this choice?\n\nb) The second step interleaves the updates of z and w instead of first finding the optimum z. This amounts to just running one iteration of CG but it is rather unclear why one iteration is an appropriate number. It seems one could instead some adaptive strategy where CG with a fixed accuracy. One could potentially see if allowing larger errors at the beginning of the optimization process might still allow for the method to converge. This is for instance commonly done with the batch-size of first-order method. Gradually increasing the batch-size and therefore reducing the error as one gets close to the optimum can still yield to a converging algorithm, see e.g. \nFriedlander, M. P., & Schmidt, M. (2012). Hybrid deterministic-stochastic methods for data fitting. SIAM Journal on Scientific Computing, 34(3), A1380-A1405.\n\nc) The third step consists in replacing CG with gradient descent.\n\"If CG takes N steps on average, then Algorithm 2 will be slower than SGD by a factor of at least N, which can easily be an order of magnitude\".\nFirst, the number of outer iterations may be a lot less for the Hessian-free method than for SGD so this does not seem to be a valid argument. Please comment.\nSecond, I would like to see a discussion of the convergence rate of solving (12) inexactly with krylov subspace methods. Note that Lanczos yields an accelerated rate while GD does not. So the motivation for switching to GD should be made clearer.\n\nd) The fourth step introduces a factor rho that decays z at each step. I\u2019m not really sure this makes sense even heuristically. The full update of the algorithm developed by the author is:\nw_{t+1} = w_t - beta nabla f + (rho I - beta H) (w_t - w_{t-1}).\nThe momentum term therefore gets weighted by (rho I - beta H). What is the meaning of this term? The -beta H term weights the momentum according to the curvature of the objective function. Given the lack of theoretical support for this idea, I would at least expect a practical reason back up by some empirical evidence that this is a sensible thing to do.\nThis is especially important given that you claim to decay rho therefore giving more importance to the curvature term.\nFinally, why would this be better than simply using CG on a trust-region model? (Recall that Lanczos yields an accelerated linear rate while GD does not).\n\n2) Convergence analysis\na) The analysis is only performed on a quadratic while the author clearly target non-convex functions, this should be made clear in the main text. Also see references below (comment #3) regarding a possible extension to non-convex functions.\nb) The authors should check the range of allowed values for alpha and beta. It appears the rate would scale with the square root of the condition number, please confirm, this is an important detail. I also think that the constant is not as good as Heavy-ball on a quadratic (see e.g. http://pages.cs.wisc.edu/~brecht/cs726docs/HeavyBallLinear.pdf), please comment.\nc) Sub-sampling of the Hessian and gradients is not discussed at all (but used in the experiments). Please add a discussion and consider extending the proof (again, see references given below).\n\n3) Convergence Heavy-ball\nThe authors emphasize the similarity of their approach to Heavy-ball. They cite the results of Loizou & Richtarik 2017. Note that they are earlier results for quadratic functions such as \nLessard, L., Recht, B., & Packard, A. (2016). Analysis and design of optimization algorithms via integral quadratic constraints. SIAM Journal on Optimization, 26(1), 57-95.\nFlammarion, N., & Bach, F. (2015, June). From averaging to acceleration, there is only a step-size. In Conference on Learning Theory (pp. 658-695).\nThe novelty of the bounds derived in Loizou & Richtarik 2017 is that they apply in stochastic settings.\nFinally, there are results for non-convex functions such convergence to a stationary point, see\nZavriev, S. K., & Kostyuk, F. V. (1993). Heavy-ball method in nonconvex optimization problems. Computational Mathematics and Modeling, 4(4), 336-341.\nAlso on page 2, \"Momentum GD ... can be shown to have faster convergence than GD\". It should be mentioned that this only hold for (strongly) convex functions!\n\n4) Experiments\na) Consider showing the gradient norms. \nb) it looks like the methods have not yet converged in Fig 2 and 3.\nc) Second order benchmark:\nIt would be nice to compare to a method that does not use the GN matrix but the true or subsampled Hessian (like Trust Region/Cubic Regularization) methods given below.\nWhy is BFGS in Rosenbrock but not in NN plots?\nd) \"Batch normalization (which is known to improve optimization)\" \nThis statement requires a reference such as\nTowards a Theoretical Understanding of Batch Normalization\nKohler et al\u2026 - arXiv preprint arXiv:1805.10694, 2018\n\n5) Related Work\nThe related work should include Cubic Regularization and Trust Region methods since they are among the most prominent second order algorithms. Consider citing Conn et al. 2000 Trust Region,  Nesterov 2006 Cubic regularization, Cartis et al. 2011 ARC.\nRegarding sub-sampling: Kohler&Lucchi 2017: Stochastic Cubic Regularization for non-convex optimization and Xu et al.: Newton-type methods for non-convex optimization under inexact hessian information.\n\n6) More comments\n\nPage 2\nPolyak 1964 should be cited  where momentum is discussed.\n\"Perhaps the simplest algorithm to optimize Eq. 1 is Gradient Descent\". This is technically not correct since GD is not a global optimization algorithm. Maybe mention that you try to find a stationary point\nrho (Eq. 2) and lambda (Eq. 4) are not defined\n\nPage 4: \nAlgorithm 1 and 2 and related equations in the main text: it should be H_hat instead of H.\n\nBackground\n\u201cMomemtum GD exhibits somewhat better resistance to poor scaling of the objective function\u201d\nTo be precise the improvement is quadratic for convex functions. Note that Goh might not be the best reference to cite as the article focuses on quadratic function. Consider citing the lecture notes from Nesterov.\n\nSection 2.2\nThis section is perhaps a bit confusing at first as the authors discuss the general case of a multivalue loss function. Consider moving your last comment to the beginning of the section.\n\nSection 2.3\nAs a side remark, the work of Dauphin does not rely on the Gauss-Newton approximation but a different PSD matrix, this is probably worth mentioning.\n\nMinor comment: The title is rather bold and not necessarily precise since the stepsize of curveball is not particularly small e.g. in Fig 1.\n", "title": "Interesting research direction but the paper needs a lot more work before publication", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "ryeIGPDv2m": {"type": "review", "replyto": "Sygx4305KQ", "review": "In this paper, the authors introduce a new second-order algorithm for training deep networks. The method, named CurveBall, is motivated as an inexpensive alternative to Newton-CG. At its core, the method augments the update role for SGD+M with a Hessian-vector product that can be done efficiently (Algorithm 1). While a few new hyperparameters are introduced, the authors propose ways by which they can be calibrated automatically (Equation 16) and also prove convergence for quadratic functions (Theorem A.1) and guaranteed descent (Theorem A.2). The authors also present numerical results showing improved training on common benchmarks. I enjoyed reading the paper and found the motivation and results to be convincing. I especially appreciate that the authors performed experiments on ImageNet instead of just CIFAR-10, and the differentiation modes are explained well. As such, I recommend the paper for acceptance. \n\n\nI suggest ways in which the paper can be further improved below:\n\n- In essence, the closest algorithm to CurveBall is LiSSA proposed by Agarwal et al. They use a series expansion for approximating the inverse whereas your work uses one iteration of CG. If you limit LiSSA to only one expansion, the update rule that you would get would be similar to that of CurveBall (but not exactly the same). I feel that a careful comparison to LiSSA is necessary in the paper, highlighting the algorithmic and theoretical differences. I don't see the need for any additional experiments, however.\n- For books, such as Nocedal & Wright, please provide page numbers for each citation since the information quoted is across hundreds of pages. \n- It's a bit non-standard to see vectors being denoted by capital letters, e.g. J(w) \\in R^p on Page 2. I think it's better you don't change it now, however, since that might introduce inadvertent typos. \n- It would be good if you could expand on the details concerning the automatic determination of the hyperparameters (Equation 16). It was a bit unclear to me where those equations came from. \n- Could you plot the evolution of \\beta, \\rho and \\lambda for a couple of your experiments? I am curious whether our intuition about the values aligns with what happens in reality. In Newton-CG or Levenberg-Marquardt-esque algorithms, with standard local strong convexity assumptions, the amount of damping necessary near the solution usually falls to 0. Further, in the SGD+M paper of Sutskever et al., they talked about how it was necessary to zero out the momentum at the end. It would be fascinating if such insights (or contradictory ones) were discovered by Equation 16 and the damping mechanism automatically. \n- I'm somewhat concerned about the damping for \\lambda using \\gamma. There has been quite a lot of work recently in the area of Stochastic Line Searches which underscores the issues involving computation with noisy estimates of function values. I wonder if the randomness inherent in the computation of f(w) can throw off your estimates enough to cause convergence issues. Can you comment on this?\n- It was a bit odd to see BFGS implemented with a cubic line search. The beneficial properties of BFGS, such as superlinear convergence and self-correction, usually work out only if you're using the Armijo-Wolfe (Strong/Weak) line search. Can you re-do those experiments with this line search? It is unexpected that BFGS would take O(100) iterations to converge on a two dimensional problem. \n- In the same experiment, did you also try (true) Newton's method? Maybe we some form of damping? Given that you're proposing an approximate Newton's method, it would be a good upper baseline to have this experiment. \n- I enjoyed reading your experimental section on random architectures, I think it is quite illuminating. \n- Please consider rephrasing some phrases in the paper such as \"soon the latter\" (Page 1), \"which is known to improve optimisation\", (Page 7), \"non-deep problems\" (Page 9). ", "title": "Good Paper, Accept", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}