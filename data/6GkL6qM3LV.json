{"paper": {"title": "N-Bref : A High-fidelity Decompiler Exploiting Programming Structures ", "authors": ["Cheng Fu", "Kunlin Yang", "Xinyun Chen", "Yuandong Tian", "Jishen Zhao"], "authorids": ["~Cheng_Fu1", "k6yang@eng.ucsd.edu", "~Xinyun_Chen1", "~Yuandong_Tian1", "~Jishen_Zhao1"], "summary": "", "abstract": "Binary decompilation is a powerful technique for analyzing and understanding software, when source code is unavailable. It is a critical problem in the computer security domain. With the success of neural machine translation (NMT), recent efforts on neural-based decompiler show promising results compared to traditional approaches. However, several key challenges remain: (i) Prior neural-based decompilers focus on simplified programs without considering sophisticated yet widely-used data types such as pointers; furthermore, many high-level expressions map to the same low-level code (expression collision), which incurs critical decompiling performance degradation; (ii) State-of-the-art NMT models(e.g., transformer and its variants) mainly deal with sequential data; this is inefficient for decompilation, where the input and output data are highly structured. In this paper, we propose N-Bref, a new framework for neural decompilers that addresses the two aforementioned challenges with two key design principles: (i)N-Bref designs a structural transformer with three key design components for better comprehension of structural data \u2013 an assembly encoder, an abstract syntax tree encoder, and a tree decoder, extending transformer models in the context of decompilation. (ii) N-Bref introduces a program generation tool that can control the complexity of code generation and removes expression collisions. Extensive experiments demonstrate that N-Bref outperforms previous neural-based decompilers by a margin of 6.1%/8.8% accuracy in datatype recovery and source code generation. In particular, N-Bref decompiled human-written Leetcode programs with complex library calls and data types in high accuracy.", "keywords": ["Programming Language", "Reverse engineering", "neural machine translation", "machine learning for system"]}, "meta": {"decision": "Reject", "comment": "The paper describes N-Bref, a new tool for decompilation of stripped binaries. Compared to previous tools for neural-based decompilation, this tool is based on two new ideas: a) to separate the generation of data declarations from the generation of the code itself, and b) the use of more sophisticated network architectures. These network architectures, however, all come from prior work, so the contribution in that regard is only their application to this particular problem. \n\nThe authors addressed many of complaints raised by reviewers, particularly with regards to presentation and explanations, but I think the most substantial concerns remain. \n\nThe most substantial concern is novelty. The technique is built on a combination of existing models, and its only original idea seems to be to treat the generation of data declarations and the code itself as separate tasks to be handled by independently trained networks. \n\nIn terms of results, the paper shows some quantitative improvements over prior work, although it is not so clear that those improvements matter. The quality improvement is measured in terms of AST differences, but it is not clear how often those AST differences translate into semantic differences. More importantly, the tool is restricted to un-optimized binaries, which significantly limits its applicability for any real-world application. Prior work by Katz et al. is evaluated against optimized binaries, as are other types of lifting such as the Helium project by Mendis et al [1]. Given the prevasiveness of optimization in deployed code, a tool that cannot handle it has virtually no applicability.\n\nI think some significant technical novelty could make up for the lack of evaluation against optimized binaries. Alternatively, strong results on optimized binaries would justify publication even if the technique is built from existing building blocks, but as it stands, I think the paper is too incremental to merit acceptance. \n\n[1]  Charith Mendis, Jeffrey Bosboom, Kevin Wu, Shoaib Kamil, Jonathan Ragan-Kelley, Sylvain Paris, Qin Zhao, Saman P. Amarasinghe:\nHelium: lifting high-performance stencil kernels from stripped x86 binaries to halide DSL code. PLDI 2015: 391-402\n"}, "review": {"2qLxauzOcoZ": {"type": "rebuttal", "replyto": "1x4ihnzcUWY", "comment": "We have updated our paper. Please refer to the thread \u201cRevised Paper Uploaded\u201d for a summary of major updates in our revision. Following is a detailed response to Review #3 comments.\n\n1. **Training/evaluation time**:\n \nThe training speed is ~41.6 samples/s (single V100) so training each epoch takes ~7 mins averaged across datasets. The inference is 91.9 samples/s. In practice, the training is parallelized across nodes using torch distribution data parallel interface.\n\nFor binary analysis tasks using only the encoder (Appendix G), the training speed is 228.3 steps/s (single V100), and each epoch of training 22000 data points (size of POJ-104) takes ~1.4 mins.\n\n2. **Explanation of \u2018make code generated look like human-written code\u2019**:\n\nWe admit that there is no mathematical proof showing that our code is closer to a human-written one.\nTraditional code generators aim to debug compilers, so it involves a lot of complicated bit operations and does not have many diversities in the control flow. As such, we believe their codes are hard to interpret and do not match the diversity of human-written codes. \n\n**(i)** We add uniform distributions in sampling b_depth, b_size, b_num, and E_c (Poisson) as shown in Table 1 to cover more potential control flows of human-written codes.\n\n**(ii)** We adjust the uniform probability of generating each operator tokens, for example, \u201c+, -,*\u201d are more prevailing than \u201c^, <<, >>,%\u201d. We also adjust the probability of generating different standard library functions (some functions are more commonly used than others). \n\n3. **Why do we disable compiler optimization?**\n**(1)** We disable the optimization for a fair comparison, as previous works [1,3,30] in the paper also disable optimizations. \n\n**(2)** Many optimizations change variable types and rewrite source codes. As such, the changed correct outputs will result in unfair accuracy evaluation. \n\nWe believe that this is the first step to tackle this hard issue as the optimization destroys the semantics. We will leave the decompilation of more complicated configurations as future work.\n\n4. **Explanation of Assembly graph (Figure 1)**:\n \nRegarding Figure 1 assembly graph, we update it and show the complete graph in Appendix I (finished in revision). We did not show all the nodes in Figure 1 due to page limits, and we showed only the instructions colored in green. In the training/evaluation, all the instruction nodes will be used. \n", "title": "Response to Review #2"}, "jeiN0DEhGIH": {"type": "rebuttal", "replyto": "ezglM0J2B77", "comment": "We have updated our paper. Please refer to the thread \u201cRevised Paper Uploaded\u201d for a summary of major updates in our revision. Following is a detailed response to Review #1 comments.\n\n1. **Novelty**: \n\nPlease refer to the thread \"Revised Paper Uploaded\" above for our description of the novelty. We also add a summary of our contributions to the introduction of our revision. We emphasize our contributions of (1) new architecture and representation using domain-specific knowledge. (2) system integrating and tailoring existing NN components for special structural and long translation tasks. (3) thorough analysis and insight in encoder/decoder (4) configurable dataset. \n\nN-Bref also shows strong performance in other binary analysis tasks (Appendix G).\n\n2. **Metric issue**: \n\nWe show the comparison results in graph edit distance (for string matching as you suggested) without any constraints on prediction in Appendix F in revision. N-Bref shows a 40.4% reduction on average in graph edit distance compared to traditional transformers. The sequence generation manner of the traditional transformer fails to prevent error propagation at the early stage of the prediction.\n\nTwo types of errors may occur in the program generation phase: structural errors (non-terminal-> terminal, terminal->non-terminal), and non-structural ones (terminal->terminal, non-terminal->non-terminal). \n\nToken accuracy directly reflects the precision of program structure recovery, since the predicted AST and the ground-truth one are represented as ordered and structural objects. As such, we believe token accuracy is a proper metric to measure the practical performance of binary decompilation. \n\nWith the guidance of the token accuracy on AST generation, N-Bref achieves a high structural recovery precision. In particular, on average across the datasets, only 0.977% of the tokens show structural errors compared to the average error rate (8.92%) of N-Bref. \n", "title": "Response to Review #1"}, "rHqz4Kgx__n": {"type": "rebuttal", "replyto": "XF8c0MoC-1", "comment": "We have updated our paper. Please refer to the thread \u201cRevised Paper Uploaded\u201d for a summary of major updates in our revision. Following is a detailed response to R4 comments.\n\n1. **Novelty**:  \n\nPlease refer to the thread \u201cRevised Paper Uploaded\u201d above for our description of the novelty. We also add a summary of our contributions to the introduction of our revision.\n\n2. **Tone of Presentation**:\n\nWe have improved the presentation and toned down the statements, as you suggested. We revise the description of our task to be \u2018binary decompilation\u2019. We omitted the quantitative results in the abstract of our original submission due to the page limit. The evaluation results are added in the abstract of our revised paper. \n\n3. **Dataset size**:\n\nWe thank the reviewer for pointing out the Hoppity paper. Hoppity\u2019s dataset (~300K) focuses on fixing bugs; thus, each bug between commits is a point for training. For our translation tasks,  each AST node is a point for training. The average size of AST generated is around ~130 nodes (see Appendix F). Therefore, the total size of our training points is 25000*160 = 400K, which is comparable to Hoppity\u2019s dataset and is sufficient for program translation.\n\nMoreover, our dataset size is similar to the ones used in other program translation/analysis tasks: for program translation employs 100K random generated data pairs but shorter code snippets compared to ours [i]; for vulnerability detections operates on ~12K data points for each dataset, which is sufficient to obtain high accuracy [ii].\n\nFurthermore, our token accuracy converges on 25K data points and shows sufficient generalization to other programs (see discussion of \"Capability of generalization\" below). We found that increasing the number of data points (50K and 70K) will increase accuracy by 0.91%/1.31% on average across datasets. However, doing so will significantly increase training time and computer memory consumption.\n\nFinally, our framework provides a data generator. Therefore, users can easily generate more data if needed. Data is very cheap in our case.\n\n4. **Capability of generalization**\n\nOur dataset is randomized in multiple dimensionalities (Table 1). It can cover the majority of programs within the generation range.\n\nTo justify the generalization capability of N-Bref, we further test the token accuracy of our model on the POJ-104 dataset [iii]. This is a dataset of human-written code that has a different distribution from our LeetCode test dataset. The N-Bref model trained on our math (2,2) dataset achieves 90.4% token accuracy on the unseen POJ-104 dataset.\n\n5. **Impact of N-Bref on other binary analysis tasks**:\n\nRegarding the impact of the design, our structural transformer architecture, low-level code encoding, and representations are easy for integration with various neural-based binary analysis tasks whose input is also low-level code, allowing advances in such tasks (vulnerability detection, crypto algorithm classification, malware detection, etc.) Our decoder can be leveraged in other program synthesis tasks whose output is AST.\n\nWe test N-Bref\u2019s performance on two other binary analysis tasks in Appendix F:\n\n(Task 1) Identify binary code vulnerabilities.  \n(Task 2) Measure binary code similarity. \n\nThe performance of N-Bref on these tasks is also better than baseline transformers (by 3.0%/3.85% on Task 1 and Task 2, respectively), even beating some traditional methods that operate on high-level source code (by 4.1%/5% on Task 1 and Task 2, respectively).\n\n**References**:\n\n**[i]** Chen, Xinyun, Chang Liu, and Dawn Song. \"Tree-to-tree neural networks for program translation.\" Advances in neural information processing systems. 2018.\n\n**[ii]** Zhou, Yaqin, et al. \"Devign: Effective vulnerability identification by learning comprehensive program semantics via graph neural networks.\" Advances in Neural Information Processing Systems. 2019.\n\n**[iii]** Mou, Lili, et al. \"Convolutional neural networks over tree structures for programming language processing.\" arXiv preprint arXiv:1409.5718 (2014).\n\n", "title": "Response to Review #4"}, "bkY8wnv-0ic": {"type": "rebuttal", "replyto": "6GkL6qM3LV", "comment": "We thank the reviewers for their valuable feedback. Major concerns include the representation, a need for detailed descriptions of our evaluation metric, unclear description of novelty, and small dataset size. We uploaded a revised version according to review comments. Following is a summary of our major updates (less major updates are described in response to each reviewer).\n\n1. We tone down as suggested by reviewers in the abstract, introduction, and conclusions. We add the performance results back into the abstract and introduction.\n2. We update Fig. 1. We also add a complete assembly graph in Appendix I. We clarify the usage of DT-Solver and SC-Gen in a new subsection (Sec.2.3, page 4). \n3. We add two more new binary analysis tasks -- binary similarity check and binary vulnerability detection -- using the low-level code representation method and NN structure proposed by N-Bref in Appendix. G.\n4. We update the description of our evaluation metric and present the result using a new metric (graph edit distance) in Appendix. F.\n5. We fix other minor comments brought up by the reviewers.\n\n\n**Novelty**: We make the following new contributions:\n\n**(1) Models** A new structural transformer architecture with several new design principles that have not been explored in previous works: Existing transformer architectures are designed for conventional NLP tasks. The binary decompilation task that we target to solve involves long and highly structural input and output, which is more challenging than the NLP task.\n\nTo explore the intrinsic properties of low-level code input, we leverage domain-specific knowledge to optimize transformer design with memory and graph augmentation, disentangling data types recovery and AST generation. Moreover, we carefully design the representation of the low-level input and high-level code output as a graph and AST, respectively. Both the graph and the AST are suitable for N-Bref\u2019s graph embedding and augmentation. Finally, we use hardware architecture knowledge, which has not been well explored in previous works, to develop a more efficient tokenizer for low-level code.\n\nFurthermore, the above design optimizations can be easily adapted to other binary analysis tasks (Appendix F). We justify the effectiveness of N-Bref\u2019s novel design compared to traditional transformers in Table 2, 5 (decompilation) and Table 6, 7 (new binary analysis tasks) in Appendix F.\n\n**(2) System Integration**: We construct an end-to-end decompilation system by integrating the data generator / LLC Encoder / AST encoder and decoder in a holistic manner. Such integration has not been demonstrated in prior works. We bridge the gap between low-level and high-level code by transforming both into a graph space; these graph representations preserve syntactic and semantic information in the code.\n\n**(3) Insights on encoder/decoder design**: We perform a comprehensive analysis of encoder/decoder design for decompilation tasks (Fig.4). Our analysis shows that the structural encoder/decoder has higher tolerance with increasing code length and complexity, compared to the na\u00efve transformer or traditional methods.\n\n**(4) New dataset**: Our configurable datasets uniform the code generation (removes Expression Collisions).\n\n* The explanation of dataset size is in the response to Reviewer #4\n", "title": "Revised Paper Uploaded"}, "bQmOsQMhhiF": {"type": "rebuttal", "replyto": "_H6wrgteIwo", "comment": "We have updated our paper. Please refer to the thread \u201cRevised Paper Uploaded\u201d for a summary of major updates in our revision. Following is a detailed response to Review #2 comments.\n\n1. **Clarification on the confusions**:\n\nWe update Figure 1 and fix the error. In Figure 1, the second node should be \u2018int_ptr\u2019 (fixed in revision). . We add Sec 2.3 in the paper to explain the detail of DT-Solver and SC-Gen.\nDT-Solver only generates only the left part of the AST tree. The output of DT-Solver will be the initial tree input for SC-Gen. SC-Gen continues the expansion from the intermediate result of DT-solver. The DNN structure of DT-Solver is identical to SC-Gen. \n\n2. **Metric**:\n\nWe update the description of our evaluation metric and report the result of evaluation using edit distance without constraints on the structural expansion (see Appendix F). N-Bref shows a 40.4% reduction on average in graph edit distance compared to traditional transformers.\n\nFor token accuracy, we expand the tree into the original structure. If the structural error occurs (e.g., non-terminal-> terminal nodes), we continue expanding the tree into the same structure. Note that structural error rarely happens. In particular, only 0.977% of the tokens show structural errors compared to the average error rate (8.92%) of N-Bref (on average). We thought accuracy is more interpretable than edit distance. \n\n3. **Observations on error types**:\n\nMost of the errors are flipped tokens between non-terminal operators. For example, (1) \u2018+\u2019 and \u2018-\u2019 are confusing because \u2018-\u2019 can also be compiled into \u2018add\u2019, shr, \u2018shl\u2019 instructions depending on the variable type and if the variable is constant. (2) predicting the wrong variable id (e.g., var_1 -> var_0) is also a common error, because analyzing the numerical address offset to figuring out the variable id is difficult. We will leave a more thorough error analysis as future work.\n\n4.  **Regarding novel positional encodings method mentioned**:\n\nWe found the implementation of [i] as a function implemented here (https://github.com/microsoft/icecaps/blob/master/icecaps/util/trees.py). We tried out the encoding method on the AST encoder, which does not show performance improvement compared to our positional encoding method. (discussed in the Evaluation section). Note that this method still generates the output in a sequencing manner, which does not prevent error propagation like tree expansion.\n", "title": "Response to Review #3"}, "ezglM0J2B77": {"type": "review", "replyto": "6GkL6qM3LV", "review": "\nSummary:\n\nThe paper designs a neural architecture (called N-bref) for code decompiling, i.e., translating binary code to high-level code (e.g., C/C++). \n\nExperiments show that a high token-level accuracy on some LeetCode dataset, compared with a few baseline methods including Seq2Seq and Transformer.\n\n\nMajor concerns: \n\n1. The paper does not appear to be novel. \n\nThe paper identifies 4 challenges for decompiling, but some of them are generic, such as long-dependency problems and data augmentation. For the other two challenges (datatype and control/dataflow), the paper proposes to decompose the generation into two subtasks: source code generator (SC-Gen) and data type solver (DT-Solver). \n\nTechnically, both SC-Gen and DT-Solver are modeled by the same neural architecture (but differently parametrized): a memory-augmented structural transformer. This appears to be a ragbag of existing and known models: Transformer [Vaswani et al., 2017], Tree Transformer [Sun et al., 2020], and Memory Augmentation [Cornia et al., 2020]. I do not feel this paper very exciting. \n\n2. The evaluation metric is problematic (or at least unclear).\n\nThe performance of a model is measured by \"token accuracy\" when the authors \"expand the decompiled AST from the root into the same structure as the golden AST (AST_G).\"\n\nIf this is what the authors meant, a real program is never generated. The authors assume a correct program is there, and predict the next AST token/rule assuming previous partial AST is the same as AST. \n\nSuch measure of success could be drastically different from the real performance of generation. The authors should consider string match of the generated program compared with reference, or the functional accuracy of the generated program. \n\nWith token accuracy given the correct partial AST, I would not agree with the claim that \"N-Bref successfully reverts human-written\" programs.   \n\nMinor:\n\nFor Ins2AST [Fu et al., 2019], did you use Ins2AST+Attention? That seems to be better than Ins2AST (w/o attention). \n\nDecompilation tasks is --> Decompilation task is\n", "title": "architecture design", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "1x4ihnzcUWY": {"type": "review", "replyto": "6GkL6qM3LV", "review": "The paper is interesting and addresses an interesting topic. The results seem promising, though the evaluation is done on relatively small test cases. \n\nThe proposal to divide the problem into two sub-tasks, i.e., data type solver and source code generation, is promising and can probably have impact on future decompiler proposals. \n\nThe main problem with the paper is that it is hard to read and understand. I'm aware of the page limitations, but the authors have crammed so much inte these pages that is hard to follow. Further, there are also a number of inconsistencies and unclear stuff (see below). For example, the authors claim that one thing their proposed method extends beyond earlier methods is to handle pointer references. However, I can't find in the paper how that is done. \n\nThe usefulness may be limited of this work, since they only work on unoptimized code (see e.g., page 4, 2nd paragraph). However, in reality, most code have went though substantial optimization during the compile phase.\n\nSome other comments / questions:\n* I lack information about the execution time of the training and inference. \n* Page 2, 2nd paragraph. You claim that your code generator produces similar code styles as human programmers. What do you mean by that, and how du you support that claim?\n* Fig 1b. Here are a number of strange / confusing things:\n  - Why are not all asm instructions shown in the data flow graph? For example movl (line 2), call (line 4), testq (line 5), etc. are missing.\n  - Why do you show one movss (line 9) and not the other movss (line 7)?\n  - You have mixed up lines 7 and 8 in the graph (show it as mulss -36(rbp),xmm0), which is confusing.\n", "title": "The paper proposes a neural network based decompilation infrastructure for translation of assembly code to high-level source code. The paper seems promising, presents interesting ideas and promising results but is unfortunately hard to follow and fully understand. ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "_H6wrgteIwo": {"type": "review", "replyto": "6GkL6qM3LV", "review": "The authors present a neural-based decompilation framework. They generate synthetic input data in order to make sure source code examples have a consistent code style that can be more easily learned. They predict types of variables and the actual code in two separate steps. For both steps, they employ a custom transformer architecture where evey second layer of the encoder is a graph neural network. There are two separate encoders of this kind, one conditions on a CFG obtained from static analysis of the input assembly, while the other one conditions on the partial output AST that has been generated thus far.\n\nStrengths:\n- The authors propose an end-to-end system for neural decompilation\n- Interesting use of graph neural networks to increase sensitivity to structure in a transformer.\n- Favourable comparison to multiple baselines.\n- Evaluation also considers a human-written dataset.\n\nWeaknesses:\n- It is not so easy to fully understand the approach end-to-end. Perhaps the presentation can be improved. (Though I understand that it can be challenging to fit an explanation of an ambitious approach with multiple novel components into 8 pages.)\nWhen reading the paper, it happened to me a couple times that I tried to go back to some piece of information and I did not find it at the location where I would expect to find it. Some details are discussed in the introduction, but apparently nowhere else, for example how the output of the DT-Solver is used. It would help to reorganize the paper a bit so that the exposition follows the order of operations when running the approach and to discuss which data goes where in the technical sections.\nThe paper says that DT-Solver and SC-Gen are both based on the same architecture, but DT-Solver is not really discussed in detail. As an example, it is not stated if the types of variables are chosen from some fixed set (which one?) or if the DT-Solver generates a type AST, but Figure 1 suggests a fixed set. Figure 3 is helpful, but it seems it does not show the full story for either DT-Solver or SC-Gen. Figure 1 is a bit confusing, as the shown AST does not appear to match the given source code. (E.g., there is no variable of type `int *`), and variable declarations are shown as part of a single AST of the program even though later they are treated separately.\n\n- The evaluation metric is explained rather vaguely, so I am not sure if I fully understand what is meant, but this is crucial to interpret the results. How do you \"expand the decompiled AST from the root into the same structure as the golden AST\"? What happens during expansion if a token does not match? Is the subtree removed? I guess after the expansion step, you compare AST nodes that end up at the same position in the tree?\n\n\nFurther questions:\n\nAccuracy based on syntax comparison to synthetically-generated input examples is not necessarily what an end user cares about. How well do your decompilation results preserve semantics? I.e., if token accuracy is imperfect, what kinds of mismatches do you typically get? It would also be interesting to understand a bit better the distribution of the results, e.g., how do the results change if you count the fraction of results with perfect token accuracy instead of computing averages over token accuracy?\n\nAs far as I understand, the positional encoding for ASTs drops a lot of structure information, which is then recovered by the GNN layers. Have you considered using richer positional encodings along the lines of [i]?\n\n[i] https://www.microsoft.com/en-us/research/publication/novel-positional-encodings-to-enable-tree-based-transformers/\n\n\nMinor:\n\nPage 1: \"learns to decompile the source code to assembly\". That seems backwards.\n\nConsider using \\citep and \\citet.\n\nPlease review your paper with a focus on grammar as well as whitespace and other formatting issues.\n(For example, you should use ``$\\mathit{xmm0}$`` instead of ``$xmm0$`` ,`` `control flow'`` instead of ``'control flow'``, etc.)\n\nPage 14: There is wrong indentation or missing curly braces next to the \"continue\" statement in Figure 3.\n", "title": "Nice results, somewhat chaotic presentation", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "XF8c0MoC-1": {"type": "review", "replyto": "6GkL6qM3LV", "review": "High-level view:\n\nI don\u2019t think this is necessarily a bad paper, but I think it\u2019s unacceptable for ICLR in its current form. I currently lean heavily toward rejection. After thinking over the concepts in the paper more, I might lean more strongly toward rejection or toward acceptance (if the authors can address the issues I raise below). I provide details below examination of how I\u2019ve come to my evaluation rating below.\n\nSummary:\n\nThis paper principally focuses on the idea of decompilation. Decompilation can mean many things, but the general idea as I understand it, is to take a representation of a software program from one level (e.g., program binary) and then \u201clift it\u201d to a level that is higher in abstraction (e.g., from binary to assembly, from assembly to C, from C to a lambda calculus, etc.). As I understand it, it\u2019s called decompilation because it tends to do the opposite of what a compiler does. Compilers tend to lower a representation of a software program into something that is closer to the hardware and therefore potentially more efficient. \n\nThe benefits of decompilation are numerous. One major benefit is in the ability to perform programming language \u2013 to \u2013 programming language transformation. Another, which is the focus of this paper, is for reverse engineering purposes of a binary. There are many others. As such, in my opinion this is unquestionably an important subtopic for the field of machine programming and the authors approach also seems satisfactory to me for ICLR (described below).\n\nThe authors present a new approach called: neural-based binary reverse engineering framework (N-Bref). N-Bref has a number of components that it relies on to perform its decompilation. They consist mostly of components from the programming languages community (e.g., assembly code, abstract syntax trees for encoding and decoding, etc.) and the machine learning community (e.g., deep neural networks for learning structural transformations, etc.).\n\nThe authors empirically evaluate their N-Bref\u2019s accuracy on a number of problems from the open source LeetCode problem set and generate 25,000 pairs of high-level source and low-level source which are broken into training (60%), validation (20%), and testing (20%). LeetCode problems tend to be fairly simple, self-contained, and, to my knowledge, are coding problems that are meant to help train new programmers or prepare software developers for coding interviews, amongst other things. An emerging use of LeetCode is to use it as a baseline for machine programming (MP) in a variety of different ways. In this case, the authors are using LeetCode coded solutions in MP to compiled the source code into a lower level form (assembly I believe) and then see if N-Bref can return the assembly back to the original form or some semantically equivalent form. Their empirical approach seems sound to me.\n\nOverall, the authors show better accuracy for their tested problem set against REWARD, a baseline system (a transformer), lang2logic, and Ins2AST across two dimensions: data type recovery and abstract syntax tree (AST) generation.\n\nHigh-level concerns:\n\nThere are several reasons I\u2019m not positive about this paper. Perhaps the biggest reason is I can\u2019t seem to understand what is novel about the system. That is, unless I\u2019ve just missed something, it seems that all of the core components of N-Bref are lifted from prior work with perhaps some minor augmentation. This feels largely incremental to me.\n\nOn the other hand, one could argue that N-Bref is novel because it combines a number of existing components in a unique way to achieve better performance that prior work. I can see this perspective. However, if we considered this view, it seems like the problem they are solving should be more impactful than type recovery and AST generation. I\u2019m not saying these problems aren\u2019t important \u2013 especially type recovery (I think this problem is deeply important) \u2013 but that it should go further to demonstrate more dimensions of decompilation.\n\nThe second major concern I have with this paper is the small dataset they are using. Consider, for a moment, that they are using only 25,000 input/output pairs for their training/validation/testing. Now consider a prior accepted ICLR 2020 paper, Hoppity (Dinella et al.), which trained on nearly 300k code change commits in GitHub. This looks like an order of magnitude difference in dataset empirical evaluation to me. On top of that, the only data is coming from LeetCode. We have no empirical demonstration that this approach will work on other datasets outside of LeetCode.\n\nIf the authors can address these two primary concerns by the time of decisions, I will likely slant toward the positive. If they do not (or will not), I will likely champion this paper\u2019s rejection, as I do not believe in its current form it\u2019s up to ICLR standards.\n\nLow-level concerns:\n\nThe language in the paper seems to use many strong and ambiguous claims: \u201cN-Bref outperforms previous neural-based decompilers by a large margin.\u201d First of all, what is a \u201clarge margin\u201d? There\u2019s not quantitative measurement in the word \u201clarge\u201d. Large could mean 1%, 10%, 100,000%. This kind of language is not what I expect from tier-1 publications.\n\nAnother example is: \u201cHowever, key challenges remain:\u201d where they then summarize two problems. I agree that the two problems they highlight are important. But I absolutely do not agree that those are the *only* two problems that stand in the way of decompilation.\n\nAlso, there seems to be some lack of understanding of the field of machine programming, from my perspective. For example in the abstract the authors claim \u201cdecompilation aims to reverse engineer binary executables.\u201d I 100% disagree with this definition. As I stated above, I believe, the more general space of decompilation is actually the idea of lifting a software program representation from one format to a higher-level format that increases the level of abstraction from the hardware. Moreover, I know of many decompilation systems (e.g., verified lifting is one), that has an entirely different goal than reverse engineering. Verified lifting is principally focused, as I understand it, is focused on language to language translation.\n\nPerhaps the grossest overclaim the authors make is in the introduction \n\n\u201cOur work pushes the performance of neural-based decompiler to a new level and presents a new baseline and stand dataset for future developers.\u201d \n\nI find that sentence simply unacceptable. I could never give an accept rating to a paper that makes such an outlandish claim with such a small body of evidence. Moreover, other people have used LeetCode as a baseline, so it\u2019s not the first time people have done this. So it seems wrong to me on many levels.\n\nThis continues throughout the paper \u2026\n\nThat said, these are minor nits that the authors, if they so choose, could probably fix with little effort.\n\nI would hope that in a later version of the paper the authors would tone the language down, move away from the number of strong claims they make in the paper, and provide measurable data points when making claims about performance: \u201cOur system is more accurate than <list the systems you\u2019re comparing against> from X% to Y%.\u201d Right now the only way to figure that out seems to be to deeply study the experimental evaluation, which is a bit inappropriate in my opinion. I believe it could (and should) be listed directly in the abstract and in the introduction. By hiding these details, it creates a perception of overclaiming \u2013 at least it did for me.\n", "title": "Review of N-Bref: a high-fidelity decompiler exploiting programming structures", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}