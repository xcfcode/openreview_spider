{"paper": {"title": "Meta-Active Learning in Probabilistically-Safe Optimization", "authors": ["Mariah L Schrum", "Mark Connolly", "Eric Cole", "Mihir Ghetiya", "Robert Gross", "Matthew C. Gombolay"], "authorids": ["~Mariah_L_Schrum1", "mark.connolly@emory.edu", "ercole@emory.edu", "mihir.v.ghetiya@emory.edu", "~Robert_Gross1", "~Matthew_C._Gombolay1"], "summary": "We present a probabilistically-safe, meta-active learning approach to efficiently learn system dynamics and optimal system configurations based on an LSTM encoding of sample history.", "abstract": "Learning to control a safety-critical system with latent dynamics (e.g.  for deep brain stimulation) requires judiciously taking calculated risks to gain information. We present a probabilistically-safe, meta-active learning approach to efficiently learn system dynamics and optimal configurations. The key to our approach is a novel integration of meta-learning and chance-constrained optimization in which we 1) meta-learn an LSTM-based embedding of the active learning sample history, 2) encode a deep learning-based acquisition function with this embedding into a mixed-integer linear program (MILP), and 3) solve the MILP to find the optimal action trajectory, trading off the predicted information gain from the acquisition function and the likelihood of safe control. We set a new state-of-the-art in active learning to control a high-dimensional system with latent dynamics, achieving a 46% increase in information gain and a 20% speedup in computation time.  We then outperform baseline methods in learning the optimal parameter settings for deep brain stimulation in rats to enhance the rats\u2019 performance on a cognitive task while safely avoiding unwanted side effects (i.e., triggering seizures).", "keywords": ["meta-learning", "active-learning", "safe learning"]}, "meta": {"decision": "Reject", "comment": "This paper was quite contentious.  While reviewers appreciated the detailed response by the authors, and there is consensus that the paper addresses a relevant problem and contains interesting ideas, in the end there remain several concerns.  The paper provides a complex combination of techniques from active learning, meta learning and symbolic reasoning (via MILPs), and there are concerns about the clarity of the exposition.  For a paper claiming safety properties, there is also a lack of either formal theoretical analysis of well-specified safety properties, or a compelling demonstration of its effectiveness on a real system (all experiments are carried out in simulation)."}, "review": {"DPf2Wj2qtou": {"type": "rebuttal", "replyto": "jtaUS0D2VDQ", "comment": "We would like to thank the reviewer for the reviewer's insightful comments and helpful feedback in improving our paper. Thank you!", "title": "Thank you"}, "a9KOhsCa4A9": {"type": "rebuttal", "replyto": "IuEtOAetbBT", "comment": "We would like to thank the reviewer for the reviewer's insightful comments and helpful feedback in improving our paper. Thank you!", "title": "Thank you"}, "nJcA4aMsBQO": {"type": "rebuttal", "replyto": "TRFgf94PeS_", "comment": "We hope that our responses to the reviewer\u2019s critiques relieved the concerns the reviewer had. We sought to address the reviewer\u2019s comments regarding the quality of our simulations, the ability to adapt to new environments, safety and novelty. We would like to ask the reviewer if there are any further clarifications/revisions they would like to see that might improve our score. Our discussion period ends today so this is our last opportunity to provide further clarifications and changes to our paper to satisfy the reviewer\u2019s concerns", "title": "Follow up "}, "CQl8tnESLU6": {"type": "review", "replyto": "oBmpWzJTCa4", "review": "Summary: \nThis paper presents a meta-active learning approach to obtain an LSTM-based embedding of a dynamic system and use a chance-constrained (probabilistic safe) optimization to find optimal control configuration via applying mixed-inter linear programming (MILP) on the learned embeddings of the dynamic system. The main idea is to learn a Q-function as an acquisition function to describe the future information gain, which is the percent decrease in the error of the objective (e.g. model error) via a meta-learning strategy in which the agent interacts with the environment via distribution of altered dynamics. \n\nStrength: \n-  To my best knowledge, the idea that combines an LSTM embedding of dynamics with mixed-integer programming for optimization is novel meta-active learning algorithms. \nWeakness: \n- The algorithm relies on the assumption that the safety region must be hyper-ellipsoid with a known radius and the model error must come from a Gaussian distribution with known mean and variance. This makes the algorithm hard to apply for problems when the safety region is unknown or changing over time, which happens often in most real-world problems. \n- The poor writings make it hard to access the correctness of the technical details. \n\nQuestions and comments: \n1) In Algorithm 1 line 11, should the subscript \u201cT\u201d be \u201ct\u201d? should the index \u201ci\u201d starting at line 7 be \u201ct\u201d and all the related subscript \u201ci\u201d be \u201ct\u201d instead?  \n2) In Eq (1), should the subscript of U be \u201ct:t+T\u201d instead of \u201ct:T\u201d? If keeping \u201ct:T\u201d, the sentence above Eq (1) that states \u201cthe trajectory from [t, t+T)\u201d should be \u201c[t, T]\u201d. Alternatively, the author could formally define what \u201ct:T\u201d means. Also, in the first paragraph of section 3.2, it\u2019ll flow better if the authors could give some description of the second part on the right-hand side of Eq (1). \n3) Eq (2) is a bit hard to understand. What is \\Delta_d^{t:2}? How is it set in practice? On the fourth line of the first paragraph on page 5, the paper states \u201cand \\bar{a} and \\bar{b} are the point estimates of dynamics\u201d, what is \u201c\\bar{a}\u201d and \u201c\\bar{b}\u201d?  The next sentence \u201cd represents the d-th row and j the columns\u201d is confusing. \n4) Eq (2), the first term under the square root has x_j^{(t)}^2, should the underscript \u201cj\u201d be \u201cd\u2019\u201d instead? \n4) In the ADMETS experiment, the objective is defined as the L1 norm of the predicted optimal parameter and the ground truth optimal parameter;  how is the ground truth optimal parameter obtained? If the ground truth optimal parameter is already available, what\u2019s the need for running the algorithm since the goal is to find the optimal parameters of the configurations? \n5) In the Aircraft experiment, the objective function is defined as the mean squared error of the model (see Appendix A.2), what is the model here? What is \\hat{x}_i and x_i? \n6) In Appendix (A.4.2), what is the E in equation 13? \n7) Page 8, \u201cMeta-learning for Dynamics\u201d, line 6, \u201c\u2026 most conducive \u2026\u201d should be \u201c\u2026 most conductive \u2026\u201d, line 7, \u201cWhile this approach provide .. \u201c should be \u201cWhile this approach provides \u2026\u201d \n8) Eq (20) becomes 0 >= \\prescript {o_i}  >= 1 when k_i =1, this will never be satisfied. \n ", "title": "An interesting paper that combines LSTM and mixed integer programming for meta-active learning but suffers from poor writing ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "OfUm684HKEg": {"type": "rebuttal", "replyto": "PQgxSoLHtI5", "comment": "We appreciate the reviewer\u2019s feedback and we would like to thank the reviewer for their reply. Our approach to learning a representation of a sampling history and state that can be encoded into a MILP and optimized over is entirely novel. It is a groundbreaking way to connect mixed-integer linear programming/symbolic optimization and Deep Learning and, to the best of our knowledge, has never been done before. This novel integration, importantly, allows us to impose chance-constraints and ensure system safety which is an advantage over previous approaches in safety critical domains. We believe that our learning representation opens up new and exciting opportunities to bridge connectionist and symbolic approaches.", "title": "Reply to Reviewer 3"}, "L9OC09CBJdV": {"type": "rebuttal", "replyto": "CwbnpiFFA0o", "comment": "We would like to thank the reviewer for their time invested in our paper and for their comments about the importance of our work in safety critical domains. For your convenience, we have highlighted all changes in red in the updated paper. Below we address the concerns expressed in the review.\n\n*[R3] The approach is a combination of existing techniques, thus the technical novelty is limited*\nVirtually every paper published at ICLR this year will be a \u201ccombination\u201d of one or more techniques. The vast majority of research consists of finding novel connections between different techniques that will allow one to create something that is greater than the sum of its parts. Yes, we employ LSTM models, chance-constrained programming, and Bellman\u2019s Equation, which have all been established and well-studied. Nonetheless, to the best of our knowledge, our paper is the very first to show that an LSTM can encode a powerful representation of an active learning sampling history that can be directly leveraged in a chance-constrained mixed-integer program to optimize for information gain vs. safety. Furthermore, ours is the first to show than an acquisition function can be meta-learned for the purpose of the optimization and identification of a latent system. We appreciate the reviewer\u2019s feedback that the novelty could be more clearly articulated, and we now do so in Section I.\n\n*[R3] The paper is more about solving a robotics problem*\nWe argue that our paper is about solving meta-active learning in a chance-constrained optimization framework. While we do demonstrate our algorithm in robotics domains and a healthcare domain, the primary focus of our paper is the development and validation of a novel, meta-active learning approach that achieves state-of-the-art results against machine learning and Bayesian optimization baselines. \n\n*[R3]  ICLR may not be the best venue*\nWe argue that our work is well-suited to ICLR due to the fact that a critical component of our method is its ability to meta-learned a representation of sample history which can encoded in a mixed-integer linear program. We believe that our work fits nicely under the following relevant topics listed in the ICLR guidelines: \n\u2022\tunsupervised, semi-supervised, and supervised representation learning\n\u2022\trepresentation learning for planning and reinforcement learning\n\u2022\tlearning representations of outputs or states\n\u2022\tapplications in audio, speech, robotics, neuroscience, computational biology, or any other field\n\nWe hope this response addresses your concerns. Please let us know if you have any further comments or concerns or ways in which we can further improve our paper\n", "title": "Response"}, "wmSoys-ikPP": {"type": "review", "replyto": "oBmpWzJTCa4", "review": "The authors proposed a meta-learning approach for active learning in the context of robot control.\n\nWhat I like about this work is that there're several illustrations that make it clear which problem setup it is or what the authors meant by safety. The authors also included several competing methods, and the experiments showed promising results both in terms of several metrics, including information gain and safety. \n\nHowever, I also have some concerns about this paper that needs to be addressed.\n\nIn the experiments, there are 5 methods chosen to be compared against. However, several of them are not even mentioned before or after in introduction or related work to explain their close connections to the proposed one in more details. For example, Schrum & Gombolay 2020 and Wang et al., 2018b. And that makes me doubt whether it's a poor selection of comparisons or omission of important details in writing. \n\nAgain in experiments, the method names described in 4.3 don't match the ones shown in figures. What's Random Bayesian in Figure 4? Why it disappeared in Figure 4(b) and 4(c)? Where's Epistemic Uncertainty and Maximizing Diversity? Similar for Figure 5, it was said that only Meta BO is excluded but where are the others? And even for Meta BO, the paper says it is \"not included as it was designed for optimization tasks\". But why can't the task in sec 4.2 be framed as an optimization task? What I meant is, I would like to see consistent set of methods in different tasks. Or add more tasks to show generality.\n\nThere are also inconsistency in naming. Is it BaO or Bao?\n\nAbout the safety measure. It's evaluating using a point, and all parameters like minimum risk value need to be set by hand. How useful is such a safety measure? And how is it related to more common ways of measuring safety/risks, e.g. CVaR? Also the variables in Eq. (2) are not fully explained.\n\nI'm also somewhat concerned that the authors are trying to solve too many problems in one algorithm. Safety, meta-learning, Q-learning.. But the experiments only show the results of everything combined without detailed analysis of the performance of each functionality while keeping others fixed. To some extent, the nature of the design of the method also sort of forbids modular tests. In the end, it might be just another impractical approach that incrementally adds several factors together, yet it's impossible to explain when it works or why it works.\n\n", "title": "Experiment section is somewhat problematic", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "YOCt0uTgnZq": {"type": "rebuttal", "replyto": "qPNAPk6BvmQ", "comment": "We have sought to carefully incorporate all of the reviewer\u2019s feedback. Given the reviewer\u2019s latest response that we have greatly improved our notation and addressed concerns about the assumptions of the safety region, we are wondering if the reviewer would consider raising the paper\u2019s score or offering additional points for improvement?", "title": "Response to Reviewer 4 Follow Up"}, "4dFYjG56mH0": {"type": "rebuttal", "replyto": "wmSoys-ikPP", "comment": "We would like to thank the reviewer for their reviews and their comments about our illustrations.  For the reviewer\u2019s convenience, we have highlighted all changes in red in the updated paper newly attached. Below we address the concerns expressed in the review.\n\n*[R2] Benchmarks are not mentioned in the introduction or related works.*\nWe thank the reviewer for their comment, and we now include a discussion of these methods in the related works to more properly place them in context. The additions have been highlighted in red.\n\n*[R2] Not all benchmarks are included in each environment.*\nWe agree with the reviewer that we should have demonstrated each benchmark in both environments. We have included these benchmarks in both environments as reflected in the revised version of our paper. We have updated the figures and results to make this clearer and have eliminated any inconsistencies with the benchmarks.\n\n*[R2] There are inconsistencies in naming*\nWe thank the reviewer for pointing this out and we have corrected our naming conventions so that they are consistent throughout the paper.\n\n*[R2] The safety measure must be set by hand. How useful is such a measure? How does this compare to CVaR?*\nThere are two parameters that must be set by hand: A) the minimum-safety level allowed  (i.e., $p_{min}= min_{p\u2019 \\in E}  p\u2019 $, as described in the sentence preceding Equation 11), and B) the safety vs. active learning trade-off parameter, \\lambda. We agree that other measures may be interesting to consider. In the case of CVaR; however, it is nonobvious how one would perform the necessary quadrature over the product of the weighted probability distribution within a linear programming formulation. We appreciate the reviewer\u2019s suggestion and will consider value-based and other measures in future work.\n\n*[R2] Variables in Eq. 2 are not fully explained.*\nWe thank the reviewer for their feedback and have corrected this oversight.\n\n*[R2] The authors are trying to solve too many problems at once.*\nWe disagree that we are trying to solve too many problems. We developed a novel approach to active learning that meta-learns an acquisition function while also allowing for safety constraints to be imposed. We argue that each of these components is a necessary part of our algorithm. Safe active learning is highly useful and has many applications in safety-critical domains. While our algorithm can operate without safety constraints, this would be highly dangerous in the aircraft or ADMETS domain in which safety is a critical requirement. Additionally, we compare our algorithm\u2019s meta-learned acquisition function against two state-of-the-art hand engineered acquisition functions. We show that our algorithm outperforms these approaches, demonstrating that meta-learning is a necessary component for efficient learning. \n\nTo address the reviewers concerns about a lack of modular tests, we include an ablation analysis in which we remove the active learning component in our algorithm (i.e., our algorithm only maximizes for safety) and show that the active learning component of our algorithm is crucial to efficient learning. \n\nWe hope this response addresses your concerns. Please let us know if you have any further comments or concerns or ways in which we can further improve our paper\n", "title": "Response"}, "Bg6ny050FyY": {"type": "rebuttal", "replyto": "CQl8tnESLU6", "comment": "We would like to thank the reviewer for their reviews and for their time invested in the details of our paper. For your convenience, we have highlighted all changes in red in the updated paper. Below we address the concerns expressed in the review.\n\n*[R4] The algorithm relies on the assumption that the safety region must be a hyper-ellipsoid with a known radius and the model error must come from gaussian distribution with known mean and variance. This is hard to apply for real world problems in which region is unknown/changing*\nThe mean and variance of the gaussian distribution are not known a priori and are estimated online. Additionally, we proffer that our definition of the safety hyper-ellipsoid is applicable to many, real-world problems. For example, in the situation of a damaged aircraft, one could also define this region simply as maintaining a minimum altitude above ground level, which would not be a time-varying parameter. In the ADMETS domain, the system remains safe if the memory score is greater than zero. This boundary is defined within the medical community and is a constant region which does not evolve. We do acknowledge that there are scenarios in which this safety region may change. However, estimating the safety region online is an open problem outside of the scope of this paper.\n\n*[R4] The poor writing makes it difficult to assess technical details*\nWe appreciate the reviewer\u2019s feedback and have sought to improve the writing of the paper. We took care to address all comments by the reviewer including clarification of symbols and equations. The changes are highlighted in red in our revised paper\n\n*[R4] Critiques 1-4,6,7,9*\nWe agree with the reviewer\u2019s critiques and have updated our paper accordingly. Thank you.\n\n*[R4] Critique 5: How are the ground truth optimal parameters obtained and what is the need for running the algorithm if they are already known?*\nWe agree with the reviewer that the objective of our algorithm is to determine the ground truth parameters. Within our training data set, the ground truth parameters are only known and available for offline training. These ground truth parameters are obtained from the ground truth model in our simulation and are used to calculate the reward during meta-learning. However, during testing, these ground truth parameters are unavailable to the model, and the model infers a probability distribution over these parameters.\n\n*[R4] Critique 8: Conducive should be conductive*\nBased upon the definition of conductive (having the property of transferring heat or electricity), we believe that conducive is the proper choice.\n\nWe hope this response addresses your concerns. Please let us know if you have any further comments or concerns or ways in which we can further improve our paper", "title": "Response"}, "41QB_D34RE": {"type": "rebuttal", "replyto": "TRFgf94PeS_", "comment": "We would like to thank the reviewer for their thorough review and insightful critiques. For your convenience, we have highlighted all changes in red in the updated paper. Below we address the concerns expressed in the review.\n\n\n*[R1] Critique 1: Is there any justification for the quality of the simulator and translation to the real world?*\nOur aircraft simulator leverages the equations of motions derived by Watkiss (1994) and Zhang et al (2017), which have been demonstrated to have high fidelity in real-world evaluations.  We believe the strong grounding in literature is a convincing and justifiable reason to utilize this simulation approach. The ADMETS domain is based on a simulation employed in the medical community and presented by Ashmaig et al. (2018).  Based on the facts that 1) the medical community is accepting of results demonstrated in this simulation and 2) simulation results have been shown by Ashmaig et al (2018) to transfer to the real world, we believe it is a justifiable simulation. We were unfortunately prevented due to COVID-19 from conducting an in vivo demonstration of our algorithm and we acknowledge that a lack of a real world demonstration is a limitation. \n\n*[R1] Critique 2: Is there quantification on how fast/well the trained policy adapts to new environments in test time?*\nYes, Section 5 of our original submission included this result. Figures 4 and 5 show that our algorithm learns the objective faster than the baselines and outperforms baselines in terms of computational time. Please let us know if the reviewer has further questions about these results that we can clarify.\n\n*[R1] Critique 3: What is the justification for the safety achieved being a good value?*\nOur algorithm allows the user to choose a minimum level of safety. Our algorithm cannot return a solution that does not meet or exceed this safety standard. Above and beyond this minimum safety specification, our algorithm also seeks to maximize a dual-criteria objective to be even more safe if possible while seeking out additional information for active learning. For example, in the high-dimensional domain, we set the minimum safety threshold to 60%, yet our algorithm is able to guarantee at least 87% safety when maximizing for both safety and information gain.\n\n*[R1] Critique 4: The authors should further highlight the novelty of their contribution*\nOur novel computational method is the first to meta-learn an acquisition function for the purposes of system optimization. Additionally, to the best of our knowledge, ours is the only algorithm that can embed this meta-learned active learning function in a chance-constrained linear programming framework, allowing for the imposition of safety constraints. We more clearly outline the novelty of our contribution in Section I in the revised paper. We hope this response addresses your concerns. Please let us know if you have any further comments or concerns or ways in which we can further improve our paper.\n", "title": "Response"}, "CwbnpiFFA0o": {"type": "review", "replyto": "oBmpWzJTCa4", "review": "#### Paper summary:\n\nIn this work, a meta active learning approach is proposed to learn the hidden dynamics of control systems, where safety is also a major concern. The main idea lies in doing meta-learning with Q-learning, meanwhile selecting safe actions by solving a mixed-integer linear programming problem. The performance of the proposed approach is verified from real datasets of deep brain stimulation by outperforming existing baselines for a significant gap in both accuracy and computational complexity.\n\n#### Advantages:\n\n- The paper discusses a very meaningful problem. Active learning with safety constraints is very important in many applicational domains.\n\n- From the experiments, the proposed approach shows very significant performance in both the deep brain stimulation and the dynamics recovering tasks.\n\n#### Disadvantages:\n\n- From the perspective of machine learning, the proposed approach is a combination of existing techniques, thus the technical novelty is limited. \n\n- The paper is more about solving a specific robotics problem, which may not lie in the major focus of the ICLR conference.\n\nOverall evaluation:\n\nOn one hand, I think the paper discusses a very meaningful problem, while on the other hand, I think ICLR may not be the most suitable venue to publish this work.\n", "title": "the paper is well-motivated and the proposed approach performs well empirically, while the paper is more about a specific application and somehow out of the scope of ICLR", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "TRFgf94PeS_": {"type": "review", "replyto": "oBmpWzJTCa4", "review": "The paper proposes a framework that learns the dynamic of safety-critical systems. The framework makes use of ideas from active learning, meta-learning, and reinforcement learning. As far as I understand, the framework seeks to learn an acquisition function that can evaluate the information gain from a certain action taken at a certain state. Acquisition function is a function that identifies an action the agent desires to take from a collection of unvisited states. The learning of such an acquisition function is formulated as a reinforcement learning problem, trained in a meta-learning fashion. I have the following comments:\n\n1. The proposed method is evaluated on two problems, ADMETS and damaged aircraft.  Both problems are solved using synthetic data. While it is understandable that the framework is trained on simulated data, is there any justification with respect to the quality of the data simulator used in these two problems? Why performing well on these simulated data would be an indication of the proposed method can work reasonably well in practice?\n\n2. Since the proposed framework is trained in a meta-learning fashion, is there any quantification on how fast/well (e.g. wrt. sample complexity) the trained policy adapting to new environments in test time?\n\n3. While the paper takes safety during the learning of the dynamics as a critical issue, is there any justification with respect to the percentage of safety achieved by the proposed method? For example, why an 87% safety reported in Figure 5 achieved by the proposed method is a good number?\n\n4. The key components of the proposed framework (e.g. meta-learning, reinforcement learning, active learning, safety) make sense to address the technical challenges in learning system dynamics in safety-critical domains. It is not very clear to me if the novelty of the proposed framework is appropriately highlighted. The paper can benefit from a better highlight of the novelty of the proposed method and why such novel contributions matter.", "title": "A Complex Framework that Deals with Safety-Critical Problems", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}}