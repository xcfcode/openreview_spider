{"paper": {"title": "FTSO: Effective NAS via First Topology Second Operator", "authors": ["Likang Wang", "Lei Chen"], "authorids": ["~Likang_Wang1", "~Lei_Chen7"], "summary": "Our method, named FTSO, reduces NAS's search time from days to 0.68 seconds while achieving 76.42% testing accuracy on ImageNet and 97.77% testing accuracy on CIFAR10 via searching for network topology and operators separately", "abstract": "Existing one-shot neural architecture search (NAS) methods generally contain a giant supernet, which leads to heavy computational cost. Our method, named FTSO, separates the whole architecture search into two sub-steps. In the first step, we only search for the topology, and in the second step, we only search for the operators. FTSO not only reduces NAS\u2019s search time from days to 0.68 seconds, but also significantly improves the accuracy. Specifically, our experiments on ImageNet show that within merely 18 seconds, FTSO can achieve 76.4% testing accuracy, 1.5% higher than the baseline, PC-DARTS. In addition, FTSO can reach 97.77% testing accuracy, 0.27% higher than the baseline, with 99.8% of search time saved on CIFAR10.", "keywords": ["Neural Architecture Search", "DARTS"]}, "meta": {"decision": "Reject", "comment": "Three reviewers have reviewed this manuscript, and they had severe reservations regarding the presentation quality and the lack of sufficient theoretical support behind empirical observations. Even after rebuttal, the reviewers maintained that the above issues are not fully resolved. Unfortunately, this paper cannot be accepted in its current form."}, "review": {"kBrCZZJrlbT": {"type": "review", "replyto": "7Z29QbHxIL", "review": "This work researches the issue of neural architecture search (NAS), which is of significance for practical applications of deep neural networks and has become an active research topic in the past several years. Many methods on NAS have been developed recently. The computational efficiency of search has been one of the obstacles for this line of research.\n\nStrengths of this work: \n\nThe key idea of this work is to decouple the search of network topology and the search of operators. By doing so, the computational efficiency can be substantially increased (say, reducing the search time from several GPU-days to less than one second), while well maintaining the classification performance or even slightly improving it. Experimental study on CIFAR10 and ImageNet demonstrates the advantage of the proposed method, especially the improvement on computational cost. The paper is overall well written.   \n\nWeaknesses:\n\n1. This work can do better on theoretical analysis. Currently, it mainly describes how the decoupled search (FTSO) is implemented (in Section 3). Considering the significance of the improvement on computational efficiency, it will be more valuable to discuss whether such a kind of decoupled search can be generally applied, or it can only work for certain kinds of network architecture, data, or tasks. This will provide more insights on this interesting method and theoretical contribution.\n\n2. This paper has described several interesting observations. It will be better if they are further explained and discussed. \n\n2-1. When conducting topology search, this paper only uses a simple operator (say, skip connection). It is indicated that this supernet can hardly overfit the dataset, which is understandable. Meanwhile, with such a simple operator, will the supernet underfit the data? This needs to be clarified.\n\n2-2. Similarly, in Section 4.1, it is stated that FTSO contains very few parameters and therefore can even achieve comparable performance to PC-DARTS. Why could very few parameters lead to this comparable performance? Please clarify it.\n\n2-3. At the end of Section 4.2, it is found that the best architecture obtained by the search is the shallowest and widest one. The current explanation (provided at the end of this Section) is too brief and vague for such an interesting and surprising finding. This need to be further clarified. \n\n2-4. The part at the top of page 7 discusses the performance with respect to iteration and epoch. It is not very clear. It is stated that \"although one iteration cannot surpass one epoch, it is better than a few iterations.\" However, considering that one epoch consists of multiple iterations, this statement seems to contradict to itself and is a bit hard to follow. Please clarify.   \n\n3. The second paragraph on page 4 compares the computational complexity of DARTS and the proposed method. Since this is the key part of this work, more details shall be provided on how the complexity is worked out. \n\n4. Minor issues:\n\n4-1. The first sentence of the second paragraph on page 2 needs to be revised.\n4-2. Although the investigation of correlation among different architectures is mentioned as the key motivation for this work, how is the correlation of architectures considered in this work is not presented. This shall be improved; \n4-3. In Section 4.2, the resolution of images in ImageNet is reduced to 28x28. Does this imply that the proposed method is not effective enough to deal with images of normal size?\n4-4. Figures 1 and 2 shall be enlarged.\n\n--- Thank the authors for the detailed response. After reading the response and the comments of peer reviewers, the rating is altered as follows. ", "title": "An interesting paper with very promising result", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "8LQWLpm7HBV": {"type": "rebuttal", "replyto": "kBrCZZJrlbT", "comment": "Q2-4: The part at the top of page 7 discusses the performance with respect to iteration and epoch. It is not very clear. It is stated that \"although one iteration cannot surpass one epoch, it is better than a few iterations.\" However, considering that one epoch consists of multiple iterations, this statement seems to contradict to itself and is a bit hard to follow. Please clarify.  \nA2-4: This might be a little counter-intuitive. However, it coincides with our experiment results. The reason is that when we only search for one iteration, it is obvious that the overfitting cannot happen. At the same time, because the super-net contains only skip connections, it does not need many iterations to converge. This is the reason that one iteration can perform well. When we search for a few iterations, the number of different images seen by the model is not big enough. However, since the super-net is very small, such number of gradient updates may have been enough for the super-net to get overfitted on the training set. This is the reason why a few iterations perform may worse than one or two iterations. After we have searched for one whole epoch, the super-net has seen enormous different images, this helps it to generalize better on the testing set. This is the reason why one epoch performs the best.\n\nQ3: The second paragraph on page 4 compares the computational complexity of DARTS and the proposed method. Since this is the key part of this work, more details shall be provided on how the complexity is worked out.  \nA3: Thank you for your advice. We have presented more detailed derivations in the revised paper\u2019s Section 3.\n\nQ4-1: The first sentence of the second paragraph on page 2 needs to be revised.   \nA4-1: Thank you for your advice. We have revised the paper according to your comments.\n\nQ4-2: Although the investigation of correlation among different architectures is mentioned as the key motivation for this work, how is the correlation of architectures considered in this work is not presented. This shall be improved;   \nA4-2: Thank you for your advice. We have explained this in detail in the revised paper. The first correlation is that [3] mentions that randomly varying operators in a good architecture leads to another good architecture. The second correlation is that simple operators can be treated as a special case of complex operators. For example, we can properly set the kernel weights of a convolution to make its output = input. At this time, a convolution performs as a skip connection. Thus, if a network with only skip connections performs well, it means that when substituting all the skip connections with convolutions, at least we can find a set of weights to ensure the new network performs well. In other words, suppose we define all the architectures having the same operators yet different topologies with an architecture A as A\u2019s neighbors. If A is an architecture with top accuracy among all its neighbors and B has the same topology as A, then B will also be an architecture with top accuracy among all its neighbors. The way we utilize this correlation is to first form a super-net only containing skip connections, then search for architectures on the super-net. After we found the best architecture on the shrunk super-net, we replace all its operators with convolutions to generate the final architecture.\n\nQ4-3: In Section 4.2, the resolution of images in ImageNet is reduced to 28x28. Does this imply that the proposed method is not effective enough to deal with images of normal size?  \nA4-3: This configuration is inherited from the baseline, namely PC-DARTS. In fact, while dealing with larger images, FTSO has more superiority because it avoids complex operations on the input images. Suppose we enlarge the image resolution from 28x28 to 280x280, then, the computational cost of every operator in our candidate operator set will be 100 times greater. At this time, PC-DARTS\u2019s time consumption will be about 380 GPU-days, while FTSO only requires about 1 GPU-minute.\n\nQ4-4: Figures 1 and 2 shall be enlarged.  \nA4-4: Thank you for your advice. We have revised the paper according to your comments.\n\n[3] Shu, Y., Wang, W., & Cai, S. (2019, September). Understanding Architectures Learnt by Cell-based Neural Architecture Search. In International Conference on Learning Representations.", "title": "We have revised the paper following your comments. This is the second part of our reply. "}, "V6cp8a6Mw8x": {"type": "rebuttal", "replyto": "iYQgLp4WIrr", "comment": "Thanks for your careful and valuable comments. We will explain your concerns point by point.\n\nQ1: It is not very clear how exactly the topology search algorithm ensures that each node is connected?  \nA1: In our topology search, we force every intermediate node to connect to all its previous nodes. Thus, when searching, all the nodes are connected. At the same time, for every intermediate node, we tune its connection weights to all its previous nodes via back-propagation. After the topology search terminates, for every intermediate node, we retain its connections to two previous nodes. This strategy guarantees that we can always find a path from the input node to any node in the graph. In fact, since the only difference between our topology search and the vanilla DARTS[1] is the number of candidate operators, the pruned architecture's connectivity can be guaranteed.\n\nQ2: The choice of skip connection as the operator for topology search is not well-justified. Max-pooling has no learnable parameters too. Is there any particular reason why skip connection?  \nA2: As we have shown in Table 3, the max-pooling operator can also bring satisfying results for the topology search. Here, we prefer skip connections because of two reasons. The first reason is that the skip connection operator not only requires 0 parameter, but also produces the minimum computational cost. The second reason is that max pooling greatly reduces the size of the image. Thus, we may lose too much useful information contained in the input image if the network is deep.\n\nQ3: The experiments are lacking because there is no evaluation on current benchmarks such as NAS-101, NAs1Shot1, or NAS-201 that can explicitly show how the search algorithm performs. In current experimental results, it is very difficult to judge if the proposed approach has benefits for searching 'good' architectures.  \nA3: We implement our topology search strategy on NATS-Bench (the updated version of NASBench-201) based on their official DARTS code and we have released our code in the attachment. As we mentioned in Section 4.3, according to NATS-Bench\u2019s official documents, the architecture found by DARTS is \u2018|skip_connect\\~0|+|skip_connect\\~0|skip_connect\\~1|+|skip_connect\\~0|skip_connect\\~1|skip_connect\\~2|\u2019, in which all the operators are skip connections. This might be a surprising fact. However, it is proven to be true by our reimplementation. This simple architecture provides as low as 54.30% testing accuracy on CIFAR10. For comparison, the architecture found by our topology search is \u2018|nor_conv_3x3\\~0|+|nor_conv_3x3\\~0|nor_conv_3x3\\~1|+|nor_conv_3x3\\~0|nor_conv_3x3\\~1|nor_conv_3x3\\~2|\u2019, which can achieve 93.76% testing accuracy on CIFAR10.\n\nQ4: The comparison with other differentiable NAS algorithms (e.g., DARTS and PC-DARTS[2]) can be delivered in the form of a table. The information of this comparison is all scattered in the manuscript.  \nA4: Thank you for your advice. We have revised our paper according to your suggestions.\n\n[1] Liu, H., Simonyan, K., & Yang, Y. (2018). Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055.  \n[2] Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G. J., Tian, Q., & Xiong, H. (2019). Pc-darts: Partial channel connections for memory-efficient differentiable architecture search. arXiv preprint arXiv:1907.05737.", "title": "We have revised the paper following your comments. This is the first part of our reply."}, "U45Gc-26b1": {"type": "rebuttal", "replyto": "kBrCZZJrlbT", "comment": "Thanks for your careful and valuable comments. We will explain your concerns point by point.\n\nQ1: This work can do better on theoretical analysis. Currently, it mainly describes how the decoupled search (FTSO) is implemented (in Section 3). Considering the significance of the improvement on computational efficiency, it will be more valuable to discuss whether such a kind of decoupled search can be generally applied, or it can only work for certain kinds of network architecture, data, or tasks. This will provide more insights on this interesting method and theoretical contribution.  \nA1: As we have mentioned in Section 5 and the response to Reviewer 3\u2019s Q3, FTSO is not only superior on DARTS\u2019s default search space, but also greatly dominating DARTS[1] on NATS-Bench dataset. Although currently we have only tested FTSO on image classification tasks, since nearly all the vision algorithms are based on image classification models, we have reasons to believe that FTSO can perform well on most vision tasks. At the same time, we have also discussed how to transfer FTSO to other machine learning areas in Section 5. Although, due to the limitation of time and resources, we did not test our method on other areas. However, we plan to make up experiments on language recognition in the future.\n\nQ2-1: When conducting topology search, this paper only uses a simple operator (say, skip connection). It is indicated that this supernet can hardly overfit the dataset, which is understandable. Meanwhile, with such a simple operator, will the supernet underfit the data? This needs to be clarified.  \nA2-1: It is true that the super-net containing only skip connections underfits the data. However, this is indeed our desire because, if this simple super-net already perfectly fits the data, then after replacing the skip connections with more complex operators, the finally exported architecture will absolutely overfits the data. What is more, when extracting a discrete sub-graph from the super-net, many operators with architecture weights lager than 0 are dropped. This is equivalent to set some architecture parameters to 0. For a super-net overfitting the data, it tends to find a sharp local minimum. While the super-net underfitting the data prefers a flat minimum. Thus, suppose we have found the global optimums of the both the overfitting super-net and the underfitting super-net, the introduced disturbance will impact more on the overfitting super-net. This is the reason why we say an underfitting super-net can produce better architectures.\n\nQ2-2: Similarly, in Section 4.1, it is stated that FTSO contains very few parameters and therefore can even achieve comparable performance to PC-DARTS[2]. Why could very few parameters lead to this comparable performance? Please clarify it.  \nA2-2: This is because when we say FTSO outperforms PC-DARTS, we mean that the model we find with FTSO can achieve higher testing accuracy than PC-DARTS. As we have mentioned in Q2-1, the parameter number of PC-DARTS\u2019s super-net is much larger than what is needed. Thus, few parameters of FTSO lead to higher performance.\n\nQ2-3: At the end of Section 4.2, it is found that the best architecture obtained by the search is the shallowest and widest one. The current explanation (provided at the end of this Section) is too brief and vague for such an interesting and surprising finding. This need to be further clarified.  \nA2-3: We have discussed more about this issue in the revised paper. We think the main reason is that the whole model is stacked with many cells. If the depth of each cell is too high, it leads to a very deep neural network. At that time, because all the operators in our found architecture are convolutions, we cannot use skip connections to facilitate gradients\u2019 propagation in ResNet\u2019s manner. In this way, both the vanishing and explosion of gradients may prevent the deeper models from higher performance.\n\n[1] Liu, H., Simonyan, K., & Yang, Y. (2018). Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055.  \n[2] Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G. J., Tian, Q., & Xiong, H. (2019). Pc-darts: Partial channel connections for memory-efficient differentiable architecture search. arXiv preprint arXiv:1907.05737.\n", "title": "We have revised the paper following your comments. This is the first part of our reply."}, "gHQh332A_E": {"type": "rebuttal", "replyto": "oyBFIvnpNM", "comment": "Thanks for your careful and valuable comments. We will explain your concerns point by point.\n\nQ1: The presentation is far from top tier conference standard. Examples include this sentence from the introduction.  \nA1: Thank you for your advice. We have polished the draft and updated the revised paper.\n\nQ2: Anyway, the \"mathematical proof\" aluded to is an informal argument for an example, essentially saying that searching one coordinate then the other is cheaper than searching both jointly.  \nA2: We illustrate the derivation more detailedly in the revised paper's Section 3. As a general conclusion, DARTS[1] requires $\\frac{1}{2}pn(n-3)H_{out}W_{out}C_{out}(k^2C_{in}+1)$ FLOPs and $\\frac{1}{2}n(n-3)p(k^2C_{in}+1)C_{out}$ parameters, while the total number of FLOPs and parameters of FTSO are $\\frac{1}{2}n(n-3)H_{in}W_{in}C_{in}$ and $\\frac{1}{2}n(n-3)$ respectively. Here $k$ is the kernel size, $n$ is the number of nodes, $p$ is the number of candidate operators, $C_{in}$ is the input tensor's channel number, $H_{out}$, $W_{out}$, and $C_{out}$ are the output tensor's height, width and channel number respectively. To show this result more intuitively, we assign the parameters in the formulas a set of values commonly used in NAS papers. In this concrete example, FTSO requires only $\\frac{1}{p(k^2C_{in}+1)C_{out}}=1.9\\times 10^{-8}$ times the parameters and $\\frac{1}{p(k^2C_{in}+1)}=9.8\\times 10^{-6}$ times the forward-propagation FLOPs per iteration compared to DARTS. \n\nQ3: Figures 2 and 3 are unreadable on printout.  \nA3: Thank you for your advice. We have enlarged Figure 2 and 3 in the revised paper.\n\n\nQ4: There is no code to check the results. This is a significant factor considering the empirical nature.  \nA4: Thank you for your advice. We have uploaded the code as the supplementary material\n\n[1] Liu, H., Simonyan, K., & Yang, Y. (2018). Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055.", "title": "We have polished the draft and updated the revised paper."}, "DFUIymuQCm": {"type": "rebuttal", "replyto": "iYQgLp4WIrr", "comment": "Q8: There is no empirical results how the algorithm works by varying the number of nodes in a cell. This will show that the limit of the proposed approach and also its capacity to handle various number of nodes.  \nA8: As we have shown in A3, FTSO can greatly surpass the baseline on NATS-Bench as well. This search space contains 5 nodes, which is different to our standard configuration with 7 nodes. This result clearly shows FTSO\u2019s generalizability on different search spaces and nodes. Theoretically, while the number of nodes increases, the size of the super-net dramatically increases. This leads to much more severe overfitting and computational cost in DARTS. However, in FTSO, we ease the overfitting via forming a super-net with only skip connections and reduce the computational cost from $O(n^3)$ to $O(n^2)$ as mentioned in Section 3. Thus, FTSO\u2019s advantages to DARTS can be even larger when dealing with more nodes. Although we can easily extend FTSO to more nodes, it is impractical to implement DARTS with more nodes due to DARTS\u2019s unbearable computational cost. This is the reason why we cannot provide an empirical comparison to DARTS with more nodes.\n\nQ9: The manuscript is poorly written, at least these sentences/paragraphs must be clarified and revised. ** In Section 3, \"it might be possible to cluster the architectures according to their connection topologies\" ** In Subsection 4.3, \"When we do not search for the operators, after the topology search, we assign all the remaining edges a fixed operator\" and the rest of the paragraph is difficult to follow. ** In Section 3, \"Secondly. Because our supernet contains very few parameters, it can hardly over-fit the dataset. Thus, the found architecture may generalize better on new datasets.\". This sentence is a claim without any evidence. ** Table 3 is not clear, some codes are not described in its caption. ** In Section 3, please fix \"Firstly. It allows the algorithm to converge within extremely few iterations... and so on..\"  \nA9: Thank you for your advice. We have revised our paper according to your suggestions.", "title": "We have revised the paper following your comments. This is the third part of our reply. "}, "j15zneoTybm": {"type": "rebuttal", "replyto": "iYQgLp4WIrr", "comment": "Q5: In Section 3 \"In FTSO, this problem is almost nonexistent because, the skip connection has no kernel weights to tune\", this is not very clear how the problem exists in the DARTS formulation and how the formulation in the proposed approach can mitigate this issue. There is no discussion about the bi-level program in DARTS.  \nA5: In DARTS, we use gradients to update architectures. However, to reduce the computational cost, DARTS does not evaluate the architectures after they get fully converged. Instead, DARTS gives each architecture only one iteration to tune its kernel weights. At the same time, the powerful operators, e.g., convolutions, require large number of gradient updates to get well optimized, and cannot produce meaningful outputs until their kernel weights are well tuned. Within one iteration, only those operators with zero or very few parameters can converge and achieve a relatively satisfying performance. For example, the output of a skip connection operator always equals to its input. Thus, the information contained in the input image can be well retained. Since the simpler operators can retain more meaningful information and perform better than the complex operators when the optimization is inadequate, they obtain greater architecture weights from the gradient updates and contribute more to the network\u2019s output. The result is that these simpler operators gain greater gradients and then get optimized better. Thus, in the end, the simpler operators will have the highest architecture weights and the found architectures tend to only contain the simplest operators. However, we all know that an architecture only containing skip connections cannot achieve high testing accuracy. This is the reason why DARTS cannot perform well. In contrast, our topology search utilizes only skip connections, an operator with no kernel weight. Thus, in FSTO, the super-net can converge within zero iteration. In this case, we can measure every architecture\u2019s performance precisely and reduce the bi-level optimization problem to single level, which has been widely studied and is theoretically guaranteed to be convergeable given an appropriate learning rate. This is the reason why we say that this problem is almost nonexistent in FTSO.\n\nQ6: It is very difficult to follow the overall approach, it would be better to have an algorithm/pseudocode.  \nA6: Thank you for your advice. We have revised our paper according to your suggestions. Please check Algorithm 1 and 2 for more details.\n\nQ7: \", we take the second strategy as the default configuration because, it is much more efficient, and the large amount of kernel weights, contained in the first strategy, may lead to over-fitting, and finally leads to worse performance than the second strategy. \". How does exactly large amount of kernel weights lead to overfit in NAS? Is there any proof/citation about it? The second strategy has no clear evidence to always give better results compared to the first strategy.  \nA7: Large amount of kernel weights lead to overfitting in DARTS because the goal of DARTS is to find a sub-graph from a huge super-net. Suppose the sub-graph can generalize perfectly on the testing set. Since the sub-graph is only part of the super-net, and the super-net contains many times more parameters than the sub-graph, the super-net must over-fit the dataset and cannot generalize well on the testing set. This is the reason why we say that large amount of kernel weights leads to overfitting in DARTS. PC-DARTS has mentioned this issue and reduces the size of the super-net via only computing partial input channels. For the third question, we did not say that the second strategy can always give better results than the first strategy. In fact, our opinion is that the second strategy is simpler, and it can stably provide architectures with comparable performance because, by replacing the simpler operators with complex operators in the sub-graph, the found architecture\u2019s model capacity gets promoted. For comparison, the first strategy produces a huge super-net containing relatively large number of kernel weights, thus suffers from similar overfitting problems in DARTS.", "title": "We have revised the paper following your comments. This is the second part of our reply. "}, "iYQgLp4WIrr": {"type": "review", "replyto": "7Z29QbHxIL", "review": "The paper proposes a method for Neural Architecture Search (NAS) with two stages of search. In the first  stage, the topology of the cell is searched with only one operator (skip connection) using graph pruning through gradient descents. In the later stage, there are two ways to search the operators. In the first approach, the found topology is equipped with some operators (e.g., 3x3 convolution,  skip connection, and 3x3 dilated convolution) and then the architecture parameters are optimized. Another approach is to replace all operators with one single operator e.g. convolution. The experiments show that the searching time is reduced significantly compared to DARTS and the results on CIFAR-10 and ImageNet are very competitive.\n\n\nStrengths:\n\n- The search time is reduced significantly from normal DARTS and PC-DARTS.\n- The results are competitive to the prior differentiable NAS methods with less searching time.\n\nWeaknesses:\n\n- It is not very clear how exactly the topology search algorithm ensures that each node is connected?\n- The choice of skip connection as the operator for topology search is not well-justified. Max-pooling has no learnable parameters too. Is there any particular reason why skip connection?\n- The experiments are lacking because there is no evaluation on current benchmarks such as NAS-101, NAs1Shot1, or NAS-201 that can explicitly show how the search algorithm performs. In current experimental results, it is very difficult to judge if the proposed approach has benefits for searching 'good' architectures.\n- The comparison with other differentiable NAS algorithms (e.g., DARTS and PC-DARTS) can be delivered in the form of a table. The information of this comparison is all scattered in the manuscript.\n- In Section 3 \"In FTSO, this problem is almost nonexistent because, the skip connection has no kernel\nweights to tune\", this is not very clear how the problem exists in the DARTS formulation and how the formulation in the proposed approach can mitigate this issue. There is no discussion about the bi-level program in DARTS.\n- It is very difficult to follow the overall approach, it would be better to have an algorithm/pseudocode.\n- \", we take the second strategy as the default configuration because, it is much more efficient, and the large amount of kernel weights, contained in the first strategy, may lead to over-fitting,\nand finally leads to worse performance than the second strategy. \". How do exactly large amount of kernel weights lead to overfit in NAS? Is there any proof/citation about it? The second strategy has no clear evidence to always give better results compared to the first strategy.\n- There is no empirical results how the algorithm works by varying the number of nodes in a cell. This will show that the limit of the proposed approach and also its capacity to handle various number of nodes.\n\n- The manuscript is poorly written,  at least these sentences/paragraphs must be clarified and revised:\n** In Section 3,  \"it might be possible to cluster the architectures according to their connection topologies\"\n** In Subsection 4.3, \"When we do not search for the operators, after the topology search, we assign all the remaining\nedges a fixed operator\" and the rest of the paragraph is difficult to follow.\n** In Section 3, \"Secondly. Because our supernet contains very few parameters, it can hardly over-fit the dataset. Thus, the found architecture may generalize better on new datasets.\". This sentence is a claim without any evidence.\n** Table 3 is not clear, some codes are not described in its caption.\n** In Section 3, please fix \"Firstly. It allows the algorithm to converge within extremely few iterations... and so on..\"", "title": "The paper needs more experiments and the claims are not well-justified by either empirical evidences or theories.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "oyBFIvnpNM": {"type": "review", "replyto": "7Z29QbHxIL", "review": "Title: FTSO: EFFECTIVE NAS VIA FIRST TOPOLOGY SECOND OPERATOR\n\nSummary of the paper:\n\nThe goal is to fit the hyper-parameters of the neural network namely topology (i.e. network architecture) and operator (e.g. skip connection or convolution) rather than just fit the parameters (i.e. weights). The approach is similar to DARTS which relaxes the architecture choice to obtain a continuous optimisation.\n\nThe main difference from DARTS is that rather than jointly selecting operator and topology, first the topology and then the operator is selected. It's roughly a form of coordinate descent. \n\nThe paper is empirical in nature and claims good results. \n\nPros:\n\nThe idea of optimising topology then operator appears to work well in practice. \n\nCons:\n\nThe presentation is far from top tier conference standard. \n\nExamples include, this sentence from the introduction (yes, it's one sentence): \n\n\"We first mathematically prove that, by greatly shrinking the graph of the search space, reducing the operators\u2019 complexity in magnitude, lowering the required searching period from 50 epochs to one iteration and significantly easing the Matthew effect, namely that the complex operators may never get the chance to be well tuned, thus the found architecture only contains very simple operators, and performs poorly on the testing set, FTSO reduces the required parameters by a factor of 0.53\u00d7108, decreases the FLOPs per iteration by a factor of 2\u00d7105 and significantly promotes the accuracy compared to the baseline, PC-DARTS.\"\n\n(Anyway, the \"mathematical proof\" aluded to is an informal argument for an example, essentially saying that searching one coordinate then the other is cheaper than searching both jointly.)\n\nAlso figures 2 and 3 are unreadable on printout. \n\nAlso, there is no code to check the results. This is a significant factor considering the empirical nature.\n\nRecommendation:\n\nThe paper is surely interesting to some, but needs a lot more work.", "title": "inadequate presentation quality", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}