{"paper": {"title": "Discovering Non-monotonic Autoregressive Orderings with Variational Inference", "authors": ["Xuanlin Li", "Brandon Trabucco", "Dong Huk Park", "Michael Luo", "Sheng Shen", "Trevor Darrell", "Yang Gao"], "authorids": ["~Xuanlin_Li1", "~Brandon_Trabucco1", "~Dong_Huk_Park2", "~Michael_Luo2", "~Sheng_Shen2", "~Trevor_Darrell2", "~Yang_Gao1"], "summary": "The paper proposes an unsupervised learner that discovers non-monotonic autoregressive orders for sequence generation through fully-parallelizable end-to-end training without domain-specific prior.", "abstract": "The predominant approach for language modeling is to encode a sequence of tokens from left to right, but this eliminates a source of information: the order by which the sequence was naturally generated. One strategy to recover this information is to decode both the content and ordering of tokens. Some prior work supervises content and ordering with hand-designed loss functions to encourage specific orders or bootstraps from a predefined ordering. These approaches require domain-specific insight. Other prior work searches over valid insertion operations that lead to ground truth sequences during training, which has high time complexity and cannot be efficiently parallelized. We address these limitations with an unsupervised learner that can be trained in a fully-parallelizable manner to discover high-quality autoregressive orders in a data driven way without a domain-specific prior. The learner is a neural network that performs variational inference with the autoregressive ordering as a latent variable. Since the corresponding variational lower bound is not differentiable, we develop a practical algorithm for end-to-end optimization using policy gradients. Strong empirical results with our solution on sequence modeling tasks suggest that our algorithm is capable of discovering various autoregressive orders for different sequences that are competitive with or even better than fixed orders.", "keywords": ["variational inference", "unsupervised learning", "computer vision", "natural language processing", "optimization", "reinforcement learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper deals with a particular model structure selection problem: inferring the order of a given sequence of latent variables. This problem is closely related to the matching problem that involves discrete optimization. The authors propose to cast the problem into a one-step Markov Decision problem and optimize it using the policy gradient.  The proposal here is using Variational Order Inference (VOI) using and using a Gumbel-Sinkhorn distribution to construct a proposal over approximate permutations. The approach is mathematically sound and novel.\n\nEmpirical results on image caption and code generation show promising results: method outperforms the previous Transformer-InDIGO and other baselines (Random, L2R, Common, Rare). This paper further analyzes the learned orders globally and locally, and conducts ablations.\n\nThe reviewers were overall very enthusiastic. \n"}, "review": {"vnY5YS2yOYN": {"type": "rebuttal", "replyto": "WUbsJYBCKBh", "comment": "We sincerely thank you for your constructive comments, which are helpful in improving our submission. We would like to address the comments and questions below, and we have updated our submission accordingly.\n\n>1. \u201cExplicit modeling the generation order is not a very novel idea that there have been many works on this topic.\u201d\n\nWe agree that modeling the generation order is not a novel concept by itself, and many works before have studied how to model the generation order. Indeed, our VOI's decoder network relies on a combination of Pointer Network + Transformer that several prior methods have utilized [1,2,3]. We are grateful to these works that came before us, because they have solved practical architectural issues that make the Pointer Network + Transformer models stable to train. Our use of Transformer-InDIGO as the decoder network demonstrates how solving practical architectural and stability issues can encourage future research (our research) in non-monotonic autoregressive modeling.\n\nWhile modeling generation order is not a novel concept by itself, this is only one of our contributions. Much like how prior work has made the Pointer Network + Transformer model architecture practical to train, we aim to streamline the inference of generation orders. We intend to replace inductive biases---such as model-pretraining [3], specialized loss functions [2], and generation order priors [1]---with an end-to-end data-driven method. **We believe that our contribution to creating a practical method that can infer high-quality generation orders without these inductive biases is novel.**\n\n**Another reason VOI is novel is because it is efficiently parallelized.** VOI's encoder network outputs generation orders in a single forward pass. An empirical comparison is presented in Section 5, Figure 2 of our updated paper, where we compare the runtime performance of VOI ($K=4$) with Searched Adaptive Order (SAO) on a single GPU in order to accurately measure the number of ops. Our speedup over SAO's inference time linearly increases with sequence length, and is 30 times faster than SAO for sequences of length 30. In our implementation, the time per training iteration for VOI is 6 times faster than SAO. In practice, as we distribute VOI across more GPUs, the penalty caused by the additional $K$ factor in our runtime is negligible.\n\nThank you for making this point, as it has helped us elaborate on the positioning of our method within the larger space of generation order modeling. If you have additional suggestions, we'd love to continue this discussion with you within the rebuttal period.\n\n[1] Chan et al., KERMIT: Generative Insertion-Based Modeling for Sequences\n\n[2] Stern et al., Insertion Transformer: Flexible Sequence Generation via Insertion Operations\n\n[3] Gu et al., Insertion-based Decoding with automatically Inferred Generation Order", "title": "Thank you for your review! Addressing concerns below [1/2]"}, "nM2U1qI_Akn": {"type": "rebuttal", "replyto": "SkUEed-plB", "comment": "We sincerely thank you for your constructive comments, which are helpful in improving our submission. We would like to address the comments and questions below, and we have updated our submission accordingly.\n\n>1. Clearly Define The Two Similarity Metrics\n\n**In the updated version, we have included additional descriptions about the two similarity metrics in the Section 6 of the paper**. If you have additional suggestions to further improve our clarity, we are glad to discuss with you via OpenReview, and incorporate these suggestions into our paper.\n\n>2. X_axis in the Fig 3\n\nWe thank you for recommending this clarifying detail. **In the updated version, we have now included, in Appendix E, a mapping from the X values in this Figure (it is now figure 4) to the full names of the parts of speech that these X values represent, along with example words.**\n\n>3. Ablation Studies About the 'K' and Visualizations\n\n**In Section 7 of our updated version, we have added instructions for choosing K, as well as an ablation study of the sensitivity of the encoder network to the value of K.** The main result of this study is that, while larger K generally produces better-performing encoder networks that fit more accurately to a ground truth order, K=4 appears to be sufficient, such that the encoder network\u2019s performance is not significantly different from much larger K, while running fast. \n\n**We have also updated our submission with additional visualizations of sequences generated by our model, per your request, in Appendix F.** If you would like to see additional visualizations included, we are glad to generate more, and to iterate on the type of visualizations currently included in the paper. \n\n---\n\nWe hope these could address your concern, and we thank you again for the helpful comments. We are glad to discuss further comments and suggestions.", "title": "Thank you for your review! Addressing concerns below"}, "q8IlYLX5ucI": {"type": "rebuttal", "replyto": "yMUyFJg8Wwy", "comment": "We sincerely thank you for your constructive comments, which are helpful in improving our submission. We would like to address the comments and questions below. \n\n>1. The results are not compared with other SOTA auto-regressive algorithms.\n\nWe would like to first emphasize that the main focus of our work is to present a novel approach of learning non-sequential / non-monotonic orderings, which can generalize across different sequence modeling tasks and not rely on domain assumptions. We aim to discover autoregressive orderings in an end-to-end and efficient manner. **In this paper, we focus on studying the learned orderings and the generalizability of our approach across different tasks, and focus less on beating the SOTA methods on a particular challenge.**\n\n**We would also like to clarify that the autoregressive decoder network used in our method is modular by design.** It can be any model that supports non-monotonic sequence generation, and is **not restricted to Transformer-InDIGO**. In addition, we used Transformer-InDIGO trained with fixed orderings as a baseline because it is efficient to train the model with predefined fixed orderings, and training it using left-to-right (L2R) ordering is almost equivalent to training a typical Transformer, which provide meaningful and strong baselines. We also compare our model with the searched adaptive order (SAO) because it comes from previous work for discovering non-monotonic orderings, and represents a good baseline.\n\nPrior SOTA methods have proposed domain-specific improvements such as large-scale pretraining on other datasets and task-specific model architectures, and we believe such improvements can also be incorporated into our autoregressive decoder network. **Indeed, coupling our method with these techniques is a promising idea; however, we would like to leave that for future work and focus on providing a generalizable method for inferring generation orders that is efficient, learns high-quality orders, and is modularly compatible.**\n\n>Question: How do you think about transferring knowledge from auto-regressively trained such as GPT-2 to such non-autoregressive models? Using the pertained model for the encoder and decoder would improve the results?\n\nWe believe it's a very nice idea to transfer knowledge / distill from auto-regressively pretrained models like GPT-2 to our model, and it poses an interesting research challenge. Such knowledge transfer has the potential to further improve the performance of VOI. **In fact, we found that VOI performed well without it.** For example, in the updated Gigaword results in Table 1, we find that our method outperforms, without domain-specific pre training or tuning, Transformer baselines with various fixed orderings. Our VOI performance is also approximately on par with that of PEGASUS-BASE [1], a larger Transformer model (their d_model = 3072, while our d_model = 2048) pretrained on 4 datasets and finetuned on Gigaword. As per to these findings, we believe that exploring knowledge transfer and distillation is out-of-scope for our current work and we would like to leave it for future work.\n\n---\n\nWe hope these could address your concern, and we thank you again for the helpful comments. We are glad to discuss further comments and suggestions.\n\n[1] Zhang et al., PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization\n\n[2] Gu et al., Non-Autoregressive Neural Machine Translation", "title": "Thank you for your review! Addressing concerns below"}, "ieiTPBqx3mn": {"type": "rebuttal", "replyto": "OXn2q76q7nJ", "comment": "We sincerely thank you for your constructive comments, which are helpful in improving our submission. We would like to address the comments and questions below, and we have updated our submission accordingly.\n\n>1. Effect of Sample Size K\n\n**In Section 7 of our updated version, we have added instructions for choosing K, as well as an ablation study of the sensitivity of the encoder network to the value of K.** The main result of this study is that, while larger K generally produces better-performing encoder networks that fit more accurately to a ground truth order, K=4 appears to be sufficient, such that the encoder network\u2019s performance is not significantly different from much larger K, while running fast. \n\n>2.1 Experiments on NMT\n\nWe would like to thank you for pointing us to this evaluation domain, and we have begun evaluating our method on the WMT Romanian-English machine translation task. It is our goal to include these translation results in our final submission, but given the limited amount of time and compute resource for rebuttals and the large number of training steps required, these translation results currently require time beyond the rebuttal period to be completed and entered into our submission. We will try to include these results in the final version of our paper.\n\n>2.2 Running Time\n\n**In our updated version, we have added visualizations on *Time Per Training Iteration* and the *Generation Order Search Time* for our method and prior work.** We compare the runtime performance of VOI ($K=4$) with Searched Adaptive Order (SAO) on a single Tesla P100 GPU in order to accurately measure the number of ops required. We present these figures in Section 5. Since VOI outputs latent orderings in a single forward pass, we observe that VOI is significantly faster than SAO, which searches orderings sequentially. We found that VOI's speedup over SAO to infer a generation order linearly increases as the sequence length increases, and is 30 times faster than SAO for sequences of length 30. In our implementation, the training time per iteration for VOI is 6 times faster than SAO. The training penalty introduced by VOI while inferring generation orders is minimal. In practice, as we distribute VOI across more GPUs, the $K$ factor in the runtime is effectively divided by the number of GPUs used (if we ignore the parallelization overhead), so we can achieve further speedups.\n\n>3. Figure 1\n\nIn our updated version, we have updated Figure 1 to explicitly describe the source sequence \u2018x\u2019.\n\n---\n\nWe hope these could address your concern, and we thank you again for the helpful comments. We are glad to discuss further comments and suggestions.", "title": "Thank you for your review! Addressing concerns below"}, "0l9-rOkVcAo": {"type": "rebuttal", "replyto": "WUbsJYBCKBh", "comment": ">2. \u201cFor checking the generalization of the method and better comparison w/ InDIGO (though InDIGO also conducted on MSCOCO, Django and the current comparison is sufficiently fair), I would like to increase my rating if seeing more experiments on large scale machine translation benchmarks as those in InDIGO.\u201d\n\nWe would like to thank you for pointing us to this evaluation domain, and we have begun evaluating our method on the WMT Romanian-English machine translation task, which previous work has used for evaluation. It is our goal to include these translation results in our final submission, but given the limited amount of time and compute resource for rebuttals and the large number of training steps required, these translation results currently require time beyond the rebuttal period to be completed and entered into our submission. We will try to include these results in the final version of our paper.\n\n>3. Figure 1: could consider adding x, which would better match the descriptions of the paper (modeling p(y|x) instead of p(y))\n\n**In our updated version, we have updated Figure 1 to describe VOI as implemented for conditional sequence modeling to align with your suggestion.** If you have additional formatting or clarification suggestions, we are glad to further update our figures, and provide additional comments on the implementation of VOI.\n\n>Missing References\n\nWe have added them in our related works section, and in Section 5 when we introduce the encoder and decoder architectures for conditional sequence generation.\n\n---\n\nWe hope these could address your concern, and we thank you again for the helpful comments. We are glad to discuss further comments and suggestions.", "title": "Thank you for your review! Addressing concerns below [2/2]"}, "N8eNhVf1yqb": {"type": "rebuttal", "replyto": "jP1vTH3inC", "comment": "We thank all reviewers for their helpful comments and constructive feedback! Here we summarize the recent updates to our submission:\n\n- **New results on Gigaword.** We have updated Table 1 with new results on Gigaword, which we finished after our initial paper submission. We report the Rouge-1, Rouge-2, and Rouge-L scores comparing our VOI with baselines. We find that, similar to MS-COCO and Django, our method outperforms baselines of different fixed orders.\n- **Ablation on the choices of $K$.** In Section 7 of our updated version, we have added instructions for choosing $K$, the number of latent orderings to sample per data, as well as an ablation study of the sensitivity of the encoder network in VOI to the value of $K$. \n- **Empirical runtime analysis of VOI.** In Section 5 of our updated version, we have added visualizations on *Time Per Training Iteration* and the *Generation Order Search Time*. We compare our VOI with Searched Adaptive Order (SAO) used for learning nonmonotonic orderings on Transformer-INDIGO. We implemented SAO according to the original paper's descriptions. We observe that, empirically, VOI can achieve significant speedup over SAO.\n- **Updated VOI computation graph (Figure 1).** We have updated Figure 1 to make it clearer.\n- **More visualizations.** We have added more visualizations, including those of VOI and fixed-order baselines, in Appendix F.\n- **New analysis in Section 6.3** In our updated version, we now include an additional experiment in Section 6.3. We demonstrate that VOI learns autoregressive orders that not only depend on the target tokens $\\mathbf y$, but also the content of the conditioning variable $\\mathbf x$. ", "title": "Updates to our submission"}, "WUbsJYBCKBh": {"type": "review", "replyto": "jP1vTH3inC", "review": "This paper proposes to model the generation order as latent variables for sequence generation tasks, by optimizing the ELBO involving a proposed process of Variational Order Inference (VOI). To alleviate the difficulty of optimizing discrete latent variables, the authors propose to cast it as a one-step Markov Decision problem and optimize it using the policy gradient. The authors also introduce the recent developed Gumbel-matching techniques to derive the close-form of the posterior distribution.\n\nPros:\n1. Overall, I think the research problem, i.e., explicit modeling the generation order,  in this work is interesting and worthy of discovering\n2. Casting the optimization of discrete latent variables as a one-step MDP is interesting\n3. Experiments show that the induced \"best-first\" order outperforms fixed orders, which verifies the motivation of the paper`\n4. Extensive and inspiring analysis \n\nCons:\n1. Explicit modeling the generation order is not a very novel idea that there have been many works on this topic. \n2. For checking the generalization of the method and better comparison w/ InDIGO (though InDIGO also conducted on MSCOCO, Django and the current comparison is sufficiently fair), I would like to increase my rating if seeing more experiments on large scale machine translation benchmarks as those in InDIGO. This would also further support your claim of a general-purpose approach w/ little domain knowledge (if any).\n\n-------\nMinors:\n- figure 1: could consider adding x, which would better match the descriptions of the paper (modeling p(y|x) instead of p(y))\n\nMissing references: \n\n[1] Chan, W., Kitaev, N., Guu, K., Stern, M. and Uszkoreit, J., 2019. KERMIT: Generative insertion-based modeling for sequences. arXiv preprint arXiv:1906.01604.\n\n[2] Gu, J., Wang, C. and Zhao, J., 2019. Levenshtein transformer. In Advances in Neural Information Processing Systems (pp. 11181-11191).\n\n[3] Bao, Y., Zhou, H., Feng, J., Wang, M., Huang, S., Chen, J. and Li, L., 2019. Non-autoregressive transformer by position learning. arXiv preprint arXiv:1911.10677.\n", "title": "Review", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SkUEed-plB": {"type": "review", "replyto": "jP1vTH3inC", "review": "The authors  propose the first domain-independent unsupervised learner that discovers high-quality autoregressive orders through fully-parallelizable end-to-end training without domain specific tuning.  Inspired by the variational auto-encoder, they propose an\nencoder architecture to infer autoregressive orders. To solve  the non-differentiable ELBO ( discrete latent variables), they further construct a  practical algorithm with the help of policy gradients. The experiment results, such as the  global and local statistics for learned orders, are convincing.\n\nSome problems.\n1: The authors should clearly define the two similarity metrics between autoregressive orders in the appendix.\n2: Please check up the X_axis in the Fig 3.\n3: It would be better if the authors provide more results in the appendix,  such as the ablation studies about the 'K' and visualizations of sequences generated by the baselines.\n\n", "title": "official review by AnonReviewer4", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "yMUyFJg8Wwy": {"type": "review", "replyto": "jP1vTH3inC", "review": "- Summary\nThis paper aims to decode both content and ordering of language models and proposes Variational Order Inference (VOI). The authors introduce a latent sequence variable z = (z_1, .. ,z_n) in which z_t is defined as the absolute position of the value generated. The authors model the posterior distribution of z as a Gumbel-Matching distribution which is relaxed as a Gumbel-Sinkorn distribution. To training the encoder and decoder networks, the ELBO is maximized using the policy gradient with baseline. The experimental results on Django and MS-COCO 2017 dataset show the proposed VOI outperforms the Transformer-InDIGO, as well as suggests that learned orders depend on content and best-first generation order.\n\n\n- Strong points\n\t1. The research on non-autoregressive orders to generate language is interesting, and the proposed method using Gumbel-Sinkorn distribution is mathematically well sound and novel.\n\t2. The proposed method outperforms the previous Transformer-InDIGO and other baselines (Random, L2R, Common, Rare). This paper analyzed the learned orders globally and locally, and conducted ablation studies.\n\t3. The paper is well-written and the authors also provide source codes for reproducibility.\n\n- Weak points\n\t1. The results are not compared with other SOTA auto-regressive algorithms.\n\n- Questions\n\t- How do you think about transferring knowledge from auto-regressively trained such as GPT-2 to such non-autoregressive models? Using the pertained model for the encoder and decoder would improve the results? Have you tried this strategy?", "title": "Interesting problem,  proposed methods, and experimental results.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "OXn2q76q7nJ": {"type": "review", "replyto": "jP1vTH3inC", "review": "This paper designed a new generative model by capturing the auto-regressive order as latent variables for sequence generation task. Based on combinatorical optimization techniques, the authors derived an policy gradient algorithm to optimize the variational lower bound. Empirical results on image caption and code generation showed that this method is superior than both fixed-order generation and previous adaptive-order method transformer-InDIGO. The authors further analyzed the learned orders on global and local level on COCO2017 dataset, demonstrating that the arrangement tend to follows the best-first strategy.\n\n\nConcerns:\n1. effect of sample size K: In section 5 training part, the paper claimed \"For our model trained with Variational Order Inference , we sample K = 4 latents for each\ntraining sample.\". The sample size K is used to approximate the gradient in variational order inference and it also affects the training efficiency i.e. $O(NKdl^2)$. It's not clear how the author choose the appropriate sample size K. Some analysis or experiemnt reults on the sensitivity of sample size K will help clarify this concern.\n\n\n2. experiments on nmt & running time: The papers didn't report any results on machine translation, an important task on conditional sequence generation. Since previous work(e.g. transformer-InDIGO) demonstrated superior results on several translation datasets, it's recommended that the authors also showed results on these datasets. Also one strength of the approach is its potential of fully parallelizing. A running time comparison will provide more convincing evidence to this claim.\n\n\n3. figure: Figure 1. in section 4 showed the structure of variational order inference. Considering the paper is mainly focused on conditional generation, an conditional generation version will be better by incorporating x sequence into the figure.", "title": "Interesting idea on modeling generation orders with latent variables", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}