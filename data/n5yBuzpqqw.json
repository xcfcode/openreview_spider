{"paper": {"title": "Error Controlled Actor-Critic Method to Reinforcement Learning", "authors": ["Xingen Gao", "Fei Chao", "Changle Zhou", "Zhen Ge", "Chih-Min Lin", "Longzhi Yang", "Xiang Chang", "Changjing Shang"], "authorids": ["31520160154529@stu.xmu.edu.cn", "fchao@xmu.edu.cn", "dozero@xmu.edu.cn", "13290073@student.uts.edu.au", "cml@saturn.yzu.edu.tw", "longzhi.yang@northumbria.ac.uk", "xic9@aber.ac.uk", "cns@aber.ac.uk"], "summary": "We propose a new actor-critic algorithm called Error Controlled Actor-critic which ensures confining the approximation error in value function.", "abstract": "In the reinforcement learning (RL) algorithms which incorporate function approximation methods, the approximation error of value function inevitably cause overestimation phenomenon and have a negative impact on the convergence of the algorithms. To mitigate the negative effects of approximation error, we propose a new actor-critic algorithm called Error Controlled Actor-critic which ensures confining the approximation error in value function. In this paper, we firstly present an analysis of how the approximation error can hinder the optimization process of actor-critic methods. Then, we *derive an upper boundary of the approximation error of Q function approximator, and found that the error can be lowered by placing restrictions on the KL-divergence between every two consecutive policies during the training phase of the policy.* The results of experiments on a range of continuous control tasks from OpenAI gym suite demonstrate that the proposed actor-critic algorithm apparently reduces the approximation error and significantly outperforms other model-free RL algorithms.", "keywords": ["reinforcement learning", "actor-critic", "function approximation", "approximation error", "KL divergence"]}, "meta": {"decision": "Reject", "comment": "The majority of the reviewers believe that this paper is not ready for publication. Among their concerns is that the paper has limited novelty, especially in relation to existing work that use the KL constraint. Some of the reviewers also believe that the arguments are sometimes hand-wavy and not rigorous. For example, in the discussion period after Nov. 24th, it is mentioned that the argument by the authors that \"KL constraint==>decrease the approximation error==>increase performance\" is not precise enough. I encourage the authors to take these comments into account and improve their paper."}, "review": {"Tq8A76BtXk": {"type": "review", "replyto": "n5yBuzpqqw", "review": "In this paper, the authors study the error introduced by the estimation of critic function in the Actor-Critic algorithm. Then the author proposed an algorithm that utilizes the idea of double Q learning and using a KL-divergence like regularization method to control this error. Experimentally the proposed algorithm achieves good results comparing to the vanilla Actor-Critic algorithm. This paper shows a successful routine from the theoretical analysis to a practical algorithm.\n\nHowever, one of the drawbacks of this paper to me is, the authors proposed the double Q learning method. It is not clear how this double Q help to reduce the estimation error? I am also a little bit concerned that the gap between theoretical analysis and the practical algorithm is large. In practice, the authors always use a one-step update, but there is no guarantee that the one-step update for both actor and critic can decrease the error or improve the performance. \n\nAlso, since the key idea of how to control the critic is to use a regularization based on the last policy to make sure the policy move not so 'fast'. It would be interesting to ask what will happen if we only use a smaller learning rate for the actor and a larger learning rate for the critic. Some theoretical work [1] [2] have already adopted this method to get the error controlled and make the actor converge to the stationary point. Therefore, it would highlight the result of this paper if the author can compare their algorithm and show the advantage over these algorithms that only set different learning rates. \n\nGiven the contribution of the newly proposed algorithm, I will suggest accepting this paper.\n\n[1] Wu, Yue, et al. \"A Finite Time Analysis of Two Time-Scale Actor Critic Methods.\" arXiv preprint arXiv:2005.01350 (2020).\n\n[2] Hong, Mingyi, et al. \"A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic.\" arXiv preprint arXiv:2007.05170 (2020).\n\n--- Update after discussion\n\nI agree with other reviewers' idea that the contribution of this paper is somehow limited. Since my previous suggestion (7, accept) is based on that the author can successfully address how to control the error of the critic theoretically. However, with only the experimental elaboration study, as I said in the response to the author, the contribution of this paper is limited. Thus, I would switch back to 5 (marginally reject) based on that after reading other reviewers' discussion.", "title": "Good paper starting from theoretical analysis to practical algorithms", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ioL0h8G3hz": {"type": "rebuttal", "replyto": "8VAPfxvoY4i", "comment": "Thank you for your suggestions!", "title": "Thank you for your reply! "}, "k9Bh-oiJb7": {"type": "rebuttal", "replyto": "zwXSpLesd38", "comment": "Thank you for your reply!!\n\nComment #1: now I am wondering as to what are the consequences of minimizing approximation error? Does it give an upper bound on total error accumulated? \n\nResponse #1:\n\nPlease read Response #2 first.\n\n(1) As mentioned in the Sec *INTRODUCTION*, the approximation error in value function results in *an overestimation phenomenon* and decreases the performance of RL algorithms ([1], [2], and [3]). It means that minimizing approximation error helps decrease the overestimation and increase performance. \n\n(2) We provided the results of the ablation study to verify the effect of KL constraint. We hope Reviewer #4 will run an check our code [https://github.com/SingerGao/ECAC] (**no any author information**)\n\nKL constraint==>decrease the approximation error==>increase performance.\n\nComment #2: Can the authors elaborate on the point about connection to EM? Is this a formal connection?\n\nResponse #2: When it is hard to **maximize** an objective (*log-likelihood for EM*) directly, one effective choice is to maximize a **lower bound** of the objective instead. Parallel, when it is hard to **minimize** an objective (*the approximation error for our method*) directly, one effective choice is to minimize an **upper bound** of the objective instead. This is why we derived an upper bound of the approximation error. \n\nComment #3: I don't think that those prior works rely heavily on the KL-penalty -- any penalty that is similar to the KL should also work, so I am not sure about that part in the rebuttal. \n\nResponse #3:\n\nWe do not agree with this. [7] and [8] provide only an exact analysis of KL penalty rather than other regularizations. \n\nComment #4: The auto-tuning the penalty was also done in SAC in the same way, so I am not sure why this is novel. \n\nResponse #4: \n\nPlease read our method carefully, our auto-tuning method is different from SAC. \n\nSAC: $\\min_{\\alpha}\\mathbb{E}[-\\alpha \\log \\pi(a_t|s_t)]-\\alpha \\bar H$. \n\nOur method: $\\min_{\\alpha}\\mathop{E}\\limits_{\\boldsymbol{s}\\sim P_\\mathcal{D}}[\\log \\alpha \\cdot ((\\delta_{KL}+\\delta_{entropy})-H].$\n\nComment #5: It seems like Theorem 1 and Section 3 were removed (they are not there in the paper) anymore.\n\nResponse #5: We removed Theorem 1 and Section 3 (in the initial submission) for the following reasons:\n\n- Sec 3 seems superfluous because Reviewer #1 and #4 excessively focused on it *but ignored the key idea of our work*: derive an upper boundary of approximation errors for the Q approximator and reduce the approximation error by minimizing this upper bound of the error.\n- This removal does not affect the main idea of our work. \n\nComment #6: Having used a number of baselines myself, and from multiple repositories, it does seem to me that using the right set of baselines is needed ....\n\nResponse #6:\n\nWe used the official codebases of SAC (https://github.com/haarnoja/sac) and TD3 (https://github.com/sfujim/TD3) in our comparative evaluation. In our comparative evaluation, we used a set of random seeds different the official codebases, but all algorithms used the same set of random seeds. Have you ever tried to use different random seeds in the RL experiments? The performance of RL algorithms can be drastically different by using different random seeds [6]. \n\nComment #7: As I said in my review, there should also have been a theoretical/empirical discussion/comparison of alternate techniques like slower learning rate for the policy rather than an explicit KL constraint.\n\nResponse #7: \n\nSlowing down the learning rate for the policy seems like the two-time-scale methods [4] and [5]. The idea of two-time-scale methods seems similar to the delayed policy update method (TD3 [3]) which only updates the policy after a fixed number of updates to the Q network. Delayed policy update is effective, but the benefits are limited. Moreover, *it may be hard to find a pair of two learning rates for all the tasks.* While the KL constraint stabilizes the Q function automatically. Besides, we do not provide the comparative evaluation about these in the revision because it needs time to compare the two-time-scale methods with the KL penalty. \n\n## Reference \n\n[1] Sebastian Thrun and Anton Schwartz. Issues in using function approximation for reinforcement learning. 1993. \n\n[2] Philip Thomas. Bias in natural actor-critic algorithms. ICML 2014. \n\n[3] Scott Fujimoto, et al. Addressing function approximation error in actor-critic methods. ICML 2018. \n\n[4] Wu, Yue, et al. \"A Finite-Time Analysis of Two Time-Scale Actor-Critic Methods.\" arXiv (2020).\n\n[5] Hong, Mingyi, et al. \"A two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic.\" arXiv (2020).\n\n[6] Henderson, et al. Deep Reinforcement Learning that Matters. 2017.\n\n[7]  Nino Vieillard, et al. Leverage the average: an analysis of kl regularization in rl, 2020.\n\n[8] Matthieu Geist, et al. A theory of regularized Markov decision processes, 2019.", "title": "Responses to new comments, we're looking forward to your reply!"}, "xytiZVJzaMi": {"type": "rebuttal", "replyto": "FxEtpxNRmMR", "comment": "Please let us know whether our rebuttal to your questions are satisfactory or not.\nThank you very much!", "title": "Thank you for your reply! We are looking for your feedback!"}, "XoEMmcEMn0": {"type": "rebuttal", "replyto": "CopCioX0eI", "comment": "Sorry for my writing mistakes. We organized all the comments into the following three categories: \n## Reasonable\n### Comment #1:\nThe argument following eq (15) is very informal...\n### Response #1:\nWe have revised the equation (Eq 18 in the revised paper) and the argument by following this comment. \n### Comment #2 :\n(1) What is $V^{\u03c0^{n}}$? Is it the value function of the policy $\\pi^n$.\n(2) What is the difference between $\u03c0^{\u2217,n}$ and $\u03c0^{n+1}$.\n(3) What is \u03c0 is eq(3)?\n### Response #2: \nWe have improved clarity in the revised paper.  (Secs 2 and 3.2 in the revised paper)\n\n\n## Unreasonable\n### Comment #3:\nIt is not clear what the actor critic algorithm is. What is presented in Fig 1 seems to be policy iteration, and Theorem 1 seems to be stating policy iteration (yet \u03c0n+1 has not been defined).\n### Response #3:\nActor-critic ([3], [4], and [5]) is a well-known architecture in RL. Due to the paper limitation, we only provided a very brief introduction in our initial submission. We have provided a more clear introduction to actor-critic in the revised paper (sec 2.2). \n\n- What is presented in Fig 1 is certainly actor-critic. An actor-critic method ([3], [4], and [5])  is an alternating update algorithm. Besides, we have updated Fig. 1 to make it more clear. \n\n- Theorem 1 is surely stating actor-critic.  Please do carefully read our convergence proof (Appendix A in the first submission) . \n\n### Comment #4:\nAdding the KL based penalty is perhaps a useful idea\n### Response #4:\nWe have updated the paper by adding KL-penalty literatures, [1] and [2], in the revised paper (sec 1). These works are different from ours. The **difference between the KL-control literatures and our work** are summarized as follows:\n\n- We arrived at the same conclusion as [1] and [2] by a somewhat **different route**.  The authors **directly studied the effect of KL and entropy regularization in RL** and proved that a KL regularization indeed leads to averaging errors made at each iteration of value function update.  While our idea is very different from theirs: It is impracticable to minimize the approximation error directly, so  instead we tried to **minimize an upper bound of approximation error**. \n\n- We arrived at **a more general conclusion**: approximation error can be reduced by keep new policy close to previous one.  Please see page 4 in the revised paper, the third term in the upper bound of approximation error will be smaller if the more similar the two consecutive policies,  $\\pi_{\\theta_{n}}$ and $\\pi_{\\theta_{n-1}}$, are.  **KL penalty is a effective way, but not the only way.**\n\n- [2] used KL penalty to improve the performance of DQN, while we proposed a more practical method (i.e. ECAC) which includes  a mechanism to automatically adjust the coefficient of KL term. \n\n\n### Comment #5 : \nThe authors claim \"We present a convergence analysis for actor-critic methods by Banach\u2019s Fixed Point Theorem\" to be one of the contributions. The fixed point theorem here is to show that Bellman operator is a contraction operator (a well known fact).\n### Response #5:\nThe reviewer fully misunderstood our idea. Our convergence proof (see Appendix A in the first submission) consists of two steps: \n\n(1) Prove the actor-critic operator is a \"contraction\", which is one of the sufficient conditions of Fixed Point Theorem. **Notice that the actor-critic operator, which includes an operation to improve the policy (i.e. Eq. 3), is not equal to the Bellman operator!**\n\n(2) Prove actor-critic methods fit Banach's Fixed Point Theorem, thus actor-critic methods converge to a unique optimal policy. \nBesides, **we have removed the convergence analysis** (Sec 3 in the first submission), this does not affect the main idea of our work (i.e. minimize an upper bound of approximation error).\n\n### Comment #6 : \nThe experiments are on only 5 domains. \n### Response #6:\n- We demonstrated the experimental results on **TEN** control tasks (**five of them are in Appendix D in the initial submission**).  For a conference paper, it is common to conduct experiments on only the most difficult tasks. (see TD3 [7] and SAC [8]). In the revised paper, we have moved the five results in the main body of the paper. \n\n## Reference\n[1] Matthieu Geist  et al. A theory of regularized markov decision processes, 2019.\n\n[2] Nino Vieillard  et al. Leverage the average: an analysis of kl regularization in rl, 2020.\n\n[3] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction (Second Edition). 2018. \n\n[4] Konda, V.R. and Tsitsiklis, J.N. Actor-Critic Algorithms. NIPS.1999.\n\n[5] Thomas Degris  et al. Off-policy actor-critic, 2013.\n\n[6] Agarwal, Praveen et al. \"Banach Contraction Principle and Applications\". Fixed Point Theory in Metric Spaces. \n\n[7] Scott Fujimoto et al. Addressing function approximation error in actor-critic methods. ICML 2018.\n\n[8] Tuomas Haarnoja et al. Soft actor-critic. ICML 2018.", "title": "Thank you for your comments, explain some unreasonable comments, and have improved clarity in the revised paper."}, "Wycr5L3RvU": {"type": "rebuttal", "replyto": "Tq8A76BtXk", "comment": "### Comment #1:\n\nHowever, one of the drawbacks of this paper to me is, the authors proposed the double Q learning method. It is not clear how this double Q help to reduce the estimation error?  \n\n### Response #1:\n\nAs mentioned at the beginning of Sec 4.1 in the initial submission, (sec 3.1 in the revised paper), we adopt the clipped double-Q strategy to reduce overestimation, which was proposed by Fujimoto [1]. The key idea of it is to decrease the likelihood of overestimation by increasing the likelihood of underestimation. \n\nThe main idea of our method is to derive an upper boundary of approximation errors for Q function approximator in actor-critic methods, and to reduce the approximation error by minimizing this upper bound of the error.  This is similar to the Expectation-Maximization Algorithm (EM) [2] which maximizes a lower bound of the log-likelihood instead of the log-likelihood directly. \n\nThank you for this comment, in the revision, we have added a very brief discussion about how clipped double Q reduce overestimation. \n\n### Comment #2:\n\nI am also a little bit concerned that the gap between theoretical analysis and the practical algorithm is large. In practice, the authors always use a one-step update, but there is no guarantee that the one-step update for both actor and critic can decrease the error or improve the performance. \n\n### Response #2:\n\nThe main idea of our method is to reduce the approximation error by minimizing a derived upper bound of the error (similar to the EM algorithm). We find that the KL penalty is an effective way. Even using a one-step update, our method indirectly stabilizes the Q function by using the KL penalty when training the policy during the whole training process. The presented results of our ablation study showed that the KL penalty reduced the approximation error (which is shown in Fig 4 in the revision). \n\n### Comment #3:\n\nAlso, since the key idea of how to control the critic is to use a regularization based on the last policy to make sure the policy move not so 'fast'. It would be interesting to ask what will happen if we only use a smaller learning rate for the actor and a larger learning rate for the critic. Some theoretical work [1] [2] have already adopted this method to get the error controlled and make the actor converge to the stationary point. \n\n### Response #3:\n\nThank you very much for recommending two good papers on the two-time-scale actor-critic method. The idea of two-time-scale methods seems similar to the delayed policy update method [1] which only updates the policy after a fixed number of updates to the Q network. Delayed policy update is effective, but the benefits are limited. We will compare these works to KL-penalty in the future work. Thank you!\n\n## Reference \n\n[1] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. ICML 2018.\n\n[2] Christopher M Bishop. *Pattern Recognition and Machine Learning (Information Science and Statistics)*. Springer-Verlag New York, Inc., 2006.", "title": "Responses to the comments"}, "cba7rXe8480": {"type": "rebuttal", "replyto": "HU150XC7ywq", "comment": "We organized all comments as follows:\n\n\n\n**comment#1**:KL-constraint is a good idea--as has been explored several times in literature -- but I don't think this paper cites that work...\n\n**Response#1**: Sorry for this mistake.  We have discussed the suggested papers  (Geist et al. 2019 and Vieillard et al. 2020)  in the revision.  The **difference between the KL-control papers and our work** are summarized as follows:\n\n- Our paper conducted the same conclusion as the KL-control papers by a **different route**.  The authors **directly studied the effect of KL and entropy regularization in RL** and proved that a KL regularization indeed leads to averaging errors made at each update iteration of the value function. While our idea is very different from theirs': It is impracticable to minimize the approximation error directly, instead we tried to **minimize an upper bound of approximation error** (this is similar to the Expectation-Maximization Algorithm (EM) which maximizes a lower bound of the log-likelihood instead of the log-likelihood directly). We derived **an upper boundary of approximation errors for Q function approximator** in actor-critic methods, and found that the error can be reduced by retaining a new policy close to the previous one. \n\n- Our work produced **a more general conclusion**: approximation errors can be reduced by retaining a new policy close to the previous one.  The third term of Eq.19 in the upper bound of approximation errors will be smaller if the more similar the two consecutive policies, $\\pi_{\\theta_{n}}$ and $\\pi_{\\theta_{n-1}}$ are.  **The KL penalty is not the only way**. \n\n- Vieillard et al. used the KL penalty to improve the performance of a DQN, while we proposed a better method (i.e. ECAC) which includes a mechanism to *automatically adjust the coefficient of KL term*.\n \n**comment#2**: In the abstract, it is claimed that the paper performs an *analysis* of this error. I could not find this anywhere in this paper. If this refers to Section 3.2 (Hidden dangers in AC methods), I would not call that \"analysis\". \n\n  **response#2**: Based on this comment, we decide to delete Section 3 because this does not affect main idea of our work, i.e. minimize an upper bound of approximation errors. \n\n\n**comment#3**:Theorem 1: .... Such results have been shown in the absence of function approximation for off-policy actor-critic (Degris et al. 2012). What's new here?\n\n**response#3**:Degris et al. presented convergence proofs of off-policy actor-critic **with a linear approximation of value function**. In contrast, we provide a convergence proof for actor-critic methods using function approximation (**might be linear or non-linear approximation**).  However, for the clarity of this paper, we deleted Section 3.\n\n## Unreasonable comments:\n\n**comment#4**: The reason for why KL-constraints work: I can see what the derivation does and how the authors arrive at the conclusion of KL-constraint I do not buy the argument that KL-constraints help learning because they control TD error\n\n**response#4**: In the first submission, we did not claim that \"KL-constraints help learning because they control TD error\"; however, we do arrive at the conclusion that approximation error (**rather than TD error !**) can be reduced by retaining a new policy close to the previous one.  Therefore, our main idea is to **minimize an upper bound of approximation error to reduce approximation error**.  \n\n**comment#5**:but the baselines utilized are a bit suboptimal, .... .I think the numbers are this unreliable. \n\n**response#5**:  This comment is **an absolutely unreasonable comment**! It seems the reviewer did not have too much experience with RL experiments.\n\n- The performance of RL algorithms can be drastically different by **using different random seed**. Please refer *Deep Reinforcement Learning that Matters* Henderson et al. 2017. While, to ensure fairness, it is a common way to run five experimental trials for each evaluation, each trial with a different preset random seed (**all algorithms in comparative evaluation used the same set of random seeds**). **That is why the baselines are slightly different from SAC and TD3 papers**.\n- Even if we compare the results presented in our paper with the results reported in other SAC and TD3 papers (an unfair way due to different random seeds), **ECAC still outperforms other algorithms**. For example,  SAC scored nearly 11,000 while ECAC scored 12,000 in HalfCheetah at around 1 millionth step. \n- We hope the reviewer will run an check our code [https://github.com/SingerGao/ECAC ] (**no any author information**) and take back this unreasonable comment. \n\n## Summary\n**We thank the reviewer for the comments and expect a more reasonable review**. Sorry for my poor English writing skill. I'm not a native English speaker.  I do expect to participate in international academic communications by writing English papers. ", "title": "Thank the reviewer for the review and expect a more reasonable review!"}, "ufuMIhcQT7": {"type": "rebuttal", "replyto": "n5yBuzpqqw", "comment": "We appreciate all the reviewers, ACs, and PCs. The constructive comments and suggestions from the reviewers significantly improve the quality of our revised paper. We have corrected a number of mistakes and improved clarity in the revised paper. Moreover, the following are two main changes in the revision: \n\n(1) We have removed the convergence analysis (Section 3 in the first submission). This removal does not affect the main idea of our work: derive an upper boundary of approximation errors for the Q approximator and reduce the approximation error by minimizing this upper bound of the error.  This idea is similar to the Expectation-Maximization Algorithm (EM) which maximizes a lower bound of the log-likelihood instead of the log-likelihood directly. \n\n(2) To make our result analysis more rigorous, we add  Table 1 (sec 4.2 in the revision) to show the results of max average return over five runs on all the 10 tasks.\n\n\nWe have provided explanations for some unreasonable comments given by the reviewers. ", "title": "About revision."}, "0Namc6ppAm5": {"type": "rebuttal", "replyto": "wnZ1m814UZO", "comment": "Thanks for the reviewer's careful examination and good suggestions. We have corrected the mistakes in our paper. Most importantly, to make our result analysis more rigorous, we add  Table 1 (Sec 4.2 in the revised paper) to show the results of max average return over five runs on all the 10 tasks.\n\n## Responses to the comments\n\n### comment #1:\n\n[-] The experimental result section can enjoy more rigor. It is not clear how many seeds were used to generate these results. As Joelle discussed in one of NeurIPS conferences, it is easy use limited number of seeds and deduce very different outcomes ([url]). Also why showing min and max of returns instead of something more common such as std-err.\n\n### Response #1:\n\n(1)  We have updated our paper by adding a more clear statement about random seed in *Section EXPERRIMENTS* (Sec 4.2 in our revised paper): \u201cOur results used five random seeds (each individual run applies a random seed) for 1) the Gym simulator, 2) network initialization, and 3) sampling actions from the policy during the training.\u201d\n\n(2) Using the min and max of returns is also used frequently in RL papers (such as: [1], [2], and [3] in the following reference).  In order to monitor the stability of RL algorithms, the general approach is to evaluate the policy frequently, e.g. every 1, 000 time steps. Usually, each task is run for over 1 million time steps. Thus, usually, one algorithm on one task requires at least 1, 000 datapoints. If we plot the results by using error bar  (std-err), the plotting of the experimental results will be very crowded. \n\n(3) To make our result analysis more rigorous, we add  Table 1 (Sec 4.2 in the revised paper) to show the results of max average return over five runs on all the 10 tasks. Thank you very much for this suggestion! \n\n### comment #2:\n\n[-] I think Actor-critic has been shown to converge before. Why the proof using Banach\u2019s Fixed Point Theorem important compared to other proofs?\n\n### response #2:\n\nPrevious studies presented convergence proofs of actor-critic **with a certain approximation of value function**, e.g.  Degris et al. proved the convergence of the off-policy actor-critic **with a linear approximation of value function**.  While we must provide a convergence proof for actor-critic methods using the function approximation (**might be linear or non-linear approximation**).  \n\n**We have removed the convergence analysis** (Section 3 in the first submission).  This removal does not affect the main idea of our work (i.e. minimize an upper bound of approximation error).\n\n### comment #3:\n\nDetailsP2, overestimation. => Need space after . Section 2.2: hypothesis. And the => Remove \"And\" Section 4.2: - that can be views adapting => that can be viewed as adapting - I recommend providing the difference between Q-networks and its target at n+1 iteration in a source distribution before the corresponding value on the target distribution (e.g. replace Eqn 11 and 12 and the accompanying text) - While the difference between D^n and D^n+1 is small, the corresponding error introduced can be big - I believe the use of minimized KL divergence between policies has been practiced before in the RL literature. Section 4.3: ).In order => ). In order Page 7: - Curves are smoothed uniformly for visual clarity\" => What do you mean? - using different random seeds => How many random seeds? Why not mention here? I could not find it in the Appendix either - shaded region to the minimum and maximum returns over the five runs => Why not showing standard error instead to have a better statistical significance understanding visually? - Use vectorized image formats so your images are zoomable without the loss of quality Eqn 23: Q_{ture} => Q_{true} Page 8: - \"with KL limitation have\" => \"with KL limitation has\" Section 5.2: - \"ECAC outperforms all other algorithms on the tasks except\" => What does beating mean? Have you ran statistical analysis or are you relying on the mean values?\n\n### response #3:\n\nWe appreciate the reviewer's careful examination. We have corrected the mistakes and improved clarity in the revision. \n\n## Reference\n\n[1] John Schulman et al. Proximal policy optimization algorithms. *CoRR*, 2017. \n\n[2] Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. ICML 2018.\n\n[3] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. ICML 2018.\n\n[4] Thomas Degris, et al. Off-policy actor-critic, 2013. ", "title": "Thanks for the reviewer's careful examination and good suggestions, and have improved clarity in the revised paper. "}, "CopCioX0eI": {"type": "review", "replyto": "n5yBuzpqqw", "review": "\nClarity:\n---------\nMajor:\n******\nIt is not clear what the actor critic algorithm is. What is presented in Fig 1 seems to be policy iteration, and Theorem 1 seems to be stating policy iteration (yet $\\pi^{n+1}$ has not been defined). \n\nMinor:\n*****\n1) What is $V^{\\pi^n}$? Is it the value function of the policy $\\pi^n$.\n2) What is the difference between $\\pi^{*,n}$ and $\\pi^{n+1}$.\n3) What is $\\pi$ is eq(3)?\n\n\nQuality:\n-----------\n1) The argument following eq (15) is very informal: \"the number of samples in $\\mathcal{D}^{n+1}$ is only a little more than in $\\mathcal{D}^n$\".\n2) For the contraction property of the Bellman operator, the author can consider citing standard text books instead of \"https://towardsdatascience.com/ mathematical-analysis-of-reinforcement-learning-bellman-equation-ac9f0954e19f\"\n\n\nSignificance:\n------------------\n1) The  authors claim \"We present a convergence analysis for actor-critic methods by Banach\u2019s Fixed Point Theorem\" to be one of the contributions. The fixed point theorem here is to show that Bellman operator is a contraction operator (a well known fact).\n2) The experiments are on only 5 domains. \n\n\nOverall Feedback: Adding the KL based penalty is perhaps a useful idea (this may also not be an entirely new one). It will be great if a) the algorithm can clearly stated,  b) the novelty of the method pointed out clearly by comparing it with related works c) the ideas can be supported by formal theoretical statements.", "title": "Seems to be a heuristic (perhaps not entirely new) presented in a not so clear manner", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HU150XC7ywq": {"type": "review", "replyto": "n5yBuzpqqw", "review": "Before beginning with the review, I would strongly request the authors to correct grammar errors, spelling errors, and notation inconsistencies which seriously hinder understanding in some cases.\n\nThis paper explores error accumulation in actor-critic methods. The authors claim to present an analysis of approximation error in actor-critic methods (though I couldn't find any concrete analysis of this fact) and then suggest that bounding the KL-divergence of the policy against the previous policy aids actor-critic algorithms by deriving an upper bound on the change in TD error due to faster changes in the policy.\n\nMy overall opinion is that this paper restates results that are already well known and the general theme of the method ECAC is already explored in RL literature (see KL-control literature). So, I don't really think there is any novel concept presented in the paper. I move to specific points next:\n\n- Theorem 1: I don't see what's novel here. Such results have been shown in the absence of function approximation for off-policy actor-critic (Degris et al. 2012). What's new here?\n\n- Analysis of approximation error: In the abstract, it is claimed that the paper performs an *analysis* of this error. I could not find this anywhere in this paper. If this refers to Section 3.2 (Hidden dangers in AC methods), I would not call that \"analysis\". It states some known facts without actually discussing anything formally there. So, either this claim should be removed or a valid analysis needs to be presented.\n\n- The reason for why KL-constraints work: I can see what the derivation does and how the authors arrive at the conclusion of KL-constraint I do not buy the argument that KL-constraints help learning because they control TD error -- they do stabilize the critic, as would a smaller learning rate for the policy would and lead to lower accumulation of error (for e.g., see A theory of Regularized MDPs, Geist et al. or Leverage The Average: An Analysis of KL regularization in RL) and these analyses are far more sophisticated that the argument presented in this paper. \n\n- The empirical results in the paper are OK, but the baselines utilized are a bit suboptimal, for e.g. SAC and TD3 in HalfCheetah -- having used these myself, it seems like official codebases of these papers do get better performance than what is reported.  I think the numbers are this unreliable.\n\nI think overall the algorithm with the KL-constraint is a good idea -- as has been explored several times in literature -- but I don't think this paper cites that work, nor compares to them along with not a rigorous enough evaluation and poor writing quality.", "title": "Lack of novelty, Inadequate comparison to prior work, Suboptimal baselines ", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "wnZ1m814UZO": {"type": "review", "replyto": "n5yBuzpqqw", "review": "Summary\n=========\nAuthors investigated the effect of approximation error for actor-critic. They derived an upper bound of approximation showing that minimizing the KL divergence between the two consecutive policies can drive this upper bound down. Based on their finding they  introduced the Error Controlled Actor-critic (ECAC) algorithm. They ran ablation study showing the positive impact of minimizing the KL divergence. Furthermore they compared ECAC against 4 state-of-the-art techniques showing their advantage across 4 out of 5 Mujoco domains.\n\n[+] Aside from minor grammar issues (see details) the paper is easy to follow\n\n[+] The trick to calculate alpha and beta automatically is very interesting\n\n[+] Empirical results are encouraging \n\n[-] The experimental result section can enjoy more rigor. It is not clear how many seeds were used to generate these results. As Joelle discussed in one of NeurIPS conferences, it is easy use limited number of seeds and deduce very different outcomes (https://media.neurips.cc/Conferences/NIPS2018/Slides/jpineau-NeurIPS-dec18-fb.pdf). Also why showing min and max of returns instead of something more common such as std-err.\n\n[-] I think Actor-critic has been shown to converge before. Why the proof using Banach\u2019s Fixed Point Theorem important compared to other proofs?\n\nDetails\n=========\nP2, overestimation. => Need space after .\nSection 2.2: hypothesis. And the => Remove \"And\"\nSection 4.2: \n\t- that can be views adapting => that can be viewed as adapting\n\t- I recommend providing the difference between Q-networks and its target at n+1 iteration in a source distribution before the corresponding value on the target distribution (e.g. replace Eqn 11 and 12 and the accompanying text)\n\t- While the difference between D^n and D^n+1 is small, the corresponding error introduced can be big\n\t- I believe the use of minimized KL divergence between policies has been practiced before in the RL literature.\nSection 4.3: ).In order => ). In order\nPage 7: \n\t- Curves are smoothed uniformly for visual clarity\" => What do you mean?\n\t- using different random seeds => How many random seeds? Why not mention here? I could not find it in the Appendix either\n\t- shaded region to the minimum and maximum returns over the five runs => Why not showing standard error instead to have a better statistical significance understanding visually?\n\t- Use vectorized image formats so your images are zoomable without the loss of quality\nEqn 23: Q_{ture} => Q_{true}\nPage 8:\n\t- \"with KL limitation have\" => \"with KL limitation has\"\nSection 5.2:\n\t- \"ECAC outperforms all other algorithms on the tasks except\" => What does beating mean? Have you ran statistical analysis or are you relying on the mean values?", "title": "A more analysis on a known trick in reinforcement learning", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}