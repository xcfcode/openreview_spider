{"paper": {"title": "Learning Locomotion Skills Using DeepRL: Does the Choice of Action Space Matter?", "authors": ["Xue Bin Peng", "Michiel van de Panne"], "authorids": ["xbpeng@cs.ubc.ca", "van@cs.ubc.ca"], "summary": "We compare the impact of four different action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) in terms of learning time, policy robustness, motion quality, and policy query rates.", "abstract": "The use of deep reinforcement learning allows for high-dimensional state descriptors, but little is known about how the choice of action representation impacts the learning difficulty and the resulting performance. We compare the impact of four different action parameterizations (torques, muscle-activations, target joint angles, and target joint-angle velocities) in terms of learning time, policy robustness, motion quality, and policy query rates. Our results are evaluated on a gait cycle imitation task for multiple planar articulated figures and multiple gaits. We demonstrate that the local feedback provided by higher-level action parameterizations can significantly impact the learning, robustness, and quality of the resulting policies.", "keywords": ["Reinforcement Learning", "Applications"]}, "meta": {"decision": "Reject", "comment": "After reading the paper and the reviews, I believe that the paper presents a solid contribution and a detailed empirical exploration of the choice of action space representations for continuous control trajectory tracking tasks, but has limited relevance to the ICLR audience in its present form.\n \n The conclusion that PD controllers with learned gains provide improved learning speed and sometimes better final results than the \"default\" joint torque representation is intriguing and has some implications for future work on trajectory tracking for simulated articulated rigid body characters, but it's unclear from the current results what conclusion there is to take away for general algorithm or model design. Since the only evaluation is on trajectory tracking, it's also not even clear (as pointed out by other reviewers) to what degree these results will generalize to other tasks. In fact, a contrarian view might be that PD tracking is specifically a good fit for trajectory tracking with a known reference trajectory, but might be a poor fit for accomplishing a particular task, where more open-loop behaviors might be more optimal. The passive compliance of MTUs is also shown to often not be beneficial, but it's again not clear whether this might in fact be an artifact of the trajectory tracking task.\n \n But perhaps more problematically, the primary conclusions of the paper are of limited relevance when it comes to design of algorithms or models (or at least, the authors don't discuss this), but are more relevant for practitioners interested in applying deep RL for learning controllers for articulated rigid body systems such as robots or virtual characters. As such, it's not clear what fraction of the ICLR audience will find the paper relevant to their interests.\n \n I would strongly encourage the authors to continue their research: there is a lot to like about the present paper, including an elegant reinforcement learning methodology, rigorous empirical results for the specific case of trajectory following, and some very nice videos and examples."}, "review": {"r1cBrG2Ix": {"type": "rebuttal", "replyto": "r1-cIe5Le", "comment": "Thank you for taking the time to review our latest changes. We agree that we are not yet able to provide definitive conclusions on the impact of action space choices for a general set of problems. A more complete understanding is likely to emerge over the coming years. However our work establishes that the choice of action space is an important and relevant issue because the choice of action space can have a significant impact on performance. As RL devotes increasingly more attention to motion control problems, we hope that our results will inspire more exploration of different action spaces, be it learned or manually-crafted. For the class of problems we have studied, PD targets are nearly always a better choice than Torques, which have become a prevalent choice for recent work in DeepRL.\n\nIn addition, to the best of our knowledge, we implement and evaluate some of the first DeepRL experiments with biologically-inspired action spaces (i.e. MTUs). Though our current MTU policies often underperform the other actions, we believe that we have not yet reach the limit of what can be achieved with better anatomical models.", "title": "Reply"}, "S1mSVYuSg": {"type": "rebuttal", "replyto": "r1kGbydxg", "comment": "We would like to thank the reviewers again for their feedback and suggestions. We have revised the paper to better address the points raised in the reviews. To help track the changes, we have highlighted them in blue. On page 15 of the supplemental material, you will find a new set experiments comparing performance across different initializations of the network, different network architectures, and sensitivity to the amount of exploration noise applied during training. We hope these new additions will help to address some of the previously raised concerns.\n\nWe look forward to receiving additional feedback from the reviewers.\nThank you,", "title": "New Revision"}, "SyMsZIDEg": {"type": "rebuttal", "replyto": "r1kGbydxg", "comment": "Thank you for the feedback and comments in the reviews + questions.\nPlease find below a summary of our further reflections, which we will be incorporating into the paper.\n\nAdditional experiments\n\nWe are currently performing several additional experiments, which we will add to the next version of the paper:\n- Analysis of variance of performance for multiple runs of training (Anon Rev 2)\n- Sensitivity to exploration noise, i.e., covariance matrix, (Anon Rev 2)\n- Impact of network architecture (Anon Rev 1)\n\nRe:   Relevance of paper to ICLR audience\n\n1) While much is known about learning input (state) representations, much less is known about the extent to which output (action) representations matter for continuous control policies, and whether useful \u201coutput representations\u201d can be learned. This paper doesn\u2019t provide all the answers, but it shows that output representations do matter (for locomotion): they significantly impact the learning rate, robustness, overall performance, and policy query rate.\n\n2) Many methods for continuous action spaces use physics-based simulations of locomotion as one of their key examples. However, the use of torques as the defacto unit of action is rather arbitrary; it ignores the notion that the passive properties of  actuation mechanisms, e.g., muscles, can make a significant contribution to motion control, i.e., \u201cintelligence by mechanics\u201d (Blickhan et al., 2007).This can be thought of as a kind of partitioning of the computations between the control and the physical system. Our paper provides a first look at the impact of such \u201cpartitioning\u201d.\n\n3) A question that we would like to draw more attention to is: should musculo-tendon systems (as seen in nature) and other actuation details be considered as part of the control policy or part of the environment?  I.e., can they be safely ignored because the control policy would \u201clearn the useful aspects\u201d anyhow? \n\nRe: generalization of results\n\nHere are our thoughts on this;  we will update the paper to reflect these.\n\n1) Admittedly, we do not provide a concrete answer for the generalization to arbitrary objective functions, and we will clearly state this in the paper. However physics-based locomotion is commonly used as an example in continuous-action RL work. While we do optimize for only one reward function (imitation), we do explore results for different characters, different motions, different query rates, and overall policy robustness. We believe that these are all critical dimensions for testing capabilities and generalization for locomotion control. The imitation objective can also be used as a motion prior and so it is likely to be useful more generally.  It further allows us to produce more natural motions (subjectively speaking) than previously seen for similar work. We hope that the capability of learning more natural locomotion policies, even if this arrives in part via the objective function rather than a theoretical advancement, will be of broad interest to the ICLR community.\n\n2) We believe that no single action parameterization will be the best for all problems. However, since objectives for motion control problems are often naturally expressed in terms of kinematic properties, higher-level actions such as target joint angles and velocities may be effective for a wide variety of motion control problems. We hope that our work will help open discussions around the choice of action parameterizations.\n\nRe: specificity of results to the use of a reference pose cost\n\nWhile the reward terms are mainly calculated according to joint positions and velocities, the real challenge for the control policy lies with: (a) learning to compensate for various state-dependent forces, such as gravity and ground-reaction forces, and (b) learning strategies such as foot-placement that are needed to maintain balance for all the locomotion gaits. The reference pose terms provides no information on how to achieve these \u201chidden\u201d aspects of motion control that will ultimately determine the success of the locomotion policy. There is also only a weak correlation between the action space and the actual reference poses for any of the action spaces; the action space trajectories illustrated in Figure 11 are all quite different from the actual right hip angle trajectory (not shown, but it varies smoothly over time).\n\nRe: considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work. (Anon Rev 1)\n\nWe enforce torque limits and joint limits, which are shared across all actuation models. We do not currently include activation time delays for any of the models.\n", "title": "Reply to reviews"}, "ByceP9xQx": {"type": "rebuttal", "replyto": "S1ZM6FlXx", "comment": "Thank you for your question. We do not currently have the data available to evaluate the sensitivity of the learning with respect to the amount of exploration noise. However while we were manually tuning the values for each action parameterization, we have found the performance to be fairly robust to reasonable ranges of values. Of course, if the covariance of the distribution is too small, then learning will be slow, while excessively large amount of exploration noise can cause the agent's behaviour to be unstable, resulting in frequent falling. As is often the case in RL, there is a large number of hyperparameters and other design decisions that need to be made, but in our experience, once a reasonable set of choices has been found, it tends to work well across the various examples.\n\nThe NCR, with no discounting, is used only for evaluating the performance of policies. During training, discounting is still applied when performing Bellman backup to update the value function [Algorithm 1, Line 21]. The discount factor was tuned with a manual line search on a subset of the examples, to improve final performance and learning speed.\n\nWe believe that with more computational power to performance a more systematic search of the hyperparameter space will lead to better overall performance. But we suspect that the trends we have observed between the different action parameterizations will still be evident.", "title": "Re: Sensitivity to the manually specified parameters"}, "S1ZM6FlXx": {"type": "review", "replyto": "r1kGbydxg", "review": "I was wondering if you have experimented with sensitivity of the learning with respect to the manually specified parameters.  Particularly with respect to covariance matrix Sigma, mentioned in Section 2. \n\nAlso, in Section 2 a [0,1] discount factor is introduced. However, later, in Results, it is mentioned that \"No discounting is applied when calculating the NCR\". Is discounting applied elsewhere? If so, how is it set?\n\nThank you. \n\nPaper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies. Evaluated are different planer gate cycle trajectories. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. \n\n> Significance & Originality:\n\nThe explored parameterizations are relatively standard in humanoid control. The real novelty is systematic evaluation of the various parameterizations. I think this type of study is important and insightful. However, the findings are very specific to the problem and specific tested architecture. Its not clear that findings will transferable to other networks on other control problems/domains. As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics / graphics community. \n\n> Clarity:\n\nThe paper is well written and is pretty easy to understand for someone who has some background with constrained multi-body simulation and control. \n\n> Experiments:\n\nExperimental validation is lacking somewhat in my opinion. Given that this is a fundamentally experimental paper, I would have liked to see more analysis of sensitivity to various parameters and analysis of variance of performance when policy is optimized multiple times.", "title": "Sensitivity to the manually specified parameters", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkmc-WGNx": {"type": "review", "replyto": "r1kGbydxg", "review": "I was wondering if you have experimented with sensitivity of the learning with respect to the manually specified parameters.  Particularly with respect to covariance matrix Sigma, mentioned in Section 2. \n\nAlso, in Section 2 a [0,1] discount factor is introduced. However, later, in Results, it is mentioned that \"No discounting is applied when calculating the NCR\". Is discounting applied elsewhere? If so, how is it set?\n\nThank you. \n\nPaper studies deep reinforcement learning paradigm for controlling high dimensional characters. Experiments compare the effect different control parameterizations (torques, muscle-activations, PD control with target joint positions and target joint velocities) have on the performance of reinforcement learning and optimized control policies. Evaluated are different planer gate cycle trajectories. It is illustrated that more abstract parameterizations are in fact better and result in more robust and higher quality policies. \n\n> Significance & Originality:\n\nThe explored parameterizations are relatively standard in humanoid control. The real novelty is systematic evaluation of the various parameterizations. I think this type of study is important and insightful. However, the findings are very specific to the problem and specific tested architecture. Its not clear that findings will transferable to other networks on other control problems/domains. As such for the ICLR community, this may have limited breadth and perhaps would have broader appeal in robotics / graphics community. \n\n> Clarity:\n\nThe paper is well written and is pretty easy to understand for someone who has some background with constrained multi-body simulation and control. \n\n> Experiments:\n\nExperimental validation is lacking somewhat in my opinion. Given that this is a fundamentally experimental paper, I would have liked to see more analysis of sensitivity to various parameters and analysis of variance of performance when policy is optimized multiple times.", "title": "Sensitivity to the manually specified parameters", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkEIBOgXg": {"type": "rebuttal", "replyto": "Hk5Lad0Mg", "comment": "Thank you for your question. We believe there is a dependency between the reward function and the action space. One trivial example would be a reward that simply tries to minimize the control torques exerted on the joints. In this case the torque policy will be trivial while the PD and velocity policies may be more complex. However, we would like to point out that though the reward terms are mainly calculated according to joint positions and velocities, there are implicit objectives, such as gravity compensation, contact configurations, and end effector forces, which are non-trivial functions of joint positions and velocities.\n\nWe believe that no action parameterization will be the best for all problems, but since objectives for motion control problems are often naturally expressed in terms of kinematic properties, higher-level actions such as target joint angles and velocities might be effective for a wide variety of motion control problems. We hope that this work will help open more discussions around the choice of action parameterizations.\n\nIn the case of an impedance controller, we suspect the local feedback provided by PD and velocity controllers may still prove beneficial.", "title": "Re: Influence of objective and reward functions"}, "Hk5Lad0Mg": {"type": "review", "replyto": "r1kGbydxg", "review": "I was wondering if you think the results might depend significantly on the objective and reward functions? The task at hand is to follow some reference motion (Section 3.1) and it seems that controllers with an action space similar to the target space might have steeper initial learning curves. The reward function (Section 3.4) only takes into account position and velocity, which can be directly influenced by the PD and velocity controllers.\n\nAssume that the objective is to implement an impedance controller instead of a trajectory controller and we evaluate the torque characteristics. Do you think the results will transfer to this or similar cases?This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.\n\nMy biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. \nThe authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  \nMy suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term \"DeepRL\" seems arbitrary.\n\nOn the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. \nIt's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.\n", "title": "Influence of objective and reward functions?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HkrTCGbNx": {"type": "review", "replyto": "r1kGbydxg", "review": "I was wondering if you think the results might depend significantly on the objective and reward functions? The task at hand is to follow some reference motion (Section 3.1) and it seems that controllers with an action space similar to the target space might have steeper initial learning curves. The reward function (Section 3.4) only takes into account position and velocity, which can be directly influenced by the PD and velocity controllers.\n\nAssume that the objective is to implement an impedance controller instead of a trajectory controller and we evaluate the torque characteristics. Do you think the results will transfer to this or similar cases?This paper addresses a question that is often overlooked in reinforcement learning or locomotion experiment.\n\nMy biggest point of critique is that it's difficult to draw conclusions or reason beyond the results of the experiments. \nThe authors only consider a single neural network architecture and a single reward function. For example, is the torque controller limited by the policy network?  \nMy suggestion is to vary the number of neurons or show that the same results hold for a different state representation (e.g. trained on pixel data). In the paper's current form, the term \"DeepRL\" seems arbitrary.\n\nOn the positive side, the paper is well-structured and easy to read. The experiments are sound, clear and easy to interpret. \nIt's definitely an interesting line of work and beyond the extension to 3D, I would argue that considering more realistic physical constraints (e.g. actuator constraints, communication delays etc. on real robots) could greatly improve the impact of this work.\n", "title": "Influence of objective and reward functions?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}