{"paper": {"title": "Long-tail learning via logit adjustment", "authors": ["Aditya Krishna Menon", "Sadeep Jayasumana", "Ankit Singh Rawat", "Himanshu Jain", "Andreas Veit", "Sanjiv Kumar"], "authorids": ["~Aditya_Krishna_Menon1", "~Sadeep_Jayasumana1", "~Ankit_Singh_Rawat1", "himj@google.com", "~Andreas_Veit1", "~Sanjiv_Kumar1"], "summary": "Adjusting classifier logits based on class priors, either post-hoc or during training, can improve performance on rare classes.", "abstract": "Real-world classification problems typically exhibit an imbalanced or long-tailed label distribution, wherein many labels have only a few associated samples. This poses a challenge for generalisation on such labels, and also  makes naive learning biased towards dominant labels. In this paper,  we present a statistical framework that unifies and generalises several recent proposals to cope with these challenges. Our framework revisits the classic idea of logit adjustment based on the label frequencies, which encourages a large relative margin between logits of rare positive versus dominant negative labels. This yields two techniques  for long-tail learning, where such adjustment is either applied post-hoc to a trained model, or enforced in the loss during training. These techniques are statistically grounded, and practically effective on four real-world datasets with long-tailed label distributions. ", "keywords": ["long-tail learning", "class imbalance"]}, "meta": {"decision": "Accept (Spotlight)", "comment": "This paper got 3 acceptance and 1 marginally below the threshold. After the rebuttal, the rating was raised to above the threshold. All the reviewers are positive about this submission. They agree that the method proposed in the submission is novel, the experiments are comprehensive and convincing. AC agrees and recommend acceptance. \n"}, "review": {"ygrg4ruDgP": {"type": "rebuttal", "replyto": "37nvvqkCo5", "comment": "There was an earlier comment regarding the iNaturalist results in Figure 4, which breaks down the errors by classes Many/Medium/Few samples. We can confirm that our ERM implementation achieves an error of 27.9% on the Many bucket, and that it has the expected trend of being worse on the Medium and Few samples. There seems to have been a mixup in the plot, which shall be corrected in the final version.", "title": "Regarding Figure 4"}, "xxhEHRAsbc": {"type": "review", "replyto": "37nvvqkCo5", "review": "Summary:\nThis well-written paper re-visits the idea of logit adjustment to tackle long-tailed problems. The paper begins by setting up a statistical framework and use that to deliver two ways of realizing the logits adjustment effectively. They further prove the potential of such an approach by benchmarking it with several related baselines on both synthetic and natural long-tailed datasets.\n\n###########################################################\n+ves:\n+ The paper motivates the proposed method very well, by exposing the cases of failures of certain existing approaches and addressing those shortcomings in the proposed method.\n+ The explanation about how the proposed methods standout - that is post-hoc logit adjustment with respect to the weight normalization and logit adjusted loss with respect to the LDAM loss is clear and well written.\n+ The proposed methods are very versatile as it can both be incorporated into the training, used after the training, and in combination with each other.\n+ The experiments and the analysis are both comprehensive, and the paper has a nice technical depth to it.\n+ The paper is very well-written.\n\n###########################################################\nConcerns/Potential Improvements:\n- There is no justification as to why results on synthetic datasets are provided only for the imbalance ratio of 100 while it is a community norm to benchmark on a range of imbalance ratios (typically 100, 50, 20, 10, 1)\n- Weight normalization has a term multiplied to the logits and the proposed post-hoc logit adjustment has a term added to the argmax of the logits. Therefore both the terms are independent of each other. It would be interesting to see how both of these methods work in conjunction. Does it improve the results or the cons of one just penalizes the pros of another?\n\n###########################################################\nMinor editorial/typo issues:\n- For post-hoc logit adjustment, the paper provides a separate subsection named \u201cCOMPARISON TO POST-HOC WEIGHT NORMALISATON\u201d. It would have been nice to see something similar for logit adjusted loss. The content is already there in the paper, and just has to be modified a bit to stand out separately.\n- Near equation 9, the paper mentions that \u201c(8) immediately suggests two means of optimizing for the balanced error\u201d and goes on to provide the two methods. This was not very evident from (8). Some description to link would have been nice. \n\nPOST-REBUTTAL:\n\nI thank the authors for their response. I am happy with the responses to my concerns/questions, and retain my decision of Accept.", "title": "Review of AnonReviewer2", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "G5aAtvF1LyH": {"type": "review", "replyto": "37nvvqkCo5", "review": "This papers presents a general loss function for long-tail classification with several previous work as its special cases.\n\nThis is a well-written paper, and the results are impressive. The approach builds upon prior work and a general framework is presented. The proposed approaches are eveluated on several commonly used datasets and show some improvements. \n\nMy one major technical concern are as follows:\n1. The originality of this paper is not very high since the proposed framework and its components are not novel (there might be some minor novelty such as the fisher consistency property of the objective);\n2. Regarding the post-hoc logit adjustment, I am supposing it is sensitive to $\\pi_y^\\tau$, which is not very much similar with weight normalization;\n3. For the balanced error, I am interested in why it is supposed to a better performance measure, given that the test data distribution is uniform (as per datasets used in experiments);\n4. In the experiments, e.g., Table 2, in my humble opinion, some of resfults for comparison methods are incorrect. Since prior work (including Weight normalisation, Adaptive, Equalised) report Top-1 classification error, instead of the balanced error. Hence, I guess that the comparison is not fair at all.\n\n========== after reading the authors feedback =========\n\nThanks the authors for addressing my concerns and I am convinced that this work is very much different from prior literature. In addition, the evaluation metrics are correct in the studied problem setup. Based on that, I would like to raise my score from 5 to 6.", "title": "This papers presents a general loss function for long-tail classification with several previous work as its special cases.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Dq_xZ2DZECO": {"type": "review", "replyto": "37nvvqkCo5", "review": "\nSummary:\nThis paper proposes an unifying statistical framework for imbalanced or long-tailed data, where the number of samples for some of the classes may be extremely small compared with other classes. Previous methods work empirically well, but was not consistent, meaning that even in the infinite sample limit, the minimiser doesn't result in a minimal balanced error. The paper proposes a framework based on logit adjustment in two ways: a post-hoc logit adjustment way and another way that injects the logit adjustment to the softmax cross-entropy loss function directly. The paper shows that they are both consistent. Experiments show that the proposed methods work better than previous methods.\n\nPros:\nI feel the paper is very well organized and the story of the paper is clear.  The paper explains the issues of recent methods such as weight normalization and loss modification methods in long-tail learning, and propose a simple statistical formulation with consistency guarantee. It gives further theoretical insights for the binary classification case.\n\nThe synthetic data experiments are well designed, and it gives the reader a better understanding of the behavior of the proposed method. The proposed method matches the Bayes optimal decision boundary, while previous methods and naive ERM seem to be worse, demonstrating the characteristics of proposed and previous methods. The ablation study of the scaling parameter shows how a default value of 1 is already good enough in most cases, but can be tuned for further performance gains.\n\nCons:\nThe ImageNet-LT results were shown in the Appendix, but the proposed Post-hoc correction seems to be slightly worse than weight normalization. I feel readers will be more comfortable to have this in the main paper.\n\nOther comments:\nWhat does the gray-colored highlights in Table 2 mean? If it means the best performing method, then for CIFAR-10-LT, weight normalisation should be highlighted instead of logit adjustment loss.\n\nIt would be nicer to explain that \"the proofs for the two theorems are in the supplementary\" in page 6. Since the supplementary was in a separate PDF file, I didn't notice this when I first read this paper.\n\nOn page 8, it says \"See Appendix D.4 for a plot on ImageNet-LT\", but this should be D.2 instead.\n\n*******************\nAfter rebuttal period: thank you for answering my questions and for updating the paper. I still think it is a good paper and would like to keep my score.", "title": "Simple algorithm for long-tail learning with Fisher consistency", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "4vcc9MzoNXM": {"type": "rebuttal", "replyto": "G5aAtvF1LyH", "comment": "Thanks for your encouraging review, and the detailed feedback!\n\n*# The originality of this paper is not very high since the proposed framework and its components are not novel (there might be some minor novelty such as the fisher consistency property of the objective);*\n\nCompared to prior work, we:\n1) Establish limitations with some existing techniques both theoretically (Sec 4.2, Sec 5.2) and empirically (Sec 2.1, 6.1)\n2) Provide a unified pairwise margin loss (Eqn 10) that captures existing techniques as special cases, and highlights the favourable statistical grounding of our proposed loss (Theorem 2)\n3) Empirically demonstrate on modern long-tail benchmarks that logit adjustment techniques can be superior or competitive with existing techniques.\n\n*# Regarding the post-hoc logit adjustment, I am supposing it is sensitive to \u03c0y\u03c4, which is not very much similar with weight normalization*\n\nPlease note that Table 2 only reports results for logit adjustment with \u03c4 = 1, which requires no tuning; these results are already generally superior or competitive to existing techniques.\n\nFurther, Figure 3 shows how performance varies with \u03c4 for both weight normalisation and logit adjustment. The two methods have similar trends, and in particular logit adjustment does not have greater sensitivity to the choice of \u03c4.\n\n*# For the balanced error, I am interested in why it is supposed to a better performance measure, given that the test data distribution is uniform (as per datasets used in experiments)*\n\nThe reviewer is correct that the benchmarks we report results on have a balanced *test* label distribution. However, this need not be true in real-world applications. e.g., if building a predictor for whether a patient has a disease affecting 1% of the population, it is unlikely that the test set will comprise a balance of patients with and without the disease.\n\nIn settings with imbalanced test data, the balanced error is more informative than the misclassification error. Consider a trivial classifier that predicts every patient as not having the disease: this would get a misclassification error of 1% (which appears artificially good), but a balanced error of 50% (which correctly deems the classifier to be no better than random guessing).\n\nFurther to the above, note that since the *training* label distribution is imbalanced, optimising for the balanced versus misclassification error here can strongly impact performance on rare labels. For more discussion on the merits of balanced error under class imbalance, please see [Brodersen et al., 2010], [Menon et al., 2013], [Koyejo et al., 2014].\n\n*# In the experiments, e.g., Table 2, in my humble opinion, some of resfults for comparison methods are incorrect. Since prior work (including Weight normalisation, Adaptive, Equalised) report Top-1 classification error, instead of the balanced error. Hence, I guess that the comparison is not fair at all.*\n\nPlease note that the balanced accuracy and top-1 accuracy are *equivalent* for these datasets, and so the comparisons are fair. As the reviewer notes, the test sets for these benchmarks are all balanced. Consequently, P(y) = 1/L for each y. Thus, the balanced accuracy in (2) is equivalent to weighting each per-class accuracy by P(y), which is exactly the standard accuracy. We have clarified this prior to the discussion of results.\n\nTo further clarify, employing the balanced error also compares the top-1 prediction of the model (i.e., highest scoring logit) against the true label. We have clarified this following Equation 2.", "title": "Response to R4"}, "RZtmoYRq7B-": {"type": "rebuttal", "replyto": "Dq_xZ2DZECO", "comment": "Thanks for your encouraging review, and the detailed feedback!\n\n*# The ImageNet-LT results were shown in the Appendix, but the proposed Post-hoc correction seems to be slightly worse than weight normalization. I feel readers will be more comfortable to have this in the main paper.*\n\nNote that the gains from weight normalisaton in this case were shown in Table 2. We agree however that including the plots in the body makes this clearer, and have updated Figure 3 and 4 accordingly.\n\n*# What does the gray-colored highlights in Table 2 mean? If it means the best performing method, then for CIFAR-10-LT, weight normalisation should be highlighted instead of logit adjustment loss.*\n\nThe highlighted cells indicate the best performing method. We have corrected the highlighting on CIFAR-10-LT. \n\nPlease note that Table 2 originally compared weight normalisation with the *optimal* tau against logit adjustment with \u03c4 = 1 (which is somewhat unfair to our technique). For CIFAR-10-LT, with cross-validation to select \u03c4, post-hoc logit adjustment achieves a superior performance of 18.73%. We have added a discussion of this in the \u201cResults and analysis\u201d section. We have also added results for weight normalisation with \u03c4 = 1 into Table 2, for clarity.\n\n*# It would be nicer to explain that \"the proofs for the two theorems are in the supplementary\" in page 6.*\n\nWe have added a comment to this effect.", "title": "Response to R1"}, "Py6M3mgPwHX": {"type": "rebuttal", "replyto": "Bx7-7_fGApb", "comment": "Thanks for your encouraging review, and the detailed feedback!\n\n*# From the left panel of Fig.2 it seems that the error bar of logit adjusted is even thinner than that of the Bayes predictor. It would be better to provide some explanation.*\n\nThanks for spotting this: the original Figure 2 was erroneously reported for a smaller (10) number of trials, and the Bayes result was affected by a single trial having a larger error. We have updated it to be the result for 100 trials, and the error bars are now commensurate.\n\n*# It seems that the proposed framework could work well with linear classifiers. Does it also applies to other classifiers such as cosine classifier?*\n\nThe proposed framework should be applicable whenever the underlying model produces reasonable probability estimates. Classifiers based on normalised embeddings can typically achieve this, and so we do not foresee difficulties here. Exploring this would be of interest in future work.\n\n*# Captions of tables should be put above the table contents.*\n\nWe have updated this, and fixed all typos.", "title": "Response to R3"}, "cVCDfjXip_": {"type": "rebuttal", "replyto": "xxhEHRAsbc", "comment": "Thanks for your encouraging review, and the detailed feedback!\n\n*# There is no justification as to why results on synthetic datasets are provided only for the imbalance ratio of 100 while it is a community norm to benchmark on a range of imbalance ratios (typically 100, 50, 20, 10, 1)*\n\nWe have now included in Appendix D.3 experiments with multiple imbalance ratios. The trends are consistent with the case of imbalance ratio 100, as reported in the original submission. As expected, as the imbalance ratio increases, so does the gap between baselines and the Bayes/logit adjusted predictor.\n\n\n*# Weight normalization has a term multiplied to the logits and the proposed post-hoc logit adjustment has a term added to the argmax of the logits. Therefore both the terms are independent of each other. It would be interesting to see how both of these methods work in conjunction. Does it improve the results or the cons of one just penalizes the pros of another?*\n\nWe have experimented with combining post-hoc weight normalisation and logit adjustment. However, we generally have not found significant gains from doing so. This could be because both techniques have the effect of increasing the logits of rare classes, albeit in different ways. Further exploring means of combining these techniques could definitely be an interesting direction for future study.\n\n\n*# For post-hoc logit adjustment, the paper provides a separate subsection named \u201cCOMPARISON TO POST-HOC WEIGHT NORMALISATON\u201d. It would have been nice to see something similar for logit adjusted loss. The content is already there in the paper, and just has to be modified a bit to stand out separately.*\n\nWe have followed the reviewer\u2019s suggestion, and added a new subsection heading 5.2 for contrasting against loss modification techniques.\n\n*# Near equation 9, the paper mentions that \u201c(8) immediately suggests two means of optimizing for the balanced error\u201d and goes on to provide the two methods. This was not very evident from (8). Some description to link would have been nice.*\n\nWe have updated the text following (8) to further clarify how these techniques are arrived at.", "title": "Response to R2"}, "Bx7-7_fGApb": {"type": "review", "replyto": "37nvvqkCo5", "review": "[Summary]\n\nThis paper provides a statistical framework for long-tail learning by revisiting the idea of logit adjustment based on the label frequencies. The proposed framework then yields two variant techniques that follow the paradigm of weight normalization or loss modification. Compared with the existing methods, the proposed methods are generalized and Fisher consistent for minimizing the balanced error. Finally, empirical results on four real-world datasets validate the effectiveness and statistical grounding of the proposed methods.\n\n[Pros]\n-\tThe idea of this paper is novel and interesting. The proposed framework is proved to be Fisher consistent for minimizing the balanced error and generalizes several recent methods for long-tail learning. Meanwhile, some insights about the logit adjustment technique are also revealed to help understanding.\n-\tThe experiments are sufficient and supportive to validate the effectiveness and statistical grounding of the proposed methods.\n-\tThe paper is well organized, which makes it easy to grasp the core idea.\n\n[Cons]\n-\tFrom the left panel of Fig.2 it seems that the error bar of logit adjusted is even thinner than that of the Bayes predictor. It would be better to provide some explanation.\n-\tIt seems that the proposed framework could work well with linear classifiers. Does it also applies to other classifiers such as cosine classifier?\n-\tCaptions of tables should be put above the table contents.\n-\tBesides, there are some grammatical errors and typos to be corrected. Some are lists as follows:\n  1)\tPage 3, \u2018an non-decreasing transform\u2019 should be \u2018a non-decreasing transform\u2019;\n  2)\tPage 4, \u2018which overcome\u2019 should be \u2018which overcomes\u2019;\n  3)\tPage 5, \u2018has negative score\u2019 should be \u2018has a negative score\u2019.\n  4)\tPage 5, \u2018another label with positive score\u2019 should be \u2018another label with a positive score\u2019.\n", "title": "A novel statistical framework for long-tail learning that pursues Fisher consistent for minimizing the balanced error.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}