{"paper": {"title": "Linear Last-iterate Convergence in Constrained Saddle-point Optimization", "authors": ["Chen-Yu Wei", "Chung-Wei Lee", "Mengxiao Zhang", "Haipeng Luo"], "authorids": ["~Chen-Yu_Wei1", "~Chung-Wei_Lee1", "~Mengxiao_Zhang2", "~Haipeng_Luo1"], "summary": "We prove Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) converge exponentially fast to the Nash equilibrium in the sense of last-iterate in various game settings including matrix games.", "abstract": "Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) for saddle-point optimization have received growing attention due to their favorable last-iterate convergence. However, their behaviors for simple bilinear games over the probability simplex are still not fully understood --- previous analysis lacks explicit convergence rates, only applies to an exponentially small learning rate, or requires additional assumptions such as the uniqueness of the optimal solution.\n\nIn this work, we significantly expand the understanding of last-iterate convergence for OGDA and OMWU in the constrained setting. Specifically, for OMWU in bilinear games over the simplex, we show that when the equilibrium is unique, linear last-iterate convergence is achievable with a constant learning rate, which improves the result of (Daskalakis & Panageas, 2019) under the same assumption. We then significantly extend the results to more general objectives and feasible sets for the projected OGDA algorithm, by introducing a sufficient condition under which OGDA exhibits concrete last-iterate convergence rates with a constant learning rate. We show that bilinear games over any polytope satisfy this condition and OGDA converges exponentially fast even without the unique equilibrium assumption. Our condition also holds for strongly-convex-strongly-concave functions, recovering the result of (Hsieh et al., 2019). Finally, we provide experimental results to further support our theory. ", "keywords": ["Saddle-point Optimization", "Optimistic Mirror Decent", "Optimistic Gradient Descent Ascent", "Optimistic Multiplicative Weights Update", "Last-iterate Convergence", "Game Theory"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors propose to provide fast convergence results for the OGDA and OMWU algorithms based on a reinterpretation of the metric subregularity in the saddle point problem setting. During the rebuttal period, the paper improved significantly, not only due to the diligence of the authors but also due to reactive reviewers that provided extremely constructive comments. The technical developments are quite nice: Lemma 2 allows constant step-size parameter as compared to Daskalakis and Panageas, followed by Theorem 3, which establishes the first linear rate under the saddle point metric subregularity. The numerical demonstrations are also helpful in driving the point home. Although it is not surprising that the shape of the polytope matters, it is still impactful to see the linear rate. \n\n\nps. The authors should consider including a related work comparison to the reflected FB algorithm in [1] since it reduces to the FoRB and it also provides convergence analysis for the sequence in the general monotone inclusions. \n\n[1] Cevher and Vu, \"A reflected forward-backward splitting method for monotone inclusions involving Lipschitzian operators,'' \nhttps://arxiv.org/pdf/1908.05912.pdf"}, "review": {"VipahfFX4V8": {"type": "review", "replyto": "dx11_7vm5_r", "review": "This paper studies Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weights Update (OMWU) for solving minimax problem. For OMWU, it shows that if the equilibrium is unique and the objective is x^Ty, then a constant linear stepsize results in a linear convergence for the last iterate. For OGDA, it shows that, with constant stepsize, the average duality gap converges with slow rate. Moreover, it shows that under an extra condition, the last iterate converges linearly as well.\n\n***\nStrength: The paper is solid in theorem and the proof seems correct as far as I checked. It is in a good writing and highly readable. The convergence is interesting and novel in the sense that it shows explicit linear rate with constant stepsize.\n\n***\nConcern: My main concern is about the SP-RSI. Considering it has not beed discussed in other papers, it would be better to provide a more general nonlinear objective class (besides bilinear games on polytopes) that satisfies this condition. Moreover, the role of constraint set in this paper is not clear to me. Is the constraint sets X and Y essential to the problem? What's the difficulty in constrained minimax over unconstrained minimax?\n\n***\nFor future improvement, I suggest to discuss SP-RSI more deeply, since this condition is added to overcome the intermediate difficulty in technical proof. Also the authors can be more clear about the role of constraint sets. If noting special, it'd better not emphasize the setting is constrained.\n\n***\nDuring rebuttal: I thank the toy examples provided by the authors. \n", "title": "Recommendation to Accept", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "mF_bV64807N": {"type": "review", "replyto": "dx11_7vm5_r", "review": "This paper studies optimistic gradient descent ascent (OGDA) and optimistic multiplicative weights update (OMWU) in the constrained convex-concave min-max optimization setting. For OMWU, under the assumption of unique minimum, the authors show linear rate of convergence of bilinear minmax problems over simplices. For OGDA in constrained setting, the authors show linear convergence under some error bound conditions which the authors name as saddle-point restricted secant inequality (SPRCI). Moreover, the authors prove $1/\\sqrt{T}$ rate for the average of duality gap for OGDA.\n\nEven though the results of the paper can be significant, the authors fail to mention important related works, therefore it is not clear how the contributions of the paper adds onto what is already known in these related works. Below I will list my concerns:\n\nOMWU:\n- For OMWU, the authors state that even though the analysis might look similar to Daskalakis&Panaegas, the new analysis is *very* different. Here, I would like to see more explanations. What are the different tools that the authors use to improve Daskalakis&Panaegas? I do not find it convincing when I see subjective adjectives such as \"very different\" analysis: I would like to see what exactly the contribution of the analysis on top of Daskalakis&Panaegas. It does not need to be very low-level details, but I would like to see some high level discussion of the novelty of the techniques here. \n\nLooking at the analysis of Thm 3, I see that the authors use some local arguments depending on $T_0$ to get a term $\\alpha^{T_0 - t}$ for some $\\alpha$. Then the authors make $\\alpha^{T_0}$ to the constant. Here, my concern is that a constant exponential in $T_0$ might be too large. For example, how does this rate compare to standard sublinear rate of OMWU? If the constants of the linear rate are very pessimistic, then both in theory and in practice, sublinear rate might be better.\n\nOGDA:\n- I think the main reference the authors are missing is FORB by [1]. FORB is known to be equivalent to OGDA in the unconstrained setting. Therefore, it can be seen as a specific version of OGDA in constrained case. How does the constrained OGDA the authors propose in this paper differ from FORB which is already given in constrained setting?\n\nFor instance, FORB gets $1/T$ rate [2] for the average of duality gap, whereas this paper gets $1/\\sqrt{T}$. Why does the rate degrade in this paper? It is well known for VIs that the average of duality gap has $1/T$ rate as in [2], then one can use convexity to convert this rate to a rate on the averaged iterate. It is worth noting that the rate referred in this paper due to Golowich et al., is on the last iterate, therefore not comparable to this paper. It is known in Golowich et al., and earlier due to [7] that $1/\\sqrt{T}$ is essentially tight for last iterate. However, for the averaged duality gap this paper considers this is not the case, and $1/T$ is known to be obtained with averaging ([2] and many others).\n\n- There exist a big literature on error bound conditions [6], which I suspect to be related to SPRSI proposed in this paper. I think the authors need to add the related work on metric subregularity (MS) and compare their results with the algorithms utilizing metric subregularity for linear convergence [3, 4, 5]. Moreover, I think it is necessary to see the relation of SPRSI with metric subregularity. For example, it is well known that MS holds for piecewise linear quadratic functions which include the setting of bilinear games over polytopes that the authors consider. Moreover, MS also holds for strongly convex strongly concave games. I suspect using MS in FORB analysis can directly yield similar linear convergence rates to this paper. Then, what is the advantage of SPRSI and the new constrained OGDA compared to FORB?\n\nMoreover, since the contribution of the paper is on linear convergence, how tight is the rate, and how good the condition SPRSI for detecting structure? For example, what happens when the problem is strongly convex-concave, how is the rate derived in this paper, compare to linear rate of [1] and others on similar algorithms? Similarly, can the authors compute the linear rate explicitly for some toy problems and then compare with the performance in practice to see tightness?\n\n========= after discussion with authors ========\nDuring the discussion phase, the authors addressed my concerns and improved their results. Therefore I increase my score to reflect this.\n\n[1] Malitsky, Yura, and Matthew K. Tam. \"A forward-backward splitting method for monotone inclusions without cocoercivity.\" SIAM Journal on Optimization 30.2 (2020): 1451-1472.\n\n[2] B\u00f6hm, Axel, et al. \"Two steps at a time--taking GAN training in stride with Tseng's method.\" arXiv preprint arXiv:2006.09033 (2020).\n\n[3] Latafat, Puya, Nikolaos M. Freris, and Panagiotis Patrinos. \"A New Randomized Block-Coordinate Primal-Dual Proximal Algorithm for Distributed Optimization.\" arXiv preprint arXiv:1706.02882 (2017).\n\n[4] Liang, Jingwei, Jalal Fadili, and Gabriel Peyr\u00e9. \"Convergence rates with inexact non-expansive operators.\" Mathematical Programming 159.1-2 (2016): 403-434.\n\n[5] Alacaoglu, Ahmet, Olivier Fercoq, and Volkan Cevher. \"On the convergence of stochastic primal-dual hybrid gradient.\" arXiv preprint arXiv:1911.00799 (2019).\n\n[6] Rockafellar, R. Tyrrell, and Roger J-B. Wets. Variational analysis. Vol. 317. Springer Science & Business Media, 2009.\n\n[7] Davis, Damek, and Wotao Yin. \"Convergence rate analysis of several splitting schemes.\" Splitting methods in communication, imaging, science, and engineering. Springer, Cham, 2016. 115-163.", "title": "Interesting results, however key comparisons with related works are missing, making it difficult to gauge the significance", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ac09no-xJN9": {"type": "rebuttal", "replyto": "26izprecW-p", "comment": "**5. The importance of single call methods**\n\nEG may have some advantage over single-call methods like OGDA. However, OGDA can find the equilibrium by simply letting two players play the game repeatedly following the game protocol, while EG requires more \u201ccoordination\u201d between the players. The decoupled nature of OGDA enables it to work under the case when the other player is arbitrary or adversarial, and still achieve the \u201cno-regret\u201d performance (please see [Chiang et al. 2012, Rakhlin and Sridharan 2013] for its regret bound analysis). \n\nIn the EG method, the \u201ctrue update\u201d of the learner happens on every second interaction with the opponent.  The opponent can cause high regret to the learner by playing very different strategies on odd and even interactions. This largely limits the application of EG to online learning in adversarial environments. This observation is formalized in [Golowich et al. 2020b, Appendix A.3]. More motivation of having this kind of robustness against adversarial opponents and the limitation of EG can be found in e.g. [Bowling 2005, Syrgkanis et al. 2015, Lei 2020,Golowich et al. 2020b]. \n\nTherefore, single-call methods are more widely applicable and should be given independent attention. \nIn fact, Gidel et al. 2019 also show that OGDA requires less gradient calls than EG and demonstrate that the former converges faster than the latter empirically (see their Figure 3).\n\n**6. The smaller stepsize choice of $1/(8L)$ compared with $1/(2L)$ Popov's algorithm and FORB**\n\nIn fact, we can replace the learning rate for OGDA by any real value smaller than $1/(2L)$. This only changes some universal constants in our bounds. See Footnote 3 in the updated version. We still keep the learning rate $1/(8L)$ to be consistent with the one for OMWU, where some technical lemmas indeed require $\\eta \\le 1/8$.\n\n**7. The possibility of relaxing the assumption on bounded feasible domain (metric subregularity (maybe also SP-RSI, though authors should verify this) holds with polyhedral sets)**\n\nAbout your comments on Lemma 4, in fact we did not use the boundedness assumption in the proof of Lemma 4, so Lemma 4 is unchanged. In the revised version, we remove one of our main assumption (the assumption that $\\mathcal{Z}$ is bounded), and slightly change (weaken) Definition 1. With this change, our main convergence theorem (Theorem 8) now does not rely on the boundedness assumption. \n\nAs we have shown the equivalence between MS and our condition SP-MS, and as you pointed out, MS holds for polyhedrons, linear convergence for bilinear games in unbounded polyhedron should also happen in our case. The intuition is also easy to see: notice that with an application of our Lemma 1 on $t=1, 2, \\ldots, T-1$ with $z^* = \\Pi_{\\mathcal{Z}^*}(\\hat{z}_1)$, by summing up all $T-1$ inequalities and noting that $\\hat{z}_1 = z_0$ by definition, we get \n\n$$\\\\|\\\\hat{z}\\_T - \\\\Pi_{\\mathcal{Z}^*}(\\hat{z}_1) \\\\|^2\\\\leq \\\\|\\hat{z}_1 - \\Pi _{\\mathcal{Z}*}(\\hat{z}_1)\\\\|^2.$$\n\nIn other words, $\\hat{z}_T$ all lie within a bounded region given the initial point $\\hat{z}_1$ (for all $T$). Therefore, we can view all updates only happening within a bounded subset of the polyhedron, and we can apply our conclusion for polytopes to argue the linear convergence for unbounded polyhedrons. \n\nReference:\n\n[Tseng 1995] On linear convergence of iterative methods for the variational inequality problem \n\n[Malitsky 2019] Golden ratio algorithms for variational inequalities\n\n[Chiang et al. 2012] Online Optimization with Gradual Variations\n\n[Golowich et al. 2020a] Last Iterate is Slower than Averaged Iterate in Smooth Convex-Concave Saddle Point Problems\n\n[Golowich et al. 2020b] Tight last-iterate convergence rates for no-regret learning in multi-player games\n\n[Rakhlin and Sridharan 2013] Optimization, Learning, and Games with Predictable Sequences\n\n[Hsieh et al. 2019]: On the Convergence of Single-Call Stochastic Extra-Gradient Methods\n\n[Gidel et al. 2019]: A Variational Inequality Perspective on Generative Adversarial Networks\n\n[Bowling 2005] Convergence and No-Regret in Multiagent Learning\n\n[Syrgkanis et al. 2015] Fast Convergence of Regularized Learning in Games\n\n[Lei et al., 2020]: Last iterate convergence in no-regret learning: constrained min-max optimization for convex-concave landscapes\n\n[Gilpin et al. 2008] First-Order Algorithm with $O(\\ln(1/\\epsilon))$ Convergence for -Equilibrium in Two-Person Zero-Sum Games\n", "title": "Response to AnnoReviewer3 (Part 2/2)"}, "et2rFk4Ko2e": {"type": "rebuttal", "replyto": "26izprecW-p", "comment": "Thanks again for the detailed comments. We are glad that you recognize our contribution in the OMWU part, and give many helpful suggestions for the OGDA part.  Below we provide detailed response to your questions in the latest post.  \n\n**1. how important the ''caveat'' mentioned by the authors to compare constrained OGDA and FORB algorithms.**\n\t\nThe subtle difference between OGDA and FORB may not affect the convergence property of the two algorithms. However, in our paper, we adopt a repeated game protocol as we describe in the second paragraph of Section 1, where the players only access the gradient at the point $(x_t, y_t)$. On the other hand, reflected gradient methods (including the forward-back algorithm) assume access to the gradient at $2x_t - x_{t-1}$, which may fall outside the feasible set (please see Section 3 of [Hsieh et al. 2019]). Therefore, they should be used with care in this setting.  \n\n\n\n**2. The authors should clarify why their averaging of duality gaps is comparable to averaging of operator norms as in Golowich et al. Theorem 10.**\n\nTo see why the duality gap bound is comparable to the operator norm bound, consider the following example where these two have the same order. Let $G$ be a bilinear game whose equilibrium is bounded away from the boundary of the feasible set (e.g., paper-scissor-rock). The duality gap of $(x_t,y_t)$ would be $\\alpha_t = \\max_{x\u2019, y\u2019} (x_tGy\u2019 - x\u2019Gy_t)=\\max_{z\u2019} F(z_t)^\\top (z_t-z\u2019) \\leq \\|F(z_t)\\|$ (assuming the diameter is $1$ WLOG), where the last quantity is exactly the operator norm at time $t$.\n\nOn the other hand, for a large enough $t$ such that $\\|z_t-z\\| \\geq C$ for any $z$ on the boundary (always exists since the equilibrium is away from the boundary), we have \n$$\\alpha_t = \\max_z \\left(\\frac{ F(z_t)^\\top (z_t-z) }{\\|z_t-z\\|}\\times \\|z_t-z\\|\\right) \\geq \\max_{z\\in\\text{boundary}} \\left(\\frac{ F(z_t)^\\top (z_t-z) }{\\|z_t-z\\|}\\times \\|z_t-z\\| \\right)\\geq \\|F(z_t)\\| \\times C,$$\nwhere the last step is because $\\|F(z_t)\\|=\\max_{z\u2019} \\frac{F(z_t)^\\top (z_t-z\u2019)}{\\|z_t-z\u2019\\|}$ and the maximum is always attainable on the boundary since this is a bilinear game. This shows that $\\alpha_t$ and $\\|F(z_t)\\|$ are of the same order.\n\n**3. From what I understand from remarks of the authors, Gilpin et al.'s condition is weaker than SP-RSI.**\n\n[Gilpin et al. 2008] only focused on two-player zero-sum matrix games in probability simplex, so besides the fact that their algorithm is different from ours, their result is also less general than ours (our condition with $\\beta=0$ includes general polytopes, and $\\beta>0$ includes other bilinear games with non-polytope feasible set like the example in Theorem 9). \n\n**4. I would also guess that metric subregularity is weaker than SP-RSI. Proper comparison and giving proper credit to previous works using metric subregularity. It is not hard to apply metric subregularity to standard extragradient algorithms to obtain linear rates.**\n\nWe thank the reviewer for pointing out metric subregularity and related references. We found that it is indeed closely related to our conditions. \nSpecifically, for the new form of our condition in the revision, we prove that the case with $\\beta=0$ is equivalent to the metric subregularity of an operator defined in terms of the normal cone of the feasible set and the gradient of the objective.\nWe have added the discussions and the proof of equivalence to the paper (please see Appendix F). \n\nOur condition is also similar to other error bound conditions in previous works on extragradient algorithms such as [Tseng 1995, Malitsky 2019]. We also included more discussions and citations on this in the revised version. Because error bound conditions and MS are closely related, we agree that MS should indeed lead to linear convergence rates for these algorithms. However, we are still not aware of any previous work that argues linear convergence for constrained single-call algorithms under the MS condition or other conditions comparable to or weaker than ours. We emphasize that the analysis of OGDA is not a straightforward extension of the analysis of EG and could be more challenging, as also observed by a concurrent work of [Golowich et al. 2020b] (e.g., discussions in their Section 1.1). ", "title": "Response to AnnoReviewer3 (Part 1/2)"}, "VfyOVtfF-M": {"type": "rebuttal", "replyto": "dx11_7vm5_r", "comment": "We thank all reviewers for their valuable comments, from which we learn a lot.  We have made our first revision that addresses most of the major concerns.  One notable change is that we slightly modify the convergence conditions for OGDA (including its name), so that it has a better connection with metric subregularity and other error bound conditions that have been used in the literature to show linear convergence for other algorithms (addressing issues raised by R3 and R4). Note that the new condition is **weaker** than the original one (thus making our results stronger), and it also covers the case of unbounded feasible sets (addressing issues raised by R3 and R4). We have also included more discussions and references related to this condition. \n\nIn more detail, our revision incorporates the following:\n\n**Section 1 (Introduction):**  \n- Added more motivations of studying OGDA or single-call algorithms (R3)\n\n**Section 2 (Related Work):** \n- Added a few paragraphs discussing previous works on error bound methods and metric subregularity (R3) \n- Included other related references suggested by R3 and R1\n\n**Section 4 (OMWU):**  \n- Added more comparisons between our techniques and those of [Daskalakis and Panaegas, 2019b] (R1 and R3)\n- Explained that the large leading constant in the bound for OMWU is only for simplicity (R3)\n\n**Section 5 (OGDA):**\n- Removed the boundedness assumption (R3 and R4)\n- Weakened the condition in Definition 1 and renamed it as \u201cSP-MS\u201d\n- Discussed more on the relation between Definition 1 and previous works, and pointed out our novelty (R2 and R3)\n- Modified the statement of Theorem 8 (using the new SP-MS condition)\n- Mentioned that the step size for OGDA can be as large as $1/(2L)$ (R3)\n- Added the statement ``SP-MS holds with $\\beta=3$ in Theorem 9 (R4)\n\n\n**Appendix A (Experiment)**\n- Added one more experiment subsection that investigates OMWU with non-unique equilibria. It still shows linear convergence. (R1)\n\n**Appendix F**\n- Showed the equivalence between our SP-MS with $\\beta=0$ and metric subregularity (R3)\n\n**Appendix G and Appendix H**\n- Updated the proofs for all examples we present to incorporate the new SP-MS condition  (note that all changes are very minor)\n\n**Appendix I (Proof of Theorem 8)**\n- Proved Theorem 8 based on our new SP-MS condition (the idea is the same as before)\n\n**Appendix J (Proof of Theorem 9)**\n- Added a proof to show that the example satisfies our new condition with $\\beta=3$", "title": "General Response for the Revision"}, "qLfT_o_H9Py": {"type": "review", "replyto": "dx11_7vm5_r", "review": "In this paper, the authors consider the convergence of optimistic gradient for constrained saddle-point optimization. Saddle-point problems are very popular in the ML community and lead to non-trivial extensions of the usual (projected) gradient methods. Typically, first-order methods for saddle-point problems are based on the Extra-Gradient algorithm; however, this methods implies two oracle calls per iterations which is undesirable in game theoretical contexts. Fortunately, single call variants have been introduced in the literature to overcome this drawback. Although they all resort to the same algorithm/principle when there are no constraints; single call variants are different in the constrained case. Here, the authors consider the Optimistic Gradient variant (sometimes also called Past-Extra Gradient) and show last-iterate convergence rates under more general constrained case than the previous literature.\n\nThe paper offers a strict improvement over known results of the literature. It can definitively interesting for specialists and may lead to interesting developments. More precisely, they replace the \"locally strongly convex + sufficiently small stepsize\" condition of e.g. [Hsieh2019, Th. 2] to \"introduced SP-RSI +  near-usual stepsize\" and still manage to show linear convergence. While this is interesting, in the present version, the presentation of the paper fails to meet the expectations for a paper that tackles a very specific technical point by not being pedagogical and clear enough. I am rather convinced of the interest of the approach regarding the linear convergence with multiple minimizers but the point 2 and 3 page 8 (global vs local and stepsize) are not completely conving since they may be related to the implicit unit diameter of X.\n\nAll in all, while the paper have merits, the lower quality of presentation and discussion of the results on this very technical matter makes me lean towards rejection.\n\nConcerns:\n* In the introduction (typically the second paragraph), I feel that the \"oracle call difference\" between (EG) and (OGDA) is not clear and may confuse an inexperienced reader. \n* Since the authors base their analysis on a template inequality (Lemma 1); what would happen for other single call variants such as the reflected gradient (see Chambolle and Pock \" A first-order primal-dual algorithm for convex problems withapplications to imaging\") or \"optimistic\" (Daskalakis et al. \"Training GANs with optimism\")? \n* A drawback of the paper is the lack of a conducting thread along the paper. After the first  3 sections, we go from \"Matrix games with OMWU\" to \"general case + RSI w/ OGDA\" then back to matrix games  w/ OGDA. If the authors want to point out the condition RSI and their analysis then this should go first and maybe a section for matrix games w/ two subsections 1) OGDA 2) OMWU would be interesting. In addition, would it be possible to derive a similar RSI w/ KL divergences to get similar results for OMWU?\n* [SP-RSI 1] seems taylored for matrix games, I have trouble to see when else this could be applied (or when beta can be > 0). Theorem 9 should be more \"explained\" I guess.\n* [SP-RSI 2] looks like a generalization of strong convexity. What bothers me is that i) the fact that F is monotonous should be recalled here otherwise this might be confusing; ii) since we are in the smooth case, this only works if \\|z-z*\\|<=1 i.e. assumption 1 (note here that the assumptions are defined but not actually mentioned in the results). Would the constants change is \\|z-z'\\|>1 ?  \n* Is there a direct link between these conditions and Kurdyka/Lojasiewicz-type rule. This is especially striking since Th.8 and more precisely appendix C share similarities with the Theorem 5 of \"On the convergence of the proximal algorithm for nonsmooth functions involving analytic features\" by Attouch and Bolte.\n\nMinor comments/typos:\n* From what I get, the main result concerns: Saddle-point + Constraints + Non-strongly convex/usual stepsize; this should be more explicit in the introduction/Sec. 2, maybe with a table recalling related results (eg. the unconstrained case).\n* (OGDA) and (OMWU) are basically the same algorithm with two different metrics (as stated in Sec. 3), it can thus be troubling to see them opposed in the first two sections without this precision.\n* I am not sure that the 1/sqrt(T) average duality gap rate should be put forward in the intro since it may blur the whole message. \n* Average-iterate cv: another reason not to do averaging is the lack of guarantee for non-convex losses.\n* I find the notation \"dist\" for \"\\|x-Pi(x)\\|^2\" troubling due to the square. Using dist for the same quantity without the square (or renaming dist^2) would be less confusing for me (typically in Lemma 1).\n* In \"Other notations\", in supp(u), I think u is supposed to be in the positive orthant.\n* OGDA seems actually referred to as \"Past extra-gradient\" in [Hsieh2019], \"single call\" is just the class of variants of EG w/ one call per step.\n* In the matrix game at the bottom of p4: is it standard to take entries in [-1,1] or is it just to have L=1? The equilibrium may not be unique without conditions here.\n* The top of page 5 is nice to read and explains well the general reasoning without having to specifically look into the tedious proof in appendix. However, in Theorem 3 the result is not so explicit: C_4 could be made explicit since it is not so complicated as I see from the appendix.\n* I found the proofs rather hard to follow e.g. the proof of Theorem 5 is broken down several interdependent claims but the lack of text make them hard to digest.\n", "title": "Review", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "IWm2Ey6PJo": {"type": "rebuttal", "replyto": "mF_bV64807N", "comment": "**4. Why does the rate of the average of the duality gap degrade in this paper? It is well known for VIs that the average of the duality gap has $1/T$ rate as in [B\u00f6hm, Axel, et al. 2020], then one can use convexity to convert this rate to a rate on the averaged iterate.**\n\nThere is some mis-understanding here. The $1/T$ rate due to [B\u00f6hm, Axel, et al. 2020] is the \"duality gap of the average update\", but not \"average duality gap\".  More precisely, let $\\alpha(x,y)$ be the duality gap of $(x,y)$. Then \"duality gap of the average update\" in [B\u00f6hm, Axel, et al. 2020] is  \n\n$Q_1 = \\alpha(\\overline{x}, \\overline{y})$, with $\\overline{x} = \\frac{1}{T} \\sum_{t=1}^T x_t$ and $\\overline{y} = \\frac{1}{T} \\sum_{t=1}^T y_t$,   \n\nwhile the \"average duality gap\" in our paper is   \n\n$Q_2 = \\frac{1}{T}\\sum_{t=1}^T \\alpha(x_t, y_t)$.  \n\nAn upper bound of $Q_1$ does not imply an upper bound of $Q_2$. In fact, $Q_1 \\leq Q_2$ by the convexity of $\\alpha$. \n\n$Q_2$ is related to last-iterate convergence but $Q_1$ is not. To see this, consider the case when $x_t, y_t$ cycle around the equilibrium point. $Q_1$ can be vanishing because $(\\overline{x}, \\overline{y})$ can approach the equilibrium. But $\\alpha(x_t, y_t)$ remains large for all $t$ so $Q_2$ is not vanishing. \n\nOur bound on $Q_2$ in Theorem 21 is indeed tight. This can be observed by the fact that the same average duality gap bound appears in an intermediate step of [Golowich et al, 2020] towards deriving their tight last-iterate bound (see their proof of Theorem 10). \n\n**5. It is necessary to see the relation of SPRSI with metric subregularity.**\n\nThank you for pointing this out.  We are new to this term, but based on this keyword, we indeed found some related work that we omitted. \n\nWe found that a condition that corresponds to the special case of our SPRSI1 in matrix games is proposed in [Gilpin et al. 2008, Lemma 3]. With this property, they showed that there is a variant of Nesterov\u2019s first-order smoothing method which can find the equilibrium with linear convergence rate. This convergence rate matches ours, and this is also the polytope-constrained setting that we focus on. However, their algorithm does not belong to the single-call extragradient class that we are interested in, and cannot be implemented as a no-regret online learning algorithm.  A follow-up [Mordukhovich et al. 2010] drew further connection between this condition and the metric subregularity. Therefore, metric subregularity is indeed related to our SPRSI1, and we will incorporate related discussion into our paper. Thanks for helping us discover this line of research. \n\nOn the other hand, we also want to emphasize that to our best knowledge, prior to our work neither metric subregularity nor the SPRSI1 condition was known to drive linear last-iterate convergence for single-call extragradient algorithms. \n\n**6. How tight is the rate, and how good the condition SPRSI for detecting structure. Compute the linear rate explicitly for some toy problems and then compare with the performance in practice to see tightness.**\n\nFor theoretical lower bound of the last-iterate convergence, we are only aware that [Golowich et al., 2020] that shows a $1/\\sqrt{T}$ lower bound for general smooth convex-concave functions for the extragradient algorithm in the unconstrained case. But their techniques are not readily transferred to our setting.\n\nComparing our theory and our experiments, we feel that the theoretical bound we can obtain might be too pessimistic for randomly generated games. For example, some leading constants in OMWU appear to be generally large in theory (e.g., $1/\\epsilon$ with $\\epsilon$ defined in Definition 4), but in experiments, the performance does not seem to suffer from such large constants and still converge fast.  Please also refer to our response to AnnoReviewer1 for more details. \n\nOn the other hand, it is also possible that there exist worst cases such that our bound is unimprovable. Therefore, at this point we are unable to conclude whether our bound is tight or not, but this is definitely an interesting question to investigate. \n\n[Gilpin et al. 2008] First-Order Algorithm with $O(\\ln(1/\\epsilon))$ Convergence for $\\epsilon$-Equilibrium in Two-Person Zero-Sum Games  \n[Mordukhovich et al. 2010] Applying metric regularity to compute a condition measure of a smoothing algorithm for matrix games  \n[Golowich et al. 2020] Last Iterate is Slower than Averaged Iterate in Smooth Convex-Concave Saddle Point Problems  ", "title": "Response to AnnoReviewer3 (Part 2/3) "}, "xlT_IffgHKZ": {"type": "rebuttal", "replyto": "5W6_ZHZO2iV", "comment": "We thank the reviewer for the valuable comments. We will add the related works you mentioned in our final version. We first answer your question below.  \n\n**1. It would be very interesting to see numerical estimation of the base of these exponents and see how close they match their theoretical bounds. Also the question about OMWU with a continuum of equilibria could be explored experimentally as well. Do experiments support fast convergence in this case?**\n\nWe thank the reviewer for the valuable suggestions on experiments. Generally speaking, we believe that the uniqueness is only a technical assumption for the proof. We also believe that the constants in our linear convergence rates can be improved. \n\nRegarding the tightness of our theoretical bounds, we use rock paper scissors as an example. In the experiment, OGDA with $\\eta=\\frac{1}{8}$ and random initialization gives a linear convergence rate $\\approx 1.0519^{-t}$ in terms of dist$(z_t,z^*)$. On the other hand, $1/\\sqrt{2}$ is a constant  $C$ for rock paper scissors game to satisfy the SP-RSI-1 condition and this leads to a $96\\cdot 1.00145^{-t}$ convergence guarantee provided by Theorem 8. We admit that there is a gap between theory and experiments, and it is an interesting future direction to improve the theoretical bounds.\n\nAs for the performance of OMWU when there are multiple Nash equilibria. We also did a few experiments in this case. It showed that OMWU with a continuum of equilibria also has linear convergence in these instances. Therefore, it is an interesting open problem to remove the uniqueness assumption for OMWU. \n\nWe will do more experiments and try to incorporate the results in our final versions.", "title": "AnnoReviewer1 (Part 1/1)"}, "ay9QKz7-tDx": {"type": "rebuttal", "replyto": "qLfT_o_H9Py", "comment": "We thank the reviewer for the valuable comments and suggestions. We will answer your individual questions below.\n\n**1. Since the authors base their analysis on a template inequality (Lemma 1); what would happen for other single call variants such as the reflected gradient or \"optimistic\" (Daskalakis et al. \"Training GANs with optimism\")?**\n\nRegarding counterparts of Lemma 1 for other single-call extra-gradient algorithms, please refer to [Hsieh et al. 2019, Section B.2] (see their Eq.(B.4), (B.18), (B.26)). One can observe that they are of very similar form. Therefore, we believe that we can get similar results by applying our conditions to these methods (we still need to verify it though). Note that even though the counterparts of Lemma 1 are known, there was no linear convergence result derived before. \n\n(The setting Daskalakis et al. consider is the unconstrained setting, where every single call variant is equivalent to each other.) \n\n\n**2. A drawback of the paper is the lack of a conducting thread along the paper. If the authors want to point out the condition RSI and their analysis then this should go first and maybe a section for matrix games w/ two subsections 1) OGDA 2) OMWU would be interesting. In addition, would it be possible to derive a similar RSI w/ KL divergences to get similar results for OMWU?**\n\nWe thank the reviewer for the valuable suggestions on presenting the results. In short, one may derive a similar RSI w/ KL divergences for OMWU, but the results are restricted compared to the results for OGDA since it requires more conditions and assumptions (explained below). Therefore, it is  also difficult to present the RSI conditions first and state the results for OGDA and OMWU afterward. The current flow of the paper starts from the less general OMWU results for matrix games, and then goes on to the more general OGDA results under more general conditions. \n \nBelow we briefly explain why a similar SP-RSI condition w/ KL divergences might be more restricted. In our current analysis for OMWU, we need stronger conditions to obtain similar KL divergence decrease. One condition is stated in Lemma 15. It is similar to SP-RSI-1, but it is actually stronger and requires the stronger assumption of unique equilibrium. Moreover, we need to further relate the $\\|z^*-z\\|_1$ term in Lemma 15 to KL$(z^*,z)$. This requires more efforts developed in Lemma 16-19 and Lemma 2 that are quite specific to matrix games. Our approach also forces us to use the \"two-stage\" argument to argue linear convergence. \n\nIn summary, we did not state these conditions for general functions since the generalization is much less clean compared to the OGDA counterparts. But making the OMWU result more general is indeed one of the key future directions. \n\n**3. [SP-RSI 1] seems taylored for matrix games, I have trouble to see when else this could be applied (or when beta can be $>$ 0). Theorem 9 should be more \"explained\" I guess.**\n\nSorry for the confusion. Theorem 9 actually gives an example with $\\beta>0$. In that example, we still consider a bilinear game, but the feasible set boundary is quadratically-shaped (so it is not a polytope). In fact, we can prove $\\beta=3$ in that example. On the other hand, this example can not be captured by SP-RSI-2 with any $\\beta$, showing that SP-RSI-1 is indeed useful. (More generally, piecewise linear functions with curved boundaries like this one should also be able to be captured by SP-RSI-1 with $\\beta>0$). \n\nAn example of SP-RSI-1 with $\\beta=0$ other than matrix games is when $f(x,y)$ is a piecewise linear function for both $x$ and $y$, and when the feasible sets are polytopes. To show this, we can largely reuse the proof of Theorem 5 because the set of equilibrium is also a polytope in this case. \n\nWe admit that Theorem 9 was not fully explained because of the space limit. We will provide more discussion in the revised version.\n\n**4. [SP-RSI 2] looks like a generalization of strong convexity. What bothers me is that i) the fact that F is monotonous should be recalled here otherwise this might be confusing; ii) since we are in the smooth case, this only works given assumption 1. Would the constants change is $|z-z'|>1$ ?**\n\nWe thank the reviewer for the comment in i) and will mention that in our updated version. For ii), we explained how to handle the case when $|z-z^*|>1$ in Footnote 1 on page 6. Basically, if the diameter (i.e., $\\sup_{z_1, z_2\\in\\mathcal{Z}}\\|z_1-z_2\\|$) is $D$, then all dependence on $L$ in our bounds becomes $LD$. \n\nWe hope this also addresses another earlier comment \"...point 2 and 3 on page 8 (global vs local and stepsize) are not completely convincing since they may be related to the implicit unit diameter of X''.", "title": "Response to AnnoReviewer4 (Part 1/2)"}, "PxrPqUW_S3j": {"type": "rebuttal", "replyto": "qLfT_o_H9Py", "comment": "**5. Is there a direct link between these conditions and the Kurdyka/Lojasiewicz-type rule. This is especially striking since Th.8 and more precisely appendix C share similarities with Theorem 5 of \"On the convergence of the proximal algorithm for nonsmooth functions involving analytic features\" by Attouch and Bolte.**\n\nIn short, both the Kurdyka-Lojasiewicz (KL) rule and SP-RSI (with $\\beta=0$) are conditions that guarantee linear convergence. However, the KL condition is for minimization problems with proximal gradient methods, while SP-RSI is for saddle-point (min-max) problems with the OGDA method. \n\nYou might also find the paper [Karimi et al. 2016] relevant. It provides a thorough overview on the relations between several conditions that drives linear convergence in optimization, including the KL condition you mentioned, and the Restricted Secant Inequality (RSI) condition --- where the name of our SP-RSI comes from. Although our SP-RSI condition is for saddle-point (min-max) problems, it shares some similarity with the RSI condition for minimization problems. \n\n(It is not surprising that you find Appendix C familiar because many proofs for linear convergence go through those lines of analysis)\n\n[Karimi et al. 2016] Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Lojasiewicz Condition\n\n\n**6. Is it standard to take entries in [-1,1] or is it just to have $L=1$? The equilibrium may not be unique without conditions here.**\n\nFor matrix games, it is quite standard to assume that all entries are in [-1, 1] (e.g., [Syrgkanis et al. 2015], [Rakhlin and Sridharan, 2013]), since we can always scale the game matrix to ensure this. \nSuch scaling clearly does not affect the uniqueness of the equilibrium.\n\n\n[Syrgkanis et al. 2015] Fast Convergence of Regularized Learning in Games \n\n[Rakhlin and Sridharan, 2013] Optimization, Learning, and Games with Predictable Sequences\n\n\n**7. The top of page 5 is nice to read and explains well the general reasoning without having to specifically look into the tedious proof in appendix. However, in Theorem 3 the result is not so explicit: $C_4$ could be made explicit since it is not so complicated as I see from the appendix.**\n\nWe use this constant for the conciseness of presentation. $C_4$ is $15\\eta^2\\epsilon^3 C^2\\xi^2/32$, where $\\epsilon, C, \\xi$ are all problem-dependent constants defined in Definition 2, Lemma 15, and Definition 4, respectively. Due to the page limit, we did not introduce these problem-dependent constants in the main text and had to use a single constant $C_4$ for conciseness. \n\n**Thank you for all other suggestions related to writing clarity and paper organization. They are very helpful for us to improve readability. We will try our best to include them in the final version.**\n\n", "title": "Response to AnnoReviewer4 (Part 2/2)"}, "uLNmCJ9LN8Z": {"type": "rebuttal", "replyto": "VipahfFX4V8", "comment": "We thank the reviewer for the valuable comments and suggestions. We will answer your individual questions below.\n\n**1. The role of constraint set in this paper is not clear to me. Is the constraint sets X and Y essential to the problem? What's the difficulty in constrained minimax over unconstrained minimax?**\n\nIn the literature, many unconstrained versions of our problems (bilinear games in particular) are already solved. Please see, for example, [Mokhtari et al., 2019] and [Liang and Stokes, 2019].  In their analysis, they explicitly write down the recursive relationship between the iterates of consecutive rounds and solve the recursive relationship. However, in the constrained setting, with the existence of projection, it is not direct to do the same, which forces us to use different analysis. In this sense, the constraint setting is even more challenging.\n\n**2. Considering it has not been discussed in other papers, it would be better to provide a more general nonlinear objective class (besides bilinear games on polytopes) that satisfies this condition. For future improvement, I suggest to discuss SP-RSI more deeply, since this condition is added to overcome the intermediate difficulty in technical proof.**\n\nWe thank the reviewer for the valuable suggestion. We provided some examples of nonlinear objectives in Theorems 6,7,and 9. We will add more discussions on the SP-RSI conditions in our final version. Please also refer to our response to AnnoReviewer 4's Question 3 for more details.    \n\n[Mokhtari et al., 2019]: A Unified Analysis of Extra-gradient and Optimistic Gradient Methods for Saddle Point Problems: Proximal Point Approach\n\n[Liang and Stokes, 2018]: Interaction Matters: A Note on Non-asymptotic Local Convergence of Generative Adversarial Networks", "title": "Response to AnnoReviewer2 (Part 1/1)"}, "mvWMfiZN41o": {"type": "rebuttal", "replyto": "mF_bV64807N", "comment": "We thank the reviewer for the valuable comments. We will first answer your individual questions. \n\n**1. What are the different tools that the authors use to improve [Daskalakis and Panaegas, 2019]?**\n\nThe similarity of our analysis and [Daskalakis and Panaegas, 2019] only lies in that both utilize a \u201ctwo-stage\u201d argument to show last-iterate convergence (i.e., in the first stage, a slower convergence rate is shown, and in the second stage, a faster convergence rate is shown). However, the proof details and the results are largely different.\n\nSpecifically, [Daskalakis and Panaegas, 2019] utilize tools of \u201dspectral analysis\u201d that are along the same line as [Liang and Stokes, 2018]. To show last-iterate convergence, they show that the OMWU update can be viewed as a \u201ccontraction mapping\u201d with respect to a matrix whose eigenvalue is smaller than 1. \n\nOur analysis, on the other hand, leverages analysis of online mirror descent. We start from the \u201cone-step regret bound\u201d (Lemma 1) used in online mirror descent analysis, but make use of the two negative terms that are typically dropped in online learning analysis to show that KL$(z^*,z_{t})$ will sufficiently decrease in each round. \nImportantly, our analysis does not need an exponentially small learning rate required in [Daskalakis and Panaegas, 2019]. \n\n**2. A constant exponential in $T_0$ might be too large. For example, how does this rate compare to the standard sublinear rate of OMWU?**\n\nIn the paper, we push $T_0$ to the exponent just for simplicity to show our linear convergence. An alternative is to say that there are two stages of convergence, where in the first stage the convergence rate is $O(1/t)$ and this stage lasts for the first $T_0$ steps; and in the second stage the convergence rate is $O(e^{-\\lambda t})$ (linear convergence). With this interpretation, the bound does not pay for a constant exponential in $T_0$. To keep the conciseness of the presentation, we use the original expression in the theorem, but we will add more explanations on the above two-stage view.  \n\nWe also want to stress that in terms of the last-iterate convergence, we are not aware of any \u201cstandard sublinear rate\u201d for OMWU.  The results in [Daskalakis and Panaegas, 2019] did not provide an explicit rate. In fact, they listed \u201cfinding exact rates of convergence\u201d as an open problem. To the best of our knowledge, we give the first last-iterate convergence rate for OMWU. \nAs for the average-iterate, sub-linear rates of convergence with smaller leading constants are indeed well known, but the results are not comparable to ours as we focus on last-iterate convergence.\n\n**3. The main reference the authors are missing is FORB. How does the constrained OGDA the authors propose in this paper differ from FORB which is already given in a constrained setting? I suspect using MS in FORB analysis can directly yield similar linear convergence rates to this paper. Then, what is the advantage of SPRSI and the new constrained OGDA compared to FORB?**\n\nSorry for missing FORB in our reference list (we will add it later). However, we are aware of this algorithm because it is reconsidered in [Hsieh et al. 2019] with a different name. \n\nWe want to first clarify that we did not \u201cpropose\u201d constrained OGDA, because this is an existing algorithm dating back to [Popov, 1980] (please see the literature review in Part 3). However, as we will mention in Part 3, while the single-call variants of the Extra-Gradient algorithms including constrained OGDA and constrained FORB are well known algorithms, their behavior regarding last-iterate convergence remains unclear, especially in bilinear games. \n\nDue to the similar updates of constrained OGDA and constrained FORB, we actually believe that the SPRSI condition will also give similar results shown in our paper for FORB. This requires more theoretical verification though. We are also not aware of any previous results that study MS or SPRSI in FORB or other single-call extragradient algorithms. \n\n(Side note: there are advantages of OGDA over FORB when applying them to no-regret learning, as we will point out in Part 3)\n\n[Popov 1980] A modification of the Arrow\u2013Hurwicz method for search of saddle points   \n[Dakalakis and Panaegas 2019] Last-Iterate Convergence: Zero-Sum Games and Constrained Min-Max Optimization  \n[Hsieh et al. 2019] On the Convergence of Single-Call Stochastic Extra-Gradient Methods  \n[Liang and Stokes, 2018] Interaction Matters: A Note on Non-asymptotic Local Convergence of Generative Adversarial Networks  ", "title": "Response to AnnoReviewer3 (Part 1/3)"}, "3c2_ex7Dsw": {"type": "rebuttal", "replyto": "mF_bV64807N", "comment": "We hope our responses in Part 1 and Part 2 address most of your concerns.\n\nTo even further answer how the contributions of the paper adds onto what is already known in these related works'', below we provide a more systematic literature summary and clearly point out our contributions again.\n\n**OGDA**\n\nThe OGDA algorithm we study dates back to [Popov 1980]. It belongs to a class of algorithms called \u201csingle-call extragradient algorithms\u201d, as summarized in [Hsieh et al. 2019]. Hsieh et. al. categorize this class of algorithms into three categories. Specifically, the FORB algorithm you mentioned belongs to \u201cRG\u201d, while the OGDA we study belongs to \u201cPEG\u201d. Although they are different, they are closely related (and become the same in the unconstrained setting), and actually have similar theoretical guarantees. This class of algorithms is important because they can be readily implemented as \u201cno-regret\u201d algorithms, and each player can run the algorithm independently without cooperating with the other player (though there is some caveat in applying RG as a no-regret algorithm, because the point $2x_t-x_{t-1}$ may fall outside the feasible set). \n\nBelow, we summarize the known results about single-call extragradient algorithms in two-player zero-sum games, based our best knowledge: \n\n- $O(1/T)$ convergence rate for \u201caverage-iterate\u201c is established by [Rakhlin et al. 2013]\n- Linear convergence rate for \u201clast iterates\u201d: \n    * For the unconstrained setting, linear convergence has been established for bilinear games (e.g. [Liang and Stokes, 2018]) and strongly-convex-strongly-concave functions (e.g. [Mokhtari et al., 2019]).\n    * For the constrained setting, linear convergence has been established for strongly-convex-strongly-concave functions [Malitsky and Tim, 2018],     \n\n\nFor the constrained setting, linear last-iterate convergence has not been shown for bilinear games on any single-call extragradient algorithm (including FORB). Our work makes significant progress in this specific setting, which is considered extensively in game theory and online learning problems. \n\nWe not only show that OGDA exhibits linear last-iterate convergence in bilinear games with polytope feasible sets, but also that it provably does not achieve linear last-iterate convergence if the feasible set is not polytope (surprisingly). This also indicates that the constrained setting is not a direct result from the unconstrained settings, and there are new aspects to look into. Our analysis easily recovers the results for strongly-convex strongly-concave settings. To generalize our results, we identify the more general SPRSI conditions which include both bilinear and strongly-convex-strongly-concave functions as special cases. \n\n**OMWU**\n\nOMWU algorithms are more natural for constrained settings, in particular when the constraints are the probability simplex. We are only aware of the following results for OMWU (all on probability simplex): \n\n- Rakhlin and Sridharan (2013), Syrgkanis et al. (2015) show that OMWU has $O(1/T)$ average-iterate convergence in bilinear games   \n- Daskalakis and Panaegas (2019) shows that OMWU has last-iterate convergence in bilinear games.  However,  their analysis 1) requires exponentially small (in some problem-dependent quantity) learning rate 2) does not give an explicit convergence rate, and 3) requires a unique equilibrium. \n- Lei et al. (2020) studies OMWU with general convex-concave functions, but with more assumptions\n\nOur analysis investigates the same setting as [Dakalakis and Panaegas 2019], but resolves their first two issues: our analysis holds for large learning rates, and we show linear convergence with an explicit convergence rate, when the equilibrium is unique. \n\n[Rakhlin et al. 2013] Optimization, Learning, and Games with Predictable Sequences  \n[Popov 1980] A modification of the Arrow\u2013Hurwicz method for search of saddle points  \n[Dakalakis and Panaegas 2019] Last-Iterate Convergence: Zero-Sum Games and Constrained Min-Max Optimization  \n[Hsieh et al. 2019] On the Convergence of Single-Call Stochastic Extra-Gradient Methods  \n[Mokhtari et al., 2019] A Unified Analysis of Extra-gradient and Optimistic Gradient Methods for Saddle Point Problems: Proximal Point Approach  \n[Lei et al., 2020] Last iterate convergence in no-regret learning: constrained min-max optimization for convex-concave landscapes  \n[Liang and Stokes, 2018] Interaction Matters: A Note on Non-asymptotic Local Convergence of Generative Adversarial Networks  \n[Malitsky and Tim, 2018] A Forward-Backward Splitting Method for Monotone Inclusions Without Cocoercivity  \n[Syrgkanis et al. 2015] Fast Convergence of Regularized Learning in Games  ", "title": "Response to AnnoReviewer3 (Part 3/3)"}, "5W6_ZHZO2iV": {"type": "review", "replyto": "dx11_7vm5_r", "review": "This paper studies the performance of optimistic multiplicative weights update (OMWU) and optimistic gradient descent (OGDA) in constrained zero-sum settings and provide linear convergence rate guarantees. For OMWU in bilinear games over the simplex, they show that when the equilibrium is unique, linear last-iterate convergence is achievable with a constant learning rate. In the case of projected OGDA algorithm, they introduce a sufficient condition under which it convergence fast with a constant learning learning rate. They show that bilinear games over any polytope satisfy this condition and OGDA converges exponentially fast even without the unique equilibrium assumption.\n\nThis is overall a nice paper that extends and improves our understanding about optimistic versions of OMWU and OGDA especially in constrained bilinear zero-sum games. The paper does a good job at explaining technical improvements over prior results in the area and particularly the works by Daskalakis and Panageas and Hsieh et al. \n\nThe experimental section could be slightly improved. For example in the case of OMWU it is seems hard to detect whether the error curve is best fit by an exponential even after the initial slower phase. It would be very interesting to see numerical estimation of the base of these exponents and see how close they match their theoretical bounds. Also the question about OMWU with a continuum of equilibria could be explored experimentally as well. Do experiments support fast convergence in this case? \n\nOverall, this is a nice paper and I recommend acceptance.\n\nRelated references: \nIn terms of fast convergence in bilinear zero-sum games with fixed learning rates\n[1] Proves that even with large fixed learning rates the average duality gap of alternating GDA in unconstrained bilinear zero-sum games converges to zero at a rate of O(1/t). [2] proves O(1/sqr{t}) convergence under arbitrarily large learning rates for a variant of GDA (Follow the regularized leader with Euclidean regularizer) in small constrained bilinear zero-sum games, despite divergence of the day-to-day behavior to the boundary.\n\n[3, 4] OMWU is shown to stabilize fast in bilinear constrained zero-sum games in a different sense by arguing exponentially fast shrinking of the volume of sets of initial conditions in the dual/payoff space. \n \n\n[1] Bailey et al. Finite Regret and Cycles with Fixed StepSize via Alternating Gradient Descent-Ascent. COLT 2020.\n[2]  Bailey, Piliouras. Fast and Furious learning in zero-sum games: vanishing regret with non-vanishing step sizes. Advances in Neural Information Processing Systems. 2019.\n[3] Cheung, Piliouras. Chaos, Extremism and Optimism: Volume Analysis of Learning in Games. arXiv preprint arXiv:2005.13996 (2020).\n", "title": "Good paper, improves our understanding of OMWU, OGDA in zero-sum games", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}