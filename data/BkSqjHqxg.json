{"paper": {"title": "Skip-graph: Learning graph embeddings with an encoder-decoder model", "authors": ["John Boaz Lee", "Xiangnan Kong"], "authorids": ["jtlee@wpi.edu", "xkong@wpi.edu"], "summary": "An unsupervised method for generating graph feature representations based on the encoder-decoder model.", "abstract": "In this work, we study the problem of feature representation learning for graph-structured data. Many of the existing work in the area are task-specific and based on supervised techniques. We study a method for obtaining a generic feature representation for a graph using an unsupervised approach. The neural encoder-decoder model is a method that has been used in the natural language processing domain to learn feature representations of sentences. In our proposed approach, we train the encoder-decoder model to predict the random walk sequence of neighboring regions in a graph given a random walk along a particular region. The goal is to map subgraphs \u2014 as represented by their random walks \u2014 that are structurally and functionally similar to nearby locations in feature space. We evaluate the learned graph vectors using several real-world datasets on the graph classification task. The proposed model is able to achieve good results against state-of- the-art techniques.", "keywords": ["Unsupervised Learning", "Deep learning"]}, "meta": {"decision": "Reject", "comment": "The idea of applying skip-graphs to this graph domain to learn embeddings is good. The results demonstrate that this approach is competitive, but do not show a clear advantage. This is difficult, as the variety of approaches in this area is rapidly increasing. But comparisons to other methods could be improved, notably deep graph kernels."}, "review": {"HyzM47X8l": {"type": "rebuttal", "replyto": "BkSqjHqxg", "comment": "Dear reviewers, thank you very much for all the insightful comments and suggestions. Please find below a summary of our response to each of the points that were raised. Please note that we have paraphrased some of the comments and combined similar ones.\n\nAnonymous Reviewer 2:\n\nComment 1. Use the same classifier for all the compared methods.\n\nWe have updated the results in our paper to reflect the changes from using a common classifier for all the compared methods. The changes can be found in section 3.2.\n\nComment 2. Use mean- or max-pooling to aggregate the embeddings learned by methods such as DeepWalk or node2vec and include this as a baseline.\n\nThe paper has been updated to include results from Deepwalk which was added as an additional baseline. Changes were made to sections 3.2 and 3.3.\n\nAnonymous Reviewer 3:\n\nComment 3. Provide more information about the exact values of the parameters that were used in the grid search.\n\nThe information have been added to the paper. This can be found in section 3.2.\n\nAnonymous Reviewer 1:\n\nComment 4. Need more experiment to demonstrate the power of their feature extraction methods. (Clustering, Search, Prediction etc.). Only comparing the classification accuracy by using the proposed method is not enough.\n\nWe plan to extend the experiments by using the embedding for graph clustering and graph search tasks in future works/journal extension. Due to the suggested page limit of ICLR, it is not easy to add these additional experiments without deleting some existing results in the paper. In the existing results, we have demonstrated the power of the embedding through prediction tasks and visualization of the embedding.\n\nComment 5. Presentation of the paper is weak. There are lots of typos and unclear statements. \n\nThe latest version of the paper has been proofread by several individuals and to the best of our knowledge we have removed the major typos and/or unclear statements. \n\nComment 6. What is the difference from graph kernel methods? The comparison with graph kernel is missing. \n\nWe agree that the problem studied in the deep graph kernel paper seems to be very similar, or even the same, to the one we are studying. However, the underlying approach is slightly different and we argue that that in itself is a good motivation for the work as there has not been too many work published in the area. We use an encoder-decoder model which has not been used, and for one we can identify functionally similar subgraphs. \n\nRegrettably, due to time constraints we are unable to add a comparison with Deep Graph Kernels. However, we have tested against methods that have been shown to achieve good results on the type of graphs we used (ECFP and NeuralFPS) and we have also tested against DeepWalk. The introduction section has been updated to include some discussion on this.\n\nAnonymous Reviewer 4:\n\nComment 7. Comparisons with recent work like LINE and node2vec are missing. You can compare them easily by applying the same aggregation strategy to their node embeddings.\n\nWe have already addressed the comment of a previous reviewer and have included DeepWalk as a baseline. We did not add comparisons against node2vec and LINE as these, in general, belong to the same family of methods.\n\nComment 8. As the current datasets are small (e.g., the average number of nodes per graph is around 30), it would be great to explore larger graph datasets to further investigate the method. \n\nTo compensate in a way, we tested the method on four different molecular graph datasets. Regrettably, we are unable to add experiments on more datasets at the moment.\n\nComment 9. The description about how to split the random walk sequence into 3 sub-sequences is missing. Also, the line \u201cl_min >= (n_k - 1), \u2026 >= l_max\u201d in section 2.2.2 is a mistake.\n\nSection 2.2.2 has been updated to correct this. Thank you very much for pointing this out.\n\n", "title": "Summary of changes"}, "HkpJUpfHx": {"type": "rebuttal", "replyto": "HJctDnzSe", "comment": "Thanks a lot for the invaluable comment. As suggested by reviewer2, we have added a comparison with DeepWalk (the embedding method) in Table 2.\n\n= Response to Comments 1, 2 and 5: (difference between this paper and graph kernel) =\nI would like to clarify the major differences between our work and graph kernel methods. Although recent methods on graph kernels achieved great performances on graph data, but they are actually focused on graph kernel problems, instead of the graph embedding problem. Graph kernel methods are focused on learning a kernel/similarity function between a Pair of graph objects.  Our method is learning a feature embedding for each graph object. For example, in figure 4, we showed a graph embedding result, which cannot be obtained through graph kernel methods. This paper is the first deep learning approach to graph embedding problem.  Although our graph embedding can potentially be fed into a kernel/similarity function in order to construct a graph kernel and to compare with graph kernel methods, that would be slightly beyond the scope of graph embedding problem of this paper. But we would be happy to add this comparison if this is crucial. \n\n= Response to Comment 3: (more experiments on clustering, search, prediction etc. tasks) =\nWe plan to extend the experiments by using the embedding for graph clustering and graph search tasks in future works/journal extension.  Due to the page limit of ICLR, it is not easy to add these additional experiments without deleting some existing results in the paper. In the existing results, we have demonstrated the power of the embedding through Prediction tasks and visualization of the embedding.\n\n= Response to Comment 4: (typo and unclear statements) =\nWe regret if there is any typo or unclear statement in the paper, and we will proof-read again and correct them quickly in the next revision.", "title": "Re: Comparison with Graph kernels Missing"}, "SyJR3M8Nl": {"type": "rebuttal", "replyto": "ByHWhGUVg", "comment": "Thank you for your response, we have updated the paper to include information about the parameters we tested. This can be found in sec. 3.2.", "title": "Response"}, "SJFpSv37e": {"type": "rebuttal", "replyto": "Sy52xMJXe", "comment": "Thank you for your comments and suggestions. We have updated our paper. In particular we added the following things:\n\n(a) short discussion on how the method can be used with unlabeled graphs (sec. 2.2.1)\n(b) short discussion and figure of visualization of the embeddings for one of the datasets (sec. 3.5, figure 4)\n(c) experiments were rerun to use a common classifier (neural network) for all tested methods (sec. 3.2)\n\nFurthermore, in response to your suggestion in the review, we have included DeepWalk as an additional baseline method which we tested against. The additional information is included in sec. 3.2 and sec. 3.3.", "title": "Summary of changes in response to comment"}, "rJ3HnzIVl": {"type": "rebuttal", "replyto": "HJLfcyL4x", "comment": "Thank you for your comments. Discussion about the classifier we used can be found in section 3.2 of our paper. We used a neural network classifier. The same classifier, optimized over the same set of parameters, was used to evaluated all the compared methods. \n\nWe do not claim that the method achieves absolute best performance on the tested datasets. However, we compared our approach against methods that have been shown to achieve state-of-the-art results on the type of graphs (molecular) that we used for evaluation. The results show that the method is competitive against these methods. ", "title": "Response"}, "BJr0cMIEe": {"type": "rebuttal", "replyto": "S1LDYyIVx", "comment": "Hello, we used a neural network in our experiments. The same classifier (using the same set of parameters) was used with all the evaluated methods. This information can be found in the latter part of section 3.2 of the paper.", "title": "Response"}, "S1LDYyIVx": {"type": "review", "replyto": "BkSqjHqxg", "review": "You say that any off the shelf classifier can be used to make the prediction based on embedding, what are the actual classifiers you used in your experiments?Authors take the skip-graph architecture (Kiros 2015) and apply it to classifying labeled graphs (molecular graphs). They do it by creating many sentences by walking the graph randomly, and asking the model to predict previous part and next part from the middle part. Activations of the decoder part of this model on a walk generated from a new graph are used as features for a binary classifier use to predict whether the molecule has anti-cancer properties.\n\nPaper is well written, except that evaluation section is missing details of how the embedding is used for actual classification (ie, what classifier is used)\n\nUnfortunately I'm not familiar with the dataset and how hard it is to achieve the results they demonstrate, that would be the important factor to weight on the papers acceptance.", "title": "How do you go from embedding to final classification?", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "HJLfcyL4x": {"type": "review", "replyto": "BkSqjHqxg", "review": "You say that any off the shelf classifier can be used to make the prediction based on embedding, what are the actual classifiers you used in your experiments?Authors take the skip-graph architecture (Kiros 2015) and apply it to classifying labeled graphs (molecular graphs). They do it by creating many sentences by walking the graph randomly, and asking the model to predict previous part and next part from the middle part. Activations of the decoder part of this model on a walk generated from a new graph are used as features for a binary classifier use to predict whether the molecule has anti-cancer properties.\n\nPaper is well written, except that evaluation section is missing details of how the embedding is used for actual classification (ie, what classifier is used)\n\nUnfortunately I'm not familiar with the dataset and how hard it is to achieve the results they demonstrate, that would be the important factor to weight on the papers acceptance.", "title": "How do you go from embedding to final classification?", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "Sy52xMJXe": {"type": "review", "replyto": "BkSqjHqxg", "review": "What is the classifier that you used for ECFP? In the text you only say that you used a library from a prior work. I think that for a correct comparison you should  use the same classifier for ECFP and skip-graph embeddings, because your dataset is very small and skewed, and the choice of regularization might be very important.The paper presents a method to learn graph embeddings in a unsupervised way using random walks. It is well written and the execution appears quite accurate. The area of learning whole graph representations does not seem to be very well explored in general, and the proposed approach enjoys having very few competitors.\n\nIn a nutshell, the idea is to linearize the graph using random walks and to compute the embedding of the central segment of each walk using the skip-thought criterion. Being not an expert in biology, I can not comment whether or not this makes sense, but the gains reported in Table 2 are quite significant. \n\nAn anonymous public comment compared this work to a number of others in which the problem of learning representations of nodes is considered. While this is arguably a different goal, one natural baseline would be to pool these representations using mean- or max- pooling. It would very interesting to do such a comparison, especially given that the considered approach heavily relies on pooling (see Figure 3(c))\n\nTo sum up, I think it is a nice paper, and with more baselines I would be ready to further increase the numerical score.  \n", "title": "Classifier for ECFP", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H1Q8Ckz4l": {"type": "review", "replyto": "BkSqjHqxg", "review": "What is the classifier that you used for ECFP? In the text you only say that you used a library from a prior work. I think that for a correct comparison you should  use the same classifier for ECFP and skip-graph embeddings, because your dataset is very small and skewed, and the choice of regularization might be very important.The paper presents a method to learn graph embeddings in a unsupervised way using random walks. It is well written and the execution appears quite accurate. The area of learning whole graph representations does not seem to be very well explored in general, and the proposed approach enjoys having very few competitors.\n\nIn a nutshell, the idea is to linearize the graph using random walks and to compute the embedding of the central segment of each walk using the skip-thought criterion. Being not an expert in biology, I can not comment whether or not this makes sense, but the gains reported in Table 2 are quite significant. \n\nAn anonymous public comment compared this work to a number of others in which the problem of learning representations of nodes is considered. While this is arguably a different goal, one natural baseline would be to pool these representations using mean- or max- pooling. It would very interesting to do such a comparison, especially given that the considered approach heavily relies on pooling (see Figure 3(c))\n\nTo sum up, I think it is a nice paper, and with more baselines I would be ready to further increase the numerical score.  \n", "title": "Classifier for ECFP", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SkA1KUjgl": {"type": "rebuttal", "replyto": "HyFIVpclx", "comment": "Thanks a lot for the comment.\nI would like to clarify the major differences between our work and previous work. Although the recent works (LINE, DeepWalk, node2vec) are indeed unsupervised, but they are actually focused on the node embedding problem, instead of the graph embedding problem. LINE, DeepWalk and node2vec are learning an embedding for each node in a graph or an information network.  Our method is learning an embedding for each graph object in a collection of graphs. In the example of molecular graph, our method is trying to learning representation for a molecule, while LINE, DeepWalk, node2vec are learning representation for an atom within the molecule.", "title": "Baseline comparisons"}, "HyFIVpclx": {"type": "rebuttal", "replyto": "BkSqjHqxg", "comment": "Hi,\n\nIn the abstract you have mentioned, \"Many of the existing work in the area are task-specific and based on supervised techniques\". I don't think this is a valid statement, since lot of recent work in graph embeddings is based on unsupervised methods. Like LINE and DeepWalk. \n\nAnd is there any particular reason you didn't compare your method with DeepWalk and LINE, considering the fact that they are state-of-the art methods in representation learning for graphs?\n\nRegards,", "title": "Baseline comparisons"}}}