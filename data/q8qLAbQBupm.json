{"paper": {"title": "Neural Mechanics: Symmetry and Broken Conservation Laws in Deep Learning Dynamics", "authors": ["Daniel Kunin", "Javier Sagastuy-Brena", "Surya Ganguli", "Daniel LK Yamins", "Hidenori Tanaka"], "authorids": ["~Daniel_Kunin1", "~Javier_Sagastuy-Brena1", "~Surya_Ganguli1", "~Daniel_LK_Yamins1", "~Hidenori_Tanaka1"], "summary": "By exploiting architectural symmetry, our work demonstrates that we can analytically describe the learning dynamics of various parameter combinations at finite learning rates and batch sizes for state of the art architectures trained on any dataset.", "abstract": "Understanding the dynamics of neural network parameters during training is one of the key challenges in building a theoretical foundation for deep learning. A central obstacle is that the motion of a network in high-dimensional parameter space undergoes discrete finite steps along complex stochastic gradients derived from real-world datasets. We circumvent this obstacle through a unifying theoretical framework based on intrinsic symmetries embedded in a network's architecture that are present for any dataset. We show that any such symmetry imposes stringent geometric constraints on gradients and Hessians, leading to an associated conservation law in the continuous-time limit of stochastic gradient descent (SGD), akin to Noether's theorem in physics. We further show that finite learning rates used in practice can actually break these symmetry induced conservation laws. We apply tools from finite difference methods to derive modified gradient flow, a differential equation that better approximates the numerical trajectory taken by SGD at finite learning rates. We combine modified gradient flow with our framework of symmetries to derive exact integral expressions for the dynamics of certain parameter combinations. We empirically validate our analytic expressions for learning dynamics on VGG-16 trained on Tiny ImageNet. Overall, by exploiting symmetry, our work demonstrates that we can analytically describe the learning dynamics of various parameter combinations at finite learning rates and batch sizes for state of the art architectures trained on any dataset.", "keywords": ["learning dynamics", "symmetry", "loss landscape", "stochastic differential equation", "modified equation analysis", "conservation law", "hessian", "geometry", "physics", "gradient flow"]}, "meta": {"decision": "Accept (Poster)", "comment": "\nThe paper offers a more systematic treatment of various symmetry-related results in the current literature. Concretely, the invariance properties exhibited by loss functions associated with neural networks give rise to various dynamical invariants of gradient flows. The authors address these dynamical invariants in a unified manner and study them wrt different variants of gradient flows aimed at reflecting different algorithmic aspects of real training processes. \n\nThe simplicity and the generality of dynamical invariants are both the strength and the weakness of the approach. On one hand, they provide a simple way of obtaining non-trivial generalities for the dynamics of learning processes. On the other hand, they abstracts away the very structure of neural networks from which they derive, and hence only allow relatively generic statements. Perhaps the approach should be positioned more as a conceptual method for studying invariant loss functions. \n\nOverall, although the technical contributions in the paper are rather incremental, the conceptual contribution of using dynamical invariants to unify and somewhat simplify existing analyses in a clear and clean symmetry-based approach is appreciated by the reviews and warrant a recommendation for borderline acceptance. \n"}, "review": {"a27_KGlIlo": {"type": "review", "replyto": "q8qLAbQBupm", "review": "Pros: \n- This paper is very well-written and motivated. \n- The train of thoughts is explained very clearly, such that I (admittedly not being  an expert in this field) was able to follow. \n- The idea to unify invariances of the loss function by using symmetries and derive corresponding conservation laws (for $\\lambda =0$) in the gradient flow is very elegant. \n- By adapting a modification of the gradient flow from previous works that accounts for the discrete approximation of SGD, the derived theory was able to predict the behavior of the relevant quantities during training to a remarkable accuracy.\n\nCons: I did not find any major drawbacks of this work. Just two small questions:\n- Using $\\ell^2$ regularization on a problem with scale symmetry does not seem to make sense, because the cost function\n$$ \\mathcal{L}(\\theta) + \\lambda \\|\\theta\\|^2 $$ \nwill likely not have a minimizer as soon as $\\lambda >0$. Reducing the magnitude of any $\\theta$ that is optimal for $\\mathcal{L}$ reduces the regularization, but in the limit of $\\theta=0$ the loss might jump up. Thus, the costs are not lower semi-continuous.\n- Additionally, the scale symmetry seems to naturally lead to a discontinuous loss function $\\mathcal{L}$. Is there no problem in even defining the gradient flow for such a function? Which properties of $\\mathcal{L}$ do you need to derive the continuous gradient flow equations?\n\n\nOverall, I really enjoyed reading this paper. Since I am not an expert in the field, I cannot really judge the novelty/contribution, but aside from this aspect, I clearly recommend the acceptance of this work. \n\n-----------\n- There is a typo in Section 6.1 \"graident\"\n- I stumbled upon the NeurIPS 2020 paper \"Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate\" by Li, Lyu, and Arora. Based on the abstract, this seems to be a relevant related work. \n\n\n----------------\nAfter the rebuttal: I'd like to thank the authors as well as my fellow reviewers for the interesting discussions and corresponding clarifications. Summarizing my impressions from the discussion, the two main points of criticism are that the proposed analysis is not fully predictive (depends on the norm of the gradients that depend on the empirical data), but rather provides the laws that  govern the dynamics, and that the analysis is based on a time-continuous differential equation that seems to approximate SGD well instead of being applicable to the SGD iterates directly. The validity of the continuous dynamics is demonstrated in numerical results only.  I do agree that a fully predictive framework on SGD directly would be very intersting. Yet, I think the authors are taking important steps towards such a framework, and considering the fact that SGD often behaves surprisingly/unexpectedly (as also stated by R3), I am still quite impressed how accurately the theory matches the actual SGD behavior. For our understanding of how symmetries/invariances in the weights of network architectures influence the training, I believe this paper does provide interesting insights such that I recommend its acceptance. As for a final score, I could go down to a 7 to account for the concerns raised by my fellow reviews, but I think it would mainly reflect my uncertainty about my intuition that a fully predictive analysis on SGD directly might be infeasible, and this aspect should be reflected by the confidence rather than the rating. Thus, I'll give the authors the benefit of the doubt and keep my score, since I really enjoyed reading this paper. ", "title": "Very nice analysis and accurate predictions using symmetry", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "mB9TwkPWk2J": {"type": "review", "replyto": "q8qLAbQBupm", "review": "This paper analyzes the learning dynamics of DNNs from the perspective of symmetry of some parameters. It is interesting to borrow ideas from physics, which I believe is a right way to go.\n\nSpecifically, the paper derives analytical form of parameters under the cases of translation, scale and inversion invariances, and also modified the underlying gradient flows to accommodate stochastic gradients. The results are very interesting, which I like a lot. However, I have some confusions about some results, which make me not able to give the paper a pass at this time. I will consider revising the score if the authors can clear me in the rebuttal.\n\n1. The papers talks about three invariances, and also given some examples of DNNs that satisfies these invariances. From my understanding, these invariances only apply to a very limited cases of DNN structures, e.g., the softmax layer. And in Figure 3-5, the authors verify the convolutional layers for these invariances. It is not clear to me why convolutional layers satisfy these invariances:  translation, scale and inversion. Am I missing something?\n\n2. In 6.1, a new solution for the translation invariance case is derived. However, it seems that the parameters following the solution still converge to zero? This is obviously not the case in practice.\n\n3. Third line below eq.16, it says \"there is a competition between the centripetal effect of weight decay and the centrifugal effect of discretization.\" I don;'t understand why the descretization has the centrifugal effect.\n\n\n==========\nAfter rebuttal: The rebuttal resolves most of my concerns. It is more clear to me that this is an interesting paper on describing the dynamics of DNN parameters in the training, which seems novel to me. I also realize that the closed form dynamics are not for individual parameters, but in terms of some statistics of the parameters, e.g., the sum of the parameters. This makes the results not as existing as what I thought. That is why I decide to raise my score to 6.", "title": "Interesting theoretical perspective on dynamics of DNNs but with some confusions", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "JIhoowVgfA": {"type": "rebuttal", "replyto": "aEBvSkU-km0", "comment": "We are glad that you \u201cappreciate the updates\u201d to our paper and that we have fully addressed your original suggestions for improvement. Yet, we are quite surprised that you are still inclined not to accept our paper because of the single fact that we use continuous-time dynamics and that you are now suggesting we rewrite our paper using discrete time dynamics, a comment you did not originally make. This is especially surprising, given the fact that **the existing literature has made significant theoretical progress by using continuous-time dynamics** (e.g., Saxe et al., S.S. Du et al., Jacot et al., Mandt et al.). \n\nWe understand that predictions from simple continuous time dynamics, such as gradient flow, can deviate largely from the real-world dynamics of SGD. Indeed, this very fact was our motivation to construct a better continuous time dynamics model as thoroughly explained in section 5, Supplementary Material B, and C.  Indeed the dynamics using our realistic continuous-time models, mathematically justified in the existing literature (Li et al. (2017); Feng et al. (2019); Kovachki & Stuart (2019)), lead to \u201cmore difficult\u201d, complex dynamics, confirming your \u201cexperience, dealing with SGD\u201d, and match thoroughly with empirics from of discrete time finite rate SGD learning dynamics of  state-of-the-art models with millions of parameters trained on large-scale datasets (figures 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17). Thus, given our solid theoretical and empirical groundings we would appreciate if you could be clearer where you think \u201csurprises\u201d we overlooked could come from. Mathematically, \u201csurprises\u201d can only arise from incorrect approximations. If you think there might be a \u201csurprise\u201d, point to us which exact approximation we made in the construction of our realistic model of SGD that warns you the most.\n\nIf by \u201csurprise\u201d you mean the dynamics of SGD are difficult to understand, then we agree.  While it might be \u201csurprising\u201d that the norms of the weight matrices before batch normalization are monotonically increasing through training, it is not surprising once you understand that the dynamics for the norm are driven by an exponentially moving sum of gradient norms. While it might be \u201csurprising\u201d that the column sums of the final layer of a VGG-16 model oscillates between positive and negative values, it is not surprising once you understand that their dynamics are driven by a second-order ODE describing a harmonic oscillator.  It might be equally \u201csurprising\u201d why this behavior arises for certain hyperparameter choices but not others, yet again, this is not surprising when you understand how the hyperparameters of optimization impact the coefficients of the ODE.  **Our work has already made significant progress towards demystifying the \u201csurprising\u201d dynamics of SGD**.\n\nYou are incorrect when you state that \u201cthe paper doesn't consider how the symmetries will change by considering the discrete time SGD\u201d.  As we clearly state in section 3, these symmetries hold \u201cat all points in training, no matter the loss or dataset\u201d.  **Symmetries are inherent properties of a network\u2019s architecture and are completely independent of the optimization process**. If your comment was actually about how the \u201cconservation laws\u201d will change by considering the discrete time SGD, then that\u2019s what the rest of the paper is about,  as you confirmed, \u201cI understand that the dynamics considered in the paper approximate well the discrete time SGD.\u201d", "title": "3rd Response to Reviewer 3"}, "20zskMO_Nq4": {"type": "rebuttal", "replyto": "8AhZoGfUJR", "comment": "We are very glad that you are quite satisfied with most of our paper updates and have raised your score. We thank you very much for your open-mindedness and flexibility.  However, we are surprised that you are still not voting for acceptance based on a single remaining point that \"the paper still lacks a theoretical justification for the continuous time dynamics\". Indeed, in the updated manuscript, we have dedicated the entire section 5 as well as the sections B and C of Supplementary Material to build up the continuous-time dynamics step by step. If you had not seen the updated versions of Sec. 5 and Sec. B and C, which we intensively wrote to address your concerns, we would greatly appreciate it if you could take a look, and revise your score further upwards if these sections do indeed address your last remaining concern about a theoretical justification for the equations.  If they do not, then if you could be more specific about what your concerns are (i.e. which exact steps, etc\u2026), then we would be more than happy to address your concerns even more effectively.  \n\nAlso please note: existing literature theoretically justifies the continuous time dynamics and we only strengthen it through extensive confirmation against numerical experiments. Indeed, as we discussed in section 5, solid mathematical justifications of the modified equation analysis are provided in the existing literature that we properly cite. For example, existing literature that we cite in section 5, \u201c Li et al. (2017); Feng et al. (2019) and most recently Barrett & Dherin (2020)\u201d for SGD and \u201cKovachki & Stuart (2019)\u201d for momentum provide a solid theoretical justification for the continuous time dynamics. Additionally, modeling the stochasticity of SGD with a stochastic differential equation as done in section 5 is standard practice in existing literature (see Mandt et al. (2015)). If anything, our paper provides by far the strongest empirical tests of the continuous time dynamics, by considering how they interact with symmetry, which only strengthens the \"justification for the continuous time dynamics\u201d on top of the existing literature. Based on our extensive  tests of our equations against numerical simulations in Fig. 5 and 6,  we hope there can be no doubt as to the correctness of the equations. \n\nWe again thank you for your great care in reviewing our manuscript. It has become better and more thorough because of your input.  We hope you will like the new sections we wrote with your concerns in mind.\n", "title": "2nd Response to Reviewer 3"}, "wuK9tp_Qy_": {"type": "review", "replyto": "q8qLAbQBupm", "review": "This paper studies the dynamics of the parameters while training a neural network via SGD. SGD is not studied directly but three continuous time approximations of SGD are considered: the classical gradient flow, a stochastic differential equation of Langevin type, and a \"modified\" gradient flow whose derivation has roots in the literature. For each dynamics, the authors show that some invariant properties of the loss function (which are often satisfied in practice) imply some invariant quantities for the dynamics.  \n\nOverall, the paper is rather clear. It tries to provide a physical meaning behind the dynamics of learning, which is an interesting question.\n\nHowever, I do not see any significant theoretical contribution. For instance, all derivations are simple differential calculus applications. Moreover, several models for SGD are used, each of them have their own invariant properties (which look alike) and then what? The technical contribution is not clear to me. Finally, the experimental contribution is rather mild because the numerical experiments are not discussed in the main text (except marginally in the conclusion). There is a lot of room for improvement (see below) and I don't recommend the paper for publication in this form.\n\n\nSpecific remarks:\n- The first paragraph of Section 3 is not necessary in my opinion (already explained in the intro). Moreover, the notation for the group action is a bit misleading, e.g. there is confusion between \\psi, \\psi(\\theta) and \\psi(\\theta,\\alpha)\n- Eq 2: the translation invariant suddenly applies only to a subset of the parameters.\n- Figure 1, 2 are not commented in the main text. Same for 3,4,5 (except in the conclusion)\n- The authors could recall Noether's theorem for comparison\n- The models used for SGD are not theoretically justified, except for the classical gradient flow, for which it is standard (Kushner & Yin 2003). For Eq (11) it starts to be sloppy (CLT + Forward Euler). For Eq (13), only intuition is provided.\n- Last paragraph of Page 5. It seems that the discussion applies for any \\xi (not necessarily Gaussian). Does it help to remove the Gaussian noise assumption?\n\n\n\n\n\nTo improve the paper, I suggest the authors to \n- clearly locate their work within the existing literature. This would help to understand why the questions answered in this paper are important, and to highlight their contribution.\n- Turn their paper into an experimental one. To this end, dedicate a whole section to **commented** numerical experiments. This does not mean adding more simulations, just explain them in the main text and how they contribute to the main message of the paper.\n\n\nMinor:\n\nPage 2: \"nornalization\"\nBetween Eq 3 and 4: Notation not defined\nPage 4: ReLU not defined (give the formula)\nSection 5.1, 2nd line. \\nabla is missing \nPage 6: graident\nPage 7 (two times): previosly \n\n\n", "title": "Clear paper but no significant contribution", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "q0fO_lEA4F4": {"type": "rebuttal", "replyto": "eYJkUS3I9ee", "comment": "Without batch normalization then sequential convolutional operators will respect **rescale symmetry** (assuming the activation function is homogeneous).  To understand this imagine  scaling the entire filter associated with a channel by alpha and then at the next convolutional layer scaling the components associated with that channel in all filters by 1/alpha.  If the activation function \u201cin between\u201d the convolutional  operators is homogeneous, then the output of the function will be unmodified as these scales will cancel.  This is the same argument for fully-connected networks where we consider rows and columns of sequential linear operators.  There will be as many rescale symmetries as shared channels between the two operators.", "title": "Follow Up to Reviewer 1"}, "FiWCOp5pDBp": {"type": "rebuttal", "replyto": "Mrh_wcnxRKY", "comment": "You are correct, the differential equations and their exact solutions for scale and rescale symmetry (eq 19, 20), under the realistic settings of finite batch size and finite learning rates, depend on the data via the norms of the gradients, which must be obtained empirically (see Supplementary Material G for our implementation details). However, we still believe our analytic understanding of the dynamics of parameter combinations in realistic settings is of value, as we explain below.\n\n**Defining the relationship between gradient norms, optimization hyperparameters, and learning dynamics.** As discussed in section 4, predicting weight combinations based upon gradient flow leads to data agnostic conservation laws. However, if you are a practitioner who cares about learning dynamics with finite step size, with or without momentum, and with finite batch size, you would soon find that these quantities are *not conserved* and instead present highly complex dynamics (Fig. 1 and Fig. 5).  Prior to our theory, this complex dynamics may seem quite disconcerting to a practitioner. For example, in Fig. 5 the overall scale of weights can go up, go down, or even exhibit nonmonotonic dependence (middle row).  Also in Fig. 5 the difference in weight norms gives complex dynamics (bottom row).\n\nA practitioner seeing all this complexity might be startled and might wonder: (1) why are these quantities even changing when the gradient flow theory claims they should be conserved? (2) does there exist any simple analytic dynamical law that governs their motion, and if so, what is that law and how do the myriad possible quantities (general data structure, momentum terms, weight decays, batch size, learning rate, etc..) all interact to generate this law; and (3) can this law predict the motion quantitatively?    \n\nSections 5 and 6 answer these questions definitively, and equation 19 and 20 are the answers. \n\n**Intriguingly, we show there exist very simple analytic laws that govern the complex dynamics in Fig. 1 and 5.** Moreover, we show that these dynamics, which could have depended on the data in a priori arbitrarily complex ways, *instead depend on the data **only** through the norms of the gradient*.  Thus the situation is simpler than one might have had any reason to initially believe, given the complexity of the dynamics seen in Fig. 1 and Fig. 5.  Of course, to solve these analytic laws of motion, one must integrate these equations with the observed gradient norms.  A nice analogy comes from other spheres of mechanics. For example, the motions of planets are governed by analytic laws, and their discovery was fundamental. But to predict the motion of more than two planets, these analytic laws must still be numerically integrated.  In our setting, we feel these analytic laws of motion are a very useful contribution, as they describe the complex dynamics of thousands of weight combinations in real world networks and datasets, and reduce the dependence on the data only to gradient norms that act through integral terms.  Indeed, these analytic laws suggest the presence of strong oscillations with momentum, **a prediction which we then tested empirically and confirmed**  (Fig. 6 right panel), demonstrating the power of our theory to predict interesting phenomena and the quantitative hyperparameter choices that generate them.\n\nMost importantly, these analytical laws match empirical results on real-world DNN trained on real-world datasets exactly.  **We feel this is not a trivial accomplishment.**  Understanding the exact functional relationship between the data dependence (and more precisely isolating this data-dependence specifically to  gradient norms), the optimization hyperparameters of batch size, learning rates, and momentum terms,  and the learning dynamics of certain weight combinations is the main contribution of this work.\n\nFuture work could consider how to use this precise understanding to obtain analytic control over the weight combinations prior to even training.  For example, one route might be assuming certain properties of the gradient norm such as it is constant through training or proportional to the weight combination itself.  Another route would be considering how normalizing the gradients or adapting the learning rate might simplify the integral term.  It's worth noting that we empirically compute this integral term by slightly modifying the PyTorch SGD optimizer with an additional buffer keeping an exponentially moving average of gradient norms during training (see Supplementary Material G), which is very similar to the buffers introduced by adaptive learning algorithms such as Adam, Adagrad, and RMSprop.  Understanding the relationship between these analytic laws and adaptive optimization is a promising direction for future work.", "title": "2nd Response to Reviewer 4"}, "fIF4xs4em9f": {"type": "rebuttal", "replyto": "q8qLAbQBupm", "comment": "We sincerely thank all the reviewers for their positive and constructive feedback that has significantly reshaped our paper. We have incorporated all the suggestions in our updated manuscript, and we hope that the reviewers will reconsider their ratings to reflect the revisions. Below is an overview of the revisions.  Please also see our individual responses to each of the reviewers for the broader context.\n\n## New Figures\nMain text:\n- Figure 1. Neuron level dynamics are simpler than parameter dynamics.\n- Figure 4. Modeling discretization.\n- Figure 5. Exact dynamics of VGG-16 on Tiny ImageNet.\n- Figure 6. Momentum leads to harmonic oscillation.\n\nSupplementary Materials:\n- Figure 11. The planar dynamics of Momentum on VGG-16 on Tiny ImageNet.\n- Figure 12. The spherical dynamics of Momentum on VGG-16 on Tiny ImageNet.\n- Figure 13. The hyperbolic dynamics of Momentum on VGG-16 on Tiny ImageNet.\n- Figure 16. The per-neuron spherical dynamics of Momentum on VGG-16 on Tiny ImageNet.\n- Figure 17. The per-neuron hyperbolic dynamics of Momentum on VGG-16 on Tiny ImageNet.\n\n## Major revisions \n- Motivating our work in the context of existing literature (section 1, 2)\n- Renamed \u201cInversion Symmetry\u201d to \u201cRescale Symmetry\u201d (section 3) \n- Consistent notation and rigorous proof for gradient/Hessian properties (section 3 and Supplementary Material A)\n- An updated section dedicated to commented empirics (section 6)\n- New theoretical and empirical study of stochastic gradient descent **with momentum** (see section 5, 6, and Supplementary Material B, E, H)", "title": "Overview of Paper Update"}, "3NHruIAUwk": {"type": "rebuttal", "replyto": "wuK9tp_Qy_", "comment": "Thank you for your detailed reviews and constructive suggestions which have reshaped our manuscript significantly. We are glad that you find our question interesting and think our presentation is clear, but with \u201croom for improvement\u201d. We have incorporated all of your specific remarks and minor comments, which you can find in the updated manuscript.  We will now  explain how we have incorporated your two major suggestions about (i) locating our work within broader context and (ii) creating a new section in the main text focusing on the empirics. \n\n### Locating our work in the existing literature\nTo better locate our work within the existing literature, we have (i) intensively edited the introduction, (ii) added a whole new discussion reviewing existing literature of learning dynamics, (iii) created a new table in section A in the Supplementary Materials presenting how we unify and generalize the existing literature on the geometry of loss landscapes through the lens of symmetry.\n\n**Our work opens up a new direction for understanding training dynamics, by not making simplifying assumptions, but rather restricting our predictions to meaningful parameter combinations (see updated section 1 & 2).**  A foundational question in deep learning theory is: what, if anything, can we quantitatively predict about the training dynamics of large-scale, non-linear neural network models driven by real-world datasets and optimized via stochastic gradient descent with a finite batch size, learning rate, and with or without momentum? In order to make headway on this extremely difficult question, existing works have made major simplifying assumptions on the network, such as restricting to identity activation functions Saxe et al., infinite width layers Jacot et al., or single hidden layers Saad & Solla. Many of these works have also ignored the complexity introduced by stochasticity and discretization by only focusing on the learning dynamics under gradient flow. In the present work, we make the first step in an orthogonal direction. Rather than introducing unrealistic assumptions on the model or learning dynamics, we uncover restricted, but meaningful combinations of parameters with simplified dynamics that can be solved exactly without introducing a single assumption (see the new Fig. 1 in our paper). We make this fundamental contribution by using the tools of symmetry and modified equation analysis, which have been underutilized in deep learning. Our work provides a new arsenal of tools to analyze learning dynamics and evidence that exact predictions are possible even for non-linear architectures, real-world datasets, and learning rules at both finite step sizes, batch sizes, and with or without momentum. For example, in the case of VGG-16 with batch normalization trained on Tiny-ImageNet (one of the model/dataset combinations we considered in section 6) there are 12,751 distinct symmetries which means that we can analytically describe the learning dynamics of 12,751 parameter combinations, again at finite step sizes, batch sizes, and with or without momentum, and we confirm our analytic predictions against numerical simulations. To our knowledge, this has never been done before in any real world setting.   \n\n**We have added a whole new section describing how our symmetry-based proof on the geometry of the loss landscapes unifies, simplifies and generalizes existing literature (see Supplementary Material A).** Inspired by your suggestion, we have added a whole new section A in the Supplementary Materials formalizing our derivation of symmetry induced geometric properties and created a new subsection detailing how they relate to the existing literature. First, you are correct that \u201call derivations are simple differential calculus\u201d. However, we believe that the mathematical simplicity of our proof strategy via symmetry is a strength, not a weakness, just as in Noether\u2019s theorem. Notably, our symmetry-based proofs automatically, in a unified manner, yield as many as 15 formulas for the geometric properties of loss landscapes, in the form of constraints on gradients and Hessians.  Remarkably, to the best of our knowledge, some of the 15 formulas that we derive are new (see table 1 in the Supplementary Materials). Others have been derived in previous literature, but many of their proofs are tedious, algebraic, or sometimes make unnecessary assumptions such as biases can\u2019t be included or the properties only hold per layer. Conversely, our strategy is as simple as, if you can identify a differentiable invariance for a set of parameters, then the geometric properties associated with that invariance hold for those parameters. In the future, we expect that our proof strategy of harnessing symmetry will become the standard when investigating the geometry of the loss landscape. So we believe our general, unifying proof strategy itself is a significant contribution.", "title": "Response to Reviewer 3 (1/2)"}, "IuHg86njQxY": {"type": "rebuttal", "replyto": "wuK9tp_Qy_", "comment": "### Strengthening our empirical contributions\n\n**New theoretical and empirical study of stochastic gradient descent \u201cwith momentum\u201d (see section 5, 6, and Supplementary Material B, E, H).** Motivated by your constructive feedback, we have generalized our theoretical and empirical results to include stochastic gradient descent with momentum. Momentum is a commonly used hyperparameter when training deep neural networks and is thus a necessary consideration to obtain theory that can empirically match up with large scale real-world deep networks. To expand our analysis we have taken the following approach:  (1) We considered how momentum alone affects the gradient flow ODE to derive a continuous-time limit for momentum.  (2) We demonstrate how modified equation analysis interacts with this limiting differential equation, resulting in a second-order ODE for the momentum dynamics.  (3) We apply this equation of motion with our symmetries to derive exact solutions for the dynamics of meaningful parameter combinations.  Surprisingly, the solutions we obtain take the form of driven harmonic oscillators where the driving force is given by the gradient norms, the friction is defined by the momentum constant, the spring coefficient is defined by the regularization rate, and the mass is defined by the the learning rate and momentum constant. Amazingly, we again find a theoretical parallel between physics and deep learning. For most standard hyperparameter choices, our solutions are in the overdamped setting and align well with our previous first-order solutions up to a time rescaling.  However, for large values of the momentum constant, we can push the solution into the underdamped regime where we would expect harmonic oscillation and indeed, we empirically verify our nontrivial predictions even at scale for VGG-16 trained on Tiny ImageNet.  Our success expanding our original analysis to the setting of momentum also provides a rubric for how future work might consider the impact of adaptive optimizers, such as Adam, Adagrad, RMSprop, or the more recent LARS/LAMB, on the training dynamics of neural networks. \n\n**An updated section dedicated to commented empirics (see section 6, and Supplementary Material H).** We really appreciate your suggestion of how to focus more directly on the empirical results we already have and their qualitative conclusions. We agree that the original form of our numerical experiments was verbose and diluted the main findings and conclusions of our experiments. We have reworked this section, by in part moving most of the derivation of the solution to the appendix and rather focusing on the commented experiments in section 6 in the main text. We invite you to re-read this section and hope that this addresses your constructive feedback. While space constrained us from further discussing our empirics in the main paper, nevertheless, we added a new section H in the Supplementary Materials presenting all of our experimental findings per layer (Fig. 8,9,10,11,12,13) and per neuron (Fig. 14,15,16,17).\n\n\nWe would like to thank you again for your very helpful feedback that has significantly improved our paper. We sincerely hope that these major revisions have now addressed all of your concerns.\n", "title": "Response to Reviewer 3 (2/2)"}, "PZnR3CgAja7": {"type": "rebuttal", "replyto": "LDk_3oOVHSp", "comment": "We thank you for clearly recognizing the contribution of our work as \u201cthe current work makes accurate predictions on DNNs trained in a real-world setting \u2026 which is a very complicated problem\u201d.  To address your technical comments: (1) In section 3 of the updated manuscript, we have introduced a new notation system to discuss subsets of parameters to avoid any confusion. (2) Throughout the updated manuscript, we have replaced all the \u201cinversion symmetry\u201d to \u201crescale symmetry\u201d. This terminology of \u201crescale invariance/equivariance\u201d is commonly used in the related work Neyshabur et al. Now we will address your larger remarks:\n\n**The parameter combinations do depend on data.** We apologize for any confusion that we may have created, which lead the reviewer to conclude that parameter combinations that we predict \u201care unaffected by the real-world datasets.\u201d  While the parameter combinations are conserved under gradient flow, they are *not* conserved at finite learning rate, and the gradient norms can drive changes in these quantities (see equations 19, 20); therefore their learning dynamics are indeed directly driven by the data (except for translation symmetry).  We have significantly revised our discussion in section 6 to highlight this data dependence.\n\n**Symmetry transformations do impact the learning dynamics as the gradient and regularizer need not to respect symmetry.**  You are correct that, by its very definition, the symmetry transformations on parameters have no effect on the outputs and loss of the network. However, these transformations have a crucial impact on the learning dynamics. This is because, even if the network outputs are unaffected under some transformation of the parameters, the gradient and regularization for these parameters can change.  For example, in the case of scale symmetry, if we increase the norm for parameters preceding a batch normalization layer, then the output of the network doesn\u2019t change, but the gradient and L2 regularization for these parameters decrease and increase, respectively.  This property has been used to explain how batch normalization directly controls the effective learning rate for its parameters, impacting their learning dynamics. Therefore, understanding the dynamics of these norms is of crucial importance to understand how the effective learning rate evolves throughout training and thus the network as a whole. Remarkably, our work provides the exact solution for this quantity. Overall, our work provides a theoretically tractable direction for exploring how the training hyperparameters (weight decay, momentum, batch size, learning rate) affect the learning dynamics.\n\n**Principles of symmetry have been and will be crucial in designing new network architectures and optimizers.** Understanding how symmetries in the loss affects learning dynamics through gradient and Hessian geometries has been crucial in designing new network architectures and optimizers. For example, one of the important motivations behind the invention of Batch Normalization by S. Ioffe and C. Szegedy was the realization that the scale invariance of Batch Normalization can stabilize gradient signal propagation while leaving networks\u2019 outputs intact. Similarly, the motivation for the invention of the Path-SGD optimizer by B. Neyshabur et al., was the fact that gradient descent does not respect rescale equivariance even when the network outputs respect rescale invariance. In this work, we have unified and generalized the geometric properties of the gradient and Hessian induced by symmetry (see a new section A in the Supplementary Materials), thus providing a theoretical foundation for principled design of architectures and optimizers to achieve certain geometric goals.\n\nThank you again for your feedback. Your suggestion to focus on the broader horizon has significantly helped us refine our paper from motivation to interpretation of our theoretical and empirical results. We hope these updates have clarified the potential future impact of our work.", "title": "Response to Reviewer 4"}, "XEbv4iQjnES": {"type": "rebuttal", "replyto": "mB9TwkPWk2J", "comment": "Thank you for your constructive review. We are glad that you like our approach and results, finding them very interesting. We will now respond to your specific questions, hopefully clarifying any confusion we have generated:\n\n1. **Nearly every parameter in modern neural network architectures is involved in one of the three invariances discussed in our paper.** As we discussed in section 3, at any hidden unit with batch normalization, the scale symmetry applies to the parameters into this neuron. At any hidden unit with a homogeneous activation function ($f(x)$ obeying $f(\\alpha x) = \\alpha^n f(x)$, for example ReLU obeys when $n=1$) the rescale symmetry applies to the parameters in and out of this neuron. In the case of consecutive convolutional layers, this means the filters into a channel and the filters out of that channel. And as you mentioned, the parameters immediately preceding a softmax function observe translation symmetry. Thus, most parameters in modern deep neural networks are involved in at least one symmetry.  In the case of VGG-16 with batch normalization trained on Tiny-ImageNet (one of the model/dataset combinations we considered in section 6) there are 12,751 distinct symmetries and every single parameter is involved in at least one of these symmetries.  See table 5 in Supplementary Material G for a breakdown of the number/type of symmetries for the models we used.\n2. **Parameters that respect translational symmetry do not converge to zero, their sum does.** As you can see in equation 18 (section 6), the projection of parameters observing translation symmetry $\\theta$ onto the all-ones vectors $\\langle \\theta, \\mathbb{1}_{\\mathcal{A}} \\rangle$ exponentially converges to zero. Please note that the parameters $\\theta$ are not going to zero, their sum is. There are many ways for this projection to converge to zero without the parameters themselves converging to zero, unlike in the case of scale symmetry.\n3. **Discretization of circular motion leads to a growing norm due to the curvature of the trajectory (see section 6 and Supplementary Material C).** Motivated by your question, we have clarified our explanation in the main text (section 6) and added further intuition in the Supplementary Materials, which we will go over now (see Figure 7 in Supplementary Material C.) Consider a particle moving in circular motion as discussed/depicted in Figure 7. For an infinitesimal step size, then our particle will stay on the circle defined by its initialization.  However, for a finite step size then with each discrete update we \u201cfall off\u201d the circle that we were just on to a circle with a larger radius.  The larger the step size, the greater the change in radius.  Intuitively, we can understand discretization to be leading to a centrifugal effect moving our particle away from the origin. We can formulate these intuitions through modified equation analysis, where we can derive that discretization of these first order dynamics leads to a negative acceleration term, countering the acceleration into the origin needed for circular motion. We have further expanded on the modified equation analysis in the Supplementary Material in the updated manuscript.\n\nWe sincerely hope that our reply has clarified your confusions, and you could now revise the score in light of the updated manuscript and the detailed discussion above.  However, please let us know if you have any further questions.", "title": "Response to Reviewer 1"}, "0xL88ho6NxO": {"type": "rebuttal", "replyto": "a27_KGlIlo", "comment": "We appreciate that you find our paper very well-written, clear, and elegant without finding any major drawbacks.\n\n**Clarifying our contribution in the light of existing literature.** To help you better understand the novelty and contribution of our work in the light of existing literature, we have added (i) a new discussion in the related work section in the main text and (ii) a new table in section A in the Supplementary Materials presenting how we unify and generalize existing literature on the geometry of loss landscapes through the lens of symmetry. We would also like to thank you for the reference to the work by Li et al. Indeed this work is very relevant and we discuss it in section A of our Supplementary Material how the geometric properties they use in their work fits into our framework of symmetry.\n\n**Addressing your two small questions.** You are correct that (i) loss functions with scale symmetry are discontinuous at the origin, and (ii) that minimizing such a function with L2 regularization under gradient flow does not make sense as you explained.  Yet, we know that any neural network using batch normalization has scale symmetry in the weights and it is common practice to train these networks with weight decay. Furthermore, we do not see this problem in empirics (Figure 5). This discrepancy was the exact issue that motivated us to construct more realistic continuous models of stochastic gradient descent as discussed in section 5.  As we further explained, it is discretization that counteracts the centripetal force of weight decay preventing the weights from collapsing to the origin.  See our updated discussion on this explanation in section 6.", "title": "Response to Reviewer 2"}, "LDk_3oOVHSp": {"type": "review", "replyto": "q8qLAbQBupm", "review": "The current work studies the implications of continuous symmetries of a DNN on its weight dynamics. Specifically, they consider linearly realized symmetries (such as a shift/translation to the logits), and use the fact that the loss/hessian/mini-batches, are insensitive to such shift to decouple their dynamics. They do so both in the continuous/vanishing-learning-rate case and for small learning rates and manage to provide accurate quantitative predictions for the dynamics of these quantities.\n\nOn the positive side, the current work makes accurate predictions on DNNs trained in a real-world setting (real datasets, convolutional layers, finite learning rates, mini-batches etc...) which is a very complicated problem. They do so with a refreshing toolbox, that of symmetries. \n\nOn the other hand, what makes their quantities tractable, seems to be the very fact that they have no impact on the DNNs final outputs. For example, this is why they are unaffected by the real-world dataset. While being a clever trick, it can be viewed as an inherent limitation of the approach: one understands quantities that have no bearing on what the DNN learns. I don't see, for instance, how one expects to perform architecture exploration (as mentioned in the discussion), using quantities that have no implications on the DNNs predictions. If the authors can argue that their approach has a broader horizon, it may increase its potential impact. \n\nTwo technical comments: \n\n1. I find the use of subset notation and definition of large theta confusing: It seems that small theta is the set of parameters but it is a subgroup of large theta. \n2. Inversion symmetry commonly refers to a discrete Z_2 symmetry, whereas the authors take it to be some representation of GL1.\n\n \n\n\n\n ", "title": "Dynamics of some weight combinations of real world DNN can be predicted ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}