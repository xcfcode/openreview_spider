{"paper": {"title": "Feature Incay for Representation Regularization", "authors": ["Yuhui Yuan", "Kuiyuan Yang", "Jianyuan Guo", "Jingdong Wang", "Chao Zhang"], "authorids": ["yuyua@microsoft.com", "kuiyuanyang@deepmotion.ai", "1701214082@pku.edu.cn", "jingdw@microsoft.com", "chzhang@cis.pku.edu.cn"], "summary": "", "abstract": "Softmax-based loss is widely used in deep learning for multi-class classification, where each class is represented by a weight vector and each sample is represented as a feature vector. Different from traditional learning algorithms where features are pre-defined and only weight vectors are tunable through training, feature vectors are also tunable as representation learning in deep learning. Thus we investigate how to improve the classification performance by better adjusting the features. One main observation is that elongating the feature norm of both correctly-classified and mis-classified feature vectors improves learning: (1) increasing the feature norm of correctly-classified examples induce smaller training loss; (2) increasing the feature norm of mis-classified examples can upweight the contribution from hard examples. Accordingly, we propose feature incay to regularize representation learning by encouraging larger feature norm. In contrast to weight decay which shrinks the weight norm, feature incay is proposed to stretch the feature norm. Extensive empirical results on MNIST, CIFAR10, CIFAR100 and LFW demonstrate the effectiveness of feature incay. ", "keywords": ["feature norm", "regularization", "softmax loss", "feature incay"]}, "meta": {"decision": "Invite to Workshop Track", "comment": " + An intriguing novel regularization method: encouraging larger norms for the feature vector input to the last softmax layer of a classifier.\n + Resonably extensive experimental validation shows that it improves test accuracy to some degree.\n - While a motivation is given, the formal analysis of what is really going on remains very superficial and limited.\n\nTechnical note: Simply scaling the softmax layer's input would not change class rankings, so any positive effect of this regularizer on classification performance is due to it changing the learning dynamic in the upper layers as well. The paper could be much stronger if it did provide an analysis regarding how the global learning dynamic is affected in all layers, by the interaction between weight decay and the last layer's feature incay.\n"}, "review": {"rJdF7cMUz": {"type": "rebuttal", "replyto": "SyJOEkTHf", "comment": "Thanks for your comments. \n\n\"The paper could be much stronger if it did provide an analysis regarding how the global learning dynamic is affected in all layers, by the interaction between weight decay and the last layer's feature incay.\"\n\nWe have always contained the analysis you mentioned in the supplementary meterials of our paper.\n\nPlease check Section 6.6 and Figure 7.", "title": "Justification about the technical note"}, "SkNxPOYlf": {"type": "review", "replyto": "ryALZdAT-", "review": "The analyses of this paper (1) increasing the feature norm of correctly-classified examples induce smaller training loss, (2) increasing the feature norm of mis-classified examples upweight the contribution from hard examples, are interesting. The reciprocal norm loss seems to be reasonable idea to improve the CNN learning based on the analyses. \n\nHowever, the presentation of this paper need to be largely improved. For example, Figure 3 seems to be not relevant to Property2 and may be show the feature norm is lower when the samples is hard example. Therefore, the author used reciprocal norm loss which increases feature norm as shown in Figure 4. However, both Figures are not explained in the main text, and thus hard to understand the relation of Figure 3 and 4. The author should refer all Figures and Tables. \n\nOther issues are:\n-Large-margin Soft max in Figure 2 is not explained in the introduction section. \n-In Eq.(7), P_j^I is not defined. \n- In the Property3, The author wrote \u201c where r is lower bound of feature norm\u201d. \n However, r is not used.\n-In the experimental results, \u201cRN\u201d is not defined.\n-In the Table3, the order of \\lambda should be increasing or decreasing order. \n- Table 5 is not referred in the main text. \n\n== Updated review == \nThe presentation has been improved, I have increased the rate from 5 to 6. \nFollowing are further comments for presentation. \n\n-\tFig.2 \u201c the increasing L2 norm \u201c seems to  \u201cthe order of L2 norm \u201d\n-\tPp.4 the first sentence above Eq.(7) \u201cAccording to definition \u2026\u201d  should be improved . \n-\tpp.5, the first sentence of the second paragraph \u201cThe feature norm can be optimized ..\u201d is not clear. \n-\tIt would be better put Figure 5 under Property3. \n-\tD should be defined in Property3. \n-\tpp.8 wrote \u201cHowever, 259-misclassfied examples are further introduced\u201d. However, in Table 5, it seems to be 261. \n-\tSection 5. is \u201cConclusion and future work\u201d. However, future work is not mentioned. \n", "title": "Interesting analysis of feature norm, but the paper needs improvements", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BkEcWHKlf": {"type": "review", "replyto": "ryALZdAT-", "review": "Pros:\n1. It provided theoretic analysis why larger feature norm is preferred in feature representation learning.\n\n2. A new regularization method (feature incay) is proposed.\n\nCons:\nIt seems there is not much comparison between this proposed method and the concurrent work \"COCO(Liu et al. (2017c))\".", "title": "This paper investigates how to finetune feature norms of correctly-classified and mis-classified feature vectors to improve learning process. Based on the analysis, they proposed feature incay to encourage larger feature norm. Experimental results on four datasets demonstrate the effectiveness of the proposed method.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "ryRBHPFxz": {"type": "review", "replyto": "ryALZdAT-", "review": "The manuscript proposes to increase the norm of the last hidden layer to promote better classification accuracy. However, the motivation is a bit less convincing. Here are a few motivations that are mentioned.\n(1) Increasing the feature norm of correctly classified examples helps cross entropy, which is of course correct. However, it only decreases the training loss. How do we know it will not lead to overfitting?\n(2) Increasing the feature norm of mis-classified examples will make gradient larger for self-correction. And the manuscript proves it in property 2. However, the proof seems not complete. In Eq (7), increasing the feature norm would also affect the value of the term in parenthesis. As an example, if a negative example is already mis-classified as a positive, and its current probability is very close to 1, then further increasing feature norm would make the probability even closer to 1, leading to saturation and smaller gradient.\n(3) Figure 1 shows that examples with larger feature norm tend to be predicted well. However, it is not very convincing since it is only a correlation rather than causality. Let's use simple linear softmax regression as a sanity check, where features to softmax are real features rather than hidden units. Increasing the feature norm seems to be against the best practice of feature normalization in which each feature after normalization is of variance 1.\n\nThe manuscript states that the feature norm won't be infinitely increased since there is an upper bound. However, the proof of property 3 seems to only apply to the certain cases where K<2D. In addition, alpha is in the formula of upper bound, but what is the upper bound of alpha?\n\nThe manuscript does comprehensive experiments to test the proposed method. The results are good, since the proposed method outperforms other baselines in most datasets. But the results are not impressively strong.\n\nMinor issues:\n(1) For proof of property 3, it seems that alpha and beta are used before defined. Are they the radius of two circles?", "title": "The idea is interesting, but motivation requires more justification. Results are good, but not very impressive", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1ASOQr-f": {"type": "rebuttal", "replyto": "BkEcWHKlf", "comment": "Thanks a lot for your positive and constructive comments!\n\nWe provide the response to \"It seems there is not much comparison between this proposed method and the concurrent work 'COCO(Liu et al. (2017c))'.\"\n\n(1) Both \"COCO\" and \"Feature Incay\" increase the L2-norm of feature representations, which is the common reason for performance improvement. \n\n(2) There are several clear differences. \n     a. \"COCO\" normalizes and rescales all features to have the same L2-norm while\"Feature Incay\" adds a new regularizer that prefers features with larger L2-norm. \"COCO\" uses the optimal scale value that is fixed during training while \"Feature Incay\" increases the feature norm without constraining the scale value. \n    b. \u201cCOCO\u201d optimizes feature embedding spreading on a hypersphere while \u201cFeature Incay\u201d optimizes feature embedding located between two hyperspheres with different radiuses. (see Property 3)   \n    c. \"COCO\" proposes a novel congenerous cosine loss while \"Feature Incay\" uses the original softmax loss:  \"Feature Incay\" is simpler than \"COCO\"  and it can be easily plugged into almost all the related works that use softmax loss. \n\n(3) We compare the \"COCO\" with \"RN + COCO\" on CASIA-WebFace with SphereNet-20 and find that \"Feature Incay\" can help improve the performance of \"COCO\". e.g., \"RN + COCO\" improves \"COCO\" from 98.90% to 99.02%.\n\n", "title": "Comparison with  \"COCO(Liu et al. (2017c))\"."}, "HkjBl2jZf": {"type": "rebuttal", "replyto": "ryRBHPFxz", "comment": "Thanks a lot for your insightful comments.\n\n-1- Increasing the feature norm of correctly classified examples helps cross entropy, which is of course correct. However, it only decreases the training loss. How do we know it will not lead to overfitting?\n\nGood question. In our experiments, we don't find the feature incay will lead to overfitting. e.g., by considering feature incay, RN + Softmax decreases the training loss and improves the Softmax from 91.41% to 92.16% on the test set of CIFAR10. It remains an open problem to provide theoretical analysis about whether increasing the feature norm will lead to overfitting currently. \n\n\n-2-  Increasing the feature norm of mis-classified examples will make gradient larger for self-correction. And the manuscript proves it in property 2. However, the proof seems not complete. In Eq (7), increasing the feature norm would also affect the value of the term in parenthesis. As an example, if a negative example is already mis-classified as a positive, and its current probability is very close to 1, then further increasing feature norm would make the probability even closer to 1, leading to saturation and smaller gradient.\n\nThanks for pointing this problem. The proof of property 2 is indeed complete.  In your described case, increasing the feature norm will not lead to smaller gradients for both the weight vectors of the ground truth category and the wrongly predict category , which instead will have larger gradients. \n\nWe give the reasons below. For a mis-classified sample i with ground truth label y_i.  It is true that \"When the mis-classified f_i has probability of class k(k!=y_i) close to 1, then increase the feature norm of f_i will make the probability of class k even closer to 1\", but this will not cause \"saturation and smaller gradient\" for all w_k and w_(y_i).  According to Equation (7):\n\n(1) the gradient of w_(y_i) : when  j=y_i, h(i)=1, P_j^i is close to 0, then (P_j^i-h(i)) is close to -1, so the gradients for the weight vector of ground truth category  can be increased by increasing the norm of f_i; \n(2) the gradient of w_(k): when j=k, h(i)=0, as that P_k^i is close to 1, (P_k^i-h(i)) is close 1, the gradients for weight vector of the wrongly predict category can be increased by increasing the norm of f_i. \n(3) the gradients of other w_j:  when j!=y_i && j!=k ,  (P_j^i-h(i)) is close to 0, thus the gradients is close to zero.\n \n\n\n-3- Figure 1 shows that examples with larger feature norm tend to be predicted well. However, it is not very convincing since it is only a correlation rather than causality. Let's use simple linear softmax regression as a sanity check, where features to softmax are real features rather than hidden units. Increasing the feature norm seems to be against the best practice of feature normalization in which each feature after normalization is of variance 1.\n\nThanks for pointing out this interesting problem.  As we observe that the feature norm and the classification accuracy is positively related,  and we investigate whether increasing the feature norm explicitly could improve the performance and find that the classification accuracy is improved with the feature incay.  It also remains an open problem to provide theoretical analysis about whether  it is correlation or causality currently.\nIncreasing the feature norm is not against the best practice of feature normalization.  In fact, increasing the feature norm before normalization can also help improve the final performance, which is shown in Table 1 and stated in the last sentence of Section 4.2.(\"feature incay can even promote the A-softmax with normalized features by elongating the features before normalization\")\n\n\n\n-4- The manuscript states that the feature norm won't be infinitely increased since there is an upper bound. However, the proof of property 3 seems to only apply to the certain cases where K<2D. In addition, alpha is in the formula of upper bound, but what is the upper bound of alpha?\n\nThanks for pointing out this issue. Our property essentially is not limited to K<2D. We updated Property 3 for both K<2D and K>=2D case: \n\"...(2) to ensure the maximal intra-class distance is smaller than the minimal inter-class distance, the upper bound of feature norm is 3*alpha, especially when K < 2D, the upper bound in a tighter range of [(1 + sqrt(2))*alpha, 3*alpha]\". So 3*alpha is a general upper bound whether K<2D or K>=2D. Especially, when K<2D, we can formulate a tighter range for the upper bound.\n\nWhat's the upper bound of alpha is an interesting problem, but it is not our current interest. The main point of Property 3 lies in that the ratio of the upper bound beta to the lower bound alpha is bounded: beta/alpha <= 3.  \n\n-5- For proof of property 3, it seems that alpha and beta are used before defined. Are they the radius of two circles?\n\nYes, they are the radius of the two circles. ", "title": "Justification about our motivation and other issues"}, "HkbH3Pobz": {"type": "rebuttal", "replyto": "SkNxPOYlf", "comment": "Thanks for your comments. \n\n-1- The presentation of this paper need to be largely improved.\nWe have improved the presentation of our paper and updated the pdf files according to your advice.  \n\n-2- Figure 3 seems to be not relevant to Property 2 and may be show the feature norm is lower when the samples is hard example. \nActually, Figure 3 is relevant to Property 2. We revised the description and re-plot Figure 3 in the paper to make their relation much clearer and avoid the possible misunderstanding.\n\nWe provide a short explanation below. The purpose of Figure 3 is to show that the mis-classified examples(we can also call them \"hard examples\") tend to be of small feature norm, which has been re-plot based on your advice. Property 2 is proposed to state that we need to increase the feature norm of mis-classified examples(tend to with small feature norm), which makes larger gradient and helps correcting the mis-classified examples.\nEspecially, the fifth column in Table 5 shows that by increasing the feature norm of mis-classified examples, the \"RN + Softmax\" correctly classifies 336 examples that are mis-classified by \"Softmax\".  \n\n-3- The author used reciprocal norm loss which increases feature norm as shown in Figure 4. However, both Figures are not explained in the main text, and thus hard to understand the relation of Figure 3 and 4.\nThanks for pointing out this problem. We now added the explanations in the main text. Figure 4 is used to show the Reciprocal Norm Loss can result in more intra-class compactness by increasing the small feature norm faster than the large ones. Figure 3 is not related to Figure 4, and it is about Property 2. \n \n-4- Large-margin Soft max in Figure 2 is not explained in the introduction section. \nThanks for pointing out this problem. We provided the explanation of Large-margin Softmax loss in the first paragraph of Section 2 (Related work). We will put it to the introduction section if it is necessary.\n\n-5- In Eq.(7), P_j^I is not defined. \nWe have added the definition of P_j^i in Eq.(7) in the updated paper. In fact, we have also defined P_j^i in Property 4.\n\n-6- In the Property 3, The author wrote \u201c where r is lower bound of feature norm\u201d. However, r is not used.\nThanks for pointing out this problem, which is a typo. \"r\" should be replaced with \"alpha\".\n\n-7- In the experimental results, \u201cRN\u201d is not defined.\n \"RN\" refers to the feature incay with form of Reciprocal Norm. We have added that \"... RN(Reciprocal Norm loss) plus the baseline method. e.g., RN + Softmax means combining the feature incay with Softmax loss.\" in the updated paper.\n\n-8- In the Table 3, the order of \\lambda should be increasing or decreasing order. \nWe have resorted it in decreasing order.\n\n-9- Table 5 is not referred in the main text. \nThanks for pointing out this problem. We have discussed the results in Table 5 in the first paragraph in Section 4.5 and added the reference to Table 5 in the updated paper.\n", "title": "Update on the presentation issues of our paper."}}}