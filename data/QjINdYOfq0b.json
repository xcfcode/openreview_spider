{"paper": {"title": "ABS: Automatic Bit Sharing for Model Compression", "authors": ["Jing Liu", "Bohan Zhuang", "Peng Chen", "Yong Guo", "Chunhua Shen", "Jianfei Cai", "Mingkui Tan"], "authorids": ["~Jing_Liu8", "~Bohan_Zhuang1", "~Peng_Chen2", "~Yong_Guo1", "~Chunhua_Shen1", "~Jianfei_Cai1", "~Mingkui_Tan2"], "summary": "We present a novel super-bit model, a single-path method, to automatically search for optimal model compression configurations.", "abstract": "We present Automatic Bit Sharing (ABS) to automatically search for optimal model compression configurations (e.g., pruning ratio and bitwidth). Unlike previous works that consider model pruning and quantization separately, we seek to optimize them jointly. To deal with the resultant large designing space, we propose a novel super-bit model, a single-path method, to encode all candidate compression configurations, rather than maintaining separate paths for each configuration. Specifically, we first propose a novel decomposition of quantization that encapsulates all the candidate bitwidths in the search space. Starting from a low bitwidth, we sequentially consider higher bitwidths by recursively adding re-assignment offsets. We then introduce learnable binary gates to encode the choice of bitwidth, including filter-wise 0-bit for pruning. By jointly training the binary gates in conjunction with network parameters, the compression configurations of each layer can be automatically determined. Our ABS brings two benefits for model compression: 1) It avoids the combinatorially large design space, with a reduced number of trainable parameters and search costs. 2) It also averts directly fitting an extremely low bit quantizer to the data, hence greatly reducing the optimization difficulty due to the non-differentiable quantization. Experiments on CIFAR-100 and ImageNet show that our methods achieve significant computational cost reduction while preserving promising performance. ", "keywords": ["Quantization", "Pruning", "Model Compression", "AutoML"]}, "meta": {"decision": "Reject", "comment": "The paper proposes to integrate multiple bit configurations (including pruning) into a single architecture, and then automatically select bit resolution through binary gates. The overall approach can be differentiable and optimized with parameters. However, as pointed out by the reviewers, the novelty of this paper can be the big question. Also, it seems to be hard or unpractical to employ different number of bits for layers, given the standard GPU and CPU hardware. "}, "review": {"6hQ7hscFGUA": {"type": "rebuttal", "replyto": "zmSsNI1hbK", "comment": "Thanks for your constructive comments and suggestions.\n\n**Q3.1.** Discussions with concurrent work (van Baalen, 2020).\n\n**A3.1.** Our ABS and Bayesian Bits (van Baalen, 2020) are developed concurrently that share a similar idea of quantization decomposition. Critically, our ABS differs from Bayesian Bits in several aspects:\n\n1) The quantization decomposition in our methods can be extended to non-power-of-two bit widths (i.e., $b_1$ can be set to arbitrary appropriate integer values), which is a general case of the one in Bayesian Bits. \n2) The optimization problems are different. Specifically, we formulate model compression as a single-path subset selection problem while Bayesian Bits casts the optimization of the binary gates to a variational inference problem that requires more relaxations and hyperparameters.\n3) Our compressed models with less or comparable BOPs outperform those of Bayesian Bits by a large margin on ImageNet (See Table A). \n\nWe have included the discussions in Section 2 and the corresponding results in Table 2 of the revised paper.\n\nTable A: Comparisons of Bayesian Bits and ABS on ImageNet. \u201c*\u201d denotes that we get the results from the figures in (van Baalen, 2020).\n\n|      Network     |        Method      | Top-1 Acc.(%)  | BOPs(G) | \n|  :--------------:   | :-------------------: | :-----------------:  | :----------: |\n|                        |   Full-precision  |        70.7          |   1857.6   | \n|   ResNet-18    | Bayesian Bits*  |        69.5          |    35.9      |\n|                        |    ABS (Ours)    |        **70.8**          |     **32.3**     | \n|                        |   Full-precision  |        71.9           |    308.0   | \n|  MobileNetV2  | Bayesian Bits*  |        70.9           |     10.8    |\n|                        |    ABS (Ours)    |        **71.7**           |     **10.8**    | \n\n**Q3.2.** Does the cost $R$ consider both weight and activation bits? If yes, the input resolution change should affect BOPs and will change the optimal bit-width.\n\n**A3.2.** Yes, the computational costs $R(\\cdot)$ consider the bitwidths of both weights and activations. Specifically, we use Bit-Operation (BOP) count to measure the computational costs following (Guo et al., 2020; Ying et al., 2020). We agree that the change of input resolution will affect BOPs and change the optimal bitwidth. However, in our paper, we mainly focus on searching for optimal model compression configurations and only consider a fixed input resolution to exclude the effect of input resolution. We will incorporate the input resolution into the search space to investigate its effect in future work.\n\n**Q3.3.** The correlation between the bitwidth and pruning rate.\n\n**A3.3.** We have shown the correlation between the bitwidth and pruning rate in Figures 3 and 4. If a layer is set to a high pruning rate, our ABS tends to select a higher bitwidth to compensate for the performance drop. In contrast, if a layer is with a low pruning rate, our ABS tends to select a lower bitwidth to reduce the model size and computational costs. We have included the discussions in Section F.\n\n**Q3.4.** How is the \u2018search cost\u2019 calculated? Does 'search time\u2019 mean the fine-tuning process after training the uncompressed network? Is it equal to other works? (Table 7)\n\n**A3.4.** According to the definitions in (Stamoulis et al., 2019; Liu et al., 2019a), the standard search cost refers to the time of finding an optimal architecture or compressed model. Following these papers, we compute the search cost to represent the time of training both the model weights and compression configurations. Thus, the cost of the fine-tuning does not belong to the search cost. It is worth noting that we compute the search cost of all the methods in Table 7 in the same way.\n\n**Q3.5.** Results in terms of average bitwidth.\n\n**A3.5.** Thanks for your suggestion. We have provided the results of the average bitwidth in Tables 1 and 2 in the revised paper. \n\nWe have also summarized all of our updates in our general response. If there are any additional comments on the revised paper, please don\u2019t hesitate to let us know.\n", "title": "Our response to Reviewer #3"}, "llViRpE_lgm": {"type": "rebuttal", "replyto": "U_K4jyhHpK7", "comment": "**Q2.5.** Results in terms of memory footprints. \n\n**A2.5.** From Table C, ABS compressed ResNet-56 outperforms other methods with fewer memory footprints. These results show the effectiveness of our proposed ABS in terms of memory footprints reduction. According to your suggestions, we add a figure to show the compression capability of our method in terms of memory footprint in Figure 2. We have included the results and the corresponding discussions in Section 4.1.\n\nTable C: Comparisons of different methods w.r.t. memory footprint. We compress ResNet-56 using different methods and report the results on CIFAR-100.\n\n|       Method      | Memory footprint (KB) | M.f. comp. ratio | Top-1 Acc.(%) |  Top-5 Acc.(%)   |\n| :------------------: |  :---------------------------: |  :----------------:  | :------------------: |  :------------------: |\n|  Full-precision  |            5653.4              |          1.0          |        71.7           |         92.2           |\n|  4-bit precision |             711.7               |          7.9          |      70.9\u00b10.3      |        91.2\u00b10.4     |\n|       DNAS        |             708.9               |          8.0          |       71.5\u00b10.2     |        91.3\u00b10.1     |\n|        HAQ         |             700.0               |          8.1          |       71.3\u00b10.1     |        91.1\u00b10.1     |\n| ABS-Q (Ours)  |             674.5               |          8.4          |       71.5\u00b10.2     |        91.6\u00b10.2     |\n|   ABS (Ours)    |             **657.3**               |         **8.6**           |       **71.6\u00b10.1**     |        **91.8\u00b10.4**     |\n\n**Q2.6.** How can we optimize alpha^p and alpha^q? Is it sensitive to model accuracy? If the goal of this work is a fast exploration of compression configurations, optimizing alpha_p and alpha_q should be easy, but there is no relevant information in the manuscript (hence, Table 7 is not very reliable).\n\n**A2.6.** We need to clarify that $\\alpha^p$ and $\\alpha^q$ are two learnable parameters rather than the predefined hyperparameters. Thus, we can not directly investigate the sensitivity of these parameters. To better understand Eq. (13) and Eq. (14), we equivalently change the formulation by moving $\\alpha^q$ and $\\alpha^p$ from the right to the left of the inequality operator. Note that the indicator function is non-differentiable. To address this, we use the straight-through estimator (STE) (Bengio et al., 2013; Zhou et al., 2016) to estimate the gradients of the indicator function. Therefore, we are able to learn $\\alpha^p $ and $\\alpha^q$ in conjunction with other network parameters via stochastic gradient descent. Combined with the single-path scheme, our ABS shows promising search efficiency (See Table 7 in the revised paper). We have made it clearer in the revised paper.\n\nWe have also summarized all of our updates in our general response. If there are any additional comments on the revised paper, please don\u2019t hesitate to let us know.", "title": "Our response to Reviewer #2 (part 2) "}, "wIuO1pH5xMd": {"type": "rebuttal", "replyto": "-hrzH16qpf", "comment": "Thanks for your constructive comments and suggestions.\n\n**Q1.1.** Both channel pruning and residual quantization are existing ideas, this might reduce the novelty aspect of this paper.\n\n**A1.1.** The main novelty of this paper is to automatically search for optimal model compression configurations in a single-path framework rather than channel pruning or residual quantization. Existing compression methods (Wu et al., 2018; Dong & Yang, 2019) maintain separate paths for each configuration, which gives rise to a huge number of trainable parameters and high computational overhead. To address this, we present a super-bit that encodes all candidate configurations and introduce learnable binary gates to automatically determine the optimal configurations.  \n\nWe believe that our method provides a new perspective to automatic model compression. Also, the novelty of the trainable binary gates has been acknowledged by AR3 \u201cthe proposed super-bit use different gate function (forward step, backward sigmoid) and different loss function, so there is some novelty\u201d and AR2 \u201cthe idea of presenting trainable binary quantization gate signals to automatically assign the number of quantization bits to layers is interesting\u201d. \n\n**Q1.2.** Results on MobileNetV3.\n\n**A1.2.** Thanks for your suggestion. Due to the limited rebuttal period, results for MobileNetV3 on CIFAR-100 will be included before the end of the rebuttal. Results on ImageNet will be reported in the final version.\n\n**Q1.3.** Joint search for network architecture, pruning, and quantization.\n\n**A1.3.** Thanks for your suggestion. Our paper focuses on improving the search efficiency and the accuracy-vs-efficiency trade-off for automatic model compression rather than neural architecture design. Extension to architecture search is beyond the scope of this paper but indeed is a natural extension of our approach by directly introducing the architecture configurations (e.g., kernel size) into the search space. We have included it as a future work discussion in the conclusion.\n\n**Q1.4.** Suggestions on writing improvement.\n\n**A1.4.** Thanks for your suggestion. We have improved the mentioned Figure 1 and the corresponding descriptions in the revised paper. \n\n**Q1.5.** Confusion on 0-bit quantization.\n\n**A1.5.** Thanks for your suggestion. We use the 0-bit filter-wise quantization for pruning instead and have improved the corresponding descriptions in the revised paper.\n\n**Q1.6.** Performance improvement brought by alternative training.\n\n**A1.6.** As mentioned in Section 3.3, the gradient approximation of the binary gates inevitably introduces noisy signals, which incurs optimization difficulty during training. To solve this, we propose to train the binary gates of weights and activations in an alternative manner. From Table E, training in an alternative manner does improve performance. We have included these results in Section 4.2.    \n\nTable E: Effect of the alternative training scheme. We report the results of ResNet-56 on CIFAR-100.\n\n|       Network     |    Method   | Top-1 Acc.(%) | Top-5 Acc.(%)  | BOPs (M) |\n| :------------------: | :------------: | :-----------------: | :------------------: |  :-----------: |\n|                         |      Joint      |    71.3 \u00b1 0.2    |    91.6 \u00b1 0.3      |   1942.4   |\n|    ResNet-56    |  Alternative |    **71.6 \u00b1 0.1**    |   **91.8 \u00b1 0.4**      |   **1918.8**   |\n\n\n**Q1.7.** Choice of B (the number of filters in a group).\n\n**A1.7.** As mentioned in the implementation details in Section 4, for ResNet-20 and ResNet-56 on CIFAR-100, we set B to 4. For ResNet-18 and MobileNetV2 on ImageNet, B is set to 16 and 8, respectively. \n\n**Q1.8.** Minor issues.\n\n**A1.8.** Thanks for pointing out these issues. We have revised our paper according to your suggestions.\n\nWe have also summarized all of our updates in our general response. If there are any additional comments on the revised paper, please don\u2019t hesitate to let us know.\n", "title": "Our response to Reviewer #1"}, "i4hNJI7A3B7": {"type": "rebuttal", "replyto": "-hrzH16qpf", "comment": "**Q1.2.** Results on MobileNetV3.\n\n**A1.2.** Thanks for your suggestion. We apply our methods to MobileNetV3 on CIFAR-100. Following LSQ+ (Bhalgat et al., 2020), we introduce a learnable offset to handle the negative activations in hard-swish. We show the results in Table D. Due to the limited rebuttal period, results on ImageNet will be reported in the final version. From the results, even on compact MobileNetV3, our proposed ABS still outperforms other methods, which demonstrates its effectiveness. We have included the results and corresponding descriptions in Section G.\n\nTable D: Comparisons of different methods with MobileNetV3 on CIFAR-100.\n\n|       Method     | BOPs (M) | BOP comp. ratio | Top-1 Acc.(%) | Top-5 Acc.(%) |\n|:-----------------:|:----------:|:---------------------:|:-----------------:|:------------------:|\n|  Full-precision |  68170.1  |            1.0            |          76.1        |         93.9         |\n| 6-bit precision |   2412.6   |           28.3           |     76.1 \u00b1 0.0    |     93.7 \u00b1 0.0    |\n|          DQ         |   2136.3   |           31.9           |     75.9 \u00b1 0.1    |     93.7 \u00b1 0.1    |\n|         HAQ        |   2191.7   |           31.1          |      76.1 \u00b1 0.1    |     93.5 \u00b1 0.0   |\n|        DNAS       |  2051.9    |           33.2          |     76.1 \u00b1 0.1     |     93.7 \u00b1 0.1    |\n|  ABS-P (Ours)  | 59465.8  |            1.1           |     76.0 \u00b1 0.0     |     93.5 \u00b1 0.0    |\n|  ABS-Q (Ours) |   2021.9   |          33.7           |     76.1 \u00b1 0.1     |     93.7 \u00b1 0.1    |\n|    ABS (Ours)   |   **2006.6**   |          **34.0**           |     **76.1 \u00b1 0.1**     |     **93.7 \u00b1 0.1**    |\n", "title": "Results on MobileNetV3"}, "QpgLuS162qP": {"type": "rebuttal", "replyto": "QjINdYOfq0b", "comment": "We sincerely thank all reviewers for their valuable comments. We have revised our submission and summarized our updates as follows:\n\n1. We have improved the writing and updated Figure 1 according to the suggestions. \n2. We have added discussions with the concurrent work (van Baalen, 2020) in Section 2 and included comparison results in Table 2. \n3. We have provided the results in terms of average bitwidth in Tables 1 and 2. \n4. We have included more results with lower BOPs in Figure 2(a). \n5. We have included the results in terms of memory footprint in Figure 2(b). \n6. We have provided more results in terms of resource-constrained compression on the BitFusion platform in Section 4.2. \n7. We have included the results in terms of performance improvement brought by alternative training in Section 4.2. \n8. We have provided detailed quantization configurations of different methods in Section C. \n9. We have provided more analyses in terms of the relationship between bitwidth and pruning rate in Section F. \n10. We have included the results in terms of MobileNetV3 in Section G.\n\nIf there are any additional comments on the paper, please don\u2019t hesitate to let us know.\n", "title": "General Response"}, "u2jORPnFfz": {"type": "rebuttal", "replyto": "U_K4jyhHpK7", "comment": "Thanks for your constructive comments and suggestions.\n\n**Q2.1.** For Tables 1 and 2, are all methods assuming the same quantization configurations (e.g., layer-wise vs channel-wise and symmetric vs asymmetric, etc..)? \n\n**A2.1.** All the methods in Tables 1 and 2 use layer-wise and symmetric quantization schemes. Note that we obtain the results of all the compared methods using the quantization settings of their original papers. We have included the detailed quantization configurations for each method in Section C in the revised paper.\n\n**Q2.2.** For Tables 1 and 2, why top-1 accuracy numbers are very high for all methods? How can the low-precision models achieve near lossless or better performance compared with full-precision models?\n\n**A2.2.** Similar phenomenons can also be observed in the quantization literature, such as LSQ (Esser et al., 2020) and TET (Jin et al., 2019). For example, in LSQ, the 4-bit ResNet-18 even outperforms the full-precision one by 0.6% in the Top-1 accuracy. One possible reason is that model quantization acts as a regularizer to avoid the overfitting problem and improve the generalization ability of deep networks. Therefore, the low-precision models are able to achieve nearly lossless or better performance compared with the full-precision counterparts. \n\n**Q2.3.** Why not decrease BOPs while lowering top-1 accuracy for 'ABS (ours)' such that the gain of ABS is maximized?\n\n**A2.3.** Thanks for your suggestions. We have updated the results with different BOPs in Figure 2(a) in the revised paper. From the results, our ABS outperforms other methods by a large margin, especially at low BOPs settings. \n\n**Q2.4.** Marginal improvement in terms of BOPs and performance benefit on hardware.\n\n**A2.4.** Our proposed ABS achieves significant improvement in terms of BOPs, especially at low BOPs settings. For example, in Figure 2(a), our ABS compressed ResNet-56 model yields much fewer BOPs (395.25 vs. 536.24) but achieves comparable performance compared with the fixed-precision counterpart. Compared with low BOPs settings, the representational capability of the compressed models with high BOPs is higher. Hence, the improvement brought by our ABS will be smaller at high BOPs settings.\n\nTo further demonstrate the effectiveness of our ABS on hardware devices, we further apply our methods to compress MobileNetV2 under the resource constraints on the BitFusion architecture (Sharma et al., 2018). The results are shown in Table B. Compared with fixed-precision quantization, ABS achieves better performance with lower latency and energy. We have included the results and corresponding descriptions in Section 4.2.\n\nTable B: Resource-constrained compression on BitFusion. We evaluate the proposed ABS under the latency- and energy-constrained and report the Top-1 and Top-5 accuracy of MobileNetV2 on ImageNet.\n\n|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Method            |               | 6-bit precision |     ABS (Ours)     | \n|:-----------------------:|:-----------------------:|:----------------:|:-----------: |\n|                                  |  Top-1 Acc.(%)    |         71.8        |   **72.0**  |\n| Latency-constrained |  Top-5 Acc.(%)    |         90.3        |   **90.4**  | \n|                                  | Latency (ms)       |          24.9       |   **17.2**  |\n|                                  |  Top-1 Acc.(%)    |          71.8        |   **72.0**  |\n| Energy-constrained  |  Top-5 Acc.(%)    |          90.3        |   **90.3**  |\n|                                  |   Energy (mJ)      |         32.8         |   **26.3**  |\n", "title": "Our response to Reviewer #2 (part 1)"}, "-hrzH16qpf": {"type": "review", "replyto": "QjINdYOfq0b", "review": "Summary:\n\tThe paper presents a technique called ABS (Automatic Bit Sharing), that combines group-wise filter pruning with residual-aware quantisation in a NAS framework. The presented results on a range of image classification tasks show great performance when evaluating using an Accuracy/BOps metric. In general, I think the paper shipped interesting ideas in terms of automating residual quantisation, but it could be better if the network architecture space is jointly optimised.\n\n\nStrength:\n1. The paper is well-written (although it has some minor flaws), but in general easy to follow.\n2. I like the formulation of using thresholds to automatically decide numerical representations, this looks neat and novel to me.\n3. It is the first time for me to see residual quantisation in a NAS framework and the formulation of the indicator function looks interesting.\n4. The composed search space gives a reduction in search time.\n\nWeakness:\n1. Both channel pruning and residual quantisation are existing ideas, this might reduce the novelty aspect of this paper.\n2. This work focuses on searching for compression on pretrained architectures, such as ResNets and MobileNets. However, these models are not state-of-the-art on the considered datasets, and one might argue that these models are inherently more redundant than models like MobileNetV3. Also, the authors do not consider jointly optimise the network architectures, which could naturally be an extension of this work and might help to further boost the accuracy. \n\nMy suggestions & confusions:\n1. I stumbled with the number representation until finishing reading section 3.1. It is probably worth mentioning that you are applying fixed-point quantisation to residual terms at the very beginning of the paper, especially in figure 1.\n2. The argument that you\u2019ve introduced 0-bit quantisation for pruning is very misleading. Apparently your pruning is channel-wise but quantisation is layer-wise, it is naturally to consider you used 0-bit quantisation on the entire layer, but this is not the case.\n3. In Page 6, you mentioned you train the binary gates for weights and activations alternatively and this provides better performance. Could you quantify the performance gap? Also, if one simply looks at eq(17), this interleaved optimisation is not captured in this over-simplified optimisation target. \n4. Third line of page 6, I guess you mean group-wise sparsity?\n5. Page 6: Specifically, if a group of filters are ... -> if a group of filters is \u2026?\n6. You mentioned you are using group-wise sparsity, do you show the number of groups anywhere? What is your choice of B in Eq(16) for all the experiments? Or you simply mean you are doing channel-wise sparisity? I am a little confused here.\n\n", "title": "A good paper but maybe lacks novelty", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "U_K4jyhHpK7": {"type": "review", "replyto": "QjINdYOfq0b", "review": "Inspired by gradient-based NAS of single-path formulation, the authors propose a super-bit model, a single-path method, to decide the optimal number of quantization bits and pruning of a group of filters. While it can be a time-consuming process to study the impact of quantization of certain filters (or layers) on model accuracy, the proposed scheme finds a particular compression configuration in a trainable manner. The experimental results show that the proposed method presents higher model accuracy or lower computational cost (measured as the bit-operation count).\n\nWhile the idea of presenting trainable binary quantization gate signals to automatically assign the number of quantization bits to layers is interesting, this reviewer has the following concerns:\n\n- For Table 1 and 2, are all methods assuming the same quantization configurations (e.g., layer-wise vs channel-wise and symmetric vs asymmatric, etc..)? Detailed quantization configuration should be included for fair comparisons.\n\n- For Table 1 and 2, why top-1 accuracy numbers are very high for all methods? How 6-bit precision of MobileNetV2 can be 71.8% (almost no accuracy degradation)? Moreover, the accuracy of the 4-bit-quantized ResNet-18 on ImageNet is even higher than that of full-precision ResNet-18. Even so, why not decrease BOPs while lowering top-1 accuracy for 'ABS (ours)' such that the gain of ABS is maximized?\n\n- BOPs look quite similar for many configurations (including the second row of a fixed number of quantization bits). Even though this reviewer does not believe BOPs can show performance benefits in hardware well, this reviewer cannot see distinguished advantages of ABS in Table 1 and Table 2. The gain on BOPs of ABS seems to be marginal.\n\n- The results do not describe memory footprint savings. Can ABS provide a reduced memory footprint compared to previous ones for a target model accuracy? In Figure 2, if BOPs can be replaced with memory footprints, it would be useful to estimate the compression capability of the proposed method. In Figure 3 and 4, comparisons are missing.\n\n- How can we optimize alpha_p and alpha_q? Is it sensitive to model accuracy? If the goal of this work is a fast exploration of compression configurations, optimizing alpha_p and alpha_q should be easy, but there is no relevant information in the manuscript (hence, Table 5 is not very reliable).\n\nOverall, unfortunately, this reviewer cannot easily find the advantages of the proposed methods in the experiments. ", "title": "Official review comments #2", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "zmSsNI1hbK": {"type": "review", "replyto": "QjINdYOfq0b", "review": "The paper describes the method to determine optimal quantization bit-width and pruning configuration for the neural network compression. Different from other approaches, the proposed method integrates multiple bit configurations (including pruning) into a single architecture, which is named \u201cSuper-bit\u201d. The architecture uses binary gates to automatically select bit resolution. In addition, the super-bit model is differentiable and jointly trainable with parameters.\n\nThe idea of decomposing bit configuration into 2, 4, 8 bits, and using binary gates does not seems to be novel [van Baalen, 2020]. This paper also adopts the trainable gate parameter and unifies the pruning scheme.\n\nCompared to [van Baalen, 2020], the proposed super-bit use different gate function (forward step, backward sigmoid) and different loss function, so there is some novelty. However, there is no direct comparison of the algorithm. To be fair, [van Baalen, 2020] is quite recent work and both ideas may be developed concurrently. So, I think inserting a single paragraph of comparison would be enough.\n\nUsing cost function R is a good idea to directly reduce the computation cost. Experiments, especially ABS-P and ABS-Q are well designed and suitable.\n\nSome minor questions/suggestions:\n1)\tDoes the cost R consider both weight and activation bits? If yes, the input resolution change should affect BOPs and will change the optimal bit-width. It would be meaningful to show the correlation between optimal bit-width and the number of filters (but not necessary).\n2)\tHow is the \u2018search cost\u2019 calculated? As mentioned, the key point of the algorithm is to simultaneously train weights and configurations. Is \u2018search time\u2019 means the fine-tuning process after training the uncompressed network? Is it equal to other works? (Table 5)\n3)\tIn Figure 3 & 4, there is no layer assigned to 2-bit. Can you provide average bit-width for ABS networks?\n", "title": "Good work, but the novelty might be questionable", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}