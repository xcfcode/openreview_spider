{"paper": {"title": "Cross-Entropy Loss Leads To Poor Margins", "authors": ["Kamil Nar", "Orhan Ocal", "S. Shankar Sastry", "Kannan Ramchandran"], "authorids": ["nar@berkeley.edu", "ocal@eecs.berkeley.edu", "sastry@eecs.berkeley.edu", "kannanr@eecs.berkeley.edu"], "summary": "We show that minimizing the cross-entropy loss by using a gradient method could lead to a very poor margin if the features of the dataset lie on a low-dimensional subspace.", "abstract": "Neural networks could misclassify inputs that are slightly different from their training data, which indicates a small margin between their decision boundaries and the training dataset. In this work, we study the binary classification of linearly separable datasets and show that linear classifiers could also have decision boundaries that lie close to their training dataset if cross-entropy loss is used for training. In particular, we show that if the features of the training dataset lie in a low-dimensional affine subspace and the cross-entropy loss is minimized by using a gradient method, the margin between the training points and the decision boundary could be much smaller than the optimal value. This result is contrary to the conclusions of recent related works such as (Soudry et al., 2018), and we identify the reason for this contradiction. In order to improve the margin, we introduce differential training, which is a training paradigm that uses a loss function defined on pairs of points from each class. We show that the decision boundary of a linear classifier trained with differential training indeed achieves the maximum margin. The results reveal the use of cross-entropy loss as one of the hidden culprits of adversarial examples and introduces a new direction to make neural networks robust against them.", "keywords": ["Cross-entropy loss", "Binary classification", "Low-rank features", "Adversarial examples", "Differential training"]}, "meta": {"decision": "Reject", "comment": "The paper challenges claims about cross-entropy loss attaining max margin when applied to linear classifier and linearly separable data. This is important in moving forward with the development of better loss functions. \n\nThe main criticism of the paper is that the results are incremental and can be easily obtained from previous work. \n\nThe authors expressed certain concerns about the reviewing process. In the interest of dissipating any doubts, we collected two additional referee reports. \n\nAlthough one referee is positive about the paper, four other referees agree that the paper is not strong enough. \n\n\n\n"}, "review": {"rklW51EUy4": {"type": "rebuttal", "replyto": "BkgLGizLJV", "comment": "We repeatedly and clearly stated in our response to Reviewer 2 and Reviewer 3: Our most critical results are Theorem 3, Theorem 4 and Remark 3. Anyone who has read the list of our contributions on page 2 would not miss this. Anyone who has read the discussion section would understand that Theorem 3 is our most critical result -- just like Reviewer 1 did. \n\nWe understand from the review that Reviewer 5 was able to see the previous reviews and our responses. Given this fact, Reviewer 5 must have seen in our responses that our most critical results are Theorem 3, Theorem 4 and Remark 3. Therefore, it seems extremely absurd that Reviewer 5 tried to summarize our contributions in two points and not mention any of our most critical results. As a result, we strongly question the objectivity and the fairness of their evaluation.\n\nOur paper is the first work that finds a connection between the existence of adversarial examples and the specific choice of training loss function (cross-entropy with soft-max) and the low dimensionality of the features of the training dataset. Anyone who thinks this result is insignificant should reconsider their level of expertise in the field and possibly give themselves confidence 1 or 2 -- not 5.\n\nWe were very careful in our choice of words when making statements about what is correct and what is not correct in (Soudry et al., 2018). We stated **their conclusion was incorrect** due to neglecting the bias term; we did not say their proof was incorrect. Nevertheless, we appreciate the great effort Reviewer 5 did in praising the work (Soudry et al., 2018) and trying to remove the taint we could potentially bring to it while writing a review for our paper.", "title": "Main results are Theorem 3-4 and Remark 3: They are completely, and probably intentionally, ignored"}, "S1l0f9m8JE": {"type": "review", "replyto": "ByfbnsA9Km", "review": "This paper presents a very specialized example to show that gradient descent on the cross-entropy loss WITHOUT REGULARIZATION leads to poor margin, which is very unrealistic. Moreover, I have the following concerns:\n\n1. In the two points classification example shown in Section 2, I want to see the plot of iteration versus cross-entropy loss during the gradient descent.\n\n2. Whether it makes sense to use cross-entropy loss to quantify loss for two-class classification problem with one point in each class? Statistically, it seems not reasonable at all.\n\n3. In Corollary 1, the authors made a further assumption, x^Ty=1, which is very unnatural.\n\n4. In the numerical results section, I want to see some results on some benchmark dataset. The presented numerical results are too weak to support the proposed differential training.", "title": "I do not think the proposed approach can be better than the cross-entropy loss in practice.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkgLGizLJV": {"type": "review", "replyto": "ByfbnsA9Km", "review": "Due to the large variance in reviewer scores, I was asked to give this additional review.\n\nBackground: [Soudry et al. 2018] showed that the iterates of gradient descent, when optimizing logistic regression on separable data, converge to the L2 max-margin (SVM) solution for homogeneous linear separators (without bias). These results were later extended to other models and optimization methods.\n\nThis paper has two main results:\n1)\tIt clarifies that the results of [Soudry et al. 2018] do not apply to logistic regression when the linear separator has a bias term (\u201cb\u201d). This is because the homogenous max margin solution in the extended [x,1] space is not the same as non-homogeneous max margin solution in the original space: the first has a penalty on the size of the bias term, i.e.\nmin_{w,b} ||w||^2 + b^2 s.t. y_n (w\u2019x_n+b) >= 1\n, while the latter does not: \nmin_{w,b} ||w||^2  s.t. y_n(w\u2019x_n+b) >= 1\n2)\tIt suggests using differential training to correct this issue.\n\n\nHowever, I do not believe these contributions are enough for a publication in ICLR. First, (2) is simply a combination of two known results, as mentioned by Reviewer 2. Second, though I commend the authors for pointing out (1), I do not feel this by itself warrants a publication, for the following reasons:\na) It is very simple to explain (1) in only a few lines (as I did above). Therefore, it would be more informative just to write (1) as a comment on the original paper (the ICLR 2018 forum is still open), not as a completely new publication. For me, all the numerical demonstrations and examples of this simple issue did not add much.\nb)\tRegularizing the bias term usually does not make a significant difference to the sample complexity (see the end of section 15.1.1 in the textbook \u201cUnderstanding Machine Learning: From Theory to Algorithms\u201d by Shai Shalev Shwartz.). Furthermore, the main motivation behind [Soudry et al. 2018] was to explain implicit bias and generalization in deep networks, where there such max-margin results (which penalize all the parameters) could be used to derive generalization bounds (e.g., https://arxiv.org/abs/1810.05369).\nc)\tLastly, the authors here say that \u201cthe solution obtained by cross-entropy minimization is different from the SVM solution\u201d. This (as well as the title and abstract) may mislead the readers to think there is something wrong in the proofs of [Soudry et al. 2018] and later papers, and that logistic regression does not converge to the max-margin solution for homogeneous linear separators. However, the max-margin solution for homogeneous linear separators is also called the \u201cmax margin\u201d or SVM solution (just for a different family). For example, see the previous paper on the topic [\u201cMargin Maximizing Loss Functions\u201d, Rosset et al. 2004] or section 15.1.1 in the textbook \u201cUnderstanding Machine Learning: From Theory to Algorithms\u201d by Shai Shalev Shwartz.  As I see it, the only issue in [Soudry et al. 2018] is the sentence \u201cA bias term could be added in the usual way, extending x_n by an additional \u20191\u2019 component.\" which is confusing since it cannot be applied directly to the SVM solution. The authors should aim to pinpoint this issue, and clarify their phrasing to avoid such confusions.\n\n", "title": "Insufficient novelty and significance. Also, the phrasing of the results is somewhat misleading.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJeCZjJ52m": {"type": "review", "replyto": "ByfbnsA9Km", "review": "Summary: \nThis paper investigates the properties of minimizing cross-entropy of linear functions over separable data (looks like logistic loss). The authors show a simple example where the minimizer of the cross-entropy loss leads to maximum margin hyperplane where the bias term is regarded as an extra dimension, which is different from the standard max. margin solution of  SVMs with bias not regarded as an extra dimension. The authors then propose a method to obtain the latter solution by minimizing the cross-entropy loss.\n\n\nComments:\n\nThere is a previously known result quite related to this paper: \n\nIshibashi, Hatano and Takeda: Online Learning of Approximate Maximum p-Norm Margin Classifiers with Bias, COLT2008. \n\nTheorem 2 of Ishibashi et al. shows that the hard margin optimization with linear classifier with bias is equivalent to those without bias over pairs of positive and negative instances. \n\nCombined with Theorem 3 of (Soudry et al., 2018)), I am afraid that the main result Theorem 5 can be readily derived. \n\nFor this reason, I am afraid that the main technical result is quite weak.\n\nAfter Rebuttal:\nI read the authors' comments. I understand more the technical contribution of the paper and raised my score. But I also agree with Reviewer 3.\n", "title": "The technical results can be obtained by a simple combination of previous work.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ByxobfCsT7": {"type": "rebuttal", "replyto": "S1l15DRl67", "comment": "1) We changed the titles of Section 2 and Section 3 to reflect their importance.\n\n2) We added citations to (Ishibashi et al., 2008) and one of its references, (Keerthi et al., 2000), in the first paragraph of Section 4.", "title": "Paper has been updated"}, "S1l15DRl67": {"type": "rebuttal", "replyto": "rJeCZjJ52m", "comment": "Dear Reviewer 2,\n\nThank you for your review, and thanks for pointing out this reference. We were not aware of this past work, and it certainly deserves a reference.\n\nNevertheless, our main technical result is Theorem 3 and Remark 3 -- not Theorem 5. As the title of our submission reflects, and as the list of our contributions on page 2 describes, differential training is not the heart of our work. As we stated in our response to Reviewer 1, differential training was introduced in this paper only to open a door for further research and not to finish this paper with a negative result. \n\nPlease note that Theorem 3 and Theorem 4, along with the related remarks, are original. We would appreciate if you have any suggestions to further highlight that Section 3 is the critical part of our work.", "title": "Response to Reviewer 2: Main result is not Theorem 5"}, "HJgZ7PCga7": {"type": "rebuttal", "replyto": "rJegWQkshX", "comment": "Dear Reviewer 3,\n\nThanks for reviewing our paper.\n\n1a) The goal of our submission is not to make further positive claims about the use of cross-entropy minimization; it is the opposite. As Reviewer 1 also stated, we wanted to challenge the faith of the community in the use of cross-entropy loss, and we wanted to show that minimizing this loss function on low-dimensional datasets such as images can lead to extremely poor margins. For this reason, the title of our submission is very accurate. We updated Figure 1 to highlight the drastic difference between the SVM solution and the solution obtained by the cross-entropy minimization.\n\n1b) As we clearly stated in Remark 1, normalizing a dataset in the input space does not correspond to normalizing the features of the points if the feature mapping is nonlinear. In particular, we will not have normalized features if we use neural networks. If we want to get a right intuition about the effect of cross-entropy minimization on neural network training, we cannot simply assume the features of the training points will be normalized. This is why we strictly avoid the assumption of a normalized dataset, as explained in Remark 1.\n\n2a) It is unfortunate, and somewhat curious, that our results in Section 3 (Theorem 3 and the remarks following it) were completely neglected. Section 3 clarifies why the conclusions of the works [1,2,3,4,5] are erroneous and shows that the reality is drastically different from their conclusions. Showing that there was a critical error in a line of previous works, which leads to a drastic change in the conclusion, is not an \"incremental contribution\". In fact, given [1] appeared in ICLR last year, it is essential that the ICLR community be given the correction this year.\n\n2b) Theorem 3 and Remark 3 are the most critical results of our paper. Please make sure you have understood them. The last paragraph of Section 5 verifies that the assumptions of Theorem 3, the low-dimensionality of the features, indeed arises in practice. In other words, the assumptions of Theorem 3 are not an edge case, and the conclusion of Theorem 3 has critical implications for practice.\n\n3) Our paper starts with the question \"Is cross-entropy loss really the right cost function to use with gradient descent algorithm?\". We use linear classifier and linearly separable dataset to answer this question on a simple setting. By doing so, our work gives intuition that the cross-entropy loss function has responsibility in the poor margin of the decision boundaries. We introduce differential training as a method to improve the margin **while still using gradient descent algorithm**. As we stated in the Discussion section, this allows the feature mapping to remain trainable while ensuring a large margin, and therefore, it provides an initial attempt to combine the benefits of neural networks and the SVM. And please note that when [1,2,3,4,5] claimed that cross-entropy loss finds the same solution with the SVM, they did not suggest that the ML community stop using cross-entropy minimization and replace it with SVM.\n\n[1] Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable data. In International Conference on Learning Representations, 2018.\n[2] D. Soudry, E. Hoffer, M. Shpigel Nacson, S. Gunasekar, and N. Srebro. The Implicit Bias of Gradient Descent on Separable Data. ArXiv e-prints, 2018.\n[3] M. Shpigel Nacson, J. Lee, S. Gunasekar, P. H. P. Savarese, N. Srebro, and D. Soudry. Convergence of Gradient Descent on Separable Data. ArXiv e-prints, 2018a.\n[4] M. Shpigel Nacson, N. Srebro, and D. Soudry. Stochastic Gradient Descent on Separable Data: Exact Convergence with a Fixed Learning Rate. ArXiv e-prints, 2018b.\n[5] Ziwei Ji and Matus Telgarsky. Risk and parameter convergence of logistic regression. CoRR, abs/1803.07300, 2018.", "title": "Response to Reviewer 3"}, "BylxpBRx6Q": {"type": "rebuttal", "replyto": "ryx8tGiohX", "comment": "Dear Reviewer 1,\n\nThank you for reading our submission closely, and thanks for appreciating our results.\n\nAs you have also noticed, Section 3 of our paper, and Theorem 3 in particular, is the punch line of our work. The algorithm, differential training, was introduced in this paper only to open a door for further research and not to finish this paper with a negative result. That is, we wanted to show that there could be a solution for the problem we have identified. We agree that further study of differential training for neural networks is necessary and important, and that is our ongoing work.", "title": "Response to Reviewer 1"}, "ryx8tGiohX": {"type": "review", "replyto": "ByfbnsA9Km", "review": "The paper challenges recent claims about cross-entropy loss attaining max margin when applied to linear classifier and linearly separable data. Along the road, it presents a couple of nice results that I find quite interesting and I believe they provide useful insights. Finally it presents a simple modification to the cross-entropy loss, which the authors refer to as differential training, that alleviates the problem for the case of linear model and linearly separable data.\n\nCONS:\nI find the paper useful and interesting mainly because of its insightful results rather than the final algorithm. The algorithm is evaluated in a very limited setting (linear model, synthetic data, binary classification); it is not clear if similar benefits would carry over to nonlinear models such as deep networks. In fact, I strongly encourage the authors to do a generalization comparison by comparing the **test accuracy** obtained by their modified cross-entropy against: 1. Vanilla cross-entropy as well as 2. A deep model large margin loss function (e.g. as in \"Large Margin Deep Networks for Classification\" by Elsayed). Of course on a realistic architecture and non-synthetic datasets (e.g. CIFAR-10).\n\nPROS:\nPutting the algorithm aside, I find the theorems interesting. In particular, Theorem 3 shows that some earlier claims about cross-entropy's ability to attain large margin (in the linearly separable case) is misleading (due to neglecting a bias term). This is important as it changes the faith of the community in cross-entropy and more importantly creates hope for constructing new loss functions with improved margin.\nI also find the connection between the dimension of the subspace that contains the points and quality of margin obtained by cross-entropy insightful.\n", "title": "A set of nice results that is insightful and clarifies some controversy ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rJegWQkshX": {"type": "review", "replyto": "ByfbnsA9Km", "review": "This paper studies the cross-entropy loss for binary classification problems. The authors show that if the norms of samples in two linear separable classes are different, gradient descent based methods minimizing cross-entropy loss may give a linear classifier that gives small margin.\n\nPros\n\n1. The paper is clearly written and very easy to follow. \n\n2. The authors show that for two point classification problems, if the norms of the points are very different then gradient descent will give a very small margin.\n\n3. Further theoretical results are given explaining the relation between cross-entropy loss and SVM.\n\n4. A new loss function called differential training is proposed, which is guaranteed to give SVM solution.\n\nCons\n\n1. My biggest concern is that, the paper, especially the title, may be slightly misleading in my opinion. Although the authors keep claiming that cross-entropy loss can lead to poor margins in certain circumstances (which I agree), in fact Theorem 1 and Theorem 2 have already clearly shown the connection between the cross-entropy solution and the maximum margin direction. For example, Theorem 1 literally proves that when the two points have the same norm (normalized data?), cross-entropy loss leads to maximum margin. Theorem 2 also clearly states that cross-entropy loss and SVM are closely related. Based on these two theorems, perhaps \u2018cross-entropy loss is closely related to maximum margin\u2019 is a more convincing statement.\n\n2. The theoretical results given in this paper is slightly incremental. As the authors mentioned, Theorem 1 and Theorem 2 are essentially already proved in previous works. The other results are not very significant either.\n\n3. The authors do not clearly state the advantages of the differential training method compared to SVM. It seems that one can just use SVM if the goal is maximum margin classifier.\n", "title": "interesting work, but slightly incremental", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}