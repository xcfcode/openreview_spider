{"paper": {"title": "Free Lunch for Few-shot Learning:  Distribution Calibration", "authors": ["Shuo Yang", "Lu Liu", "Min Xu"], "authorids": ["~Shuo_Yang5", "~Lu_Liu7", "~Min_Xu5"], "summary": "The code is available at: https://github.com/ShuoYang-1998/Few_Shot_Distribution_Calibration", "abstract": "Learning from a limited number of samples is challenging since the learned model can easily become overfitted based on the biased distribution formed by only a few training examples. In this paper, we calibrate the distribution of these few-sample classes by transferring statistics from the classes with sufficient examples. Then an adequate number of examples can be sampled from the calibrated distribution to expand the inputs to the classifier. We assume every dimension in the feature representation follows a Gaussian distribution so that the mean and the variance of the distribution can borrow from that of similar classes whose statistics are better estimated with an adequate number of samples. Our method can be built on top of off-the-shelf pretrained feature extractors and classification models without extra parameters. We show that a simple logistic regression classifier trained using the features sampled from our calibrated distribution can outperform the state-of-the-art accuracy on three datasets (~5% improvement on miniImageNet compared to the next best). The visualization of these generated features demonstrates that our calibrated distribution is an accurate estimation. ", "keywords": ["few-shot learning", "image classification", "distribution estimation"]}, "meta": {"decision": "Accept (Oral)", "comment": "This paper proposes a novel and powerful data augmentation strategy for few-shot learning, producing convincing improvements over current approaches. The request by the reviewers to include additional ablations, more backbones, and an additional dataset have been satisfactorily  resolved, with the results remaining strong. The reviewers are all unanimous in their recommendation that the paper be accepted for publication.\n"}, "review": {"zuXW6WDf8ia": {"type": "review", "replyto": "JWOiYxMG92s", "review": "Summary\n\nThe paper proposes a method to calibrate the underlying distribution of a few samples in the few-shot classification scenario. The idea is to estimate a feature distribution of a few samples of a novel class from base class distributions. The authors assume that every dimension in the feature vector follows a Gaussian distribution. Based on the observation that the mean and variance of the distribution with respect to each class are correlated to the semantic similarity of each class, base class distribution can be transferred to the novel class distribution. After distribution calibration, features can be generated from the calibrated distribution and the generated features are used to train classifiers. SVM and logistic regression classifier are used to verify the approach on the mini-imagenet and CUB datasets.\n\nPros\n-   The idea can be applied to any types of feature extractors or classifiers when the Gaussian distribution assumption holds.\n-   The proposed approach shows competitive performance on two benchmarks, mini-imagenet and CUB.\n-   Extensive ablation studies verify hyper-parameter settings and their sensitivities.\n\nCons\n- Many hyper-parameters are involved in the approach. ( Turkey\u2019s transformation parameter lambda, top-k base distributions, dispersion parameter alpha, number of generated features)\n-   Some hyper-parameter setting should be different depending on the dataset. (dispersion parameter alpha)\n-   The paper uses the previous work (Mangla et al. 2020) as a baseline and applies their approach on top of it. While theoretically the approach can be applied to any types of feature extractors, only one baseline improvement is shown in the paper.\n\n\nRating\n\nI give marginally below the threshold. The paper shows good performance and also claims the approach can be applied on top of any classifiers or feature extractor. The general applicability and effectiveness are the strength of the paper. However, the proposed approach is only verified on one baseline approach (Mangla et al. 2020). More empirical data would strengthen the claim of the paper.\n\nQuestions\n\nThe approach outputs calibrated distribution, i.e., a mixture of Gaussian distributions. These distributions can be directly used to classify query samples by calculating the likelihood of the sample on each distribution. How does likelihood classification perform compared to the retrained classifiers (SVM or RL)?\nThe optimal values for top-k parameter and alpha parameter are different depending on the dataset. Are optimal values for lambda and the number of generated features same or different depending on the dataset? Did the authors use the same lambda and number of generated features for both mini-imagenet and CUB experiments?\nThe authors only applied the proposed method on one baseline approach. Is the approach effective for more variety of backbone networks and losses? \n\nFeedback\n\n- Table1 shows that there is a correlation between feature distributions and semantic similarities. It would be interesting to see how the distribution calibration performs on each class depending on the similarity level of top-k base distributions.\n- The empirical evidence is not sufficient to claim the approach applies to general network architectures and few-shot learning approaches. Experiments with more baseline approach to show the effectiveness of the idea is recommended.\n", "title": "Marginally below threshold.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "4adsJMz8hA2": {"type": "rebuttal", "replyto": "JWOiYxMG92s", "comment": "We really appreciate all three reviewers for their valuable comments.  \nWe\u2019ve uploaded a revised draft incorporating reviewer feedback. Below is a summary of the main changes:\n\n- The experiments on various backbones are included in Table 5. (R3 & R4)\n- The experiments on various baseline models are included in Table 6. (R4)\n- We also set new state-of-the-art performance on a larger dataset, tieredImageNet, in Table 3. (R3)\n- We clarify the values of hyper-parameters on all three datasets in Section 4.1.3, and update the Figure 5 to provide more analysis. (R4)\n- We visualize the feature distribution of randomly selected base classes and novel classes, as well as the distribution of novel classes after Tukey\u2019s transformation in Figure 6 (in Appendix). (R1)\n\nWe really hope our responses and revisions address all reviewers\u2019 concerns!", "title": "Summary of Revisions"}, "5JluQatbhby": {"type": "rebuttal", "replyto": "pW5UgHEp4JU", "comment": "**Q3: How does likelihood classification perform compared to the retrained classifiers (SVM or LR)?**\n\nA3: \nThanks for this comment! We have included the result using maximum likelihood in Table2 and Table 3 as below:\n\n| Methods  &nbsp;|&nbsp; miniImageNet 1-shot&nbsp;|&nbsp; tieredImageNet 1-shot|\n|:--|:--:|:--:|\n|Maximum Likelihood with DC&nbsp;|&nbsp; 66.91 $\\pm$ 0.17&nbsp;|&nbsp; 75.92 $\\pm$ 0.60|\n|SVM with DC&nbsp;|&nbsp; 67.31 $\\pm$ 0.83&nbsp;|&nbsp; 77.93 $\\pm$ 0.12|\n|Logistic Regression with DC&nbsp;|&nbsp; **68.57** $\\pm$ **0.55**&nbsp;|&nbsp; **78.19** $\\pm$ **0.25**|\n\nWe found it can achieve competitive performance while training a SVM / LR classifier using the samples from the calibrated distribution can further improve the performance.\n\n**Q4: Table 1 shows that there is a correlation between feature distributions and semantic similarities. It would be interesting to see how the distribution calibration performs on each class depending on the similarity level of top-k base distributions.**\n\nA4: \nThanks for this comment!\nWe found that the higher similarities between the retrieved base class distribution and the novel class ground-truth distribution, the higher the performance improvement our method will bring as shown in Table 9:\n\n|  Novel class         &nbsp;|&nbsp;   Top-1 base class similarity &nbsp;|&nbsp;Top-2 base class similarity   &nbsp;|&nbsp; DC improvement |\n|:----|:----:|:----:|:----:|\n| malamute &nbsp;|&nbsp; 93\\% &nbsp;|&nbsp; 85\\% &nbsp;|&nbsp; \u219121.30\\% acc|\n| golden retriever    &nbsp;|&nbsp;   85\\% &nbsp;|&nbsp; 74\\% &nbsp;|&nbsp; \u219118.37\\% acc  |\n| ant| 71\\% &nbsp;|&nbsp; 67\\% &nbsp;|&nbsp;\u21919.77\\% acc|\n\nWe really hope our response addresses your concern. If you have any other questions, we are very happy to continue discussions!\n\n[A] Vinyals et al., Matching Networks for One Shot Learning, NeurlPS 2016\n[B] Snell et al., Prototypical Networks for Few-shot Learning, NeurlPS 2017\n[C] Rusu et al., Meta-Learning with Latent Embedding Optimization, ICLR 2019\n[D] Liu et al., An Ensemble of Epoch-wise Empirical Bayes for Few-shot Learning, ECCV 2020\n[E] Chen et al., A Closer Look at Few-shot Classification, ICLR 2019\n", "title": "Response to AnonReviewer4 (2/2)"}, "pW5UgHEp4JU": {"type": "rebuttal", "replyto": "zuXW6WDf8ia", "comment": "We appreciate the thorough review of R4. We found your main concerns are the applicability of our model and clarifications on hyper-parameters. Please find our responses below:\n\n**Q1: The paper uses the previous work (Mangla et al. 2020) as a baseline and applies their approach on top of it. While theoretically the approach can be applied to any types of feature extractors, only one baseline improvement is shown in the paper.**\n\nA1:\nThanks for this suggestion! We have included more experiment results to show our model's applicability and robustness from three aspects: Distribution Calibration (DC) on different backbones, baselines, and datasets.\n1. DC on different backbones.\nWe show DC can significantly and consistently improve the performance when applying different backbone structures, as shown in Table 5:\n\n| Backbones  &nbsp;|&nbsp; without DC &nbsp;|&nbsp; with DC |\n|:--|:--:|:--:|\n| conv4 &nbsp;|&nbsp; 42.11 $\\pm$ 0.71 &nbsp;|&nbsp; **54.62** $\\pm$ **0.64** (\u2191 **12.51**)|\n| conv6 &nbsp;|&nbsp; 46.07 $\\pm$ 0.26 &nbsp;|&nbsp; **57.14** $\\pm$ **0.45**(\u2191 **11.07**)|\n| resnet18 &nbsp;|&nbsp; 52.32 $\\pm$ 0.82 &nbsp;|&nbsp;**61.50** $\\pm$ **0.47** (\u2191 **9.18**)|\n| WRN28 &nbsp;|&nbsp; 54.53 $\\pm$ 0.56 &nbsp;|&nbsp;**64.38** $\\pm$ **0.63** (\u2191 **9.85**)|\n| WRN28+Rotation Loss &nbsp;|&nbsp; 56.37 $\\pm$ 0.68 &nbsp;|&nbsp;**68.57** $\\pm$ **0.55** (\u2191 **12.20**)|\n\n2. DC on different baselines.\nWe show DC can also improve over previous few-shot learning baselines when training using the sampled features from DC as in Table 6:\n\n| Methods &nbsp;|&nbsp; without DC  &nbsp;|&nbsp; with DC |\n|:--|:--:|:--:|\n| Baseline[E] &nbsp;|&nbsp; 42.11 $\\pm$ 0.71 &nbsp;|&nbsp; **54.62** $\\pm$ **0.64** (\u2191 **12.51**)|\n| Baseline++[E]&nbsp;|&nbsp;48.24 $\\pm$ 0.75 &nbsp;|&nbsp; **61.24** $\\pm$ **0.37** (\u2191 **13.00**)|\n\nAlso, DC is applicable to different traditional machine learning classifiers, i.e., maximum likelihood classifier, SVM and Logistic Regression as shown in Table 2 and Table 3:\n\n| Methods  &nbsp;|&nbsp; miniImageNet 1-shot&nbsp;|&nbsp; tieredImageNet 1-shot|\n|:--|:--:|:--:|\n| E3BM[D] \\(previous sota\\) &nbsp;|&nbsp; 63.80 $\\pm$ 0.40 &nbsp;|&nbsp; 71.2 $\\pm$ 0.40 |\n|Maximum Likelihood with DC&nbsp;|&nbsp; 66.91 $\\pm$ 0.17&nbsp;|&nbsp; 75.92 $\\pm$ 0.60|\n|SVM with DC&nbsp;|&nbsp; 67.31 $\\pm$ 0.83&nbsp;|&nbsp; 77.93 $\\pm$ 0.12|\n|Logistic Regression with DC&nbsp;|&nbsp; **68.57** $\\pm$ **0.55**&nbsp;|&nbsp; **78.19** $\\pm$ **0.25**|\n\n3. DC on a more large-scaled dataset.\n\nBesides miniImageNet and CUB, we also benchmark on tieredImageNet, as in Table 3. We also set the new state-of-the-art on tieredImageNet.\n\n| Methods &nbsp;|&nbsp; 5way1shot &nbsp;|&nbsp; 5way5shot|\n|:--|:--:|:--:|\n| Matching Net[A] &nbsp;|&nbsp; 68.50 $\\pm$ 0.92 &nbsp;|&nbsp; 80.60 $\\pm$ 0.71|\n|Prototypical Net[B]&nbsp;|&nbsp;65.65 $\\pm$ 0.92 &nbsp;|&nbsp;83.40 $\\pm$ 0.65|\n|LEO[C] &nbsp;|&nbsp;66.33 $\\pm$ 0.05&nbsp;|&nbsp; 82.06 $\\pm$ 0.08|\n|E3BM[D]&nbsp;|&nbsp; 71.20 $\\pm$ 0.40&nbsp;|&nbsp; 85.30 $\\pm$ 0.30|\n|Distribition Calibration (Ours)&nbsp;|&nbsp; **78.19** $\\pm$ **0.25** &nbsp;|&nbsp;**89.90** $\\pm$ **0.41**|\n\n**Q2: Many hyper-parameters are involved in the approach. (Turkey's transformation parameter lambda, top-k base distributions, dispersion parameter alpha, number of generated features). Some hyper-parameter settings should be different depending on the dataset. (dispersion parameter alpha)**\n\nA2:\nThanks for this comment! As shown in Figure 3, Figure 4 and Figure 5, as well as Section 4.5 and 4.6, we have thoroughly analyzed how these four hyperparameters will affect the performance. In sum, we found the selection of these hyperparameters are not dataset-dependent (we use the same value for three datasets), except for the dispersion parameter alpha. This is because different datasets have different Gaussian distribution of their features and setting different dispersions can better approximate the ground truth distribution. \nWe have added related clarifications in Section 4.1.3 in our updated version of the submission! The values of all hyper-parameters on three datasets are as below:\n\n| Hyper-parameter &nbsp;|&nbsp; miniImageNet &nbsp;|&nbsp; tieredImageNet &nbsp;|&nbsp; CUB|\n|:--|:--:|:--:|:--:|\n| $k$ &nbsp;|&nbsp; 2 &nbsp;|&nbsp; 2&nbsp;|&nbsp; 2|\n| $\\lambda$&nbsp;|&nbsp;0.5&nbsp;|&nbsp;0.5&nbsp;|&nbsp;0.5|\n|number of generated features&nbsp;|&nbsp;750&nbsp;|&nbsp;750&nbsp;|&nbsp;750|\n|$\\alpha$&nbsp;|&nbsp;0.21&nbsp;|&nbsp;0.21&nbsp;|&nbsp;0.3|\n\n", "title": "Response to AnonReviewer4 (1/2)"}, "2_UN_ouU5CG": {"type": "rebuttal", "replyto": "6oQZy4VR38", "comment": "\nThanks for taking the time to review our paper! We are happy to respond to your comments and questions as below:\n\n**Q1: One of the claims of the paper is that the proposed algorithm is pre-trained feature extractor agnostic. However, there are no experiments to validate this claim. Consider adding feature extractors trained in different ways.**\n\nA1: \nThanks for your comment! We have updated the paper and add the results on various backbones as Table 5. We show significant and consistent improvement on various backbones structures. A summary of the results are as below:\n\n| Backbones  &nbsp;|&nbsp; without DC &nbsp;|&nbsp; with DC |\n|:--|:--:|:--:|\n| conv4 &nbsp;|&nbsp; 42.11 $\\pm$ 0.71 &nbsp;|&nbsp; **54.62** $\\pm$ **0.64** (\u2191 **12.51**)|\n| conv6 &nbsp;|&nbsp; 46.07 $\\pm$ 0.26 &nbsp;|&nbsp; **57.14** $\\pm$ **0.45**(\u2191 **11.07**)|\n| resnet18 &nbsp;|&nbsp; 52.32 $\\pm$ 0.82 &nbsp;|&nbsp;**61.50** $\\pm$ **0.47** (\u2191 **9.18**)|\n| WRN28 &nbsp;|&nbsp; 54.53 $\\pm$ 0.56 &nbsp;|&nbsp;**64.38** $\\pm$ **0.63** (\u2191 **9.85**)|\n| WRN28+Rotation Loss &nbsp;|&nbsp; 56.37 $\\pm$ 0.68 &nbsp;|&nbsp;**68.57** $\\pm$ **0.55** (\u2191 **12.20**)|\n\n**Q2: Some of the popular few-shot datasets were not included in the experimental section, namely Tiered-ImageNet [1] and Meta-Dataset [2]. The former is a larger dataset than the ones used, and the latter provides cross-domain results for the approach. The proposed algorithm assumes that statistics from the meta-train classes would transfer to the few-shot classes, which would be tested more thoroughly in the cross-domain setting.**\n\nA2: \nThanks for this suggestion! We have included the result on tieredImageNet as Table 3 . We are happy to see our method can also set new state-of-the-art results on tieredImageNet! Here is a summary of the comparison:\n\n| Methods &nbsp;|&nbsp; 5way1shot &nbsp;|&nbsp; 5way5shot|\n|:--|:--:|:--:|\n| Matching Net[A] &nbsp;|&nbsp; 68.50 $\\pm$ 0.92 &nbsp;|&nbsp; 80.60 $\\pm$ 0.71|\n|Prototypical Net[B]&nbsp;|&nbsp;65.65 $\\pm$ 0.92 &nbsp;|&nbsp;83.40 $\\pm$ 0.65|\n|LEO[C] &nbsp;|&nbsp;66.33 $\\pm$ 0.05&nbsp;|&nbsp; 82.06 $\\pm$ 0.08|\n|E3BM[D]&nbsp;|&nbsp; 71.20 $\\pm$ 0.40&nbsp;|&nbsp; 85.30 $\\pm$ 0.30|\n|Distribition Calibration (Ours)&nbsp;|&nbsp; **78.19** $\\pm$ **0.25** &nbsp;|&nbsp;**89.90** $\\pm$ **0.41**|\n\n\n \n**Q3: In the feature space, it is intuitive that the means of similar classes would be correlated. Is there a justification for why this is true for the variances?**\n\nA3: \nWe empirically found this is also true for the variance. This is also found in [E], in which a hierarchical Bayesian model is learned to build a prior over category means and variance. Please refer to Figure 3 in their paper for a display of mean and variance.\n\n**Q4: Tukey's Ladder of Powers transformation is used only on the few-shot samples and not the meta-train samples. Is there a reason for that? If the pre-trained feature extractor is trained using a certain metric (cosine distances for example), I would imagine transforming all the features would be beneficial.**\n\nA4: \nThanks for pointing it out! We have tried applying the transformation on both meta-trained samples and samples from test tasks. We found the performance is slightly lower as below:\n\n|  &nbsp;|&nbsp; miniImageNet 5way1shot |\n|--|--|\n| Transform meta-train and novel features &nbsp;|&nbsp;  67.34 $\\pm$ 0.73  |\n| Transform novel features&nbsp;|&nbsp; **68.57** $\\pm$ **0.55** |\n \nA possible reason is that the meta-trained samples have already been in the shape of Gaussian due to sufficient training samples as shown in the left of Figure 6  (in Appendix) in our updated paper. Thus, transformation on the meta-trained samples is not necessary.\n\n**Q5: The backbone used in the experiments is trained using a supervised and self-supervised loss. What are the results without the self-supervised loss?**\n\nA5: \nWe show the performance improvement in both cases below: \n\n| Backbones  &nbsp;|&nbsp; without DC &nbsp;|&nbsp; with DC |\n|:--|:--:|:--:|\n| without self-supervised loss &nbsp;|&nbsp; 54.53 $\\pm$ 0.56 &nbsp;|&nbsp;**64.38** $\\pm$ **0.63** |\n| with self-supervised loss &nbsp;|&nbsp; 56.37 $\\pm$ 0.68 &nbsp;|&nbsp;**68.57** $\\pm$ **0.55**|\n\n**Q6: How many samples are drawn from the calibrated distribution for the numbers in Table 2?**\n\nA6:\n 750 samples per class. Thanks for this comment, and we have included it in Section 4.1.3.\n\n We really hope our response addresses your concern. If you have any other questions, we are very happy to continue discussions!\n\n\n[A] Vinyals et al., Matching Networks for One Shot Learning, NeurlPS 2016\n[B] Snell et al., Prototypical Networks for Few-shot Learning, NeurlPS 2017\n[C] Rusu et al., Meta-Learning with Latent Embedding Optimization, ICLR 2019\n[D] Liu et al., An Ensemble of Epoch-wise Empirical Bayes for Few-shot Learning, ECCV 2020\n[E] Salakhutdinov et al., One-Shot Learning with a Hierarchical Nonparametric Bayesian Model, ICML Workshop 2012", "title": "Response to AnonReviewer3"}, "1dxkkh6xBWA": {"type": "rebuttal", "replyto": "DD7-TM9kZb", "comment": "\n\n\nThanks for taking the time to review our paper! We are happy to respond to your comments and questions as below:\n\n**Q1:Looking at Figure 4, the 1-shot accuracy with only 1 retrieved class is already very strong. Instead of sampling from the \"calibrated\" distribution, can we simply retrieve examples from the nearest class? Namely, find the nearest class, randomly sample some examples, and use their features to augment the novel classes. This ablation should make clear what additional value the sampling procedure adds.**\n\nA1:  \nThanks for this comment! We have included the comparison of training using samples from the calibrated distribution, different number of retrieved features from nearest class, and only using support set in Table 7 (in Appendix) as shown below: \n\n| Training data &nbsp;|&nbsp; accuracy on miniImageNet 5way1shot |\n|:--|:--:|\n| Support set only &nbsp;|&nbsp; 56.37 $\\pm$ 0.68 |\n| Support set + 1 feature from the nearest class&nbsp;|&nbsp;62.39 $\\pm$ 0.49|\n| Support set + 5 features from the nearest class&nbsp;|&nbsp;59.73 $\\pm$ 0.42|\n| Support set + 10 features from the nearest class&nbsp;|&nbsp;58.93 $\\pm$ 0.49|\n| Support set + 100 features from the nearest class&nbsp;|&nbsp;57.33 $\\pm$ 0.48|\n| Suppport set + 100 features sampled from calibrated distribution (ours)&nbsp;|&nbsp;**68.53** $\\pm$ **0.32**|\n\n\nWe empirically found the retrieved features can improve the performance compared to only using the support set but can damage the performance when increasing the number of retrieved features, where the retrieved samples probably serve as noisy data for tasks targeting different classes.\n\n\n**Q2:In equation (6), how important is it to include the novel class feature into the mean? Can we simply do $\\mu_{prime} = \\sum_{i \\in S_N} \\mu_i / k$?**\n\nA2:\nThanks for this comment! We have added Table 8 (in Appendix) as below:\n\n|  &nbsp;|&nbsp; miniImageNet 5way1shot |\n|:----|:----:|\n|Distribution calibration w/o novel feature &nbsp;|&nbsp; 59.38 $\\pm$ 0.73|\n| Distribution calibration w/ novel feature &nbsp;|&nbsp; **68.57** $\\pm$ **0.55** |\n\nWe found the support set is essential for performance when calibrating the distribution.\n\n**Q3:This method makes an important assumption that the feature distribution is gaussian. How well does this assumption hold in practice? Can the authors provide some analysis of the feature distribution? Ideally both before and after Tukey's Ladder of Powers transformation.**\n\nA3: \nThanks for this suggestion! We have included the feature distribution of 5 randomly sampled base classes and the feature distribution of 5 randomly sampled novel classes before/after Tukey's transformation in Figure 6 (in Appendix).\nThe base classes' feature distributions satisfy the Gaussian assumption well (left) while the feature distributions of novel classes are more skewed (middle). After transformation (right), the calibrated novel class distributions are more aligned with the Gaussian-like base feature distribution.\n\nWe hope our responses address your concerns and happy to continue the discussion if there are any other questions!\n", "title": "Response to AnonReviewer1"}, "6oQZy4VR38": {"type": "review", "replyto": "JWOiYxMG92s", "review": "Summary:\n\nThis paper identifies the problem of biased distributions in few-shot learning and proposes to fix it. In few-shot learning, only a few samples per class are available; this makes estimating the class distribution difficult. The paper proposes a distribution calibration algorithm that makes use of the meta-train class distributions to calibrate the few-shot class distributions. Once calibrated, more samples are drawn from this distribution to learn a classifier that generalizes better. This approach does not require additional learnable parameters and can be (potentially) built on-top of any pre-trained feature extractor. Empirical results show that this approach achieves state-of-the-art results on Mini-ImageNet and CUB.\n\nPros:\n1. This paper identifies and tries to tackle an important problem in few-shot learning - estimation of the class distribution. Due to the limited number of samples, this problem is difficult, but important for few-shot learning. The proposed algorithm is simple and effective in tacking this problem.\n2. As opposed to other related works, the proposed algorithm does not have any learnable parameters. It makes use of the features obtained for the meta-train and few-shot samples.\n\nCons:\n1. One of the claims of the paper is that the proposed algorithm is pre-trained feature extractor agnostic. However, there are no experiments to validate this claim. Consider adding feature extractors trained in different ways.\n2. Some of the popular few-shot datasets were not included in the experimental section, namely Tiered-ImageNet [1] and Meta-Dataset [2]. The former is a larger dataset than the ones used, and the latter provides cross-domain results for the approach. The proposed algorithm assumes that statistics from the meta-train classes would transfer to the few-shot classes, which would be tested more thoroughly in the cross-domain setting.\n\nClarifications:\n1. In the feature space, it is intuitive that the means of similar classes would be correlated. Is there a justification for why this is true for the variances?\n2. Tukey's Ladder of Powers transformation is used only on the few-shot samples and not the meta-train samples. Is there a reason for that? If the pre-trained feature extractor is trained using a certain metric (cosine distances for example), I would imagine transforming all the features would be beneficial.\n3. The backbone used in the experiments is trained using a supervised and self-supervised loss. What are the results without the self-supervised loss?\n4. How many samples are drawn from the calibrated distribution for the numbers in Table 2?\n\nNotes:\n1. The performance of few-shot learning algorithms has been traditionally evaluated using the averaged accuracy over multiple tasks, but that is not the only way to do it. Look at [3] for details.\n\n[1] Mengye Ren et al. Meta-Learning for Semi-Supervised Few-Shot Classification.\n[2] Eleni Triantafillou et al. Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples.\n[3] Guneet S. Dhillon et al. A Baseline for Few-Shot Image Classification.", "title": "Simple and effective method for calibrating the few-shot class distribution", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "DD7-TM9kZb": {"type": "review", "replyto": "JWOiYxMG92s", "review": "This paper presents a simple and intuitive data augmentation method for few shot image classification. The proposed method assumes the feature distribution of a single class to be gaussian. Based on this assumption, the proposed method samples features from a gaussian distribution that is constructed using the statistics of the similar base classes. Combined with logistic regression, the proposed method achieves strong performance on two standard few-shot image classification benchmarks. This submission also carries out an ablation study on several design choices, which I really appreciate.\n\nThe submission is well-written and clear. The proposed method is novel and can inspire future augmentation-based methods in few-shot image classification.\n\nI have a few more requests/ablations that I am curious to see. \n\n- Looking at Figure 4, the 1-shot accuracy with only 1 retrieved class is already very strong. Instead of sampling from the \"calibrated\" distribution, can we simply retrieve examples from the nearest class? Namely, find the nearest class, randomly sample some examples, and use their features to augment the novel classes. This ablation should make clear what additional value the sampling procedure adds.\n- In equation (6), how important is it to include the novel class feature into the mean? Can we simply do \\mu_prime = \\sum_{i \\in S_N} \\mu_i /  k?\n- This method makes an important assumption that the feature distribution is gaussian. How well does this assumption hold in practice? Can the authors provide some analysis of the feature distribution? Ideally both before and after Tukey\u2019s Ladder of Powers transformation. \n\n", "title": "A simple data augmentation for few-shot image classification", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}