{"paper": {"title": "Boosting Network: Learn by Growing Filters and Layers via SplitLBI", "authors": ["Zuyuan Zhong", "Chen Liu", "Yanwei Fu", "Yuan Yao"], "authorids": ["zyzhong19@fudan.edu.cn", "corwinliu9669@gmail.com", "yanweifu@fudan.edu.cn", "yuany@ust.hk"], "summary": "", "abstract": "Network structures are important to learning good representations of many tasks in computer vision and machine learning communities. These structures are either manually designed, or searched by Neural Architecture Search (NAS) in previous works, which however requires either expert-level efforts, or prohibitive computational cost. In practice, it is desirable to efficiently and simultaneously learn both the structures and parameters of a network from arbitrary classes with budgeted computational cost. We identify it as a new learning paradigm -- Boosting Network, where one starts from simple models, delving into complex trained models progressively.\nIn this paper, by virtue of an iterative sparse regularization path -- Split Linearized Bregman Iteration (SplitLBI), we propose a simple yet effective boosting network method that can simultaneously grow and train a network by progressively adding both convolutional filters and layers. Extensive experiments with VGG and ResNets validate the effectiveness of our proposed algorithms.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper considers how to learn the structure of deep network by beginning with a simple network and then progressively adding layers and filters as needed. The paper received three reviews by expert working in this area. R1 recommends Weak Reject due to concerns about novelty, degree of contribution, clarity of technical exposition, and experiments. R2 recommends Weak Accept and has some specific suggestions and questions. R3 recommends Weak Reject, also citing concerns with experiments and writing. The authors submitted a response that addressed many of these comments, but R1 and R3 continue to have concerns about contribution and the experiments, while R2 maintains their Weak Accept rating. Given the split decision, the AC also read the paper. While we believe the paper has significant merit, we agree with R1 and R3 on the need for additional experimentation, and believe another round of peer review would help clarify the writing and contribution. We hope the reviewer comments will hep authors prepare a revision for a future venue."}, "review": {"S1x5dCfciS": {"type": "rebuttal", "replyto": "SylwBpNKDr", "comment": "We thank all the reviewers for their insightful and constructive comments. We have substantially revised the paper as suggested by the reviewers, and summarize the major changes as follows:\n\n1. In Introduction, we explicitly highlight our contributions, and explain that (1) we for the first time, define the task of boosting network, and present the algorithms of boosting network. In particular,  our algorithm is built upon the existing deep network optimizer -- SplitLBI, \n\n2. In methodology,  we rewrote and clearly gave more mathematical definition, and intuitions about the background of SplitLBI in Sec. 3.1, as suggested. Specifically, the proximal map, and loss functions of SplitLBI have been more clearly defined. \n\n3. In section 3.2, we give the intuition about Eq (8) (old Eq (4)): \n    \u201cThis above equation means $(W^{t}$ is projected on $\\Gamma^{t}$, and the selected subset  $\\widetilde{W}^{t}$ include the parameters existed in both $W^{t}$ and $\\Gamma^{t}$. \u201c\n\n4. As suggested by reviewers, we add some explanations to Autogrow: \u201cAutogrow is one of the most efficiency methods in growing networks. Specifically, Autogrow can grow layers from a seed network, but their approach does not explore the filter configuration of each block. If compared aganist our GT-layers Alg, the results networks have much deeper with a large number of parameters; and thus their growing process is less efficiency than ours.\u201d\n\n5. We also update the experimental PlainNet results in Tab. 6, by using 4 blocks as the initialization, as suggested.\n", "title": "Summary of Changes"}, "H1enMAfcoS": {"type": "rebuttal", "replyto": "rkxUaz-AFH", "comment": "Very thanks very much for the suggestions and comments. We answer the specific questions below,\n\nQ1 : The proposed BoN is built upon the existing SplitLBI algorithm that can identify the sparse approximation of the weight structure. The contributions are mainly from incorporating the adaptive criteria for adding filters and layers. Thus contribution is incremental and novelty is limited.\nA1 : Thanks. Actually not. Despite our algorithm is built upon SplitLBI, which is one of most recent optimizers, the main focus of this paper is about boosting network, i.e., efficiently growing filters and layers of network. The goal of our paper is boosting networks, and we propose new algorithm to reveal our goal by using Split LBI as a tool. The original SplitLBI is an optimization algorithm and the usage of \\Gamma is for sparsity. \n\nWe summarize the contributions: (1) A novel learning paradigm - Boosting net- work (BoN), is for the first time, studied in this paper; and one starts from simple models, delving into complex trained models progressively. (2) We propose a novel GT-filters Alg, which can si- multaneously effectively grow the filters of each layer, and train the network parameters, given an initial network. (3) We present GT-layers Alg which grows and trains layers, by exploring the over- parameterized model weight and structural sparsity model weight set. \n \n\nQ2 : The projection operator in equation (4) is a key step of the boosting procedure, but is not clearly defined and explained.\nA2 :  Thanks. We add some explanations. \n\u201cThis above equation means $(W^{t}$ is projected on  $\\Gamma^{t}$, and \nthe selected subset\n $\\widetilde{W}^{t}$ include the parameters existed in both $W^{t}$ and  $\\Gamma^{t}$. \u201c\n\nQ3 : For experiments, results shows good results on simple baselines, more complicated or large-scale datasets should be included for evaluations. Also, only Autogrow is compared to the proposed method, to make the results more convincing, other architecture searching approaches (e.g. NAS) should be added to comparisons.\n\nA3: Thanks. But in our paper, we demand the following properties of an algorithm qualified as BoN:\n(1)It should incorporate both architecture growth (including filters and layers) and parameter learning simultaneously, in which the width and depth of network can be gradually updated, and the parameters of network should be updated at the same time; \n(2)It should provide a comparable classifier for prediction tasks, as the state-of-the-art hand- crafted architectures on the same dataset; \n(3)Its computational requirements, the total parameters of final boosted network, and memory footprint should remain bounded, ideally in the same order of magnitude as training a manually engineered architecture on the same dataset. \n\nThe first two criteria express the essence of boosting network; the third criterion identifies the key difference from NAS and other trivial or brute-force solutions, such as randomly searching. \nAs the field of representation learning moves closer towards artificial intelligence, it becomes important to efficiently and simultaneously learn both the structures and parameters of a network from arbitrary classes on mobile devices or even Internet of Things (IoT) devices. \nTo sum up, the goal of our work is boosting networks in an efficient way, which is quite different from NAS, which is searching for good structure. It\u2019s very hard for our algorithm to make a direct and far comparison to NAS, in term of GPU computational cost, and total parameters of final network structures.   \n \n\nQ4 : For the GT-layers Alg, would it more efficient to boost layer first before boosting the filters?\nA4 : Thanks. Thanks for this point. The GT-Filters Alg in GT-Layers algorithm aims at finding the optimal configuration of the filter number.  Generally and practically, it is a good practice to use the same number of filters in each convolutional layer, as has done in many previous manually designed architectures. On the other hand, some of our pilot experiments suggest that, if alternatively growing filters and layers, it may greatly improve the computational cost, but do not incur significant performance improvement.  Thus, the algorithm is designed as boosting filters, and then boosting layers.\n\n\nQ5: Will GT-layers be robust to the case when the number of filters is overly specified?\nA5: Thanks.  If the number of filters overly specified, the model may be also over-parameterized. \n", "title": "For Reviewer #1"}, "rkeT26zcoB": {"type": "rebuttal", "replyto": "HygAtPPZ5S", "comment": "Thanks very much for the suggestions and comments. We answer the specific questions below,\n\nQ1: The GT-Layers algorithm can be better motivated. Why is GT-Filters only run once at the beginning, rather than iteratively as the number of layers increases?\nA1 : Thanks for this point. The GT-Filters Alg in GT-Layers algorithm aims at finding the optimal configuration of the filter number.  Generally and practically, it is a good practice to use the same number of filters in each convolutional layer, as has done in many previous manually designed architectures. On the other hand, some of our pilot experiments suggest that, if alternatively growing filters and layers, it may greatly improve the computational cost, but do not incur significant performance improvement.\n\n\nQ2 : Why does the procedure go from bottom blocks to up blocks and, by the way, what are bottom blocks and up blocks?\nA2 : Following the data streaming from input to output, we boosting the layers from bottom blocks to up blocks as in Fig. 1(b). \nThis idea is rooted in Deep Belief Network, which utilized unsupervised pre-training layer-by-layer, given the input data streaming.\n\nQ3: Why measure training accuracy to determine when to add layers?\nA3 : Thanks. Yes, in our Boosting algorithms,  one starts from simple models, delving into complex trained models progressively. If the training accuracy is not high, it indicates that the network doesnot have enough capacity (not over-parameterized) to learn the training data. Thus it is a good measure.\n\n\nQ4 : Can a similar analysis be made for wall clock time, i.e., how long the models actually take to train? A similar study (FLOPs, wall clock time, etc.) would also be very useful for the current comparison to Autogrow, as these metrics are often just as important, if not more important, than the number of parameters of the final model.\n\nA4: The running time given in the paper of Autogrow is the experiments on ImageNet, in order to compare the running time on CIFAR10, we run the source code of Autogrow released by the author and running on our single TITAN X (Pascal) GPU. Results are: \n\nAutogrow on Cifar10 (using ResNet block): \nmain_gradual.py :  ACC- 93.75%; running-time-9.74h;\nmain_add.py:         ACC- 92.57%; running-time-7.36h \n\nOur method on Cifar10: \nResNet:     ACC-94.60% ; time-4.0h;\n(PlainNet: ACC-94.65%; time-3.0h)\n", "title": "For Reviewer #2"}, "rklaQTf5oS": {"type": "rebuttal", "replyto": "HklWUCF79H", "comment": "Thank you very much for your comments.\n\nQ1: The method is not clearly explained and rigorously formulated.\n\n(Q1-1) what's \u0393? Is it a copy of W or not? What's the exact mathematical function of loss L() w.r.t. W and \u0393? How does the neural architecture change after adding \u0393?\n(A-1): Yes, we had clearly defined \\Gamma in Eq (3). \\Gamma is the structural sparsity parameter to explore important subnetwork architectures by inverse scale space where those important parameters become nonzero faster than others. Mathematically, in Eq (3), \\Gamma learned to approximate W but not a copy of W. Thus the \\Gamma is initialized as the same dimension as W, while most values of \\Gamma should be zeros (sparse). Here W is the model parameters of deep networks.\n \nWe update the loss function w.r.t W and \u0393 in the revised version\n\n\n(Q1-2) why \u0393 can be approximated by gradients in Eq. (3)...\n(A-2). Thanks. We update this point, and give more detailed mathematical definition, and intuition in the revised\n\n\n(Q-3) why \u0393 is necessary? How does it compare with enforcing group Lasso on W directly, like what was done in Nonparametric Neural Networks [1]?\n(A-3). To efficiently boosting the network, we need the merit of learning both an over-parameterized model weight set (Over-Par set) as the Stochastic Gradient Descent (SGD), and structural sparsity model weight set (Stru-Spa set) in a coupled inverse scale space. Thus the weight parameter W is for Over-Par set, and \u0393 is for structural sparsity set. Thus it is necessary to introduce \u0393.\n\nOne can enforce group lasso on W or as Nonparametric Neural Networks [1], but we still need the structural sparsity set \u0393, to help measure whether the filters or layers should be boosted. In that sense, the structural sparsity set selected by  Nonparametric Neural Networks, can also potentially be utilized as an alternative to \u0393; and we take it as a future work.\n\n\nQ2: Improvement on experiments.\n\n(Q2-1) Include the learned width in Table 1.\n(A2-1): Thanks. The learned width is actually shown in Tab 1. Particularly, in Tab 1 of our unrevised paper, the \u201cUP-Net\u201d column describes our learned width of each network, for example, 8@3*3 means kernel size is 3 with 8 channels we learned. We highlight this point in the revised version.\n\n(Q2-2) the pairs of PlainNet is hard to judge because different neural architectures are used. AutoGrow uses 4 blocks while this paper uses 5 blocks. ...\n(A2-2). Thanks. For fair comparison we add one experiment on 4-blocks plainnet.  It shows that our method has much less parameters while still keeps almost same performance.\n\n4-Block-PlainNet(J=30,\\epsilon=0.3,\\tau=0.5):\nCIFAR10:  ACC-94.30%; Layer-16; Param-2.577M;\nCIFAR100:  ACC-75.35%; Layer-15; Param-13.332 M;\n\nCompared with 5-Block-PlainNet, which has the results as \nCIFAR10:  ACC-94.65\uff1b Layer-19; Param-7.70M;\nCIFAR100:  ACC-75.66\uff1b Layer-18; Param-29.83M;\n\n\n(Q2-3) In the experiments of layer growing, please clarify if filter growth is also applied or not.\n(A2-3). In the experiments of layer growing, we do use filter growth, as explicitly explained in Sec. 3.3 The detailed procedure is : 1). Initialize the base network. 2). Grow filters to determine appropriate channels of each block(simply, layers between 2 pooling layers in PlainNet called a block; and layers having same output size in ResNet called a block), i.e., \u201clearning the filter configuration of each conv of each block;\u201d  3). After getting number of channels, we begin to grow layer.  We further highlight this point.\n\n(Q2-4) clarify \"their growing process is not efficient.\" AutoGrow is very efficiency ...\n(A2-4).  Yes, indeed, AutoGrow is one of the most efficiency methods for the proposed BoN task; but it is less efficient than ours. we elaborate it in the revised paper,\n\nWe also empirically compare the running time: \nWe run the source code released by the author with their default setting, and running on our single TITAN X (Pascal) GPU.\nAutogrow on Cifar10 (using ResNet block): \nmain_gradual.py : ACC- 93.75%; running-time-9.74h;\nmain_add.py: ACC- 92.57%; running-time-7.36h \nOur method on Cifar10: \nResNet: ACC-94.60% ; time-4.0h;\n(PlainNet: ACC-94.65%; time-3.0h)\n\nQ3: Some minor issues:\n\n(Q3-1) networks with \"20 filters\" and \"100 neurons\" are used as the seeds. How critical are they?\n(A3-1).For the initial number of units in linear layer, we have used other trial such as 50, 200, 1000 neurons, the final results do not have significant difference: the accuracy difference is <0.02%. And for the initial number of filters, we also tried 8, 10, 16 filters, the results also do not have significant difference. This is to say, the initial seed is not so critical, so we stick to 20 filters, and 100 neurons in our experiments for convenience. \n\n(Q3-2) \"To the best of our knowledge, .. over-claimed.\n(A3-2).Thank. We revised the claim as suggested.\n\n\n[1] Huang et al., Split LBI: An iterative regularization path with structural sparsity. NeurIPS 2016.\n", "title": "For reviewer #3"}, "rkxUaz-AFH": {"type": "review", "replyto": "SylwBpNKDr", "review": "Summary:\nThis paper focuses on topic of searching for the optimal architecture for the deep network. Building on the split linearized bregman iteration strategy, the authors propose two practical algorithms to boost network, namely GT-filters Alg and GT-layers Alg. The proposed algorithms can simultaneously grow and train a network by progressively adding both convolutional filters and layers. The experiments conducted on VGG and ResNets display the comparable accuracies between the BoN and the standard big models, but with much more compact representations and balanced computational cost.\n\nStrengths:\n1 The authors introduce two simple but practical algorithms for augmenting the architectures of deep network. The quite promising results are achieved on baselines, w.r.t. the balance of prediction performance and model complexity with the budgeted computational resources.\n2 The paper is clearly written and easy to follow. \n\nWeaknesses:\n1 The proposed BoN is built upon the existing SplitLBI algorithm that can identify the sparse approximation of the weight structure. The contributions are mainly from incorporating the adaptive criteria for adding filters and layers. Thus contribution is incremental and novelty is limited. \n2 The projection operator in equation (4) is a key step of the boosting procedure, but is not clearly defined and explained.\n3 For experiments, results shows good results on simple baselines, more complicated or large-scale datasets should be included for evaluations. Also, only Autogrow is compared to the proposed method, to make the results more convincing, other architecture searching approaches (e.g. NAS) should be added to comparisons.\n\nOther questions:\n1 For the GT-layers Alg, would it more efficient to boost layer first before boosting the filters? \n2 Will GT-layers be robust to the case when the number of filters is overly specified?", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}, "HygAtPPZ5S": {"type": "review", "replyto": "SylwBpNKDr", "review": "This paper proposes an architecture search method for deep convolutional neural network models that progressively increases the number of filters per layer as well as the number of layers, and the authors refer to this general approach as boosting networks. The algorithm for increasing the number of filters is based on split linear Bregman iteration, and the algorithm for increasing the number of layers proceeds block by block, increasing the layers per block until the accuracy does not increase. The experiments convincingly demonstrate gains in performance and smaller network sizes compared to baseline models, naive boosting methods, and a related method called Autogrow.\n\nIn my view, there are two main areas for further improvement for this work. First, the GT-Layers algorithm can be better motivated. Why is GT-Filters only run once at the beginning, rather than iteratively as the number of layers increases? Why does the procedure go from bottom blocks to up blocks (and, by the way, what are bottom blocks and up blocks)? Why measure training accuracy to determine when to add layers? I understand that these are all fairly heuristic choices, but nevertheless there needs to be proper motivation for all of the above.\n\nSecond, an additional effort should be made to compare to additional prior work in architecture search. It seems like the authors are suggesting that this method should be more computationally efficient and find smaller architectures, and demonstrating this empirically would greatly strengthen the paper. In particular, the existing results depicting the final number of parameters in the learned model are particularly striking to me. I appreciate that the authors included an experiment showing that a standard ResNet cannot be trained with the same number of FLOPs as the network found by your method. Can a similar analysis be made for wall clock time, i.e., how long the models actually take to train? A similar study (FLOPs, wall clock time, etc.) would also be very useful for the current comparison to Autogrow, as these metrics are often just as important, if not more important, than the number of parameters of the final model.\n\nA thorough pass through the paper for spelling and grammar would be very useful.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 1}, "HklWUCF79H": {"type": "review", "replyto": "SylwBpNKDr", "review": "This paper studies a very interesting topic: automatically grow filters and layers in neural networks and find an \"optimal\" width and depth for neural networks. The method is motivated by SPLITLBI, and its effectiveness is verified by experiments and comparison with AutoGrow. I tend to accept this paper, before the following questions can be answered:\n1. I guess the major issue in this paper is that the method is not clearly explained and rigorously formulated, although it's an extension of SPLITLBI.\n-- 1.1. what's \u0393? Is it a copy of W or not? What's the exact mathematical function of loss L() w.r.t. W and \u0393? How does the neural architecture change after adding \u0393?\n-- 1.2. why \u0393 can be approximated by gradients in Eq. (3)? What's the intuition behind?\n-- 1.3. why \u0393 is necessary? How does it compare with enforcing group Lasso on W directly, like what was done in Nonparametric Neural Networks [1]?\nWithout clarifying those, people can hardly learn from and use this paper.\n\n2. Experiments\n-- 2.1. Include the learned width in Table 1.\n-- 2.2. In comparison with AutoGrow, the pairs of ResNet is fair, but the pairs of PlainNet is hard to judge because different neural architectures are used. AutoGrow uses 4 blocks while this paper uses 5 blocks. It's unclear if the benefit comes from the method or just from an additional block.\n-- 2.3. In the experiments of layer growing, please clarify if filter growth is also applied or not.\n-- 2.4 clarify \"their growing process is not efficient.\" If I read the AutoGrow paper correctly, efficiency is one of the their claims and they showed that the growing process is as fast as \"training a single DNN\", and they scaled to ImageNet, which is not covered in this paper.\n\nMinors:\n1. networks with \"20 filters\" and \"100 neurons\" are used as the seeds. How critical are they?\n2. \"To the best of our knowledge, this is the first algorithm for BoN that can simultaneously learn the network structures and parameters from training data.\" is over-claimed. Lots of pruning methods can do it, although they start from a large one and prune it down.\n\n[1] Philipp, George, and Jaime G. Carbonell. \"Nonparametric neural networks.\" arXiv preprint arXiv:1712.05440 (2017).", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 3}}}