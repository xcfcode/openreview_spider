{"paper": {"title": "Automated Generation of Multilingual Clusters for the Evaluation of Distributed Representations", "authors": ["Philip Blair", "Yuval Merhav", "Joel Barry"], "authorids": ["pblair@basistech.com", "yuval@basistech.com", "joelb@basistech.com"], "summary": "Applying simple heuristics to the Wikidata entity graph results in a high-quality semantic similarity dataset.", "abstract": "We propose a language-agnostic way of automatically generating sets of semantically similar clusters of entities along with sets of \"outlier\" elements, which may then be used to perform an intrinsic evaluation of word embeddings in the outlier detection task. We used our methodology to create a gold-standard dataset, which we call WikiSem500, and evaluated multiple state-of-the-art embeddings. The results show a correlation between performance on this dataset and performance on sentiment analysis.", "keywords": ["Natural language processing", "Applications"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "The reviewers (and I) welcome new approaches to evaluation, and this work appears to be a sound contribution to that space. I suggest that, because the paper's incremental findings are of somewhat narrower interest, it is a good fit for a workshop, so that the ideas can be discussed at ICLR and further developed into a more mature publication."}, "review": {"SyORSSu4g": {"type": "rebuttal", "replyto": "HkDOVF3ml", "comment": "Thank you for your comments.\n\nOur primary goal was to investigate a way to automatically generate outlier detection test cases without the\nrequired labor and domain-knowledge bias of human-created test sets. While our method may have only yielded 500 test groups (a total of 2,000-3,000 test cases for each language), we believe that the WikiSem500 dataset shows promise to this end. Especially, the strength of our approach is in the number of languages it can scale up to. We believe it can be applied for any language with decent Wikidata/Wikipedia coverage. Our first release of WikiSem500 includes five major languages, but we are planning a second release with significantly more languages. As far as we know, previous approaches don\u2019t scale to other languages without significant manual efforts. We\u2019ve updated the related work section with more details about the disadvantages of existing datasets compared to WikiSem500. \n\nIn the spirit of this, while we did not consider using Mechanical Turk for filtering out test cases, we would\nhave likely decided against it had we done so. Requiring human quality assurance as a step in the pipeline would necessitate having one or more language experts in each language one is attempting to construct test cases for. Instead, we opted to iteratively perform quality checks on our system's output and nail down the common characteristics of \"bad\" test groups. Once we had developed a fully fleshed-out system of heuristics for English, we found that very little work was required to extend them into an equally-fleshed-out system in other languages.\n\nRegarding your third point. Camacho-Collados & Navigli (2016) motivated the outlier-detection task by the high human performance compared to other tasks like word similarity, which we agree with. We also reproduced their 90+% human performance on a subset of our English dataset which the annotators had sufficient domain knowledge on. Our main contribution is that we created a larger and more diverse dataset, with both words and phrases, and in multiple languages. Their dataset was manually created like most existing datasets.\n\nWith respect to this paper being an appropriate fit for ICLR, we would like to note that prior to submitting we\nasked the conference coordinators this exact question:\n\u201cWill ICLR consider work about word embedding evaluation? In particular, a paper proposing a new evaluation dataset.\u201d\n \nTo which we received the following response:\n\"[T]hat is considered a relevant topic because it's about representing words. Better metrics and datasets are key to eventual modeling improvements. Thank you.\"\n\nFinally, thank you for bringing our attention to that citation which escaped our notice. We have updated the paper to include it.\n", "title": "Scalability and Goodness-of-Fit"}, "BywsBS_El": {"type": "rebuttal", "replyto": "r1rwrnWVg", "comment": "Thank you for your comments.\n\nAs far as we know, the majority of existing datasets, like the analogy dataset proposed by Mikolov et al., are in English and focus on single words only. Also, the Mikolov word analogies dataset contains 14 categories, and only about half of them are for semantic evaluation (e.g., \"US Cities\", \"Common Capitals\", \"All Capitals\"). In contrast, our dataset contains hundreds of categories, making it a more diverse and challenging dataset for general-purpose evaluation of word representations. The Mikolov dataset has the advantage of including syntactic categories, which we left for future work. In the related work section we focused mainly on problems with existing word similarity datasets. We've updated it to include word analogies as well.\n\nCould you please elaborate more on entity typing? Are you referring to fine-grained named entity recognition that can be used for building a similar dataset?", "title": "Differences with the Analogy Dataset"}, "HkXPSKOXg": {"type": "rebuttal", "replyto": "Sy7ie31mg", "comment": "Thank you for the feedback! Indeed, differing degrees of difficulty are the intent of the three outlier classes, with O1 outliers being the most difficult to detect (as they are the most similar to the cluster entities) and O3 being the easiest. In order to test this, we have generated three datasets with only one of each outlier class. Figure 2 in the updated paper illustrates that there is in fact a strong correlation between outlier class and performance.  \n\nAdditionally, we have updated the paper to include results which compare our evaluations to performance on the analogy task. We find a strong correlation (Spearman's rho = 0.88) between performance on the two metrics.", "title": "Results by Outlier Class and Comparison with the Analogy Task"}, "ryxHVFdXg": {"type": "rebuttal", "replyto": "S1Uhl7J7x", "comment": "Thank you for the feedback! Yes, it is a concern that the reported human performance is quite low. We have taken your advice and performed a second set of human evaluations on a smaller dataset with more familiar topics. Performance (described in the updated paper) was drastically improved, with an overall human accuracy of 93% (each annotator missed exactly one of the 15 clusters) and 96.4% agreement among annotators. This significantly higher performance would seem to indicate that lack of domain knowledge was the primary cause of the low performance on the full dataset, which is further corroborated by inspecting the misses on the larger human evaluation. For example, on one of the most missed clusters, a large number of people falsely identified \"Paris green\" as an outlier among a set of insecticides (the true outlier was \"Bordeaux mixture,\" a fungicide). We hypothesize that many falsely associated the term \"Paris green\" with the color. On the other hand, if one had an intimate knowledge of pesticides, they would (in theory) be able to distinguish the true outlier. \n\nAnecdotally, we have found that most challenging test cases (such as the one above) often contain O1 outliers, for the distinction between them and the cluster entities can be quite subtle. We believe this is a strength of our approach, however, for embeddings are only able to do well on this dataset if they encode the domain-specific semantic meanings of words and phrases in addition to their usage in everyday speech.", "title": "Second Round of Human Evaluations"}, "Sy7ie31mg": {"type": "review", "replyto": "SkuqA_cgx", "review": "This paper propose to evaluate word representations by detecting outliers from a set of words (or phrases). If I understand correctly, three different methods are proposed to generate outliers, based on the distance in the wikidata tree. These different ways to generate the data probably lead to outliers with different characteristics (in particular some might be easier to detect). Have the authors evaluated the word representations on the different subsets of the data?\n\nAs stated by the authors, another popular evaluation of word representations are analogy questions, introduced by Mikolov et al. Have the authors considered to compute the correlation between the proposed method and these analogy questions?This paper introduces a new dataset to evaluate word representations. The task considered in the paper, called outlier detection (also known as word intrusion), is to identify which word does not belong to a set of semantically related words. The task was proposed by Camacho-Collados & Navigli (2016) as an evaluation of word representations. The main contribution of this paper is to introduce a new dataset for this task, covering 5 languages. The dataset was generated automatically from the Wikidata hierarchy.\nEntities which are instances of the same category are considered as belonging to the same cluster, and outliers are sampled at various distances in the tree. Several heuristics are then proposed to exclude uninteresting clusters from the dataset.\n\nDeveloping good ressources to evaluate word representations is an important task. The new dataset introduced in this paper might be an interesting addition to the existing ones (however, it is hard to say by only reviewing the paper). I am a bit concerned by the lack of discussion and comparison with existing approaches (besides word similarity datasets). In particular, I believe it would be interesting to discuss the advantages of this evaluation/dataset, compared to existing ones such as word analogies. The proposed evaluation also seems highly related to entity typing, which is not discussed in the paper.\n\nOverall, I believe that introducing ressources for evaluating word representations is very important for the community. However, I am a bit ambivalent about this submission. I am not entirely convinced that the proposed dataset have clear advantages over existing ressources. It also seems that existing tasks, such as entity typing, already capture similar properties of word representations. Finally, it might be more relevant to submit this paper to LREC than to ICLR.", "title": "Reporting results on the different subsets of the data", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "r1rwrnWVg": {"type": "review", "replyto": "SkuqA_cgx", "review": "This paper propose to evaluate word representations by detecting outliers from a set of words (or phrases). If I understand correctly, three different methods are proposed to generate outliers, based on the distance in the wikidata tree. These different ways to generate the data probably lead to outliers with different characteristics (in particular some might be easier to detect). Have the authors evaluated the word representations on the different subsets of the data?\n\nAs stated by the authors, another popular evaluation of word representations are analogy questions, introduced by Mikolov et al. Have the authors considered to compute the correlation between the proposed method and these analogy questions?This paper introduces a new dataset to evaluate word representations. The task considered in the paper, called outlier detection (also known as word intrusion), is to identify which word does not belong to a set of semantically related words. The task was proposed by Camacho-Collados & Navigli (2016) as an evaluation of word representations. The main contribution of this paper is to introduce a new dataset for this task, covering 5 languages. The dataset was generated automatically from the Wikidata hierarchy.\nEntities which are instances of the same category are considered as belonging to the same cluster, and outliers are sampled at various distances in the tree. Several heuristics are then proposed to exclude uninteresting clusters from the dataset.\n\nDeveloping good ressources to evaluate word representations is an important task. The new dataset introduced in this paper might be an interesting addition to the existing ones (however, it is hard to say by only reviewing the paper). I am a bit concerned by the lack of discussion and comparison with existing approaches (besides word similarity datasets). In particular, I believe it would be interesting to discuss the advantages of this evaluation/dataset, compared to existing ones such as word analogies. The proposed evaluation also seems highly related to entity typing, which is not discussed in the paper.\n\nOverall, I believe that introducing ressources for evaluating word representations is very important for the community. However, I am a bit ambivalent about this submission. I am not entirely convinced that the proposed dataset have clear advantages over existing ressources. It also seems that existing tasks, such as entity typing, already capture similar properties of word representations. Finally, it might be more relevant to submit this paper to LREC than to ICLR.", "title": "Reporting results on the different subsets of the data", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "S1Uhl7J7x": {"type": "review", "replyto": "SkuqA_cgx", "review": "I like the idea of evaluating via outlier detection, it seems like a very natural way to go. However, this intuition does not fit well with the human performance of 68.9%.\nIs this just an issue of broad domain? One way to test this would be to select a biased sample of clusters that tend to appear more in news corpora; e.g. a cluster of US presidents might be more familiar to the average annotator than a cluster of regions in Middle Earth.\nIf it is not a domain issue (after all, the annotators had access to Wikipedia), then this might indicate a problem in the dataset construction. Perhaps some of the outliers aren't different enough from the cluster? You might be able to check this via inter-annotator agreement per cluster (which is also interesting to report).This paper describes a new benchmark for word representations: spotting the \u201codd one out\u201d. The authors build upon an idea recently presented at the RepEval workshop, but are able to collect a significantly larger amount of examples by relying on existing ontologies.\n\nAlthough the innovation is relatively incremental, it is an important step in defining challenging benchmarks for general-purpose word representations. While humans are able to perform this task almost flawlessly (given adequate domain knowledge), the experiments in this paper show that current embeddings fall short. The technical contribution is thorough; the dataset construction appears logical, and the correlation analysis is convincing. I would like to see it accepted at ICLR.", "title": "Why is human performance so low?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1qL1Uy4x": {"type": "review", "replyto": "SkuqA_cgx", "review": "I like the idea of evaluating via outlier detection, it seems like a very natural way to go. However, this intuition does not fit well with the human performance of 68.9%.\nIs this just an issue of broad domain? One way to test this would be to select a biased sample of clusters that tend to appear more in news corpora; e.g. a cluster of US presidents might be more familiar to the average annotator than a cluster of regions in Middle Earth.\nIf it is not a domain issue (after all, the annotators had access to Wikipedia), then this might indicate a problem in the dataset construction. Perhaps some of the outliers aren't different enough from the cluster? You might be able to check this via inter-annotator agreement per cluster (which is also interesting to report).This paper describes a new benchmark for word representations: spotting the \u201codd one out\u201d. The authors build upon an idea recently presented at the RepEval workshop, but are able to collect a significantly larger amount of examples by relying on existing ontologies.\n\nAlthough the innovation is relatively incremental, it is an important step in defining challenging benchmarks for general-purpose word representations. While humans are able to perform this task almost flawlessly (given adequate domain knowledge), the experiments in this paper show that current embeddings fall short. The technical contribution is thorough; the dataset construction appears logical, and the correlation analysis is convincing. I would like to see it accepted at ICLR.", "title": "Why is human performance so low?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1ncpcX-g": {"type": "rebuttal", "replyto": "SkuqA_cgx", "comment": "It is mentioned in a footnote within the paper, but here is the link to the WikiSem500 dataset and a sample evaluation script: https://github.com/belph/wiki-sem-500", "title": "Dataset Link"}}}