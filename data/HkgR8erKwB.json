{"paper": {"title": "PAC-Bayesian Neural Network Bounds", "authors": ["Yossi Adi", "Alex Schwing", "Tamir Hazan"], "authorids": ["yossiadidrum@gmail.com", "aschwing@illinois.edu", "tamir.hazan@technion.ac.il"], "summary": "We derive a new PAC-Bayesian Bound for unbounded loss functions (e.g. Negative Log-Likelihood). ", "abstract": "Bayesian neural networks, which both use the negative log-likelihood loss function and average their predictions using a learned posterior over the parameters, have been used successfully across many scientific fields, partly due to their ability to `effortlessly' extract desired representations from many large-scale datasets. However, generalization bounds for this setting is still missing.\nIn this paper, we present a new PAC-Bayesian generalization bound for the negative log-likelihood loss which utilizes the \\emph{Herbst Argument} for the log-Sobolev inequality to bound the moment generating function of the learners risk.", "keywords": ["PAC-Bayesian bounds", "PAC-Bayes", "Generalization bounds", "Bayesian inference"]}, "meta": {"decision": "Reject", "comment": "This paper proposes PAC_Bayesian bounds for negative log-likelihood loss function. A few reviewers raised concerns around 1) distinguish their contributions better from prior work (eg Alquier). 2) confounders in their experiments. Both reviewers agreed that the paper, as it is written, does not provide sufficient evidence of significance. In addition, experiments shown in the paper varies two things - # parameters (therefore expressiveness and potential generalizability) and depth at each setting. As pointed out, this isn\u2019t right - in order to capture the effect, one has to control for all confounders carefully. Another concerned raised were around Theorem 2 - that it contains data-distribution on the right hand side, which isn\u2019t all that useful to calculate generalization bounds (we don\u2019t have access to the distribution). We highly encourage authors to take another cycle of edits to better distinguish their work from others before future submissions. \n"}, "review": {"Skxxg4nDqH": {"type": "review", "replyto": "HkgR8erKwB", "review": "This paper suggests a PAC-Bayesian bound for negative log-likelihood loss function. Many PAC-Bayesian bounds are provided for bounded loss functions but as authors point out, Alquier et al. (2016) and Germain et al. (2016) extend them to  unbounded loss functions. I have two major concerns regarding this paper:\n\n1- Technical contribution: Since Alquier et al. (2016) has already introduced PAC-Bayesian bounds for the hinge-loss, I think the technical contributions of this paper is not significant enough for the publication. Moreover, the particular format of the bound in Theorem 2 is problematic since the right hand side depends on the data-distribution. When presenting the generalization bound, we really want the right hand side to be independent of the distribution (given the training set) and that is the whole point of calculating the generalization bounds. In particular, I don't see why inequality (1) is any better than inequality (2).\n\n2- Experiments: The main issue with the correlation analysis done in Section 6 is that authors only change depth of the networks and then check the correlation of the generalization bound to the test error. The problem is that in all those networks deeper ones generalize better so it is not clear that the correlation is due to a direct relationship to generalization or a direct relationship to depth. For example, if we take 1/depth as a measure, it would correlate very well with generalization in all these experiments but 1/depth is definitely not the right complexity measure or generalization bound. To improve the evaluation, I suggest varying more hyperparameters to avoid the above issue.\n\n\n***************************\n\nAfter rebuttals:\n\nUnfortunately, my concerns are not addressed adequately by authors. Therefore, my evaluation remains the same.", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 4}, "r1em1aSnsS": {"type": "rebuttal", "replyto": "rkxF8d2tsB", "comment": "We would like to thank you for clarifying your concern. \nThe distribution dependence concern can be mitigated by standard concentration of measure bounds (e.g., Chebyshev's inequality), while assuming that the variance of the gradient norm is bounded (which holds since the gradients decay very fast).\nRegarding the experiments. The MGF is correlated with the gradients norm (fig. 1a) which may be correlated with the depth, depends on model architecture and the chosen transfer functions. However, we would like to clarify that the bound (computed in Table 2) is both the expected gradient norm and the KL term, hence, there is an interesting balance between both of them which causing lower generalization bounds for deeper models.", "title": "Response"}, "r1llUSjHoS": {"type": "rebuttal", "replyto": "Bye8oCvAKr", "comment": "We thank the reviewer for taking the time to review our paper.\nRegarding the Gaussian/log-concave assumption: \nWe believe such an assumption is reasonable for various input domains (especially compared to the assumptions in previous work, such as linearity, convexity, bounded loss, etc.). \n\nRegarding the connection between the bound and the results:\nThe premise is that using the proposed bound we can choose better prior distribution over the weights, which results in better posterior distribution over the weights which translates to better generalization and better uncertainty estimates.\n\n\n\n", "title": "Authors response"}, "rkg-AVjBiS": {"type": "rebuttal", "replyto": "rkgITCHrqS", "comment": "Regarding the significance concern: \nWe kindly disagree with the reviewer that the classification loss is always bounded. While the zero-one loss function is bounded, it cannot be used for training. The hinge-loss can be bounded under the assumption of bounded inputs and Lipschitz function corresponding to the model.\nWhile computing the Lipschitz constant in NP-Hard for deep nets (https://arxiv.org/pdf/1805.10965.pdf), The Lipschitz constant increases in the depth of the network, while our bound holds for unbounded input and non-Lipschitz functions, and decreases in the depth of the network. Hence, Lipschitz-type bounds are very crude, e.g., they do not distinguish between different activation functions with the same Lipschitz constant. For example, our bound achieves fast-rate (1/m) for deep networks, and as far as we know this is the only bound that achieves it under this setting. \n\nRegarding previous work for unbounded loss functions: \nWe kindly ask the reviewer to clarify. As far as we know, the other results for unbounded loss functions consider linear models, while our bound considers deep networks, which is certainly a significant difference. Can you please elaborate on the difference, which we might overlooked. We also point out that although our treatment holds for any (almost anywhere) smooth loss function, the NLL loss is the most widely used loss these days, and this extension only is probably of interest to the community.  \n\nRegarding the Herbst argument: \nTo the best of our knowledge, the Herbst argument was never used in PAC-Bayesian setting (nor in any generalization bound). Moreover, our use of the Herbst argument is different than previous works in functional analysis (e.g., Gross 1975, Ledoux 2001) that use it for Lipschitz functions, while our work use it for *non-Lipschtiz* functions. \n\nRegarding the experiments: \nWe agree with the reviewer that depth is not the only parameter that is being changed, however, since the goal of this experiment was to explore the effect of the new MGF bound (the complexity of the model), which is dominated by the norm of the gradients w.r.t the input to the model, keeping the number of model parameters roughly the same is essential in order to create comparable KL values between the models. Increasing the number of parameters may cause higher KL values which will greatly affect the generalization bound.\nRegarding the repeated experiments, we will add mean and std measures for all experiments in the final manuescript.\n\nRegarding writing and title: \nWe will clarify all above comments for the final manuscript", "title": "Authors response"}, "BJggBmjHsH": {"type": "rebuttal", "replyto": "Skxxg4nDqH", "comment": "Regarding comparison with Alquier et al. (2016):\nWe kindly disagree with this assessment, which treats deep networks as linear models. Alquier et al. proved their bound for the hinge loss for the linear case which does not hold for deep networks. Our bound holds for any (almost everywhere) smooth loss function, which includes both linear models and deep neural models with the hinge loss. To better emphasize the above point, we presented the applicability of our bound to the linear case of the NLL loss function, which is the logistic regression case. We were focused on the NLL loss function for deep networks due to its extensively usage nowadays. Moreover, our bound achieves fast-rates (1/m), as presented in Figure 1(b), for deep networks while maintaining fixed prior variance, in contrast to Alquier et al.\n\nRegarding theorem 2 format:\nTraditionally, this is the view when presenting a generalization bound, a view which we also shared for a long time. Over time we came to the conclusion that this limits our understanding of Bayeisan deep networks. Our bound utilizes the distribution D and the prior distribution over the weights p, both distributions do not rely on the training data S. \n\nRegarding why inequality (1) is better than inequality (2):\nWe appreciate the reviewer allowed a specific example to emphasize our contribution. Inequality (1) cannot be computed since it requires to sample training set S multiple times and evaluate the Moment Generating Function (MGF) with respect to it. Since S dimension is about 60k (MNIST/Fashion-MNIST/CIFAR), one would need an infinite amount of time to estimate the MGF. \nInstead, one can use the independence assumption of S to decompose the MGF (similar to Equation (12) in the appendix). We tried to go this path, and since it estimates the expectation of an exponential function, it reaches infinity (nan) for lambda that is approximately ~15-20 on MNIST (recall, m=60k), i.e., the rate of this approach is much lower than the conventional $\\sqrt{m}$. In contrast, since our bound relies on gradients in the log-domain, we can show fast-rate bound (i.e., lambda = 60k) when the network turns deeper (see Figure 1b).   \n\nRegarding the depth experiments: \nIt is not true that deeper networks generalize better in all cases. The generalization bound consists of two terms, the complexity of the model (our new MGF bound, which is dominated by the norm of the gradients) and the complexity of the learning (which is controlled by the KL-divergence). The goal of this experiment was to explore the effect of the new MGF bound (the complexity of the model). Hence, we kept the number of model parameters roughly the same in order to create comparable KL values between the models. Increasing the number of parameters may cause higher KL values which will greatly affect the generalization bound.\nWe agree that if we keep the KL-divergence fixed, then the MGF decreases with the depth, but this varies with the components that are being used (convolutional layers, skip-connections, fully connected, etc.). According to Figure 1(a), we see that it is not the depth that matters but the norm of the gradient w.r.t the input, which is exactly what\u2019s dominates the MGF bound. \n", "title": "Authors response"}, "Bye8oCvAKr": {"type": "review", "replyto": "HkgR8erKwB", "review": "The paper offers PAC-generalization bounds for Bayesian Neural Networks based on a previous result by Alquier et al. (Theorem 1) which connects the generalization gap to the log partition function of the same gap for the prior distribution on the learned parameters (which is identical to the ELBO bound used in Bayesian neural networks for NLL loss). Due to the fact that the optimal bound occurs for the true posterior, the PAC-bayesian bounds offer a novel interpretation as an objective for BNNs.\n\nThe authors note that the log partition function can in general be easily unbounded for loss functions based on NLL (as in the BNN case); their result shows that if the norm of the gradient is bounded, that is enough to bound the overall generalization gap. \n\nWhile this appears to be a technically impressive feat, the the assumptions involved in Theorem 2 seem significant (probably unavoidable for a theoretically tractable statement). Primarily, the conditional of x given y is Gaussian/log-concave (or at least unimodal, more generally ) but the motivation is based on deep neural networks (for why the gradient is bounded).  \n\nThe authors also specialize their bound to the case of logistic regression. Interestingly, the gap in this case has an additive term proportional to the product of the label cardinality and the input dimension (I'm not sure whether how significant this is in terms of tightness).\n\n In experiments, the authors explore and analyze the tightness of the proposed bounds for various hyperparameters like the variance of the weights prior.\n\nThey also perform an exhaustive comparison of the BNN models against non-bayesian alternatives, but it is not clear how the new contributions from the generalization bounds are relevant to the results in, say Section 6.2", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 1}, "rkgITCHrqS": {"type": "review", "replyto": "HkgR8erKwB", "review": "Summary:\nThis paper proposes a PAC-Bayesian generalization bound for Bayesian neural networks. The author discuss that earlier generalization bounds hold for bounded loss function, whereas the proposed generalization bound considers negative log likelihood loss which can be unbounded. Therefore, earlier results do not hold here. \n\nThe technical approach used is along lines of PAC Bayes framework and specifically for this loss function which requires bounding the log-partition function, the authors follow the Herbst Argument for bounding the log-partition function. \n\nContribution: \nThe paper uses straight forward PAC Bayes approach and the only bottleneck is bounding the log-partition function for which the authors use an earlier result (Herbst Argument for bounding the log-partition function. )\n\nSignificance: \nMy biggest concern with this work is its significance. As we know classification loss is bounded and for regression loss, as long as we have bounded input and a Lipschitz function corresponding to the NN, the output is bounded. Also as authors mention, there have been two earlier results covering other unbounded loss functions. Therefore, I do not feel that extending those results to NLL is a good enough contribution. Especially since the extension uses a known approach  (Herbst Argument for bounding the log-partition function. )\n\nExperiments: \n* From the explanations, it seems each architecture is trained once, which is not acceptable. How can one refute the effect of a specific initial value? A good scientific practice entails having mean and variance bar for different values or at least repeating the experiment multiple times and reporting the avg. \n* According to the paper, the architectures in table 2, fig 1 are made by keeping the number of parameters roughly the same. Then the authors increase the depth. Note that to keep the # of parameters the same, they have to decrease the width as they increase the depth. Therefore, this cannot be a correct analysis of effect of the depth. As depth is not the only parameter that is changed between the architectures.\n\n\nWriting: \nThe writing is overall ok which some vague places such as \n*first page, last paragraph, line 1: \".. our PAC Bayesian bound is tightest when we learn...\" the authors do not discuss what is the space of options for the bound and only mention the case when it is tightest. Therefore the claim is confusing\n*first page, last paragraph, last line: \"..better uncertainty than the baseline\". The authors do not specify the baseline which makes this whole claim vague.\n\nTitle: \nThis is a minor issue but worth mentioning. The title is vague and confusing. In the first read one might think this paper provides PAC-Bayesian bounds for usual NNs (which has been considered and written about many times in the literature). The authors should mention that the considered networks are Bayesian NNs.", "title": "Official Blind Review #4", "rating": "3: Weak Reject", "confidence": 3}}}