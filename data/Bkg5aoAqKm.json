{"paper": {"title": "Fast Binary Functional Search on Graph", "authors": ["Shulong Tan", "Zhixin Zhou", "Zhaozhuo Xu", "Ping Li"], "authorids": ["laos1984@gmail.com", "zhixin0825@gmail.com", "zhaozhuoxu@gmail.com", "pingli98@gmail.com"], "summary": "Efficient Search by Neural Network based searching measures.", "abstract": "The large-scale search is an essential task in modern information systems. Numerous learning based models are proposed to capture semantic level similarity measures for searching or ranking. However, these measures are usually complicated and beyond metric distances. As Approximate Nearest Neighbor Search (ANNS) techniques have specifications on metric distances, efficient searching by advanced measures is still an open question. In this paper, we formulate large-scale search as a general task, Optimal Binary Functional Search (OBFS), which contains ANNS as special cases. We analyze existing OBFS methods' limitations and explain they are not applicable for complicated searching measures. We propose a flexible graph-based solution for OBFS, Search on L2 Graph (SL2G). SL2G approximates gradient decent in Euclidean space, with accessible conditions. Experiments demonstrate SL2G's efficiency in searching by advanced matching measures (i.e., Neural Network based measures).", "keywords": ["Binary Functional Search", "Large-scale Search", "Approximate Nearest Neighbor Search"]}, "meta": {"decision": "Reject", "comment": "This paper proposes an Optimal Binary Functional Search (OBFS) algorithm for searching with general score functions, which generalizes the standard similarity measures based on Euclidean distances. This yields an extension of the classical approximate nearest neighbor search (ANNS). As observed by the reviewers, this work targets an important research direction. Unfortunately, the reviewers raised several concerns regarding the clarity and significance of the work. The authors provided a good rebuttal and addressed some concerns, but not to the degree that reviewers think it passes the bar of ICLR. We encourage the authors to further improve the work to address the key concerns. "}, "review": {"BklgOiC2jQ": {"type": "review", "replyto": "Bkg5aoAqKm", "review": "Post-rebuttal\n------------------\nI have read the rebuttal and I better understand the paper. Given that, I am going to raise my rating by one point for the following reason:\n- The manuscript presents a novel solution to a general problem and it is a valid solution. However, the solution is somewhat obvious, which is not necessarily a bad thing, which is why I am raising my rating by a point. However, an easy solution like the one proposed in the manuscript means that OBFS considered in this manuscript is not as general as the authors let on -- there is an implicit assumption that f(x_i, q) is close to f(x_j, q) if x_i is close to x_j.\n- While the authors answered a lot of my clarification questions, the manuscript seems still a little hard to parse and can be significantly improved for easier reading and understanding.\n\n=========================================\nPros\n-------\n[Originality/Significance] The manuscript focuses on a very general and important problem and proposes a scheme to solve this general problem. The authors present some theoretical and empirical results to demonstrate the utility of the proposed scheme.\n\nLimitations\n----------------\n[Clarity] While the problem being addressing is extremely important, and the proposed solution seems reasonable, the manuscript is really hard to follow. For example, Definition 3 and Theorem 1 are extremely hard to understand. \n\n[Clarity/Significance] Moreover, I feel that the authors should be more precise in pointing out why current graph based search algorithms are just not trivially applicable to OBFS. The nature of the approximate Delaunay graph is that it can be built for any given similarity function (the level of approximation obviously depends on the similarity function, but that is an existing issue with graph-based methods). Given the graph, I do not understand why the basic search algorithm on this similarity graph would not be an approximate solution to OBFS. Hence I believe the authors need to clarify why the existing graph based algorithms do not directly translate. \n\n[Significance] While Definition 1 considers topological spaces, SL2G is assuming that X and (maybe) Y are in R^d (for different values of d). So does that mean that SL2G does not solve the general OBFS?\n\n[Significance/Correctness/Clarity] The assumptions in Theorem 2 (as well as the supporting Proposition 1 in Appendix B) seems quite unreasonable. In moderately high dimensional X, doesn't the curse of dimensionality imply that this condition will not hold in most case? In there any reason why/how this would be circumvented? Moreover, in Proposition 1 (in Appendix B), the quantity C_r needs to be precisely defined since it could in general be exponential in the number of dimensions. Also, the assumption in Proposition 1 where \\lambda^* > 0 is fairly strong in high dimensional data since data gets really sparse in high dimensions. Finally, the last step in Proposition 1 (where the failure probability obtained from the union bound is connected to condition (b) in Theorem 2) is not clear at all -- it is not apparent how E and F related to S and how p relates to every ball containing a point in S. This is a very important step and needs better exposition. \n\n[Clarity/Significance] I am unable to understand the baseline HNSW-SBFG (or the motivation for it) in the empirical section. It would be good to clarify this. \n\n\nGeneral comments\n---------------------------\n[Significance] Finally, I believe that it would be good to see a connection between the success of SL2G to relationship between |f(x1, q) - f(x2, q)| and ||x1 - x2 ||_2 since the author emphasize that the proposed scheme can be seen as \"gradient descent in Euclidean space\" (although the authors would need to also precisely explain what they mean by that statement).\n\n[Originality] Some related work that the authors should position their proposed problem/solution against:\n- There is some work on \"max-kernel search\" which can perform similarity search with general notions of similarity (than just Euclidean metrics).\n- There is some work on search with Bregman divergences which handle asymmetric similarity functions and also incorporate notions of gradient descent over convex sets.\n\nMinor comments/typos\n---------------------------------\n- The authors should present the precise SL2G algorithm given the graph in the manuscript.\n- l^2 --> \\ell_2\n- gradient decent --> gradient descent\n- Table 1, f(q, x) --> f(x, q)", "title": "Promising novel idea; needs further clarification and development", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "BkebSdD9aX": {"type": "rebuttal", "replyto": "BklgOiC2jQ", "comment": "6. [Significance] Finally, I believe that it would be good to see a connection between the success of SL2G to relationship between |f(x1, q) - f(x2, q)| and |x1 - x2 |_2 since the author emphasize that the proposed scheme can be seen as \"gradient descent in Euclidean space\" (although the authors would need to also precisely explain what they mean by that statement).\n\n[Response] As we mentioned in the paragraph after Theorem 2, the success and accuracy of our algorithm depend on the radius of curvature of the level sets. We believe |f(x_1, q) - f(x_2, q)| and |x_1 - x_2 |_2 together with the density of dataset might affect the speed of convergence, which we have not covered in this paper.\n\n7. [Originality] Some related work that the authors should position their proposed problem/solution against\u2026\n\n[Response] Thank you for pointing out these related works. First of all, we would like to emphasize that our work is very different from similarity search, so most of the existing methods in this field does not apply to our problem. \n``Max-kernel search\" is defined on a Hilbert space, so it has to be symmetric. Bregman divergence does not have to be symmetric, but both variables must come from the same convex set. Even if we apply Bregman ball tree to a similarity search problem, we do not think it performs well on finding top-10 nearest neighbors.\nWe will analyze these works in our updated manuscript. \n\n8. The authors should present the precise SL2G algorithm given the graph in the manuscript.\n\n[Response] Thanks for your suggestion. To make the manuscript more self-contained, we will list the algorithms (graph construction and greedy search) in the Appendix, although they are common algorithms for search on graph methods.\n\nFinally, we really appreciate your time and detailed comments.\n", "title": "Responses to the Comment 6, 7&8 "}, "S1gakuPcTX": {"type": "rebuttal", "replyto": "BklgOiC2jQ", "comment": "3. [Significance] While Definition 1 considers topological spaces, SL2G is assuming that X and (maybe) Y are in $R^d$ (for different values of d). So does that mean that SL2G does not solve the general OBFS?\n\n[Response] Thanks for pointing this out. The answer is \u201cno\u201d, it only solves OBFS when X and Y are subsets of Euclidean spaces. Actually, we are usually interested in Euclidean spaces or its subsets in real applications, as mentioned below Definition 1.\n\n4. [Significance/Correctness/Clarity] The assumptions in Theorem 2 (as well as the supporting Proposition 1 in Appendix B) seems quite unreasonable. In moderately high dimensional X, doesn't the curse of dimensionality imply that this condition will not hold in most case? \u2026\n\n[Response]Thank you for reading the proposition and theorem very carefully. \nWe consider an asymptotic setting that the number of data points growing to infinity and the dimension of X is fixed. When the region E is fixed, \\lambda^* is proportional to the number of data points, so it goes to infinity. \nOf course, one can consider a high dimensional setting when n and d increase simultaneously. However, C_r is still not critical since it is not in the exponent. In the failing probability formula, the volume of r/2 ball depends on d and plays a much more important role than C_r when d increases. If we hope the failing probability still goes to 0, then we should require log \\lambda^* is much greater than d log d.\nAbout the implication from Proposition 1 to condition (b) in Theorem 2, it is a simple geometry property. We recall that F is a set of centers of open r/2-balls whose union covers E. For a fixed open r-ball, say B, its center is covered by at least one open r/2-ball with the center in F. This open r/2-ball is contained in B by triangle inequality. The open r/2-ball contains at least one data point, which also belongs to B. This implies every open r-ball contains a data point, which is the assumption (b) in Theorem 2. We believe our proof is mathematically correct and clear.\nWe believe this is a nontrivial result. As the number of data points increases, we have more \u201cbad\u201d data points. Here, \u201cbad\u201d data point means it is far away from the local optimum of $f$, but it is a local optimum in greedy search on the graph. The theorem and proposition show that even if we have more bad data points, the failure probability of the greedy search still goes to 0.\n\n5. [Clarity/Significance] I am unable to understand the baseline HNSW-SBFG (or the motivation for it) in the empirical section. It would be good to clarify this. \n\n[Answer] HNSW-SBFG is quite similar to the original HNSW. We just replace the metric measure in HNSW, such as l2 or cosine, with the focusing search binary functional f. Beyond that, the graph construction and greedy search approaches of HNSW-SBFG are same as the original HNSW. Note that, to let HNSW-SBFG be applicable, we set X and Y in the same space (both 64-dimensional). In this way, f(x_i,x_j) will output a value no matter f is symmetrical (e.g., MLP-Em-Sum) or asymmetrical (e.g., MLP-Concate). If f is asymmetrical, f(x_i,x_j) is problematic actually. That why HNSW-SBFG works even worse on MLP-Concate datasets. If X and Y have different dimensions, HNSW-SBFG will be not applicable.\n", "title": "Responses to the Comment 3, 4&5"}, "HkehwwDqTX": {"type": "rebuttal", "replyto": "BklgOiC2jQ", "comment": "1. While the problem being addressing is extremely important, and the proposed solution seems reasonable, the manuscript is really hard to follow. For example, Definition 3 and Theorem 1 are extremely hard to understand. \n\n[Response] Thanks for your comments. We will add more explanations for the theory part and make it easier to access. Specifically, although the problem is in an asymmetric setting, readers can still assume f(x,y) = -|x-y| as a typical example to understand the definitions and theorem. For example, assuming f is the negative l2-norm, then definition 3 means we will connect two data points in the Delaunay graph if the Voronoi cell is \u201cadjacent\u201d to each other. Here, adjacency means their boundary has nonempty intersection. Theorem 1 means that, for an arbitrary query, a greedy search on Delaunay graph with any initial point can find the nearest neighbor of the query.\n\n2. [Clarity/Significance] Moreover, I feel that the authors should be more precise in pointing out why the current graph-based search algorithms are just not trivially applicable to OBFS. \u2026\n\n[Response] Thank you for this comment. We are going to assume \"basic search algorithm on similarity graph\" indicates the previous search on graph methods, such as HNSW or Bregman ball tree (you mention in a later comment). These algorithms require f(x,y) defined on the product of two identical spaces. OBFS is much more general and does not have such an assumption. \n\nSuppose we still assume x and y are from the same space and plug in f as a \"similarity function\" in HNSW, which is exactly the baseline, HNSW-SBFG, we used in experiments. Particularly, in the recommendation-system scenario, we embed users and items in the same Euclidean space. As shown in the experimental results on page 8, the performance of HNSW-SBFG is much poorer than HNSW-SL2G. We believe original HNSW or any other existing similarity graph based algorithms require f performs like a similarity function. A well behaved f in recommendation system should not measure the similarity between user and item.\n\nIt is also worth to mention that, although we provide guarantees for SBFG, but most of general f's, e.g., neural networks, does not satisfy the condition in Theorem 1.\n", "title": "Responses to the Comment1&2"}, "SJxBPmP9pm": {"type": "rebuttal", "replyto": "Byx3R3E03m", "comment": "The authors do not demonstrate sufficient value of performing approximation in this specific fashion. For instance, in Theorem 2, the authors start with the concavity assumption of the scoring function f(). Then it is natural to apply a gradient ascent method on the neighborhood graph. And the authors did not quantitatively or qualitatively justify their specific approach.\n\n[Response] Thanks for your comments. Nodes on the neighborhood graph are discrete points in the space. Searching on the neighborhood graph is quite different from gradient descent in the continuous space. That is why we try to figure out the conditions in which the proposed method will work well. To the best of our knowledge, this is the first work discusses this point. We provided the theoretical analysis and empirical experiments for the proposed approach. \n\nLately, numerous publications have shown that distilled models can achieve very high quality and render scoring function separable. The authors should at least compare their method against distillation and Maximum Inner Product Search based approaches. \n\n[Response] For related distilled models, could you specify the particular papers? Thanks. \n\nThe MIPS problem is a special case of the Binary Functional Search problem. Although the proposed method (SL2G) is not designed for the MIPS problem but for more complex searching measures, it can be applied for MIPS, the corresponding empirical study can be found in Appendix D.", "title": "Responses to the comments"}, "Byx3R3E03m": {"type": "review", "replyto": "Bkg5aoAqKm", "review": "This work extends the approximate nearest neighbor search (ANNS) algorithm to a more general setting. Instead of search with a \"separable\" similarity measure, the authors propose Optimal Binary Functional Search (OBFS), where the scoring function f() is in general non-separable. The exact construction of the Binary Function Graph wrt f() and X is computationally expensive. The specific approximate algorithm of OBFS proposed in the paper is to:\n1) First construct an L2 Delaunay graph for based on the dataset X only and;\n2) Perform greedy search with the L2 Delaunay graph.\n\nThe authors also discuss various conditions under which, the approximation method can achieve close to optimal value.\n\nSome of the concerns I have with this work:\n\n1) The authors do not demonstrate sufficient value of performing approximation in this specific fashion. For instance, in  Theorem 2, the authors start with the concavity assumption of the scoring function f(). Then it is natural to apply a gradient ascent method on the neighborhood graph. And the authors did not quantitatively or qualitatively justify their specific approach.\n\n2) Lately, numerous publications have shown that distilled models can achieve very high quality and render scoring function separable. The authors should at least compare their method against distillation and Maximum Inner Product Search based approaches.\n\nOverall, this research direction is interesting, but this specific work falls short for a publication at ICLR.\n\n", "title": "Fast Binary Functional Search on Graph", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}