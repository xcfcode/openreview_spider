{"paper": {"title": "Bit-Pragmatic Deep Neural Network Computing", "authors": ["Jorge Albericio", "Patrick Judd", "Alberto Delmas", "Sayeh Sharify", "Andreas Moshovos"], "authorids": ["jorge.albericio@gmail.com", "judd@ece.utoronto.ca", "delmas1@ece.utoronto.ca", "sayeh@ece.utoronto.ca", "moshovos@ece.utoronto.ca"], "summary": "A hardware accelerator for DNNs whose execution time for convolutional layers is proportional to the number of activation *bits* that are 1.", "abstract": "We quantify a source of ineffectual computations when processing the multiplications of the convolutional layers in Deep Neural Networks (DNNs) and propose Pragrmatic (PRA), an architecture that exploits it improving performance and energy efficiency. \nThe source of these ineffectual computations is best understood in the context of conventional multipliers which generate internally multiple terms, that is, products of the multiplicand and powers of two, which added together produce the final product. At runtime, many of these terms are zero as they are generated when the multiplicand is combined with the zero-bits of the multiplicator. While conventional bit-parallel multipliers calculate all terms in parallel to reduce individual product latency, PRA calculates only the non-zero terms resulting in a design whose execution time for convolutional layers is ideally proportional to the number of activation bits that are 1. Measurements demonstrate that for the convolutional layers on Convolutional Neural Networks and during inference, PRA improves performance by 4.3x over the DaDiaNao (DaDN) accelerator and by 4.5x when DaDN uses an 8-bit quantized representation. DaDN was reported to be 300x faster than commodity graphics processors. \n\n", "keywords": ["Deep learning", "Applications"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "Unfortunately, none of the reviewers, nor the AC, have strongly supported for the acceptance of this paper. The fact that fixed-point arithmetic is the focus of this work, while floating-point arithmetic is much more common, is also a concern. The PCs thus don't think this work can be accepted to ICLR. However, we are happy to invite the authors to present their work at the Workshop Track."}, "review": {"B1zpZJ4ug": {"type": "rebuttal", "replyto": "By14kuqxx", "comment": "We updated the paper to present a new enhanced encoding that further boosts performance at almost no hardware cost. We would like to clarify that the baseline designs evaluated do include the dispatcher cost.", "title": "Performance update"}, "ry4CMYlwg": {"type": "rebuttal", "replyto": "By14kuqxx", "comment": "My apologies for the confusion. I posted a comment meant for a different submission here. I deleted the comment and restored the paper. ", "title": "Please ignore previous comment -- wrong submission"}, "S1w7UMlPe": {"type": "rebuttal", "replyto": "By14kuqxx", "comment": "We are reluctant to complete the reviewer ratings as stated as we think we would mischaracterize the reviews. For the most part, the reviews were concerned with the fit to ICLR (this is for you to interpret what the inclusion of \"hardware\" in the CFP means). The technical comments we addressed below.\n\nThe main take away for the ML community here is that it is possible to improve performance and energy efficiency by taking advantage of the distribution of zero bits in the activation stream. This opens up additional opportunities for exploration such as adjusting the precision to zero out portion of the activations, or adjusting the activation function to avoid numbers with many bits that are 1. We presented one application where the precisions were adjusted to get an additional boost in performance. We hope that this work will serve as motivation for followup work in hardware (better encoding) and in network design.\n\nThe technique was presented and evaluated for fixed-point hardware but it is applicable to other representations such as floating point (which would require further investigation).\n\nAt the hardware level this is a unique design that exploits value content to reduce computations. We do show that there is a heavy bias toward zero bits. We also present a wide parallel engine that exploits shift-and-add units in a practical manner getting much of the benefit that is possible. The key challenge in hardware is that the straightforward application of shift-and-add results in an impractically large, power inefficient design. We proposed techniques to overcome these. To the best of our knowledge this is the first design of a wide data-parallel engine whose performance varies with the zero bit content of the inputs.\n\nThe area and energy efficiency results are based on  post-synthesis layout and were collected using actual activity measurements.", "title": "Reviewer Rating"}, "SkI1CYtre": {"type": "rebuttal", "replyto": "ByPiBkOre", "comment": "Thank you for your encouraging comments and all the best for 2017. Please consider the following:\n\n1. The approach presented could be valuable for floating-point as well. In fact, our next step is to try to apply it to training and FP. Here's why it can be used for FP as well: When doing multiplication in FP, the exponents are added and the mantissas are multiplied. Even for single-precision FP, that would be a 23bx23b multiplication which would take much longer than adding  the exponents. Our approach would reduce the time needed to perform these operations. A proper study would be needed to determine the performance and energy characteristics of such a design.\n\n2. Most work on acceleration for inference uses 16-bit fixed point at present, and many end-users will be performing primarily classification and in many cases on mobile and embedded platforms. So, while our design target is primarily these systems, the user base will be very large. \n\n3. As you appropriately point out, there is work that suggests that even smaller precision and at the extreme, single-bit arithmetic may be usable. In our work we show that the bit-pragmatic approach offers great benefits for two commonly used platforms: 16-bit fixed-point and 8-bit quantized tensorflow-like. The approach itself combines judicious use of parallelism to maintain wide memory references, 2-level shifting to reduce area costs, and on-the-fly recoding to save computations. We would like to think that these concepts can be applicable to other architectures and thus believe that it would be valuable to have the paper presented. Moreover, the proposed architecture opens up new opportunities for network design where the precision and the values can be tuned to improved performance and energy efficiency.", "title": "Re: An interesting but very narrow DNN hardware accelerator"}, "HyrFIGuNx": {"type": "rebuttal", "replyto": "HkkRFeH4x", "comment": "Thank you for your comment and question, we will revise to clarify: In short, both designs were synthesized to operate at 980Mhz as limited by the access latency of the eDRAM blocks implementing the SB and the NM. Accordingly, a comparison in cycles is equivalent to a comparison in time. The key contribution here is the idea of processing only the essential bits in a practical manner. Further optimizations at the circuit level should be possible.\n\nLonger version: Pragmatic compared to DaDN omits the multipliers thurs reducing cost per output term. However, since it processes a bit a time, it needs more parallelism. Hence the need for more of these simpler units. The key challenge as far as operating frequency is concerned with Pragmatic is the communication cost over the relatively longer wires. Fortunately, since there is lots of parallelism even within the computations of each output activation, the design can be pipelined avoiding any increase in clock cycle. It is for this reason why Pragmatic can match the latency of DaDN.  \nWe used the best technology that was available to us (65nm). We fully expect the design to be synthesizable in a better logic technology (e.g., 28nm). The synthesis tools can easily pipeline away any delays given the lack of cross-cycle dependencies and similarly a designer can also pipeline the design at various levels. For example, fetching the next set of weights while processing the current one, or even fetching two sets of weights ahead if necessary.\n", "title": "Re: Good read, some questions about performance in practice. --> same frequency for both designs"}, "H1iL4CfVg": {"type": "rebuttal", "replyto": "rJ_HlyGEx", "comment": "Thank you for your comment identifying this as interesting work. We believe that this work opens up new opportunities for the ML community and we do demonstrate one such opportunity in the paper: trim precision and boost performance even further and beyond what was previously possible (see Stripes reference). Other opportunities for investigation exist such as a adjusting activation values and considering the implications for training. To the best of our knowledge, there is no other accelerator whose performance depends primarily on the 1 bit content of activations.\n\nRegarding your recommendation and given that the CFP includes \" Implementation issues, parallelization, software platforms, ***hardware***\" could you please reconsider whether the best way to communicate your opinion on whether this fits with the conference is to rate it below acceptance? Wouldn't it best to rank the paper first on its technical merit and then have a discussion on what is the interpretation.vision behind the inclusion of the term \"hardware\" in the ICLR call for papers for the future of ICLR?\n\nIMHO collaboration between the ML and the computer hardware community is important for further innovation on both domains. Computing hardware performance is not going to improve anymore as it used to and specialized designs seem to be the only viable way forward from a hardware perspective. One way to foster this cross-discipline work is by having works such as pragmatic appear and become known to the ICLR community. Pragmatic may be a victim of its own success: it offers nearly 4x speedup over the faster previously proposed accelerator (which itself was claimed to be 300x faster than commodity GPUs) without requiring any changes to the network.\n", "title": "Please reconsider given the CFP"}, "ryKFAO7Wl": {"type": "rebuttal", "replyto": "By14kuqxx", "comment": "We added an appending with the essential bit distributions but I mislabeled the graphs in that revision. We updated the original PDF with the correct labels and explanation. The main paper body remains as it was in Nov 4.", "title": "Ignore Nov 11 revision"}}}