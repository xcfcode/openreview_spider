{"paper": {"title": "Annealing Gaussian into ReLU: a New Sampling Strategy for Leaky-ReLU RBM", "authors": ["Chun-Liang Li", "Siamak Ravanbakhsh", "Barnabas Poczos"], "authorids": ["chunlial@cs.cmu.edu", "mravanba@cs.cmu.edu", "bapoczos@cs.cmu.edu"], "summary": "We study fundamental property of leaky RBM. We link the leaky RBM and truncated Gaussian distribution and propose a novel sampling algorithm without additional computation cost.", "abstract": "Restricted Boltzmann Machine (RBM) is a bipartite graphical model that is used as the building block in energy-based deep generative models. Due to numerical stability and quantifiability of the likelihood,  RBM is commonly used with Bernoulli units. Here, we consider an alternative member of exponential family RBM with leaky rectified linear units -- called leaky RBM.  We first study the joint and marginal distributions of leaky RBM under different leakiness, which provides us important insights by connecting the leaky RBM model and truncated Gaussian distributions.  The connection leads us to a simple yet efficient method for sampling from this model, where the basic idea is to anneal the leakiness rather than the energy; -- i.e., start from a fully Gaussian/Linear unit and gradually decrease the leakiness over iterations.  This serves as an alternative to the annealing of the temperature parameter and enables numerical estimation of the likelihood that are more efficient and more accurate than the commonly used annealed importance sampling (AIS).  We further demonstrate that the proposed sampling algorithm enjoys faster mixing property than contrastive divergence algorithm, which benefits the training without any additional computational cost.", "keywords": ["Deep learning", "Unsupervised Learning"]}, "meta": {"decision": "Reject", "comment": "This paper identifies a joint distribution for an RBM variant based on leaky-ReLU activations. It also proposes using a sequence of distributions, both as an annealing-based training method, or to estimate log(Z) with AIS.\n \n This paper was borderline. While there is an interesting idea, the reviewers weren't generally as excited by the work as for other papers. One limitation is that unit-variance Gaussian RBMs aren't a strong baseline for comparison, although that is the focus of the main body of the paper. An update to the paper has results for binary visibles in an appendix, although I'm not sure exactly what was done, if the results are comparable, or if there is a large cost of projection here."}, "review": {"BkPezTnSx": {"type": "rebuttal", "replyto": "S14pSJW4l", "comment": "1. WRT truncated Gaussian interpretation and recent work of Su et., al (2016):\nWe have added a paragraph explaining their approach. Note that in contrast to Su et., al (2016), which uses a truncated Gaussian distribution to model the joint and conditional distributions, the analysis in our paper shows that the marginal distribution has region-wise energy function where the energy of each region is similar to that of a Gaussian distribution. Therefore, we call the marginal p(v) in (leaky) ReLU RBM a \u201cunion\u201d of truncated Gaussian distributions. We have further clarified this point in section xxx of the revision. Since this work was put on Arxiv on 11/15/2016, after ICLR submission deadline, 11/5/2016, we could not cite this paper before. \n\n2. WRT Nair & Hinton\u201910: \nThe heuristic sampling technique used in that paper samples using max(0, x+N(0, \u03c3(x)). The reviewer is correct that in expectation this is monotonic. However, the expectation f(x) = E[max(0, x+N(0, \u03c3(x))], which corresponds to the activation function of sampling routine for Nair & Hinton\u201910 does not have a simple analytic form. Therefore we do not know the energy function associated with their sampling heuristic, which in turn prevents evaluation of the likelihood in that case. However, Ravanbakhsh et al.\u201916 show that the proper Laplace approximation assuming a \u201csoftplus\u201d unit is N(log(1+exp(x)) , \u03c3(x)), which indeed has a similar functional form to the sampling heuristic of Nair & Hinton\u201910. Although in this case the energy function is available, it contains a polylogarithmic term (see table 1. In Ravanbakhsh et al.\u201916) that requires evaluation of an infinite series and therefore it is not practical. In fact, this motivated us to focus on the case of leaky ReLU RBM, which has a tractable energy function and fast exact sampling procedure from conditionals. We have further clarified this in section 2.1 of revised version.\n\n3. WRT the binary data: \nWe put the result and the discussion in the revision. See Appendix F.3.  We study both toy datasets and two benchmarks, including MNIST and Caltech 101.\n", "title": "Re: some claims are incorrect, experiments should be improved"}, "rkizf63Be": {"type": "rebuttal", "replyto": "SkSNwcVEl", "comment": "1. WRT binary dataset: \nwe put the result and the discussion in the revision. See Appendix F.3.  We study both toy datasets and two benchmarks, including MNIST and Caltech 101.\n\n2. WRT PCD: \nWe put the result of PCD in the Appendix F.3. It shows PCD does not perform well as CD in the ReLU-RBM case though it has better performance in binary RBM case. Note that we show that the proposed \u201cannealing leakiness\u201d idea can also be used for learning. We observe that it has a better mixing than both CD and PCD, with no additional computation cost. Note that the additional cost of our training algorithm is due to projection (i.e. the model) rather than the new sampling procedure.\n\n3. WRT to the projection cost: \nsee Appendix F.2.\n", "title": "Re: no title"}, "S10ifp3rl": {"type": "rebuttal", "replyto": "Hyd8QeSVl", "comment": "1. WRT binary RBM result: \nwe put the result and the discussion in the revision. See Appendix F.3. We study both toy datasets and two benchmarks, including MNIST and Caltech 101.\n\n2. WRT Baseline comparison (Nair & Hinton, and spike-and-slab RBM): \nBased on Ravanbakhsh et al.\u201916, the standard ReLU does not result in valid exponential family RBM and therefore even if it defines a valid distribution we do not know a close form for its joint distribution p(v,h) and marginals p(v). This also means we cannot evaluate the likelihood of standard ReLU RBM as defined in Nair & Hinton\u201910. The heuristic sampling technique used in that paper samples using max(0, x+N(0, \u03c3(x)). In expectation, this corresponds to a monotonic activation function f(x) = E[max(0, x+N(0, \u03c3(x))], which does not have a simple analytic form. Therefore we do not know the energy function associated with their sampling heuristic, which in turn prevents evaluation of the likelihood in that case. See also answer to a similar comment from reviewer 1. The other extension, such as spike-and-slab RBM, is based on Nair & Hinton\u201910. With the elementary calculation, the marginal distribution has one more integration without closed form than leaky RBM and it seems non-trivial to estimate it directly.  Therefore, the likelihood evaluation is also unknown to the community. In the original papers of both works, they all consider the classification results only. \nWe are the first work to show the baseline performance of RBM with the commonly-used (leaky) ReLU activation function and demonstrate that it could outperform Boolean hidden units.\n\n3. WRT to the novelty of this paper: \nThe analysis of this paper is based on Ravanbakhsh et., al (2016) and Yang et., al (2012). The first contribution of this paper is connecting the leaky ReLU RBM and the notion of \u201cunion\u201d of truncated Gaussian distribution (Section 3.1). We then propose the novel sampling strategy by utilizing the property of Gaussian distribution which benefits the partition function estimation (Section 4). The next contribution is we are able to give the baseline performance of RBM with the commonly-used (leaky) ReLU activation function in terms of the likelihood.We further show that the proposed sampling is better than the conventional Gibbs sampling by studying the optimization performance (Section 5). \n", "title": "Re: A new model of RBM is proposed, where the conditional of the hidden is a leaky ReLU. In addition an annealed AIS sampler is also proposed to test the learned models quantifiably"}, "SJkY7p2Bg": {"type": "rebuttal", "replyto": "H1GEvHcee", "comment": "We thank all reviewers for their careful reading of the paper and constructive feedback. We have extensively revised the paper to address these requests for clarifications and experiments. Here, we explain some of these and point to related updates in our revision. Please check the individual replies below the reviews.\n", "title": "Upload revision"}, "H19Wmp3Bg": {"type": "rebuttal", "replyto": "Hy5_Hg5Vl", "comment": "1. WRT the correctness and the computational cost: \nFor the correctness, please see Appendix E, where we discuss the connection between annealing the leakiness and energy. We show that annealing the leakiness is the special case of annealing the energy by properly choosing a special distributions as the initial distribution. The theory of general AIS has been studied by several literatures (e.g. Neal (1996)). Therefore, those theoretical guarantees without strong assumptions on the initial distributions are all applicable to sampling by annealing leakiness.  \nFor the computational concern, the inner loop is the same as standard CD except for one more flip-flops to decrease leakiness parameter in every iteration, which can be ignored compared with the projection step. The computational time of inner loop and the projection step is shown in Appendix F.2.\n\n2. WRT the GRBM:\nIn Section 4.2, we mention our implementation can reproduce the exactly same results as the code provided by http://www.cs.toronto.edu/~tang/code/GaussianRBM.m\nAlthough we tried to grid learning rate between 1e-1~1e-6 with hand-tuned learning rate decay schedule and the momentum parameter between 0.9~0.1, we cannot reproduce the result from the same group (e.g. Tang et., al (2012)).  As we mention in the paper, we conjecture the  unsatisfactory results are caused by the parameter tuning and well-designed learning weight decay. We would appreciate if anyone can provide the right parameters to us and we can rerun the experiments.This also shows the difficulty of training Gaussian RBM with Birnary units.  \n", "title": "Re: no title"}, "rkwxJw17g": {"type": "rebuttal", "replyto": "SJx-ZRAzx", "comment": "\n1 WRT step-by-step derivation: We will revise the description of Sec 2 and Sec 3, and put more detailed derivation in the appendix. The revision will come in the next week.\n\n2: WRT f: Note that the non-linearity $f$ of the unit translates to the mean of the conditional probability of the activation in ReLU RBM.\n\n3 WRT base measure: The base measure g (aka carrier measure) in exponential family distributions ensures that the distribution is well-defined (this is necessary when using the conventional notation where the log-normalization constant, F, is defined to be the integral of the transfer function). \n\n4 WRT projection step:  One of our contributions in this paper is to explicitly show that the SDP constraint is necessary for training leaky ReLU RBM when there is \u201cunbounded\u201d regions (see fig 1). We will move some of the Appendix C that elaborates on this to the main body. One standard way to deal with SDP constraint is using the SVD to do projection directly as we proved in Theorem 2. The reviewer is correct in pointing out the computational burden of SVD. We also observed that scaling the algorithm to thousands of hidden units is not practical. Therefore, as we also point out in the conclusion, a more advanced optimization algorithm, such as Hsieh et al.\u201911, is highly motivated. For example, the method of of Hsieh et al.\u201911 solves the Graphical lasso problem by avoiding the projection steps with barrier functions and  careful line search step. However, we leave computational improvements on the projection step to future work.\n\n5 WRT the figure 4: As we run algorithms to convergence,  the likelihood will improve as we show the Table 3. This portion of iterations were chosen for better visualization. We will add the result for comparing CPU time in the revision as requested. Please note that the complexity of CD and Mix are the almost the same. Mix only need a few more constant time steps which can be ignored compared with sampling steps. \u201cLeaky\u201d is more time-consuming because of computing and decomposing the covariance matrix as we discussed in Sec 5. \n\n6 WRT Nair & Hinton\u201910: Based on Ravanbakhsh et al.\u201916, the standard ReLU does not result in valid exponential family RBM and therefore even if it defines a valid distribution we do not know a close form for its joint distribution p(v,h) and marginals p(v). This also means we cannot evaluate the likelihood of standard ReLU RBM as defined in Nair & Hinton\u201910. We are the first work to show the baseline performance of RBM with (leaky) ReLU activation function and demonstrate that it could outperform Boolean hidden units.\n", "title": "Re: Derivations and baselines"}, "HyE6R4mXl": {"type": "rebuttal", "replyto": "S1CLjJ-7g", "comment": "Thanks for your feedback. We will clarify the role of the integral of f^{-1} in the text. The revision will come in next few days.\n\nWe are aware of the difficulties in training Gaussian RBM and similar to Nair&Hinton\u201910 our goal here is to improve Gaussian RBM. However, as opposed to Nair&Hinton\u201910 we are interested in the \u201cgenerative modelling\u201d and proper evaluation of the likelihood. In all experiments, we use exponential family with \u201cunit\u201d dispersion parameter (corresponding to $\\sigma=1$ for Gaussian), and we show consistent evaluation of Bernoulli and ReLU RBM by comparison to the \u201cexact\u201d partition function in some settings (see Table 2).\n\nNote that we study the RBM with \u201cGaussian visible units\u201d; we study the transformation from full-Gaussian (with efficient sampling) to Relu-Gaussian RBM and this sampling strategy as well as our truncated Gaussian interpretation does not carry over to Bernoulli visible units. Please see Section 3.1. ", "title": "RE: Clarity and better comparisons"}, "S1CLjJ-7g": {"type": "review", "replyto": "H1GEvHcee", "review": "It would be nice if the authors can clarify the anti-derivative of f^-1 (right before equation 3)\nGaussian RBMs are notoriously hard to train, and the AIS estimation is depending on \\sigma as well as the initial AIS factorial distribution. Also due to the fact that the visibles is continuous, the nats are log probability DENSITIES rather than log probability, which means they are very sensitive the the residual noise model \\sigma.\n\nInfact, the Stepped sigmoid units or ReLU from Nair and Hinton 10 was motivated by how to improve Gaussian RBMs.\n\nA better and more convincing experiment is to use bernoulli visible units buy Relu/leaky relu hiddens as you have here, but train it on MNIST and report log-likelihood in Nats.\nThis is well studied and a typical RBM would get around -84 nats. In addition, samples of MNIST would be good as well.\n\nBased on previous work such as the stepped sigmoid units and ReLU hidden units for discriminatively trained supervised models, a Leaky-ReLU model is proposed for generative learning.\n\nPro: what is interesting is that unlike the traditional way of first defining an energy function and then deriving the conditional distributions, this paper propose the forms of the conditional first and then derive the energy function. However this general formulation is not novel to this paper, but was generalized to exponential family GLMs earlier.\n\nCon: \nBecause of the focus on specifying the conditionals, the joint pdf and the marginal p(v) becomes complicated and hard to compute.\n\nOn the experiments, it would been nice to see a RBM with binary visbles and leaky ReLu for hiddens. This would demonstrate the superiority of the leaky ReLU hidden units. In addition, there are more results on binary MNIST modeling with which the authors can compare the results to. While the authors is correct that the annealing distribution is no longer Gaussian, perhaps CD-25 or (Faast) PCD experiments can be run to compare agains the baseline RBM trained using (Fast) PCD.\n\nThis paper is interesting as it combines new hidden function with the easiness of annealed AIS sampling, However, the baseline comparisons to Stepped Sigmoid Units (Nair &Hinton) or other models like the spike-and-slab RBMs (and others) are missing, without those comparisons, it is hard to tell whether leaky ReLU RBMs are better even in continuous visible domain.\n\n", "title": "Clarity and better comparisons", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hyd8QeSVl": {"type": "review", "replyto": "H1GEvHcee", "review": "It would be nice if the authors can clarify the anti-derivative of f^-1 (right before equation 3)\nGaussian RBMs are notoriously hard to train, and the AIS estimation is depending on \\sigma as well as the initial AIS factorial distribution. Also due to the fact that the visibles is continuous, the nats are log probability DENSITIES rather than log probability, which means they are very sensitive the the residual noise model \\sigma.\n\nInfact, the Stepped sigmoid units or ReLU from Nair and Hinton 10 was motivated by how to improve Gaussian RBMs.\n\nA better and more convincing experiment is to use bernoulli visible units buy Relu/leaky relu hiddens as you have here, but train it on MNIST and report log-likelihood in Nats.\nThis is well studied and a typical RBM would get around -84 nats. In addition, samples of MNIST would be good as well.\n\nBased on previous work such as the stepped sigmoid units and ReLU hidden units for discriminatively trained supervised models, a Leaky-ReLU model is proposed for generative learning.\n\nPro: what is interesting is that unlike the traditional way of first defining an energy function and then deriving the conditional distributions, this paper propose the forms of the conditional first and then derive the energy function. However this general formulation is not novel to this paper, but was generalized to exponential family GLMs earlier.\n\nCon: \nBecause of the focus on specifying the conditionals, the joint pdf and the marginal p(v) becomes complicated and hard to compute.\n\nOn the experiments, it would been nice to see a RBM with binary visbles and leaky ReLu for hiddens. This would demonstrate the superiority of the leaky ReLU hidden units. In addition, there are more results on binary MNIST modeling with which the authors can compare the results to. While the authors is correct that the annealing distribution is no longer Gaussian, perhaps CD-25 or (Faast) PCD experiments can be run to compare agains the baseline RBM trained using (Fast) PCD.\n\nThis paper is interesting as it combines new hidden function with the easiness of annealed AIS sampling, However, the baseline comparisons to Stepped Sigmoid Units (Nair &Hinton) or other models like the spike-and-slab RBMs (and others) are missing, without those comparisons, it is hard to tell whether leaky ReLU RBMs are better even in continuous visible domain.\n\n", "title": "Clarity and better comparisons", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SkU4kvyQx": {"type": "rebuttal", "replyto": "BJaN3T0Gx", "comment": "We will include the sampled images in the appendix. The revision will come in the next week.\n\nHowever, one should note that single layer RBM does not adequately model CIFAR10 and SVHN (considered here) when compared to multilayer models. Moreover the difference in sample quality (between leaky ReLU and conventional RBM) are not visually significant. We instead focused on quantitative evaluation of the log-likelihood in this paper (which shows a significant improvement).\n", "title": "Re: Why not plot generated samples by the Leaky-ReLU RBM?"}, "SJx-ZRAzx": {"type": "review", "replyto": "H1GEvHcee", "review": "Could you please clarify the notation and provide additional derivations for Sec 2 and 3 ? Typically, one works backwards from the energy function to derive the conditionals, and associated non-linearities. The work of Ravanbaksh et al 2016 seems to allow for the reverse direction (from non-linearity to energy), but I find that the authors do not provide sufficient background material for me to understand the approach. I believe the paper should as self-contained as possible. The notation and description could also be improved: f is clearly the non-linearity of the hidden units, yet it is described as \"the conditional probability of the activation\". What is the base measure g? \\tilde{F}^* of Eq. 5 is not defined. In general, the paper could greatly benefit from a detailed appendix which shows how one goes from the leaky-relu non-linearity over h to Eqs 3-4-5.\n\nI would also ask the authors to further discuss the projection method in the main body of the paper. What is the cost of this projection step ? If an SVD is required per gradient evaluation, this would make the algorithm cubic in the layer size, i.e. prohibitively expensive.\n\nRegarding the results of Fig 4, why did the authors not plot the curves to convergence? A likelihood vs cpu-time curve would also help illustrate the cost of each sampling method, and account for the projection step. A direct comparison to Nair & Hinton 2010 is also noticeably absent.The authors propose a novel energy-function for RBMs, using the leaky relu max(cx, x) activation function for the hidden-units. Analogous to ReLU units in feed-forward networks, these leaky relu RBMs split the input space into a combinatorial number of regions, where each region defines p(v) as a truncated Gaussian. A further contribution of the paper is in proposing a novel sampling scheme for the leaky RBM: one can run a much shorter Markov chain by initializing it from a sample of the leaky RBM with c=1 (which yields a standard multi-variate normal over the visibles) and then slowly annealing c. In low-dimension a similar scheme is shown to outperform AIS for estimating the partition function. Experiments are performed on both CIFAR-10 and SVHN.\n\nThis is an interesting paper which I believe would be of interest to the ICLR community. The theoretical contributions are strong: the authors not only introduce a proper energy formulation of ReLU RBMs, but also a novel sampling mechanism and an improvement on AIS for estimating their partition function. \n\nUnfortunately, the experimental results are somewhat limited. The PCD baseline is notably absent. Including (bernoulli visible, leaky-relu hidden) would have allowed the authors to evaluate likelihoods on standard binary RBM datasets. As it stands, performance on CIFAR-10 and SVHN, while improved with leaky-relu, is a far cry from more recent generative models (VAE-based, or auto-regressive models). While this comparison may be unfair, it will certainly limit the wider appeal of the paper to the community. Furthermore, there is the issue of the costly projection method which is required to guarantee that the energy-function remain bounded (covariance matrix over each region be PSD). Again, while it may be fair to leave that for future work given the other contributions, this will further limit the appeal of the paper.\n\nPROS:\nIntroduces an energy function having the leaky-relu as an activation function\nIntroduces a novel sampling procedure based on annealing the leakiness parameter\nSimilar sampling scheme shown to outperform AIS\n\nCONS:\nResults are somewhat out of date\nMissing experiments on binary datasets (more comparable to prior RBM work)\nMissing PCD baseline\nCost of projection method\n", "title": "Derivations and baselines", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SkSNwcVEl": {"type": "review", "replyto": "H1GEvHcee", "review": "Could you please clarify the notation and provide additional derivations for Sec 2 and 3 ? Typically, one works backwards from the energy function to derive the conditionals, and associated non-linearities. The work of Ravanbaksh et al 2016 seems to allow for the reverse direction (from non-linearity to energy), but I find that the authors do not provide sufficient background material for me to understand the approach. I believe the paper should as self-contained as possible. The notation and description could also be improved: f is clearly the non-linearity of the hidden units, yet it is described as \"the conditional probability of the activation\". What is the base measure g? \\tilde{F}^* of Eq. 5 is not defined. In general, the paper could greatly benefit from a detailed appendix which shows how one goes from the leaky-relu non-linearity over h to Eqs 3-4-5.\n\nI would also ask the authors to further discuss the projection method in the main body of the paper. What is the cost of this projection step ? If an SVD is required per gradient evaluation, this would make the algorithm cubic in the layer size, i.e. prohibitively expensive.\n\nRegarding the results of Fig 4, why did the authors not plot the curves to convergence? A likelihood vs cpu-time curve would also help illustrate the cost of each sampling method, and account for the projection step. A direct comparison to Nair & Hinton 2010 is also noticeably absent.The authors propose a novel energy-function for RBMs, using the leaky relu max(cx, x) activation function for the hidden-units. Analogous to ReLU units in feed-forward networks, these leaky relu RBMs split the input space into a combinatorial number of regions, where each region defines p(v) as a truncated Gaussian. A further contribution of the paper is in proposing a novel sampling scheme for the leaky RBM: one can run a much shorter Markov chain by initializing it from a sample of the leaky RBM with c=1 (which yields a standard multi-variate normal over the visibles) and then slowly annealing c. In low-dimension a similar scheme is shown to outperform AIS for estimating the partition function. Experiments are performed on both CIFAR-10 and SVHN.\n\nThis is an interesting paper which I believe would be of interest to the ICLR community. The theoretical contributions are strong: the authors not only introduce a proper energy formulation of ReLU RBMs, but also a novel sampling mechanism and an improvement on AIS for estimating their partition function. \n\nUnfortunately, the experimental results are somewhat limited. The PCD baseline is notably absent. Including (bernoulli visible, leaky-relu hidden) would have allowed the authors to evaluate likelihoods on standard binary RBM datasets. As it stands, performance on CIFAR-10 and SVHN, while improved with leaky-relu, is a far cry from more recent generative models (VAE-based, or auto-regressive models). While this comparison may be unfair, it will certainly limit the wider appeal of the paper to the community. Furthermore, there is the issue of the costly projection method which is required to guarantee that the energy-function remain bounded (covariance matrix over each region be PSD). Again, while it may be fair to leave that for future work given the other contributions, this will further limit the appeal of the paper.\n\nPROS:\nIntroduces an energy function having the leaky-relu as an activation function\nIntroduces a novel sampling procedure based on annealing the leakiness parameter\nSimilar sampling scheme shown to outperform AIS\n\nCONS:\nResults are somewhat out of date\nMissing experiments on binary datasets (more comparable to prior RBM work)\nMissing PCD baseline\nCost of projection method\n", "title": "Derivations and baselines", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJaN3T0Gx": {"type": "review", "replyto": "H1GEvHcee", "review": "Please plot some samples generated by Leaky-ReLU RBM. The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.  A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.\n\n It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work.\n 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.\n 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.\n 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.", "title": "Why not plot generated samples by the Leaky-ReLU RBM? ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Hy5_Hg5Vl": {"type": "review", "replyto": "H1GEvHcee", "review": "Please plot some samples generated by Leaky-ReLU RBM. The authors proposed to use leaky rectified linear units replacing binary units in Gaussian RBM.  A sampling method was presented to train the leaky-ReLU RBM. In the experimental section, AIS estimated likelihood on Cifar10 and SVHN were reported.\n\n It's interesting for trying different nonlinear hidden units for RBM. However, there are some concerns for the current work.\n 1. The author did not explain why the proposed sampling method (Alg. 2) is correct. And the additional computation cost (the inner loop and the projection) should be discussed.\n 2. The results (both the resulting likelihood and the generative samples) of Gaussian RBM are much worse than what we have experienced. It seems that the Gaussian RBM were not trained properly.\n 3. The representation learned from a good generative model often helps the classification task when there are fewer label samples. Gaussian RBM works well for texture synthesis tasks in which mixing is an important issue. The authors are encouraged to do more experiments in these two direction.", "title": "Why not plot generated samples by the Leaky-ReLU RBM? ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}