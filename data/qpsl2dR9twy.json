{"paper": {"title": "Communication in Multi-Agent Reinforcement Learning: Intention Sharing", "authors": ["Woojun Kim", "Jongeui Park", "Youngchul Sung"], "authorids": ["~Woojun_Kim1", "~Jongeui_Park1", "~Youngchul_Sung1"], "summary": "This paper propose a new communication scheme named intention sharing to enhance the coordination among agents.", "abstract": "Communication is one of the core components for learning coordinated behavior in multi-agent systems.\nIn this paper, we propose a new communication scheme named  Intention Sharing (IS) for multi-agent reinforcement learning in order to enhance the coordination among agents. In the proposed IS scheme, each agent generates an imagined trajectory by modeling the environment dynamics and other agents' actions. The imagined trajectory is the simulated future trajectory of each agent based on the learned model of the environment dynamics and other agents and represents each agent's future action plan. Each agent compresses this imagined trajectory capturing its future action plan to generate its intention message for communication by applying an attention mechanism to learn the relative importance of the components in the imagined trajectory based on the received message from other agents. Numeral results show that the proposed IS scheme outperforms other communication schemes in multi-agent reinforcement learning.", "keywords": ["Multi-agent reinforcement learning", "communication", "intention", "attention"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors study co-ordination in multi-agent systems. Specifically they propose a scheme where agents model future trajectories through the environment dynamics and other agents' actions, they then use this to form a plan which forms the agents' intention which is then communicated to the other agents.\n\nThe major concerns raised by the reviewers were around novelty, lack of ablations and significance of results as improvements were modest. During the rebuttal, the authors have extended their work with ablations and have conducted a statistical test. While it is true the current results present a small improvement, i think this is an interesting contribution in the field of emergent communication"}, "review": {"K0iZIto-phI": {"type": "review", "replyto": "qpsl2dR9twy", "review": "---- Summary ----\nThe paper proposes a new method for training communication in multiagent systems. The method involves agents producing imagined future trajectories using learned environment dynamics, and then communicating some parts of these trajectories to other agents. This communication scheme is evaluated on several cooperative communication domains, outperforming other methods.\n\n---- Reasons for score ----\nThe paper addresses an important problem, the method is sensible, and the empirical results suggest the method proposed works. My main concern is a lack of ablations, which makes it hard to tell which parts of the method are important.\n\n---- Pros ----\n1. The problem is well motivated. The existing literature on learned multi-agent communication primarily addresses the problem of communicating information about partially observed states. However, this is not all there is to communication, and the communication of agent intentions is an important topic to address.\n\n2. The method suggested - basing messages on predicted future trajectories - is sensible and well motivated.\n\n3. The experiments compare the method to a good range of baseline algorithms for multiagent communication, and it performs well.\n\n---- Cons/Questions ----\nThe method proposed has several moving parts, and without ablations it is hard to tell which are the most important pieces. There are a few experiments I would like to see added to work out what is important in the method:\n* A comparison to the uniform attention case. While the qualitative analysis suggests the attention mechanism is meaningfully used, it would be good to show how this affects performance.\n* An ablation where all the attention is put on the first timestep, to show the effect of the imagined trajectories. While this is similar to the other communication methods discussed in the paper, I am not sure whether it directly corresponds to any of them.\n* A comparison to a case where the agents receive only their own messages to condition their next actions. At the moment is unclear whether the advantage is gained from sharing intentions between agents, or by having a prediction of the future.\n\nAll the environments used are modifications of existing environments; was there a reason to change these? Particularly, the Cooperative-Navigation environment looks like it could have been used as-is, for an easier comparison to previous work?\n\n---- Post-rebuttal ----\nThe additional ablation studies on the attention and MADDPG-p are helpful, and address most of my concerns - though it is still not clear to me whether any of the baselines compared to is exactly equivalent to the method used with attention weights (1, 0, 0, ...). I share the concerns of Reviewer 4 about the significance of the results - this is still not in the paper, and not present for the new experiments. Overall, I have not changed my rating.", "title": "Interesting method for an important problem. Would be improved by ablations to know what matters.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "37PtXFsUTvG": {"type": "review", "replyto": "qpsl2dR9twy", "review": "[Summary]\n\nPaper proposed to generate the communication message in MARL with the predicted trajectories of all the agents (include the agent itself). An extra self-attention model is also stacked over the trajectories to trade off the length of prediction and the possible explaining away issue.  The whole model is trained via a canonical MARL objective while the trajectory prediction model utilizes direct supervision collected from the environments. Experiments on several toy MARL benchmark demonstrates the effectiveness of the proposed method.\n\n[Strengh]\n\n+) The idea of communication with imagined intention is motivated properly with rich psychological background and also technically sound.\n\n+) The paper is overall clear and well-written. I found there are enough technical details to reproduce the main results of the main approach.\n\n+) Still limited though in terms of the converted domains, the empirical evaluations deliver impressive results over a reasonable collection of counterparts.\n\n[Weakness]\n\nThe main concerns I have with this submission lies in the overall novelty and the evaluation of the proposed method. After reading this paper, indeed I find the authors failed to capture some important research in this narrow area and it's still not clear how does the proposed method really works and whether it is sensitive to some specific implementing factors.\n\n-) The idea of intention sharing based on prediction is essentially not novel esp. in the MARL domain. In the CogSci & AI community, theory-of-mind (ToM) and its application in multi-agent execution (either collaborative or zero-sum games) have been well studied for years [1, 2, 3]. Although I may agree that there are few prior works [4] on sending these predictions as messages to other agents, it does not really introduce that many new ideas to how collaborative or adversarial agents could benefit from such ToM-based intention prediction. However, the authors failed to capture these counterparts in their discussion and evaluations. Specifically, I would like to see how does the proposed method formulate its intention prediction differently than the prior work and whether it could enjoy advantages in performance with such differences.\n\n-) The proposed method is essentially quite complex (stacked prediction, transformer, etc) than its Bayesian counterparts, while the authors only provide an overall evaluation against several MARL baselines. It will be critical to also conduct a serious ablation study given the overall complexity of the proposed method, i.e. whether to use the transformer, the architectural choice of the transformer (num. of heads, etc), and the length of predicted trajectories (H). Also, the disagreement between the predicted trajectories and actual observation and its relation to the performance should also be investigated.\n\n-) Although the selected tasks are all canonical to MARL, given the fact a growing number of recent MARL learners have been moved on to more challenging tasks, I feel it would be necessary to include some mini MOBA games or other tasks with vision-based observations. \n\n[1] Bayesian models of human action understanding\n\n[2] Theory-based Social Goal Inference\n\n[3] Bayesian Theory of Mind: Modeling Joint Belief-Desire Attribution\n\n[4] Machine theory of mind\n\n[Suggestions&Questions]\n\n(1) Add ablation studies on the use of self-attention model and the length of prediction (H).\n\n(2) Visualize&compare the predicted and observed trajectories, add some discussion on how such disagreement would affect the performances.\n\n(3) (minor) Try the proposed method on mini MOBA games or tasks with vision-based observations.\n\n(4) Add citations to the related work on ToM and its application in MAS.\n\n[Post-rebuttal]\n\nI have read through all the other reviews and the rebuttal. Would like to thank the authors for their efforts in improving this submission. However, the main issue on the lack of novelty remains and I also find R4's concern on the significance of results is valid. Therefore, I will keep my initial justification as is. \n", "title": "Review for paper \"Communication in Multi-Agent Reinforcement Learning: Intention Sharing\"", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "1njA1TRHroG": {"type": "review", "replyto": "qpsl2dR9twy", "review": "Summarization and Strengths:\nThis paper studies how to learn coordinated behavior among multiple agents by learning a communication protocol. Specifically, compared to existing works, this paper proposes to generate messages based on not only current information but also future information (referred to as imagined trajectory) (Section 4.1). Additionally, the attention module in Section 4.2 is learned to weigh between current and future information dynamically. Overall, the paper is well written, clearly explains the proposed approach, and compares to appropriate state-of-the-art baseline methods.\n\nWeaknesses: \nWhile the motivation of this work is clear, the experimental results fail to support the claim. The results in Figure 3 show a small performance difference between IS (proposed approach) and baselines due to the large variance. Performing a statistical test, such as the t-test, to verify whether the proposed approach achieves statistically significant results than baselines will be helpful. \n\nFurther Questions:\n1. In general, learning the attention mechanism improves results, but is it necessary for the proposed approach? Adding an abbreviation analysis, such as the performance difference between with and without attention, will be helpful.\n2. The action predictor (Equation (2)) predicts the peer agents' actions based on the agent's own observation. While the peer action prediction based on the agent's own observation supports the decentralized execution, the local observation may not include sufficient information about the others depending on a multiagent domain and thus the action prediction can fail. Instead, wouldn't it be better to predict the peer agents' actions based on both the agent's own observation and received message m_{t-1} because the received message can contain useful information about the peers' behaviors? \n3. As noted in footnote 1, the message at m_{t-1} is used for all H prediction steps. How does the performance differ with respect to different H values?\n\nMinor comments: \n1. Typo in Page 1: ''How to harness ... partial observation.'' -> ``How to harness ... partial observation.''\n2. In Equation (2), the subscript \"t\" is missing (e.g., \\hat{a}^{i-1} -> \\hat{a}^{i-1}_{t})\n3. Adding pseudo algorithm in Appendix will further clarify the proposed approach\n4. For PP domain explanation (Section 5.1), it is unclear whether prey can run away from predators. In general, preys can run away in PP, but in this paper, preys seem to be fixed and cannot move around.\n5. For PP and CN domain explanation (Section 5.1), it is unclear what each agent observes.\n6. The intention prediction is also related to the theory of mind and opponent modeling (e.g., He et al., ICML-16, Raileanu et al., ICML-18, Rabinowitz et al., ICML-18), which can be added in the related work section (Section 2).\n\nHe He, Jordan Boyd-Graber, Kevin Kwok, Hal Daum\u00e9 III. Opponent Modeling in Deep Reinforcement Learning. ICML-16\n\nRoberta Raileanu, Emily Denton, Arthur Szlam, Rob Fergus. Modeling Others using Oneself in Multi-Agent Reinforcement Learning. ICML-18\n\nNeil C. Rabinowitz, Frank Perbet, H. Francis Song, Chiyuan Zhang, S.M. Ali Eslami, Matthew Botvinick. Machine Theory of Mind. ICML-18\n\nI have read over the rebuttal and discussion and will keep my evaluation score as it was since the concerns about the weak performance result still remain.\n", "title": "Paper is well written, clearly explains the proposed approach, compares to appropriate state-of-the-art baseline methods, but there is significant room for improvement in the experimental results", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Qr0XjvNnZS": {"type": "rebuttal", "replyto": "qpsl2dR9twy", "comment": "We thank all reviewers for the valuable comments. We revised our paper based on the reviewers' comments and here is our response to the common comments.\n\n- We added ablation studies on the Attention Module (AM) and the length of the imagined trajectory H as Fig. 4.  It is observed that ITGM without AM increases the training speed as compared to the MADDPG baseline. The final performance by ITGM without AM is almost the same as that of the baseline in the PP environment whereas the final performance by ITGM without AM increases in the TJ environment. Please see Fig. 4 in the revised paper. Furthremore, if we add AM to ITGM, then both the training speed and the final performance are increased. Hence, using both ITGM and AM is important.   \n    \n- We added Theory of Mind (ToM) and Opponent Modeling (OM) in the related work section. We expect that ToM and OM can be used for our action predictor. We summarized the difference between the proposed IS scheme and (ToM, OM) as follows:\n    (a) Our approach uses communication to share the intention, whereas ToM and OM infer other agents' goal or action capturing the intention.\n     (b) Our approach uses future information by rolling-out the policy, whereas ToM and OM infer only the current or just next time-step information. \n- We conducted an ablation study to examine whether performance improvement is gained from sharing intention or having a prediction of the future in Appendix C.\n-  We added the pseudo-code in Appendix D.\n-  We conducted the PP environment with additional 10 different seeds (The performance is averaged over 20 different seeds). \n", "title": "Common Response "}, "WlnKmGkzywQ": {"type": "rebuttal", "replyto": "K0iZIto-phI", "comment": "Thank you for the valuable comments. Here is our reply.\n\nRegarding additional ablation studies: \n- As mentioned in the common response, we added ablation studies regarding AM and the length of the imagined trajectory H. Please see Fig. 4 in the revised paper.\n\n\nRegarding the modifications of existing environments:\n- We modified the existing environments to  require more coordination among agents. As mentioned in the main paper, the agents in the PP environment should learn to spread out as well as capture the preys in order to gain more rewards. In addition, we increased the size of the agent in the CN environment so that the agent has to be more careful not to crash with other agents.\n\nRegarding \"all the attention is put on the first timestep\": \n- As mentioned in the main paper, the previously proposed communication schemes are the special case corresponding to $\\alpha=(1,0,\\cdots, 0)$, which means that all the attention is put on the first timestep. Especially, DIAL sends the learned features of the information in the first time step as messages. \n\nRegarding \"A comparison to a case where the agents receive only their own messages to condition their next actions.\"\n- We compared the proposed IS scheme with the case in which the agents receive only their own messages to condition their next actions, and the result is provided in Appendix D. It is seen that MADDPG-p in which each agent uses own imagined trajectory as an additional input yields better performance as compared to the MADDPG baseline. However, the proposed IS scheme outperforms MADDPG-p. Thus, sharing intention, which is a core idea of this paper, is more important than having a prediction of the future.\n", "title": "Response to Reviewer 4"}, "hCsxoX9O2jZ": {"type": "rebuttal", "replyto": "1njA1TRHroG", "comment": "Thank you for the valuable comments. As mentioned in the common response, we added ablation studies regarding AM and the length of the imagined trajectory. In addition, we have included ToM and OM in the revised related work section. Here is our reply:\n\nRegarding the action predictor: \n-  We agree on that the received messages can be helpful to predict other agents' actions, especially in partially observable environments. The action predictor in our approach takes only its own observation as input and is trained in the manner of supervised learning. However, the action predictor can be replaced with the previously proposed opponent modeling approach and can take the received messages, as the reviewer mentioned. We added the flexibility of choosing one of other methods as the action predictor into the revised paper.\n\nRegarding the PP and CN environment:\n- The preys in our PP environment are fixed. We focus on the predators: The predators should learn to spread out in order to capture the prey ($N=2, N=3$) and the predators in the PP ($N=4$) should learn to capture prey in a group of two. The agent in the PP environment observes the positions of predators and preys and the agent of the CN environment observes the positions of agents and landmarks.\n\nRegarding a statistical test:\n- We used deterministic evaluation based on 20 episodes generated by the corresponding deterministic policy after the training ends. We conducted a pairwise t-test to verify the proposed IS scheme on the PP environment: (a) In the PP (N=2) environment, the proposed IS scheme outperforms MADDPG and Comm-OA with 95\\% confidence level (CL) and outperforms DIAL and ATOC with 80\\% confidence level whereas the performance of TarMAC is similar with the proposed IS scheme. (b) In the PP (N=3) environment, the proposed IS scheme outperforms all baselines with 99\\% confidence level. (c) In the PP (N=4) environment, the proposed IS scheme outperforms Comm-OA, ATOC, and TarMAC with 95\\% confidence level and outperforms MADDPG and DIAL with 90\\% confidence level.", "title": "Response to Reviewer 1 "}, "S299PhWJ7G_": {"type": "rebuttal", "replyto": "37PtXFsUTvG", "comment": "Thank you for the valuable comments. Here is our reply:\n\nRegarding the novelty of our proposed method:\n- We agree on that the notion of intention was studied in the MARL community. For example, in Theory of Mind (ToM) and Opponent Modeling (OM), each agent  infers other agents' goal or action or belief, which captures  other agents' intention. \n    It has been shown in these works that the inference of other agents' goal or action or belief can improve the performance. In our work, instead of inference, we use communication to share the agents' intention. Unlike the above inference-based  approaches, the agent in our approach generates their own intention and use it as the messages. In addition, the previous approaches infer the current (or just next-time step) information of other agents.     However, we use the imagined trajectory, which captures the future behavior far beyond the current time step. To the best of our knowledge, our work is the first that introduces communication for sharing the intention among agents.\n\nRegarding ablation studies on AM and the length of the imagined trajectory H: \n- As mentioned in the common response, we added ablation studies regarding the attention module and the length of imagined trajectory H. Please see Fig. 4 in the revised paper.\n\nRegarding \"Visualize and compare the predicted and observed trajectories, add some discussion on how such disagreement would affect the performances.\":\n- We have already provided a visualization of the imagined trajectory and the observed trajectory in Fig. 5. It is difficult to precisely analyze the influence of the disagreement on the performance, but it is observed in Fig. 5 that the disagreement often occurs in the early stage of episode since the agents do not receive any messages from other agents at the first step of each episode.  It is also observed in Fig. 5 that the disagreement decreases as time goes. This means that the agents know other agents' plan ", "title": "Response to Reviewer 3"}, "ZalTguO3x4": {"type": "rebuttal", "replyto": "HMLeBTQRP5e", "comment": "Thank you for the valuable comments. Here is our reply:\n\nRegarding the use of intention in the literature: \n- As mentioned in the common response, we added  Theory of Mind (ToM) and Opponent Modeling (OM), which use the notion of intention, in the related work section. However, please see the common response regarding the difference of our method from these existing methods.\n\nRegarding more ablation studies:\n- As mentioned in the common response, we added ablation studies regarding AM and the length of imagined trajectory H into the revised paper.\n\nRegarding the rationale of determining who maintain the goal or who change the plan: \n- Intuitively, we think that some agents whose policy is sensitive to the received messages (e.g. the weights connected to the received messages are quite large) will be trained to be the agents who tend to change the plan, other agents whose policy is less sensitive to the received messages will be trained to be the agents who tend to maintain the plan.\n", "title": "Response to Reviewer 2"}, "HMLeBTQRP5e": {"type": "review", "replyto": "qpsl2dR9twy", "review": "Summary: This work proposes to use 'intention' of each agent to enhance the message sharing scheme of MARL. For training, MADDPG is used as a backbone MARL and implement ITGM and AM on top of it. The proposed method is compared with several baseline approaches from three different environments.\n\nStrengths:\n+ The paper is well-written and technically sounds. \n+ The motivation is clear.\n\nWeaknesses:\n- This work wants to see the effectiveness of the use of intention as a communication scheme under MARL. However, in general, this is not the first to use intention. I suggest to review some relevant works and show how 'intention' has been explored in a similar/different way in the literature. \n- Although visualization of imagined trajectory is given in Figure 4, it does not fully demonstrate the idea of this approach. More qualitative evaluation seems required to validate from various aspects. \n- It seems that attention is used to capture which waypoint is more important in an imagined trajectory rather than whose imagined trajectory is more important. If so, is it really important? Where is the ablative study of using attention?\n- Intuitively, when two agents set the same goal (catching same prey), what is the rationale of determining who maintain the goal or who change the plan? ", "title": " well-written and technically sounds", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}