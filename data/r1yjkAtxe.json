{"paper": {"title": "Spatio-Temporal Abstractions in Reinforcement Learning Through Neural Encoding", "authors": ["Nir Baram", "Tom Zahavy", "Shie Mannor"], "authorids": ["nirb@campus.technion.ac.il", "tomzahavy@campus.technion.ac.il", "shie@ee.technion.ac.il"], "summary": "A method for understanding and improving deep agents by creating spatio-temporal abstractions", "abstract": "Recent progress in the field of Reinforcement Learning (RL) has enabled to tackle bigger and more challenging tasks. However, the increasing complexity of the problems, as well as the use of more sophisticated models such as Deep Neural Networks (DNN), impedes the understanding of artificial agents behavior. In this work, we present the Semi-Aggregated Markov Decision Process (SAMDP) model. The purpose of the SAMDP modeling is to describe and allow a better understanding of complex behaviors by identifying temporal and spatial abstractions. In contrast to other modeling approaches, SAMDP is built in a transformed state-space that encodes the dynamics of the problem. We show that working with the \\emph{right} state representation mitigates the problem of finding spatial and temporal abstractions. We describe the process of building the SAMDP model from observed trajectories and give examples for using it in a toy problem and complicated DQN policies. Finally, we show how using the SAMDP we can monitor the policy at hand and make it more robust.", "keywords": ["Reinforcement Learning", "Deep learning"]}, "meta": {"decision": "Reject", "comment": "The paper combines semi-Markov models with state aggregation models to define a Semi-Aggregated Markov Decision Process (SAMDP). As per the reviews, the need for spatio-temporal representations is recognized as an important problem. Where the work currently falls short is the loose description of framework, which should be formalized for clarity and analysis, and in terms of its contributions and benefits with respect to the broader literature on hierarchical RL methods.\n The anonymous reviewers (and the additional public review/comment) provide a number of clear directions to pursue for the next iteration of this work; indeed, there is strong general enthusiasm for the greater goals (as shared by the relevant body of work in the research community), but the work is seen as requiring further clarity in formalization and presentation."}, "review": {"HkepkkKPe": {"type": "rebuttal", "replyto": "r1yjkAtxe", "comment": "This work is interesting and one of the few papers out there to address the problem of autonomous skill acquisition in Deep RL. The importance of this problem cannot be overstated since Hierarchical RL is not only about using a meta-controller to appropriately use a specific skill from a repertoire of previously learned subgoals, but also about devising a set of subgoals through experience. Subgoals are inherently tied to the topology of the underlying problem and hence clustering approaches make sense. \n\nI would like to point out that a very similar attempt has been done in one of the past works in this area - https://arxiv.org/abs/1605.05359 - Option Discovery for Hierarchical Reinforcement Learning using Spatio-Temporal Clustering. This work presents a spectral method PCCA+ (Perron Cluster Analysis) that identifies abstractions aligned with the topology of the state space as opposed to typical K-means on spectral components (a comparison to other clustering methods has been visually shown to illustrate this). The approach is meant to naturally construct options (macro-actions) from one abstract state to another by ascending on the membership function of each granular state onto the target abstract state. It is shown to work on toy tasks like 3-room and Taxi (see version 1 for taxi) and several other aspects like interpretable termination functions and incorporating reward structure into the abstractions are shown visually.  Finally, an attempt to extend this approach to large state spaces through deep learning representations from unsupervised learning is shown. The idea is essentially the same as one used in this submission - Aggregate the states in original MDP and identify an aggregated state space through clustering (Clustering is done on the representation learned through a recurrent dynamics model). This is followed by applying the spectral method on the aggregated MDP to identify option policies. This pipeline was able to discover subgoals in the complex Atari game Seaquest such as filling oxygen (visually illustrated as well). \n\nI definitely think this paper needs to cite the above work and also have suggestions to investigate a loopy way of identifying the abstractions through developmental learning done on auxiliary tasks. For instance, an initial deep network could just learn to do inverse dynamics or depth prediction or next-state prediction and learn reasonable representations. This could be used to cluster and identify some subgoals to provide an initial policy. This policy gets refined over time with new abstractions and new options. I am curious to see how well this would work for sparse reward tasks. See Stochastic Neural Networks for Hierarchical RL - https://openreview.net/forum?id=B1oK8aoxe (Carlos Florensa, Yan Duan, Pieter Abbeel) for a similar treatment on continuous tasks. Some other tricks could be to not learn prediction of next state, but just have a Siamese Style Network that takes in a triplet of (s,a,s') and predicts whether it is a correct or wrong transition.\n\nAnother interesting direction is to investigate the applicability of these approaches to figuring out perceptual reward functions from a set of expert demonstrations for inverse RL. This is again inspired from another submission to ICLR - Unsupervised Perceptual Rewards for Imitation Learning (Pierre Sermanet, Kelvin Xu, Sergey Levine) - https://openreview.net/forum?id=Bkul3t9ee. They currently use a heuristic method to figure out reward functions by segmenting the video of the expert demo. One could think of the points where the segments are identified as some sort of subgoals of the task being demonstrated. So connecting subgoals with demonstration trajectories is an interesting idea that the authors could explore.\n\nOverall, I think this is a very hard problem to solve. The authors have made a good attempt and I really encourage them to keep working on this. ", "title": "Comments and Suggestions"}, "SkmNdVlrg": {"type": "review", "replyto": "r1yjkAtxe", "review": "No questionsThe paper starts by pointing out the need for methods that perform both state and temporal representation learning for RL and which allow gaining insight into what is being learned (perhaps in order to allow a human operator to intervene if necessary). This is a very important goal from a practical point of view, and it is great to see research in this direction. For this reason, I would like to encourage the authors to pursue this further. However, I am not at all convinced that the current incarnation of this work is the right answer. Part of the issues are more related to presentation, part may require rethinking.\nIn order to get the \"interpretability\", the authors opt for some fairly specific ways of performing abstraction. For example, their skills always start In a single skill initiation state, and likewise end in one state. This seems unnecessarily restrictive, and it is not clear why this restriction is needed (other than convenience). Similarly, clustering is the basis for forming the higher level states, and there is a specific kind of clustering used here. Again, it is not clear why this has to be done via clustering as opposed to other methods. Ensuring temporal coherence in the particular form employed also seems restrictive. There is a reference to supplementary material where some of these choices are explained, but I could not find this in the posted version of the paper. The authors should either explain clearly why these specific choices are necessary, or (even better) try to think if they can be relaxed while still keeping interpretability. \nFrom a presentation point of view, the paper would benefit from formal definitions of AMDP and SAMDP, as well as from formal descriptions of the algorithms employed in constructing these representations (eg Bellman equations for the models, and update rules for the algorithms learning them). While intuitions are given, the math is not precisely stated. The overhead of constructing an SAMDP (computation time and space) should be clarified as well.\nThe experiments are well carried out and it is nice to have both gridworld experiments, where visualization are easy to perform and understand, as well as Atari games (gridworld still have their place despite what other reviewers might say). The results are positive, but because the proposed approach has many moving parts which rely on specific choices, significance and general ease of use are unclear at this point. Perhaps having the complete supplementary would have helped in this respect.\nSmall comment: The two lines after Eq 2 contain typos in the notation and a wrong sign in the equation.", "title": "No questions", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Hklb2oxBx": {"type": "review", "replyto": "r1yjkAtxe", "review": "No questionsThe paper starts by pointing out the need for methods that perform both state and temporal representation learning for RL and which allow gaining insight into what is being learned (perhaps in order to allow a human operator to intervene if necessary). This is a very important goal from a practical point of view, and it is great to see research in this direction. For this reason, I would like to encourage the authors to pursue this further. However, I am not at all convinced that the current incarnation of this work is the right answer. Part of the issues are more related to presentation, part may require rethinking.\nIn order to get the \"interpretability\", the authors opt for some fairly specific ways of performing abstraction. For example, their skills always start In a single skill initiation state, and likewise end in one state. This seems unnecessarily restrictive, and it is not clear why this restriction is needed (other than convenience). Similarly, clustering is the basis for forming the higher level states, and there is a specific kind of clustering used here. Again, it is not clear why this has to be done via clustering as opposed to other methods. Ensuring temporal coherence in the particular form employed also seems restrictive. There is a reference to supplementary material where some of these choices are explained, but I could not find this in the posted version of the paper. The authors should either explain clearly why these specific choices are necessary, or (even better) try to think if they can be relaxed while still keeping interpretability. \nFrom a presentation point of view, the paper would benefit from formal definitions of AMDP and SAMDP, as well as from formal descriptions of the algorithms employed in constructing these representations (eg Bellman equations for the models, and update rules for the algorithms learning them). While intuitions are given, the math is not precisely stated. The overhead of constructing an SAMDP (computation time and space) should be clarified as well.\nThe experiments are well carried out and it is nice to have both gridworld experiments, where visualization are easy to perform and understand, as well as Atari games (gridworld still have their place despite what other reviewers might say). The results are positive, but because the proposed approach has many moving parts which rely on specific choices, significance and general ease of use are unclear at this point. Perhaps having the complete supplementary would have helped in this respect.\nSmall comment: The two lines after Eq 2 contain typos in the notation and a wrong sign in the equation.", "title": "No questions", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Bk_36FaXl": {"type": "rebuttal", "replyto": "S1in7g1mg", "comment": "Thanks for the constructive feedback. Indeed I had a problem with the references format, and significant literature was missing.\nI updated the paper, but I couldn't find a way to update it on the site. I'll try to solve this.\n\nThanks,\nNir ", "title": "Feedback"}, "r1QGpF6Xe": {"type": "rebuttal", "replyto": "H1NxPiyQe", "comment": "The algorithm is indifferent to discrete/stochastic states. Regarding actions:\nIn this work we analyzed discrete-action policies, however, SAMDP can also be applied for continuous-action policies that maintain a value function (since our algorithm depends on it for construction and evaluation), as in the case of actor-critic methods.\n\nI also revised the paper to emphasize the fact that the main purpose of SAMDP is to analyze trained policies, however, I couldn't find how to update the current version. Is it still possible?\n\nThanks,\n\nNir", "title": "Author response"}, "H1NxPiyQe": {"type": "review", "replyto": "r1yjkAtxe", "review": "Can you please revise the paper to clarify the purpose of SAMDP? In the description of the method, it appears that SAMDP is an learning architecture for training. Reading the results, it appears to have been used post training for analysis.\n\nCan this method be applicable to continuous states and actions? The paper presents a method for visualization and analysis of policies from observed trajectories that the policies produce. The method infers higher level skills and clusters states. The result is a simplified, discrete higher-order state and action transition matrix. This model can be used for analysis, modeling, and interpretation. To construct semi-aggregated MDP the authors propose combining ideas for creating semi-MPDs and agregrated-MDPs. The method consists of choosing features, state clustering, skill inference, reward and skill length inference, and model selection. The method was demonstrated on a small grid-world problem, and DQN-trained agent for playing Atari games.\n\nThe authors correctly identify that tools and means for interpretibility of RL methods are important for analysis, and deployment of such methods for real-world applications. This is particularly true in robotics and high-consequence systems.\n\nThe end-result of the presented method is a high-level transition matrix. There is a big body of literature looking into hierarchical RL methods where lower level skills are combined with higher level policies. The presented method has the similar result, but the advantage of the presented method is that it comes up with a structure and analyzes already trained agent, which is very interesting. The paper would benefit from emphasizing this difference, and contrasting with the broader body of literature.\n\nTo build the model, the authors propose combining the ideas from two existing ideas, semi-MPDs and agregrated-MDPs with using modified k-means for state clustering. It appears that the novelty of the presented method is limited. The paper would have been stronger if the authors explicitly stated the contributions over combining existing methods, and better highlighted the practical utility of the method. \n\nThe evaluation section would be made stronger with more analytical results and precise evaluation, showing full strength of the method.\n\nThe paper is difficult to read. To improve readability:\n- The Semi-Aggregated MDP section should include more precise description of the methods. The narrative that builds intuition is welcome. In addition to the existing narrative, algorithms and formulas where applicable should be included as well. \n- The paper should be self-contained. For example, more background on Occams Razor principle should be included.\n- Reduce the number of acronyms, in particular similarly sounding acronyms. Define acronyms before using. \n- Be more clear on the contributions, contrast with relevant literature, and the specific benefits of the presented method.\n- Fix typos, formatting mistakes etc., as they can be distracting for reading. \n\nThe approach of reverse engineering the hierarchy, and learning high-level transition matrix is very interesting and promising. Perhaps the method can be used to outperform single network approach by using the model as an input to more specialized hierarchical trainers and learn complex behaviors more optimally then possible with one large network approach. Unfortunately, the paper falls short in the novelty, precision, and clarity.", "title": "Clarification", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyWhcBxEg": {"type": "review", "replyto": "r1yjkAtxe", "review": "Can you please revise the paper to clarify the purpose of SAMDP? In the description of the method, it appears that SAMDP is an learning architecture for training. Reading the results, it appears to have been used post training for analysis.\n\nCan this method be applicable to continuous states and actions? The paper presents a method for visualization and analysis of policies from observed trajectories that the policies produce. The method infers higher level skills and clusters states. The result is a simplified, discrete higher-order state and action transition matrix. This model can be used for analysis, modeling, and interpretation. To construct semi-aggregated MDP the authors propose combining ideas for creating semi-MPDs and agregrated-MDPs. The method consists of choosing features, state clustering, skill inference, reward and skill length inference, and model selection. The method was demonstrated on a small grid-world problem, and DQN-trained agent for playing Atari games.\n\nThe authors correctly identify that tools and means for interpretibility of RL methods are important for analysis, and deployment of such methods for real-world applications. This is particularly true in robotics and high-consequence systems.\n\nThe end-result of the presented method is a high-level transition matrix. There is a big body of literature looking into hierarchical RL methods where lower level skills are combined with higher level policies. The presented method has the similar result, but the advantage of the presented method is that it comes up with a structure and analyzes already trained agent, which is very interesting. The paper would benefit from emphasizing this difference, and contrasting with the broader body of literature.\n\nTo build the model, the authors propose combining the ideas from two existing ideas, semi-MPDs and agregrated-MDPs with using modified k-means for state clustering. It appears that the novelty of the presented method is limited. The paper would have been stronger if the authors explicitly stated the contributions over combining existing methods, and better highlighted the practical utility of the method. \n\nThe evaluation section would be made stronger with more analytical results and precise evaluation, showing full strength of the method.\n\nThe paper is difficult to read. To improve readability:\n- The Semi-Aggregated MDP section should include more precise description of the methods. The narrative that builds intuition is welcome. In addition to the existing narrative, algorithms and formulas where applicable should be included as well. \n- The paper should be self-contained. For example, more background on Occams Razor principle should be included.\n- Reduce the number of acronyms, in particular similarly sounding acronyms. Define acronyms before using. \n- Be more clear on the contributions, contrast with relevant literature, and the specific benefits of the presented method.\n- Fix typos, formatting mistakes etc., as they can be distracting for reading. \n\nThe approach of reverse engineering the hierarchy, and learning high-level transition matrix is very interesting and promising. Perhaps the method can be used to outperform single network approach by using the model as an input to more specialized hierarchical trainers and learn complex behaviors more optimally then possible with one large network approach. Unfortunately, the paper falls short in the novelty, precision, and clarity.", "title": "Clarification", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1in7g1mg": {"type": "review", "replyto": "r1yjkAtxe", "review": "Your references are in completely the wrong format! Please pay attention to the formatting of references, otherwise reading your paper is well nigh impossible! If you cite a paper in line in the text, such as Peng and Williams (1993), you should formulate your sentence as \"Peng and Williams (1993) focused on simpler problems\". Or you can write a citation in the passive mode as \"Some researchers focused on simpler problems, e.g. (Peng and Williams, 1993). Please do not use the current format, as you are in violation of basic rules in scientific papers. \n\nSecond, your literature review of work on skill learning in MDPs is woefully adequate. and does not even begin to touch on the bulk of the important work. There is a large thread of work on the use of \"bottleneck\" states and regions (e.g, Simsek, Hengst, et al.), which is not cited. There is work by Johannsen and Barto (JMLR) on VISA using causal analysis of action models to infer options. There is much work on spectral methods to infer option hierarchies as well. Finally, there is a large and growing body of work on hierarchical methods in deep RL (e.g, Durugkar et al., \"Deep RL with Macro Actions\", Arxiv, 2016; Kulkarni et al., Integrating intrinsic motivation and deep RL, Arxiv 2016). A search on Arxiv will retrieve many other related papers. \n\nPlease avoid excessive use of acronyms, and in particular, don't use acronyms like AMDP, before they are defined in the paper. \nThe framework of semi-Markov decision processes (SDMPs) has been long used to model skill learning and temporal abstraction in reinforcement learning. This paper proposes a variant of such a model called a semi-aggregated MDP model. \n\nThe formalism of SAMDP is not defined clearly enough to merit serious attention. The approach is quasi heuristic and explained through examples rather than clear definition. The work also lacks sufficient theoretical rigor. \n\nSimple experiments are proposed using 2D grid worlds to demonstrate skills. Grid worlds have served their purpose long enough in reinforcement learning, and it is time to retire them. More realistic domains are now routinely used and should be used in this paper as well. \n\n", "title": "Incorrect format for references and inadequate literature survey ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HJSCsaf4g": {"type": "review", "replyto": "r1yjkAtxe", "review": "Your references are in completely the wrong format! Please pay attention to the formatting of references, otherwise reading your paper is well nigh impossible! If you cite a paper in line in the text, such as Peng and Williams (1993), you should formulate your sentence as \"Peng and Williams (1993) focused on simpler problems\". Or you can write a citation in the passive mode as \"Some researchers focused on simpler problems, e.g. (Peng and Williams, 1993). Please do not use the current format, as you are in violation of basic rules in scientific papers. \n\nSecond, your literature review of work on skill learning in MDPs is woefully adequate. and does not even begin to touch on the bulk of the important work. There is a large thread of work on the use of \"bottleneck\" states and regions (e.g, Simsek, Hengst, et al.), which is not cited. There is work by Johannsen and Barto (JMLR) on VISA using causal analysis of action models to infer options. There is much work on spectral methods to infer option hierarchies as well. Finally, there is a large and growing body of work on hierarchical methods in deep RL (e.g, Durugkar et al., \"Deep RL with Macro Actions\", Arxiv, 2016; Kulkarni et al., Integrating intrinsic motivation and deep RL, Arxiv 2016). A search on Arxiv will retrieve many other related papers. \n\nPlease avoid excessive use of acronyms, and in particular, don't use acronyms like AMDP, before they are defined in the paper. \nThe framework of semi-Markov decision processes (SDMPs) has been long used to model skill learning and temporal abstraction in reinforcement learning. This paper proposes a variant of such a model called a semi-aggregated MDP model. \n\nThe formalism of SAMDP is not defined clearly enough to merit serious attention. The approach is quasi heuristic and explained through examples rather than clear definition. The work also lacks sufficient theoretical rigor. \n\nSimple experiments are proposed using 2D grid worlds to demonstrate skills. Grid worlds have served their purpose long enough in reinforcement learning, and it is time to retire them. More realistic domains are now routinely used and should be used in this paper as well. \n\n", "title": "Incorrect format for references and inadequate literature survey ", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}