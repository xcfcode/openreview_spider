{"paper": {"title": "Frustratingly Short Attention Spans in Neural Language Modeling", "authors": ["Micha\u0142 Daniluk", "Tim Rockt\u00e4schel", "Johannes Welbl", "Sebastian Riedel"], "authorids": ["michal.daniluk.15@ucl.ac.uk", "t.rocktaschel@cs.ucl.ac.uk", "j.welbl@cs.ucl.ac.uk", "s.riedel@cs.ucl.ac.uk"], "summary": "We investigate various memory-augmented neural language models and compare them against state-of-the-art architectures.", "abstract": "Current language modeling architectures often use recurrent neural networks. Recently, various methods for incorporating differentiable memory into these architectures have been proposed. When predicting the next token, these models query information from a memory of the recent history and thus can facilitate learning mid- and long-range dependencies. However, conventional attention models produce a single output vector per time step that is used for predicting the next token as well as the key and value of a differentiable memory of the history of tokens. In this paper, we propose a key-value attention mechanism that produces separate representations for the key and value of a memory, and for a representation that encodes the next-word distribution. This usage of past memories outperforms existing memory-augmented neural language models on two corpora. Yet, we found that it mainly utilizes past memory only of the previous five representations. This led to the unexpected main finding that a much simpler model which simply uses a concatenation of output representations from the previous three-time steps is on par with more sophisticated memory-augmented neural language models.", "keywords": ["Natural language processing", "Deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "Reviewers found this paper to be a rigorous and \"thorough experimental analysis\" of context-length in language modelingv through the lens of an \"interesting extension to standard attention mechanism\". The paper reopens and makes more problematic widely accepted but rarely verified claims of the importance of long-term dependency.\n \n Pros:\n - \"Well-explained\" and clear presentation\n - Use of an \"inventive baseline\" in the form a ngram rnn \n - Use a impactful corpus for long-term language modeling\n \n Cons: \n - Several of the ideas have been explored previously.\n - Some open questions about the soundness of parameters (rev 1)\n - Requests for deeper analysis on data sets released with the paper."}, "review": {"SyY-HmVKe": {"type": "rebuttal", "replyto": "ByIAPUcee", "comment": "(I read the paper after knowing it got accepted to ICLR 2017)\n\nThis paper is quite illuminating to me, as it shows that language modeling may not be the right task to show whether a model has the ability to hold and use long-term information. However, it is up for debate whether that is because of the specific datasets used in the paper.", "title": "Nice read!"}, "BkegySgUx": {"type": "rebuttal", "replyto": "HkU1NSxNg", "comment": "Thank you for your review. We updated the paper and clarified the model description.", "title": "Re: Review"}, "HJHiC4xLl": {"type": "rebuttal", "replyto": "BJwzd6GEl", "comment": "Thank you for your review and for suggesting a deeper analysis of the Wikipedia dataset. We updated the paper accordingly.", "title": "Re: Review"}, "HkiwCNeIl": {"type": "rebuttal", "replyto": "ByldhqHVe", "comment": "Thank you for your review. We added Ba et al., Reed & de Freitas, and Gulcehre et al. to the related work section.", "title": "Re: Review"}, "B1QFqz_Xg": {"type": "rebuttal", "replyto": "rJnb_WCzg", "comment": "Thank you for your questions. \n\nAddressing the first: vector h_t is the output vector of the LSTM unit at step t. We reduced the cell state vector dimensionality to maintain a stable number of total parameters. Note that it is not the case that standard attention mechanism is allowed to work with vectors that have three times more dimension, because we increase the number of dimension of h_t for Key-Value and Key-Value-Predict models. However, then we divided this vectors into key-value vectors (k, v) and into key-value-predict (k,v,p) vectors respectively. For example, in Figure 2c we have reported results on Wikipedia dataset. The hidden dimension for baseline LSTM model is 300, for Attention model is 296. The hidden dimension for Key-Value model is 560, but it is divided into key and value vectors which both have size 560/2 = 280. The hidden dimension for Key-Value-Predict model is 834 and it is divided into key, value and predict vectors. Each of them has size of 834/3 = 278. We chose the size of hidden dimension to ensure the same number of model parameters for a fairer comparison. \n\nWe also compared with higher dimensional hidden size of context vectors and found improvements with the same relative results. However, as all models benefited from this more or less equally, it did not change the ranking of the systems that we investigated. \n\nThank you for pointing out the mistake in Equation 1. We updated the paper.", "title": "Re: How is h_t exactly computed and did you compare with higher dimensional context vectors?"}, "rkuXufuQg": {"type": "rebuttal", "replyto": "SykQiDkmg", "comment": "Thank you for your questions. Output vector h is divided into three equal parts: k, v, p (see Equation 11). Actually, there are two ways of implementing it. We can create 3 separate vectors k, v, p (each of them has dimension k), or create one output vector h which is a concatenation of k, v, p. To make this more clear, consider this example: The hidden dimension reported in the Figure 2c (denoted by k) is a dimension of the hidden output of the network. For example, the hidden dimension of the Key-Value-Predict models is 834 - which means that the dimension of vector h is 834 and the dimension of vectors k, v and p is 834/3 = 278. To ensure a comparable number of parameters for all models, we adjusted the hidden size of the networks such that all models have approximately the same number of parameters. \n\nRegarding the N-gram model description, we made a modification to clarify the model architecture. The idea is basically to split the output vector into N-1 different parts and use each of these for a different future time step. As you point out, a natural consideration is to compare this with attention models over the full past output vectors. The language modeling experiments on the Wikipedia corpus (Figures 2a, 2b) showed quite clear improvements in terms of perplexity when comparing this N-gram separation with a standard attention model over the full previous output vectors.\n\nThank you for pointing out these papers. We also found another related paper \u201cUsing Fast Weights to Attend to the Recent Past. J Ba, GE Hinton, V Mnih, JZ Leibo, C Ionescu, in NIPS 2016\u201d and will add these to our related work.", "title": "Re: Few questions"}, "S1-swG_Ql": {"type": "rebuttal", "replyto": "SJDimi1me", "comment": "Thank you very much for your questions and comments. We agree that unrolling BPTT for more steps could improve results. In fact, based on your suggestion we ran more experiments, unrolling 35 steps, and found improvements with the same relative results.\nHowever, as all models benefited from this more or less equally, it did not change the ranking of the systems that we investigated. We will update the results in the paper.\n\nWe ran experiments for attention windows only between sizes 1 and 15 (see Figure2a). One of our main finding is that increasing the attention window beyond this range does not significantly improve the performance of models.\n\nWe are going to release our Wikipedia dataset and add a link to the paper. Thank you also for pointing us to the WikiText dataset. \n\nThe size of vocabulary was restricted to the 77k most frequent words, encompassing 97% of the training vocabulary.\n\nRegarding the conclusion we draw from the experiments with CBT you are indeed right, they only hold for 2 of the 4 categories (verbs and prepositions), and the overall situation including named entities and common nouns is not as clear as it was presented. We will update the paper to reflect this.", "title": "Answer re: BPTT length, Wikipedia corpus, and CBT results"}, "SJDimi1me": {"type": "review", "replyto": "ByIAPUcee", "review": "I agree with many of the thoughts in your paper, particularly that long term dependencies are quite difficult and that they've not yet been well solved by any architecture, but have some questions regarding the experiments and resulting conclusions.\n\nUnrolling BPTT for only 20 timesteps during training seems minimal given standard language modeling on word level PTB unrolls for 35 timesteps. The maximum attention window of only length 15 also seems quite small (Figure 2a). Was there a reason you trained for only that many timesteps? Generally I've seen attention mechanisms work better the more history they have access to.\n\nIs it also possible to release your Wikipedia corpus earlier than later? Being able to view that would be useful as there's only so much about the pre-processing and set-up that can be described in the paper. It may also provide an insight into some of the behaviour of the models. The WikiText dataset, used in the Pointer Sentinel Mixture Model and Neural Language Models with a Continuous Cache papers, are based on Wikipedia as well and has shown that models effective in exploiting long range dependencies do better, though those works use a history of hundred or more and access memory using a pointer rather than weighted summation.\n\nFor the Wikipedia corpus, you cut the vocabulary to 77k words - was 77k an arbitrary number or a result of a certain frequency cut-off?\n\nYou also state in the conclusion that the results on the Wikipedia corpus and CBT show that key-value-predict decomposition outperform simpler attention mechanisms. While the results on your Wikipedia corpus show that, the results on the CBT corpus don't seem as definitive. The results of the attention model for example are quite similar to the key-value-predict model (though slightly better for named entities, slightly worse for verbs). Am I misreading those figures? I'd generally not consider the deltas between the models significant but also don't have substantial experience with the CBT dataset in the past.This paper explores a variety of memory augmented architectures (key, key-value, key-predict-value) and additionally simpler near memory-less RNN architectures. Using an attention model that has access to the various decompositions is an interesting idea and one worth future explorations, potentially in different tasks where this type of model could excel even more. The results over the Wikipedia corpus are interesting and feature a wide variety of different model types. This is where the models suggested in the paper are strongest. The same models run over the CBT dataset show a comparable but less convincing demonstration of the variations between the models.\n\nThe authors also released their Wikipedia corpus already. Having inspected it I consider it a positive and interesting contribution. I still believe that, if a model was found that could better handle longer term dependencies, it would do better on this Wikipedia dataset, but at least within the realm of what . As an example, the first article in train.txt is about a person named \"George Abbot\", yet \"Abbot\" isn't mentioned again until the next sentence 40 tokens later, and then the next \"Abbot\" is 15 tokens from there. Most gaps between occurrences of \"Abbot\" are dozens of timesteps. Performing an analysis based upon easily accessed information, such as when the same token reappears again or average sentence length, may be useful as an approximation for the length that an attention window may prefer.\n\nThis is a well explained paper that raises interesting questions regarding the spans used in existing language modeling approaches and serves as a potential springboard for future directions.", "title": "Questions re: BPTT length, Wikipedia corpus, and CBT results", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJwzd6GEl": {"type": "review", "replyto": "ByIAPUcee", "review": "I agree with many of the thoughts in your paper, particularly that long term dependencies are quite difficult and that they've not yet been well solved by any architecture, but have some questions regarding the experiments and resulting conclusions.\n\nUnrolling BPTT for only 20 timesteps during training seems minimal given standard language modeling on word level PTB unrolls for 35 timesteps. The maximum attention window of only length 15 also seems quite small (Figure 2a). Was there a reason you trained for only that many timesteps? Generally I've seen attention mechanisms work better the more history they have access to.\n\nIs it also possible to release your Wikipedia corpus earlier than later? Being able to view that would be useful as there's only so much about the pre-processing and set-up that can be described in the paper. It may also provide an insight into some of the behaviour of the models. The WikiText dataset, used in the Pointer Sentinel Mixture Model and Neural Language Models with a Continuous Cache papers, are based on Wikipedia as well and has shown that models effective in exploiting long range dependencies do better, though those works use a history of hundred or more and access memory using a pointer rather than weighted summation.\n\nFor the Wikipedia corpus, you cut the vocabulary to 77k words - was 77k an arbitrary number or a result of a certain frequency cut-off?\n\nYou also state in the conclusion that the results on the Wikipedia corpus and CBT show that key-value-predict decomposition outperform simpler attention mechanisms. While the results on your Wikipedia corpus show that, the results on the CBT corpus don't seem as definitive. The results of the attention model for example are quite similar to the key-value-predict model (though slightly better for named entities, slightly worse for verbs). Am I misreading those figures? I'd generally not consider the deltas between the models significant but also don't have substantial experience with the CBT dataset in the past.This paper explores a variety of memory augmented architectures (key, key-value, key-predict-value) and additionally simpler near memory-less RNN architectures. Using an attention model that has access to the various decompositions is an interesting idea and one worth future explorations, potentially in different tasks where this type of model could excel even more. The results over the Wikipedia corpus are interesting and feature a wide variety of different model types. This is where the models suggested in the paper are strongest. The same models run over the CBT dataset show a comparable but less convincing demonstration of the variations between the models.\n\nThe authors also released their Wikipedia corpus already. Having inspected it I consider it a positive and interesting contribution. I still believe that, if a model was found that could better handle longer term dependencies, it would do better on this Wikipedia dataset, but at least within the realm of what . As an example, the first article in train.txt is about a person named \"George Abbot\", yet \"Abbot\" isn't mentioned again until the next sentence 40 tokens later, and then the next \"Abbot\" is 15 tokens from there. Most gaps between occurrences of \"Abbot\" are dozens of timesteps. Performing an analysis based upon easily accessed information, such as when the same token reappears again or average sentence length, may be useful as an approximation for the length that an attention window may prefer.\n\nThis is a well explained paper that raises interesting questions regarding the spans used in existing language modeling approaches and serves as a potential springboard for future directions.", "title": "Questions re: BPTT length, Wikipedia corpus, and CBT results", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SykQiDkmg": {"type": "review", "replyto": "ByIAPUcee", "review": "1. How do you compute k,v, and p from h? is it just that you have 3 times hidden units and use each part for k,v, and p respectively? I can guess it. But it is better to make it explicit in the paper.\n\n2. You description about n-gram model in section 2.4 can be even more clear. It is not very clear how you split the LSTM output to N-1 vectors. Also why do you need to do this? Why not simply use previous N-1 LSTM outputs to attend?\n\n3. Miller et al., 2016 is not the only paper that explored key-value memory (as stated in related work section). It has also been explored in Reed and de Freitas 2015 [1] and Gulcehre et al., 2016 [2].\n\nReferences:\n\n[1] Scott Reed and Nando de Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279,\n2015.\n\n[2] Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, and Yoshua Bengio. Dynamic neural turing\nmachines with soft and hard addressing schemes. CoRR, abs/1607.00036, 2016. URL http:\n//arxiv.org/abs/1607.00036.\nThis paper focusses on attention for neural language modeling and has two major contributions:\n\n1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as well.\n2. Authors report that very short attention span is sufficient for language models (which is not very surprising) and propose an n-gram RNN which exploits this fact.\n\nThe paper has novel models for neural language modeling and some interesting messages. Authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and CBT task.\n\nI am convinced with authors\u2019 responses for my pre-review questions.\n\nMinor comment: Ba et al., Reed & de Freitas, and Gulcehre et al. should be added to the related work section as well.\n", "title": "Few questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ByldhqHVe": {"type": "review", "replyto": "ByIAPUcee", "review": "1. How do you compute k,v, and p from h? is it just that you have 3 times hidden units and use each part for k,v, and p respectively? I can guess it. But it is better to make it explicit in the paper.\n\n2. You description about n-gram model in section 2.4 can be even more clear. It is not very clear how you split the LSTM output to N-1 vectors. Also why do you need to do this? Why not simply use previous N-1 LSTM outputs to attend?\n\n3. Miller et al., 2016 is not the only paper that explored key-value memory (as stated in related work section). It has also been explored in Reed and de Freitas 2015 [1] and Gulcehre et al., 2016 [2].\n\nReferences:\n\n[1] Scott Reed and Nando de Freitas. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279,\n2015.\n\n[2] Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, and Yoshua Bengio. Dynamic neural turing\nmachines with soft and hard addressing schemes. CoRR, abs/1607.00036, 2016. URL http:\n//arxiv.org/abs/1607.00036.\nThis paper focusses on attention for neural language modeling and has two major contributions:\n\n1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as well.\n2. Authors report that very short attention span is sufficient for language models (which is not very surprising) and propose an n-gram RNN which exploits this fact.\n\nThe paper has novel models for neural language modeling and some interesting messages. Authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and CBT task.\n\nI am convinced with authors\u2019 responses for my pre-review questions.\n\nMinor comment: Ba et al., Reed & de Freitas, and Gulcehre et al. should be added to the related work section as well.\n", "title": "Few questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJnb_WCzg": {"type": "review", "replyto": "ByIAPUcee", "review": "How is h_t exactly computed and did you compare with higher dimensional context vectors? I'm a bit confused about whether these are some affine transformation of the LSTM states or the states themselves. I'd expect the differences between the Key-Value-Predict system and the standard attention-based RNN to be smaller if the standard attention mechanism is allowed to work with vectors that have three times more dimensions. \n\nSome unrelated detail: In Equation 1 I think that the 1^T should be on the right side of the W^h h_t multiplication such that it leads to an outer product instead of a dot product.The paper presents an investigation of various neural language models designed to query context information from their recent history using an attention mechanism. The authors propose to separate the attended vectors into key, value and prediction parts. The results suggest that this helps performance. The authors also found that a simple model which which concatenates recent activation vectors performs at a similar level as the more complicated attention-based models.\n\nThe experimental methodology seems sound in general. I do have some issues with the way the dimensionality of the vectors involved in the attention-mechanism is chosen. While it\u2019s good that the hidden layer sizes are adapted to ensure similar numbers of trainable parameters for all the models, this doesn\u2019t control for the fact that key/value/prediction vectors of a higher dimensionality may simply work better regardless of whether their dimensions are dedicated to one particular task or used together. This separation clearly saves parameters but there could also be benefits of having some overlap of information assuming that vectors that lead to similar predictions may also be required in similar contexts for example. Some tasks may also require more dimensions than others and the explicit separation prevents the model from discovering and exploiting this. \n\nWhile memory augmented RNNs and RNNs with attention mechanisms are not new, some of these architectures had not yet been applied to language modeling. Similarly (and as acknowledged by the authors), the strategy of separating key and value functionality has been proposed before, but not in the context of natural language modeling. I\u2019m not sure about the novelty of the proposed n-gram RNN because I recall seeing similar architectures before but I understand that novelty was not the point of that architecture as it mainly serves as a proof of the lack of ability of the more complicated architectures to do better. In that sense I do consider it an inventive baseline that could be used in future work to test the ability of other models that claim to exploit long-term dependencies. \n\nThe exact computation of the representation h_t was initially not that clear to me (the terms hidden and output can be ambiguous at times) but besides this, the paper is quite clear and generally well-written.\n\nThe results in this paper are important because they show that learning long-term dependencies is not a solved problem by any means. The authors provide a very nice comparison to prior results and the fact that their n-gram RNN is often at least competitive with far more complicated approaches is a clear indication that some of those methods may not capture as much context information as previously thought. The success of the separation of key/value/prediction functionality in attention-based system is also noteworthy although I think this is something that needs to be investigated more thoroughly (i.e., with more control for hyperparameter choices). \n\n\nPros:\nImpressive and also interesting results.\nGood comparison with earlier work.\nThe n-gram RNN is an interesting baseline.\n\n\nCons:\nThe relation between the attention-mechanism type and the number of hidden units weakens the claim that the key/value/prediction separation is the reason for the increase in performance somewhat.\nThe model descriptions are not entirely clear.\nI would have liked to have seen what happens when the attention is applied to a much larger context size.\n", "title": "How is h_t exactly computed and did you compare with higher dimensional context vectors?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkU1NSxNg": {"type": "review", "replyto": "ByIAPUcee", "review": "How is h_t exactly computed and did you compare with higher dimensional context vectors? I'm a bit confused about whether these are some affine transformation of the LSTM states or the states themselves. I'd expect the differences between the Key-Value-Predict system and the standard attention-based RNN to be smaller if the standard attention mechanism is allowed to work with vectors that have three times more dimensions. \n\nSome unrelated detail: In Equation 1 I think that the 1^T should be on the right side of the W^h h_t multiplication such that it leads to an outer product instead of a dot product.The paper presents an investigation of various neural language models designed to query context information from their recent history using an attention mechanism. The authors propose to separate the attended vectors into key, value and prediction parts. The results suggest that this helps performance. The authors also found that a simple model which which concatenates recent activation vectors performs at a similar level as the more complicated attention-based models.\n\nThe experimental methodology seems sound in general. I do have some issues with the way the dimensionality of the vectors involved in the attention-mechanism is chosen. While it\u2019s good that the hidden layer sizes are adapted to ensure similar numbers of trainable parameters for all the models, this doesn\u2019t control for the fact that key/value/prediction vectors of a higher dimensionality may simply work better regardless of whether their dimensions are dedicated to one particular task or used together. This separation clearly saves parameters but there could also be benefits of having some overlap of information assuming that vectors that lead to similar predictions may also be required in similar contexts for example. Some tasks may also require more dimensions than others and the explicit separation prevents the model from discovering and exploiting this. \n\nWhile memory augmented RNNs and RNNs with attention mechanisms are not new, some of these architectures had not yet been applied to language modeling. Similarly (and as acknowledged by the authors), the strategy of separating key and value functionality has been proposed before, but not in the context of natural language modeling. I\u2019m not sure about the novelty of the proposed n-gram RNN because I recall seeing similar architectures before but I understand that novelty was not the point of that architecture as it mainly serves as a proof of the lack of ability of the more complicated architectures to do better. In that sense I do consider it an inventive baseline that could be used in future work to test the ability of other models that claim to exploit long-term dependencies. \n\nThe exact computation of the representation h_t was initially not that clear to me (the terms hidden and output can be ambiguous at times) but besides this, the paper is quite clear and generally well-written.\n\nThe results in this paper are important because they show that learning long-term dependencies is not a solved problem by any means. The authors provide a very nice comparison to prior results and the fact that their n-gram RNN is often at least competitive with far more complicated approaches is a clear indication that some of those methods may not capture as much context information as previously thought. The success of the separation of key/value/prediction functionality in attention-based system is also noteworthy although I think this is something that needs to be investigated more thoroughly (i.e., with more control for hyperparameter choices). \n\n\nPros:\nImpressive and also interesting results.\nGood comparison with earlier work.\nThe n-gram RNN is an interesting baseline.\n\n\nCons:\nThe relation between the attention-mechanism type and the number of hidden units weakens the claim that the key/value/prediction separation is the reason for the increase in performance somewhat.\nThe model descriptions are not entirely clear.\nI would have liked to have seen what happens when the attention is applied to a much larger context size.\n", "title": "How is h_t exactly computed and did you compare with higher dimensional context vectors?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}