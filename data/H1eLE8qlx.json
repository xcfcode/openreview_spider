{"paper": {"title": "Options Discovery with Budgeted Reinforcement Learning", "authors": ["Aurelia L\u00e9on", "Ludovic Denoyer"], "authorids": ["aurelia.leon@lip6.fr", "ludovic.denoyer@lip6.fr"], "summary": "The article describes a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective, and a new RL learning framework called Bi-POMDP.", "abstract": "We consider the problem of learning hierarchical policies for Reinforcement Learning able to discover options, an option corresponding to a sub-policy over a set of primitive actions. Different models have been proposed during the last decade that usually rely on a predefined set of options. We specifically address the problem of automatically discovering options in decision processes. We describe a new RL learning framework called Bi-POMDP, and a new learning model called Budgeted Option Neural Network (BONN) able to discover options based on a budgeted learning objective. Since Bi-POMDP are more general than POMDP, our model can also be used to discover options for classical RL tasks. The BONN model is evaluated on different classical RL problems, demonstrating both quantitative and qualitative interesting results.", "keywords": ["Reinforcement Learning"]}, "meta": {"decision": "Reject", "comment": "Most of the reviewers agreed that the proposed budgeted options framework was interesting, but there were a number of serious concerns raised about the work. Many of the reviewers found the assumptions of the approach to be somewhat odd, and while the particular formulation in the paper was generally assessed as novel, it has connections to a number of previous works that were not explored in detail. Finally, the experimental evaluation is conducted on simple tasks with few comparisons, so it is very difficult to make concrete conclusions about how well the method works."}, "review": {"HyNitOAIg": {"type": "rebuttal", "replyto": "Syo2DYy4g", "comment": "Thank you for your review.\n\nWe deleted the Bi-POMDP framework (see comment made to reviewer AnonReviewer1)\n\nConcerning lines 4 & 6, we modified the paper in that direction:\n\u2022  sigma_t is equal to 0 or 1, randomly taken from P(sigma_t=1) = sigmoid(W*z_{t-1} + W*a_{t-1} + W*x_t) (in our experiments) using a Bernoulli distribution\n\u2022  o_t = f(y_t) = relu(W*y_t) in our experiments\n\u2022  In both cases, the  W  corresponds to different matrix of parameters (see the appendix of the paper)", "title": "deletion of Bi-POMDP and precisions"}, "H1OLY_08g": {"type": "rebuttal", "replyto": "r1bD3ZGBl", "comment": "Thank you for your detailed review. Here are some comments and modifications we have made based on your remarks:\n\n1.   We deleted the Bi-POMDP framework which was confusing; the BONN model is now just described as a model that can ask for an additional costly observation y_t.\n\n2.   The interest of using recurrent network instead of  classical feedforwards nets is to keep memory of the lastly chosen options in order to guide the choice of the actions.\n\n3.   We fully understand that a comparative evaluation of BONN w.r.t existing options model would be interesting. But such a comparison is not trivial since existing models are not based on the use of two different observations x_t and y_t. Even if we could compare models with the same input setting, such a comparison would necessitate to be able to evaluate if the options chosen by a model are better than the one chosen by another model (the two models being able to both solve the RL task e.g moving to a goal position). As far as we know, there is no such comparative study in the option literature.\nThe model the closest to our is the one presented in Bacon & Precup ,2015b which is described in a short paper without enough details to be implemented correctly", "title": "comments and deletion of Bi-POMDP"}, "r1_1YORLe": {"type": "rebuttal", "replyto": "Sy98yIwre", "comment": "First, thank you very much for your detailed and interesting review.\n\nConcerning the design of observations x_t and y_t, you\u2019re right in saying that the choice of them is crucial and we added a small paragraph in the paper pointing out this aspect. However, we would like to say that, first, on any RL problem, one can consider the setting where y_t corresponds to the full observation while x_t is an empty observation, the agent learning to acquire observations at relevant time steps. Even with this simple configuration, and as shown in the experiments, our algorithm exhibits an interesting ability to discover options. Second, in many cases, the definition of x_t and y_t is quite natural or at least easier than defining the full catalogue of possible options (for example x_t is the internal state of the agent \u2013 the observation of the room in our case \u2013 and y_t is the full knowledge of the task to solve \u2013 the goal position in our case). The paper by Heess et al. has now been added in the related work section and is effectively based on the same kind of decomposition of the observation (without budget in their case).\n\nWhen x_t corresponds to the empty observation, the option computed from y_t can indeed be seen as a (stochastic) macro action. This has been explained in the new version of the paper.", "title": "design of observations"}, "HJHd_4eHe": {"type": "review", "replyto": "H1eLE8qlx", "review": "No questionsThe paper presents an approach to constructing hierarchical RL representations which relies on assuming agents that need to spend cognitive effort in order to choose their actions.  The paper p[roposes a specific way of formulating option construction via what they call a \"Bi-POMDP\". This idea is potentially very interesting, plausible form a cognitive science point of view, and definitely deserves attention. However, there are some problems which do not make the paper acceptable in its current form. I am listing them here in order of importance.\n1. It is not clear from the description why a Bi-POMDP is not a POMDP. POMDPs allow for vector-based observations.  Suppose the observation vector is (x_t, \\sigma_t * y_t). This seems like it would result in a POMDP which is identical to the proposed model. The paper should include an example of a Bi-POMDP which is *not* a POMDP, or be revised to use specific POMDP terminology (see eg the use of augmented MDPs in hierarchical RL, which *are MDPs* but do not work in the original state space)\n2. The paper make some specific assumptions about the abstractions (eg determinism in certain places). It is not clear why these are needed at all. Similarly, there are some very specific assumptions regarding the form of the approximations used (Relu, GRUs etc).  Are these necessary? In principle one could implement the ideas in the paper with other, simpler architectures. Was this the first set of choices, or was it arrived at after some experimentation? It is important to understand how much of the performance achieved is due to the specific (fairly powerful) architectures and what one could get through simpler means (eg, feedforward nets)\n3. The paper seems quite similar in spirit to Bacon & Precup, 2015b; in fact, it seems that the use of a value function or model that they discuss is a way to provide a y_t. However, there is no direct comparison to that approach. Since it is very related, it would be useful to perform some of those same experiments. Also, their paper works entirely in the MDP, not POMDP framework, so some clarification is needed here regarding the use of POMDPs instead.\n4. The choice of domains is somewhat limited to simple tasks, while some of the recent approaches in hierarchical RL use more complex domains (Atari, Minecraft etc). Ideally, the experiments should be extended to some of these more complex tasks.\n5. What are the theoretical properties of the proposed approach? Eg, is the proposed algorithm convergent? If Bi-POMDP is a POMDP, then one should be able to leverage POMDP results to build some theory here. If it is not a POMDP, then we need some understanding of how easy/hard a Bi-POMDP is to solve\n6. The paper contains many grammar problems and some broken references, and should be proof-read thoroughly \nIn summary, while the proposed approach is quite interesting and definitely worth exploring, the paper is not ready for publication in its current form.", "title": "No questions", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "r1bD3ZGBl": {"type": "review", "replyto": "H1eLE8qlx", "review": "No questionsThe paper presents an approach to constructing hierarchical RL representations which relies on assuming agents that need to spend cognitive effort in order to choose their actions.  The paper p[roposes a specific way of formulating option construction via what they call a \"Bi-POMDP\". This idea is potentially very interesting, plausible form a cognitive science point of view, and definitely deserves attention. However, there are some problems which do not make the paper acceptable in its current form. I am listing them here in order of importance.\n1. It is not clear from the description why a Bi-POMDP is not a POMDP. POMDPs allow for vector-based observations.  Suppose the observation vector is (x_t, \\sigma_t * y_t). This seems like it would result in a POMDP which is identical to the proposed model. The paper should include an example of a Bi-POMDP which is *not* a POMDP, or be revised to use specific POMDP terminology (see eg the use of augmented MDPs in hierarchical RL, which *are MDPs* but do not work in the original state space)\n2. The paper make some specific assumptions about the abstractions (eg determinism in certain places). It is not clear why these are needed at all. Similarly, there are some very specific assumptions regarding the form of the approximations used (Relu, GRUs etc).  Are these necessary? In principle one could implement the ideas in the paper with other, simpler architectures. Was this the first set of choices, or was it arrived at after some experimentation? It is important to understand how much of the performance achieved is due to the specific (fairly powerful) architectures and what one could get through simpler means (eg, feedforward nets)\n3. The paper seems quite similar in spirit to Bacon & Precup, 2015b; in fact, it seems that the use of a value function or model that they discuss is a way to provide a y_t. However, there is no direct comparison to that approach. Since it is very related, it would be useful to perform some of those same experiments. Also, their paper works entirely in the MDP, not POMDP framework, so some clarification is needed here regarding the use of POMDPs instead.\n4. The choice of domains is somewhat limited to simple tasks, while some of the recent approaches in hierarchical RL use more complex domains (Atari, Minecraft etc). Ideally, the experiments should be extended to some of these more complex tasks.\n5. What are the theoretical properties of the proposed approach? Eg, is the proposed algorithm convergent? If Bi-POMDP is a POMDP, then one should be able to leverage POMDP results to build some theory here. If it is not a POMDP, then we need some understanding of how easy/hard a Bi-POMDP is to solve\n6. The paper contains many grammar problems and some broken references, and should be proof-read thoroughly \nIn summary, while the proposed approach is quite interesting and definitely worth exploring, the paper is not ready for publication in its current form.", "title": "No questions", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "ryOFjyP7g": {"type": "rebuttal", "replyto": "rJwIc9JXl", "comment": "Dear reviewer,\n\nFirst of all, thank you for your questions.\nConcerning the definition of Bi-POMDP, you are right, Bi-POMDP can be mapped to a POMDP with different types of actions ('classic' actions and 'acquisition' actions). We decided to introduce the term Bi-POMDP to avoid to define different types of actions, each time step thus corresponding to choosing one 'classic action'. It has the advantage to allow a simpler comparison with classical RL models, and to clearly state that we consider different possible types of observations, at different prices. We have added a small paragraph making the connection between Bi-POMD and POMDP explicit.\n\nThe latent variable o_t computed with y_t in Fig.1 represents the option latent vector. It is indeed related to \\sigma_t : the model doesn\u2019t use y_t if \\sigma_t=0, and uses it (and compute o_t) if \\sigma_t=1. We modified the legend of Fig.1  to make things clearer.\n\nWe are not sure to understand your remark on Algorithm 2. Line 6, \u201coption level\u201d, means that the model acquires the observation y_t, and compute a new option o_t based on y_t. We replaced \u201cgenerate a new option o_t\u201d by \u201ccompute a new option o_t=f(y_t)\u201d that is maybe more explicit.\nLine 7 (if it is what you meant), \u201cactor level\u201d, the state of the actor z_t is then initialized with the new option o_t and the other observation x_t. \n\nThe model has been compared with the classical recurrent policy gradient algorithm using a classical RNN architecture without the option level, but not with DQN that will certainly obtain the same kind of results on these tasks.\n\nAs explained in the paper, the lambda value is fixed to 0 at the beginning of the training and then slowly increased. The performances are not very dependant on the increasing speed for \\lambda which mainly change the learning speed.\n\nWe added the year and publisher for Bacon & Precup, we\u2019re sorry for the oversight.", "title": "Answers"}, "H1dzjyP7x": {"type": "rebuttal", "replyto": "BkO51g1Qx", "comment": "Thank you for your questions and remarks.\n\nConcerning the terminology, our goal was not to redefined well known concepts. We modified Section 2.2 to use standard terminology and to make the section clearer.\n\nConcerning the comparisons with AHMM architectures, we agree that our model has connections with the AHMM and added a paragraph on this point in the related work.  However, there are still many differences between BONN and AHMM:\n\n- contrary to others options models (and AHMM), BONN doesn\u2019t learn a discrete set of options but \u201ccontinuous vectors\u201d computed with neural networks. In BONN each option o_t is a vector computed based on y_t.\n\n- in AHMM, the state space is partitioned into a sequence of partitions P1, P2... corresponding to the levels of abstractions (section 3.2 of \u201cPolicy Recognition in the AHMM\u201d by Hung Bui et al.), which corresponds to our 2 different observations x_t and y_t. Then, each partition Pi is partitioned into regions R1, R2\u2026 and for each region a set of policies is defined. In BONN, only the space partition is defined (with the two levels xt and yt). In some cases, like in the rooms experiments, we can considered that the state space is partitioned into different regions like for AHMM (as the observations brutally change when changing room). However, BONN can be more general as it uses directly the observation x_t and y_t : it would also work for a robot that see some meters in front of him (this observation would be x_t) and do sometimes a scan of the room (the observation would be y_t). \nBONN would work exactly the same in this case, contrary to AHMM that would need to define regions. Moreover, in AHMM one must fix the number of policies in each region, and they terminates when the agent exit the region, which is not the case for BONN (where we really don\u2019t fix anything concerning the policies, and a policy could begin in a room and terminate two rooms after).\n\n- in BONN, options are not pre-defined by a user like in AHMM, but are discovered while minimizing the number of time a new option is computed (=\u2019cognitive effort\u2019)", "title": "AHMM"}, "rJwIc9JXl": {"type": "review", "replyto": "H1eLE8qlx", "review": "Why do you state that Bi-POMDP are more general than POMDP?  It seems every problem you describe for the Bi-POMDP case can be equally well modelled in a standard POMDP, introducing an action for choosing the sense y.\n\nSimilarly, the augmented reward (Eqn 3) seems like it can be re-written as a standard POMDP reward function, where there is a cost to applying the \u201cobserve y\u201d action.\n\nWhy is y in Fig.1 modelled as a latent variable?  This seems directly related to \\sigma.\n\nCan you add precise specification of lines 6 & 6 in Algorithm 2?\n\nDid you perform empirical comparison to any other option learning method, e.g. Bacon & Precup?  Also, any comparison to other method that doesn\u2019t use option, e.g. standard DQN?\n\nHow much data is necessary to fit the parameter \\lambda?\n\nWhat is the year and publisher for Bacon & Precup?The paper tackles the very important problem of learning options from data. The introduction of the budget constraint is an interesting twist on this problem, which I had not seen before (though other methods apply other constraints.)\n\nI must say I\u2019m not very convinced by the need to introduce the Bi-POMDP framework, where the conventional POMDP framework would do.  In discussions, the authors suggest this makes for simpler comparison with RL models, but I find that it rather obscures the link to POMDP models.\n\nThe proposed method makes an interesting contribution, distinct from the existing literature as far as I know.  The extension to discover a discrete set of options is a nice feature for practical applications.\n\nIn terms of the algorithm itself, I am actually unclear about lines 4 & 6.  At line 4, I don\u2019t know how \\sigma_t is computed. Can you give the precise equation?  At line 6, I don\u2019t know how the new option o_t is generated. Again, can you give the precise procedure?\n\nThe paper contains several empirical results, on contrasting simulated domains. For some of these domains, such as CartPole, it\u2019s really not clear that options are necessary. In my mind, the lack of comparison to other options learning methods is a limitation of the current draft.\n", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Syo2DYy4g": {"type": "review", "replyto": "H1eLE8qlx", "review": "Why do you state that Bi-POMDP are more general than POMDP?  It seems every problem you describe for the Bi-POMDP case can be equally well modelled in a standard POMDP, introducing an action for choosing the sense y.\n\nSimilarly, the augmented reward (Eqn 3) seems like it can be re-written as a standard POMDP reward function, where there is a cost to applying the \u201cobserve y\u201d action.\n\nWhy is y in Fig.1 modelled as a latent variable?  This seems directly related to \\sigma.\n\nCan you add precise specification of lines 6 & 6 in Algorithm 2?\n\nDid you perform empirical comparison to any other option learning method, e.g. Bacon & Precup?  Also, any comparison to other method that doesn\u2019t use option, e.g. standard DQN?\n\nHow much data is necessary to fit the parameter \\lambda?\n\nWhat is the year and publisher for Bacon & Precup?The paper tackles the very important problem of learning options from data. The introduction of the budget constraint is an interesting twist on this problem, which I had not seen before (though other methods apply other constraints.)\n\nI must say I\u2019m not very convinced by the need to introduce the Bi-POMDP framework, where the conventional POMDP framework would do.  In discussions, the authors suggest this makes for simpler comparison with RL models, but I find that it rather obscures the link to POMDP models.\n\nThe proposed method makes an interesting contribution, distinct from the existing literature as far as I know.  The extension to discover a discrete set of options is a nice feature for practical applications.\n\nIn terms of the algorithm itself, I am actually unclear about lines 4 & 6.  At line 4, I don\u2019t know how \\sigma_t is computed. Can you give the precise equation?  At line 6, I don\u2019t know how the new option o_t is generated. Again, can you give the precise procedure?\n\nThe paper contains several empirical results, on contrasting simulated domains. For some of these domains, such as CartPole, it\u2019s really not clear that options are necessary. In my mind, the lack of comparison to other options learning methods is a limitation of the current draft.\n", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkO51g1Qx": {"type": "review", "replyto": "H1eLE8qlx", "review": "\nFirst, please refrain from making up confusing and obfuscating terminology for concepts that are well defined using standard definitions. For example, equation 1 and the preceding equation need to be discarded. The correct and standard terminology to use here is that R(s1, a1, ....) is known as the discounted sum of rewards, or the return. It is confusingly termed here as the \"reward\". If R is the reward, what is r(s_t,a_t)? Also, J(\\pi) is confusingly NOT the expected reward, but the expected sum of rewards (the little r's, not the big R). The little \"r\" is also called \"reward\" and confuses your definition of R and r! Why make up terms on your own when the literature has standard terms, namely the value function, the return, and the discounted sum of reward etc.? (see Puterman, MDP textbook, or Sutton/Barto RL text, or Bertsekas/Tsitsiklis, NDP book). \n\nSecond, please cite and discuss in detail the work of Hung Bui et al., (Journal of AI Research) on the abstract hidden Markov model (or AHMM). It is *exactly* equivalent to your BONN architecture. How does your work differ from AHMM and please provide a detailed comparison, both experimentally and theoretically. There is also prior work on learning AHMMs, see Johns et al., AAAI 2005. This paper proposes an approach to learning hierarchical actions or options in reinforcement learning using the so-called BONN model (for budgeted options with neural networks). The approach is an interesting mixture of the old and the new. Some ideas seem very related to previous work in the literature, such as variants of hidden Markov models proposed by Hung Bui and others (abstract HMM, hierarchical POMDP by Theocharous et al., IROS 2005; Murphy et al., NIPS, ICRA). \n\nThe major difference is that unlike the prior work using a graphical model, this paper uses a gated recurrent network neural model to implement the learning of options from data. The approach is based on minimizing some quantity called the \"cognitive effort\", but this is confusingly explained, and not very precise. The basic idea here is to define a budget that modified the immediate reward, and so its minimization is viewed as minimizing cognitive effort. The approach seems a bit ad hoc. \n\nExperiments are reported on a variety of simple discrete and continuous control benchmark domains. ", "title": "Standardize terminology and relate your work to abstract HMMs", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "r1VyahzNg": {"type": "review", "replyto": "H1eLE8qlx", "review": "\nFirst, please refrain from making up confusing and obfuscating terminology for concepts that are well defined using standard definitions. For example, equation 1 and the preceding equation need to be discarded. The correct and standard terminology to use here is that R(s1, a1, ....) is known as the discounted sum of rewards, or the return. It is confusingly termed here as the \"reward\". If R is the reward, what is r(s_t,a_t)? Also, J(\\pi) is confusingly NOT the expected reward, but the expected sum of rewards (the little r's, not the big R). The little \"r\" is also called \"reward\" and confuses your definition of R and r! Why make up terms on your own when the literature has standard terms, namely the value function, the return, and the discounted sum of reward etc.? (see Puterman, MDP textbook, or Sutton/Barto RL text, or Bertsekas/Tsitsiklis, NDP book). \n\nSecond, please cite and discuss in detail the work of Hung Bui et al., (Journal of AI Research) on the abstract hidden Markov model (or AHMM). It is *exactly* equivalent to your BONN architecture. How does your work differ from AHMM and please provide a detailed comparison, both experimentally and theoretically. There is also prior work on learning AHMMs, see Johns et al., AAAI 2005. This paper proposes an approach to learning hierarchical actions or options in reinforcement learning using the so-called BONN model (for budgeted options with neural networks). The approach is an interesting mixture of the old and the new. Some ideas seem very related to previous work in the literature, such as variants of hidden Markov models proposed by Hung Bui and others (abstract HMM, hierarchical POMDP by Theocharous et al., IROS 2005; Murphy et al., NIPS, ICRA). \n\nThe major difference is that unlike the prior work using a graphical model, this paper uses a gated recurrent network neural model to implement the learning of options from data. The approach is based on minimizing some quantity called the \"cognitive effort\", but this is confusingly explained, and not very precise. The basic idea here is to define a budget that modified the immediate reward, and so its minimization is viewed as minimizing cognitive effort. The approach seems a bit ad hoc. \n\nExperiments are reported on a variety of simple discrete and continuous control benchmark domains. ", "title": "Standardize terminology and relate your work to abstract HMMs", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}