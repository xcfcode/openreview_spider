{"paper": {"title": "Towards Information-Seeking Agents", "authors": ["Philip Bachman", "Alessandro Sordoni", "Adam Trischler"], "authorids": ["phil.bachman@maluuba.com", "alessandro.sordoni@maluuba.com", "adam.trischler@maluuba.com"], "summary": "We investigate the behavior of models trained to answer questions by asking sequences of simple questions.", "abstract": "We develop a general problem setting for training and testing the ability of agents to gather information efficiently. Specifically, we present a collection of tasks in which success requires searching through a partially-observed environment, for fragments of information which can be pieced together to accomplish various goals. We combine deep architectures with techniques from reinforcement learning to develop agents that solve our tasks. We shape the behavior of these agents by combining extrinsic and intrinsic rewards. We empirically demonstrate that these agents learn to search actively and intelligently for new information to reduce their uncertainty, and to exploit information they have already acquired.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper proposes information gain as an intermediate reward signal to train deep networks to answer questions. The motivation and model are interesting, however the experiments fail to deliver. There is a lack of comparative simple baselines, the performance of the model is not sufficiently analyzed, and the actual tasks proposed are too simple to promise that the results would easily generalize to more useful tasks. This paper has a lot of good directions but definitely requires more work. I encourage the authors to follow the advice of reviewers and explore the various directions proposed so that this work can live up to its potential."}, "review": {"ryje4cgNx": {"type": "rebuttal", "replyto": "BytGYPeNe", "comment": "The intent of our paper is to explore the idea of information seeking, to propose several tasks which demand that an agent exhibit information seeking behaviour, and to show that existing techniques (with a bit of effort) are capable of solving these tasks. Our primary focus is not on the particular optimization method (i.e. GAE), model architecture (i.e. DNN), or training heuristics (e.g. information gain and various low-level tweaks that improved the performance of our models). Our focus is on the tasks themselves and whether or not agents can be trained to perform the information seeking required to solve them. We didn't discuss training details extensively in the paper, since that was not our focus, but some non-trivial tweaks and tricks were required for maximizing the performance of our models on these tasks.", "title": "response to review"}, "SkqDBodml": {"type": "rebuttal", "replyto": "BkYqj6PXl", "comment": "The information seeking tasks proposed in our paper are all standard RL tasks. The actions performed by our agents are the questions they ask. Question answering is performed by the environment in the settings we explore, and is thus not included in the set of actions for our agents. However, it would be possible to develop settings in which both asking and answering are performed by one or more agents, rather than by the environment.\n\nThe method we use for training our agents, i.e. Generalized Advantage Estimation, is a standard RL method which extends the advantage-based actor-critic approach used in A3C in a manner analogous to that in which TD(lambda) extends standard TD(k). GAE trains the policy using a weighted sum of n-step truncated actor-critic estimates of the policy gradient for many values of n. See the GAE paper by Schulman et al. for more details. We have not directly compared basic GAE with asynchronous GAE (which would represent the direct swap of GAE for standard actor-critic in A3C). Comparing against methods which use multiple slightly out-of-synch versions of the policy (i.e. A3C) for improved stability/exploration in the policy-gradient setting, and against methods which use value-based estimation (i.e. DQN) would be a useful addition to our current set of experiments. We wanted to a/b test policy gradient versus Q-learning for the original submission but were unable to do so due to time constraints.\n\nRegarding whether our comparison with DRAW is fair for the cluttered MNIST tasks, we directly address this point in the paper. The \"information efficiency\" criterion which we use for measuring agent performance was not explicitly considered by the developers of DRAW. This makes the comparison unfair in the sense that we designed our agents to perform well according to this criterion, while DRAW optimized only for final accuracy on this task. In our second setting our agent does not have access to more information than would be available to DRAW. If DRAW were to place its first glimpse such that the glimpse was at the center of the image and scaled to cover the entire image, it would receive roughly the same information as our agent receives through the summary (which is simply a 13x13 \"DRAW glimpse\" scaled to cover the entire image). Comparisons on new tasks or performance criteria are often challenging, since naive baselines can perform poorly enough to be uninformative and existing powerful methods have not been optimized for the new task/criteria. This is the case with our cluttered MNIST setting, so we chose an existing powerful method which offered a strong baseline according to our criterion.", "title": "Response to pre-review question."}, "BkYqj6PXl": {"type": "review", "replyto": "SyW2QSige", "review": "Can the proposed information task be posed as a standard RL task? I think if you map both asking and answering to actions then this is a special case of standard RL setting. Have you compared to standard RL methods including DQN and A3C?\n\nThe experiment section is cluttered and hard to understand. For example, is there a fair comparison between DRAW and proposed method for cluttered MNIST task? Looks like the first setting proposed method use smaller field of view and does worse while in the second setting the proposed method has more information (the summary) and does better. But none of the settings offer fair comparison.\nThis paper proposed to use Generalized Advantage Estimation (GAE) to optimize DNNs for information seeking tasks. The task is posed as a reinforcement learning problem and the proposed method explicitly promotes information gain to encourage exploration.\n\nBoth GAE and DNN have been used for RL before. The novelty in this paper seems to be the explicit modeling of information gain. However, there is insufficient empirical evidence to demonstrate the benefit and generality of the proposed method. An apple to apple comparison to previous RL framework that doesn't model information gain is missing. For example, the cluttered MNIST experiment tried to compare against Mnih et al. (2014) (which is a little out dated) with two settings. But in both setting the input to the two methods are different. Thus it is unclear what contributed to the performance difference.\n\nThe experiment section is cluttered and hard to read. A table that summarizes the numbers would be much better.", "title": "Question about comparisons", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BytGYPeNe": {"type": "review", "replyto": "SyW2QSige", "review": "Can the proposed information task be posed as a standard RL task? I think if you map both asking and answering to actions then this is a special case of standard RL setting. Have you compared to standard RL methods including DQN and A3C?\n\nThe experiment section is cluttered and hard to understand. For example, is there a fair comparison between DRAW and proposed method for cluttered MNIST task? Looks like the first setting proposed method use smaller field of view and does worse while in the second setting the proposed method has more information (the summary) and does better. But none of the settings offer fair comparison.\nThis paper proposed to use Generalized Advantage Estimation (GAE) to optimize DNNs for information seeking tasks. The task is posed as a reinforcement learning problem and the proposed method explicitly promotes information gain to encourage exploration.\n\nBoth GAE and DNN have been used for RL before. The novelty in this paper seems to be the explicit modeling of information gain. However, there is insufficient empirical evidence to demonstrate the benefit and generality of the proposed method. An apple to apple comparison to previous RL framework that doesn't model information gain is missing. For example, the cluttered MNIST experiment tried to compare against Mnih et al. (2014) (which is a little out dated) with two settings. But in both setting the input to the two methods are different. Thus it is unclear what contributed to the performance difference.\n\nThe experiment section is cluttered and hard to read. A table that summarizes the numbers would be much better.", "title": "Question about comparisons", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}