{"paper": {"title": "On Episodes, Prototypical Networks, and Few-Shot Learning", "authors": ["Steinar Laenen", "Luca Bertinetto"], "authorids": ["~Steinar_Laenen1", "~Luca_Bertinetto1"], "summary": "We analysed the effectiveness of episodic learning in Prototypical Networks and found out that, despite adding complexity and hyper-parameters, it severely affects its performance.", "abstract": "Episodic learning is a popular practice among researchers and practitioners interested in few-shot learning. It consists of organising training in a series of learning problems, each relying on small \u201csupport\u201d and \u201cquery\u201d sets to mimic the few-shot circumstances encountered during evaluation.\nIn this paper, we investigate the usefulness of episodic learning in Prototypical Networks, one of the most popular algorithms making use of this practice.\nSurprisingly, in our experiments we found that, for Prototypical Networks, it is detrimental to use the episodic learning strategy of separating training samples between support and query set, as it is\na data-inefficient way to exploit training batches. This \u201cnon-episodic\u201d version of Prototypical Networks, which corresponds to the classic Neighbourhood Component Analysis, reliably improves over its episodic counterpart in multiple datasets, achieving an accuracy that is competitive with the state-of-the-art, despite being extremely simple.", "keywords": ["few-shot learning", "meta-learning", "metric learning", "deep learning"]}, "meta": {"decision": "Reject", "comment": "This paper is right on the borderline. It questions the utility of episodic training from a novel perspective, driven by a comparison to NCA, with thorough experiments. The hypothesis that more pairwise comparisons per batch/episode benefit learning is also quite interesting, but some reviewers didn\u2019t feel this was convincingly presented.\n\nPrototypical networks are indeed a popular method for FSL, but I do as well think that NCA is more closely related to matching networks, and that it makes more sense for that to be the focus of experimentation. Matching networks involve more direct pairwise comparisons, and so a leave-one-out baseline with this model would probably be a useful comparison.\n\nWhile I appreciate the desire to focus on a fundamental aspect of FSL and not chase state of the art, I think that it\u2019s important to show where one should go from here. That is, as the reviewers pointed out there are many mechanisms beyond vanilla PNs that have yielded better results than those presented in this paper. I don\u2019t think matching SOTA is necessary here, but it would be nice to show that the insights here complement other mechanisms in FSL.\n"}, "review": {"3r0gbxU-Dfd": {"type": "review", "replyto": "_TGlfdZOHY3", "review": "Summary: This paper proposes to use neighborhood component analysis in lieu of prototype loss to train embedding functions of few-shot learning. This method takes full advantage of relations between all sampled points in an episode to facilitate learning, and it removes the distinction between support and query samples during meta-training time.\n\nReason for score: Overall, I lean towards reject. This paper proposes an interesting method that improves upon Prototypical Networks, and performs on-par with other baseline methods. However, the paper does not advance our understanding of how episodic training interacts with few-shot learning beyond the addition of more performance numbers to compare against. \n\nPros: The proposed method is a straight-forward improvement to Prototypical networks. In the wide range of scenarios evaluated in the experiments, the proposed method consistently outperforms ProtoNet. This proposed method should also be easy to implement, making integration with other FSL methods based on ProtoNets feasible. \nWhile not exactly nouveau, using NCA to train embedding functions has not been done before (to my best knowledge) and is a theoretically sound approach.\n\nCons:\n1. Perhaps unsurprisingly, the performance of NCA is lower than some FSL methods that use additional capacity. Even so, I think presenting only favorable comparisons in the performance tables is counter productive as it fails to capture the research context of this work. \n2. The conclusion from the ablation studies on batch size is still unclear to me. The \u201cNCA fixed batch composition\u201d setting seems to perform better than NCA in 3 of the 4 plots in figure 4. This particular setting is interesting as it allows control of the number of classes in each batch. As the batchsize is fixed in the ablation, we won\u2019t know what is the relation between the number of ways and performance of this fixed batch variant of NCA. This ablation is also confusing in that neither \u201cno proto\u201d nor \u201cno S/Q\u201d significantly improves performance, yet their combination performs well. A full ablation that systematically covers all hyperparameters would be helpful in furthering understanding in this direction.\n3. The motivation of the paper feels unclear: on one hand the authors claim that they aim at understanding the (un)usefulness of episodic learning, yet on the other hand this paper doesn\u2019t present any results beyond the final performance number to aid with this understanding. Visualizations and/or theoretical arguments would be greatly appreciated.\n\nMinor points (suggestions, not related to score):\nIntroduction\n\u201cThese results legitimately cast a doubt\u201d -> \u201cThese results cast a doubt\u201d\nRelated Work\n\u201cBetween 2016 and 2017\u201d feels unnecessary\n\u201cMatching and Prototypical\u2026weighted by either an LSTM or a simple average, respectively\u201d: Matching Networks also proposes a sample average variant. Their main difference lies in one uses cosine similarity while the other uses euclidean distance.\n\u201cDifferently from these papers\u201d -> Different\nBACKGROUND AND METHOD\nIn 3.3, point 3 is not necessarily correct. Why would some examples be more likely than others in the episodic scheme? All FSL benchmarks (used in this paper) are close to class balanced, and an hierarchical sampling scheme for episodes results in full coverage each epoch just like standard supervised learning.\nIn 3.4 \u201cwhich is the probability that image i is sampled from image j\u201d. This is under the assumption that each support image defines a Gaussian in the embedding space. This is not true in general. The probabilistic interpretation of NCA should be further explained. \n\n[Post rebuttal]\nI am still leaning against the acceptance of this paper due to concerns about the limited impact of this submission. The topic of \"limitations of episodic training\" has been widely studied and in fact most SoA few-shot learning methods adopt a combination of supervised and episodic training for this precise reason. \n\nThe exploration into the relation between the number of sample pairs and learning performance is indeed correct, but no strong conclusions can be reached due to the logical jumps required. The authors established that the performance is correlated with number of pairs with a log/square-root curve, and that ProtoNet performance is similar to that of subsampled NCA (fig3). The mechanism behind why this is has not been elucidated. I think many questions can be explored to strengthen this paper, for example:\nAre classes embedded tighter together? \nAre hard negatives pushed further apart? \nDoes NCA induce a non-euclidean geometry that is more expressive than the euclidean geometry naturally induced by ProtoNets?\nCan the classification problem be converted into a pair comparison problem, so that PAC learning theory can be used to explain the shape of this curve? \nWhat is the sample complexity of the NCA classifier compared to the Prototypical classifier?\n\nRegarding the proposed method, NCA is certainly an improvement over ProtoNets, but performs worse than existing methods in most experiments. This makes me doubtful of the impact of this work on the methodological front. Better modelling of relationships between few-shot examples has been implicitly and explicitly studied too. For instance, many works adopt sample level set functions in the form of transformers, attention modules, and graph neural networks. Arguably, these additional architectures are more expressive \"deep\" alternatives to NCA, and hence achieves better performance than the proposed method.", "title": "Review of On Episodes, Prototypical Networks, and Few-Shot Learning ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "FgXSA1gvj_B": {"type": "review", "replyto": "_TGlfdZOHY3", "review": "The paper's starting point is the question whether the episodic training is beneficial, or not, for FSL / Prototypical Networks. The work can be seen as a follow-up of the recent works showing that simple baselines can outperform rather sophisticated few-shot learning models. Towards answering this question, this paper points out that Prototypical Networks (PN) are related to Neighborhood Component Analysis (NCA), and NCA can be considered as an episodic training-free alternative of PN.\n\nIn more detail, PN aims to learn per-class prototypes based on sample averaging in the feature space. NCA, in contrast, aims to maximize the ratio of total similarity between same-class example pairs to the total similarity between different-class pairs. Due to their similarities in terms of their formulations, the paper claims that NCA loss can be considered as an alternative to PN loss to do non-episodic representation learning for few-shot learning purposes. In addition, the paper has a few strong claims, such as episodic training is \u201cdetrimental to learning\u201d and \u201cunder no circumstance beneficial to differentiate between support and query set within a training batch\". Clearly, these are intriguing claims.\n \nHowever, there is a gap between the claims and the experimental validation. First, even if ProtoNet loss and NCA loss seem to be similar to each other, they're nevertheless different models, and it takes quite a significant manipulation to convert PN to NCA. Therefore, the fact one particular non-episodic-training based model gives superior results compared to those of PN with episodic-training, does tell us much about detrimental effects of episodic training for PN or in general. Second, while the paper's observations that NCA has the advantage of using more pairwise similarities within a batch compared to PN is indeed insightful, it rather points out to certain weaknesses in the way per-batch / per-episode data is being utilized by the PN formulation, instead of problems about episodic training.\n\nOverall, the paper has interesting observations about PN's weaknesses and shows why one particular simple non-episodic training / non meta-learned approach (NCA) can yield superior results compared to PN, which is a relatively mode sophisticated & well-established approach. However, the paper's (over-strong) claims remain mostly unsupported, which makes the otherwise interesting work poorly framed. The paper, with more water-tight arguments only, could otherwise be a valuable contribution but it requires quite significant & fundamental revisions throughout the paper, therefore, is not ready for publication in its current form.\n\nPost-rebuttal: I would like to thank for the detailed responses and the careful revisions made in the paper. Overall, the paper is now definitely improved in certain ways and initiates an important discussion on the value of episodic training & classifier synthesis for few-shot learning, as opposed to typically-simpler metric-learning based approaches. The paper also approaches this problem from an interesting point of view, by focusing on sample utilization in the episodic training of PN.  \n\nHowever, I still find that the the paper remains somewhat weak in its current form for the following reasons:\n- I maintain my view that NCA vs PN are not direct alternatives to each other, considering that PN allows learning a representation that is optimized for class-average to sample comparisons, whereas, NCA uses a sample-to-sample distance based loss. The fact that the very construction of these two models, despite the similarities pointed out, blurs the strength of the overall NCA vs PN based discussion on the value of episodic training.\n- The claims about the advantages of non-episodic training of NCA is mainly based on the observation that NCA creates more positive/negative labels. However, it is not clear whether it is the more efficient utilization of training samples or just the differences in terms of the predictive model formulations. (Perhaps, averaging based prototype computation is a bad idea, after all, which may not have directly anything to do with episodic training.)\n- To this end, Fig. 3 is indeed interesting, but again the results are not very clear. Here, careful optimizer re-tuning specially for each case can be necessary as subsampling degrades the gradient approximation quality, which creates the question how much fundamentally important efficiency in batch utilization is, as long as one uses a proper optimizer. \nOverall, I think the paper makes a valuable step in an interesting direction but the paper fails to make a strong-enough case. Overall, I  improve my rating by a single level to 4, but find that the paper is not stronger than this in its current form.", "title": "Interesting work, unsupported claims", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "9NHkZZJK-Oo": {"type": "rebuttal", "replyto": "Rn6JpS-8Wn6", "comment": "We thought that repeating this for other architectures was unfeasible from a computational budget perspective, and thus we opted to do an as thorough as possible analysis on the most popular architecture.\n\n\\> _\u201cFEAT and DeepEMD use episodic learning and report Resnet12 results\u201d_\n\nGiven the number of papers on the topic (about 5 a week in the last year alone), the comparison with the state-of-the-art of Sec. 4.4 cannot possibly be exhaustive.\nInstead, it is intended to be a way to contextualise the vanilla NCA performance with respect to other modern _simple_ methods. Since we do not claim to improve state-of-the-art but only to be competitive, and given that our contributions revolve around furthering the understanding of previous methods and not around the proposal of a new one, we thought this would have been sufficient.\nWe slightly edited the text in Sec. 4.4 to make our criteria clearer.\n\nFEAT and DeepEMD are two interesting papers that came out this year and test on ResNet12 backbone. We did not include them in our comparison for the same reason we did not include the self-distillation variant of [Tian et al.]: they present a fairly complicated setup (in particular, they both perform adaptation at test time) with insights that are likely to be applicable also to our method.\nMore specifically, the FEAT paper experiments with different set-to-set strategies to adapt the model learned on seen classes to the new classes presented at test time. From Figure 2b in their paper, it should be clear how their contribution is in the adaptation layer, which could be used in our implementation as well. Moreover, the introduction of these set-to-set functions requires additional capacity in the form of several FC layers (see Fig.5 of their paper).\nDeepEMD shows the benefit of working with local information (as opposed to image-level embeddings) in a metric-learning FSL framework.\nTo do so, the average pooling layer is removed and individual images are considered as a collection of local embeddings rather than individual representations.\nThis is an important insight, but it is likely to provide improvement for different methods too, including the vanilla NCA.\nMoreover, also here during evaluation part of the model is adapted at test time.\nAs an aside, note that since DeepEMD formulation expressly does not require a functional separation between query and support images (see Fig.2 of their paper), the conclusions on data-inefficiency of PNs we draw in our paper might also benefit this method (although of course this needs to be proven empirically)\n\n\\> _\u201cin your rebuttal, you quantified how many pairs of relationships are used in each ablation setting. These interesting observations could be connected and presented more coherently in the manuscript.\u201d_\n\nWe appreciate the suggestion to improve the clarity of the discussion.\nWe have included in the ablation study (Sec. 4.3) a discussion on the number of pairs visited in each variant.\nFurthermore, in Appendix A.9 we have also included a discussion on the effect of the number of pairs for the experiment of Fig. 2, to which we refer in the answer to AnonReviewer4 (the second reviewer).\n\nWe relate these new observations to our extended discussion about the number of pairs at the end of Sec. 3.3.\n\n\\> _\u201cCould a theoretical framework on sampling pairwise relationships and generalization be established to explain the issues of episodic learning.\u201d_\n\nWe are not entirely sure what it is meant here with \u201ctheoretical framework\u201d.\nIf the request is to outline more precisely the extent of the data-inefficiency brought by episodic sampling in PNs, we hope that the extension of the combinatorial analysis we added in the latest update is satisfactory.\n\nAnother interpretation is that this reviewer would appreciate a framework that is capable of generalising the conclusions that can be drawn from our work to any method that samples batches with episodic learning.\nThis is surely interesting, but we believe it should be considered as future work that can be inspired by our results, and not as something to be already expected as a contribution to our paper.\n\nMoreover, an all-encompassing framework that covers generic meta-learning algorithms could result in a very challenging endeavor, unless strong simplifications are introduced.\nUnlike Prototypical Networks (and the simple version of Matching Networks that we considered), for many meta-learning algorithms the support set and the query set have different functional roles, i.e. training and testing a base-learner within the meta-learning inner loop (see [Hospedales et al.]).\nAs such, the sampling-inefficiency component outlined in this paper will be inevitably confounded with an algorithmic design that is hard to ablate without significantly altering its formulation, thus making it hard to design conclusive experiments.\n\nWe thank again this reviewer for their time and valuable insights. We hope that our answers address the concerns raised.", "title": "Additional answers to AnonReviewer1 - part 3/3"}, "ugIVEZg1Cpa": {"type": "review", "replyto": "_TGlfdZOHY3", "review": "#### Summary\n\nThe submission investigates the properties of episodic training and its impact on learning using Prototypical Networks as a case study. The paper draws a connection between Prototypical Networks and Neighbourhood Component Analysis (NCA), noting that their loss functions are similar but that NCA is trained non-episodically, which allows it to learn from the relationship between all example pairs in a batch.\n\nWhen controlling for batch size, the paper claims to show that NCA (combined with a nearest-centroid inference strategy) performs better than Prototypical Networks, as evidenced by experiments on CIFAR-FS and mini-ImageNet. Ablation experiments are performed, claiming to show that applying the NCA loss to batches sampled episodically allows Prototypical Networks to bridge the performance gap with NCA, and that the partition of examples within a batch into support and query sets is detrimental to Prototypical Networks training. Finally, NCA is evaluated alongside comparable competing approaches on mini-ImageNet, CIFAR-FS, and tiered-ImageNet, and is claimed to yield results comparable or superior to the state-of-the-art.\n\n#### Strengths and weaknesses\n\n* **+** The value of episodic training is increasingly being questioned, and the submission approaches the topic from a new and interesting perspective.\n* **+** The connection between nearest-centroid few-shot learning approaches and NCA has not been made in the literature to my knowledge and has potential applications beyond the scope of this paper.\n* **+** The paper is well-written, easy to follow, and well-connected to the existing literature.\n* **-** The extent to which the observations presented generalize to few-shot learners beyond Prototypical Networks is not evaluated, which may limit the scope of the submission\u2019s contributions in terms of understanding the properties of episodic training.\n* **-** The Matching Networks / NCA connection makes more sense in my opinion than the Prototypical Networks / NCA connection.\n* **-** A single set of hyperparameters was used across learners for a given benchmark, which can bias the conclusions drawn from the experiments.\n\n#### Recommendation\n\nI\u2019m leaning towards acceptance. I have some issues with the submission that are detailed below, but overall the paper presents an interesting take on a topic that\u2019s currently very relevant to the few-shot learning community, and I feel that the value it brings to the conversation is sufficient to overcome the concerns I have.\n\n#### Detailed justification\n\nThe biggest concern I have with the submission is methodological. One one hand, the authors went beyond the usual practice of reporting accuracies on a single run and instead trained each method with five different random initializations, and this is a practice that I\u2019m happy to see in a few-shot classification paper. On the other hand, the choice to share a single set of hyperparameters across learners for a given benchmark leaves a blind spot in the evaluation. What if Prototypical Networks are more sensitive to the choice of optimizer, learning rate schedule, and weight decay coefficient than NCA? Is it possible that the set of hyperparameters chosen for the experiments happens to work poorly for Prototypical Networks? Would we observe the same trends if we tuned hyperparameters independently for each experimental setting? In its current form the submission shows that Prototypical Networks are sensitive to the hyperparameters used to sample episodes *while keeping other hyperparameters fixed*, but showing the same trend while doing a reasonable effort at tuning other hyperparameters would make for a more convincing argument. This is why I take the claim made in Section 4.2 that \"NCA performs better than all PN configurations, no matter the batch size\" with a grain of salt, for instance.\n\nI also feel that the submission misses out on an opportunity to support a more general statement about episodic training via observations on approaches such as Matching Networks, MAML, etc. I really like the way Figure 1 explains visually how Prototypical Networks miss out on useful relationships between examples in a batch and is therefore data-inefficient. To me, this is one of the submission\u2019s most important contributions: the suggestion that a leave-one-out strategy could allow episodic approaches to achieve the same kind of data efficiency as non-episodic approaches, alleviating the need for a supervised pre-training / episodic fine-tuning strategy. To be clear, I don\u2019t think the missed opportunity would be a reason to reject the paper, but I think that showing empirically that the leave-one-out strategy applies beyond Prototypical Networks would make me lean more strongly towards acceptance.\n\nThe connection drawn between Prototypical Networks and NCA feels forced at times. In the introduction the paper claims to \"show that, without episodic learning, Prototypical Networks correspond to the classic Neighbourhood Component Analysis\", but Section 3.3 lists the creation of prototypes as a key difference between the two which is not resolved by training non-episodically. From my perspective, NCA would be more akin to the non-episodic counterpart to Matching Networks without Full Contextual Embeddings \u2013 albeit with a Euclidean metric rather than a cosine similarity metric \u2013 since both perform comparisons on example pairs.\n\nThis relationship with Matching Networks could be exploited to improve clarity. For instance, row 6 of Figure 4 can be interpreted as a Matching Networks implementation with a Euclidean distance metric. With this in mind, could the difference in performance between \"*1*-NN with class centroids\" and *k*-NN / Soft Assignment noted in Section 4.1 \u2013 as well as the drop in performance observed in Figure 4\u2019s row 6 \u2013 be explained by the fact that a (soft) nearest-neighbour approach is more sensitive to outliers?\n\nFinally, I have some issues with how results are reported in Tables 1 and 2. Firstly, we don\u2019t know how competing approaches would perform if we applied the paper\u2019s proposed multi-layer concatenation trick, and the idea itself feels more like a way to give NCA\u2019s performance a small boost and bring it into SOTA-like territory. Comparing NCA without multi-layer against other approaches is therefore more interesting to me. Secondly, 95% confidence intervals are provided, but the absence of identification of the best-performing approach(es) in each setting makes it hard to draw high-level conclusions at a glance. I would suggest bolding the best accuracy in each column along with all other entries for which a 95% confidence interval test on the difference between the means is inconclusive in determining that the difference is significant.\n\n#### Questions\n\n1. In Equation 2, why is the sum normalized by the total number of examples in the episode rather than the number of query examples?\n1. Can the authors comment on the extent to which Figure 2 supports the hypothesis that NCA is better for training because it learns from a larger number of positives and negatives? Assuming this is true, we should see that Prototypical Networks configurations that increase the number of positives and negatives should perform better for a given batch size. Does Figure 2 support this assertion?\n1. Can the authors elaborate on the \"no S/Q\" ablation (Figure 4, row 7)? What is the point of reference when computing distances for support and query examples? Is the loss computed in the same way for support and query examples? The text in Section 4.3 makes it appear like the loss for query examples is the NCA loss, but the loss for support examples is the prototypical loss. Wouldn\u2019t it be conceptually cleaner to compute leave-one-out prototypes, i.e. leave each example out of the computation of its own class\u2019 prototype (resulting in slightly different prototypes for examples of the same class)? In my mind, this would be the best way to remove the support/query partition while maintaining prototype computation, thereby showing that the partition is detrimental to Prototypical Networks training.\n\n#### Additional feedback\n\n1. This is somewhat inconsequential, but across all implementations of episodic training that I have examined I haven\u2019t encountered an implementation that uses a flag to differentiate between support and query examples. Instead, the implementations I have examined explicitly represent support and query examples as separate tensors. I was therefore surprised to read that \"in most implementations [...] each image is characterised by a flag indicating whether it corresponds to the support or the query set [...]\"; can the authors point to the implementations they have in mind when making that assertion?\n1. I would be careful with the assertion that \"during evaluation the triplet {w, n, m} [...] must stay unchanged across methods\". While this is true for the benchmarks considered in this submission, benchmarks like Meta-Dataset evaluate on variable-ways and variable-shots episodes.\n1. I\u2019m not too concerned with the computational efficiency of NCA. The pairwise Euclidean distances can be computed efficiently using the inner- and outer-product of the batch of embeddings with itself.\n", "title": "Review", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "bvSgT0ilRNP": {"type": "rebuttal", "replyto": "_TGlfdZOHY3", "comment": "We thank all the reviewers for their time and valuable comments.\n\nWe answered each reviewer in-thread, and we have edited our submission incorporating the suggestions.\n(Apologies for the deleted messages in the OpenReview forum, we made a mistake with the markdown).\n\nBelow a list of the main changes: \n* As suggested by AnonReviewer4, we have included results for Matching Networks in Appendix A.7. We show that the episodic batch setup is detrimental in this case as well, which corroborates the findings of the main paper.\n* We have extended the analysis on the number of pairs exploited by different batch strategies (Appendix A.8), showing that NCA can exploit a number of extra pairs that grows as $O(w^2(m^2+n^2))$ (where $w$, $m$ and $n$ are the number of ways, queries and shots respectively).\n* We have included detailed comments on how we chose the (non-episodic) hyper-parameters for PNs and NCA to ensure a fair comparison (Appendix A.5) \n* We have reported and discussed the ablation experiments also for batch sizes 128 and 512 (Appendix A.7).\n* In Sec 4.1, we have commented on the influence of outliers in the performance gap between the Soft Assignment and 1-NN with Centroid.\n* We have also made small edits throughout the text, to address the reviewers suggestions and to improve the clarity of some of our points.\n", "title": "General response"}, "Rn6JpS-8Wn6": {"type": "rebuttal", "replyto": "Zbu5UKkWm9s", "comment": "\\> _\u201cHonestly, I am still confused on whether to judge this paper as a methodological paper or an understanding paper.\u201d_\nUsing the terminology suggested by this reviewer, our paper mostly falls into the \u201cunderstanding\u201d category.\n\nOur goal was not to develop a new SOTA method.\nInstead, motivated by the recent success of simple cross-entropy-based methods in FSL,  we wanted to investigate whether episodic learning is always a correct choice, and we started with a case study on Prototypical Networks, which is arguably one of the most popular algorithms and has largely influenced the community (it has 1700 citations, and many methods are directly based on them).\n\nOur paper draws the connection between Prototypical Networks and NCA, where NCA can be seen as the non-episodic variant of PNs (and of Matching Network too, as shown in the experiments of Appendix A.7).\nWith our analysis, we show that the fundamental problem of the episodic strategy in PNs is the poor data efficiency.\n\nNonetheless, we believe our paper also provides a minor practical contribution, as the vanilla NCA makes a simple and appealing baseline for future work.\n\n\\> _\u201cOther works have also made similar observations and have come up with methods to address this problem. \u201c_\n\nTo the best of our knowledge, the analysis we did on Prototypical and Matching Networks is novel, and we kindly invite the reviewer to specify which works they are referring to.\nThere are papers that show how non-episodic, cross-entropy-based methods can perform well, which we cited in the paper since the Introduction (e.g. [Chen et al.; Wang et al.; Dhillon et al.; Tian et al.]. However, that is still considerably different from proposing a case study which illustrates where the shortcomings of a previous method are, and how to overcome them.\nImportantly, we believe that the insight we provide is important because it puts into discussion what was thought to be the edge brought by episodic learning, as it can be gathered by these quotes:\n> [Snell et al., 2017] _\u201cThe use of episodes makes the training problem more faithful to the test environment and thereby improves generalization.\u201d_\n\n > [Vinyals et al., 2016]: _\u201c[..] our training procedure is based on a simple machine learning principle: test and train conditions must match. Thus [we] train our network to do rapid learning [..], much like how it will be tested when presented with a few examples of a new task.\u201d_\n\nInstead, for Prototypical Networks and Matching Networks this episodic structure is at the core of a serious data-inefficiency problem, which actually negatively affects the performance.\n\nAs we mentioned in the paper, the use-case of [Raghu et al.] is the paper most similar to ours in spirit. They show that MAML does not change its internal representation very rapidly, but rather learns features during training which are reused at test time.\nWe believe that our paper, [Raghu et al.] and the several cross-entropy baselines mentioned above can be considered, together, as an indication for the community that episodic learning should be considered with caution when designing a new FSL method.\n\n\\> _\u201cMost recent FSL papers report experimental results using several backbone architectures [..] This is an important point if the authors claim the methodological advances as an important contribution.\u201d_\n\nAs discussed in the points above, the aim of our paper is to understand a phenomenon, and not to beat the state-of-the-art. For this reason, we dedicated our computational budget mainly to the many experiments of Sec. 4.1 to 4.3.\n\nWe can see that in Fig.2 we trained 16 different setups (there were more for 1-shot PNs that we did not plot since they performed significantly worse), for both CIFAR-FS and miniImageNet (Fig. 5 in Appendix A.7).  Moreover, each number that we report is the average accuracy over 50k episodes, where we trained 5 models with 5 different seeds and evaluated them on 10k episodes for each seed, which has been particularly appreciated by another reviewer.\n_\u201c[..] the authors went beyond the usual practice of reporting accuracies on a single run and instead trained each method with five different random initializations, and this is a practice that I\u2019m happy to see in a few-shot classification paper\u201d._\n\nThis means that for Fig. 2 and 5, we already had to train 120 models - excluding any hyper-parameter search and the episodic batch setups we did not report.  \nNote that this suite of experiments has been helpful in determining the best set of episodic hyper-parameters to use for the PNs implementation we compare against in the two tables of Sec. 4.4.\n\n[continues below]\n", "title": "Additional answers to AnonReviewer1 - part 2/3"}, "Zbu5UKkWm9s": {"type": "rebuttal", "replyto": "lczoUYgg_fq", "comment": "We thank the reviewer for their reply and further comments.\n\n\\> _\u201cI agree the toy example presented in figure 1 is certainly interesting, but it is nonetheless a toy example.\u201d_\n\n\\> _\u201cWhile I don't claim that PN uses point-to-point connections more efficiently than NCA, this figure [Fig.1] does not provide strong arguments against that claim.\u201d_\n\nAt the end of Sec. 3.3, we specify the combinatorial formulas to exactly compute the number of positives and negatives in each setting, and show that the number of pairs in the non-episodic setting is strictly larger than in the episodic setting. The toy example is useful to visually outline the discrepancy between the two setups, but the observations can be generalised to any batch composition.\nTo make this more clear, we now report the total number of pairs contributing to the loss in Table 1. Moreover, in Appendix A.8 we derive how the total number of pairs used in the loss of NCA is strictly greater than the one used in the loss of PNs, no matter the batch size and the hyper-parameters used for the episode composition, and how that difference grows exponentially, and as $O(w^2(m^2+n^2)$.\nMore precisely, the number of _extra_ pairs that the NCA loss can utilise from a batch is $\\frac{1}{2}w(w(m^2 + n^2) - m - n)$.\nUsing for example the episodic batch composition used by [Snell et al.] for their 5-shot model training (w=20, m=15, n=5), we can see how this difference is important (about 50k unused pairs for a batch of size 400).\n\nWe hope that this extension to the combinatorial analysis can show more immediately the important difference in batch exploitation between the methods considered, and we thank the reviewer for having pointed in this direction with their comments.\n\nOne could still posit that it is possible that, for PNs, the way the pairs are sampled within a batch is beneficial despite not all of them are exploited, e.g. by somehow mimicking the episode composition seen at test time.\nIn the second experiment of Sec. 4.2 (from the paragraph now titled \u201cEpisodic batches vs. random sub-sampling of standard batches) we demonstrated how this is actually not the case. We show that even if we sample only a _subset_ of the distances used in the NCA loss computation, we still get better - and at most equal - performance compared to episodic setups for prototypical networks which use an equivalent number of distance pairs.\n\nThis result shows that, in PNs, there is nothing inherently special about the episodic strategy, and it is at best equivalent to randomly discarding pairs within the NCA framework.\nThis experiment thus shows that PNs do not use point-to-point connections more efficiently than NCA, and that it is beneficial to fully exploit the data within a batch.\n\n\\> _\u201cThere is also the fact that as gradients are back propagated through the centroid to each support embedding in ProtoNets, all pairs of support-query connections are still being used.\u201d_\n\nThis is correct, and all the \u201cpairs\u201d that we have counted take this into account already, which includes the previous answer in this thread, and also in the answer to AnonReviewer4 (the second reviewer). Notice that we stated in the original submission, at the end of Sec.3.3: _\u201cWhen considering the number of distances contributing to the loss, points within the support  set  count  independently  and  thus  we  get  to wmn= 9 positives  and w(w\u22121)mn=18 negatives.\u201d_\nWhereas, if we only considered distances with centroids, the number of positives and negatives would be 3 and 6, respectively.\nGiven your remark we have now made this more clear in the paper.\n\n\\> _\u201cWhat I find missing are illustrative experiments that show how NCA's more efficient use of data improves the learned embedding function. Are support embeddings of different classes better separated? Or are embeddings of the same class more well grouped?\u201d_\n\nNote that the evaluation framework used in all our experiments already implies this, because the inductive bias on which nearest-neighbour methods  are based is that points with the same labels are closer together in the embedding space.\nAs such, better FSL performance does indeed translate to better class separateness between the learned embeddings.\n\n[continues below]", "title": "Additional answers to AnonReviewer1 - part 1/3"}, "_QYoOFQo_Yv": {"type": "rebuttal", "replyto": "3r0gbxU-Dfd", "comment": "We thank this reviewer for their time, comments and suggestions.\n\n\\> _\u201cThis paper proposes an interesting method [..] However, the paper does not advance our understanding of how episodic training interacts with few-shot learning beyond the addition of more performance numbers.\u201d_\n\n\\> _\u201cThis paper doesn\u2019t present any results beyond the final performance number to aid with this understanding.\u201d_\n\nShowing a difference in performance is surely part of our analysis (Fig.2, Table ~1~ 3 and ~2~ 4).\nHowever, we believe we go well beyond \u201cthe addition of more performance numbers\u201d.\nWe show why and how episodic learning in PNs is considerably data inefficient in several ways:\n* Intuitively, with a simple toy-case (the visualization of Fig.1) that shows how, by dividing the batch in support and query (sub-)set, many distances that would constitute useful training information are simply ignored.\n* By connecting the loss of PNs to the NCA in several steps (equations 2, 3, 4, 5 and 6)\n* By demonstrating that, by using episodes, PNs is not better than using the NCA loss while randomly discarding training examples from each batch (Fig.3).\n\nWe would also like to point out to the reviewer a comparison against Matching Networks [Vinyals et al.] we reported in Appendix A.7, as suggested by AnonReviewer4 (the second reviewer). The results corroborate those obtained for Prototypical Networks in the main paper: the separation of roles between images in the support and query sets, typical of episodic learning, is detrimental to the performance of not only PNs, but also Matching Networks.\n\n\\> _\u201cVisualizations and/or theoretical arguments would be greatly appreciated.\u201d_\n\nWe provide the reader with several visualizations to illustrate our points (Fig.1 to Fig.4).\nSome of them are illustrative, others detail the experimental results.\nIn particular, we would like to report a quote from AnonReviewer4: \u201cI really like the way Figure 1 explains visually how Prototypical Networks miss out on useful relationships between examples in a batch and is therefore data-inefficient. To me, this is one of the submission\u2019s most important contributions\u201d\n\n\\> _\u201cunsurprisingly, the performance of NCA is lower than some FSL methods that use additional capacity [..] presenting only favorable comparisons [..] is counter productive\u201d_.\n\nDuring the design of our experimental protocol, because of the large number of NCA/PNs configurations considered, we opted to perform experiments on the most popular architecture used in the evaluations of the recent literature, i.e. ResNet12.\nWhen choosing against which methods to compare in the final tables, given the very high number of papers on the topic, we clearly had to select a subset of them.\nWe decided to select this subset by picking recent methods making use of episodic learning or presenting simple high-performing baselines based on pre-training with the cross-entropy loss.\nWe do actually compare against methods that perform similarly or better than us. To make it more evident, we edited the tables by bolding the best method in each column, along with all other entries for which a 95% confidence interval test results in a non meaningful difference.\nIn the paper update, we removed the multi-layer variant from the tables, as it is not very informative and it does not change the conclusions.\n\nNonetheless, if we missed important comparisons please let us know which ones and we will update the tables accordingly.\n\n[continues below]", "title": "Answer to AnonReviewer1 - part 1/2"}, "lt-Ql_h2js": {"type": "rebuttal", "replyto": "A_g1zDJJWzi", "comment": "\\> _\u201cI am curious if you have done a comparison with baseline NCA, i.e. equation (3). I have not found the comparison in A.1 which only contains some discussions but no direct comparison.\u201d_\n\nWe used a vanilla NCA throughout the experiments.\n* The NCA loss (eq. 3) is used to train all the methods that are referred to as NCA in the plots and tables of Section 4. The NCA is optimised during training and there is no test-time adaptation (except for one experiment in Appendix A.1).\n* Section A.1 from the Appendix seeks to understand whether adapting to the support set provided during the evaluation by learning a positive semi-definite matrix is beneficial. Results are actually provided, in Table 5 in the Appendix, and show only small improvements. As we said in A.1: \u201cgiven the computational cost, we opted for non performing adaptation to the support sets in our experiments of Sec. 4 [the experimental section of the paper]\u201d.\n\nDoes this answer this reviewer\u2019s question? We were unsure how to interpret this comment.\n\nWe would also like to point out to the reviewer a comparison against Matching Networks [Vinyals et al.] we reported in Appendix A.7. The results corroborate those obtained for Prototypical Networks in the main paper: the separation of roles between images in the support and query sets, typical of episodic learning, is detrimental to the performance of not only PNs, but also Matching Networks.\n\n\\> _\u201cI would like to follow the discussions on the paper and understand the contributions well.\u201d_\n\nWe hope that our answers have addressed this reviewer\u2019s comments about our submission.\nIf not, we are happy to continue the conversation.\n\n* [Vinyals et al.] Matching Networks for One Shot Learning - NeurIPS 2016\n* [Zhang et al.]Understanding deep learning requires rethinking generalization - ICLR 2017", "title": "Answer to AnonReviewer3 - part 2/2"}, "A_g1zDJJWzi": {"type": "rebuttal", "replyto": "oVtw72_1b6S", "comment": "We thank this reviewer for their comments and their interest in following the discussion. The review contains three comments, that we address in order below:\n\n\\> _\u201cThe sections 3.1, 3.2 and 3.3 are not the contributions of the paper [..] I do not see the technical contributions of the paper other than the claimed novel experimental settings\u201d_\n\nWe titled Section 3 \u201cBackground and method\u201d because it was important to give details on episodic learning, Prototypical Networks and NCA in order to introduce the readers to our contributions, which are mainly in Section 4, the experimental section.\nIn particular, our contributions are:\n * We highlighted the connection between NCA and PNs, and demonstrated that for PNs \u201cepisodes\u201d are a data-inefficient way of exploiting the training signal available in a batch.\n * We showed empirically that, for PNs, episodic learning achieves an analogous performance to discarding distance pairs, at random, from a standard mini-batch using the NCA (Fig.3).\n* We showed that training this vanilla NCA loss, an extremely simple method with almost no hyper-parameters, leads to performance that is competitive with the state-of-the-art.\n\nThese results are novel and, given the relevance of Prototypical Networks (~1700 citations) and the great amount of work it has influenced, should be of significant interest for this community.\n\nWe would like to highlight some of AnonReviewer4\u2019s (the 2nd reviewer) comments, which highlight the contributions of this paper from an external perspective: _\u201cThe value of episodic training is increasingly being questioned, and the submission approaches the topic from a new and interesting perspective.\u201d_; and\n_\u201cThe connection between nearest-centroid few-shot learning approaches and NCA has not been made in the literature to my knowledge and has potential applications beyond the scope of this paper.\u201d_\n\nRegarding the lack of technical contribution mentioned by this reviewer: ICLR\u2019s guidelines stress on the importance of novel findings, which applies to our case.\nFor instance, \u201cUnderstanding deep learning requires rethinking generalization\u201d [Zhang et al.] has been nominated as one of the three best papers from 2017\u2019s edition of ICLR and does not introduce anything new, algorithmically; its contributions revolve around the experimental findings.\nClearly, we do not want to qualitatively compare ourselves to that paper, we just wanted to show that it is possible for a paper to be accepted (and sometimes to thrive) mainly for the novel insights obtained by the experimental analysis.\n\n\\> _\u201cThe paper shows a robust experimentations and comparisons with prior arts, however, I don't understand how the three settings mentioned in the section 3.4 are evaluated.\u201d_\n\nApologies if this was not clear. We simply tried all the variants to perform classification on the same model trained with the NCA, and we picked the 1-NN with centroids for the rest of the experiments as it was the best.\nWe discussed this in the third paragraph of Section 4.1: \u201cWe compared the inference methods discussed in Sec. 3.4 on miniImageNet and CIFAR-FS. Results can be found in Table ~3~ 2.  We chose to use 1-NN with class centroids in all our experiments, as it performs significantly better than k-NN or Soft Assignment.\u201d\n\nTable ~3~ 2 used to be in the Appendix, but for clarity we moved it in the main text, close to where it is referenced.\n\n[continues below]", "title": "Answer to AnonReviewer3 - part 1/2"}, "UVTuwdBzdAe": {"type": "rebuttal", "replyto": "ugIVEZg1Cpa", "comment": "We thank this reviewer for their time on writing a very detailed and insightful review.\n\n\\> _\u201cNCA would be more akin to the non-episodic counterpart of Matching Networks without Full Contextual Embeddings \u2013 albeit with a Euclidean metric rather than a cosine similarity metric \u2013 since both perform comparisons on example pairs.\u201d_\n\n\\> _\u201cShowing empirically that the leave-one-out strategy applies beyond Prototypical Networks would make me lean more strongly towards acceptance\u201d_\n\nWe initially considered including Matching Networks, but eventually we decided against it for practical reasons:\n* The original paper proposes multiple variants.\n* The original paper uses cosine similarity, which has been subsequently shown to be a poor choice in the FSL setting.\n* To the best of our knowledge, an official implementation of Matching Networks does not exist.\n\nHowever, we agree that Matching Networks are very relevant for us, and we thank this reviewer for specifying a setting to extend our analysis.\nWe followed the suggestion and repeated the ablation analysis of Fig.4 to Matching Networks without contextual embeddings and with a Euclidean metric.\nResults are illustrated in Section A.7, and corroborate those coming from PNs analysis: the separation of roles between images in the support and query sets is also significantly detrimental to the performance of Matching Networks\n\n\\> _\u201cThe choice to share a single set of hyperparameters [..] leaves a blind spot in the evaluation\u201d_.\n\nDuring the experimental design, we believe we dedicated a significant effort in ensuring apple-to-apple comparisons against a very competitive implementation of PNs.\nAs a testimony of this effort, the results achieved by our implementation of PNs are very competitive (see for example the comparison to recent papers where architectures of similar capacity were used [Wang et al.], [Chen et al.])\nHowever, in our submission we did not do a great job in explaining our choices.\n\nIn general, every time we did something that departed from the original PNs implementation, we verified that this was beneficial also for PNs. In particular:\n* We always use the normalisation adopted in SimpleShot [Wang et al.], which is beneficial also for PNs.\n* In the comparison tables of Sec.4, we always used PNs\u2019 5-shot model, which in our implementation always outperforms the 1-shot model (for both 1- and 5-shot). Instead, [Snell et al.] train and test with the same number of shots.\n* Apart from the episodes hyper-parameters of PNs, which we did search and optimise over to create the plots of Fig.2, the only other hyper-parameters of PNs are those related to the training schedule, which are the same as NCA. \nTo set them, we started from the simple SGD schedule used by SimpleShot [Wang et al.] and only marginally modified it by increasing the number of training epochs to 120, increasing the batch size to 512 and setting weight decay and learning rate to 5e-4 and 0.1 respectively. We observed that these changes were beneficial for both NCA and PNs.\n\nA few points to show empirically that the small set of choices we made for the hyper-parameters are beneficial for both NCA and PNs and that we conducted fair comparisons:\n\na) As a sanity check, we tried to train PNs with the learning schedule used in [Snell et al] for both miniImageNet and CIFAR-FS, and we observed consistently inferior performance with respect to what we obtained with ours (between -1% and -2.5% depending on the dataset).\n\nb) We trained both the NCA and PNs with the training schedule used in SimpleShot [Wang et al., 2019]. Detailed results are reported in Table ~5~ 6 in the Appendix. The schedule we used in the paper is considerably better for both PNs and NCA.\n\nc) Finally, we have run a small 4x3 grid search for the learning rate and weight decay hyper-parameters. For both NCA and PNs, we used the best setup from Fig.2.\nResults on the validation set of CIFAR-FS are reported in the ASCII tables below.\n\nPerformance refers to testing on 1-shot / 5-shot. Confidence intervals are omitted for reason of space, but are between 0.15 and 0.20.\n\n[continues below]\n", "title": "Answer to AnonReviewer4 - part 1/3"}, "KoBxsNgB3Qy": {"type": "rebuttal", "replyto": "_QYoOFQo_Yv", "comment": "\\> _\u201cA full ablation that systematically covers all hyperparameters would be helpful\u201d_\n\n\\> _\u201cThe \u201cNCA fixed batch composition\u201d setting seems to perform better than NCA in 3 of the 4 plots in figure 4. This particular setting is interesting as it allows control of the number of classes in each batch.\u201d_\n\nWe agree that doing an ablation for each hyper-parameter would be interesting, but would also make the number of experiments explode, considering that we are running 5 seeds per configuration. To still address the spirit of this comment, we repeated the experiment of Fig.4 with different batch sizes.\nResults are reported in Appendix A.7 and can help us explain the discrepancy between NCA and NCA with fixed batch composition (the second quoted question above).\nThe difference between \u201cNCA with fixed-batch composition\u201d and NCA is the highest with batch size 128 (1.3% on average), decreases with batch size 256 (0.5% on average) and becomes negligible with batch size 512 (NCA is actually 0.2% better).\n\nThis is likely due to the number of positives available in an excessively small batch size.\nSince our vanilla NCA relies on using distance pairs and creates batches by simply sampling images randomly from the dataset, there is a limit to how small a batch can be.\nAs an example, consider the extreme case of a batch of size 4.\nFor the datasets considered, it is very likely that such a batch will contain no positive pairs.\nFor a batch size of 128 and a training set of 64 classes, with a parameter-free sampler the NCA will have in expectation only one positive pair per class.\nConversely, the NCA ablation with a fixed batch composition (i.e. with a set number of images per class)  will have a higher number of positive pairs (at the cost of a reduced number of classes per batch).\nWe believe this can explain the difference, as positive pairs constitute a less frequent (and potentially more informative) training signal.\nFor the sake of simplicity, and since this only affects smaller batch sizes, we opted to use a vanilla, parameter-free sampler for the NCA in the rest of our experiments.\nNotice that, for batch size 512, there is even a slight (0.2%) decrease in performance using the fixed-batch composition w.r.t. the vanilla NCA.\n\nWe thank the reviewer for this suggestion and we believe that this is an interesting result. Understanding how to further improve NCA\u2019s sampling strategy (a choice which is likely to be highly dependent on the distribution of classes within a dataset, besides the batch size) is an avenue for future work.\n\n\\> _\u201cThis ablation [Fig4, row 6 and 7] is also confusing in that neither \u201cno proto\u201d nor \u201cno S/Q\u201d significantly improves performance, yet their combination performs well.\u201d_\n\nWe thank this reviewer for pointing it out.\nWe believe this phenomenon can be explained by counting the number of distance pairs contributing to the loss in each ablation. Using the formulas described at the end of Section 3.3, we compute the number of positives and negatives used for each ablation. \nFor row 6 there are 480 positives, and 14,880 negatives. For row 7 there are 576 positives and 20,170 negatives. In both cases, the number is significantly lower than the corresponding NCA, which gets 896 positives and 31,744 negatives, which is drastically more than either row 6 or 7 and likely explains the jump in performance that this reviewer pointed out.\nMoreover, from row 5/6 to 7 we see a slight increase in performance, which can also be explained by the (slightly) larger number of distance pairs used in the loss. \n\n\n\\> [Sec 3.3] _\u201cPoint 3 is not necessarily correct. Why would some examples be more likely than others in the episodic scheme?\u201d_\n\nWe simply meant that, by sampling episodes with replacements, it is unlikely that all the images of the dataset will be visited. By doing a full pass of the dataset, instead, this is guaranteed. In any case, we did not observe this being a meaningful difference in our ablation studies.\n\n\\> Thanks for the minor comments detailed at the end of the review - we edited the paper to address them.\n\nWe hope that the above answers address the reviewer's comments - we are happy to discuss further.", "title": "Answer to AnonReviewer1 - part 2/2"}, "GtQqX7QbymE": {"type": "rebuttal", "replyto": "tvrcW8Eyefp", "comment": "At first glance, this does not fully support the hypothesis. However, we can see that the number of positive pairs is much higher in a=16 than in a=8. Since the positive pairs constitute a less frequent (and potentially more informative) training signal, this can explain the difference. The a=32 variant has an even higher number of positives than a=16, but the loss in performance there could be explained by a drastically lower number of negatives, and by the fact that the number of ways used during training is lower.\nSo, while indeed generally speaking the higher number of pairs the better (which is also corroborated by Figure 3, as we move right on the x-axis both the NCA and PNs performances increase), one should also consider how this interacts with the positive/negative balance and the number of classes present within a batch.\n\n\\> _\u201cCan the authors elaborate on the \"no S/Q\" ablation (Figure 4, row 7)? What is the point of reference when computing distances for support and query examples? Is the loss computed in the same way for support and query examples?\u201d_\n\nThe way the ablation is performed is as follows: we take all the images in the query set, and all the prototypes in the support set. Then, the NCA loss is computed on the union of the query points and prototypes - there is no difference in loss computation between both sets. Equation (5) in Appendix A.2 formulates the loss precisely. We apologise if this was not clear, and we have adapted the text accordingly.\n\n\\> _\u201cWouldn\u2019t it be conceptually cleaner to compute leave-one-out prototypes? [..] In my mind, this would be the best way to remove the support/query partition while maintaining prototype computation\u201d_\n \nWe thought  that the current form of the ablation is a simple way to go from 5-shot prototype training to the \u201cno S\\Q\u201d ablation without changing the batch setup drastically. In the current form,  the combination of the \u201cno prototype\u201d ablation with  the \u201cno S\\Q\u201d ablation naturally leads to the NCA, as shown in Equations (4), (5), and (6). We think this additive property of the ablation is important in showing the independent effect of each one.\n\nThe ablation suggested by the reviewer is neat, and does remove the support/query separation while maintaining prototype computation. However, due to its leave-one-out nature, all possible positive distance pairs will be seen in the loss function. This is because the distance of each point will be computed to prototypes consisting of the remaining points in the set of images from the same class, which essentially recovers all of the gradient signal. Part of what we wanted to investigate from the ablation is whether generating prototypes in the support set also leads to loss of training signal, as there are no positive pairs being computed between images belonging to the same prototype.\n\n**Additional feedback**\n\n\\> _\u201cI haven\u2019t encountered an implementation that uses a flag to differentiate between support and query examples.\u201d_\n\nWe believe the term \u201cflag\u201d is unclear and creates confusion - we modified the text accordingly.\nWhat we meant is that each sample in a batch is either belonging to the support or the query set, and that these are mutually exclusive (and thus can be represented as a flag).\n\n\\> _\u201cBenchmarks like Meta-Dataset evaluate on variable-ways and variable-shots episodes\u201d_\n\nThanks for pointing this out - we corrected the sentence accordingly.\n\n\\> _\u201cI\u2019m not too concerned with the computational efficiency of NCA.  The pairwise Euclidean distances can be computed efficiently using the inner- and outer-product of the batch of embeddings with itself.\u201d_\n\nWe agree, and that\u2019s the implementation we used to efficiently compute pairwise Euclidean distances. We opted for writing the remark on computational efficiency because we thought someone could be interested in possible practical scenarios far from few-shot classification, such as segmentation (where samples correspond to pixel embeddings), or scenarios with a strict computational budget (e.g. real-time processing).\n\nWe hope that the above answers address the reviewer's comments - we are happy to discuss further.\n\n* [Chen et al.] A Closer Look at Few-shot Classification - ICLR 2019\n* [Snell et al.] Prototypical Networks for Few-shot Learning - NeurIPS 2017\n* [Wang et al.] SimpleShot: Revisiting Nearest-Neighbor Classification for Few-Shot Learning - arXiv:1911.04623\n", "title": "Answer to AnonReviewer4 - part 3/3"}, "GJu6ea7LFis": {"type": "rebuttal", "replyto": "zd5p5m0VIcE", "comment": "\\> _\u201cWhile the paper observation [..] is indeed insightful, it rather points out to certain weaknesses in the way per-batch / per-episode data is being utilized by the PN formulation, instead of problems about episodic training.\u201d_\n\nWe believe this comment arises from the misunderstanding from point 1 above, because the _\u201cway per-episode data is being utilized by the PN formulation\u201d_ is what we mean when we say that PNs use episodic training.\nPNs\u2019 \u201cinner loop\u201d [Hospedales et al, 2020] amounts to no parameter adaptation, and thus the \u201cepisodic\u201d nature of PNs is limited to how episodes are sampled and what role each point within an episode has (support points have to form prototypes and their distances to corresponding query points have to be minimised).\nHence, we think that this reviewer might believe that we are claiming that our results apply to the entire episodic learning literature. This is not the case, as we qualified the results being specific to PNs throughout the text. We hope that the two re-worked sentences detailed in the previous answer clarify the confusion.\n\nMoreover, please note that, following the suggestion of AnonReviewer4 (the second reviewer), we applied the ablation analysis of Fig.4 also to Matching Networks [Vinyals et al.], showing analogous results to the ones obtained for PNs: the separation of roles between images in the support and query sets, typical of episodic learning, is detrimental to the performance of not only PNs, but also Matching Networks.\nClearly, this does not extend the considerations of this paper to algorithms that have not been tested, but we believe these are important results, and that they are timely within this community.\n\n\\> _\u201cIt takes significant manipulation to convert PN to NCA\u201d_\n\n\\> _\u201cThe fact one particular non-episodic-training based model gives superior results compared to those of PN with episodic-training, does [not] tell us much about detrimental effects of episodic training for PN.\u201d_\n\nThe similarity between PNs and NCA is stated by the very authors of PNs [Snell et al.], who consider it as the closest method in their related work section: _\u201cour method is most similar to the non-linear extension of NCA [..] A key distinction [..] is that we form a softmax directly over classes, rather than individual points\u201d_.\nStarting from the PNs loss, the NCA loss is an intuitive \u201cepisode-free\u201d baseline because it does not rely on the roles of query and support set.\nImportantly, this frees us from the burden of tuning the three (hyper-)parameters {ways, queries, shots} that need to be specified to construct an episode.\n\nWe hope that our comments address the concerns this reviewer has - we are happy to discuss further.\n\n* [Chen et al., 2019] A Closer Look at Few-shot Classification - ICLR 2019\n* [Hospedales et al.] Meta-Learning in Neural Networks: A Survey - arXiv:2004.05439\n* [Snell et al.] Prototypical Networks for Few-shot Learning - NeurIPS 2017\n* [Vinyals et al.] Matching Networks for One Shot Learning - NeurIPS 2016\n* [Wang et al.] SimpleShot: Revisiting Nearest-Neighbor Classification for Few-Shot Learning - arXiv:1911.04623\n", "title": "Answer to AnonReviewer2 - part 2/2"}, "zd5p5m0VIcE": {"type": "rebuttal", "replyto": "FgXSA1gvj_B", "comment": "We thank this reviewer for their time and comments, to which we answer below.\n\n\\> _\u201cThe paper's (over-strong) claims remain mostly unsupported, which makes the otherwise interesting work poorly framed.\u201c_\n\n\\> _\u201cThere is a gap between the claims and the experimental validation.\u201d_\n\nWe would kindly ask this reviewer to be more specific - in what way are our claims mostly unsupported? Reading the review, it seems that the reviewer concern is that we are claiming our findings are applicable to all methods making use of episodic learning. This is not our intention, and we qualify our work multiple times, starting from the title. However, we understand where we might have been unclear, and we propose a simple way to improve the clarity of the text below.\nWe would like to address two partial quotes from our abstract that are mentioned in the second paragraph of this review, as this reviewer has called them _\u201cintriguing\u201d_, but _\u201cstrong\u201d_.\n\n**1.**  _\u201cwe investigate the usefulness of episodic learning in Prototypical Networks [..] it is under no circumstance beneficial to differentiate between support and query set within a training batch.\"_\nWe did evaluate PNs in a variety of settings and, despite the fact that our implementation of PNs has a higher performance than the ones reported in recent literature (e.g. [Wang et al.], [Chen et al.]), we found it to always be outperformed by NCA (that\u2019s what we meant with \u201cunder no circumstances\u201d).\nHowever, we believe the \u201cunder no circumstance\u201d qualifier might be misinterpreted, as it could sound like we are referring to all the algorithms making use of episodic training. This is not our intention and we thank the reviewer for pointing this out.\nWe changed the sentence to _\u201cwe found that, for Prototypical Networks, it is detrimental to use the episodic learning strategy of separating training samples between support and query set, as it is a data-inefficient way to exploit training batches.\u201d_\nTo avoid possible ambiguities, we have also edited a sentence in the last paragraph of the Introduction, specifying again that the conclusions regard Prototypical Networks (while in the submitted version this was implicit from the sentence before).\n\n**2.** _\u201c[episodic training] is detrimental to learning\u201d_.\nWe wrote that, for Prototypical Networks, episodic learning is detrimental to _performance_, and we believe we provided arguments and data to justify this.\nWe showed it empirically on a number of different setups (Fig.2, Fig.4) and we show that the episodic strategy of PNs is not better than using the NCA and randomly discarding pairs from within a batch (Fig. 3).\nMoreover, we show the intuition behind why this is the case by analysing the losses (eq. 2 to 6) and using a simple visualization to better illustrate the concept (Fig.1)\n\n[continues below]\n", "title": "Answer to AnonReviewer2 - part 1/2 - we believe the low score might come from a misunderstanding around what we intend to claim"}, "tvrcW8Eyefp": {"type": "rebuttal", "replyto": "UVTuwdBzdAe", "comment": "[PNs with a=16, 5-shots, batch-size=512]\n~~~\n+-----------+---------------+---------------+---------------+\n|       \t |   wd = 1e-3   |   wd = 5e-4   |   wd = 1e-4  |\n+-----------+---------------+---------------+---------------+\n| lr = 0.5  | 44.61 / 57.66 | 56.12 / 70.95 | 59.01 / 74.31 |\n| lr = 0.1  | 59.89 / 74.94 | 60.46 / 75.53 | 58.97 / 73.75 |\n| lr = 0.05 | 61.08 / 76.02 | 60.91 / 75.79 | 59.28 / 74.24 |\n| lr = 0.01 | 59.78 / 74.64 | 58.82 / 73.85 | 58.34 / 73.22 |\n+-----------+---------------+---------------+---------------+\n~~~\n\n[NCA, batch-size=512]\n~~~\n+-----------+---------------+---------------+---------------+\n|       \t |   wd = 1e-3   |   wd = 5e-4   |   wd = 1e-4  |\n+-----------+---------------+---------------+---------------+\n| lr = 0.5  | 50.28 / 64.31 | 58.24 / 72.83 | 61.23 / 75.72 |\n| lr = 0.1  | 62.52 / 76.97 | 62.87 / 77.10 | 61.45 / 75.70 |\n| lr = 0.05 | 63.49 / 77.55 | 62.77 / 76.90 | 61.29 / 75.58 |\n| lr = 0.01 | 61.70 / 75.92 | 60.85 / 75.08 | 60.04 / 74.32 |\n+-----------+---------------+---------------+---------------+\n~~~\n\nNotice how the performance of NCA and PNs are highly correlated across the board, and it is not the case that PNs are more sensitive to the learning rate or weight decay. For both PNs and NCA, the maximiser is actually at {wd=1e-3, lr=0.05} and achieves about 0.5% better performance than what we used (i.e. wd=5e-4, lr=0.1); however, this is to be expected because we chose the hyper-parameters early in the project. As a matter of fact, our focus initially was not the comparison between NCA and PNs, and we were running exploratory experiments on both as baselines.\n\n\\> [Re. the] _\u201cmulti-layer concatenation trick, idea feels more like a way to give NCA\u2019s performance a small boost. Comparing NCA without multi-layer against other approaches is therefore more interesting\u201d_.\n\nWe agree the focus should be on the vanilla NCA, and that concatenating features from different layers might turn to be an advantage for other methods too.\nWe removed the multi-layer variant from the main tables and deferred the description of its effects to Appendix A.1.\nAll the considerations made in Sec. 4.4 still apply: vanilla NCA can still be considered competitive, and sometimes (on CIFAR-FS) superior, to the state-of-the-art.\n\n\n\\> _\u201cI would suggest bolding the best accuracy in each column\u201d_.\n\nWe agree this improves legibility - we edited the tables accordingly.\n\n\\> _\u201ccould the difference in performance between \"1-NN with class centroids\" and k-NN / Soft Assignment [..] be explained by the fact that a (soft) nearest-neighbour approach is more sensitive to outliers?\u201d_\n\nWe agree with the reviewer that this is the most likely explanation.\nSince the classes between training and evaluation are disjoint, the model is unlikely to produce calibrated probabilities.\nAs such, within the softmax, outliers behaving as false positives can happen to highly influence the final decision, and those behaving as false negatives can end up being almost completely ignored (their contribution is squashed to zero).\nWith the nearest centroid classification approach outliers are still clearly an issue, but their effect can be less dramatic.\n\n**Questions**\n\n\\> _\u201cIn Equation 2, why is the sum normalized by the total number of examples in the episode rather than the number of query examples?\u201d_\n\nThanks for pointing this out. This is a typo and has now been corrected.\n\n\\> _\u201cCan the authors comment on the extent to which Figure 2 supports the hypothesis that NCA is better for training because it learns from a larger number of positives and negatives? Assuming this is true, we should see that PNs configurations that increase the number of positives and negatives should perform better for a given batch size. Does Figure 2 support this assertion?\u201d_\n\nThis is a good observation, and we believe that Fig. 2 does indeed support the hypothesis, with some minor caveats that we now discuss. Below, we plot a table outlining the number of positives and negatives (gradients contributing to the loss) for the NCA and different episodic configurations of PNs. We consider the case where the batch size is 512.\n~~~\n+------+-----------------+-------+--------+-----------------+\n| rank |   method\t    | # pos | # neg  | # total pairs  |\n+------+------------------+-------+--------+----------------+\n|\t0 | NCA     \t    |  1792 | 129024 |    \t    130816 |\n|\t1 | PNs 5-shot a=16 |  1760 |  54560 |     \t    56320   |\n|\t2 | PNs 5-shot a=8  |   960 |  60480 |     \t    61440  |\n|\t3 | PNs 5-shot a=32 |  2160 |  32400 |     \t    34560  | \n|\t4 | PNs 1-shot a=8  |   448 |  28224 |     \t    28672  |\n+------+------------------+-------+--------+----------------+\n~~~\nFirst, notice that the ranking can almost be fully explained by the number of total pairs in the right column, except for 5-shot a=16 and 5-shot a=8. \n\n[continues below]", "title": "Answer to AnonReviewer4 - part 2/3"}, "oVtw72_1b6S": {"type": "review", "replyto": "_TGlfdZOHY3", "review": "This paper investigates the usefulness of episodic learning in prototypical learning which is a popular practice in few-shot learning. The authors propose a non-episodic prototypical network which basically corresponds to the classical neighborhood component analysis and they claimed that this network reliably improves over its episodic counterpart in multiple datasets. I have the following comments on the paper:\n\n1. The sections 3.1, 3.2 and 3.3 are not the contributions of the paper. Only section 3.4 can be considered as something new from experimental point of view and not methodologically new. k-NN, 1-NN with class centroids and soft assignments are all some specific experimental settings. Therefore, I do not see the technical contributions of the paper other than the claimed novel experimental settings which is also marginal.\n\n2. The paper shows a robust experimentations and comparisons with prior arts, however, I don't understand how the three settings mentioned in the section 3.4 are evaluated in the tables.\n\n3. I am curious if you have done a comparison with baseline NCA, i.e. equation (3). I have not found the comparison in A.1 which only contains some discussions but no direct comparison.\n\n4. Anyways, The paper is written in good English and I haven't found any typos yet.\n\nBased on my current understandings and above comments, currently I recommend for a weak rejection. However, I would like to follow the discussions on the paper and understand the contributions well.", "title": "Technical contributions are not clear and experiments are generic", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}