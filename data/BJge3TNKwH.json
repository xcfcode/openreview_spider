{"paper": {"title": "Sliced Cramer Synaptic Consolidation for Preserving Deeply Learned Representations", "authors": ["Soheil Kolouri", "Nicholas A. Ketz", "Andrea Soltoggio", "Praveen K. Pilly"], "authorids": ["skolouri@hrl.com", "naketz@hrl.com", "a.soltoggio@lboro.ac.uk", "pkpilly@hrl.com"], "summary": "\"A novel framework for overcoming catastrophic forgetting by preserving the distribution of the network's output at an arbitrary layer.\"", "abstract": "Deep neural networks suffer from the inability to preserve the learned data representation (i.e., catastrophic forgetting) in domains where the input data distribution is non-stationary, and it changes during training.  Various selective synaptic plasticity approaches have been recently proposed to preserve network parameters, which are crucial for previously learned tasks while learning new tasks. We explore such selective synaptic plasticity approaches through a unifying lens of memory replay and show the close relationship between methods like Elastic Weight Consolidation (EWC) and Memory-Aware-Synapses (MAS).  We then propose a fundamentally different class of preservation methods that aim at preserving the distribution of internal neural representations for previous tasks while learning a new one. We propose the sliced Cram\\'{e}r distance as a suitable choice for such preservation and evaluate our Sliced Cramer Preservation (SCP) algorithm through extensive empirical investigations on various network architectures in both supervised and unsupervised learning settings. We show that SCP consistently utilizes the learning capacity of the network better than online-EWC and MAS methods on various incremental learning tasks.", "keywords": ["selective plasticity", "catastrophic forgetting", "intransigence"]}, "meta": {"decision": "Accept (Spotlight)", "comment": "The paper addresses an important problem (preventing catastrophic forgetting in continual learning) through a novel approach based on the sliced Kramer distance. The paper provides a novel and interesting conceptual contribution and is well written. Experiments could have been more extensive but this is very nice work and deserves publication."}, "review": {"SkxaLppiiS": {"type": "rebuttal", "replyto": "BylVcusk5r", "comment": "Thank you very much for a positive evaluation of our work and for setting time aside to carefully read our paper. We appreciate your valuable time spent on serving the community. Below please find our responses. \n\n\n1) \"I think the experimental section relies heavily on MNIST (e.g. permuted MNIST, auto-encoding MNIST), which I think is not a hard enough task. It is though widely used in the continual learning community, though maybe it should not anymore. But I think given the reliance of the field on these datasets it makes sense, plus I think the strength of the paper is not in the empirical evaluation but rather in the derivation of the method.\"\n\nWe agree with the reviewer that our experimental section heavily relied on the MNIST dataset. Reviewer #3 also raised the same point, however, as you mentioned the main strength of the paper is certainly not in our empirical evaluation. To that end,  we added the experiments requested by Reviewer #3 on MNIST-to-SVHN and sequential learning on CIFAR-100. We demonstrate that the results on the new datasets are consistent with the ones reported in the original submission. Please see the extended supplementary material in the revised manuscript.\n\n2) \"I think while the authors put quite a bit of effort in explaining the difference between KL and Cramer distance, I would have appreciated a even more detailed exposition. I think the difference between these metrics is not well understood by the majority in the community.\"\n\nThis is a great point. Although we are restricted by the page limit of the conference, we did our best to revise the manuscript to further clarify the characteristics of these metrics and made additional connections to integral probability metrics (IPMs).\n\nYet again we thank the reviewer for a candid evaluation of our work. ", "title": "Response to Official Blind Review #1"}, "Byg4_nTisr": {"type": "rebuttal", "replyto": "SJlTfBCMqH", "comment": "Thank you for the positive evaluation of our work and your insightful comments. We appreciate your valuable time spent on serving the community. Below please find our responses.  \n\nWe agree with the reviewer that the neuro-scientific terms used in our paper might make our manuscript, unnecessarily, hard to follow for many readers. To that end, we have revised the Introduction and Preliminaries sections and added rigorous definitions of these terms to provide a more enjoyable read for the readers.\n\nWe agree with both reviewers that the permuted-MNIST task is artificial and overly simple, which was precisely the reason for us to include the results on the SYNTHIA dataset in the paper. We also agree with the reviewer that our article could benefit from additional experiments; hence, we performed the requested experiments on MNIST-to-SVHN and sequential learning on CIFAR-100. Please see the extended supplementary materials in the revised manuscript. \n\nRegarding the comparison with IMM and PGMA, we first note that we have added these methods to our references. Next, we would like to point out the fundamental differences between the methods included in our work (i.e., online-EWC, MAS, and SCP) and IMM and PGMA. Below we enumerate these differences: \n\nThe core idea behind IMM [2] is to learn a new model for the new task using the old optimized parameters as an initialization and then consolidate the old and new models into a single model via moment matching (mean-IMM or mode-IMM). However, in our case, we only utilize one model and use the concept of synaptic plasticity, where we selectively rigidify the network parameters to preserve the previously learned representation while learning a new task. As you can see, there is a fundamental difference between the two approaches, which makes the direct comparison challenging. \n\nPGMA [3] is yet another fascinating approach that is fundamentally different from, and arguably significantly more complicated than our proposed method. At its core, PGMA uses the concept of memory replay, which is an orthogonal approach to selective synaptic plasticity that is the focus of our work. In short, PGMA learns an autoencoder-based generative model for old tasks. To avoid catastrophic forgetting, PGMA then replays samples from old tasks using its generative model while learning the new task. \n\nGiven: 1) the significant differences between IMM and PGMA and our proposed method, 2) the short period for the response, and 3) the unfortunate overlap of the ICLR rebuttal deadline and the CVPR2020 submission deadline, we could not provide the comparison to these methods at this time.   \n\nFinally, we have updated the caption of Figure 6 to make it easier to follow. In particular, we clarify that the blue and red shadings on the plots indicate the durations in which the models were trained on ``summer'' and ``winter'' data, respectively.\n\n[1] Aljundi et al. Memory Aware Synapses: Learning what (not) to forget, ECCV 2018.\n\n[2] Lee et al. Overcoming Catastrophic Forgetting by Incremental Moment Matching, NIPS 2017.\n\n[3] Hu et al. Overcoming Catastrophic Forgetting for Continual Learning via Model Adaptation, ICLR 2019.", "title": "Response to Official Blind Review #3"}, "BylVcusk5r": {"type": "review", "replyto": "BJge3TNKwH", "review": "I think the paper is written quite well, and the approach makes a lot of sense. I think the idea of replacing generalizing the KL to sliced Cramer distance is quite interested. The authors put in some effort in explaining why they propose this alternative distance between distributions. \n\nI think overall this is a great paper, very informative. If I need to nitpick, I think the experimental section relies heavily on MNIST (e.g. permuted MNIST, auto-encoding MNIST), which I think is not a hard enough task. It is though widely used in the continual learning community, though maybe it should not anymore. But I think given the reliance of the field on these datasets it makes sense, plus I think the strength of the paper is not in the empirical evaluation but rather in the derivation of the method. \n\nI think while the authors put quite a bit of effort in explaining the difference between KL and Cramer distance, I would have appreciated a even more detailed exposition. I think the difference between these metrics is not well understood by the majority in the community. ", "title": "Official Blind Review #1", "rating": "8: Accept", "confidence": 4}, "SJlTfBCMqH": {"type": "review", "replyto": "BJge3TNKwH", "review": "[Summary]\nThis paper proposes a new method for overcoming catastrophic forgetting in continual learning, based on distribution-based regularization using the sliced Cramer distance, i.e. Sliced Cramer Preservation (SCP). Unlike previous work on catastrophic forgetting, this paper tackles unsupervised learning scenarios as well as supervised learning. They evaluate the proposed SCP on permutated MNIST, sequential learning in autoencoder task, and sequential learning for segmentation. \n\n[Pros]\n- This paper tackles unsupervised learning scenarios beyond the classification on benchmark datasets.\n- This paper employes sliced Cramer distance with theoretical justification.\n- The analysis on EwC and MAS in terms of geometric view\n- Experimental results look promising.\n\n[Cons]  \n- Even if MAS[1] describes the details on synaptic concept and Hebbian rule, many readers might be not familiar with synaptic or neuro-science terms. So, in prelimiary session, more explanation can help readers to understand.\n- In addition to permute-MNIST, it is required to be evaluated on more conventional tasks such as MNIST->SVHN or CIFAR-10, 100 datasets. Also, more recent work should be compared such as IMM [2] and PGMA [3] for supervised learning scenarios.\n- All graph result figures can be improved for enhancing legibility. In Figure 6, In particular, the subtitle of Summer case confuses me.\n \n[1] Aljundi et al. Memory Aware Synapses: Learning what (not) to forget, ECCV 2018.\n[2] Lee et al. Overcoming Catastrophic Forgetting by Incremental Moment Matching, NIPS 2017.\n[3] Hu et al. Overcoming Catastrophic Forgetting for Continual Learning via Model Adaptation, ICLR 2019.", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 3}}}