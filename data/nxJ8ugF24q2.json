{"paper": {"title": "Learning Disconnected Manifolds: Avoiding The No Gan's Land by Latent Rejection", "authors": ["Thibaut Issenhuth", "Ugo Tanielian", "David Picard", "Jeremie Mary"], "authorids": ["~Thibaut_Issenhuth1", "~Ugo_Tanielian1", "~David_Picard1", "~Jeremie_Mary1"], "summary": "", "abstract": "Standard formulations of GANs, where a continuous function deforms a connected latent space, have been shown to be misspecified when fitting disconnected manifolds. In particular, when covering different classes of images, the generator will necessarily sample some low quality images in between the modes. Rather than modify the learning procedure, a line of works aims at improving the sampling quality from trained generators. Thus, it is now common to introduce a rejection step within the generation procedure.\nBuilding on this, we propose to train an additional network and transform the latent space via an adversarial learning of importance weights. This idea has several advantages: 1) it provides a way to inject disconnectedness on any GAN architecture, 2) the rejection avoids going through both the generator and the discriminator saving computation time, 3) this importance weights formulation provides a principled way to estimate the Wasserstein's distance to the true distribution, enabling its minimization. We demonstrate the effectiveness of our method on different datasets, both synthetic and high dimensional, and stress its superiority on highly disconnected data.", "keywords": []}, "meta": {"decision": "Reject", "comment": "The paper proposes to train a rejection sampler in the latent space of a GAN to learn disconnected data manifolds. Reviewers raised concerns about some theoretical aspects of the method as well as about the lack of larger scale datasets (ImageNet) in the experiments. Authors responded to these concerns but some of them still remain (including $\\hat{\\gamma}(z)$ not guaranteed to be a probability distribution and lack of more convincing experiments). I still think the work is promising, and encourage the authors to revise and resubmit the paper addressing these points highlighted by the reviewers. "}, "review": {"eRI0qPhGCLq": {"type": "rebuttal", "replyto": "LqLTyYUko1", "comment": "We share the answer to Reviewer 4's concerns. \n\nA neural network can be considered as a bounded Lipschitz function (e.g. a ReLU network is piece-wise linear). Thus, it comes from [1, Theorem 3.1] that the empirical measure $\\frac{1}{n} \\sum w_\\varphi(z)$ converges to $E_{z\\sim Z}[w_\\varphi(z)]$. In our setting the consistency is an immediate consequence of theorems on Donsker's classes for the Lipschitz cases.\n\n[1] van der Vaart, Aad W., and Jon A. Wellner Wellner. \"Empirical processes indexed by estimated functions.\" Asymptotics: particles, processes and inverse problems. Institute of Mathematical Statistics, 2007. 234-252.\n", "title": "Authors Response"}, "VuLbZqMAZjE": {"type": "rebuttal", "replyto": "9JjQ0zEmPn9", "comment": "1,2. A neural network can be considered as a bounded Lipschitz function (e.g. a ReLU network is piece-wise linear). Thus, it comes from [1, Theorem 3.1] that the empirical measure $\\frac{1}{n} \\sum w_\\varphi(z)$ converges to $E_{z\\sim Z}[w_\\varphi(z)]$. In our setting the consistency is an immediate consequence of theorems on Donsker's classes for the Lipschitz cases. \n\n[1] van der Vaart, Aad W., and Jon A. Wellner Wellner. \"Empirical processes indexed by estimated functions.\" Asymptotics: particles, processes and inverse problems. Institute of Mathematical Statistics, 2007. 234-252.\n\n3,4. We do not provide any new claim or comparison between these two algorithms, and were just presenting standard results for Monte-Carlo algorithms: Rejection Sampling ensures sampling from the target distribution, but the acceptance rate (1/M) can be very large in high-dimension; Metropolis-Hastings ensures asymptotic convergence. We refer to [2] for the proofs of these properties and more details. \n\n[2] Robert, Christian, and George Casella. Monte Carlo statistical methods. Springer Science & Business Media, 2013.", "title": "Authors response"}, "4owPgMppex": {"type": "rebuttal", "replyto": "YOLB7Z_zZMu", "comment": "2. There was a typo in our first answer. As written in the paper, the quantity that we track is $(\\{E}_{z\\sim Z} [w_\\varphi (z)] -1)^2$.  This quantity goes nearby 0 after a few training steps, ensuring that the penalty term forces $\\{E}_{z\\sim Z} [w_\\varphi (z)]  = 1$, so that $\\widehat{\\gamma}$ defines a normalized probability distribution. ", "title": "Authors response"}, "8aZR9RqrwSM": {"type": "rebuttal", "replyto": "TopfdzCOt8", "comment": "First, we would like to thank you for reviewing our paper and for your valuable comments. We will try to answer your two main concerns.\n\n 1) \" It is argued that the procedure will produce a new distribution $\\widehat{\\gamma}(z) \\propto \\gamma(z) w_{\\varphi}(z)$. There is no proof of this claim, and I suspect that this claim is not correct. \"\nThis is not a claim and is the definition of our method: it learns an importance weighting function $w_\\varphi$, that assigns an importance weight $w_\\varphi(z) \\in R$ to each point $z \\sim \\gamma(z)$ in the latent space. Given that the function is normalized, i.e. $\\int_z \\gamma(z) w(z) dz = E_\\gamma w_\\varphi=1$, then $\\widehat{\\gamma}(z) = \\gamma(z) w_\\varphi(z)$ is a well-defined probability distribution. \nIn practice, we add a strong regularization term forcing $\\widehat{\\gamma}$ to define a normalized distribution (self-normalization). We track the quantity $(\\{E}_{z\\sim Z} [w_\\varphi (z)] -1)^2$ during training, and it is indeed 0 after a few training steps, which gives us a guarantee that $\\widehat{\\gamma}$ defines a distribution.\n\n2) \"Optimizing equation 3 will not give the Wasserstein distance any more.\"\nActually, the method is indeed estimating and minimizing the Wasserstein distance. The Wasserstein distance consists in estimating two integrals. On the left side (target distribution), we do not bring any modification to the standard formulation. On the right side (generated distribution), the generated distribution is modified by the learnt importance weights, which is a classical way of performing importance sampling.\n\n3) \"Minor: Should there be a quanitifer in eqn (3) restricting  to 1-Lipschitz models?\"\nIn Related Work, equation 1, we mention that $D_\\alpha \\in A$ are functions restricted to the class of 1-Lipschitz functions.\n\nFurthermore, as asked by AnonReviewer1, we have updated the paper with according experiments on Swiss Roll and Mixture of 25 Gaussians (Figure 6 and Table 3 in Appendix). It supports our point: latentRS surpasses or matches the other methods on EMD, which is an empirical measure of the Wasserstein distance between generated and target distributions. Methods based on density-ratios do not always improve the goodness of fit measured in terms of EMD.", "title": "Authors response to AnonReviewer2"}, "6LRRozauhQz": {"type": "rebuttal", "replyto": "jafny_CBEOw", "comment": "First, we would like to thank you for reviewing our paper and for your valuable comments. We will try to answer your main concerns.\n\n1) \"It is not clear why the objective in equation (3) corresponds to the optimization of Wasserstein distance in the space of images w.r.t. the parameters alpha and phi. I mean that there are even no guarantees that $\\widehat{\\gamma}$ is a distribution.\"\nFirst, regarding $\\widehat{\\gamma}$, we add a strong regularization term forcing $\\widehat{\\gamma}$ to define a normalized distribution (self-normalization). We track the quantity $(\\{E}_{z\\sim Z} w_\\varphi (z) -1)^2$ during training, and it is indeed 0 after a few training steps, which gives us a guarantee that $\\widehat{\\gamma}$ defines a distribution.\nFrom here, it is clear that we are minimizing the Wasserstein distance. The Wasserstein distance consists in estimating the difference of two integrals. On the left side (target distribution), we do not bring any modification to the standard setting. On the right side (generated distribution), the generated distribution is modified by the learnt importance weights, which is a very classical way of performing importance sampling. \n\n2) \"Firstly, I do not understand why the rejection sampling is tractable. The regularization term does not provide any guarantees for the maximum value of the density ratio.\"\nYou are right, if the regularization term fails, the constant can be higher than what we wanted. However, we use strong penalty terms (high $\\lambda_2$) and did not empirically observe a constant higher than the maximum that we set. \n\n3) \"Secondly, even if the rejection sampling is tractable, I still find the MH algorithm more efficient: it does not require the evaluation of the constant.\"\nActually, as explained in the Related Work section, the two algorithms are difficult to compare. On one side, Rejection Sampling ensures to sample the target distribution, with no guarantee on the computational budget; on the other side, Metropolis-Hastings only ensures asymptotic convergence, but with an explicit control of the computational budget. Indeed, in practice, Metropolis-Hastings is used by selecting the last sample of a Markov Chain. \nMoreover, one could also use the MH algorithm with the learnt latent importance weights. We empirically tried it and we saw no improvement over latentRS. \n\n4) \"DRS does not assure sampling from the target distribution since it adjusts the constant and uses an approximation of density ratio. In contrast, the MH algorithm provides some guarantees by upper bounding the total variation distance between the stationary distribution and the target (see Neklyudov 2019).\"\nYes, you are right, there is a typo there. We were comparing Rejection Sampling vs Metropolis-Hastings in their original form and assumptions, not in the context of GANs. As mentionned above, Rejection Sampling ensures to sample the target distribution, with no guarantee on the computational budget; while Metropolis-Hastings ensures asymptotic convergence, but with an explicit control of the computational budget (length of the Markov Chain). \nMoreover, we would like to point out that the assumptions of rejection sampling/Metropolis-Hastings are not fullfiled in the context of GANs generating submanifolds of a high-dimensional space. Indeed, in this case, the support of the target distribution is not included in the support of the modelled distribution (see [1]). That is why DRS/MH-GANs have flaws.\n[1] Arjovsky, Martin, and Leon Bottou. \"Towards Principled Methods for Training Generative Adversarial Networks.\" ICLR 2017.\n\nFurthermore, as asked by AnonReviewer1, we have updated the paper with according experiments on Swiss Roll and Mixture of 25 Gaussians (Figure 6 and Table 3 in Appendix). It supports our point: latentRS surpasses or matches the other methods on EMD, which is an empirical measure of the Wasserstein distance between generated and target distributions. Methods based on density-ratios do not always improve the goodness of fit measured in terms of EMD.", "title": "Authors response to AnonReviewer4"}, "YOLB7Z_zZMu": {"type": "rebuttal", "replyto": "bk_wqO46OrD", "comment": "1. Yes, it multiplies by a factor which corresponds to the importance weight. Let us imagine that we want to estimate the quantity $\\int \\widehat{\\gamma}(z) f(z) dz$, but we only can sample from $\\gamma$. Then, we can rewrite the quantity of interest as follows: $\\int \\widehat{\\gamma}(z) f(z) dz = \\int \\gamma(z) \\frac{\\widehat{\\gamma}(z)}{\\gamma(z)} f(z) dz = \\int \\gamma(z) w_\\varphi(z) f(z) dz \\approx \\frac{1}{n} \\sum_{i=1,...,n} w_\\varphi(z_i) f(z_i)$; where each $z_i$ is drawn from $\\gamma$. It means that we sample from $\\gamma$ and we re-weight each sample by its importance weight. It corresponds to what we propose in equation (3). That is the standard use of importance sampling.\n\n2. Yes, you can check Figure 1.d for example. This is a visualization of $w_\\varphi(z)$ after training on a mixtures of 4 gaussians. The function is defined on the square [-1,1]x[-1,1]; its minimum is 0 and maximum is 2.", "title": "Authors response"}, "LlbcvcnF1Z": {"type": "rebuttal", "replyto": "axrkiy837Zz", "comment": "First, we would like to thank you for reviewing our paper and for your valuable comments. We will try to answer your main concerns.\n\n1) \"The paper misses some essential experiments to be faithfully compared with existing methods. It would be helpful to see the Swiss Roll experiment and the statistics on recovered modes and quality on 25 Gaussians for all the considered methods.\"\nThe main advantage of our method arises in high-dimensions since it relies on minimizing the Wasserstein distance, which is robust and well-behaved even when the supports of distributions do not intersect. On the other side, our concurrent methods rely on density-ratio estimation which have an inherent weakness in high-dimension. Indeed, in a  high-dimensional space $X$, the intersection of target and generated manifolds has a measure of 0 in $X$ (see [1]). Thus, density ratios are trivial: always 0 on the support of the generated distribution. This requires to strongly regularize the classifier that estimates the density ratios, but then the density ratios are not accurate anymore. \nFor clear comparisons with other works, we have updated the paper with according experiments on Swiss Roll and Mixture of 25 Gaussians (Figure 6 and Table 3 in Appendix). It supports our point: latentRS surpasses or matches the other methods on EMD, which is an empirical measure of the Wasserstein distance between generated and target distributions. Methods based on density-ratios do not always improve the goodness of fit measured in terms of EMD.\n[1] Arjovsky, Martin, and Leon Bottou. \"Towards Principled Methods for Training Generative Adversarial Networks.\" ICLR 2017.\n\n2) \"While the argument about one class CelebA has grounds, the DRS technique shows that it improves face generation by producing less warped nightmare-like faces.\"\nWe performed this experiment quite fairly and were not able to observe significant improvements over the GAN baseline. As mentionned in the Appendix, for DRS/SIR/MH-GAN, we selected the best from two different discriminators: 1) the 1-Lipschitz discriminator from the adversarial training, which is then fine-tuned with binary cross-entropy; 2) a non-regularized discriminator trained from scratch with binary cross-entropy. We selected the best from several checkpoints and reported the results. There is no significant improvement in terms of Precision, Recall, EMD or Avg. Hausd. \nHowever, we see that our method LatentRS slightly improves EMD and Avg. Hausd.\n\n3) \"The authors state that they use image embeddings from corresponding classification networks for each of their datasets, but how do they obtain image embeddings for CelebA?\"\nFor CelebA, we use the standard Frechet Inception Distance with Inception Network embeddings, and VGG16 pre-trained from PyTorch for the other metrics: Precision, Recall, Avg. Hausdorff and EMD. ", "title": "Authors response to AnonReviewer1"}, "jafny_CBEOw": {"type": "review", "replyto": "nxJ8ugF24q2", "review": "The paper proposes a method for an improvement of generative adversarial models via post-processing its latent variable distribution. To be more precise, the method proposes to train an additional neural network that outputs an important weight for each point of the latent space, thus reweighting the final distribution in the space of images. For the optimization of this network, the authors use the dual form of the Wasserstein distance, where they multiply the initial latent density by the output of the network. To fix the ill-behaved objective, the authors add two regularization terms to it. The proposed objective is then validated on 3 MNIST-like datasets quantitatively and on CelebA qualitatively.\n\nReview:\nMy major concern is the limited theoretical novelty together with modest empirical study. Let me clarify. I think the idea to put the filtering stage into the latent space is indeed worthy. However, the straightforward amortization of the discriminator network via a fully connected network is challenging due to the described computational problems and usually high dimensionality of the latent space. Furthermore, the verification of the method on MNIST-like data does not seem convincing, especially when the relevant works provide a comparison on ImageNet (Azadi 2018, Neklyudov 2019).\n\nAdditional comments:\n1. perhaps, I'm missing something, but for me, it is not clear why the objective in equation (3) corresponds to the optimization of Wasserstein distance in the space of images w.r.t. the parameters alpha and phi. I mean that there are even no guarantees that \\widehat{\\gamma} is a distribution.\n2. \"since the rejection sampling scheme is now tractable, we do not need to implement the MH algorithm or the importance sampling\". Firstly, I do not understand why the rejection sampling is tractable. The regularization term does not provide any guarantees for the maximum value of the density ratio. Secondly, even if the rejection sampling is tractable, I still find the MH algorithm more efficient: it does not require the evaluation of the constant; given the same proposal, MH's acceptance rate is greater or equal to the acceptance rate of the rejection sampling.\n3. the authors claim that reweighting in the latent space allows for better support coverage than the methods operating on the image space. Although I believe that such an effect occurs, I wouldn't expect the quality of images to be high. Indeed, this additional coverage could be produced by sampling from the low-density regions of the latent distribution. It is clear that such regions are underrepresented during the training. Moreover, there is empirical evidence of the deteriorating quality of images for latent distributions with higher variance (see Brock 2018).\n4. the bottom of page 3. DRS does not assure sampling from the target distribution since it adjusts the constant and uses an approximation of density ratio. In contrast, the MH algorithm provides some guarantees by upper bounding the total variation distance between the stationary distribution and the target (see Neklyudov 2019).\n\nminor comments:\n1. abstract. I would suggest finding a better analog for the phrase \"inject disconnectedness\". It does not sound like a desirable feature of your model when we speak about GANs, especially at the beginning of the paper, where few context is given. I would propose something like \"postselection\" or \"filtering\".\n2. eq. 4, the signs of regularization terms are incorrect\n3. typo on page 5, item 2). every methods -> every method\n\nReferences:\n1. (Azadi 2018) Azadi, Samaneh, Catherine Olsson, Trevor Darrell, Ian Goodfellow, and Augustus Odena. \"Discriminator rejection sampling.\" arXiv preprint arXiv:1810.06758 (2018).\n2. (Brock 2018) Brock, Andrew, Jeff Donahue, and Karen Simonyan. \"Large scale gan training for high fidelity natural image synthesis.\" arXiv preprint arXiv:1809.11096 (2018).\n3. (Neklyudov 2019) Neklyudov, Kirill, Evgenii Egorov, and Dmitry P. Vetrov. \"The Implicit Metropolis-Hastings Algorithm.\" In Advances in Neural Information Processing Systems, pp. 13954-13964. 2019.", "title": "a straightforward amortization of the discriminator outputs with a limited empirical study", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "axrkiy837Zz": {"type": "review", "replyto": "nxJ8ugF24q2", "review": "This work aims at improving the sample quality of generative models through better sampling, which is a relevant problem and has brought about a line of work [1,2,3,4,5], to name a few. By leveraging the idea of importance sampling, the authors train an additional network. The latter uses the information contained in the learned discriminator to assign importance weights to the latent points, thus defining a new distribution in the latent space. Subsequently, rejection sampling on the newly defined latent distribution is applied to obtain inputs for a generator network. By treating the problem in the latent space, the paper introduces latentRS method that compares favourably to several existing methods in terms of computational complexity for generating a sample. The authors propose one more method, latentGA, following the path in the latent space that maximizes the learned importance weights. The paper also discusses the limitations of the previously proposed methods and presents their empirical comparison on several datasets and metrics.\n\nThe method is concise and straightforward to be applicable by a broad community of ML practitioners. One of the proposed methods, latentRS, offers a significant speedup at the inference stage compared to analyzed methods while being similar in performance metrics. The paper also raises an interesting question of whether the existing enhanced sampling methods help when the target distribution is not sufficiently disconnected.\n\nHowever, there are several weaknesses in the experiments which lead to questioning the claims. While the paper's claim of careful comparison with the existing methods and the discussion of existing methods' limitations is indeed well-presented, the paper neglects the already recurring standard experiments for such methods or brushes them under the Appendix section. The generated samples from the Appendix figure for the mixture of Gaussians with n>9 show that the results are not as promising as the same experiments in the literature (the 'fake' clouds are not as nicely located on top of the true ones). n=25 is a recurring setting and seems to be a standard check for algorithms that refine GAN's sampling (e.g. DRS, DOT, DDLS). Table 2 in the Appendix lacks computer metrics for existing methods since it would demonstrate the tangible interpretable difference in this setting between comparable methods.\n\nThe paper misses some essential experiments to be faithfully compared with existing methods. It would be helpful to see the Swiss Roll experiment and the statistics on recovered modes and quality on 25 Gaussians for all the considered methods. As for more realistic image spaces, the CIFAR10 is a dataset that represents an undoubtfully disconnected manifold, and it has more potential to show the advantage of the proposed methods.\n\nReturning to the presented empirical study, these too raise a number of questions. The IPR results in Table 1 (and Table 3 in the Appendix) do not show consistent advantages for the proposed methods over the existing ones. It either favours latentRS or latentGA in terms of precision or recall alone, not both at the same time. It's understandable that when maximizing importance weights with latentGA we get higher precision; we force the generated samples to stay within true points at the cost of their diversity (which can also be seen in the synthetic experiment with Gaussians), so I guess the method is highly reliant on the hyperparameter m, which controls the 'conservativeness' of the trained importance weights network. It would be helpful to see an ablation study for the hyperparameters.\n\nGiven all the above, I am leaning towards a reject and my main concerns are as follows. The experiment with 25 Gaussians doesn't show as much improvement in sampling as existing methods implying there might be little effect in real-world datasets. I believe that the proposed methods have not been faithfully compared to the existing methods. There is no ablation study on the hyperparameters of the proposed methods.\n\nWhile the argument about one class CelebA has grounds, the DRS technique shows that it improves face generation by producing less warped nightmare-like faces. Thus, better GAN sampling techniques should ideally not only help avoid empty regions in the latent space between the nodes (inject disconnectedness) but also grasp the shape of those modes. In this regard, using an energy-based model for the latent variable might be an apt direction [5].\n\nThe authors state that they use image embeddings from corresponding classification networks for each of their datasets, but how do they obtain image embeddings for CelebA?\n\nSome minor points \u2014 the notation for the proposal and true distribution we want to sample from in section 3 is a bit confusing as the hat is usually used to denote an approximation. Also, the submission has quite a few typos \u2014 it needs proofreading.\n\nReferences:\n[1] Azadi, Samaneh, et al. \"Discriminator rejection sampling.\" arXiv preprint arXiv:1810.06758 (2018).\n\n[2] Turner, Ryan, et al. \"Metropolis-hastings generative adversarial networks.\" International Conference on Machine Learning. 2019.\n\n[3] Neklyudov, Kirill, Evgenii Egorov, and Dmitry P. Vetrov. \"The Implicit Metropolis-Hastings Algorithm.\" Advances in Neural Information Processing Systems. 2019.\n\n[4] Tanaka, Akinori. \"Discriminator optimal transport.\" Advances in Neural Information Processing Systems. 2019.\n\n[5] Che, Tong, et al. \"Your GAN is Secretly an Energy-based Model and You Should use Discriminator Driven Latent Sampling.\" arXiv preprint arXiv:2003.06060 (2020).", "title": "Treating the problem in latent space is promising, but experiments are not convincing", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "TopfdzCOt8": {"type": "review", "replyto": "nxJ8ugF24q2", "review": "## Summary:\nThe paper proposes a new algorithm for improved sampling of GANs. Since GANs are continuous functions that act on a connected latent space, they will have trouble learning distributions whose support is disconnected (for e.g., clustered data). The proposed method tries to fix this issue and is motivated by rejected sampling. However, instead of using density based algorithms for rejecting samples, the authors take a fixed pre-trained generative model and train a neural network that learns to reject samples from the latent space.\n\n## Significance:\nThe problem is well motivated and seems significant. Learning distributions with disjoint support can be difficult using the traditional GAN training, and this paper addresses this problem by learning which areas in the latent space must be avoided. \n\n## Quality:\nWhile the problem is significant, I find that the proposed method has some weaknesses in its formulation and empirical evaluation. Please see the section \"Cons\" below.\n\n## Originality:\nThe proposed method seems sufficiently novel, but I am not familiar enough with this area to know if something closely related has been done before.\n\n## Pros:\n1. On synthetic data, the proposed method can capture modes better than the considered baselines.\n1. The proposed technique produces better quality samples on GANs trained on CelebA.\n\n## Cons:\n1. While some of the experiments are convincing, I do not buy some of the arguments made in the paper. Specifically, under eqn (3), it is argued that if the GAN $G$ is kept fixed and the following adversarial training is performed for a classifier $w_\\phi$ and discriminator $D_\\alpha$, then the following procedure:\n$$ \\sup_{\\alpha \\in A} \\inf_{\\phi \\in \\Phi} E_{x \\sim \\mu} D_\\alpha(x) - E_{z \\sim Z} [ w_\\phi(z) \\cdot D_\\alpha( G(z) )]$$\nwill produce a new distribution on $z$ which is $\\widehat{\\gamma}(z) \\propto \\gamma(z) w_\\phi(z)$.\nThere is no proof of this claim, and I further suspect that this claim in not correct.\n\n1. There exist several baseline methods that consider the problem of mode collapse. Examples like PAC-GAN [Lin et al 2017] have shown to be effective, and are also provably good. Other examples include those considered in table 5 of https://arxiv.org/pdf/2010.00654v1.pdf\nFor the Stacked-MNIST dataset, the algorithms listed in table 5 seem to have much better mode coverage than the algorithms in Table 1 of this submission.  While the paper I have linked is very recent, the baseline algorithms considered in the paper are published works from 2017 onwards.\nComparing to these algorithms would make this paper much stronger.\n\n1. There are a lot of unsubstantiated claims. Some include:\n    1. Modifying the loss in equation 3 gives a new distribution $\\widehat{\\gamma}$ defined underneath equation 3.\n    1. At the bottom of page 5, the authors remark \"In our method, on the contrary, we are still looking for the discriminator maximizing the Integral Probability Metric (M\u00fcller, 1997) in equation 3, linked to optimal transport.\" .\nIn equation 3, the authors take the dual optimization problem for estimating the Wasserstein distance, and modify it. With this modification, optimizing equation 3 will not give the Wasserstein distance any more.\n\n## Minor:\n1. Should there be a quanitifer in eqn (3) restricting $D_\\alpha$ to $1-$Lipschitz models?", "title": "This work proposes a new rejection sampling technique for improving the quality of images generated from GANs", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}