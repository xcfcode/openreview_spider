{"paper": {"title": "Exploring Sparsity in Recurrent Neural Networks", "authors": ["Sharan Narang", "Greg Diamos", "Shubho Sengupta", "Erich Elsen"], "authorids": ["sharan@baidu.com", "gdiamos@baidu.com", "ssengupta@baidu.com", "eriche@google.com"], "summary": "Reduce parameter count in recurrent neural networks to create smaller models for faster deployment", "abstract": "Recurrent neural networks (RNN) are widely used to solve a variety of problems and as the quantity of data and the amount of available compute have increased, so have model sizes. The number of parameters in recent state-of-the-art networks makes them hard to deploy, especially on mobile phones and embedded devices. The challenge is due to both the size of the model and the time it takes to evaluate it. In order to deploy these RNNs efficiently, we propose a technique to reduce the parameters of a network by pruning weights during the initial training of the network. At the end of training, the parameters of the network are sparse while accuracy is still close to the original dense neural network. The network size is reduced by 8\u00d7 and the time required to train the model remains constant. Additionally, we can prune a larger dense network to achieve better than baseline performance while still reducing the total number of parameters significantly. Pruning RNNs reduces the size of the model and can also help achieve significant inference time speed-up using sparse GEMMs. Benchmarks show that using our technique model size can be reduced by 90% and speed-up is around 2\u00d7 to 7\u00d7.", "keywords": ["Speech", "Deep learning", "Supervised Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "Here is a summary of the reviews:\n \n Strengths\n Experiments are done on state-of-the-art networks, on a real speech recognition problem (R3, R1)\n Networks themselves are of a very large size (R3)\n Computational gains are substantial (R3, R4)\n Paper is clear (R1)\n \n Weaknesses\n Experiments are all done on a private dataset (R3)\n No comparison to other pruning approaches (e.g. Han et al.) (R3); AC notes that reviewers added new results which compare to an existing pruning method\n No comparison to distillation techniques (R1)\n Paper doesn't present much novelty in terms of ideas (R3)\n \n The AC encouraged feedback from the reviewers following author rebuttal and paper improvements. Reviewers stated that the improvements made to the paper made it publishable but was still closer to the threshold. R1 who had originally rated the paper 3: a \"clear reject\" updated the score to 6 (just above acceptance).\n \n Considering the reviews and discussions, the AC thinks that this paper is a poster accept. There are no serious flaws, the improvements made to the paper during the discussion paper have satisfied the reviewers, and this is an important topic with practical benefits; evaluated on a real large-scale problem."}, "review": {"rkfbrPTIg": {"type": "rebuttal", "replyto": "BylSPv9gx", "comment": "Based on the feedback from one of the reviewers, we have trained a larger sparse GRU model. This model is 2.2% worse than the GRU Dense baseline while 3.5 times faster than the dense GRU model. This larger GRU network recoups most of the loss in performance due to pruning. We believe that we can match the Dense GRU baseline performance by slightly reducing the sparsity of the network or increasing the number of hidden units. We thank the reviewer for this helpful suggestion. I have uploaded a new revision of this paper with this result. ", "title": "Revision with larger GRU model"}, "BkQDSgw8l": {"type": "rebuttal", "replyto": "BylSPv9gx", "comment": "I have uploaded a new revision which includes results using the pruning method proposed in \"Exploiting sparseness in deep neural networks for large vocabulary speech recognition\" by Yu et. al. The results show that gradual pruning used in our approach performs better (in terms of accuracy) than hard pruning. We thank the reviewers for the suggestion to compare our method with this pruning approach. ", "title": "New Revision"}, "H1fFUxUNe": {"type": "rebuttal", "replyto": "S1dYodzNe", "comment": "Thanks for your comment. I saw those papers at NIPS. We are interested in exploring these methods for RNNs. We haven't begun working on it yet but hope to do so soon. I will post an update when we have the results. ", "title": "structurally sparse RNNs"}, "ryqw9ib4l": {"type": "rebuttal", "replyto": "Hyp1XSxNl", "comment": "We thank the reviewer for the comments and feedback. We are working on adding comparisons with other pruning as pointed out by the reviewers. We will update the paper once those become available. We will also evaluate our models on a publicly available data set. Thank you again for the review. ", "title": "response"}, "B10UlqeVl": {"type": "rebuttal", "replyto": "r1mguuk4g", "comment": "We thank the reviewer for the comments and the questions. Here are the answers to the questions:\n\n1. q is not the final sparsity of the model. As mentioned in the paper, q corresponds to the 90th percentile of the absolute value of the weights of a layer (from a previously trained model). Using this value and equation (1), we can pick reasonable hyper parameters for the pruning schedule. Using these parameters, the training achieves a sparsity of ~90% for a given layer. The hyper params can then be fine tuned to increase or decrease the sparsity. We also intend to perform some more experiments measuring the impact of varying sparsity on accuracy of the model. \n\n2. We will run the experiments for GRU sparse big and update the paper with the results. \n\n3. The text is misleading. It should say \u201coverall sparsity of 88%\u201d instead of \u201caverage sparsity of 88%\u201d. I have fixed this. The overall sparsity of the network is lower than the sparsity of the individual layers due of two reasons. Firstly, we don\u2019t prune the convolution layers and final CTC cost layer. Secondly, we  don\u2019t prune the batch norm and bias parameters. Therefore, the sparsity of the individual layer is higher than the overall sparsity of the network. In order to evaluate speed-ups, it makes sense to consider the individual layer sparsity. Hope this helps. \n", "title": "response "}, "Hy_DQFkNe": {"type": "rebuttal", "replyto": "ByT_p1R7e", "comment": "Thank you for the review and the helpful comments. I have addressed all\nthe language issues that you pointed out. Here are our responses to the\ncons:\n\n1. We weren\u2019t aware of this previous work\n(http://ieeexplore.ieee.org/document/6288897/). Thanks for pointing this\nout. I have added a citation to this work and updated the paper. We will\nalso run this baseline and update the paper as soon as the results are\navailable.\n\n2. Thank you for the suggestion for using logits. It is a good one for\nimproving the argument that sparse modeling is a better approach to\nreducing the number of model parameters.  An even more fair comparison\nmight be to train a dense model at the largest size possible and use that\nto generate the targets for both the sparse and smaller dense networks so\nthat both the sparse and dense networks are trained with the same loss. We\nwill add this baseline in a future version of the paper.\n\n3. The speed-up achieved with\na sparse GRU model is compared to the dense GRU model. We think it is\ncorrect to compare the speed-ups w.r.t the same model.\nSecondly, even though the RNN Sparse medium has better performance,\nbidirectional models are difficult to deploy since transcribing the first\npart of the utterance requires the entire utterance. This increases the\nlatency for transcription. Therefore, forward only GRU models can be used\nin inference even though they might perform slightly worse than the\nbidirectional RNN models.\n", "title": "response"}, "B1kZLvhXg": {"type": "rebuttal", "replyto": "Hk__CCpGg", "comment": "Thanks for the comment. I've fixed the paper to reflect that it's the updates not the gradients that must exceed the threshold for the weight to be involved in the forward pass again. \n\nWe haven't measured how many times this happens but, as Erich mentioned, we expect it to happen rarely. I'll measure this behavior and provide the results in the paper soon. ", "title": "respose"}, "ryjxl3f7e": {"type": "rebuttal", "replyto": "Hko-VGjGl", "comment": "We haven't attempted this approach on LSTMs. However, based on the GRU results, we believe that this approach should work for LSTMs as well. ", "title": "LSTM models"}, "Hk__CCpGg": {"type": "rebuttal", "replyto": "rywfrCafe", "comment": "Thank you for the clarification, that sentence is imprecise - it is indeed the update not the gradient that must exceed the threshold.  This happens rarely, but can happen if you start pruning very early on and often.  I do not have any hard numbers - and I no longer have access to the code or results, so my colleagues will need to weigh in.\n\nIt is also possible to apply the mask only when pruning and not on every iteration - this gives weights a larger chance of exceeding the next pruning threshold.  I'll see if we can get these results into the paper.", "title": "response"}, "rywfrCafe": {"type": "review", "replyto": "BylSPv9gx", "review": "When weights are added again, which value do they initially have? In Section 3, the text describes that weights will be used again during the forward pass if their gradient is larger than the threshold for that layer. Assuming that the updated weights would be much smaller than the gradient because of the learning rate scaling and the threshold growing over time, they are quite likely to be removed again at the next iteration. This step is not precisely defined and I'm curious how common it is for weights to reappear after being pruned.The paper proposes a method for pruning weights in neural networks during training to obtain sparse solutions. The approach is applied to an RNN-based system which is trained and evaluated on a speech recognition dataset. The results indicate that large savings in test-time computations can be obtained without affecting the task performance too much. In some cases the method can actually improve the evaluation performance.\n\nThe experiments are done using a state-of-the-art RNN system and the methodology of those experiments seems sound. I like that the effect of the pruning is investigated for networks of very large sizes. The computational gains are clearly substantial. It is a bit unfortunate that all experiments are done using a private dataset. Even with private training data, it would have been nice to see an evaluation on a known test set like the HUB5 for conversational speech. It would also have been nice to see a comparison with some other pruning approaches given the similarity of the proposed method to the work by Han et al. [2] to verify the relative merit of the proposed pruning scheme. While single-stage training looks more elegant at first sight, it may not save much time if more experiments are needed to find good hyperparameter settings for the threshold adaptation scheme. Finally, the dense baseline would have been more convincing if it involved some model compression tricks like training on the soft targets provided by a bigger network.\n\nOverall, the paper is easy to read. The table and figure captions could be a bit more detailed but they are still clear enough. The discussion of potential future speed-ups of sparse recurrent neural networks and memory savings is interesting but not specific to the proposed pruning algorithm. The paper doesn\u2019t motivate the details of the method very well. It\u2019s not clear to me why the threshold has to ramp up after a certain period time for example. If this is based on preliminary findings, the paper should mention that.\n\nSparse neural networks have been the subject of research for a long time and this includes recurrent neural networks (e.g., sparse recurrent weight matrices were standard for echo-state networks [1]). The proposed method is also very similar to the work by Han et al. [2], where a threshold is used to prune weights after training, followed by a retraining phase of the remaining weights. While I think that it is certainly more elegant to replace this three stage procedure with a single training phase, the proposed scheme still contains multiple regimes that resemble such a process by first training without pruning followed by pruning at two different rates and finally training without further pruning again. The main novelty of the work would be the application of such a scheme to RNNs, which are typically more tricky to train than feedforward nets.\n\nImproving scalability is an important driving force of the progress in neural network research. While I don\u2019t think the paper presents much novelty in ideas or scientific insight, it does show that weight pruning can be successfully applied to large practical RNN systems without sacrificing much in performance. The fact that this is possible with such a simple heuristic is a result worth sharing.\n\n\nPros:\nThe proposed method is successful at reducing the number of parameters in RNNs substantially without sacrificing too much in performance.\nThe experiments are done using a state-of-the-art system for a practical application.\n\nCons:\nThe proposed method is very similar to earlier work and barely novel.\nThere is no comparison with other pruning methods.\nThe data is private and this prevents others from replicating the results.\n\n\n[1] Jaeger, H. (2001). The \u201cecho state\u201d approach to analyzing and training recurrent neural networks-with an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34.\n\n\n[2] Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems, 2015.", "title": "When weights are added again, which value do they initially have?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hyp1XSxNl": {"type": "review", "replyto": "BylSPv9gx", "review": "When weights are added again, which value do they initially have? In Section 3, the text describes that weights will be used again during the forward pass if their gradient is larger than the threshold for that layer. Assuming that the updated weights would be much smaller than the gradient because of the learning rate scaling and the threshold growing over time, they are quite likely to be removed again at the next iteration. This step is not precisely defined and I'm curious how common it is for weights to reappear after being pruned.The paper proposes a method for pruning weights in neural networks during training to obtain sparse solutions. The approach is applied to an RNN-based system which is trained and evaluated on a speech recognition dataset. The results indicate that large savings in test-time computations can be obtained without affecting the task performance too much. In some cases the method can actually improve the evaluation performance.\n\nThe experiments are done using a state-of-the-art RNN system and the methodology of those experiments seems sound. I like that the effect of the pruning is investigated for networks of very large sizes. The computational gains are clearly substantial. It is a bit unfortunate that all experiments are done using a private dataset. Even with private training data, it would have been nice to see an evaluation on a known test set like the HUB5 for conversational speech. It would also have been nice to see a comparison with some other pruning approaches given the similarity of the proposed method to the work by Han et al. [2] to verify the relative merit of the proposed pruning scheme. While single-stage training looks more elegant at first sight, it may not save much time if more experiments are needed to find good hyperparameter settings for the threshold adaptation scheme. Finally, the dense baseline would have been more convincing if it involved some model compression tricks like training on the soft targets provided by a bigger network.\n\nOverall, the paper is easy to read. The table and figure captions could be a bit more detailed but they are still clear enough. The discussion of potential future speed-ups of sparse recurrent neural networks and memory savings is interesting but not specific to the proposed pruning algorithm. The paper doesn\u2019t motivate the details of the method very well. It\u2019s not clear to me why the threshold has to ramp up after a certain period time for example. If this is based on preliminary findings, the paper should mention that.\n\nSparse neural networks have been the subject of research for a long time and this includes recurrent neural networks (e.g., sparse recurrent weight matrices were standard for echo-state networks [1]). The proposed method is also very similar to the work by Han et al. [2], where a threshold is used to prune weights after training, followed by a retraining phase of the remaining weights. While I think that it is certainly more elegant to replace this three stage procedure with a single training phase, the proposed scheme still contains multiple regimes that resemble such a process by first training without pruning followed by pruning at two different rates and finally training without further pruning again. The main novelty of the work would be the application of such a scheme to RNNs, which are typically more tricky to train than feedforward nets.\n\nImproving scalability is an important driving force of the progress in neural network research. While I don\u2019t think the paper presents much novelty in ideas or scientific insight, it does show that weight pruning can be successfully applied to large practical RNN systems without sacrificing much in performance. The fact that this is possible with such a simple heuristic is a result worth sharing.\n\n\nPros:\nThe proposed method is successful at reducing the number of parameters in RNNs substantially without sacrificing too much in performance.\nThe experiments are done using a state-of-the-art system for a practical application.\n\nCons:\nThe proposed method is very similar to earlier work and barely novel.\nThere is no comparison with other pruning methods.\nThe data is private and this prevents others from replicating the results.\n\n\n[1] Jaeger, H. (2001). The \u201cecho state\u201d approach to analyzing and training recurrent neural networks-with an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148, 34.\n\n\n[2] Han, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for efficient neural networks. In Advances in Neural Information Processing Systems, 2015.", "title": "When weights are added again, which value do they initially have?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Byslo4jMe": {"type": "rebuttal", "replyto": "ByasrziMl", "comment": "In the cited paper they use a pruning technique similar to the one in https://arxiv.org/abs/1506.02626 where the pruning is done with multiple passes, rather than in one pass.  The multiple passes are what leads to the increase in training time, not the Huffman coding.", "title": "they use multiple passes"}, "Hko-VGjGl": {"type": "review", "replyto": "BylSPv9gx", "review": "Have you tried prunning on LSTM models? Summary: The paper presents a technique to convert a dense to sparse network for RNNs. The algorithm will increasingly set more weights to zero during the RNN training phase. This provides a RNN model with less storage requirement and higher inference rate. \n\nPros:\nProposes a pruning method that doesn\u2019t need re-training and doesn\u2019t affect the training phase of RNN. The method achieves 90% sparsity, and hence less number of parameters.\n\nCons & Questions:\nJudiciously choosing hyper parameters for different models and different applications wouldn\u2019t be cumbersome? In equation 1, is q the sparsity of final model? Is there a formula to know what is sparsity, number of parameters and accuracy of final model given a set of hyper parameters, before going through training? (Questions answered)\n\nIn table3, we see a trade-off between number of units and sparsity to achieve better number of parameters or accuracy, or in table5 better speed. Good, but where are the results for GRU sparse big? I mean, accuracy must be similar and still get decent compression rate and speed up. Just like RNN Sparse medium compared with RNN Dense. I can\u2019t see much advantage of pruning and getting high speed-up if you are sacrificing so much accuracy. (Issue fixed with updated data)\n\nWhy sparsity for table3 and table5 are different? In text: \u201caverage sparsity of 88%\u201d but in table5 is 95%? Are the models used in table3 different from table5? (Issue fixed)\n", "title": "Have tried on LSTM?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "r1mguuk4g": {"type": "review", "replyto": "BylSPv9gx", "review": "Have you tried prunning on LSTM models? Summary: The paper presents a technique to convert a dense to sparse network for RNNs. The algorithm will increasingly set more weights to zero during the RNN training phase. This provides a RNN model with less storage requirement and higher inference rate. \n\nPros:\nProposes a pruning method that doesn\u2019t need re-training and doesn\u2019t affect the training phase of RNN. The method achieves 90% sparsity, and hence less number of parameters.\n\nCons & Questions:\nJudiciously choosing hyper parameters for different models and different applications wouldn\u2019t be cumbersome? In equation 1, is q the sparsity of final model? Is there a formula to know what is sparsity, number of parameters and accuracy of final model given a set of hyper parameters, before going through training? (Questions answered)\n\nIn table3, we see a trade-off between number of units and sparsity to achieve better number of parameters or accuracy, or in table5 better speed. Good, but where are the results for GRU sparse big? I mean, accuracy must be similar and still get decent compression rate and speed up. Just like RNN Sparse medium compared with RNN Dense. I can\u2019t see much advantage of pruning and getting high speed-up if you are sacrificing so much accuracy. (Issue fixed with updated data)\n\nWhy sparsity for table3 and table5 are different? In text: \u201caverage sparsity of 88%\u201d but in table5 is 95%? Are the models used in table3 different from table5? (Issue fixed)\n", "title": "Have tried on LSTM?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Sk6HGWWWx": {"type": "rebuttal", "replyto": "SJ7AkwAgx", "comment": "Thanks for pointing out the issue with the font. I've uploaded a new copy which fixes this problem. ", "title": "Fixed font issue"}, "SJ7AkwAgx": {"type": "rebuttal", "replyto": "BylSPv9gx", "comment": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!", "title": "ICLR Paper Format"}}}