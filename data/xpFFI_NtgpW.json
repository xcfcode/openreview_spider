{"paper": {"title": "Rethinking Embedding Coupling in Pre-trained Language Models", "authors": ["Hyung Won Chung", "Thibault Fevry", "Henry Tsai", "Melvin Johnson", "Sebastian Ruder"], "authorids": ["~Hyung_Won_Chung1", "~Thibault_Fevry1", "henrytsai@google.com", "~Melvin_Johnson1", "~Sebastian_Ruder2"], "summary": "Decoupling output embedding shapes leads to more transferable Transformer layers and prevents over-specialization of a Transformer's upper layers to the pre-training task.", "abstract": "We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art pre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to significantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By reallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on standard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that allocating additional capacity to the output embedding provides benefits to the model that persist through the fine-tuning stage even though the output embedding is discarded after pre-training. Our analysis shows that larger output embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the number of parameters at the fine-tuning stage. ", "keywords": ["natural language processing", "transfer learning", "efficiency", "pre-training"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper makes a thorough investigation on the idea of decoupling the input and output word embeddings for pre-trained language models.  The research shows that the decoupling can improve the performance of pre-trained LMs by reallocating the input word embedding parameters to the Transformer layers, while further improvements can be obtained by increasing the output embedding size.  Experiments were conducted on the XTREME benchmark over a strong mBERT.  R1&R2&R3 gave rather positive comments while R4 raised concerns on the model size.  The authors gave detailed response on these concerns but R4 still thought the paper is overclaimed because the experiments were only conducted in a multilingual scenario.  "}, "review": {"4V290Jqach0": {"type": "rebuttal", "replyto": "xpFFI_NtgpW", "comment": "We thank the reviewers for their time and thoughtful feedback. All reviewers noted that our empirical results were strong and well supported by analysis. Reviewers further highlighted that the paper is well written and strongly motivated (R1, R3), that our strategy is novel (R3) and simple yet effective (R4), and that the findings will be \u201cuseful in practice to the general NLP community\u201d (R2) and \u201cpromote future model design and the understanding of the transformer\u201d (R3).\n\nWe respond to individual reviews in a reply to the corresponding review. We have uploaded a new version fixing typos and adding information reviewers asked for (see the individual replies for more information).", "title": "We updated the paper and answered each reviewer individually."}, "pgXnmzWQ7uz": {"type": "rebuttal", "replyto": "9JJBO0Lb9Jj", "comment": "> But again my concern is the same, the number of parameters in fine-tuning is larger than XLM-R plus the additional pre-training time (more than XLM-R). \n\nIn the paper, we mentioned that RemBERT was trained with 3.5x fewer tokens during pre-training compared to XLM-R. However, RemBERT has more parameters during pre-training. We believe that these factors lead the reviewer to think that RemBERT requires additional pre-training time. \n\nIn order to make a fair comparison, we profiled the XLM-R and RemBERT using exactly the same hardware setting. We ran XLM-R for 10000 steps just to be able to profile reliably and did not complete the training. In addition, for XLM-R, we used the exact training setting from the paper (Conneau et al. 2020). XLM-R was trained with 4 times larger batch size (8192 vs. 2048), which significantly increases the training time. Therefore, even though the RemBERT model had more pre-training parameters, each gradient update step was faster; it took 0.348 sec. for RemBERT whereas XLM-R takes 0.6208 sec. The total estimated pre-training time is listed in the table below. XLM-R pre-training takes 52% more time than RemBERT.\n\n|| Pre-training time|\n|:---  |:---:|\n|XLM-R|259 hr|\n|RemBERT|170 hr|\n\nOverall, these profiling results suggest that RemBERT is faster to train and was able to outperform XLM-R with much less resources during pre-training.\n\n\n> Yes, the performance increases but the main objective of the paper is not quite satisfied (reducing the number of parameters in FT which are actually larger in RemBERT than XML-R). Maybe you can alter the abstract accordingly and focus on reinvestment, which actually is helping in making your point in the experiments.\n\nWe would like to clarify that the main objective of the paper is not to reduce the number of FT parameters. Our main objective was to study the practice of embedding coupling and make parameter allocation in pre-trained language models more efficient overall and representations more generalizable. If there are specific parts of the paper that suggest that our objective is to reduce the FT parameters, please let us know. We will modify the text to reduce the source of confusion.\n\n\nThe remaining part of the response addresses stylistic feedback and minor comments.\n\n> Check the use of \\citep{} and \\citet{}.\n\nWe fixed incorrect uses of \\citep in the main text and the Appendix.\n\n> Table 7 as main results was never cited in this paper (but only in appendix).\n\nWe have fixed the link to Table 7.\n\n> Page 3-> \"It consists of 12 Transformer layers with a hidden size H of 768 and 12\" the sentence seems incomplete.\n\nThanks for catching this. We fixed this in the revised text.\n\n> Page 5 -> \"increasing the number of Transformer layers L results in the best performance\" I am wondering if reinvestment in L gains higher performance than in H because of the larger number of parameters (10M more).\n\nIn our experiments, 10M more parameters (or 5.6% more) do not result in a performance difference of 1.7, which is the difference in the average score between the \u201dReinvested in $H$\u201d and \u201creinvested in $L$\u201d models in Table 6. We tried removing 2 layers of the \u201cReinvested in $L$\u201d model so that the total number of parameters is about 164M, which is 4M less than the \u201cReinvested in $H$\u201d. The performance was almost identical to the original model without the 2 layers removed.\n\n> Section 4: How about a proportional increase in E_{in} and E_{out} affects the performance (e.g increasing or decreasing both with the same proportion, especially decreasing which lines up with paper goal i.e providing flexibility with less number of parameters.)\n\nAgain, we want to argue that our goal is not reducing the number of parameters.\n\n> Page 7 -> \"For both E_{out} = 128 and E_{out} = 768, removing the last layer improves performance\". Seems in disagree with Figure 1. Acc with 10 layers is lower than that with 12 layers for E_{out}=128. Similar is the case with E_{out}=768.\n\nWe would like to clarify that \u201cremoving the last layer\u201d refers to the data point with 11 layers, not 10 (as the full model has 12 layers). In Figure 1, we indeed see a slight bump in performance with 11 layers for $E_{\\rm{out}} =128$ and $E_{\\rm{out}} = 768$. \n\n> How about the extra pre-training and fine-tuning time compared with the baseline?\n\nPlease refer to the table we included in the response to R1 where we discuss the training/inference time of our strategies.\n\n> For the final rebalanced mBERT described in page 5, how are the hyper-parameters decided?\n\nWe built the RemBERT model combining various improvement strategies. For example, we added 8 extra layers because \u201cReinvested in $L$\u201d performed the best. However, adding too many layers may result in instability typical of deep neural networks. Therefore, we also used the \u201cReinvested in $H$\u201d recipe. All the dimensions are multiple of 128 to make sure that our model efficiently harnesses accelerators. The vocabulary size was chosen to be the same as XLM-R.\n", "title": "continued response"}, "9JJBO0Lb9Jj": {"type": "rebuttal", "replyto": "LlWEQtK73-r", "comment": "> \u201cIt seems that the number of the parameters is much more than those public ones. I am curious if it is a fair comparison in Table 7.\u201d \n\nWe reemphasize that we constructed RemBERT to have a number of fine-tuning parameters that matches XLM-R\u2019s as closely as possible while making the most effective use of our techniques and the accelerators that were available to us. Since we used Google Cloud TPUs, tensor dimensions that are multiples of 128 are more efficient. The resulting model has 575M vs XLM-R\u2019s 559M parameters, so only around 16M (or 2.9%) more parameters. We respectfully disagree with the characterization that 2.9% more parameters are \"much more\". We think the parameter sizes are similar enough for such large models to ensure a fair comparison. In order to back up our claim, we ran the following experiment. We removed the last 2 layers (each layer has 16M parameters) of RemBERT so that the total number of parameters is now 16M fewer than that of XLM-R. The XNLI accuracy is only reduced from 80.8 to 80.6. Considering that RemBERT has a 4% higher score averaged over 7 tasks, a drop of 0.2% is small. This demonstrates that the effect of a 32M parameters for these large models is not significant and doesn\u2019t change our conclusion that RemBERT outperforms XLM-R.\n\nAdditionally, as mentioned in the paper, RemBERT is trained with 3.5X fewer tokens and covers 10 more languages than XLM-R. Therefore, per-language capacity of RemBERT (5.23M params/lang) is lower for RemBERT than that of XLM-R (5.59M params/lang).\n\nFinally, we found that the pre-training time for XLM-R is 52% more time than that of RemBERT. For details on how we got these numbers, please refer to our second response.\n\nWith these factors in mind, we believe that our comparison turns out to be unfair for RemBERT and our claim that RemBERT outperforms XLM-R is a very conservative one.\n\n> \u201cThe current approach does help in reducing the number of parameters in the fine-tuning stage by increasing E_{out}. \u2026 merely a series of experiments with different values of E_{in}, E_{out}, and # of layers in the baseline.\u201d\n\nWe believe that this paragraph is based on a comparison between models with different sizes, which we believe is not fair. We try our best to vary only one variable in a table such that the effect of that variable can be studied in isolation. Since models in Table 4 have 100M parameters during fine-tuning, comparing them to the baseline with 177M parameters (43% more) mixes the effects of the number of parameters and larger embedding size. As such, we can\u2019t draw conclusions from such a comparison.\n\nThe purpose of Table 4 is to show that given the same number of fine-tuning parameters, increasing the output embedding size improves the performance and not that increasing the output embedding at much smaller model size can outperform the baseline.\n\nTherefore, we respectfully argue that the statement that \u201cThe performance with much larger E_{out} is a marginal improvement over baseline unless the saved parameters are reinvested\u201d misses the main purpose of Table 4. We will make it clear in Table 4 that they should not be compared to the baseline.\n\n> \u201cseems like a trade-off between fine-tuning and pre-training\u201d\n\nWe would like to highlight that our paper\u2014to our knowledge\u2014is the first one that shows that there exists such a trade-off between the number of pre-training and fine-tuning parameters. This is indeed a trade-off between exchanging slightly increased pre-training computation for improved downstream performance.\n\nTypically a model is used for inference many times whereas it is trained very few times as the cost of training is not justified otherwise. Furthermore, well-trained models are often publicly released so that they can be used by many users for fine-tuning and inference. In such scenarios, the cost of training is amortized over a number of people. Therefore, we believe that the cost of additional compute that can be shared between multiple inference runs and people is justified. But little or no previous work has been done on studying such trade-offs between pre-training and fine-tuning. We believe that our paper fills the gap by exploring various ways of adding parameters only during pre-training.\n\n> \u201cTable 7 [...] is your major contribution\u201d\n\nWe respectfully disagree. While Table 7 demonstrates that our methods work at scale and can be used to outperform the state of the art with similar model size, our main contributions are the effects of embedding decoupling, the importance of the output embedding size, and analyses with regard to their impact on model behavior.\n\n> \u201cthe optimal value of E_{out} appears to be 768 (similar in the baseline)\u201d\n\nWe think there might be some misunderstanding here. In Table 4, we show that $E_{out} = 3072$ performs even better than $E_{out} = 768$.\n", "title": "We included additional data to back up our claim that the comparison between RemBERT and XLM-R is fair. We also clarified the main objective of the paper."}, "VjmZsGDjJ1F": {"type": "rebuttal", "replyto": "959iTeqc8ov", "comment": "> \u201cReallocating the parameters of input embedding to additional transformer layers might affect the pre-training and inference speed, it will be helpful to show the training and inference speed of different parameter reallocation strategies.\u201d\n\nThanks for the suggestion. We ran profiling for our reallocation strategies as well as larger output embedding experiments.\n\n|                                \t        | Pre-training speed  [steps/s] \t|   # PT params   \t|    # FT params \t|\n|:--------------------------------\t| :--------------------:\t| :-----------------:\t|    :--------:|\n| $E_{\\textrm{out}} = 128$       \t|                         13.2 \t|        115M           \t|        100M \t|\n| $E_{\\textrm{out}} = 768$       \t|                         11.3 \t|        192M     \t|        100M \t|\n| $E_{\\textrm{out}} = 3072$       |                          7.6 \t|        469M            |        100M \t|\n| 11 add. layers (Table 5) \t         |                          7.2 \t|        193M \t        |        100M \t|\n\nWe show above the pre-training speed for the models in Tables 4 and 5. The fine-tuning speed should be the same as they share the same 12 layer architecture with the same hidden size. When the output embedding size is increased from 128 to 3072 (factor of 24 increase), the pre-training speed is reduced only by 42%. This is because the additional compute associated with larger output embedding is highly optimized via dense matrix multiplication. \n\n\n\n|                   \t| Pre-training speed [steps/s] \t| Inference speed [s] \t| # PT params \t| # FT params \t|\n|-------------------\t|-----------------------------:\t|:-------------------:\t|:-----------:\t|:-----------:\t|\n| Baseline          \t|                         10.6 \t|               21.7 \t|        177M \t|        177M \t|\n| Reinvested in $H$ \t|                         7.8 \t|                20.3 \t|        260M \t|        168M \t|\n| Reinvested in $L$ \t|                         6.7 \t|                24.7 \t|        270M \t|        178M \t|\n\nThis table shows the pre-training and inference speed for the reinvesting strategies in Table 6. During pre-training both reinvested models are slower than the baseline. This is expected because of the additional pre-training capacity in the output embedding. More parameters translate to a more expensive all-reduce operation. Comparing between \u201cReinvested in $H$\u201d and \u201cReinvested in $L$\u201d, the latter is slightly slower during pre-training because the forward and backward pass through the deeper model can\u2019t be easily parallelized whereas the additional compute associated with the larger hidden size consists of highly optimized dense matrix multiplication.\n\nTo measure the inference speed, we ran the inference (i.e., forward pass only) on the XNLI test set which has 112,350 examples with batch size of 256. Since XNLI has only 3 classes, the task specific computation is negligible. The measured time does not include setting up the input pipeline. We ran with 8 Google Cloud TPUs. \n\nWe observe that the \u201cReinvested in $H$\u201d is the fastest. The reduced input embedding makes the embedding lookup faster while the larger hidden size is efficiently computed with highly optimized matrix multiplication. The \u201cReinvested in $L$\u201d is 12.3% slower than the baseline, mainly due to the additional layers.\n\nSince the performance of \u201cReinvested in $L$\u201d was better than \u201cReinvested in H\u201d and the baseline, there exists a tradeoff between inference speed and the fine-tuning performance as well. This tradeoff motivated our design of the full-scale RemBERT model, which reinvests in both $H$ and $L$ in an attempt to strike the right balance.\n\nWe note that the inference speed depends on the hardware and software. For example, some hardware can be optimized for the embedding lookup operation, in which case the baseline model may perform better than the \u201cReinvested in $H$\u201d model.\n\n\n> Why are the pre-training and fine-tuning parameters of coupled and decoupled models the same in Table 2? Shouldn\u2019t decoupling the input and output embeddings double the parameters of the embeddings?\n\nYes you are correct. This is a typo. We fixed the number of parameters in Table 2. Thanks for catching this.\n", "title": "We added pre-training and inference speed information."}, "QrtlLdOK8j": {"type": "rebuttal", "replyto": "0r5Css9Y2qM", "comment": "> \u201cSome model details are missing. Although I know what the input and output embedding are, it is still a bit hard to follow in Section 4 and 5.\u201d\n\nWe added further details on the Transformer architecture and the decoupling information in the text and table captions not just in Sections 4 and 5 but throughout the paper. Please let us know if there are other model details that you believe are missing.\n\n> \u201cLots of your model designs are empirical such as the embedding size, and it is a bit boring to optimize those hyperparameters, and sometimes we even do not know why it works.\u201d\n\nWe agree with the reviewer\u2019s notion that improving models by tweaking hyperparameters such as embedding size can be effective but less interesting from a research point of view. However, given that one of the most important contributions of our paper happens to be increasing the output embedding size, we put significant emphasis (dedicate the entire section 6, which is also the longest section) on understanding why increasing the output embedding improves the performance. We found that larger output embedding size during pre-training improves the transferability to fine-tuning tasks and to other languages. In fact, R2 mentions this as one of the strong points (item 3) and R4 mentions that \u201cThe insights of model design for more efficient fine-tuning are well supported by the analysis.\u201d For these reasons, we hope that the reviewer can appreciate our efforts in better understanding the profound impact decoupling and the choice of the output embedding size may have on pre-trained language models.\n\nOverall, our analysis illustrates that there are no universally optimal hyper-parameters. Instead, we highlight trade-offs that have so far been unexplored between additional compute only during pre-training (e.g., larger output embedding) and improved fine-tuning performance.\n\nGiven the overall positive nature of the review and the only minor negative points raised, we would like to encourage the reviewer to potentially reconsider their score.\n", "title": "Added more details about the model architecture throughout the paper."}, "qgMqDIpzqsF": {"type": "rebuttal", "replyto": "HN8h7VWbbR7", "comment": "> \u201cYet, authors draw some conclusions on Page 4 based on this table, without reporting any variance or statistical significance tests.\u201d\n\nThe average variance for the scores of the $E_{\\textrm{in}} = 768, \\ E_{\\textrm{out}} = 128$ and the $E_{\\textrm{in}} = 128, \\ E_{\\textrm{out}} = 768$ model in Table 3 is 0.8 and 0.05 respectively across three runs. Note that our main point in this paragraph does not require the improvement of the smaller model to be statistically significant. Instead, our main finding here is the fact that the smaller model is **not significantly worse** than the larger one. This is surprising in our opinion, given that the smaller model has 77M (or 43%) fewer parameters during fine-tuning. We will clarify this in the revised version, highlighting that not the magnitude of the improvement of the smaller model is important but that it is competitive with the larger model with a much smaller number of parameters.\n\nFor the probing experiments in Table 9, we found that the standard deviation between experiments to be very small, agreeing with the findings of Tenney et al. (2019) who reported about 0.2 (or variance of 0.04) for most tasks.\n\n> \"Shouldn\u2019t #PT params in table 2 for the \u201cDecoupled\u201d method be more than #PT params for the \u201cCoupled\" method ?\"\n\nWe have fixed the typo that misspecified the number of pre-training parameters in Table 2. Thank you for catching the typo.\n\n> \"More figures like Figure 1, on other tasks in addition to XNLI, would be really helpful in making the observations more conclusive.\"\n\nPer your suggestion, we will run additional experiments to generate more figures like Figure 1. We will update the manuscript with the additional figures.\n\nIf the reviewer feels like we adequately answered their concern, given that this is the only negative point noted by them, we\u2019d like to encourage them to potentially reconsider their score. Otherwise we\u2019d be happy to continue the conversation.\n", "title": "We adjusted our claim regarding Table 3."}, "LlWEQtK73-r": {"type": "review", "replyto": "xpFFI_NtgpW", "review": "This work studied the impact of embedding coupling in pre-trained language models, by taking a multilingual model as backbone. The major finding is that decoupling the input and output embedding shapes can bring benefits, and the output embedding plays an important role in the transferability of pre-trained representation. A rebalanced mBERT is designed by combing and scaling up the investigated techniques, achieving strong results on the XTREME benchmark.\n\nThis paper is well written, and the proposed strategy is simple yet effective for obtaining more transferable language representations. The insights of model design for more efficient fine-tuning are well supported by the analysis.\n\nMy concern is about the model efficiency and the true source of performance improvement. It seems that the number of the parameters is much more than those public ones. I am curious if it is a fair comparison in Table 7. \n\nThe current approach does help in reducing the number of parameters in the fine-tuning stage by increasing E_{out}. Even in your all experiments, the optimal value of E_{out} appears to be 768 (similar in the baseline), where performance is quite similar to baseline. However, increasing E_{out} to a much larger value (i.e 3072) drastically increases the number of parameters in pre-training, even as compared to baseline, which seems like a trade-off between fine-tuning and pre-training. The performance with much larger E_{out} is a marginal improvement over baseline unless the saved parameters are reinvested. The reinvestment of parameters creates a larger improvement over baseline (Table 6). The authors must conclude with optimal values of E_{in} and E_{out}, otherwise the paper is merely a series of experiments with different values of E_{in}, E_{out}, and # of layers in the baseline.\n\nTry referencing Table 7 in the main explanation of RemBERT rather than in the Appendix, as it is your major contribution. But again my concern is the same, the number of parameters in fine-tuning is larger than XLM-R plus the additional pre-training time (more than XLM-R). Yes, the performance increases but the main objective of the paper is not quite satisfied (reducing the number of parameters in FT which are actually larger in RemBERT than XML-R). Maybe you can alter the abstract accordingly and focus on reinvestment, which actually is helping in making your point in the experiments.\n\n\n\nMinor comments:\n\nCheck the use of \\citep{} and \\citet{}, e.g., in the last sentence of Section 2 and Footnote 10.\n\nTable 7 as main results was never cited in this paper (but only in appendix), which is not a proper organization way.\n\nPage 3-> \"It consists of 12 Transformer layers with a hidden size H of 768 and 12\" the sentence seems incomplete.\n\nPage 5 -> \"increasing the number of Transformer layers L results in the best performance\" I am wondering if reinvestment in L gains higher performance than in H because of the larger number of parameters (10M more). Maybe you can keep reinvestment the same in both H and L for fair comparison and better results projection.\n\nSection 4: How about a proportional increase in E_{in} and E_{out} affects the performance (e.g increasing or decreasing both with the same proportion, especially decreasing which lines up with paper goal i.e providing flexibility with less number of parameters.)\n\nPage 7 -> \"For both E_{out} = 128 and E_{out} = 768, removing the last layer improves performance\". Seems in disagree with Figure 1. Acc with 10 layers is lower than that with 12 layers for E_{out}=128. Similar is the case with E_{out}=768.\n\nHow about the extra pre-training and fine-tuning time compared with the baseline? \n\nFor the final rebalanced mBERT described in page 5, how are the hyper-parameters decided? \n\n", "title": "much larger model size?", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "959iTeqc8ov": {"type": "review", "replyto": "xpFFI_NtgpW", "review": "Summary:\n \nThis work investigated the strategy of reallocating parameters of multilingual language models for improving their cross lingual transferability. Authors first decoupled the input and output embeddings and showed that the capacity of output embedding is more important than input embedding. Then,  they proposed a Rebalanced mBERT (RemBERT) model that reallocates the input embedding parameters of mBERT model to the output embedding and additional layers. Experimental results on XTREME benchmark showed that RemBERT significantly outperformed XLM-R with similar model size.\n\nPros:\n \n- The paper is well written and easy to follow. The comparison of embeddings parameters ratios of different language models in Table 1 gives a very good motivation of reallocating the parameters of embeddings.\n \n- Authors conducted several ablation studies to understand how the capacity of  different parts of the model (e.g., input and output embeddings, transformer layers) contributes to the final performance. \n\nCons:\n\n- Reallocating the parameters of input embedding to additional transformer layers might affect the pre-training and inference speed, it will be helpful to show the training and inference speed of different parameter reallocation strategies.\n \nQuestions:\n\n- Why are the pre-training and fine-tuning parameters of coupled and decoupled models the same in Table 2?  Shouldn\u2019t decoupling the input and output embeddings double the parameters of the embeddings?\n", "title": "Good Analysis", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "0r5Css9Y2qM": {"type": "review", "replyto": "xpFFI_NtgpW", "review": "This paper systematically studies the impact of embedding coupling with multilingual language models.  The authors observe that while na\u00a8\u0131vely decoupling the input and output embedding parameters does not consistently improve downstream evaluation metrics, decoupling their shapes comes with a host of benefits. Moreover, they achieve significantly improved performance by reinvesting saved parameters to the width and depth of the Transformer layers on the XTREME benchmark.\n\nThis paper is well-written and strongly motivated. The idea of decoupling embedding is novel, and the evaluation results are strong.  \n\nStrength:\n\n+ The systematical study of the impact of embedding coupling on state-of-the-art pre-trained language models.  This paper also thoroughly investigates reasons for the benefits of embedding decoupling and observes that an increased output embedding size enables a model to improve on the pre-training task, which correlates with downstream performance. They also find that it leads to Transformers that are more transferable across tasks and languages. Those empirical results will promote future model design and the understanding of the transformer for textual representation learning.\n+ The paper proposes a method to reinvest saved parameters to the width and depth of the Transformer layers and achieve significantly improved performance on the XTREME benchmark over a strong mBERT.\n\nWeakness:\n\n- Some model details are missing. Although I know what the input and output embedding are, it is still a bit hard to follow in Section 4 and 5. I strongly recommend that the authors revise those parts and introduce the model details, such as decoupling and model architecture in your experiments.\n- Lots of your model designs are empirical such as the embedding size, and it is a bit boring to optimize those hyperparameters, and sometimes we even do not know why it works.\n\nQuestions:\n\n- Could you please introduce the model details of your decoupled model in Section 4?", "title": "A well-written paper and an exciting idea", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HN8h7VWbbR7": {"type": "review", "replyto": "xpFFI_NtgpW", "review": "Transformer based bidirectional LMs pre-trained using Masked Language Model loss typically share input and output token embeddings. This paper makes an interesting investigation about decoupling input and output embeddings and gains which can be obtained out of this decoupling. In particular, this paper shows that the pre-training performance of transformers and the transferability of the learned representations can be improved by increasing the dimension of output embeddings while reducing the dimension of input embeddings. Better performance while pre-training and improved transferability further helps the performance while finetuning on downstream tasks. Parameters saved while reducing the dimensions of input embeddings can be re-invested into increasing the depth or width of the transformer layers. I believe that the findings in this paper are going to be useful in practice to the general NLP community working on Transformers and Multilingual problems.\n\nWeak Points:\n1. In table 3, (E_in=768,E_out=128) is just 0.1 point worse on average than (E_in=128, E_out=768).  Yet, authors draw some conclusions on Page 4 based on this table, without reporting any variance or statistical significance tests.\n    E.g. \n    >  the model pretrained with a larger output embedding size slightly outperforms the comparison method on average despite \n    having 77M fewer parameters during fine-tuning\n\n    > Reducing the input embedding dimension saves a significant number of parameters at a noticeably smaller cost to accuracy \n    than reducing the output embedding size.\n\nI would request the authors to report the variance in Table 3 and also in Table 9. Otherwise, it\u2019s hard to rely on conclusions drawn from such minor differences in performance.\n\n\nStrong Points:\nStrong empirical results.\n1. Table 4 clearly shows that increasing the dimension of output embeddings improves transfer to downstream tasks while having the same number of trainable parameters during the finetuning stage.  \n2. Table 6 shows that reinvesting the parameters saved from reducing the dimension of input embeddings into additional transformer layers yield improved performance. Further, RemBERT performs at par with XLM-R while having a comparable number of trainable parameters during the finetuning stage.\n3. Sufficient amount of analysis in Section 6 to establish the usefulness of having higher dimensional output embeddings for improved pre-training and better transferability of the learned representations across tasks. \n\nOther comments/questions:\n1. Shouldn\u2019t #PT params in table 2 for the \u201cDecoupled\u201d method be more than #PT params for the \u201cCoupled\" method ?\n2. More figures like Figure 1, on other tasks in addition to XNLI, would be really helpful in making the observations more conclusive.\n\n\n\n", "title": "Transformer based bidirectional LMs pre-trained using Masked Language Model loss typically share input and output token embeddings. This paper makes an interesting investigation about decoupling input and output embeddings and gains which can be obtained out of this decoupling.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}