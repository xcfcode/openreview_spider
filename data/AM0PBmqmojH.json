{"paper": {"title": "Warpspeed Computation of Optimal Transport, Graph Distances, and Embedding Alignment", "authors": ["Johannes Klicpera", "Marten Lienen", "Stephan G\u00fcnnemann"], "authorids": ["~Johannes_Klicpera1", "marten.lienen@in.tum.de", "~Stephan_G\u00fcnnemann1"], "summary": "We propose the locally corrected Nystr\u00f6m (LCN) method for kernels, develop two fast approximations of entropy-regularized optimal transport (sparse Sinkhorn and LCN-Sinkhorn) and evaluate them for embedding alignment and graph distance regression.", "abstract": "Optimal transport (OT) is a cornerstone of many machine learning tasks. The current best practice for computing OT is via entropy regularization and Sinkhorn iterations. This algorithm runs in quadratic time and requires calculating the full pairwise cost matrix, which is prohibitively expensive for large sets of objects. To alleviate this limitation we propose to instead use a sparse approximation of the cost matrix based on locality sensitive hashing (LSH). Moreover, we fuse this sparse approximation with the Nystr\u00f6m method, resulting in the locally corrected Nystr\u00f6m method (LCN). These approximations enable general log-linear time algorithms for entropy-regularized OT that perform well even in complex, high-dimensional spaces. We thoroughly demonstrate these advantages via a theoretical analysis and by evaluating multiple approximations both directly and as a component of two real-world models. Using approximate Sinkhorn for unsupervised word embedding alignment enables us to train the model full-batch in a fraction of the time while improving upon the original on average by 3.1 percentage points without any model changes. For graph distance regression we propose the graph transport network (GTN), which combines graph neural networks (GNNs) with enhanced Sinkhorn and outcompetes previous models by 48%. LCN-Sinkhorn enables GTN to achieve this while still scaling log-linearly in the number of nodes.", "keywords": ["Optimal transport", "sinkhorn distance", "locality sensitive hashing", "nystr\u00f6m method", "graph neural networks", "embedding alignment"]}, "meta": {"decision": "Reject", "comment": "The authors propose to approximate the kernel matrix used in the Sinkhorn algorithm by a combination of sparse + low rank approximation. To do so, the authors propose to compute a low rank approximation of a sparsified (thresholded below a certain value to be 0) kernel matrix using Nystr\u00f6m, and then correct it by adding back the true entries at non-sparse entries, after removing those obtained from the approximation. This results in a matrix whose application then results in sparse + low-rank.\n\nThe first version of the paper contained mostly experimental evidence, which was deemed a bit short by some reviewers.The authors have added theoretical material on the way. Although I believe these are worthy additions, as AC, I do not feel comfortable accepting the paper as of now, because I believe these additions were not properly reviewed. I understand this must be disappointing for the authors, who have sprinted to add new content during the rebuttal phase, but I hope they agree that the rebuttal process is not here to handle entirely new sections, but rather to improve existing parts. In particular, that section should be reviewed by authors knowledgeable on low rank kernel factorization, something I did not see in the pool of reviewers. I also believe the paper still has a few shortcomings. Taken together, I therefore recommend a re-submission.\n\nideas to improve the paper\n\n- the authors claim to use Nystr\u00f6m on a sparsified matrix (see eq. 4). The sparsified kernel is no longer positive definite. I would like the authors to comment on this. I understand Nystr\u00f6m could be used naively without any psd-ness guarantees, but I think a heads-up is needed.There are, furthermore, several local/global factorizations of kernel matrices available out there (e.g. MEKA, https://www.jmlr.org/papers/v18/15-025.html), the main difference here being that the product by such approximation must be guaranteed to be positive for it to work in the Sinkhorn algorithm. I would expect that bounds in expectation to break down sometimes, and therefore result in \"catastrophic\" failures (i.e. nan's). I think that an algorithm that claims to improve or replace another one, and which has such blind spots, needs such additional experiments (I have read the Limitations section in Appendix B, something more precise would improve the paper). I understand these were not part of the original Nystr\u00f6m paper for Sinkhorn, but since this is an increment over that previous work, therefore lacking a bit its originality, more knowledge needs to be contributed.\n\n- For instance, since the authors write an entire paragraph on this (Appendix B), I am surprised that there is not direct mention to the fact that a sparse sinkhorn may simply *not* converge, because it may not satisfy the fully indecomposable property required of matrices for Sinkhorn's algorithm to converge. \n\n- i dont think that users have the various identities (14,15) in mind when they think about \"backpropagating\" through Sinkhorn. What is typically needed is to compute the differentiable properties of the regularized OT matric and/or of the regularized OT cost w.r.t. *point locations* (i.e. x_i). The statement \"LCN-Sinkhorn works flawlessly with automatic backpropagation\" is misleading in the sense that it ignores that problem altogether. Since so many extensions of OT today relay on that differentiability, the section, as it is written now, is problematic.\n\n- several methods claim to be faster of more efficient than Sinkhorn to solve OT. Either these methods display faster theoretical convergence (e.g. by using acceleration) or display faster practical convergence (e.g. heavy ball variants) using synthetic, controlled datasets. Using synthetic data helps exhibit highlight relevant regimes for regularization parameters, including those where LSE Sinkhorn may converge but LCN does not work, or vice-versa. I understand that the authors' wanted to use real data, but it would be great to clarify whether that setup was used because LCN works better there (in which case this becomes more of a paper at the intersection of OT and word embeddings) or because this happened to be the first and only example the authors thought of."}, "review": {"U-YKhZtosDC": {"type": "rebuttal", "replyto": "AM0PBmqmojH", "comment": "Dear readers,\n\nWe would like to highlight that the gradients proposed in Eq. 15 and the second part of Eq. 13 are incorrect. For guidance on how to derive the analytical gradient dP/dC please instead refer to the proofs in https://arxiv.org/pdf/1805.11897.pdf.\n\nNote that the experimental results presented in this paper are not affected by this.", "title": "Incorrect gradients dP/dC"}, "HitMn8_rtmQ": {"type": "rebuttal", "replyto": "nyPTEhZyWns", "comment": "Indeed, you are right again. The current derivation only considers the inner optimization of P, not the optimization of the Lagrange multipliers. Interestingly, empirically this is not a horrible approximation. Nevertheless, it is wrong.\n\nThe approach of deriving the gradients in the provided reference is indeed similar to ours, and can be adapted to derive the gradients we are interested in. However, the result involves computing the inverse of an NxN Hessian matrix. This would be prohibitively expensive for the large datasets we are considering. Moreover, it would not be improved by LCN-Sinkhorn. We will thus remove this part of Prop. 1 (Eq. 15 and the second part of Eq. 13) in all future versions. Unfortunately, this means that datasets with large point clouds will need to rely on regular backpropagated gradients for the transport plan P.\n\nNote that none of our experiments use gradients of P. Therefore no other part of our paper is affected by this.\n\nWe would like to again thank you for your continued support and diligence. We sincerely apologize for this oversight. We will post a warning in this forum to prevent people from using the current version of the paper.", "title": "Apologies, Prop. 1 shortened"}, "Olkt04M7HU5": {"type": "rebuttal", "replyto": "p6wVnzzwHIz", "comment": "We've compiled a small PDF with the updated relevant proof here: https://web.tresorit.com/l/CjDkh#IDDU8AvMEQWG3pc_6nXWfg\n\n[Edit]: Maybe it furthermore helps to remind oneself of the Wolfe dual problem when reasoning about \u03b1 and P. This formulation also uses the fact that the derivative of the Lagrangian w.r.t. the primal variable (in our case P) has to be 0 at the optimum.", "title": "Updated proof"}, "2PtC6akxycu": {"type": "rebuttal", "replyto": "BWJSFFWeEW9", "comment": "Dear AC,\n\nWe would like to thank you for revisiting and again becoming involved with our work. We appreciate the time you have invested and your attention to detail. Our work has substantially improved after every one of your comments.\n\nIndeed, you are partially right. Note that Prop. 1 concerns regular Sinkhorn and LCN-Sinkhorn. We thus implicitly assume that C has total support to guarantee existence of the minimizer. We should have stated this explicitly. Furthermore, the current proof indeed contains a mistake involving Eq. 46. \\bar{P} lies on the boundary of the marginal constraints. The gradient thus does not have to be zero, as wrongly stated after Eq. 46. Fortunately, there is a way around this and Prop. 1 is still correct.\n\nThe Sinkhorn distance is a convex optimization problem, a solution exists (assuming total support), and the positivity and marginal constraints are linear. Therefore, strong duality holds by Slater's condition. Furthermore, due to convexity the minimizer \\bar{P} of the dual problem is the same as the minimizer of the primal problem. Denoting the Lagrange multipliers by \u03b1_1 to \u03b1_5 we can thus identify x=(C, \u03b1), y=P, and f(C, \u03b1, P) as the derivative of the dual w.r.t. P, i.e. f(C, \u03b1, P) = C + \u03bb logP + \u03bb + \u03b1_1 \\vec{1}^T - \u03b1_2 \\vec{1}^T + \\vec{1} \u03b1_3^T - \\vec{1} \u03b1_4^T - \u03b1_5; note that \u03b1_1 to \u03b1_4 are vectors, while \u03b1_5 is a matrix. At the optimum \\bar{P} we have f(C, \u03b1, P) = 0 and thus g(C, \u03b1) = \\bar{P}. We then obtain the same diagonal Jacobian as in Eq. 47 and the same derivative as in Eq. 48, independently of the Lagrange multipliers. The remaining proof remains the same.", "title": "Solved via duality, Prop. 1 is still correct"}, "RVCkyv-KnXn": {"type": "rebuttal", "replyto": "7ADOwXXGc2s", "comment": "We would like to thank the PC for the thoughtful and detailed assessment, as well as the invaluable suggestions. We look forward to revising this work for an even better and more complete contribution.\n\nTo avoid future confusion we furthermore want to briefly clarify some details mentioned in the suggested improvements.\n- We do not calculate the Nystr\u00f6m matrix of the sparsified kernel but sparsify the Nystr\u00f6m matrix itself. Therefore Nystr\u00f6m's PSD requirements are not affected.\n- The point locations only affect the cost matrix C and we derive gradients for exactly this matrix. So the problem you describe is precisely the one we consider.", "title": "Clarifications"}, "7E8KZoQ0t-C": {"type": "review", "replyto": "AM0PBmqmojH", "review": "Overall, I found this paper interesting, and I think it does address a relevant problem for the community.\n\n\nI have been playing with Nystrom approximations myself and I know the results are a bit disappointing, but it is grounded on strong theory. Then, it is pretty much welcome that attempts to patch the problems of Nystroms are made, so Optimal Transport becomes more scalable.\n\nThe reason for why my judgement is below acceptance is that I believe both theory and results altogether are not strong enough so they live up to what is promised in the abstract. Typically when new methods are proposed with the promise of bridging a gap and solving a relevant problem, I hope they will have a thorough theoretical justification, or the results will be compelling. None of this convincing enough here.\n\n1)On the theoretical side, I missed a convergence analysis, as the one in the Nystrom method. The theory side focuses on some derivative calculations, but I would love to see how the interplay between sparsity, Nystrom method and LSH lead to better convergence. I acknowledge this can be hard to do, though. Without having the theory it is hard to understand whether any reported experimental result is a consequence of choosing particularly good example for their method.\n\n2)I found the experimental/methodological side was a bit disconnected from the rest; the paper contains  several vignettes about some applications/improvements in the practical side, but when reading that felt like belonging to other paper. I recommend authors work in creating a more coherent story\n2a) I found the discussion on Multi-Head OT unjustified and even a bit misleading. The Authors refer to some NLP papers, like arguing OT plays a role there. But none of these papers have any OT at all. The analogy of  \"softmax for rows\" is not convincing as this is simply a softmax applied many times. There is a world of difference between that and the result of sinkhorn algorithm, but the narrative seems to downplay the actual difference between them. I recommend the authors elaborate more on this connection, because otherwise it is hard to follow (and the subsequent results).\n2b)Results on translation seem impressive (Table 2), but raise a concern. Why would your method outperform Sinkhorn if it is only approximation? is it perhaps the result of randomness? since an explanation of this phenomenon is missing I am led to believing. Authors should improve the exposition of the baseline \"original\". Why does full Sinkhorn does better than original?. In summary, I think authors should improve the discussion about the validity/significance of their empirical results, highlighting the regimes when they are supposed to express and when they are not.\n2c)The main figure is Fig 2. I recommend authors build on this and expand those results so it is clear when their method is better and when it is not", "title": "Interesting paper and potentially significant.But theory is lacking, and experimental section can be improved", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "tznp4Sy9iN1": {"type": "rebuttal", "replyto": "dbtvdqdH5cT", "comment": "Seeing that the end of the discussion period is drawing near, we would again like to highlight that we have addressed all the issues you have raised in your review, some of which were based on an unfortunate misinterpretation.\n\nWe sincerely hope that you can find the time to revisit the significantly clearer and improved new version of our paper. These improvements are to a large part due to your helpful comments and suggestions.", "title": "Gentle Reminder to Revisit the Improved Paper"}, "srDf56ikag": {"type": "rebuttal", "replyto": "7E8KZoQ0t-C", "comment": "Seeing that the end of the discussion period is drawing near, we would like to again highlight the significant improvements your comments prompted us to make to the paper.\n\nConsidering the significant effort we have put into deriving the **theoretical convergence analysis** you were missing in the original version, we sincerely hope that you can find the time to revisit the substantially improved revised version. Other reviewers have already positively highlighted the added theoretical foundation provided by this analysis.\n", "title": "Gentle Reminder to Revisit the Improved Paper"}, "XGprfy8b9aw": {"type": "review", "replyto": "AM0PBmqmojH", "review": "This paper studies how to approximate Sinkhorn computation using more efficient kernel matrix representations (low-rank approach + LSH based sparse approach). Neither of both ideas is completely new, the authors studies a combination of them that hasn't been explored in the literature, and use the proposed tech in three applications: ranking, embedding alignment, graph distance regression. \n\nPro:\n\n- Authors have attempted multiple applications to prove the effectiveness with quantitative metrics. The main contribution seems the empirical validations. \n\nCon:\n\n- I think the presentation is a bit out of focus. Some sections can be left for appendix (e.g., Backpropagation in Sec 4. and Sec 5.) since some are either very standard in the literature or not really solidly experimented. Since I consider the main contribution to be the empirical evidences, the space should left for more details on those empirical experiments (for reproducibility purpose).\n\n- the paper's idea is not particularly innovative. (yet it is not the sole reason for my scoring). \n\n- Some relevant works on low-rank ideas have not been compared/cited. e.g.  \n\nForrow, Aden, et al. \"Statistical optimal transport via factored couplings.\" The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019.\n\n- The writing can be improved. A lot citations/references are not particularly relevant to what this paper is about and make some parts not enough self-explained. \n\n----\nThe authors addressed my concerns and I raised my evaluation ratings. \n\n\n\n\n", "title": "A practical OT work that enhances performance on three application tasks", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "vBXK-WHwiZ3": {"type": "rebuttal", "replyto": "V8nR8bgkpPk", "comment": "## Ad 1) Optimal transport evaluation\n\nSince your comment can be interpreted in multiple ways we would like to address each one of them.\n\n**a) Why did we compare transport plans, not Sinkhorn distances?**\n\nWe actually compare both! Table 1 also gives the relative error of the overall Sinkhorn distance and we further analyze the distance approximation in Appendix L. Note that the Sinkhorn distance is a single value and not 10^6 values like the transport plan.\n\nAppendix L also explains why we defer the Sinkhorn distance  to the appendix, instead of the transport plan: \u201cKeep in mind that in most cases the OT plan is more important than the raw distance approximation, since it determines the training gradient and tasks like embedding alignment don\u2019t use the distance at all. This becomes evident in the fact that sparse Sinkhorn achieves a better distance approximation than LCN-Sinkhorn but performs worse in both downstream tasks investigated in Sec. 7.\u201d\n\n**b) Why did we analyze the transport plan, not the cost matrix C or the similarity matrix K?**\n\nSince we are interested in approximating Sinkhorn we think that analyzing its result instead of its input provides more conclusive information. Otherwise we would be missing many crucial influences.\n\n**c) Why did we use PCC to compare transport plans?**\n\nComparing the values of 10^6 pairs is rather challenging. We primarily use the Pearson correlation coefficient (PCC) of the approximate transport plan entries $P_{ij}^{LCN}$ since this directly shows how well the values are correlated with the ground truth $P_{ij}$. In the best case this value will be 1, which means that the values lie on a perfect line with 0 width. The entries of $P_{ij}^{LCN}$ will then be exactly scaled versions of the ground truth $P_{ij}$ (around the fixed mean). In the worst case it will be 0 or even negative. We complement this with the IoU of top 0.1%, which puts more focus on the (most important) highest values.\n\nWe found that PCC and IoU reflect the actual performance much better than e.g. RMSE or MAE because almost all values in the transport plan are close to 0. If a row has a single $P_{ij}$ close to 1 that means that 9,999 values will be close to 0. Measures like the RMSE and MAE then only focus on the unimportant 9,999 values close to 0, while PCC and IoU will show how well the 1 important value is reflected.\n\nTo show that these summary statistics do not hide the bigger picture we also plot the raw values in Figure 1. Factored OT looks similar to the other low-rank approximation (Nystr\u00f6m Sinkhorn) in this comparison, i.e. it is almost flat.\n\nIf you are interested nonetheless, below you can find the RMSE and MAE (std. devs are of the order 0.01e-4). Note that on average LCN-Sinkhorn performs best even for these heavily biased measures.\n\n||MAE/10^-4 (EN-DE)|RMSE/10^-3 (EN-DE)|MAE/10^-4 (EN-ES)|RMSE/10^-3 (EN-ES)|MAE/10^-4 (EN-FR)|RMSE/10^-3 (EN-FR)|MAE/10^-4 (EN-RU)|RMSE/10^-3 (EN-RU)|\n|-|-|-|-|-|-|-|-|-|\n|Factored OT|1.58|3.94|1.63|4.83|1.62|4.70|1.49|2.87|\n|Multiscale OT|1.19|4.05|1.17|4.78|1.15|4.63|1.18|3.13|\n|Nystr\u00f6m Skh.|1.54|3.93|1.59|4.82|1.57|4.69|1.45|2.87|\n|Sparse Skh.|1.55|4.88|1.43|4.97|1.46|4.98|1.70|4.76|\n|LCN-Sinkhorn|1.27|3.50|1.16|3.71|1.18|3.73|1.33|3.04|\n\n## Ad 2) End-to-end performance\nWe are happy to hear that you agree on this point and that you appreciate the fact that we include the end-to-end performance in our evaluation. Most previous works forego this analysis.\n\nWe have omitted factored OT from experiments 2 and 3 due to its OT plan approximation in the first experiment being even worse than Nystr\u00f6m Sinkhorn and its non-competitive runtime.\n\nIf you are still curious to see the factored OT results, these are the results for experiment 2. Factored OT is more than 2x slower than LCN-Sinkhorn and other methods, which is due to factored OT\u2019s slow iterative refinement scheme. Moreover, it completely fails to align the embeddings. While this might be expected based on experiment 1, we do realize that results as clear as this are rather uncommon. Note that we let factored OT use as many landmarks as fit into GPU memory (200, i.e. 5x more than LCN-Sinkhorn) and the partially positive results in experiment 1 show that our implementation is correct. We would also again like to highlight that our experimental setup is completely transparent and reproducible, including the source code. This result thus just highlights this task's difficulty.\n\n||Time (s)|EN-ES|ES-EN|EN-FR|FR-EN|EN-DE|DE-EN|EN-RU|RU-EN|Avg.|\n|-|-|-|-|-|-|-|-|-|-|-|\n|Factored OT|210|0.0 \u00b1 0.0|0.1 \u00b1 0.0|0.2 \u00b1 0.2|0.1 \u00b1 0.0|0.0 \u00b1 0.1|0.1 \u00b1 0.0|0.0 \u00b1 0.0|0.0 \u00b1 0.0|0.1|\n\n## Summary\nWe would like to thank the reviewer for their response and are happy to have clarified any last questions. Since you agree that the paper has substantially improved since your first evaluation we would appreciate it if this would be reflected in an updated overall score.", "title": "Resolved additional concerns"}, "od4lWjpTVhU": {"type": "rebuttal", "replyto": "XGprfy8b9aw", "comment": "## Streamlined text\nWe have removed Section 5 and integrated its content into the graph transport network section, for which these improvements are an important part. We have already tested these improvements separately in Table 3.\n\nWe have furthermore improved the introduction (removing many less relevant citations), pushed back the related work section so it doesn\u2019t overwhelm the reader, and added many explanations and definitions.\n\n## Experimental details\nWe strived to include all relevant details for fully reproducing the experiments in the main paper and in Appendices H, I, and J. The source code is available in the supplementary material. If there is some missing detail, we are happy to include it.\n\n## Contribution\nPlease note that we don\u2019t just study one particular combination of representations, but we are the first to study a sparse approximation for Sinkhorn and also the first to combine a sparse approximation with a low-rank approximation.\n\nAlso, please note that in the first experiment we directly investigate the Sinkhorn approximation. Approximating the transport plan might be viewed as a ranking task, but calling this an application seems rather misleading.\n\n## Factored OT\nWe have added a comparison to factored OT (from Forrow, Aden, et al. \"Statistical optimal transport via factored couplings.\") to our experiments. The method performs comparatively well for high regularization, where it performs similarly to LCN-Sinkhorn (see Fig. 4, 9). However, similar to Nystr\u00f6m-Sinkhorn, it largely fails to approximate the transport plan at any reasonably low regularization (see Table 1, Fig. 3). Moreover, due to its iterative approach its runtime is multiple times higher than those of the other methods (see Table 9).\n\n## Theoretical analysis\nTo complement our empirical results we added a new section with a theoretical analysis. This should also improve the reader\u2019s understanding of our method.\n", "title": "Streamlined text, factored OT, theoretical results"}, "q6Ej0jW_lXX": {"type": "rebuttal", "replyto": "RMShAVkNfCr", "comment": "## Improved readability\nWe have incorporated all of your suggestions and clarified all your questions in the revised version of the paper (including your minor comments). We have furthermore improved the introduction, pushed back the related work section, and integrated the enhanced OT section in the graph transport section to make the paper easier to understand.\n\nRegarding your particular comments:\n- We have added definitions and explanations to address your Cons 1a-g, i-m\n- Ad h1) The cost functions are described in Appendix H. We use the negative dot-product for word embeddings (experiments 1 & 2) and the L2 distance in GTN (experiment 3).\n- Ad h2 & comment 1) There are many special cases for which log-linear algorithms exist and we tried to cover them in our related work section. The convolutional approach you mentioned (we assume you refer to Solomon et al. 2015. \u201cConvolutional wasserstein distances\u201d) is only applicable to very low-dimensional (i.e. 2-3 dimensional) data, not the high-dimensional spaces we focus on. For example, the word embeddings we work with have 300 dimensions.\n- Ad comments 2-3) We have changed the related work accordingly.\n- Ad minor comment 4) We now include Dvurechensky\u2019s result directly in the paper and have made the bound in Appendix A more precise. The $\\lambda$ we chose in our experiments was the one that performed best for full Sinkhorn. Also, sparse Sinkhorn actually performs especially well for very low $\\lambda$ (see Fig. 4), so we don\u2019t expect this to be a problem.\n\n## Fully parallel\nOur method is actually fully parallelizable. All experiments and runtime measurements were performed with a massively parallel PyTorch implementation running on GPUs.", "title": "Fully parallel, improved readability and definitions"}, "8NHbV8PrT4O": {"type": "rebuttal", "replyto": "dbtvdqdH5cT", "comment": "## Contributions\nIt seems to us like there has been some confusion about our paper\u2019s contributions. Our method does not \u201cbalance (and join) Sinkhorn and Nystr\u00f6m in one distance\u201d. Instead, we approximate the similarity matrix $K=\\exp(-C/\\lambda)$ using (1) a sparse approximation and (2) a fused sparse/Nystr\u00f6m approximation (which we call locally corrected Nystr\u00f6m). We then use this approximate K _inside_ Sinkhorn and show that doing so yields a better and faster approximation of Sinkhorn than previous methods.\n\n## Improved readability\nWe have incorporated all of your helpful suggestions in the revised version. In particular:\n- We have significantly improved the introduction, now starting with a general problem description, as suggested.\n- We added many more explanations and definitions to address your various questions, e.g. regarding $s$, $t$, $K^{\\text{sp}}$, $\\bar{P}_{\\text{LCN}}$, and the cost function. This should prevent future misinterpretations of Eq. 5 (previously Eq. 4).\n- We improved the motivation for providing explicit gradients.\n- We have removed the OT enhancements section and moved its content to the graph transport network section, where we use them.\n\n## Nystr\u00f6m approximation\nSinkhorn uses kernels of the form $\\exp(-C/\\lambda)$. Kernels like these (e.g. the Gaussian kernel) typically have a reproducing kernel Hilbert space that is infinitely dimensional. The resulting Gram matrix thus always has full rank and can not be reconstructed by a low-rank matrix such as the one provided by the Nystr\u00f6m method (!). We have made this more clear in the paper.\n\n## Table 1\nWe are not sure if we understand your comment. The text describes the experimental setup before referring to Table 1. The goal of our method is to approximate Sinkhorn, so in this experiment it is not a baseline but the ground truth.\n\n## Theoretical analysis\nTo address your comment on validity we provide a new theoretical analysis in Section 4. In particular, we show that (a) LCN provides significant benefits over Nystr\u00f6m in both a uniform and clustered data model, (b) the error of approximate Sinkhorn is bounded, and (c) approximate Sinkhorn enjoys the same converge bounds as regular Sinkhorn.\n", "title": "Improved readability and theoretical analysis"}, "R9jkFCs5AWk": {"type": "rebuttal", "replyto": "AM0PBmqmojH", "comment": "We would like to thank all reviewers for their helpful feedback!\n\nWe believe that we have addressed all issues in the revised version of the paper. We are now happy to have an even stronger, more approachable and thorough contribution to the community.\n\nIn particular, we have:\n- Added a thorough **theoretical analysis** (Section 4, with proofs and notes in the appendix), which introduces 2 new theorems that analyze the maximum error of locally corrected Nystr\u00f6m for (a) uniformly distributed data and (b) clustered data. We then adapt previously established theoretical results to bound the approximation error of sparse Sinkhorn and LCN-Sinkhorn and their speed of convergence.\n- Revised the first paragraph to start with a general problem description and removed less relevant work from it.\n- Made it more clear in the introduction that our goal is accelerating entropy-regularized OT for point sets.\n- Moved the related work section back to prevent overwhelming the reader.\n- Improved the description of the Sinkhorn algorithm in and around Equation 2.\n- Added Equation 3 and an improved description to better explain the approach of sparse Sinkhorn.\n- Improved the motivation for locally corrected Nystr\u00f6m by explaining why the matrix K is full rank.\n- Explicitly described how we use K_LCN.\n- Improved the motivation for providing explicit gradients and moved this part to the back of the theoretical analysis.\n- Streamlined the reading flow by removing the \u201cenhanced OT\u201d section and integrating its content into the \u201cgraph transport network\u201d section, of which the proposed improvements (learnable unbalanced OT and multi-head OT) are an integral part, even if they are of independent interest.\n- Added factored OT as another low-rank approximation baseline to the experiments.\n- Improved the experimental description, e.g. better described the original Wasserstein Procrustes method.\n- Better described how standard deviation is denoted in our experiments.\n- Added several missing definitions, e.g. for the LSH AND-OR construction in Appendix A.\n", "title": "Overview of Improved Paper"}, "SJ_ybYduWQ": {"type": "rebuttal", "replyto": "7E8KZoQ0t-C", "comment": "## Ad 1) New theoretical convergence analysis\nAs suggested, we have now added a full convergence analysis to the paper (see Section 4). In our understanding, such an analysis requires 2 parts:\n1. How good is the approximation?\n2. How fast does the approximation converge?\n\nTo answer (1) we first analyze the maximum error of the similarity matrix computed by LCN in comparison to regular Nystr\u00f6m in (a) a uniformly distributed data model and (b) a clustered data model. We then use the result by Altschuler et al. 2019 to relate these bounds to bounds for the final Sinkhorn approximation.\nTo answer (2) we only need to slightly modify the bound by Dvurechensky et al. 2018 to show that both sparse Sinkhorn and LCN-Sinkhorn have the same convergence rate bound as regular Sinkhorn.\n\n## Ad 2) Streamlined text\nWe tried to make the paper more coherent by improving the introduction and integrating the section on enhanced OT in the graph transport network section, for which these are an important component.\n\nAd 2a) We agree and have mostly removed this part of the motivation.\n\nAd 2b) LCN-Sinkhorn performing better than full Sinkhorn on word embedding alignment is most likely due to regularization effects caused by the approximation, which lead to better generalization. The original method used by Grave et al. 2019 performs Sinkhorn on an iteratively increasing, randomly sampled subset of the embeddings. Our experiments clearly show that this approach is far from ideal. We have added these explanations to the paper.\n\n_All_ figures and tables in the paper show standard deviations, thus the significance of our results is clearly marked. Furthermore, please note that as opposed to previous works we don\u2019t just show results on some potentially cherry-picked datasets but make the effort of fully integrating our approximations in a pipeline and evaluate end-to-end real-world performance. \n\nAd 2c) We have restructured the text to make this figure\u2019s description more prominent.", "title": "New theoretical convergence analysis, streamlined text"}, "RMShAVkNfCr": {"type": "review", "replyto": "AM0PBmqmojH", "review": "Summary:\nThe paper considers the problem of approximating Sinkhorn divergence and corresponding transportation plan by combining low-rank and sparse approximation for the Sinkhorn kernel and using Nystrom iterations as a substitute for Sinkhorn's iterations. The corresponding approach is amenable to differentiation and can be used as a building block in different architectures. Numerical experiments in several settings are performed to compare  the proposed approach with existing ones and demonstrate its scalability.\n\nEvaluation:\nI believe the proposed framework is a valuable contribution in terms of practical performance and wide list of applications where OT could not be used before because of the high computational cost. So, I would recommend accepting this paper. \n\nPros:\n1. High scalability of the proposed approach and linear up to log factors in dimension complexity.\n2. Flexibility of the framework due to a combination of sparse and low-rank approximations, which are complementary to each other.\n\nCons:\n1. Some parts of the paper seem to be not clear for a general audience.\n\na. First page. $n$ is undefined.\n\nb. First paragraph of Sect. 2. What is \"set of embeddings\"?\n\nc. Last but one paragraph on p.2. $d$ is not defined.\n\nd. In (1) $F$ stands for the Frobenius product, does it?\n\ne. Proposition 1. $N$ is not defined.\n\nf. First paragraph of Sect. 5. What is \"OT with multiple heads\"?\n\ng. What is meant as embedding?\n\nh. In the experiments, what is used as the cost function to define the Sinkhorn kernel $K$? If this is an $L_2$ distance, the convolutions can be used to accelerate the standard Sinkhorn and it would be nice to see the comparisons with convolutional Sinkhorn, which is also log linear.\n\ni. Appendix A. $B,r,b$ are not defined when they are first used.\n\nj. In (17), how was the last equality obtained?\n\nk. In (19), (20), how were the first equalities obtained?\n\nl. Appendix E, first paragraph. What is \"log-sum-exp trick\"?\n\nm. Appendix G. What is \"similarity matrix\"?\n\n2. As far as I understood, the proposed approach is not amenable to parallel computations on GPU as opposed to standard Sinkhorn.\n\n\nMinor comments\n1. Maybe it is too strong to state in the abstract that this is the first log-linear time algorithm given that when the Sinkhorn kernel corresponds to a convolution, the Sinkhorn's algorithm is log-linear by using the FFT.\n2. Bibliographical note. (Altschuler et al., 2017) did not show $1/\\varepsilon^2$ bound for the Greenkhorn. Their bound for Greenkhorn is the same $1/\\varepsilon^3$ as for the Sinkhorn. The bound for Sinkhorn was improved to $1/\\varepsilon^2$ in http://proceedings.mlr.press/v80/dvurechensky18a.html and the bound for Greenkhorn was improved to $1/\\varepsilon^2$ in http://proceedings.mlr.press/v97/lin19a.html.\n3. Bibliographical note. Quadratic regularization for OT was proposed in https://arxiv.org/abs/1704.08200.\n4. Appendix A. I believe that in this framework a general value of the regularization parameter $\\lambda$ is used. If it is the case, then the number of Sinkhorn iterations to find an $\\varepsilon$-solution to the regularized problem is $1/(\\varepsilon \\lambda)$. This follows from http://proceedings.mlr.press/v80/dvurechensky18a.html Theorem 1 and an estimate for $R$ in Lemma 1. The bound $1/\\varepsilon^2$ corresponds to finding an $\\varepsilon$-solution for the non-regularized problem. In this case one has to set $\\lambda=\\varepsilon/(4 \\ln n)$, which may be too small.", "title": "Good practical approach, but not sufficiently clear for a general audiece", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "dbtvdqdH5cT": {"type": "review", "replyto": "AM0PBmqmojH", "review": "Proposal:\n- First log-linear time algorithms for entropy-regularized OT that work for complex \n  real-world tasks using high-dimensional spaces with little to no loss in accuracy\n  ... many claims in one statement\n- Locally Corrected Nystr\u00f6m\n  ... this would deserve a single paper - just to proof everything is still fine and valid\n  in particular to alternatives\n- In the way this paper is written I am positive it gets accepted \n  (because it fits the writing style of the more recent papers in the field)\n  and contains sufficient novelty\n  ... but I always wonder if good (published) science originates from clarity (or confusion)\n\ncomments:\n- sorry but the text is very hard to follow !\n- 'Optimal transport is concerned with the problem' (on p.3) - I think some introductive\n  work may not harm in the first page\n- the reader is thrown up by terms and references - in my view more confusing than enlighting\n  --> it may not harm to add some brief explainations of terms (from Cuturi:)\n  'A transport plan is a flow on that graph satisfying source (a i flowing out of each node i) \n   and sink (b j flowing into each node j 0 ) ... --- which is simple an optimal flow\n  in a graph ... I am not sure why we not simple can call it like this but need to come up with\n  new terms\n- The paper is written (following the very strange title\n   ... although Cuturi did the same it would be nice if we can stop having marketing titles\n  but focus a bit on science again ... in particular in 10 years many things proposed nowadays\n  are not lightspeed or warpspeed anymore) like providing an all issues solving theory \n  --> this does not improve the readability of the paper\n  For example Eq.2 what is the 'meaning' of (s) and (t) -- I have an idea but it is not written there\n- it is hard to proofread and verify a paper if it is written with the objective to confuse the\n  reviewer ;-)\n- widely incremental work by combing some known ideas (Nystreom, LSH, ...) - with a lot of addon\n  theory which is probably correct but not very clear in the presentation\n- 'Since the Nystr\u00f6m method is a low-rank approximation it only accounts for the\nglobal structure of the kernel matrix K and not for the local structure around each point x.'\n  - this is actually wrong (!) - if the landmarks are indepentent and the number of landmarks\n  aligns with the rank of the matrix - Nystroem will provide a perfect (!) reconstruction\n\t--> there is a lot of work on the approximation bound of Nystroem (and related methods) - see\n  e.g. work by Dhillon\n- where is the definition of 'sparse approximation K^sp' used in Eq 3? \n- how precisely does \\bar{P} (after Eq 3) link to the part around Eq 2? - is the Kernel K_{ij} used\n  here as well and / or where is the actually input data Kernel matrix \n- in Eq 1 what should be a cost function here and how do you obtain C_{ij}?\n- Ok Eq 4 is an actual proposal by balancing (and joining) sinkhorn and Nystroem in one distance formulation\n  and it would not harm to motivate why and where you need such a distance in advance \n  (problem statement --> solutions --> particular strategy --> outlined proposal --> evaluation + proofs)\n- 'Most modern ML models are trained using backpropagation' - lets rephrase it as: nowadays neural network\n  approaches are trained by backprop ... there are many other methods which are not at all trained by backprob\n  for good reasons \n- '... Usually we want to learn embeddings which act as point sets X p and X q and therefore need gradients' \n  - well yes, if we stick on neural networks we need vectorial inputs and hence are often looking for (costly)\n  embeddings - if we do not use NN we may not have this problem (but others)\n- 'We can either estimate these gradients via automatic differentiation' - this is in general the more costly\n  way to do things and I am happy to see that explicit derivations are given\n- regarding table 1 --> before you come with numbers (where you measured something) it would be good to specify\n  details of your scenario (which are omitted before) - in particular which data, which cost function, which parameters \n  a.s.o. -- and although I understand that you like your method most it would still be good to provide some\n  oldfashion baseline (and not - not from sinkhorn)\n- 'We propose the graph transport network (GTN) to evaluate approximate Sinkhorn and enhanced optimal transport and advance the state of the art on this task.' --- fantastic on page 6 you actually outline a more userfriendly motivation\n", "title": "Review on Warpspeed Computation of Optimal Transport, Graph Distances, and Embedding Alignment", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}