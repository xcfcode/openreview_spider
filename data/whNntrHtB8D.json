{"paper": {"title": "Gradient Based Memory Editing for Task-Free Continual Learning", "authors": ["Xisen Jin", "Junyi Du", "Xiang Ren"], "authorids": ["~Xisen_Jin3", "~Junyi_Du1", "~Xiang_Ren1"], "summary": "We propose a task-free memory-based continual learning algorithm that edits stored examples over time", "abstract": "Prior work on continual learning often operate in a \u201ctask-aware\u201d manner, by assuming that the task boundaries and identifies of the data examples are known at all times. While in practice, it is rarely the case that such information are exposed to the methods (i.e., thus called \u201ctask-free\u201d)\u2013a setting that is relatively underexplored. Recent attempts on task-free continual learning build on previous memory replay methods and focus on developing memory construction and replay strategies such that model performance over previously seen examples can be best retained. In this paper, looking from a complementary angle, we propose a novel approach to \u201cedit\u201d memory examples so that the edited memory can better retain past performance when they are replayed. We use gradient updates to edit memory examples so that they are more likely to be \u201cforgotten\u201d in the future. Experiments on five benchmark datasets show the proposed method can be seamlessly combined with baselines to significantly improve the performance.", "keywords": ["Continual learning", "task-free continual learning"]}, "meta": {"decision": "Reject", "comment": "The range of the initial reviews was fairly high with overall scores ranging from 4 to 7.\n\nThe authors provided a good response that answered most of the reviewers' comments and questions. One of the reviewers even increased their score following the authors' response. \n\nThe focus of some of our discussions and what ultimately led to my suggestion was the related work of MIR [1]. The methodological differences between (the three versions of) MIR in [1] and GMED [this paper] appear less significant than what the current submission suggests. While there is some disagreement between the authors and Reviewer1 about the exact differences, I find that the current manuscript does not acknowledge the close relationship between these two contributions. Further, from the experimental standpoint and without further justifications the gains from GMED+MIR could be attributed to using more replay (from combining GMED and MIR).\n\n\nIn their response, the authors disputed the view of Reviewer1. I believe the source of the confusion between the author and the reviewer might be captured in this sentence from the author response to Reviewer1: The approach does not learn a generator that can \u201cgenerate examples that are more forgettable for the classifier\u201d; instead, feeding more forgettable examples in GEN-MIR aims at reducing the forgetting of the generator.\n\nLooking at Equations 2, 3 and Algorithm 1 from [1], in GEN-MIR while two different procedures are used to obtain forgettable examples for the generator (B_G in Alg. 1) and the classifier (B_C in Alg. 1), the generator is used in both cases. In other words, the generator is used to generate examples for both itself and for the classifier. So, I think it's fair to say that the generator does indeed generate examples that are more forgettable for the classifier (Eq. 2). \n\n\nI strongly encourage the authors to prepare another version of their work where the differences between MIR [1] and their contribution are clearly highlighted and the results show the advantages of GMED (including memory-editing in data space). \n\n\n[1] Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, and Tinne Tuytelaars. Online continual learning with maximally interfered retrieval. In NeurIPS 2019. https://arxiv.org/abs/1908.04742"}, "review": {"RM3qGt9pr8": {"type": "review", "replyto": "whNntrHtB8D", "review": "## Summary\n\nThis paper proposes a task-free continual learning method called GMED that extends experience replay.\nThe key idea is to modify the individual data points in the replay memory to maximize the one-step forgetting at the current time step.\n\n---\n\n## Pros\n\n- The idea of editing the data in a replay buffer with gradient descent is novel.\n- The experiments show a significant performance gain compared to the baselines.\n\n---\n\n## Cons\n\n### Lack of justification for editing memory\n\nGiven a somewhat arbitrary objective function, updating the data in memory by gradient descent will make the data points move away from their original distribution. Since continual learning aims to learn the data's distribution, I think modifying each data point does not align with this goal.\n\nOne possible explanation for the performance improvement is that the editing acts as a kind of data augmentation. As shown in Figure 5, the difference between the edited data and the original data is minuscule. Since the performance would drop if the editing is significant, the magnitude of updates should be small such that the data do not deviate too far from the original distribution.\n\nFrom this point of view, I think it is critical to show that GMED is better than other data augmentation tricks.\nThe authors already performed an ablation study showing that GMED is better than adding random noise to the data.\nHowever, there is no much detail about the experiment to verify its validity. Since it is an important experiment, I strongly recommend the authors to provide enough details for reproducing.\nAlso, I would like to see a comparison with other standard data augmentation methods.\n\n### Confusing motivation and writing\n\nThe fundamental hypothesis of this paper is:\n> replaying examples that are likely to be forgotten by the current model helps retain its test performance.\n\nHowever, the final algorithm is far from this hypothesis. As I mentioned previously, I think it is closer to a data augmentation algorithm.\n\n### Unreasonable experimental setting\n\nFor the Split CIFAR-100 and Split mini-ImageNet experiments, the authors use a memory size of 10,000. However, the size of the whole training set is 50,000, which means the memory can store up to 20% of the whole data. In my opinion, the current trend is to keep the memory size under 1-2% of the whole dataset, which corresponds to 500-1000 in the case of CIFAR100. Note that the authors also follow this trend for MNIST and CIFAR-10. With 20% of the dataset, I suspect that even i.i.d. training on the memory would not differ much from i.i.d. training on the whole dataset.\n\nAlso, I think it is more constructive to compare models under a fixed number of examples in memory, not the number of bytes as in Table 3. While trying to equalize the memory usage of ER(+GMED) and CN-DPM, the number of examples in memory became unreasonably large for ER(+GMED) (over 40% of the whole data).\n\nThe brutal fact is that the current level of continual learning is far from practical. At present, most CL papers work on toy problems like CIFAR-100. At this level, I think comparing the actual memory usage is not meaningful. In my opinion, what the community should look for is an efficient algorithm that can scale to real-world problems. For this reason, I think it is more meaningful to constrain the number of examples in memory. Also, note that if we scale up the image size, the size of image data can be more dominant than CNN's parameter size.\n\n---\n\n## Overall evaluation\n\nThe proposed method does not have any theoretical justification, and several settings in the experiments are unrealistic. Nonetheless, since the experiments show significant improvements over baselines, I think it can be an effective technique. Unfortunately, at the current state of the paper, I cannot exclude the possibility that simpler data augmentation tricks on the replay memory may outperform the proposed method. Although I currently vote for rejection, I am willing to raise the score if the authors can offer enough evidence that GMED is better than trivial data augmentation tricks.\n\n---\n\n## Post-rebuttal\n\nDuring the rebuttal period, the experiments largely improved, resolving the majority of my concerns. I raise my rating accordingly. I expect the authors to reflect other suggestions as well in the final version. Especially, R1 raised a serious concern about the similarity between GMED and GEN/AE-MIR. A proper comparison should be included in the final version.", "title": "Intriguing Empirical Results, But Lacking Theoretical Justification", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "WEys4feIdki": {"type": "review", "replyto": "whNntrHtB8D", "review": "**Summary**\n\nThe authors propose a gradient-based memory editing scheme for replaying samples that are undergoing the most forgetting, coined GMED. They also propose a hybrid method with MIR [1]. An extensive evaluation on standard benchmarks shows some improvements throughout. \n\n**Concerns**\n\nMy main concern is that the authors seem to have completely missed that gradient-based memory editing has been proposed in [1]. Specifically, in their GEN-MIR and AE-ER methods, gradient-based optimization is performed on some latent codes of the data to maximize the \"forgetting\" measurement. \n\nGEN-MIR seems superior to GMED as it can perform multiple gradient-based edits on the data to achieve a significantly different replay sample. GMED can't because it re-uses the initial label as the label of the editable sample. More GMED edits will thus increase the probability that the initial label is wrong. I think this is why the authors only do one edit. You can see, however, that one edit doesn't seem to change the replay data, as exemplified in Figure 5. [1] also proposes AE-MIR when $p(x)$ is difficult to model online. Furthermore, I would argue that gradient-based editing is more sensible in latent space (GEN-MIR and AE-MIR) than in input space (GMED) because it will be much easier to stay on the data manifold. It is also much more computationally efficient. \n\nAll of section 4.1 and most of section 4.2, including \"estimating forgetting online\" can be found in [1]. Algorithm 1 in section 4.3 is also astonishingly similar to the ones in MIR. \n\nI think this work is too incremental to merit a publication at a top conference like ICLR. Thus, I encourage the authors to either submit to a lower-tier conference or further develop the methodology.\n\n**minor concerns**\n\n- The related work seems to be missing a branch of task-free CL, namely Continual-Meta Learning. \n  \n- no need to re-explain Reservoir Sampling\n\n- typos: a lot of hyphens are missing. e.g. *gradient-based* memory editing\n\n\n__________\n\n**POST-REBUTTAL**\n\nSadly, I'm decreasing my score a notch because the authors lack a deep understanding of MIR [1] that would make it evident that GMED is *much* closer to the three methods proposed in MIR. Specifically, the authors have responded to my concern about the lack of novelty:\n\n    In GEN-MIR, the classifier and the generator separately retrieve most forgettable examples for themselves. The generator is indeed optimized for \u201cmaximizing the forgetting\u201d, but the \u201cforgetting\u201d here is measured for the generator - i.e., the generator retrieves most forgettable examples for the generator itself. The approach does not learn a generator that can \u201cgenerate examples that are more forgettable for the classifier\u201d; instead, feeding more forgettable examples in GEN-MIR aims at reducing the forgetting of the generator.\n\nThis is simply not true. If you take a look at Equation 2 in MIR, you will see that the generator is generating forgettable examples **for the classifier**. It also uses it for itself, see Equation 3. GEN-MIR is thus a gradient-based memory editing for CL.\n    \n    AE-MIR (...) It is a hybrid approach of example compression and ER-MIR. There is no online optimization towards more forgettable examples for the classifier like GMED\n\nAgain, just like GEN-MIR, AE-MIR uses gradient-based memory editing for CL for the classifier.\n\nIn GMED lies somewhere between ER-MIR and [GEN-MIR, AE-MIR]. My guess is that the MIR's author didn't propose the GMED method because it doesn't make a lot of sense to update edit a sample **and not edit its label**. \n\nHere is an intuition on the behavior of each method: let's say your models sequentially learning to visually classify objects. The model is now learning about zebras and it's causing some interference on the horse's representation.\n\n- ER-MIR will search in its buffer and retrieve a horse for the classifier to do replay on.\n- GEN-MIR will search inside the latent space of a generative model to find generated horses for the classifier to do replay on.\n- AE-MIR will search the latent space of an autoencoder to retrieve past horses that appeared in the data stream for the classifier to do replay on.\n- GMED randomly samples some data in the buffer, e.g. a car, and takes one gradient update on the car such that it resembles more a horse. Then the classifier is fed that modified image (i.e. x) as well as the unchanged horse label (i.e. y)\n\nThe empirical section shows us that one needs to add MIR to GMED to obtain the best results. This comes as no surprise. Combining methods with each other and increasing computing needs and/or replay will give you a better performance on forgetting.\n\n\n\n\n_________\n\n\n[1] Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, andTinne Tuytelaars.  Online continual learning with maximally interfered retrieval.  In NeurIPS 2019.\n\n\n", "title": "ok paper, but gradient-based memory editing for CL was already proposed", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "CuMaSQTpTRP": {"type": "rebuttal", "replyto": "Aj3dUm_xl22", "comment": "We thank the reviewer for the careful check into the experiments! Here are our responses to new questions, with some new experimental results.\n\n**The gap between ER and iid-training on the memory is close**\n\nWe believe it is a question towards the general practice of memory-based continual learning . However, memory-based continual learning approaches are still useful in the online continual learning (OCL) setup. In OCL, a model is trained over an online stream of data and may be queried *at any time step*; the practice above, however, requires a new model to be trained each time the model is queried for testing. Therefore, we argue memory based continual learning algorithms should not be compared against the approach above.\n\nBesides, the good performance of iid-training on memory could be a gain from longer epochs of training until convergence. However, in OCL, the model visits a single pass of the online stream, and there is no \"validation set\" to determine convergence. Retraining over the whole task also does not adhere to the OCL setup.\n\n**Although it is not a strictly fair comparison, the standard data augmentation seems to be far more effective than GMED. The standard augmentation boosts the performance significantly, while GMED improves the accuracy by only about 1 percentage point.**\n\nWe very much thank the reviewer for verifying the benefit of data augmentation with experiments in an iid-offline setting. We also performed our own experiments in the OCL setup. Given that the main concern is \"whether editing on memory acts worse than standard data augmentation\", we design the following experiment: each time we draw a mini-batch of examples from the memory for replay, we transform the examples with standard data augmentation techniques (random crop, random horizontal flip, random rotation). We feed both the original mini-batch and the transformed mini-batch to the model for replay.\n\nWe see data augmentation has significantly improved performance over the reported results for ER; however, GMED can be built upon data augmentation to further improve the performance; also, the improvement is not diminished because of the data augmentation applied. We run new experiments for 5 runs with the same set of random seeds. We used the same set of hyperparameters as before.\n\nMethod | Acc. \n---- | ----\nER (reported) | 33.34\nER + GMED (reported) | 35.01\nER + data aug. | 46.13\nER + GMED + data aug. | 49.38*\n\n*: Better than ER + data aug. with a p-value < 0.05 in a single-tailed paired t-test\n\nPreviously, we used a memory size of 10,000 for split CIFAR because the memory below this number yields very weak classifiers (e.g. with a memory size of 1000, the accuracy is around 8%). After applying data augmentation, we find the performance has improved, and thus we also report the results on CIFAR-100 with a memory size of 1,000.\n\nMethod | Acc. \n---- | ----\nER + data aug. | 14.16\nER + GMED + data aug. | 14.87\n\nWe thank the reviewer for pointing out the huge benefit of data augmentation in OCL; previously, given that data augmentation is a generally applicable approach for memory-based approaches,  for consistence with previous works, we did not apply data augmentation for any approaches. However, given such a significant gain of data augmentation, it may be preferable to evaluate the effectiveness of algorithms on the basis where data augmentation is applied. We will report results with standard data augmentation for other methods when they are ready.\n\n**iid-offline score is too low**\n\nThanks for pointing out the issue. We used the settings from [1] (e.g. family of optimizers) to run iid-offline experiments (for Split MNIST, Split-CIFAR-10, and Split mini-Imagenet), and our reported number of baselines are mostly consistent with [1]. We will re-run iid-offline experiments on Split-CIFAR 100.\n\n\n[1] Aljundi, Rahaf, et al. \"Online continual learning with maximal interfered retrieval.\" Advances in Neural Information Processing Systems. 2019", "title": "Response to Reviewer #2"}, "lhXErUPOrSZ": {"type": "rebuttal", "replyto": "WEys4feIdki", "comment": "**Q2: GMED edits will increase the probability that the initial label is wrong. I think this is why authors only do one edit**\n\nIt is possible but if we edit examples too drastically. However, by making editing rather conservative, we have achieved better performance on several datasets. \n\nOur editing mechanism is motivated by a well-studied hypothesis (forgettable examples are more helpful). The approach is reasonably approximating our hypothesis; even though it may not be perfect but it is one good enough solution to improve performance on several datasets.\n\n**Q3: You can see, however, that one edit doesn't seem to change the replay data, as exemplified in Figure 5**\n\nIndeed, our editing will not bring significant changes in the input space, which is a preferable consequence of the proposed algorithm, because it confirms examples stay in original distributions and the original label is likely to be correct for the edited example. However, just like it is possible to create \u201cadversarial examples\u201d that could flip model\u2019s predictions with indistinguishable differences by humans, a small change on input examples may affect model behaviors in a significant way. Our experiments confirm such small updates are useful.\n\n\n**Q4: Does GMED work when p(x) is difficult to model?**\n\nIn our experiments, we show that in the most challenging dataset, mini-ImageNet, GMED brings statistically significant improvements. Besides, on Split-CIFAR 10, which is believed to be a challenging dataset for generative replay based methods [1], GMED also brings improvements. Therefore, even when p(x) is moderately difficult, GMED still works well.\n\n**Q5: All of section 4.1 and most of section 4.2, including \"estimating forgetting online\" can be found in [1]. Algorithm 1 in section 4.3 is also astonishingly similar to the ones in MIR.**\n\nSection 4.1 states our hypothesis of how memory editing should be performed. We cited [1], and also another methodological paper [2], and an empirical study [3] to support that \u201cexamples that are likely to be forgotten should be prioritized for replay\u201d is a common hypothesis applied in previous methodology papers and with empirical justification.\n\nThanks for pointing out the issue of section 4.2. It seems the current writing has caused some confusion. We will update the draft and add references right after our introduction about how to estimate forgetting online.\n\nSince MIR and GMED apply the same hypothesis (examples that are likely to be forgotten should be prioritized for replay), it is natural that the algorithms share some steps. However, we argue that the example editing in GMED is orthogonal to example retrieval in MIR, and they can be even combined to achieve better results (the reported MIR-GMED method).\n\n**Q6: Missing comparison & references of a branch of task-free CL, namely Continual Meta learning**\n\nThanks for pointing out. The proposed GMED method is focused on improving existing memory-based methods. We will add references to these methods in our related works section.\n\n[1] Aljundi, Rahaf, et al. \"Online continual learning with maximal interfered retrieval.\" Advances in Neural Information Processing Systems. 2019.\n[2] Chaudhry, Arslan, et al. \"Using hindsight to anchor past knowledge in continual learning.\" arXiv preprint arXiv:2002.08165\n[3] Toneva, Mariya, et al. \"An empirical study of example forgetting during deep neural network learning.\" ICLR 2019.\n", "title": "Response to Reviewer 1 (2/2)"}, "i6bw1W9715L": {"type": "rebuttal", "replyto": "RM3qGt9pr8", "comment": "Thanks for your insightful comments!\n\n**Q1 Lack of justification for editing memory. Given a somewhat arbitrary objective function, updating the data in memory by gradient descent will make the data points move away from their original distribution. Since continual learning aims to learn the data's distribution, I think modifying each data point does not align with this goal.**\n \nAccording to Hypothesis 1, it is true that we expect data points to stay within their original distribution while maximizing their estimated forgetting. To achieve this, we have introduced regularization terms, and performed very conservative updates on examples (e.g., only one example at each time step). Our approach is reasonably approximating our hypothesis; even though it may not be perfect but it is one good solution that could improve performance.\n\n**Q2. Comparison between GMED and other data augmentation tricks & The fundamental hypothesis of this paper is: replaying examples that are likely to be forgotten by the current model helps retain its test performance. However, the final algorithm is far from this hypothesis. I think it is closer to a data augmentation algorithm.**\n\nThanks for your thoughtful comments. We agree that data augmentation could be a reason behind GMED\u2019s improvement - the random editing baseline reported in our paper improves over counterparts without editing on split CIFAR-10. In our follow-up experiments, we verified that GMED outperforms other variants of data-augmentation approaches and the following alternative editing objectives.\n\n- Editing examples to the opposite direction as GMED (Flipped GMED)\n- Creating adversarial examples during training with PGD-like editing objective [2]\n\nThe results are shown in the table below (we select four datasets where there are clear differences in results between different memory-based algorithms). We re-run ER, ER-GMED methods with a new and the same set of 5 random seeds. We see GMED outperforms alternative editing methods.\n\nMethod/Dataset | Split MNIST | Rotated MNIST | Split CIFAR-10 | Split mini-ImageNet\n-------- | ------- | ------- | ------- | ------\nER+GMED | 82.82 | 77.76 | 36.21 | 28.19\nER+Flipped GMED | 74.82 | 77.41 | 32.67 | 27.96 \nER+Adversarial examples | 75.02 | 72.75 | 33.64 | 25.07\n\n **Q3. One possible explanation for the performance improvement is that the editing acts as a kind of data augmentation. As shown in Figure 5, the difference between the edited data and the original data is minuscule. Since the performance would drop if the editing is significant, the magnitude of updates should be small such that the data do not deviate too far from the original distribution.**\n\nWe further provide clarification regarding the reasoning of the reviewer\u2019s reasoning above. We agree we should not make significant changes to memory examples. GMED takes measures to prevent edited examples from deviating too much from original examples (e.g. regularization term in Eq. 3). However, The small editing performed also does not imply the algorithm is similar to data augmentation. We argue that a small but principled change in input examples may affect model behaviors in a significant way. For example, in the literature that studies adversarial attacks, adversarial examples may effectively flip the model's predictions, even if they look visually the same as original examples.\n\n**Q4. Questions about memory sizes**\n\nFor split mini-Imagenet, we borrowed the experimental setups from [1]. We applied the same memory size on split CIFAR-100 given the similarity of the stream length, the total number of tasks, and the total number of classes. We agree that memory-based continual learning algorithms should try to reduce the size of the memory. However, we find that under 500~1000 memory examples, the performance on these two datasets is too low (below 10% accuracy) to draw meaningful conclusions.\n\n[1] Aljundi, Rahaf, et al. \"Online continual learning with maximal interfered retrieval.\" Advances in Neural Information Processing Systems. 2019.\n[2] Madry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" ICLR 2018\n\n", "title": "Response to Reviewer #2"}, "QmFMSXGtAAj": {"type": "rebuttal", "replyto": "WEys4feIdki", "comment": "Thanks for your thoughtful comments! \n\n**Q1: Has the gradient-based memory editing already been proposed in [1]? What are the differences between GMED and GEN-MIR / AE-MIR**\n\nWe appreciate the reviewer's careful check into [1]. However, we would like to point out there are some misunderstandings of GEN-MIR and AE-MIR in the review, which may have caused the impression that GMED and GEN-MIR / AE-MIR are similar.\n\nIn GEN-MIR, the classifier and the generator separately retrieve most forgettable examples for themselves. The generator is indeed optimized for \u201cmaximizing the forgetting\u201d, but the \u201cforgetting\u201d here is measured for the generator -  i.e., the generator retrieves most forgettable examples for the generator itself. The approach does not learn a generator that can \u201cgenerate examples that are more forgettable for the classifier\u201d; instead, feeding more forgettable examples in GEN-MIR aims at reducing the forgetting of the generator. In contrast, GEMD aims at  \u201cediting examples so that they are more forgettable for the classifier. We agree that the approach above (learning to generate forgettable examples for the classifier) can be potentially an extension of GMED and we are interested in further study.\n\nWe noticed the GEN-MIR method in [1], but we did not discuss it in the paper because the performance is outperformed by ER-MIR by a large margin on Split MNIST and only a small performance improvement is observed on Permuted MNIST. \n\nMethod/Dataset | Split MNIST | Permuted MNIST \n-------- | -------- | -------- \nER \t\t|    80.96\t| 79.69\nER-GMED\t|\t82.68\t| 79.70\nER-MIR (task-free version) |\t84.88 | 79.96\nER-MIR-GMED |\t87.86\t| 80.11\nGEN-MIR (reported in [1])\t|\t82.1\t| 80.04\n\nFurthermore, we do not see reported results for Split CIFAR10 and Split mini-ImageNet for GEN-MIR in [1], while there are reported results for ER-MIR. In fact, training a generative model on more challenging datasets compared to MNIST especially in an online (single-pass), task-free setup is challenging, which is also pointed out in [1]. GMED does not require a generative model, and therefore it is extendable to more complicated datasets (e.g. split mini-imagenet) and improves performance on these datasets. \n   \n\nAE-MIR relies on an *encoder-decoder pre-trained offline* to compress input data before storing them in the memory with the encoder, and perform MIR search over them and decode them for replay. It is a hybrid approach of example compression and ER-MIR. There is no online optimization towards more forgettable examples for the classifier like GMED.", "title": "Response to Reviewer 1 (1/2)"}, "f-ZwHWwUpUe": {"type": "rebuttal", "replyto": "Hg7njTZ8L7o", "comment": "Thanks for your positive comments!\n\n**Q1: One comment I have here is that the way this motivation is presented in the paper can be improved; perhaps with a more clearly written paragraph in the preliminaries. On my first reading of the introduction for example, this sentence for example**\n\nThanks for pointing out this clarity issue. We will update the updated version of the paper.\n\n**Q2: the reasoning here is that because naturally forgettable examples help during replay, making an example \"forgettable\" will naturally improve its usefulness. I can't fault this reasoning based on the results but to me it is not naturally clear that this follows. Would the authors be able to provide some intuition for what that means from an optimization perspective?**\n\nThanks for the question. Given the empirically validated hypothesis of \u201creplaying forgettable examples are more helpful\u201d, we just take the most effective way that aligns with this hypothesis -- i.e., following the gradient of forgetting -- to search in the input space for most forgettable examples. It justifies our editing algorithm, which performs iterative optimization towards finding such forgettable examples.\n\n**Q3: Including results of EWC and MbPA**\n\nThanks for pointing out! We tried EWC but it does not perform well in a class-incremental learning setup (i.e. a set of class labels appear incrementally in the stream, which is the setting applied for split MNIST, split CIFAR, and split mini ImageNet in our paper.) We will update the results of MbPA when it is ready.\n\n\n\nFinally, we would like provide clarifications regarding the comment \"Further, the authors propose an online metric for how much the network forgets an example (by comparing the loss at consecutive timesteps)\". We would like to clarify that the technique of estimating forgetting online is applied in a previous work ER-MIR [1] which is discussed and included as one of our baselines; however, there are differences on what the algorithm does after estimating forgetting online: in ER-MIR, the estimated forgetting is used for retrieving most forgettable example from the memory; while our proposed method tries to editing memory example to that they are more forgettable. We compared two methods in the paper and also showed GMED can be seamlessly combined with MIR (GMED) to achieve better performance than MIR because they are mostly orthogonal approaches.\n\n[1] Aljundi, Rahaf, et al. \"Online continual learning with maximal interfered retrieval.\" Advances in Neural Information Processing Systems. 2019.", "title": "Response to Reviewer #4"}, "UAYMVvcvTld": {"type": "rebuttal", "replyto": "JAiWUXy2XRE", "comment": "Thanks for your insightful comments!\n\n**Q1: Some reported results are underperforming reported results in other papers**\n\nThe performance of a method depends on several factors, such as the size of the replay memory, the number of tasks for manually created datasets (e.g. permuted MNIST). We follow experimental setups from [1] for split MNIST, permuted MNIST, split CIFAR-10, and split mini-ImageNet; we follow [2] for rotated MNIST experiments. It is possible that other papers do not use identical settings, which makes it hard to directly compare the results.\n\nWe also note that for the MIR baseline, we modified the strategy of sample examples from their memory so that MIR can be run in an online task-free setup (which is discussed in Appendix A: the original MIR filter out memory examples that belong to the same task as the current data stream, which assumes knowledge about tasks boundaries.) We notice that it causes some performance drop on MIR.\n\n**Q2: Can the idea of memory editing also be added as an additional loss to other methods too (not only to ER and MIR)**\n\nYes, it is true, but probably it requires some modifications. For example, when applying GMED on MIR, we performed adaptation that the algorithm draws a separate set of examples for replay and edit, considering that examples replayed by MIR are already the most forgettable ones.\n\n**Q3: Random editing seems promising. Where does improvement of GMED comes from**\n\nIt is certain that replaying examples itself (i.e., the ER baseline) greatly alleviates forgetting compared to simple online training. Our method builds upon ER and tries to improve the performance of it. The minor improvement of random edit over ER or MIR demonstrates that regularization effects (or data augmentation) can be a reason behind the performance improvement of GMED. However, we note that editing examples following GMED outperform over alternative data augmentation baselines. We include the results in our response to Reviewer #2.\n\n**Q4: The regularization is a special way of weighing the two loss term**\n\nThanks for pointing out! We included this in the updated version of the paper.\n\n**Q5: formatting issues and typos**\n\nThanks for pointing out! We will update them in the updated version.\n\n[1] Aljundi, Rahaf, et al. \"Online continual learning with maximal interfered retrieval.\" Advances in Neural Information Processing Systems. 2019.\n[2] Chaudhry, Arslan, et al. \"Using hindsight to anchor past knowledge in continual learning.\" arXiv preprint arXiv:2002.08165\n", "title": "Response to Reviewer #3"}, "Hg7njTZ8L7o": {"type": "review", "replyto": "whNntrHtB8D", "review": "This paper deals with continual learning. Specifically, given a stream of tasks we want to maximise performance across all tasks. Typically neural networks suffer from catastrophic forgetting which results in worse performance on tasks seen earlier in training. There are many proposed solutions to this problem. One specific set of approaches are \"memory based\" algorithms. Here we store some training examples in memory from the tasks seen thus far. These are then mixed in with new training data so as to encourage the model to not forget past tasks. \n\nThere are two central question for approaches of these kinds: what examples to store and how to use them. This paper details a method that is complementary to most approaches. Specifically, given a system of storing examples (here reservoir sampling) and using them, this methods deals with how best to extract performance from an example. Specifically, it \"edits\" the example before storing and replaying it, to make it more useful to the model. \n\nThe main motivation for the editing process is as follows: past works have shown that the most useful examples to replay are those that are most likely to be forgotten. Thus, if we take our examples in memory and update them (via gradient ascent) to be \"more forgettable\", then when replayed these will result in the most benefit to the model. \n\n* One comment I have here is that the way this motivation is presented in the paper can be improved; perhaps with a more clearly written paragraph in the preliminaries. On my first reading of the introduction for example, this sentence for example, \"_edits examples stored in the memory with gradient-based updates so that they are more likely to be forgotten_\" sounds like you're trying to get the model to forget examples more which is the opposite of what we want (as opposed to making the example inherently more forgettable so it is better when replayed). \n\n* The reasoning here is that because naturally forgettable examples help during replay, _making_ an example \"forgettable\" will naturally improve its usefulness. I can't fault this reasoning based on the results but to me it is not naturally clear that this follows. Would the authors be able to provide some intuition for what that means from an optimisation perspective? \n\nFurther, the authors propose an online metric for how much the network forgets an example (by comparing the loss at consecutive timesteps). Then when taking an example from memory, we perform a gradient update on the image to optimise this metric. \n\nLastly, we replay the edited example to the network and perform a gradient update based on both this edited example and the current training example. Figure 2 is very useful and well done to illustrate this. \n\nThe experiments are performed across standard tasks and show modest to good improvement over the range of datasets. \n\n * Two baselines that would be useful to include in the table are EWC (https://arxiv.org/pdf/1612.00796.pdf) and MbPA ((https://arxiv.org/pdf/1802.10542.pdf), the latter of which is a memory based method that also does not require tasks IDs. \n\nLastly there is some good visualization and discussion of what editing does. \n\nOverall this is a good paper with that deals with an interesting problem and proposes an interesting method of increasing performance.\n\n*  I think some ways to strengthen this paper would include addressing some of the comments above, specifically around motivating the \"editing\" process better and clarifying some of the language around it. \n* Finally, the field of continual learning in general could do with moving to more large scale tasks. This would allow testing these methods more thoroughly. While the results here are promising, they do not increase performance uniformly across all tasks -- for example on mini Imagenet which is harder than MNIST -- this method does not help. In general considering larger scale tasks (based on Imagenet, Omniglot or reinforcement learning like Atari, for example) would provide more evidence and allow further improvement of the method. \n\nIn the current form however, this is still a very good submission and I would recommend acceptance. ", "title": "An interesting method to improve memory based continual learning ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "JAiWUXy2XRE": {"type": "review", "replyto": "whNntrHtB8D", "review": "Differentiation\u00a0with respect to forgetting looks a promising\u00a0idea that can\u00a0complement differentiation with respect to current task accuracy and leads the CL algorithm towards a more stable solution by a good tradeoff between stability\u00a0and plasticity.\n\nOne major concern about this paper is the empirical\u00a0validations. Some baselines like AGEM are underperforming compared to reported numbers on other papers. The idea of memory editing\u00a0can also be added as an additional loss to other methods too (not only to ER and MIR). Is that true?\n\nRandom edits look quite promising. It may suggest that most of improvement is coming\u00a0from replay (it can be anything) and the regularization effect that it provides to not to overfit to the data.\n\nIsn't figure 4 telling something trivial? Because\u00a0each class has a unique pattern of active inputs and that leads similar activation patterns on\u00a0data and subsequently to a similar gradient profile within the class.\u00a0\u00a0\n\nIn equation 6 when we replace the forgetting measure (d_t) with equation 5 loss function \\ell is going to have a different\u00a0coefficient\u00a0(-\\alpha-\\beta) which basically is a special way of weighing the two loss terms on \\theta and \\theta'. It's good to elaborate on this more.\n\nThe connection of eq. 1 and eq. 2 to te final editing formula (stated in eq. 3) is a little loose. Especially starting from eq. 2 approximation using eq. 3 is not the most natural or organic step. \n\nThe paper is well written and easy to follow.\u00a0\npage 2 --->... one online model...\npage 3 ---> an categorization ...\npage 4 ---> also show that ...\nAlso note that citations are not properly using parentheses.\u00a0", "title": "This paper proposes\u00a0a gradient based method to improve the samples stored\u00a0in the replay buffer of continual learning algorithms by taking derivatives with respect to the forgetting measure. The paper's idea is nice and intuitive\u00a0but the presentation and empirical\u00a0validation\u00a0can be improved.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}