{"paper": {"title": "Theoretical bounds on estimation error for meta-learning", "authors": ["James Lucas", "Mengye Ren", "Irene Raissa KAMENI KAMENI", "Toniann Pitassi", "Richard Zemel"], "authorids": ["~James_Lucas1", "~Mengye_Ren1", "~Irene_Raissa_KAMENI_KAMENI1", "~Toniann_Pitassi1", "~Richard_Zemel1"], "summary": "We prove novel minimax risk lower bounds and upper bounds for meta learners", "abstract": "Machine learning models have traditionally been developed under the assumption that the training and test distributions match exactly. However, recent success in few-shot learning and related problems are encouraging signs that these models can be adapted to more realistic settings where train and test distributions differ. Unfortunately, there is severely limited theoretical support for these algorithms and little is known about the difficulty of these problems. In this work, we provide novel information-theoretic lower-bounds on minimax rates of convergence for algorithms that are trained on data from multiple sources and tested on novel data. Our bounds depend intuitively on the information shared between sources of data, and characterize the difficulty of learning in this setting for arbitrary algorithms. We demonstrate these bounds on a hierarchical Bayesian model of meta-learning, computing both upper and lower bounds on parameter estimation via maximum-a-posteriori inference.", "keywords": ["meta learning", "few-shot", "minimax risk", "lower bounds", "learning theory"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors study the theoretical performance of a meta-learning in two settings. In the first one the overall number of possible tasks is limited and tasks are close in KL-divergence. The second setting is MAP estimation (in a hierarchical Bayesian framework) for a family of linear regression tasks. Lower and upper bounds are provided on minimax parameter estimation error.\nThis paper has spurred a lot of discussion among reviewers and (competent) external commentators. Most of these criticisms were right on target, but the authors managed to convince the reviewers and myself that there was simply an issue of presentation of the main results. I suggest the authors to take into serious considerations all the aspects raised by the reviewers that has generated misinterpretations of the presented results."}, "review": {"fucdd0klKwK": {"type": "review", "replyto": "SZ3wtsXfzQR", "review": "Pros: \n1 - The paper seems to be well written, have a good review of the references and necessaries for understanding the problem. \n2 - Some of the results found in the paper seem to be interesting. For instance, the asymptotic analysis provided in paper gives some insight on the performance of meta learning algorithms with the number shots, ratio of observation noise to the sampling noise and the number of tasks. \n\nCons:\n1 - It seems that most of the results of the paper are based on Loh (2017). It is expected that the author differentiate their contributions  with those of that reference more clearly. \n2 - Some of the notations are misleading. This is a minor issue and up to the authors to change it or not but it may help with the readability of the paper. Some notations like $\\theta$ are usually used for parameters in models. In this paper, $\\theta$ is used as a function. I understand why the authors decided to use it as a function but it may be a bit misleading. \n3 - Some of the assumptions in the paper can be very restrictive. For instance, it is assumed that the distributions are $2\\delta$-separated while close in the KL divergence sense. Isn't this too restrictive? Maybe it is good for the authors to try to talk more about the implications of such assumptions. How does this restrict the space of interested probability distributions? \n4 - Another example of a restrictive assumption is bounded minimum singular values. How does such a restriction affect the space of considered solutions? \n5 - There has been no effort in comparing with any bounds that are available currently. ", "title": "Well written paper with potentially interesting insights on the theory of meta learning but some issues exist", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "cx0hSiwEVhj": {"type": "review", "replyto": "SZ3wtsXfzQR", "review": "The paper studies the information-theoretic lower bounds in the minimax setting of meta-learning. The paper also discusses upper and lower bounds in the hierarchical Bayesian framework of meta linear regression. The novelty of the paper is two-fold: a) it proves a novel meta-learning local packing result to compute the conditional information between training task samples and the novel task data distribution and b) it compares the lower bound of the risk to the risk of posterior estimate in meta linear regression. In addition, the authors verify the dependence of risk on various parameters in 2 different experiments.\n\nMy major concern is that in theorem 3, $Mn$ needs to grow exponentially with $d$ to be significantly better than the case when there are no training tasks available (theorem 2). However, theorem 4 states that $Mn$ just needs to depend polynomially on the dimension $d$. Hence, the upper bound and the lower bound on the risk have a gap on the parameter $d$. Thus, it is not clear if the lower bound in Theorem 1 is tight. It would be great to have a discussion on this and the assumptions that one needs to take to improve upon this gap. \n\nFew other concerns:\n1. In the hierarchical regression setting, the assumptions for theorem 3 and theorem 4 are different. A discussion on how the lower bounds will change, if we assume different variance of task parameters $\\sigma_{\\theta_1}, \\cdots, \\sigma_{\\theta_{M+1}}$ and task-specific observed samples $\\sigma_1, \\cdots, \\sigma_{M+1}$, will be helpful.\n2.  The proof of theorem 3 focuses on packing the parameter space $\\theta$ by an $\\epsilon$-net on $B_2(4\\delta)$, where $\\delta$ is fixed later on. However, the model assumes $\\theta$ coming from ball $B_2(1)$. There seems to be some mismatch in the theorem statement and the proof.   \n3. The risk in theorem 4 involves few more parameters whose effect on the risk hasn't been checked in experiments e.g. the ratio of variance in observed samples of novel task v/s variance in task parameters. Also, it will be interesting to see how the results change when the data distribution is changed since the risk depends on the singular values of the data matrix. \n\nI have read the proofs. They are easy to read and understand. My scores are slightly on the lower side because I am concerned about the tightness of the lower bound in Theorem 1. I am happy to discuss this with the authors and other reviewers during the discussion period.\n\n**After Rebuttal**\nI have read the reviews of other reviewers and the responses of the authors to the questions posed by the reviewers and Ahmad Beirami. I understand that the authors have taken a pessimistic approach to compute the lower bounds of risk in the minimax setup of meta-learning. However, I believe the paper is an important step in this direction. Theorem 3 and Theorem 4 are important additions to the paper, which show a margin between the pessimistic lower bound of the risk and the actual risk with the introduction of structure to the learning setup. Overall,  I enjoyed reading the paper, and I have increased my score after the rebuttal.", "title": "Official Review #1", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "TgjExKH8sp": {"type": "rebuttal", "replyto": "-CsCC9DRNZC", "comment": "Thanks for your response.\n\nYou are correct that as $\\beta$ grows larger the relatedness between the tasks drops. Regarding the example, we designed this example to illustrate the differences between $R_{\\mathcal{P}}^*$ and $R^*$ (as this issue arose in prior discussion) and not necessarily as an application of our lower bounds.\n\nIn terms of the lower bounds, we consider Theorem 3 to serve as a more complete example of how these bounds work under different values of $\\beta$. Intuitively, $\\beta$ acts as a constraint on the meta-training tasks mutual information term in Theorem 1.", "title": "Thank you for the question"}, "QukGU9lfBqn": {"type": "rebuttal", "replyto": "SZ3wtsXfzQR", "comment": "### Summary of changes\n\nIn addition to several improvements to clarity following reviewer feedback, we have made a modification to our minimax definition that requires minimal changes to the presented theoretical results. We now require the novel task to be within $\\beta$ KL-divergence of the meta-training tasks in the definition of $R_{\\mathcal{P}}^*$. This provides a clear differentiation between the iid and meta-learning minimax risks (see example below). The necessary changes to the rest of the paper are summarized below:\n\n- In the statement of Theorem 1, we additionally require the $\\beta$ KL constraint in the packing set. No change to proof needed.\n- No changes needed to corollaries (though we clarified presentation)\n- In Theorem 3, we require an additional condition depending on $\\beta$\n\nIn our bounds, $\\beta$ effectively restricts our choice of packing set to those that satisfy the required KL constraint. This affects the proof of Theorem 3, where we construct such a packing. Some informal intuition about the packing set may be useful: we can visualize it as building a\npacking of a sphere. Decreasing delta reduces the radius of the\nsphere and packs the points more closely. The beta constraint\nmeans that we are only allowed to consider those packings where\nthe sphere has radius at most beta.\n\n### An example where $R^* \\neq R_{\\mathcal{P}}^*$\n\nConsider 4 distributions,\n\n$\\mathcal{P} = \\\\{ Ber(0.2), Ber(0.3), Ber(0.7), Ber(0.8) \\\\},$\n\nwhere we observe 1 sample, $x$, from one of these distributions. In the IID setting, the best we can do is to guess $Ber(0.8)$ if we see heads and $Ber(0.2)$ if we see tails. Looking at the conditional probabilities:\n\n$P(p=0.2 | x = 1) = 0.1$\n\n$P(p=0.3 | x = 1) = 0.15$\n\n$P(p=0.7 | x = 1) = 0.35$\n\n$P(p=0.8 | x = 1) = 0.4$\n\nSo we are wrong 60\\% of the time (the other case is symmetrical).\n\nOn the other hand, in the meta learning setting if we choose $\\beta = 0.1$ with $M = 1$, then the possible configurations of tasks are as follows:\n\nMeta-training $p = 0.2$ or $0.3$; Novel $p = 0.2$ or $0.3$\n\nMeta-training $p = 0.7$ or $0.8$; Novel $p = 0.7$ or $0.8$\n\nFor a total of 8 possible settings. [The KL between the closer distributions is $\\approx 0.026$, and $\\approx 0.339$ for $Ber(0.3)$ and $Ber(0.7)$ ]\n\nIf we observe many samples in the meta-training task (large n), then we know that the novel task can be from only one of two distributions. Hence our error rate with one sample will be at most 50\\% (slightly better if we actually compute it).", "title": "Uploaded latest revision"}, "_0eP86hC_Cm": {"type": "rebuttal", "replyto": "zlkt0FrsGHv", "comment": "Thank you for your comment.\n\nThis issue stems from a misinterpretation of these bounds. This bound has the right kind of behavior, matching the intuition that the minimax risk should decrease with increasing n: seeing more examples of the $M$ tasks can decrease the error on the novel task. The asymptotic behavior, as n goes to infinity is not an issue. The packing set can be constructed with knowledge of the sample size --- therefore both $\\delta$ and $\\alpha$ may depend on $n$, $M$, and $k$. This prevents the bound from becoming negative (as is the case in our proof of Theorem 3). We have clarified this in our latest revision.\n\nIn fact, one of the references provided by Ahmad has a bound of this form too: \"Minimax Lower Bounds for Transfer Learning with Linear and One-hidden Layer Neural Networks\", Kalan et al. Equations 5.2 and 5.3 in their appendix also _appear_ to have negative risk for large $n$. This is not the case.", "title": "The bounds are not useless: the packing set construction can prevent negative bounds"}, "6nLfPVEr2A": {"type": "rebuttal", "replyto": "SZ3wtsXfzQR", "comment": "We have received a high volume of comments on our paper. Some of these are valid points that we are addressing in the current version. However, it should be noted that these involve clarifications, notational issues, further discussion of relevant appendix details in the main paper, and **not** errors in the paper. Following this lengthy discussion, we provide a summary of the broad points here.\n\n### Claim: Section 4 considers only finite $\\mathcal{P}$\n\nThis is not true. We consider uncountable $\\mathcal{P}$, and produce our bounds via finite local packings of the space --- a standard technique discussed in e.g. Loh, 2017.\n\n### Claim: Theorem 3 (and its proof) is invalid\n\nThis is incorrect, and stems from a misunderstanding of the proof. First, it is claimed that $\\delta$ must be a property of the concept class, but our bounds apply with any valid packing set produced for $\\mathcal{P}$. It is standard practice to introduce dependence on the sample size and other problem parameters in these results (see e.g. Raskutti et al. 2011, proof of Theorem 1.b).\n\nSecond, it is claimed that $\\delta \\geq 1$ for large $n$. This is also false ($\\delta$ decreases) and is easily verifiable from the proof. We have made this absolutely clear in the proof.\n\n### Corollary 1, Corollary 2, Theorem 2\n\nAhmad pointed out that fixing all other parameters and taking $n\\rightarrow\\infty$ leads to a negative bound. However, the parameters $\\delta$ and $\\alpha$ can depend on $n$ itself --- see Theorem 3 in our work, or similar approaches in Raskutti et al. 2011 for examples. We will point out that these can depend on sample size and properties of $\\mathcal{P}$ in the main paper.\n\nRegarding Corollary 2, Ahmad stated that this bound is loose when $n=0$. We agree with this assessment, but this is besides the point. As stated in our paper, Corollary 2 is derived via application of the data processing inequality. As such, we expect that it will be tightest when $n \\rightarrow \\infty$ and there is infinite data from each of the meta-training tasks available. We will clarify this further in our paper.\n\nFurther, Ahmad argued that Theorem 2 in our paper is invalid. We pointed out that this can be derived directly from existing results in the literature (which was fully detailed in our appendix at the time of submission). We will summarize this derivation in the main paper.\n\n### Claim: $R^* = R_{\\mathcal{P}}^*$\n\nAhmad claims that the side information has no effect on our proposed minimax risk. We understand that the current notation allows for this interpretation and are working on clarifications to the definition and theorem statement to clarify the differences between the minimax risk with and without side information ($R^*$ and $R_{\\mathcal{P}}^*$). We are confident that these changes may be included without significant modification of our results. We are putting together a concrete example to further clarify this relationship and will include this alongside any necessary modifications to Theorem 1 in the next few days.\n\n### References\n\nLoh, 2017, \"On Lower Bounds for Statistical Learning Theory\"\n\nRafail Z Khas\u2019 minskii, 1979, \"A lower bound on the risks of non-parametric estimates of densities in the uniform metric\"\n\nRaskutti et al., 2011 \"Minimax rates of estimation for high-dimensional linear regression over $\\ell_q -balls$\"", "title": "Overview of current discussions"}, "56r7pxW0eqi": {"type": "rebuttal", "replyto": "cb7OGr4eMqZ", "comment": "> \"Can you please explain how this bound will not become negative for large $n$?\"\n\nIn the two lines of the proof following this equation, we choose a value for $\\delta^2$ which guarantees that this term is positive (in fact: $\\geq 1/4)$).\n\n(In fact, there was a constant error in the proof that we just spotted. This was trivial to fix and is corrected in the latest version.)", "title": "This follows from the next 2 lines"}, "yCE7A3Q6Qce": {"type": "rebuttal", "replyto": "pNGfD2PpYzB", "comment": "First, we sincerely thank you for your interest in our submission and the discussion that you have generated. We intend to introduce several clarifications to our paper as a direct result of these interactions.\n\n> \"Nothing in the Theorem statement hints at the fact that the authors are assuming $2\\delta$-separability\"\n\nWe do not need to assume separability in the Theorem statement. Within the proof we construct a local-packing set with bounded cardinality for the problem space. This has the desired $\\delta$ separation, so that we can invoke Corollary 1.\n\n> \"Even more troubling is that $\\delta \\geq 1$ for large enough $n$ or $k$\"\n\nIn fact, $\\delta$ decreases with $n$, $M$, and $k$. This should be read as, $\\delta^2 = \\frac{d\\sigma^2}{64\\gamma^2(2^{-d}nM + k)}$. This is clear from how we use $\\delta^2$ after it's value is assigned. However, we will improve the equation formatting to make this clearer in the revised version. You do raise a good point that we should include necessary conditions on the problem dimensions for this packing set to be well-defined --- we have included these explicitly now too.\n\nIn general, you seem to have misunderstood this theorem and proof. The first step in our proof is to find a packing set of $\\mathcal{P}_{LR}$ which satisfies $\\delta$-separability and the KL-divergence constraint. To do this, we use a (admittedly confusing but common) scaling trick, that we also discuss in our response to Reviewer #1. This allows us to embed the packing set within the unit ball, with any $\\delta < 1/2$ --- that is, we have built an uncountable set of valid local-packings. We then invoke our lower bound results using these packing sets, and choose a $\\delta$ depending on the problem parameters to retrieve the desired bound.\n\nSeveral related proofs can be found in Raskutti et al., 2011. In particular, our proof follows a similar format to the proof of their Theorem 1(b).\n\nIf you remain interested, we would be very happy to continue our discussion offline once our anonymity is lifted.", "title": "There are several significant misunderstandings here"}, "m2fONlhmeyW": {"type": "rebuttal", "replyto": "h9wkQh7jPDk", "comment": "You are unfortunately correct that the proof of Corollary 1 is not stated anywhere in the paper... We missed this as (fortunately) the proof is a one-liner given the results proved in the appendix. The proof follows directly from Theorem 1, Lemma 3, and Lemma 4. We have, of course, added this to the appendix.\n\nProofs for all other results are given in the appendix (we double checked).\n\nRegarding your other comments, could you please be specific regarding the notation that you found confusing? This would help us to improve clarity.", "title": "An embarrassing omission"}, "cJnrieA-4tO": {"type": "rebuttal", "replyto": "mRDKrdNOX7T", "comment": "Though we replied to you directly, we want to state here too that we will be including several changes in our revised version based on this discussion (and the other thread).\n\nWe plan to include discussion of the references that Ahmad provided that we are missing. We will also clearly described the role of $\\alpha$ and $\\delta$, and generally improve clarity around Theorem 2 and our corollaries in Section 4 (whose details are currently too hidden in the appendix).", "title": "We agree that this is valuable"}, "j7WR11ianE5": {"type": "rebuttal", "replyto": "WdxF8Xhs6_T", "comment": "We should have included in our previous response that the derivation we describe above is shown in full in our appendix, Sections B and B.1.", "title": "A useful pointer: Our Appendix B through B.1"}, "6tULHPIv41r": {"type": "rebuttal", "replyto": "XGURjj-yE2o", "comment": "Thank you for the timely reply.\n\nWe agree that Ahmad has raised several valuable points, and we will be working on these in our revised version (hopefully completed within the next few days).", "title": "We agree, and will be including changes in revision"}, "LYT9PQd1HuU": {"type": "rebuttal", "replyto": "TG2mofWEDt", "comment": "\"$P_{M+1} \\notin \\\\{P_1,\\ldots,P_M\\\\}$ may only help lower the novel task's minimax risk if $\\mathcal{P}$ is a finite set.\"\n\nWe certainly constructed this condition with finite sets in mind. However, we could also include a continuous separation condition (i.e. delta-separation introduced later in Section 4) for uncountable $\\mathcal{P}$ (a condition that could already apply in Theorem 1).\n\n\"can you please characterize the minimax risk with side information in this case and show that it is smaller than that with no side information?\"\n\nIn your toy example, if we know the identity of the first distribution, we know by exclusion the identity of the novel task distribution (as there are only 2). Even with finite $n$, we increase the probability that we correctly identify the novel task distribution with $k=1$.\n\nWe also included a close-in-KL condition that you do not discuss, which does not have the issues you raised with the exclusion condition.\n\nIn any case, we raised these points to highlight to yourself (and other readers), that even in the case that you are correct about our definition in Section 3, our theoretical results can adapt without additional work to cover your suggested changes.\n\n\"BTW, the entire Section 4 is built on the assumption that $\\mathcal{P}$ is a finite set\"\n\nThis is absolutely not true. Our bounds in Section 4 apply to uncountable $\\mathcal{P}$. For example, we apply them to a space of linear regression models in Section 5. The finite sets that we introduce in our results are local-packing sets, a common tool in minimax lower bounds.\n\nNoting this, we maintain that it is not possible in general to provide regularity conditions necessary for \"maxmin = minmax\". One could look to existing minimax literature for an example of such problems, e.g. Raskutti et al. 2011.\n\nWe are glad to have the opportunity to discuss these issues with you, and are thankful that this discussion has already identified several parts of our paper that when examined carefully from the outside could definitely use further clarification. However, we worry that you may be seeking out errors in our paper without a full appreciation of all of its technical content. We are by no means claiming that our paper is without fault. As with any scientific publication under review (and beyond...), mistakes are possible and we are open to identifying and discussing them with you. We ask that we both take a step back and continue this discourse at a steadier pace, after reviewing each others' references and comments.", "title": "Thanks for the response. Some clarifications and problems"}, "WdxF8Xhs6_T": {"type": "rebuttal", "replyto": "A6tlg9-kkn4", "comment": "We apologize if the presentation of Theorem 2 is misleading. As this has raised significant confusion, we will work on improving the presentation in this section to make things clearer.\n\nWe adopted the approach of Khas'minskii, 1979 and collapsed the mutual information under iid samples for these corollaries. This allows us to reason about the sample size within the bound, which was the central object of interest for us in Section 4. This loosening is discussed additionally in Yang & Barron, 1999.\n\nLooking to Loh 2017, our bound can be recovered by substituting the results of Loh's Lemma 2 into their Theorem 1, and using the above loosening. The bound that you provided does not include this local-packing, which we clearly state is utilized in our work. Your stated bound is more comparable to our Theorem 1, which is written in terms of mutual information alone.\n\nWe could keep the bounds in the corollaries in terms of KL-divergence (as in Loh), which are more familiar, but we wanted to be explicit in sample size to facilitate discussion in Section 3. This is not necessary, but we felt provided valuable insight into the comparison of the meta-learning minimax bounds with their iid counterparts.\n\nIn our minimax bounds, $\\delta$ and $\\alpha$ need to be set according to the problem space and loss function of interest. See Raskutti et al, 2011, Section III, A.3 for some related discussion. We explore only one technique (local-packing), but Theorem 1 is general and other techniques could be applied. We will make these dependencies clear in a revised version.\n\n> \"can you please refute that \"the bounds in Corollary 1 becomes negative for large enough $n$?\"\n\nSee Theorem 3, for example, which uses Corollary 1 to derive a bound which does not become negative for large $n$.\n\n> \"Can you please clarify in what sense it is a tighter bound then?\"\n\nIn the setting where we know that the environment is only partially observed $M + 1< J$, we do not expect the error to shrink to zero as $n$ grows. In this sense, Theorem 1 is only a loose lower-bound. Corollary~2 formalizes this intuition, and provides a lower-bound that does not shrink to zero with $n \\rightarrow \\infty$. This is what we mean by a tighter bound. We agree that this can be clarified in the paper.\n\n> Finally, can you please chime in on the asymptotic scaling of these bounds with different parameters?\n\nThis is difficult to summarize in general, particularly in this forum. We discuss this in our paper. Bounding the mutual information is highly problem-specific and naturally different asymptotic behaviours arise.\n\n> Can you at least evaluate these results on the toy counter-example from the other thread and show that they are meaningful, and scale correctly?\n\nThere are a lot spinning plates in this rapidly growing discussion. To get this response to you in a timely manner, we will not provide an analysis of this toy problem at the moment. In any case, we do not see how this would be generally convincing or otherwise necessary.\n\nReferences\n\nLoh, 2017, \"On Lower Bounds for Statistical Learning Theory\"\n\nRafail Z Khas\u2019minskii, 1979, \"A LOWER BOUND ON THE RISKS OF NON-PARAMETRIC ESTIMATES OF DENSITIES IN THE UNIFORM METRIC\"\n\nYang & Barron, 1999, \"Information-theoretic determination of minimax rates of convergence\"\n\nRaskutti et al., 2011 \"Minimax rates of estimation for high-dimensional linear regression over $\\ell_q -balls$\"", "title": "With a few very small steps, Theorem 1 (Loh 2017) => Theorem 2 (ours)"}, "7kvGigRxR1E": {"type": "rebuttal", "replyto": "oDGTbYa2kkM", "comment": "### Gap between 5 and 5.1\n\nWe acknowledge that this is a potential source of confusion in the current write-up, and will clarify it in the next version.\n\nHere we present an alternative view-point. We have a meta-learner in Section 5 defined by Eq 28-30, that utilizes data from the training tasks and novel task. This meta-learner is derived by taking an empirical Bayes estimate in the hierarchical Gaussian model. The bounds we derive apply in the minimax setting that we study in Sections 3 and 4, even if the model is mis-specified for the actual data distribution seen. Natural extensions of our results may introduce high-probability arguments as you suggested, to bridge the gap.\n\n### No strong dependency on the norm of $\\theta$\n\nWe assume that $\\theta$ is bounded in a 1-norm ball. The norm of $\\theta$ plays a role in the computation of the KL-divergence bounds in the lower-bounds, and in the bias computation in the upper-bounds. But due to the unit-factor this isn't visible in the presented results themselves.", "title": "Thank you for your feedback!"}, "HTWSdkWVcTX": {"type": "rebuttal", "replyto": "cx0hSiwEVhj", "comment": "### Tightness of Theorem 3 vs Theorem 4\n\nThere is indeed a gap in the derived lower and upper bounds. We discuss this gap in Section 5.3 but will provide some additional discussion. We expect that our upper-bounds are asymptotically tight, and that the looseness is due to the relatedness assumptions used in Theorem 1, which are likely too weak for meta-learning in the hierarchical Gaussian model. We also discuss the inverse exponential scaling w.r.t dimension $d$ at the end of Section 5.1. We aim to (partially) address this gap through Corollary 2, which provides tighter lower bounds in the setting where the environment is only partially observed.\n\n### Different variance assumptions\n\nTheorem 4 has a more general variance assumption (allowing different variances for the training tasks and novel task). We could introduce more general variance assumptions in Theorem~3, and need only derive a new upper-bound on the pairwise KL-divergences.\n\n### Smaller bounding ball in proof\n\nThis is a standard technique that is typically hidden in the proof of these minimax results (see e.g. Raskutti et al. 2011). While this step looks to be suboptimal, existing minimax results using this approach have provably optimal rates. Introducing a larger packing set makes the derivation much less clean; however, we are exploring this approach to see if this improves the bounds.\n\n### Parameters in empirical evaluation\n\nThank you for the suggestion. The settings we explored probe at the prominent model parameters that appear in our bound. However, we did not explore varying the data distribution. We will explore this and hope to include these results before the end of the rebuttal period.", "title": "Thank you for your feedback!"}, "ZhR0r52gXan": {"type": "rebuttal", "replyto": "AW3G8IwWU9U", "comment": "### Local-packing examples\n\nOur paper includes such a set of tasks in the space of linear regression models that we consider (Section 5.1).\n\nIn general, it is difficult to construct realistic spaces of distributions that satisfy these constraints. Fortunately, by design, these assumptions align with those used in the standard minimax framework. Therefore, we may use our results to study meta-learners in spaces of distributions studied in those works.\n\nFor example, Raskutti et al. provide several compelling examples, focused on $\\ell_q$ high-dimensional linear regression. Yang \\& Barron analyze local-packing for minimax rates, and provide several examples including data compression and non-parametric regression.\n\n### Parameter estimation vs. accuracy\n\nWe follow the typical statistical minimax risk framework where parameter estimation error is the focus. You bring up a good point, that these parametric estimation bounds do not speak to prediction performance in general. However, there are standard methods to extend the symmetric parameter loss to objectives like cross entropy. The clearest example we\u2019re aware of is given in Duchi\u2019s 2016 lecture notes, in Question 7.5. In short, the $\\delta$-packing can be applied in loss space directly to allow more general forms of risk.\n\n### Discrepancy between Section 4 and Section 5\n\nOur bounds in Section 5 are valid bounds on the minimax risk object that we introduce in Section 3. We agree that there are some discrepancies here that can be clarified significantly (see response to R2, \"Gap between 5 and 5.1\"). We are not certain what particular assumptions you are referring to, and so would appreciate clarification if necessary.\n\n#### Minor comments\n\n1. Thank you, we will make these clear in the paper.\n2. Yes, these have the same meaning. We will make these consistent. $M$ tasks are visible to the learner, but the proof requires $J$ distributions in the local-packing. In the setting $M < J-1$, we assume that the learner only sees tasks from some incomplete subset of the full packing.\n3. This is a ball of radius 1 (measured in 2-norm), centered at the origin.", "title": "Thank you for your feedback!"}, "4Anxh7oGOBS": {"type": "rebuttal", "replyto": "fucdd0klKwK", "comment": "### Contribution relative to Loh 2017\nWe certainly utilize some of the techniques presented in Loh 2017, and elsewhere in the minimax risk literature. We extend these results on several fronts: addressing generalization to novel tasks, multiple sources of training data, and novel local-packing bounds under product and mixture sampling procedures. We will make these contributions clearer in our paper.\n\n### $\\theta$ as a functional\n\nWe follow the notation introduced by Loh, 2017 and others within the minimax risk literature. We acknowledge that this notation does not agree with the standard notation used in machine learning and related fields. But it does have the advantage of being succinct.\n\nWe introduce short-hand notation ($\\theta_P = \\theta(P)$), which aligns with the standard ML notation.\n\n### Local-packing is a strong constraint\nThis is indeed a strong constraint, but is a typical one in the minimax risk setting. Further, these constraints are natural and relevant in FSL. For example, metric learning approaches to FSL such as Prototypical Networks learn an embedding space in which classes of images must be discriminable (meaning separated in the embedding space), but share features, so that novel class data are embedded within the bounds of the space.\n\nNotably, in this paper we do not introduce any additional constraints relative to the standard minimax setting. Therefore, existing packing sets can be utilized in our setting. Additionally, please see our response to R3 (\"Local-packing examples\").\n\n### Minimum singular values assumption\n\nThis assumption is potentially strong, but can be verified for large random matrices (Raskutti et. al, 2011). Note that our upper bound depends explicitly on the singular values lower-bound, and so the effect of weakening this assumption on our results can be measured.\n\n### No comparison to existing bounds\n\nThis is true, though we argue that our bounds are not immediately comparable to existing ones as, to our knowledge, these are the first minimax bounds in this meta-learning setting. We can directly compare our bounds to existing IID bounds in the minimax literature (and do so within our work). If you have any particular bounds in mind, we would be happy to consider how they compare to our results.", "title": "Thank you for your feedback!"}, "WiWZHnxykOM": {"type": "rebuttal", "replyto": "qMpyO_E9FJj", "comment": "We apologize for the delay in our reply.\n\nWe acknowledge that our comparison to the MLE estimator is misleading and does not obviously provide the guarantee that we asserted --- we apologize for this.\n\nIt seems difficult for us to reach consensus on this. If we understand correctly, you are not claiming that any of our proofs are incorrect --- only that you disagree with the validity of the minimax risk formulation in the meta-learning setting.\n\n### Addressing definition\n\nWe disagree with your claim that our definition is meaningless in this setting. However, we would like to point out that several of your suggestions can be incorporated directly into our definition without violating any of our theoretical results.\n\nFirst, we could modify the supremum to require $P_{M+1} \\notin \\\\{P_1,\\ldots,P_M\\\\}$. This requires no changes to any of our proofs. Alternatively, we could require directly that $P_{M+1}$ is always chosen to be within $\\alpha$ KL-divergence of $P_1,\\ldots,P_M$ in the supremum. We would need to add this KL constraint to Theorem 1 (making it slightly less general) but this is not problematic as we introduce this constraint for all later results anyway.\n\nSecond, we can freely modify the infimum to search over a restricted space of $\\hat{\\theta}$. We make no assumptions about the form of the estimator in our proof of Theorem 1. (Though this approach is likely to introduce looseness in the bounds).\n\n### Addressing original claim\n\nLooking back to your original proof, there are a couple points that we find problematic and would appreciate your thoughts on.\n\n> \"Now, $R$ is constant with respect to $P_1,\\ldots,P_M$ ($\\hat{\\theta}$ would not depend on $P_1,\\ldots,P_M$ which are independent of $P_{M+1}$\"\n\nThis point is not obvious to us. The estimator may certainly depend on $P_1,\\ldots,P_M$, as it is a two-stage learner that first processes data from these tasks. Could you clarify this?\n\n> \"If regularity conditions hold for the minimax risk to satisfy a minimax theorem\"\n\nWe have no reason to expect that such regularity conditions hold. Indeed, as you have pointed out, the RHS is equal to the standard minimax risk for which we do not expect to have a strong minimax property like this.", "title": "We acknowledge issues with our example. Looking back to definition..."}, "jrYmh0wt99B": {"type": "rebuttal", "replyto": "g_YCihBxVtN", "comment": "Hi Ahmad,\n\nWe appreciate your continued interest in our work!\n\nWe would first point out that Theorem 2 is not a novel result in this work, and is a standard bound from existing minimax literature (as cited in our work).\n\nIn terms of the purported divergence of these bounds, along the lines of Reviewer 2's comment, note that $\\alpha$ is also typically chosen to depend on $n$, $\\delta$, and other problem-dependent parameters. See our proof of Theorem 3, for example.\n\nFurther, our Corollary 2 loses dependence on $n$ via the data-processing inequality. Therefore, we would expect these bounds to be tightest when $n \\rightarrow \\infty$, not $n=0$.\n\nThank you for your references too, in particular we were (somehow) unaware of Kalan et al. 2012, which certainly looks related. We will read this paper more deeply, but note that their minimax risk does not consider a novel-task generalization but rather average performance over a set of tasks.\n\n", "title": "Tightness of these results"}, "Ge0aF32mrCD": {"type": "rebuttal", "replyto": "Od5XbNyGPA", "comment": "> \"This definition mathematically does not support any structure (task-relatedness constraint) among the meta-tasks that you are interested in.\"\n\nThis definition does not impose any structure, that comes later in Section 4. But importantly, this definition does not prevent structure from being introduced.\n\nWe disagree with your evaluation of our counter-example. We agree that all tasks are members of our three-distribution set $\\mathcal{P}$, but this does not imply that $R^* = R^*_{\\mathcal{P}}$.\n\nWe'll expand on this briefly. We take our meta-learner as $\\hat{\\theta} = \\frac{1}{Mn + k} (\\sum_{z \\in S_{1:M}} z + \\sum_{z' \\in S_{M+1}} z')$. Clearly this estimator will have a lower error-rate than $\\frac{1}{k}\\sum_{z' \\in S_{M+1}} z'$ (remember that $k$ is small). In fact, as $n \\rightarrow \\infty$, the error will be $O(\\epsilon)$ for the meta-learner (upper bounding $R_{\\mathcal{P}}^*$) and something like $O(1/k)$ for the iid learner. Clearly, $R_{\\mathcal{P}}^* < R^*$.\n\n> \"You may have meant for $\\mathcal{P}$ to be the set of all Bernoulli sources with parameter $p \\in (0,1)$\"\n\nNo, we explicitly constructed $\\mathcal{P}$ to contain only a small number of highly-related distributions. This is permitted in our framework of Section 3.\n\n> \"To remedy this, you could define your minimax risk by taking the sup over a set like $\\mathcal{Q}^{M+1}_{\\epsilon} = \\\\{(P_1, \\ldots, P_{M+1}) \\in \\mathcal{P}^{M+1}\\vert D(P_i||P_j) \\leq \\epsilon\\\\}$\"\n\nWe do use a constraint in Section 4 which is, in essence, similar to this. In any case, Section 3 permits us to use $\\mathcal{Q}^{M+1}_{\\epsilon}$ in place of $\\mathcal{P}^{M+1}$ --- this is not a problem. We feel that you are assuming that $\\mathcal{P}$ must necessarily be the space of all possible distributions: this is what we meant by \"worst case with respect to $\\mathcal{P}$\". But this is **not** the case. Our definition makes no strong assumptions about $\\mathcal{P}$, and certainly supports spaces of distributions with strong relatedness structures.", "title": "Revisiting our counter-example"}, "sYLbdLMwUZ5": {"type": "rebuttal", "replyto": "qhGYrNeoHo", "comment": "Hi Ahmad,\n\nThank you for your detailed comments regarding our technical contributions. We\u2019re pleased to have the opportunity to discuss this further. We will first provide a high-level discussion, explaining how our contributions are relevant in light of your comment.\n\nAt a high level we agree with one direction of your argument, but it does not capture the full scope of Theorem 1, and our other results. The main contribution of our theorems is to specify how the sample complexity/error varies as a function of the relatedness of the tasks/distributions in ${\\mathcal P}$.\n\n> \"The authors define a notion of minimax risk with side information in (2), which is denoted by $R_{\\mathcal{P}}^*$. Unfortunately, it appears that , $R^* = R_{\\mathcal{P}}^*$,  i.e., with the way the authors define risk, side information does not help in the worst-case, which also intuitively makes sense because in the worst case data points from other tasks can be arbitrarily unrelated to the task at hand! \"\n\n\nWe agree with this statement partially, assuming that what you mean by worst-case is wrt the choice of ${\\mathcal P}$. As your example shows, when the tasks in ${\\mathcal P}$ are unrelated to one another, in the meta-learning setup (where $n$ samples of each of $M$ tasks $P_1,\\ldots,P_M$ are given followed by $k$ samples from the $M+1$st=target task), the risk $R_{\\mathcal{P}}^*$ will be the same as the baseline/iid risk, $R^*$ (where we just see $k$ samples from the target task).\n\n> \"Hence, all of the lower bounds that are developed by authors on $R_{\\mathcal{P}}^*$ are also lower bounds on the original minimax risk.\"\n\nOur lower bound extends the original/iid minimax risk. It is true that in the case where the tasks in $\\mathcal{P}$ are unrelated our lower bound matches the iid case, as expected. However, our formulation also covers cases where the tasks are related to one another. An extreme example is the counter-point of the above example, where all distributions in ${\\mathcal P}$ are very related: the meta-learning risk $R_{\\mathcal{P}}^*$ will clearly be much lower than the iid risk $R^*$. Both the upper bounds and the lower bounds show this tradeoff. In such cases our lower bound differs from the iid situation. The mutual information term in the bound in Theorem 1 exactly captures the relationship between the tasks.\n\nYou suggest one way to resolve this: \"In min-max world: Enforce some relatedness between $P_{M+1}$ and $P_1,\\ldots,P_M$ via an additional constraint, perhaps by considering them to be close in some distance/divergence metric when the authors  define the inf.\"\n\nThis is exactly what we do in Corollary 1, where we constrain pairs of tasks in $\\mathcal{P}$ to have a KL-divergence less than $\\alpha$. This implies a high MI, which is why the risk decreases. In this case we clearly see the benefit of these $Mn$ samples from the $P_1,\\ldots,P_M$ tasks in the first stage of meta-learning, as the worst-case risk is lowered by a term that depends on $Mn$.\n\nTo conclude, consider an explicit counter-example where $R_{\\mathcal{P}}^* \\neq R^*$. Let $\\mathcal{P} = \\\\{P_1, P_2, P_3\\\\}$, with $P_i = \\textrm{Ber}(p_i)$, such that $|p_i - p_j| < \\\\epsilon \\ll 1$ for all $i \\neq j$. Let $M=2$, $n$ be large, and $k$ small. Now consider the naive meta-learner that treats all data as iid and computes the MLE. This estimator will certainly outperform the minimax-optimal estimator in the iid setting that uses only $k$ samples. This case where the distributions are strongly related serves as a simple counter-example to your proof.\n\nWe will revise the paper to make this main point clearer: the meta-learning lower bound **is** weaker than the iid LB in some cases -- when the tasks in ${\\mathcal P}$ are related (as quantified by mutual information/KL divergence). This is natural, and will match the upper bound.\n\nP.S. We are actively working on responses to our official reviewers and are updating our paper following their feedback. Thank you all for your patience!", "title": "Side information matters with assumptions on the space of distributions"}, "oDGTbYa2kkM": {"type": "review", "replyto": "SZ3wtsXfzQR", "review": "This paper provides a minimax novel-task risk lower bound for meta learning via information-theoretical techniques, showing the fundamental limits of meta learning. The novel-task minimax risk depends on the number of samples from the meta-training set and novel task, as well as the task similarity. The authors further investigate the meta learning problem on a hierarchical Bayesian model, discuss the lower bound and upper bound with maximum-a-posterior estimator.\n\nOverall I feel this paper study an important problem of the meta-learning and the derivation all looks correct. The main drawback is that the presented minimax lower bound is quite pessimistic that may not fully exploit the potential task similarity in practice, but I feel it is still acceptable to first study the minimax lower bound, and leave the case with more structure as future work.\n\nI would like to ask if there is a gap between the description at the beginning of Section 5 and the analysis starting from Section 5.1, as the bayesian model described at the beginning of Section 5 does not assume theta have ell_2 norm smaller or equal than 1, and in fact, cannot be universally hold as theta is sampled from a d dimensional Gaussian (can have high probability arguments instead). However, the analysis throughout Section 5 all have this constraint. I don\u2019t see strong dependency on the norm of theta (e.g. in the covering number, KL bound etc) but I hope the authors check and clarify it in the next version.\n\nOne small tip: lower bound is better represented via big-Omega notation.", "title": "A good paper that investigate the fundamental limits of meta-learning in the sense of minimax novel-task risk", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "AW3G8IwWU9U": {"type": "review", "replyto": "SZ3wtsXfzQR", "review": "The authors study the performance of a meta-learner theoretically in two settings. In the first one the overall number of possible tasks is limited and tasks are close in KL-divergence. The second setting is MAP estimation for a family of linear regression tasks. Lower and upper bounds are provided on minimax parameter estimation error.\n\nThe bounds solidify common sense knowledge about the role of the number of training tasks, number of samples, and task relatedness and diversity on the performance of meta-learner and are supported by numerical examples. I am in favor of accepting this paper. I'll provide suggestions for improving presentation.\n\n1. The assumption in the first setting is that the tasks are close in distribution space but far away in parameter space. Could you mention some examples of such set of tasks? It does not seem like a weak assumption to me, as mentioned in the paper, but then this is the nature of lower bounds. The point is not to model a typical scenario but to design hard cases that lead to high error. But in any way an example is necessary to show that this situation is possible.\n\n2. It seems to me that the lower bound in the first setting has less to do with the performance of the meta-learner than suitability of parameter estimation error as a measure of performance. At the end of the day, it is accurate predictions, and not accurate parameters, that matter. The setting is designed so that tasks have similar distributions but quite different optimal parameters, which results in a high parameter estimation error for the meta-learner. It could be that the meta-learner in fact makes quite accurate predictions (since the tasks are actually close in distribution space) despite its high parameter error.\n\n3. The first setting is worded generally (novel task generalization) while the second setting is presented as a more specific scenario (hierarchical Bayesian linear regression). The first time I was reading the paper I assumed that the first part describes general results on meta-learning and the second part shows a particular instance of the previous section, which was confusing as the second setting does not satisfy the assumptions in the first setting. I suggest presenting these two sections as two particular and different settings as they are.\n\nMinor comments:\n\n1. The statement of Theorem 1 has undefined notation. I assume I(.;.) is mutual information but it is not defined. The text defines W|\\pi and Z|\\pi but the equation uses \\pi_{M+1} and W and Z separately. What do \\pi_{M+1}, W, and Z (without the vertical bar) mean?\n\n2. The paragraph above Corollary 1 is hard to understand. Do \"training task\" and \"meta-training task\" have the same meaning? Also, if the number of previous tasks is M \\leq J-1, how can there be J previous tasks that are close?\n\n3. What is B_2(1) in 5.1?", "title": "Good results; Suggestions for improving presentation", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}