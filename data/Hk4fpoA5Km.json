{"paper": {"title": "Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning", "authors": ["Ilya Kostrikov", "Kumar Krishna Agrawal", "Debidatta Dwibedi", "Sergey Levine", "Jonathan Tompson"], "authorids": ["kostrikov@cs.nyu.edu", "kumarkagrawal@gmail.com", "debidatta@google.com", "slevine@google.com", "tompson@google.com"], "summary": "We address sample inefficiency and reward bias in adversarial imitation learning algorithms such as GAIL and AIRL.", "abstract": "We identify two issues with the family of algorithms based on the Adversarial Imitation Learning framework. The first problem is implicit bias present in the reward functions used in these algorithms. While these biases might work well for some environments, they can also lead to sub-optimal behavior in others. Secondly, even though these algorithms can learn from few expert demonstrations, they require a prohibitively large number of interactions with the environment in order to imitate the expert for many real-world applications. In order to address these issues, we propose a new algorithm called Discriminator-Actor-Critic that uses off-policy Reinforcement Learning to reduce policy-environment interaction sample complexity by an average factor of 10. Furthermore, since our reward function is designed to be unbiased, we can apply our algorithm to many problems without making any task-specific adjustments. ", "keywords": ["deep learning", "reinforcement learning", "imitation learning", "adversarial learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This work highlights the problem of biased rewards present in common adversarial imitation learning implementations, and proposes adding absorbing states to to fix the issue. This is combined with an off-policy training algorithm, yielding significantly improved sample efficiency, whose benefits are convincingly shown empirically. The paper is well written and clearly presents the contributions. Questions were satisfactorily answered during discussion, and resulted in an improved submission, a paper that all reviewers now agree is worth presenting at ICLR.\n"}, "review": {"B1luFSOHI4": {"type": "rebuttal", "replyto": "HyloBdL18N", "comment": "Thanks for your feedback regarding our paper! We extremely appreciate your efforts.\n\nWe have open sourced the original implementation of our paper:\nhttps://github.com/google-research/google-research/tree/master/dac", "title": "Our implementation"}, "S1gpuXlih7": {"type": "review", "replyto": "Hk4fpoA5Km", "review": "The paper suggests to use TD3 to compute an off-policy update instead of the TRPO/PPO updates in GAIL/AIRL in order to increase sample efficiency.\nThe paper further discusses the problem of implicit step penalties and survival bias caused by absorbing states, when using the upper-bounded/lower-bounded reward functions log(D) and -(1-log(D)) respectively. To tackle these problem, the paper proposes to explicit add a unique absorbing state at the end of each trajectory, such that its rewards can be learned as well.\n\nPro:\nThe paper is well written and clearly presented. \n\nUsing a more sample efficient RL method for the policy update is sensible and turned out effective in the experiments.\n\nProperly handling simulator resets in MDPs is a well known problem in reinforcement learning that I think is insufficiently discussed in the context of IRL.\n\n\nCons:\nThe contributions seem rather small.\na) Replacing the policy update is trivial, since the rl methods are used as black-box modules for the discussed AIL methods. \n\nb) Using importance weighting to reuse old trajectories for the discriminator update hardly counts as a contribution either--especially when the importance weights are simply omitted in practice. I also think that the reported problems due to the high variance have not been sufficiently investigated. There should be a better solution than just pretending that the replay buffer corresponds to roll-outs of the current policy. Would it maybe help to use self-normalized importance weights? The paper does also not analyze how such assumption/approximation affects the theoretical guarantees.\n\nc) The problem with absorbing states is in my opinion the most interesting contribution of the paper. However, the discussion is rather shallow and I do not think that the illustrative example is very convincing. Section 4.1.1. argues that for the given policy roll-out, the discriminator reward puts more reward on the policy trajectory than the expert trajectory. However, it is neither surprising nor problematic that the discriminator reward does not produce the desired behavior during learning. By assigning more cumulative reward for s2_a1->s1 than for s2_a2->g, the policy would (after a few more updates) choose the latter action much less frequently than with probability 0.5 and the corresponding reward would grow towards infinity until at some point Q(s2,a2) > Q(s2,a1)--when the policy would match the expert exactly. The illustrative example also uses more policy-labeled transitions than agent-labeled ones for learning the classifier, which may also be problematic. The paper further argues that a strictly positive reward function always rewards a policy for avoiding absorbing states, which I think is not true in general. A strictly positive reward function can still produce arbitrary large reward for any action that reaches an absorbing state. Hence, the immediate reward for choosing such action can be made larger than the discounted future reward when not ending the episode (for any gamma < 1). Even for state-only reward functions the problem does not persist when reseting the environment after reaching the absorbing state such that the training trajectories contain states that are only reached if the simulator gets reset. Hence, I am not convinced that adding a special absorbing state to the trajectory is necessary if the simulation reset is correctly implemented. This may be different for resets due to time limits that can not be predicted by the last state-action tuple. However, issues relating to time limits are not addressed in the paper. I also think that it is strange that the direct way of computing the return for the terminal state is much less stable than recursively computing it and think that the paper should include a convincing explanation.\n\n---------------\nUpdate 21.11.2018\n\nI think my initial assessment was too positive. During the rebuttal, I noticed that the discussion of reward bias was not only shallow but also wrong in some aspects and very misleading, because problems arising from hacky implementations of some RL toolboxes were discussed as theoretical shortcoming of AIL algorithms. Hence, I think the initial submission should be clearly rejected. However, the authors submitted a revised version that presents the root of the observed problem much more accurately. I think that the revised version is substantially better than the original submission. However, I think that my initial rating is still valid (better: became valid), because the main issues that I raised for the initial submission still apply to the current revision, namely:\n- The technical contributions are minor.\n- The theoretical discussion (in particular regarding absorbing states) is quite shallow.\n\nThe merits of the paper are:\n- Good results due to off-policy learning\n- Raising awareness and providing a fix for a common pitfall \n\nI think that the problems arising from incorrectly treated absorbing states needs to be discussed more profoundly. \nSome suggestions: \n\nSection 3.1\n\"As we discuss in detail in Section 4.2 [...]\"\nI think this should refer to section 4.1. Also the discussion should in section 4.1 should be a bit more detailed. How do common implementations implicitly assign zero rewards? Which implementations are affected? Which papers published inferior results due to this bug? I think it is also important to note, that absorbing states are hidden from the algorithm and that the reward function is thus only applied to non-absorbing states.\n\n\"We will demonstrate empirically in Section 4.1 [...]\"\nThe demonstration is currently missing. I think it would be nice to illustrate the problem on a simple example. The original example might actually work, as shown by the code example of the rebuttal, however the explanation was not convincing. Maybe it would be easier to argue with a simpler algorithm (e.g MaxEnt-IRL, potentially projecting the rewards to positive values)?\n\nSection 3.1 seems to focus too much on resets that are caused by time limits. Such resets are inherently different from terminal states such as falling down in locomotion tasks, because they can not be modelled with the given MDP formulation unless time is considered part of the state. Indeed, I think that for infinite horizon MDPs without time-awareness, time limits can not be modelled using absorbing states (I think the RL book misses to mention that time needs to be part of the state such that the policy remains Markovian, which is a bit misleading). Instead those resets are often handled by returning an estimate of the future return (bootstrapping). This treatment of time limits is already part of the TD3 implementation and as far as I understood not the focus of the paper. Instead section 3.1. should focus on resets caused by task failure/completion, which can actually be modelled with absorbing states, because the agent will always transition to the absorbing state when a terminal state is reached which is in line with Markovian dynamics.\n\nSection 4.2 should also add a few more details. Did I understand correctly, that when computing the return R_T the sum is indeed finite and stopped after a fixed horizon? If yes, this should be reflected in the equation, and the horizon should be mentioned in the paper. The paper should also better explain how  the proposed fix enables the algorithm to learn the reward of the absorbing state. For example, section 4.2. does not even mention that the state s_a was added as part of the solution. \n\n\n-------------\nUpdate 22.11.2018\nBy highlighting the difference between termination due to time-limits and termination due to task completion, and by better describing how the proposed fix addresses the problem of reward bias that is present in common AIL implementations, the newest revision further improves the submission. \nI think that the submission can get accepted and I adapted my rating accordingly.\n\nMinor:\nConclusion should also squeeze in somehow that the reward biases are caused by the implementations.\nTypo in 4.2: \"Thus, when sample[sic] from the replay buffer AIL algorithms will be able to see absorbing states there[sic]\nwere previous hidden, [...]\"\n", "title": "Sound and effective approach with little novelty, insufficient analysis of reward bias", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1gnoccmC7": {"type": "rebuttal", "replyto": "S1gpuXlih7", "comment": "We again would like to emphasize that we appreciate your patience and valuable feedback that helps us to improve our submission.\n\nWe have updated the paper to try to address your suggestions. In particular:\n\n1) As per your suggestion, we extended the last paragraph of Section 3.1 in order to clarify our discussion on episode termination because of time limits. We believe that it adds clarity to the paper because it discusses  termination  states in more detail. It also  explains the difference between absorbing states and rollout breaks. For a detailed discussion on implementation specific biases in algorithms (GAIL/AIRL) please refer section 4.1. \n\n2) In Section 4.1, we enumerate  papers affected by this problem, with specific instances. For each paper that we cite in this section, we consider the official implementations provided by the authors. In the same section, we further elaborate on how exactly these algorithms  are affected by the issue.\n\n3) In section 4.2, we assume infinite horizon for R_T since the series converges ( assuming reward bounded by r_max, the series is bounded by gamma/(1-gamma) r_max and thus can be computed either analytically or will converge in the limit using TD updates, section 3.1 now also includes a clarification of this point). We also extended Section 4.2 to clarify how absorbing states can be used by the AIL algorithms and how the corresponding transitions affect estimations of returns. Please see the second paragraph of Section 4.2.\n\n4) Regarding revisiting the illustrative example, we agree that the same reasoning might apply to Inverse RL algorithms in general and we appreciate your suggestion regarding the analysis of this simple example for MaxEnt-IRL. We unfortunately will not be able to add such an experiment before the end of the revision period, but we have added some discussion in Section 4 that the basic principle applies also to other IRL methods. This can be considered as an interesting direction of future work. e will attempt to add a better illustrative example in the final version (we just have not had time to do so), and will make sure to update the reviewers about it.\n\n5) We fixed the wrongly referenced section. Thanks for catching this. \n\nWe hope that this revision of our submission will address your concerns.\n", "title": "Response to the update from AnonReviewer2"}, "SJxux2BMCm": {"type": "rebuttal", "replyto": "H1gEMY4eAQ", "comment": "Thank you for your detailed and encouraging response. \n\nWe have updated the paper to try to address your suggestions. We hope that this revised version more appropriately positions the contribution and draws a clear distinction between MDP formulation and algorithm, as per your suggestion. In particular:\n\n1. We now make it clear that the correct handling of absorbing states is something that should be applied to any inverse reinforcement learning or reward learning algorithm, whether adversarial or otherwise, and is independent of the DAC algorithm in that sense.\n\n2. We have added the suggested citation and other papers that discuss time limits (Pardo et al: https://arxiv.org/abs/1712.00378, Tucker et al: https://arxiv.org/abs/1802.10031 ) in the related work section.\n\n3. In Section 3, we've added a discussion of time limits in MDPs, as well as a discussion of how temporal difference methods can handle infinite-horizon tasks with finite-horizon rollouts (which is what DAC does also). Please see the last paragraph of the section.\n\n4. As per your suggestion, we have removed the illustrative example in Section 4.1.1. While we do believe that an example would help illustrate the issue to the reader, we understand your reservations against the illustrative example. We would like to attempt to add a better illustrative example in the final version(we just have not had time to do so), but we will be sure to make an additional post about it if we do, to confirm that it is satisfactory.\n\n5. For the sake of clarity, we removed the last paragraph from section 4.2 that discusses our choice of the implementation of bootstrapping for the terminal states.\n\nWe appreciate your patience, and would appreciate it if you took another look at the paper and let us know if this has addressed your concerns.\n", "title": "Additional response to AnonReviewer2"}, "SJeQPruhpX": {"type": "rebuttal", "replyto": "B1gGZfAq6Q", "comment": "Thank you for your detailed response. We generally agree with the technical side of your description: MDPs with absorbing states require the absorbing states to be handled properly for IRL. This is in essence the point of this portion of our paper. We also agree that addressing this is not so much a new algorithm as it is a fix to the MDP. We have edited the paper to reflect this and clarify this point, please see the difference between the last revision and original submission (the abstract, sections 3.1 and 4). The fact that we test our solution by extending two different prior methods (GAIL and AIRL) reflects the generality of the solution.\n\nHowever, we respectfully disagree that this solution is obvious or trivial. Environments with absorbing states in the MuJoCo locomotion benchmark tasks have been used as benchmarks for imitation learning and IRL in one form or another for over two years. In this time, no one has corrected this issue, or even noted that this issue exists, and numerous works incorrectly treat absorbing states, resulting in results that are not an accurate reflection of the performance of these algorithms, as detailed in Section 5.2 and Figures 5,6 and 7 of our paper. This issue is severe, it is making it difficult to evaluate IRL and imitation algorithms, and as far as we can tell, most of the community is unaware of it. We believe that our paper will raise awareness of this issue and facilitate the development and evaluation of better IRL algorithms in the future. With your help, we have clarified this point further in our current paper. The purpose of a research paper is to communicate an idea that is relevant and important to a large subset of the community, and we believe that our paper does this.\n", "title": "Additional response to AnonReviewer2"}, "rkeAEjMcT7": {"type": "rebuttal", "replyto": "S1gzKHFunm", "comment": "We thank the reviewer for the positive and constructive feedback.\n\nWe have extended the section 5.1 of the manuscript as suggested by the reviewer.\n\nBelow are detailed answers for the reviewer\u2019s concerns: \n\n1) To simplify the exposition we omitted the entropy penalty as it does not contribute meaningfully to the algorithm performance in our experimentation. Similar findings were observed in the GAIL paper, where the authors disregarded the entropy coefficient for every tested environment, except for the Reacher environment.\n\n2) We added the performance of a random policy to the graph to be consistent with the original GAIL paper. We believe that it improves readability of the plot by providing necessary scaling.\n\n3) We already started working on additional experimentation as requested. We will update the manuscript as soon as we gather these results.\n\n4) We observed the same effect of having absorbing states in the Kuka arm tasks (Fig. 6), as in the MuJoCo environments. Also, we evaluated absorbing states within the AIRL framework for Walker-2D and Hopper environments (Fig. 7). We demonstrate that proper handling of absorbing states is critical for effectively imitating the expert policy. \n\nIn addition, we updated the paper to accommodate the minor suggestions proposed by the reviewer.\n", "title": "Response to AnonReviewer3"}, "Byebk4m5Tm": {"type": "rebuttal", "replyto": "S1gpuXlih7", "comment": "We thank the reviewer for the detailed and constructive feedback. We address the above mentioned points and add some additional experiments, as detailed below.\n\nc) \u201cBy assigning more cumulative reward for s2_a1->s1 than for s2_a2->g, the policy would (after a few more updates) choose the latter action much less frequently than with probability 0.5 and the corresponding reward would grow towards infinity until at some point Q(s2,a2) > Q(s2,a1)--when the policy would match the expert exactly.\u201d\n\u201cThe paper further argues that a strictly positive reward function always rewards a policy for avoiding absorbing states, which I think is not true in general. A strictly positive reward function can still produce arbitrary large reward for any action that reaches an absorbing state.\u201d\n\n> This is a good point, and we will discuss this situation in more detail in the final paper. However, we do not believe that this directly applies to adversarial learning algorithms, such as the ones studied in our paper. We provide discussion as well as a numerical example below, which will be included in the paper.  \n\nThe aforementioned situation can only happen in the limit, but the next discriminator update will return the policy to the previous state, in which it is more advantageous to take a loop, according to the GAIL reward definition. Therefore, the original formulation of the algorithm does not converge in this case. In contrast, learning rewards for the absorbing states will resolve this issue. \n\nMoreover, the example provided by the reviewer assumes that we can fix the reward function at some point of training and then train the policy to optimality according to this reward function; while devising a scheme to early terminate learning of the reward function is possible, it is not specified by the dynamic reward learning mechanisms of the GAIL algorithm, which alternates updates between the policy and the discriminator. Please see a simple script that illustrates the example (anonymous link):\nhttps://colab.research.google.com/drive/1gV56NLik367nslwK7iJzs8WTe5tD-BO5\n\nThis specific toy example will be included into our open source release.\n\n\u201cHence, I am not convinced that adding a special absorbing state to the trajectory is necessary if the simulation reset is correctly implemented.\u201d\n\n>  Could you please clarify what do you mean by a correct implementation of simulation resets?\n\n\u201cI also think that it is strange that the direct way of computing the return for the terminal state is much less stable than recursively computing it and think that the paper should include a convincing explanation.\u201d\n\n> We think that it is less stable to analytically compute the returns for absorbing states as it introduces a high variance for TD updates of the value network due to the fact that we bootstrapped for all states. The issue is well known and usually solved by using target networks (see https://www.nature.com/articles/nature14236).\n\n\u201cThis may be different for resets due to time limits that can not be predicted by the last state-action tuple. However, issues relating to time limits are not addressed in the paper\u201d\n\n> Although some of the benchmark tasks do have an episodic time limit, an off-policy RL algorithm can still calculate a (discounted) target value at the last time step in such environments, which is what our implementation of TD3 actually does. Please see the original implementation of TD3 for more details:\nhttps://github.com/sfujim/TD3/blob/master/main.py#L123\n\na) We note that this does make a substantial difference in terms of sample efficiency over prior work on adversarial IL, as shown in Figure 4 -- we believe that such substantial improvements in efficiency are of interest to the ICLR community, though it is not the sole contribution of our paper.\n\nb) We did use normalized importance weights, but unfortunately did not find that the resulting method performed well, while simply omitted importance weights achieved good performance. We think that the naive way of estimating importance weights increases variance of updates. We will analyze this further in the final version, but for now we would emphasize that this is not the primary contribution of the work, but only a technical detail that we discussed for completeness.\n", "title": "Response to AnonReviewer2"}, "rJgV29zq67": {"type": "rebuttal", "replyto": "rJgH5Yg0h7", "comment": "We thank the reviewer for the feedback and appreciate the strong recommendation.\n", "title": "Response to AnonReviewer1"}, "SygDehUmpQ": {"type": "rebuttal", "replyto": "rJePR4gGT7", "comment": "Thank you for your comments.\n\n1. Yes, that\u2019s correct (using TD3 algorithm). For the target part it\u2019s s\u2019 and action is produced by the action target network:  ||logD(s_a,\u30fb)-log(1-D(s_a,\u30fb)) +  \u03b3Q_theta_target(s\u2019, A_target(s\u2019),\u30fb)\u3000-Q_theta(s, a,\u30fb) ||**2.\n2. We used zero actions for the absorbing states.\n3. No, we investigated it only with off-policy case. For the off-policy version of your second question, see Figures 6 and 7. However, the part related to absorbing states is independent of off-policy training.\n", "title": "Response"}, "rJgH5Yg0h7": {"type": "review", "replyto": "Hk4fpoA5Km", "review": "The authors find 2 issues with Adversarial Imitation Learning-style algorithms: I) implicit bias in the reward functions and II) despite abilities of coping with little data, high interaction with the environment is required. The authors suggest \"Discriminator-Actor-Critic\" - an off-policy Reinforcement Learning reducing complexity up to 10 and being unbiased, hence very flexible. \n\nSeveral standard tasks, a robotic, and a VR task are used to show-case the effectiveness by a working implementation in TensorFlow Eager.\n\nThe paper is well written, and there is practically no criticism.\n\n", "title": "A Review on Adversarial Inference by Matching Priors and Conditionals Discriminator-Actor-Critic: Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "S1gzKHFunm": {"type": "review", "replyto": "Hk4fpoA5Km", "review": "This paper investigates two issues regarding Adversarial Imitation Learning. They identify a bias in commonly used reward functions and provide a solution to this. Furthermore they suggest to improve sample efficiency by introducing a off-policy algorithm dubbed \"Discriminator-Actor-Critic\". They key point here being that they propose a replay buffer to sample transitions from. \n\nIt is well written and easy to follow. The authors are able to position their work well into the existing literature and pointing the differences out. \n\nPros:\n\t* Well written\n\t* Motivation is clear\n\t* Example on biased reward functions \n\t* Experiments are carefully designed and thorough\nCons:\n\t* The analysis of the results in section 5.1 is a bit short\n\nQuestions:\n\t* You provide a pseudo code of you method in the appendix where you give the loss function. I assume this corresponds to Eq. 2. Did you omit the entropy penalty or did you not use that termin during learning?\n\n\t* What's the point of plotting the reward of a random policy? It seems your using it as a lower bound making it zero. I think it would benefit the plots if you just mention it instead of plotting the line and having an extra legend\n\n\t* In Fig. 4 you show results for DAC, TRPO, and PPO for the HalfCheetah environment in 25M steps. Could you also provide this for the remaining environments?\n\n\t* Is it possible to show results of the effect of absorbing states on the Mujoco environments?\n\nMinor suggestions:\nIn Eq. (1) it is not clear what is meant by pi_E. From context we can assume that E stands for expert policy. Maybe add that. Figures 1 and 2 are not referenced in the text and their respective caption is very short. Please reference them accordingly and maybe add a bit of information. In section 4.1.1 you reference figure 4.1 but i think your talking about figure 3.", "title": "Interesting paper on the challenges of GAIL", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HyebhMXa2X": {"type": "rebuttal", "replyto": "ByeHNeho37", "comment": "Thank you for your comments!\n\nSince our algorithm uses TD3 (https://arxiv.org/pdf/1802.09477.pdf), we highly recommend to use the original implementation of the algorithm (https://github.com/sfujim/TD3). Our reimplementation of TD3 reproduces the results reported in the original paper. Reproducing results with SAC might be harder since SAC requires tuning a temperature hyperparameter that might require additional efforts in combination with reward learning.\n\n1) We used the batch size equal to 100. We kept all transitions in the replay buffer.\n2) That\u2019s correct. For HalfCheetah, after performing 1K updates of the discriminator we performed 1K updates of TD3. During early stage of development we tried the aforementioned suggestion of simultaneously updating the discriminator and policy, and it produced worse results.\n3) Yes, we will include it in  the appendix.\n4) We used gradient penalty described in https://arxiv.org/abs/1704.00028 and implemented in TensorFlow https://www.tensorflow.org/api_docs/python/tf/contrib/gan/losses/wargs/wasserstein_gradient_penalty with a coefficient equal to 10.\n\nAn additional note regarding reproducing results. Please take into account, that depending on when you subsample trajectories to match the original GAIL setup, you need to use importance weights. Specifically, if you first subsample expert trajectories taking every Nth transition, and then add absorbing states to the subsampled trajectories, you will need to use importance weight 1/N for the expert absorbing states while training the discriminator. We will explicitly mention this detail in next version of the submission.\n\nWe would like to emphasize that upon publishing the paper we are going to open source our implementation.\n\nFeel free to request any additional information. We will be glad to provide everything to help you to reproduce our results.", "title": "Response"}, "S1gOUczL3m": {"type": "rebuttal", "replyto": "BkxfyEhmhm", "comment": "Thank you for sharing the link. The arxiv paper linked is concurrent work. As such our off-policy algorithm was novel at time of release and remains a primary contribution of this work. We will add this paper to the related work section as a concurrent work in the next update.\n\nThe requested ablation study is already presented in Fig. 6 and Fig.7 where we compare adversarial imitation learning approaches with and without the absorbing states. Due to the bias present in the original reward, the baseline without absorbing state information fails to learn a good policy. We derive why this happens in Section 4.1. \n\nAlso, we would like to emphasize that our paper is not limited to off-policy training but also addresses other issues of adversarial imitation learning algorithms. We first identify the problem of biased rewards, which we then experimentally validate across GAIL and AIRL (note that the other paper is centered around GAIL, and not adversarial imitation learning in general). Following that we introduce absorbing states as a fix for this issue, while empirically validating that our proposed solution solves tasks which are unsolvable by AIRL.", "title": "RE: Another paper with off policy imitation learning  "}, "H1xlBqzU2X": {"type": "rebuttal", "replyto": "BygoS4eHnQ", "comment": "Thank you again for your comments.\n\n1. We did not have sufficient time to collate these results before the deadline, but we will add them to the appendix for a future revision.\n\n2. In Fig. 7, we run the absorbing state versus non-absorbing state experiments on the more standard Hopper and Walker2D environments. We understand those experiments are with AIRL algorithm and it will be more comprehensive if we ran the same experiment with GAIL algorithm and environments from Fig. 4. However, we were constrained by the page limits and chose to show how our fix to the reward bias not only works across different adversarial algorithms (GAIL in Fig. 6 and AIRL in Fig. 7) but also works on demonstrations collected from humans on a Kuka arm. We will add the figures for the experiments you mentioned in the comment to the next version of the paper.", "title": "RE: Response"}, "B1l_dvkgn7": {"type": "rebuttal", "replyto": "Byl5uav0oX", "comment": "Thank you for your comments.\n\nAt the moment, we plot results only for 1 million steps. In the original implementation of GAIL, the authors use 25M steps to report the results. With 25M steps we are able to replicate results reported in the original GAIL paper. We do have one example of how the methods compare to each other when trained for 25M steps in our submission. This can be seen in the top left sub-plot in Figure 4. We will add the plots with 25M steps in the next update of the paper.\n\nWe perform ablation experiments and visualize the results in Figure 6. The \u2018no absorbing\u2019 baseline corresponds to off-policy GAIL while the red line corresponds to DAC. Thanks for pointing this out. We will add a clarification in the text to make the comparison clearer.\n", "title": "Original GAIL results and ablation experiments "}}}