{"paper": {"title": "Surprise-Based Intrinsic Motivation for Deep Reinforcement Learning", "authors": ["Joshua Achiam", "Shankar Sastry"], "authorids": ["jachiam@berkeley.edu", "sastry@coe.berkeley.edu"], "summary": "Learn a dynamics model and use it to make your agent boldly go where it has not gone before.", "abstract": "Exploration in complex domains is a key challenge in reinforcement learning, especially for tasks with very sparse rewards. Recent successes in deep reinforcement learning have been achieved mostly using simple heuristic exploration strategies such as $\\epsilon$-greedy action selection or Gaussian control noise, but there are many tasks where these methods are insufficient to make any learning progress. Here, we consider more complex heuristics: efficient and scalable exploration strategies that maximize a notion of an agent's surprise about its experiences via intrinsic motivation. We propose to learn a model of the MDP transition probabilities concurrently with the policy, and to form intrinsic rewards that approximate the KL-divergence of the true transition probabilities from the learned model. One of our approximations results in using surprisal as intrinsic motivation, while the other gives the $k$-step learning progress. We show that our incentives enable agents to succeed in a wide range of environments with high-dimensional state spaces and very sparse rewards, including continuous control tasks and games in the Atari RAM domain, outperforming several other heuristic exploration techniques. \n", "keywords": ["Reinforcement Learning"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "The paper proposes an intuitive method for exploration, namely to build a model of the system dynamics and explore regions where this approximation differs from the observed data (i.e., how \"surprised\" the agent was by an observation). The idea is a nice one, and part of the benefit comes from the simplicity and wide applicability of the approach.\n \n The main drawback of this paper is simply that the resulting approach doesn't substantially outperform existing approaches, at least not to a degree where it seem like the paper should should be clearly accepted to ICLR. On the continuous control tasks, the advantage over VIME seems very unclear (at best the results are mixed, showing sometimes surprisal and sometime VIME do better), and on the Atari games no comparison is made against many of the methods tuned for this setting, such as Gorila (Nair, 2015) which achieves some of the best results we are aware of on the Venture game, which is definitely the strongest result in this current paper, but not as good as this previous work. We know the settings are different, but overall it seems like the approach is largely outperformed by existing approaches, and thus the advantage mainly comes from runtime. This is certainly an interesting take, but needs to be studied a lot more thoroughly before it would make a really compelling case. We would like to recomend this paper to the workshop track. The pros/cons are as follows:\n \n Pros:\n + Simple and intuitive method for exploration\n \n Cons:\n - Doesn't seem to substantially outperform existing methods\n - No comparison to many alternative approaches for some of the \"better\" results in the paper."}, "review": {"Sks0z_o4g": {"type": "rebuttal", "replyto": "rkB7a1zEx", "comment": "Hi, and thanks for your remarks! \n\nRegarding potential: I agree with the spirit of this remark, although not the example (Go). Surpise about dynamics is almost certainly not the full story for intrinsic motivation - and I think in Bellemare et al's paper on psuedocounts, they made an argument against it (suggesting that this was too conservative) - but I think it's useful because the dynamics provide a rich and \"low-hanging\" source of information to inspire an agent with. I say \"low-hanging\" because in most systems we care about, dynamics are complicated enough that they can't be trivially learned, so you can get a useful surprise signal from most transitions, which helps with the real problem of reward sparsity. \n\nI think competence-based models might be a little harder to define and implement satisfactorily - like, if your competence measure involves a count over possible futures your agent can reach, you need to know the dynamics to estimate that, and if you don't know the dynamics, you have to learn them, and until you learn them well, you could have substantial errors in your competence measure.\n\nThere have been some recent papers on using unsupervised auxiliary losses (by people out of Deepmind) in deep RL, and those looked promising. They used reward prediction, additional value replay, and pixel control as auxiliary tasks, and found that they could get great gains on sample efficiency. So, if I were to start thinking about other forms of learning progress or surprise to use as intrinsic rewards, those might be the first places I'd look. \n\nAs for the example of Go - our approach is really best-suited for environments where reward signals are so rare that randomly initialized policies will almost never find them. A game like Go has a well-defined reward signal, because you're guaranteed to get a reward every episode from whichever of the two players won the game. \n\nI think we did a decent job of emphasizing that this approach is really meant for those kind of sparse reward environments, but if you think it needs to be a little more clear on that front, I could revise some of the language in the paper to reflect that.\n\nAnd the problem of too-quickly learning a trivial dynamics function: we did see that happening on MountainCar with the learning progress incentive, as we mention in the paper. \n\nRegarding computation time concerns: since you and AnonReviewer3 had similar concerns, I tried to answer both of your remarks on this in the same response. So, see above for something thorough/complete. But, in the speed test that we have in the paper, the per-step cost was 3 times better for our method than VIME. (Initialization costs ignored.) This is pretty good, no? ", "title": "Response"}, "r1Oxi8s4x": {"type": "rebuttal", "replyto": "S1xjmZzVg", "comment": "Hi, thank you for your time and review!\n\nWith regards to the performance comparison with VIME: it's true that VIME definitely performs better on MountainCar and CartpoleSwingup, but our methods did outperform VIME on HalfCheetah, and we believe that the comparison on SwimmerGather is more ambiguous. Although the performance of VIME for SwimmerGather appears to be stronger, this is based on a smaller sample size of experiments; the VIME curve is a median of only two runs of the algorithm with different random seeds, while the curve for our methods is the median of ten runs. If you would like, we can show the comparison of the top two runs of our methods against the two runs for VIME (although this is also unfair); in this comparison, our method edges out VIME. However, I feel obliged to note that concurrent work by Tang and Houthooft et al, also under review in ICLR17, reported results for SwimmerGather that clearly exceeded both VIME and our approach.\n\nRegarding the speed comparison with VIME: what follows is intended to address the concerns that both you and AnonReviewer2 expressed.\n\nIf it isn't clear from the text and figure, the per-iteration speedup is a factor of three in our speed test (that is, excluding initialization time). For how we arrived at this number: \n\nTime to 15 Iterations = Total Algorith Execution Time - Initialization Time\nIntrinsic Reward Time = Time to 15 Iterations - Baseline Time to 15 Iterations*\nSpeedup over VIME = Our Intrinsic Reward Time / VIME Intrinsic Reward Time\n\n*The baseline is from running TRPO with no bonuses. The initialization time and time to 15 iterations for VIME, our approach, and the baseline are what we report in the table.\n\nI think you make an excellent point that a big O style analysis is useful here, because the speedup over VIME depends on a couple of factors which may vary between experiments and hyperparameters. I'll describe the analysis here, and incorporate it into the next version of the paper.\n\nLet's focus on the per-iteration time cost here, and treat the initialization time as negligible. (For longer experiments, this is reasonable.) Suppose that for both VIME and our approach, you use equivalent neural network architectures for the dynamics models. \n\nAt each iteration, there is a time cost for fitting the dynamics model. Let's say VIME requires (fit time)_VIME, and ours requires (fit time)_SURPRISE. \n\nTo calculate each intrinsic reward, VIME requires both a forward and a backward pass through the network. Let's say that the time cost of a forward pass through the network is t_forward, and the time cost for a backward pass is t_backward. Our method only requires forward passes, and even though VIME uses a more complex model (because you sample multiple parameters from the Bayesian neural network parameter distribution to evaluate a forward pass), we'll assume that the forward pass costs are equivalent. \n\nSuppose the batch size (number of rewards per iteration that need to be computed) is batch_size.\n\nBecause VIME requires a separate** forward/backward pass for each (state, action, next state) tuple to compute an intrinsic reward, we obtain the per-iteration cost of VIME as\n\n(per iteration)_VIME = (fit time)_VIME + batch_size * (t_forward + t_backward) / n_threads,\n\nwhere n_threads is how many different threads you can divide the reward computation load between. \n\nOn the other hand, in our method, no backward pass is necessary. So we can take advantage of the fact that forward passes are highly parallelizable. Suppose that for a given architecture of dynamics model, as many as 'k' inputs can be processed in a forward pass simultaneously. The per-iteration cost of our surprisal method is then:\n\n(per iteration)_SURPRISAL = (fit time)_SURPRISE + batch_size * t_forward / (n_threads * k).\n\nThe speedup over VIME, as the batch_size becomes larger (so the fit times become negligible by comparison) aproaches something that is the same order of magnitude as 'k'. In the experiment we ran for the speed comparison, 'k' works out to about 7 if I'm not remembering incorrectly, so this is pretty good. And in that experiment, the batch_size was only 5000, so we were pretty far from maxing out the speed advantage; on experiments like SwimmerGather and in the Atari domain, I imagine the difference would be more pronounced. I will double-check this sometime wthin the next week.\n\nA speedup factor between 2 and 10, in our view, is substantial. We think it is likely that the speedup will usually fall in this range, even though more complex dynamics models may fall on the shorter end of that. But if you think the claims in our paper are excessive, we are open to the possibility of moderating the language. \n\n** The forward and backward passes for different (state, action, next state) tuples have to be separate because of limitations in existing deep learning toolkits. To the best of my knowledge, none of Tensorflow, Theano, Torch, or their derivatives have tools for taking k inputs and returning k gradients efficiently -- they only seem to take k inputs and return k forward passes efficiently, or take k inputs and return 1 gradient, or take k inputs and return k gradients slowly (computing them separately, as in the VIME implementation).  \n\nSo, to be clear, the speedup over VIME is largely based on implementation, not theory. (Hopefully this was well-described in the paper; if not, I am happy to clarify.)\n\nOverall, we agree with the critique that our work is incremental, but we think our methods represent useful tools in the RL arsenal. ", "title": "Response"}, "B1-PR4iNe": {"type": "rebuttal", "replyto": "ryBd9w4Ex", "comment": "First, thanks for your time and remarks!\n\nOur primary goal here was to introduce, motivate, and proof-of-concept our methods, and we felt that our slate of experiments was sufficient to achieve that end. We did some experiments in Montezuma's Revenge, but we weren't able to find a hyperparameter combination that produced good results before we decided to move on. \n\nI agree that it would be nice to compare directly against Bellemare et al's psuedocount-based methods, but this would have required many more (time-consuming and expensive) experiments. I think a useful future work would focus specifically on a rigorous comparison and benchmarking between different methods for intrinsic motivation - along the lines of \"Benchmarking Deep Reinforcement Learning for Continuous Control,\" by Duan et al. - which could compare well-tuned implementations of psuedocounts, VIME, our methods, and counting with hashes (concurrent work also submitted to ICLR17 by Tang and Houthooft et al.). To facilitate this possibility, we will be releasing our code at some point in the near future.", "title": "Response"}, "SkuT7P_Xx": {"type": "rebuttal", "replyto": "rJu6_Mk7l", "comment": "Unfortunately, random rewards - even with periodic resets - don't solve the harder test domains. During my experiments, I did try what you suggested, resetting the random function every 200 iterations if the average extrinsic return was zero. But, this approach did not work. \n\nWith regards to whether the random rewards were competitive with the best of the variants, I don't think the data supports the claim that they were competitive in any meaningful way. \n\nThe random rewards were mostly successful in Sparse MountainCar (which they usually solved), Sparse CartpoleSwingup (where 3 of 10 runs achieved nonzero performance), and Sparse HalfCheetah (where 4 of 10 runs achieved nonzero performance). In SparseSwimmer and SwimmerGather, the random rewards made effectively no progress.\n\nSparse MountainCar is a very low-dimensional, essentially toy task, so I would not take performance here to be a strong indicator in favor of random rewards. \n\nSparse CartpoleSwingup is harder, but - and this discussion is missing from the paper, and perhaps should be included in supplementary material - this environment is trivial to solve with a trick unrelated to intrinsic motivation. The task is made artificially difficult by what essentially amounts to an adversarial rescaling of the action input, which makes exploration very hard. If you rescale your actions appropriately, you will swing the pole hard enough to explore the space better and solve the task. So, here also, while the random reward performance is interesting, it is not great - and the best surprise-based methods clearly outperform this baseline. \n\nSparse HalfCheetah seems to be the most interesting performance for the random rewards. In this task, the agent has to go forward 5 units before receiving a reward. But a randomly initialized policy can get a maximum forward distance of as many as 4 units. This is insufficient to get a reward, but it means that Sparse HalfCheetah is only a `hard exploration' task in a very metastable sense. Many modes of motion enable the agent to go far enough to start getting rewards. Furthermore, with surprise-based intrinsic motivation, this task was able to be solved in 9 of 10 runs (for AKL-1); this is a very significant improvement from the random baseline.\n\nThe Sparse Swimmer and SwimmerGather tasks are examples of tasks where very few modes of motion are useful, and so most random exploration will - and in our experiments, did - fail.\n\nAside from the empirical results, which I think are reasonably compelling, there is also a good intuitive way of thinking about it. Intrinsic rewards will only help you solve a task if they are, in some sense, 'aligned' with the extrinsic rewards. Reward space is very high dimensional, so if you sample random points for it they are unlikely to align with extrinsic rewards. But if you start at a random point and sweep through in a principled way, you are more likely to eventually wind up in a point that aligns, to some extent, with the extrinsic rewards. Then you are able to successfully reinforcement learn. This is what the surprise-based motivation does.\n\nAlso, I agree with you in principle that the random baseline should be plotted with the rest, but in practice I found that this cluttered the graphs so substantially that those results were no longer be visible. \n", "title": "I wish they could work so well"}, "rJu6_Mk7l": {"type": "review", "replyto": "Bk8aOm9xl", "review": "I'm intrigued by \"RAN\", your random baseline with the surprise of a random, fixed, untrained model. Judging from Fig 5 (it should really be plotted with the rest), this is sometimes competitive with the best of your variants, and it got me thinking: maybe a random-but-fixed shaping reward is often enough to make exploration consistent enough to get off the ground. But of course it can get unlucky, so I'd be very curious to see another baseline: the same as RAN, but sampling a new random model after each couple of iterations if there was no progress in real reward. Maybe none of the (fancier) surprise/learning progress tricks are even necessary?This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game \u201cventure\u201d, maybe).\n\nNovelty: none of the proposed types of intrinsic motivation are novel, and it\u2019s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).\n\nPotential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent\u2019s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.\n\nComputation time: I find the paper\u2019s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?", "title": "more random consistent baselines", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkB7a1zEx": {"type": "review", "replyto": "Bk8aOm9xl", "review": "I'm intrigued by \"RAN\", your random baseline with the surprise of a random, fixed, untrained model. Judging from Fig 5 (it should really be plotted with the rest), this is sometimes competitive with the best of your variants, and it got me thinking: maybe a random-but-fixed shaping reward is often enough to make exploration consistent enough to get off the ground. But of course it can get unlucky, so I'd be very curious to see another baseline: the same as RAN, but sampling a new random model after each couple of iterations if there was no progress in real reward. Maybe none of the (fancier) surprise/learning progress tricks are even necessary?This paper explores the topic of intrinsic motivation in the context of deep RL. It proposes a couple of variants derived from an auxiliary model-learning process (prediction error, surprise and learning progress), and shows that those can help exploration on a number of continuous control tasks (and the Atari game \u201cventure\u201d, maybe).\n\nNovelty: none of the proposed types of intrinsic motivation are novel, and it\u2019s arguable whether the application to deep RL is novel (see e.g. Kompella et al 2012).\n\nPotential: the idea of seeking out states where a transition model is uncertain is sensible, but also limited -- I would encourage the authors to also discuss the limitations. For example in a game like Go the transition model is trivially learned, so this approach would revert to random exploration. So other forms of learning progress or surprise derived from the agent\u2019s competence instead might be more promising in the long run? See also Srivastava et al 2012 for further thoughts.\n\nComputation time: I find the paper\u2019s claimed superiority over VIME to be overblown: the gain seems to stem almost exclusively from a faster initialization, but have very similar per-step cost? So given that VIME is also performing very competitively, what arguments can you advance for your own method(s)?", "title": "more random consistent baselines", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}