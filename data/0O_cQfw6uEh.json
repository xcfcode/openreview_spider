{"paper": {"title": "Gradient Origin Networks", "authors": ["Sam Bond-Taylor", "Chris G. Willcocks"], "authorids": ["~Sam_Bond-Taylor1", "christopher.g.willcocks@durham.ac.uk"], "summary": "A new model that uses the negative gradient of the loss with respect to the origin as a latent vector is found to be superior to equivalent networks.", "abstract": "This paper proposes a new type of generative model that is able to quickly learn a latent representation without an encoder. This is achieved using empirical Bayes to calculate the expectation of the posterior, which is implemented by initialising a latent vector with zeros, then using the gradient of the log-likelihood of the data with respect to this zero vector as new latent points. The approach has similar characteristics to autoencoders, but with a simpler architecture, and is demonstrated in a variational autoencoder equivalent that permits sampling. This also allows implicit representation networks to learn a space of implicit functions without requiring a hypernetwork, retaining their representation advantages across datasets. The experiments show that the proposed method converges faster, with significantly lower reconstruction error than autoencoders, while requiring half the parameters.", "keywords": ["Deep Learning", "Generative Models", "Implicit Representation"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper presents a new inference mechanism for latent variable models, by taking the derivative of log-likelihood with respect to a zero-valued vector. Initially, the reviewers raised concerns mostly regarding the limited experimentation and missing baselines. However, in the revised version, the authors addressed most of these concerns. \n\nGiven that most reviewers are positive after the revision and since the proposed method is simple and interesting, I recommend accepting this paper."}, "review": {"afDk9eNH48N": {"type": "rebuttal", "replyto": "yiPHZgE5QR", "comment": "We agree that single-step GONs for mean-field implementations are linear as you've helpfully shown clearly above. However, multiple-step GONs (see Figure 2b and Table 1) for this are able to give non-linear encodings (because the second step uses a $z$ that is a linear function of $x$ so now $F(z)$ is dependent on $x$). In our experiments we found that the non-linearity induced by multiple-steps didn\u2019t provide quantitative improvement when dealing with high-dimensional datasets while it significantly increased run-time (see Figure 2b and Table 1).\n\nHere is a [Colab notebook](https://colab.research.google.com/gist/cwkx/1f3db3c088334fdccb24822ee280bb2a/non-linear-2-step-gon-encodings-example.ipynb) demonstrating a 2-step GON giving clear non-linear encodings on your sphere test example.", "title": "Multi-step GONs are non-linear"}, "WZyMW4mzuWT": {"type": "rebuttal", "replyto": "XQ-f34Kl7e1", "comment": "1. Using a linear encoding like this is clearly not scalable. To show our point, for our bedrooms experiment (128x128x3 compressed to 2048), your above notebook example with a linear encoder would use **100,663,296 parameters** whereas **GONs use 0**. And the qualitative results are impressive in both cases.\n\t- GONs are presented in a general form; we don't dispute that mean-field GONs encode linearly. Our high dimensional test cases show they outperform their non-linear AE equivalents (which are standard convolutional AE architectures).\n\t- As stated throughout our abstract and paper, one of the main use cases of GONs is their applications in implicit networks.\n2. As discussed in our previous comment, MADE is an autoregressive model.\n3. We just used MADE as a simple example to demonstrate GONs capability for your toy benchmark. Other likelihood models such as PixelVAE or MAE can be used for higher dimensional data. \n4. In the method section of our paper we discuss modelling in broad terms of $p(x|z)$ and do not state that this has to be a mean-field model. Our experiments demonstrate the efficacy of GONs with simple independent output distributions outperforming autoencoders while using significantly fewer parameters, but GONs can also be applied to more complex decoders.\n5. Consider an autoregressive model $p(x_1|z)p(x_2|x_1,z)\\cdots p(x_n|x_{<n},z)$. The gradient of $p(x_2|x_1,z)$ with respect to $z$ is dependent on $x_1$ and the gradient of $p(x_n|x_{<n},z)$ with respect to $z$ is dependent on $x_{<n}$. This is what the code we shared in our initial answer does.\n\nTo summarise our position:\n- GONs are presented in a general form that supports both linear and non-linear encodings dependent on how $p(x|z)$ is modelled.\n- Linearly encoding GONs significantly outperform autoencoders on high dimensional data with substantially fewer parameters, while converging faster and generalising better.\n\nIn light of your strong views on this, we are more than happy to add a statement to our paper that the mean-field implementation of GONs equates to a linear encoding and recheck, to make sure we are consistent with such claims - this does not otherwise affect the narrative, contributions, or any part of this paper: abstract, introduction, method, results, discussion, and conclusions.", "title": "We are not obscuring the point"}, "yuNYcU5JPB_": {"type": "rebuttal", "replyto": "L6b8kpQDDES", "comment": "When the generative function models each data component independently, the encoding function is linear, however, this is not a fundamental property of GONs since conditional dependencies can be modelled effectively. We'll make sure we make no such claims in the final camera ready version. Besides, GONs seem to excel in high dimensional data spaces as demonstrated by our strong empirical results, significantly outperforming autoencoders in real world cases, fitting better while also generalising better, even with small latent vectors despite making this simple independence assumption. This allows applications such as modelling $p(x|z)$ using an implicit network with advantages such as superesolution.\n\nAdditionally, MADE is not a standard autoencoder, it is an autoregressive model obtained by masking an autoencoder\u2019s weights such that each output neuron is conditioned only on the previous values. This is different to a standard autoencoder which, unlike MADE, models the components of x independently but requires a bottleneck to prevent the identity function from being learned. Our example demonstrates this in a latent conditional model, comparable to a PixelVAE [1] or MAE [2], both of which require an encoder to compress data to latent vectors. On the other hand, our example demonstrates that GONs can achieve this without an encoder.\n\n[1] Gulrajani, I., Kumar, K., Ahmed, F., Taiga, A. A., Visin, F., Vazquez, D., & Courville, A. (2016). Pixelvae: A latent variable model for natural images. ICLR.\n\n[2] Ma, X., Zhou, C., & Hovy, E. (2019). MAE: Mutual posterior-divergence regularization for variational autoencoders. ICLR.", "title": "GONs are not mean-field models and this is not a main limitation"}, "c2Tf26KFK_W": {"type": "rebuttal", "replyto": "yiPHZgE5QR", "comment": "There's an error in your reasoning, we define the latent variable computation in terms of $p(x|z)$ (Equation 7) which can be modelled such that the components of $x$ have conditional dependencies, causing $\\frac{\\partial F}{\\partial z}$ to be dependent on $x$. A simple counter example to your unit sphere example is to model $p(x|z)$ using a conditional MADE [1], which can easily reconstruct the data distribution with 2D latent variables, without an encoder. Here is a [Colab notebook](https://colab.research.google.com/gist/samb-t/d3a4d7d7204bda3a0c81995e654d00d4/made-gon-sphere.ipynb) demonstrating this.\n\nAs for the high-quality samples being just due to a large $z$ dimension, please see Figure 10c where we demonstrate GONs reconstruction ability for a variety of latent sizes. Indeed, GONs outperform autoencoders even with small latent spaces.\n\n[1] Germain, M., Gregor, K., Murray, I., & Larochelle, H. (2015). MADE: Masked Autoencoder for Distribution Estimation. In International Conference on Machine Learning (pp. 881-889). PMLR.", "title": "GONs are not linear models"}, "EYWJekQyYEm": {"type": "review", "replyto": "0O_cQfw6uEh", "review": "The paper proposes GONs which seek to build a generative model with an \u201cimplicit\u201d encoder that comes essentially for free with the use of a few re-parameterization tricks. The main idea being that existing generative models with an encoder are \u201credundant\u201d in that the decoder itself has the ability to compute the gradient with respect to a latent vector, z, which itself can be thought of as the \u201cencoding\u201d. Since the choice of what initial latent vector to choose arises here, the paper advocates for simply choosing a z_0 which is a zero vector. In addition to the \u201cexplicit\u201d formulation, there is also an implicit GON which is proposed that can generalize implicit generative models  (like SIREN) to entire distributions as opposed to a single data point, as they are currently used.\n\nOverall, I think this is very interesting work but incomplete. Considering GONs are a completely new category of generative models, it would greatly help to study each piece in more detail (theoretically or empirically) to establish what makes GONs successful, different, and how this improves our understanding of implicit representations in neural networks. \n\n\nStrengths:\n\n+ An interesting and novel formulation of encoding schemes from decoders that do not need any additional training or networks. \n+ The paper explores several different variants of GONs \u2014 from a variational alternative, implicit, and a classifier. Which greatly expands its scope of application in new problems. \n+ GONs generalize implicit generative models like SIRENs to work with an entire data distribution with very few parameters, which I think is a great benefit. This also naturally allows for variational alternatives, meaning we can sample from complex high dimensional distributions using very simple networks. \n+ The implicit GON also enables finer grid sampling in the input space, enabling its use in applications like super resolution naturally \u2014 but to any image from the training distribution. \n\n\nWeaknesses:\n\n* The paper is very dense in terms of ideas, and as such falls short in thoroughly evaluating all of them. For example, the paper contributes several ideas like GONs, implicit GONs, variational GONs, which is great but it would help if each one of those pieces were studied in some more detail so they can be compared and contextualized better with existing approaches. For example, in the formulation itself the GON loss is presented \u201cas is\u201d, but I think it warrants some more study. \n    * For example, why is just a single step \u201csufficient\u201d to estimate \u201cz\u201d? Does the quality of \u201cz\u201d improve if you take multiple smaller steps? How stable is this for different datasets? The empirical studies show promise, that indeed this can work reasonably well in reconstructing different datasets, but it would greatly help to justify some of these choices further. \n    * In the explicit case, how important is the choice of \u201cF\u201d ? The choice of activation function is explored but what about the architecture/ number of parameters for a given dataset? \n* In all the experiments, the reconstruction losses are shown are for the training set, how do the validation set samples get reconstructed?  It\u2019s not clear if GONs are so effective in reconstructing because they are memorizing the data? \n* How does the performance of GONs change as the size of the output space grows larger? For e.g. 128x128 or 256x256? \n* Some of the terminology is also confusing. What does it mean when you \u201coverfit\u201d to an entire distribution? I understand its usage for a single image, but it's not clear what this means for an entire dataset. Are the samples from Figure 4 all from the *same* trained GON? \n* Is Figure 7 from an explicit GON or an implicit GON? If its explicit, how are the number of parameters comparable to an implicitGON? Clearly an explicit model will have a lot more number of parameters. esp as the size of the images increase?\n* I really like and appreciate the variationalGON experiments. How do they compare with  standard VAEs? Can they recover CelebA 64x64 images? How would they compare on quantitative metrics like FID etc.?\n* In the super resolution experiment, can it super resolve *any* image from the distribution it was trained on? For e.g. in figure 5. is it just a matter of resampling the grid to 256x256 and running them through the pre-trained model for any sample from p(x)?\n\n---------- Update on the revised manuscript ----------\n\nI have read the new version of the paper and it reads a lot better. The new expanded methods section, and the definitions for different variations of GONs makes the paper much stronger and easier to understand. I appreciate and like the new experiments that show GONs capabilities on LSUN, comparisons with VAE on ELBO. \n\nMost of my concerns have been addressed in this version. I think this paper makes an interesting and novel contribution and I will raise my score accordingly. \n\n", "title": "An interesting new perspective on generative modeling and implicit representation learning, but incomplete in its execution.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "OTL9r-5XmOs": {"type": "review", "replyto": "0O_cQfw6uEh", "review": "This paper proposes a new type of generative models with a new inference method of latent variables. Specifically, the gradient of latent variables with respect to zero vector is taken as the inferred latent variables. Based on this, the authors generalize the propose model to implicit and variational versions and demonstrate the models on image datasets.\n\nPros: the proposed method is easy and straightforward to implement. \n\nCons:\n1. The model assumption that the one step gradient from zero vector equals to latent vector is quite limited and greatly constrains the model expressiveness. A justification that such assumption is reasonable is badly needed.\n\n2. Formulation needs to be carefully checked. For example, Eqn 2 is not entirely correct to me. The second term should not be binary cross entropy as there is no categorical variable involved. Also, please avoid using abbreviations (L^BCE, L^CCE) at the first time to introduce them, which are confusing. \n\n3. Experimental results are not sufficient to demonstrate the efficacy. Need more quantitative analysis and experiments on more challenging datasets. \n\n4. The claim that it saves parameters compared to VAE is confusing. In the variational version, parametrizations of mu(x) and sigma(x) are also required. A principled way to very this claim is to show that with the variational version, the method could use much less parameters compared VAE while has the better synthesis quality. \n\nOverall, the method proposed in this paper is new and promising. However, given the current unclear formulation and lack of strong experimental results, I recommend a rejection. ", "title": "Lack of solid formulation and strong experiments", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "CDj6ZkV5Ft": {"type": "rebuttal", "replyto": "0O_cQfw6uEh", "comment": "We thank the reviewers for their valuable comments. During this rebuttal period we have made a very significant revision to the original paper, including the addition of Section 3.1, a new derivation from empirical Bayes (proof of conditional empirical bayes in Appendix A), added Table 1, showing quantitatively that GONs significantly outperform competing single-step methods, Table 2, demonstrating that variational GONs achieve substantially lower ELBO than VAEs (on 5/6 datasets) including large complex datasets (CelebA), Figure 9, showing qualitatively that GONs can well represent large complex datasets (added higher resolution LSUN Bedrooms and CelebA), Figure 2b, confirming that a single gradient step is sufficient, Figures 2c and 16, demonstrating GON generalisation, Figure 17, showing that GONs allow superresolution on test data, Table 1 and Figures 2a, 3, and 10 have been updated to include means/standard deviations (error envelopes) from multiple runs, alongside various new discussions.", "title": "Summary of revision changes"}, "ibJJkF7_G5v": {"type": "rebuttal", "replyto": "O9dl7y_u12v", "comment": "Thank you for your kind words and for updating the score. We have implemented these suggested changes; error bars (as envelopes) representing standard deviation have been added to Table 1 and Figures 2a, 3, and 10 however we deemed it inappropriate for the other figures due to significant line overlapping impacting the presentation of these results.", "title": "Response to Reviewer 1"}, "WVQgsB-Mi7q": {"type": "review", "replyto": "0O_cQfw6uEh", "review": "This paper introduces a \"new\" inference method for autoencoder-type models, where the encoder is taken as a gradient of the decoder with respect to a zero-initialized latent variable. The method is evaluated for both a deterministic autoencoder and a VAE on toy image data (cifar10 being the most complex of them) and applied to convolutional decoder and to SIREN-type implicit representation networks. This is, for all intents and purposes, a single step iterative inference setup. In its VAE variant it is extremely similar to old-school iterative inference, albeit with a single gradient step. \n\nThe paper is very-well written and interesting. The method seems to be getting very good results,. Still, the paper seems to be rushed. The results are only on small scale and toyish datasets, and there are very few baselines. \n\nIn its current state I recommend rejection due to rather limited novelty (although it's cool to see that this type of inference works for implicit scene representations) and very limited evaluation. There are also very many links to existing literature that are not properly described. Let me elaborate.\n\nBaselines:\nTo determine the efficacy of this method, the authors would have to compare against some similar methods including:\n* old-school multi-step variational inference\n* semi-amortized variational inference\n* the proposed method with multiple gradient steps\n* the proposed method with detached gradient (as in: not use 2nd order gradients)\n* a fully-convolutional autoencoder with parameters tied between the encoder and decoder. This is for two reasons: a) this would reduce the number of parameters by half, making it more similar to GON, but also b) the transposed-convolution used in such a setup corresponds almost exactly to the gradient of the encoder, which is an idea very similar to GONs.\n\nMissing links to the literature:\n* the above fully-conv AE setup.\n* model-agnostic meta-learning (and related, e.g. CAVIA, LEO etc), where the \"latents\" are produced by single- or multi-step optimization.\n\nMissing experiments:\nWe would need more evidence to determine if such a simple method is useful. A good experiment would be e.g. on imagenet.\n\nFurther suggestions:\nSubfigures in fig2 and 3 (and most of figs in the appendix) use different scales on the Y axis. It would be easier to read the figures if the scaled were normalized within a single figure.\n\nUpdate: I've updated the score given the authors' response, see my comment below.", "title": "Very interesting paper and findings, but seems somewhat rushed.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "80wLrgIYXMT": {"type": "rebuttal", "replyto": "WVQgsB-Mi7q", "comment": "Thank you for your constructive feedback and excellent suggestions. We will address your comments point-by-point:\n\n> \u201cThe results are only on small scale and toyish datasets\u201d\n\nIn our latest update we have added experiments on larger scale more complex datasets: 128x128 LSUN Bedrooms for the GON, and CelebA 64x64 for the variational GON.\n\n> \u201cBaselines: To determine the efficacy of this method, the authors would have to compare against some similar methods including...\u201d\n\nFor our non-variational approach, we have added quantitative comparisons with autoencoders+tied weights, our approach with detached gradients, our approach with multiple gradient descent steps, both with and without detached gradients, as suggested, as well as with a GLO (Bojanowski 2018) which assigns a latent vector to each data point and jointly optimises these with the network parameters (Table 1). We find that GONs achieve the lowest validation losses on 3 of the 5 datasets tested. Notably, all other single step approaches result in significantly high reconstruction loss.\n\nWe have also added quantitative comparisons with vanilla VAEs in Table 2, finding GONs to achieve lower ELBO on 5 of the 6 datasets. We aim to add comparisons with other variational approaches as suggested, in another update.\n\n> \u201cb) the transposed-convolution used in such a setup corresponds almost exactly to the gradient of the encoder, which is an idea very similar to GONs.\u201d\n\nWhile the gradient through a single convolution layer is indeed related to transposed convolutions, when convolutions are composed and/or interleaved with other functions, the gradient becomes much more complex. Indeed, the gradient of deep MLPs corresponds to a product of networks. Additionally, restricting the architecture to using tied-weights is not necessarily applicable to more complex architectures whereas using the gradient can be applied to any function.\n\n> \u201cMissing links to the literature\u2026\u201d\n\nWe have now integrated discussion of autoencoders with tied weights and the connections with model-agnostic meta-learning into the Discussion section.\n\n> \u201cMissing experiments: We would need more evidence to determine if such a simple method is useful. A good experiment would be e.g. on imagenet.\u201d\n\nWe hope that the aforementioned additional quantitative experiments (Tables 1 and 2) including experiments on CelebA, and qualitative examples on LSUN Bedrooms assuage your concerns. Experiments have also been added to evaluate our claim that a single gradient step is sufficient and that GONs do not memorise datasets in Figures 2b and c respectively.\n\n> \u201cFurther suggestions: Subfigures in fig2 and 3 (and most of figs in the appendix) use different scales on the Y axis. It would be easier to read the figures if the scaled were normalized within a single figure.\u201d\n\nAll figures which we deemed legible when normalised with a single figure (previously Figures 2, 9, 10, and 11) have been changed as per your suggestion (now Figures 2 and 10).", "title": "Response to Reviewer 1"}, "wcc22YBuryA": {"type": "rebuttal", "replyto": "UEmjMeEUIE", "comment": "> \u201cIs Figure 7 from an explicit GON or an implicit GON? If its explicit, how are the number of parameters comparable to an implicitGON? Clearly an explicit model will have a lot more number of parameters. esp as the size of the images increase?\u201d\n\nFigure 7 is from an explicit GON, the caption has been updated to clarify this. The number of parameters used for implicit GONs and explicit GONs are shown in the captions of Figure 4 and 7 respectively. While not a direct comparison since Figure 7 is a variational GON and the numbers of parameters are chosen to be approximately comparable, this does provide some insight. We find that implicit models are able to better represent data when fewer parameters are available or when the image size is larger.\n\n> \u201cI really like and appreciate the variationalGON experiments. How do they compare with standard VAEs? Can they recover CelebA 64x64 images? How would they compare on quantitative metrics like FID etc.?\u201d\n\nIn the latest update we have included quantitative comparisons between variational GONs and VAEs in terms of ELBO on the test set in Table 2. This includes evaluation on the CelebA 64x64 dataset. In 5 of the 6 datasets tested on, the GON approach achieves lower ELBO than the VAE. Samples from the variational GON trained on CelebA can also now be found in Figure 9b in order to assess this qualitatively.\n\n> \u201cIn the super resolution experiment, can it super resolve any image from the distribution it was trained on? For e.g. in figure 5. is it just a matter of resampling the grid to 256x256 and running them through the pre-trained model for any sample from p(x)?\u201d\n\nYes, it can super resolve any image thanks to the generalisation ability of GONs, and it is as simple as you state; Figure 17 has been updated to contain super-samples of images in the MNIST test set.", "title": "Response to Reviewer 2"}, "UEmjMeEUIE": {"type": "rebuttal", "replyto": "EYWJekQyYEm", "comment": "Thank you for your excellent comprehensive feedback. We have incorporated many of your suggestions in the most recent update. We will address your comments point-by-point:\n\n> \u201cThe paper is very dense in terms of ideas, and as such falls short in thoroughly evaluating all of them. For example, the paper contributes several ideas like GONs, implicit GONs, variational GONs, which is great but it would help if each one of those pieces were studied in some more detail so they can be compared and contextualized better with existing approaches. For example, in the formulation itself the GON loss is presented \u201cas is\u201d, but I think it warrants some more study. \u201d\n\nWe have greatly expanded the method section, deriving our approach from empirical Bayes. Specifically, there is now a preliminaries section which introduces the concept of empirical Bayes and variational autoencoders in detail; additionally, the method section is now divided into sections, covering our contributions (GON, variational GON, implicit GON, and generalisations) in more detail as well as more thoroughly introducing the surrounding concepts.\n\n> \u201cFor example, why is just a single step \u201csufficient\u201d to estimate \u201cz\u201d? Does the quality of \u201cz\u201d improve if you take multiple smaller steps? How stable is this for different datasets? The empirical studies show promise, that indeed this can work reasonably well in reconstructing different datasets, but it would greatly help to justify some of these choices further. \u201d\n\nOur new derivation from empirical Bayes shows that if we consider z_0 a noisy approximation of z, then we can use a single gradient step to calculate the expected value of p(z|x). As mentioned in our response to Reviewer 3, we also provide an explanation from a function approximating perspective, namely, the derivative of a deep MLP corresponds to a product of networks, allowing efficient modelling of high dimensional data. \n\nWe have added experiments to evaluate the claim that a single step is sufficient in Figure 2b, finding that when jointly optimised, multiple steps offer no notable improvement over a single step, and training with gradients of z detached results in significantly worse performance. This can also be seen over a broad range of datasets, trained over long periods, in Table 1.\n\nIn terms of stability, we have observed no issues when training these models. Standard architectures are used and the Adam optimiser with default values. This is the case even on high resolutions data (up to 128x128 images tested). Additionally, we find them to be extremely consistent over multiple runs.\n\n> \u201cIn the explicit case, how important is the choice of \u201cF\u201d ? The choice of activation function is explored but what about the architecture/ number of parameters for a given dataset?\u201d\n\nIn Figure 10b the effect of number of parameters is explored. We find that GONs outperform their equivalent autoencoder in all cases. As the number of parameters is increased, this lead lessens due to diminishing returns. While we only use simple architectures, we have found all variations (e.g. upsampling, transposed convolutions, instance normalisation) to be effective.\n\n> \u201cIn all the experiments, the reconstruction losses are shown are for the training set, how do the validation set samples get reconstructed? It\u2019s not clear if GONs are so effective in reconstructing because they are memorizing the data?\u201d\n\nWe have added extra experiments to assess this. In Figure 2c, Table 1, and Figure 16 training and validation losses are plotted for both GONs and their equivalent autoencoder; this demonstrates that GONs not only do not memorise the data, but appear to generalise better than autoencoders. \n\n> \u201cHow does the performance of GONs change as the size of the output space grows larger? For e.g. 128x128 or 256x256?\u201d\n\nWe find the performance to be on par with smaller sized outputs. This is evaluated qualitatively by reconstructing 128x128 LSUN Bedrooms data with a convolutional GON in Figure 9a where a substantial amount of detail is modelled.\n\n> \u201cSome of the terminology is also confusing. What does it mean when you \u201coverfit\u201d to an entire distribution? I understand its usage for a single image, but it's not clear what this means for an entire dataset. Are the samples from Figure 4 all from the same trained GON?\u201d\n\nThank you for pointing out this misnomer, the caption has been adjusted accordingly. This experiment is meant to compare with implicit representation networks which are trained on a single image. We show that GONs can represent whole datasets to a high degree of fidelity. To answer your question explicitly, the images in Figure 4 are indeed all from the same trained GON.\n\n(Continued Below)", "title": "Response to Reviewer 2"}, "g-QgCiC9YK9": {"type": "rebuttal", "replyto": "OTL9r-5XmOs", "comment": "Thank you for your very helpful feedback, we will try to address your comments point-by-point:\n\n> \u201cThe model assumption that the one step gradient from zero vector equals to latent vector is quite limited and greatly constrains the model expressiveness. A justification that such assumption is reasonable is badly needed.\u201d\n\nWe have updated the paper, deriving our approach from empirical Bayes. In summary, for a latent variable z and data point x, if we have a noisy observation of z, i.e. z_0=z+N(0,I), then empirical Bayes\u2019 allows us to obtain the expected value of p(z|x) using a single gradient step. When a normally distributed prior is assumed over z, then we can choose z_0 as the origin since it is the mean of p(z_0).\n\nWe also provide an explanation, from a function approximating perspective:\n1. The gradient itself is a non-linear function that can approximate functions: the derivative of a deep MLP corresponds to a product of networks.\n2. Using the gradient as an encoder offers good initialisation since it inherently provides an improved estimate of the latent vector.\n3. Good latent spaces should satisfy local consistency (points close in latent space should be close in output space). Similar data points have similar gradients so this is satisfied. The exact gradient is thus relatively unimportant; the network\u2019s prior must be the gradient operation but since the gradient is relatively unimportant, this does not severely restrict the network\u2019s expressiveness.\n\n> \u201cFormulation needs to be carefully checked. For example, Eqn 2 is not entirely correct to me. The second term should not be binary cross entropy as there is no categorical variable involved. Also, please avoid using abbreviations (L^BCE, L^CCE) at the first time to introduce them, which are confusing.\u201d\n\nThank you for pointing this out. The variational GON formulation has been altered to be more general.\n\n> \u201cExperimental results are not sufficient to demonstrate the efficacy. Need more quantitative analysis and experiments on more challenging datasets.\u201d\n\nWe have added a number of additional experiments analysing the GON formulation including the effect of multiple gradient descent steps, confirming our hypothesis that a single step is sufficient (Figure 2b) and the ability for GONs to generalise (Figure 2c); qualitative experiments on more challenging datasets: reconstructions on LSUN Bedrooms (Figure 9a) and samples from a variational GON trained on CelebA (Figure 9b); and quantitative analysis comparing GONs with other approaches as suggested by Reviewer 1 where we find GONs to be competitive with multi-step approaches (Table 1) as well as a comparison between variational GONs and VAEs in terms of validation ELBO (Table 2). In a future update, more experiments will be added.\n\n> \u201cThe claim that it saves parameters compared to VAE is confusing. In the variational version, parametrizations of mu(x) and sigma(x) are also required. A principled way to very this claim is to show that with the variational version, the method could use much less parameters compared VAE while has the better synthesis quality.\u201d\n\nTo implement a variational GON we integrate the reparameterization trick into the decoder network. Specifically, the forward pass takes input z, is mapped by two linear layers to mu(z) and sigma(z), the reparameterization trick is applied, then the rest of the function is performed to obtain p(x|z). This allows us to use the GON update step to obtain z from the original z_0, while still parameterising mu and sigma. The parameters are thus reused in the derivative as the encoder so that there are just under half as many. We have attempted to clarify this in the variational GON section of the method. As suggested, we quantitatively verify this in Table 2 and find that the variational GON achieves lower validation ELBO than an equivalent VAE with almost twice as many parameters on 5 of the 6 datasets.", "title": "Response to Reviewer 3"}}}