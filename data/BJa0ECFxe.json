{"paper": {"title": "Information Dropout: learning optimal representations through noise", "authors": ["Alessandro Achille", "Stefano Soatto"], "authorids": ["achille@cs.ucla.edu", "soatto@cs.ucla.edu"], "summary": "We introduce Information Dropout, an information theoretic generalization of dropout that highlights how injecting noise can help in learning invariant representations.", "abstract": "We introduce Information Dropout, a generalization of dropout that is motivated by the Information Bottleneck principle and highlights the way in which injecting noise in the activations can help in learning optimal representations of the data. Information Dropout is rooted in information theoretic principles, it includes as special cases several existing dropout methods, like Gaussian Dropout and Variational Dropout, and, unlike classical dropout, it can learn and build representations that are invariant to nuisances of the data, like occlusions and clutter. When the task is the reconstruction of the input, we show that the information dropout method yields a variational autoencoder as a special case, thus providing a link between representation learning, information theory and variational inference. Our experiments validate the theoretical intuitions behind our method, and we find that information dropout achieves a comparable or better generalization performance than binary dropout, especially on smaller models, since it can automatically adapt the noise to the structure of the network, as well as to the test sample.", "keywords": ["Theory", "Deep learning"]}, "meta": {"decision": "Reject", "comment": "The authors all agree that the theory presented in the paper is of high quality and is promising but the experiments are not compelling. The reviewers are concerned that the presented idea and connections to existing methods, while neat, may not be impactful as the promise of the theory does not bear out in practice. One reviewer is concerned that the presented theory is still not useful, stating that the \"information bottleneck thus only becomes meaningful when the capacity of the encoding network is controlled in some measurable way, which is not discussed in the paper\". In general, they seem to agree that the experimental evaluation is still preliminary and unfinished. As such, it would seem that the authors could make the paper far more compelling by demonstrating more compelling improvements on benchmark experiments and submitting to a future conference."}, "review": {"ByUcqJ7Le": {"type": "rebuttal", "replyto": "BJa0ECFxe", "comment": "Following the suggestions of the reviewers, we updated the paper with new experiments and plots.\n\nFirst, to empirically validate that, by increasing the value of the parameter \\beta, we can obtain representations that are increasingly minimal and invariant while remaining discriminative (sufficient), we created a new dataset, called \u2018Occluded CIFAR\u2019. The experiment in Sec. 6 shows precisely this effect, thus validating the theoretical intuition. Indeed, by increasing \\beta, we also prevent overfitting and the overall quality of the representation actually improves.\n\nIn Figure 5 in Appendix D, we added a comparison between Information Dropout and binary dropout using the same settings as [Springenberg et al., 2014]. For both methods we obtain a slightly better testing error than the original paper and, as also observed in the previous experiments, Information Dropout performs comparably or better than dropout.\n\nIn Figure 6, we plot the amount of information flowing through the dropout layers of a CNN as the number of filters varies. This plots supports some of the theoretical intuitions, and we show empirically that information dropout automatically selects a lower noise level for smaller networks, and that the units in the higher layers contain on average more information relative to the task than units in the bottom layers.", "title": "Paper update: new experiments"}, "SyFlT_Krg": {"type": "rebuttal", "replyto": "rJ6ELPbVe", "comment": "We thank the reviewer for the comments. We would like to address some of the theoretical points which were raised.\n\nThe information bottleneck method is a generalization of the notion of minimal sufficient statistic [TPB99]. Sufficiency and minimality as defining criteria for a representation are no less theoretically grounded than likelihood or posterior since, by definition, a minimal sufficient statistic captures all and only the variability in the data that matters for the task. No more, and no less.\n\nMoreover, unlike a generic Bayesian framework, the IB method further allows making the tradeoff between complexity and fidelity of the representation explicit, and directly models the effect of nuisances (Sec. 3). \n\nThus, from a theoretical standpoint, the IB Lagrangian is our starting point, and defines what we call an optimal representation. The key question now is how to compute it, and how to design and learn representations that optimize it. Our theoretical contribution is to show that this can be done efficiently using a very specific architecture (dropout layer) and, using this result, to establish connections to seemingly unrelated theories such as variational dropout.\n\n>> Bayesian interpretations lend a theoretical basis to parameter noise, but activation noise has no such motivation.\n\nAs shown by [KSW15], and as we briefly mention in Sec. 2, activation noise and parameter noise are equivalent (via the \u201clocal reparameterization trick\u201d). So activation noise can be motivated in a Bayesian setting as well. Our framework has the results derived in [KSW15] from a Bayesian point of view as a special case, and moreover it can address the specific role of nuisances. Finally, we provide an alternative information-theoretic motivation to activation noise.\n\n>> The information bottleneck indeed limits the information that can be passed through, but there is no rigorous argument for why this should improve generalization.\n\nAs argued in the first paragraph, and supported in the subsequent analysis, (a) Information Dropout reduces the effects of nuisance variability. The fact that (b) reducing nuisance variability prevents overfitting is clear, and so is the fact that (c) preventing overfitting improves generalization. Specifically on (a), Information Dropout penalizes the passage of information (second term of the IB Lagrangian); information due to nuisance variability contribute none to the task; therefore, it can be discarded with minimal loss (first term of the IB Lagrangian).\n\n[KSW15] Diederik Kingma, Tim Salimans, and Max Welling, \"Variational Dropout and the Local Reparameterization Trick\", 2015\n\n[TBP99] Naftali Tishby, Fernando Pereira, and William Bialek, \"The Information Bottleneck Method\", 1999", "title": "Why is such an information bottleneck a good idea from a theoretical standpoint?"}, "B1YPiOFrg": {"type": "rebuttal", "replyto": "BJa0ECFxe", "comment": "We thank all the reviewers for their comments. We would like to provide some clarification regarding the experiments in the paper, and address some of the concerns which were raised.\n\n>> The CIFAR-10 results are worse than those in the paper that originally proposed the network architecture they use (Springenberg et al). The VAE results on MNIST are also horrible.\n\nIf we use exactly the same architecture of Springenberg et al., then our results on CIFAR are, as predicted by the theory, comparable asymptotically, and better for smaller nets. We have added experiments that show this in the revised version to be uploaded soon. Also, our results on VAE are comparable to [KW13] for a similar architecture.\n\nNote, however, that the goal of our experiments is not to improve state-of-the-art on CIFAR-10 or MNIST, but to illustrate the effect of Information Dropout when compared to other forms of dropout, and to validate the intuition derived from the theory. For this reason, for the experiments in the paper we chose the simplest empirical settings, and modified the All Convolutional Net to isolate potentially confounding factors: we removed weight decay, increased the batch size to reduce gradient noise, simplified the architecture by removing the initial dropout layer, and used less aggressive learning rates and no fine tuning.  We also replaced ReLU with Softplus to make the results comparable with those of [KSW15]. This also served to validate the theory which applies to both ReLU and Softplus. \n\nMany factors affect empirical performance, only few of which are relevant to validating our theory. To the latter hand, we went to great length to ensure that the experiments are *controlled*. Only under careful control can the experiments be convincing in validating the theory.\n\nNevertheless, as suggested by the reviewers, we are currently exploring other experiments that would further illustrate the tradeoff between invariance to nuisances and sufficiency as mediated by the coefficient \\beta. We will add these along with the further tests using the same architecture of Springerberg, as described above.\n\n>> The results on CIFAR-10 in Figure 3(b) seem to be on a validation set\n\nWe are using the same nomenclature of [KSW15], since we want to make a direct comparison with their experiment. As customary for CIFAR, the data is divided into a disjoint training set (50,000 samples) and validation/test set (10,000 samples). We feel that \"validation\" here is more appropriate.\n\n[KSW15] Diederik Kingma, Tim Salimans, and Max Welling, \"Variational Dropout and the Local Reparameterization Trick\", 2015\n\n[KW13] Diederik P Kingma, Max Welling, \"Auto-Encoding Variational Bayes\", 2013\n", "title": "More details on experiments and further tests"}, "rJuVwKH7x": {"type": "rebuttal", "replyto": "BJa0ECFxe", "comment": "A personal communication asked whether there are cases in which a stochastic representation of the data can obtain a better value of the IB Lagrangian than any deterministic representation; in response to this, we added a remark in Section 3 saying that this indeed can happen. \n\nIn response to a question by the reviewer, we added to Section 2 a few examples of nuisances that act as a group on the data.\n\nWe updated the MNIST and CIFAR experiments: all the qualitative results are the same as before, but we slightly changed the hyperparameters and the optimization method to provide a more accurate and fairer comparison between the algorithms.\n\nFinally, we added an appendix to fill a gap in the narrative between Equation (2),  where the two distributions in the KL term were the actual prior and posterior of z, and Section 4, where we assume an approximated prior whose parameters are learned independently. Specifically, we show that if the approximated prior of the activations is chosen to be factorized, as we do, then our loss function differs from the actual IB Lagrangian by the total correlation of z. As a consequence, our approximation is correct when the components of z are mutually independent, and the loss function we use actually encourages this independence.\n\nWe would like to thank all the people that gave us early feedback on the paper.", "title": "Paper update"}, "rkwomlEXx": {"type": "rebuttal", "replyto": "rJ_bWAyXg", "comment": "Thank you very much for your comments. Indeed, we think it would be interesting to study Information Dropout as an attention mechanism. Regarding the nuisances as a group action, we are going to expand the discussion section to that effect. In the meantime, here are some common examples of group actions:\n\n- Many tasks, for example object detection or classification, require invariance to local affine or perspective transformations of the image (like translations, rotations and scaling), and to changes in contrast and brightness (i.e. affine transforms of the pixel values). Therefore, these are all examples of finite dimensional groups that act as a nuisance on the image;\n\n- Similarly, small (diffeomorphic) deformations of the image, like those originating from a slight change of the point of view (ignoring possible occlusion effects), are a nuisance for many tasks, and have the structure of an infinite dimensional group;", "title": "Examples of nuisances that can be seen as groups"}, "rJ_bWAyXg": {"type": "review", "replyto": "BJa0ECFxe", "review": "It is very interesting how Information Dropout works almost like an attention mechanism! Paper is very well written, so no major clarification questions.\n\nIn the discussion on nuisances having the structure of a group action, could you please give some more intuition and examples about which nuisances can be seen as groups ?\nPaper summary\nThis paper develops a generalization of dropout using information theoretic\nprinciples. The basic idea is that when learning a representation z of input x\nwith the aim of predicting y, we must choose a z such that it carries the least\namount of information about x, as long as it can predict y. This idea can be\nformalized using the Information Bottleneck Lagrangian. This leads to an\noptimization problem which is similar to the one derived for variational\ndropout, the difference being that Information dropout allows for a scaling\nfactor associated with the KL divergence term that encourages noise. The amount\nof noise being added is made a parameterized function of the data and this\nfunction is optimized along with the rest of the model. Experimental results on\nCIFAR-10 and MNIST show (small) improvements over binary dropout.\n\nStrengths\n- The paper highlights an important conceptual link between probabilistic\n  variational methods and information theoretic methods, showing that dropout\ncan be generalized using both formalisms to arrive at very similar models.\n- The presentation of the model is excellent.\n- The experimental results on cluttered MNIST are impressive.\n\nWeaknesses\n- The results on CIFAR-10 in Figure 3(b) seem to be on a validation set (unless\n  the axis label is a typo). It is not clear why the test set was not used. This\nmakes it hard to compare to results reported in Springenberg et al, as well as\nother results in literature.\n\nQuality\nThe theoretical exposition is high quality. Figure 2 gives a nice qualitative\nassessment of what the model is doing. However, the experimental results\nsection can be made better, for example, by matching the results on CIFAR-10 as\nreported in Springenberg et al. and trying to improve on those using information\ndropout.\n\nClarity\nThe paper is well written and easy to follow.\n\nOriginality\nThe derivation of the information dropout optimization problem using IB\nLagrangian is novel. However, the final model is quite close to variational\ndropout.\n\nSignificance\nThis paper will be of general interest to researchers in representation learning\nbecause it highlights an alternative way to think about latent variables (as\ninformation bottlenecks). However, unless the model can be shown to achieve\nsignificant improvements over simple dropout, its wider impact is likely to be\nlimited.\n\nOverall\nThe paper presents an insightful theoretical derivation and good preliminary\nresults. The experimental section can be improved.\n\nMinor comments and suggestions -\n- expecially -> especially\n- trough -> through\n- There is probably a minus sign missing in the expression for H(y|z) above Eq (2).\n- Figure 3(a) has error bars, but 3(b) doesn't. It might be a good idea to have those\nfor Figure 3(b) as well.\n- Please consider comparing Figure 2 with the activity map of a standard CNN\n  trained with binary dropout, so we can see if similar filtering out is\nhappening there already.", "title": "Nuisances as group actions.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJL9MerEe": {"type": "review", "replyto": "BJa0ECFxe", "review": "It is very interesting how Information Dropout works almost like an attention mechanism! Paper is very well written, so no major clarification questions.\n\nIn the discussion on nuisances having the structure of a group action, could you please give some more intuition and examples about which nuisances can be seen as groups ?\nPaper summary\nThis paper develops a generalization of dropout using information theoretic\nprinciples. The basic idea is that when learning a representation z of input x\nwith the aim of predicting y, we must choose a z such that it carries the least\namount of information about x, as long as it can predict y. This idea can be\nformalized using the Information Bottleneck Lagrangian. This leads to an\noptimization problem which is similar to the one derived for variational\ndropout, the difference being that Information dropout allows for a scaling\nfactor associated with the KL divergence term that encourages noise. The amount\nof noise being added is made a parameterized function of the data and this\nfunction is optimized along with the rest of the model. Experimental results on\nCIFAR-10 and MNIST show (small) improvements over binary dropout.\n\nStrengths\n- The paper highlights an important conceptual link between probabilistic\n  variational methods and information theoretic methods, showing that dropout\ncan be generalized using both formalisms to arrive at very similar models.\n- The presentation of the model is excellent.\n- The experimental results on cluttered MNIST are impressive.\n\nWeaknesses\n- The results on CIFAR-10 in Figure 3(b) seem to be on a validation set (unless\n  the axis label is a typo). It is not clear why the test set was not used. This\nmakes it hard to compare to results reported in Springenberg et al, as well as\nother results in literature.\n\nQuality\nThe theoretical exposition is high quality. Figure 2 gives a nice qualitative\nassessment of what the model is doing. However, the experimental results\nsection can be made better, for example, by matching the results on CIFAR-10 as\nreported in Springenberg et al. and trying to improve on those using information\ndropout.\n\nClarity\nThe paper is well written and easy to follow.\n\nOriginality\nThe derivation of the information dropout optimization problem using IB\nLagrangian is novel. However, the final model is quite close to variational\ndropout.\n\nSignificance\nThis paper will be of general interest to researchers in representation learning\nbecause it highlights an alternative way to think about latent variables (as\ninformation bottlenecks). However, unless the model can be shown to achieve\nsignificant improvements over simple dropout, its wider impact is likely to be\nlimited.\n\nOverall\nThe paper presents an insightful theoretical derivation and good preliminary\nresults. The experimental section can be improved.\n\nMinor comments and suggestions -\n- expecially -> especially\n- trough -> through\n- There is probably a minus sign missing in the expression for H(y|z) above Eq (2).\n- Figure 3(a) has error bars, but 3(b) doesn't. It might be a good idea to have those\nfor Figure 3(b) as well.\n- Please consider comparing Figure 2 with the activity map of a standard CNN\n  trained with binary dropout, so we can see if similar filtering out is\nhappening there already.", "title": "Nuisances as group actions.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}