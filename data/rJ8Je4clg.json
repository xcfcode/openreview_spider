{"paper": {"title": "Learning to Play in a Day: Faster Deep Reinforcement Learning by Optimality Tightening", "authors": ["Frank S.He", "Yang Liu", "Alexander G. Schwing", "Jian Peng"], "authorids": ["frankheshibi@gmail.com", "liu301@illinois.edu", "aschwing@illinois.edu", "jianpeng@illinois.edu"], "summary": "We propose a novel training algorithm for reinforcement learning which combines the strength of deep Q-learning with a constrained optimization approach to tighten optimality and encourage faster reward propagation.", "abstract": "We propose a novel training algorithm for reinforcement learning which combines the strength of deep Q-learning with a constrained optimization approach to tighten optimality and encourage faster reward propagation. Our novel technique makes deep reinforcement learning more practical by drastically reducing the training time.  We evaluate the performance of our approach on the 49 games of the challenging Arcade  Learning Environment, and report significant improvements in both training time and accuracy.", "keywords": ["Reinforcement Learning", "Optimization", "Games"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors have proposed an improvement to q-learning which imposes upper and lower constraint bounds on the q-function, which are implemented using quadratic penalties. This speeds up learning, at least for a limited set of Atari games. The reviewers are strongly divided on the merits of this paper. The authors have worked to be responsive to the reviewers' concerns, however, and the paper has been improved, so I side with the positives."}, "review": {"BkngXNlPx": {"type": "rebuttal", "replyto": "SJ8uwSGVx", "comment": "We updated our revision to contain a proof about stochastic MDPs and the latest results that we were able to get. We provide a more detailed summary of our modifications in a comment to the AC. We will continue to evaluate our method. Thanks so much for your time and consideration.", "title": "latest results"}, "rJA0MNeDg": {"type": "rebuttal", "replyto": "Ske_zvGNl", "comment": "We updated our revision to contain a proof about stochastic MDPs and the latest results that we were able to get. We provide a more detailed summary of our modifications in a comment to the AC. We will continue to evaluate our method. Thanks so much for your time and consideration.", "title": "latest results"}, "ryXTM4lvl": {"type": "rebuttal", "replyto": "BJhbTXKEx", "comment": "We updated our revision to contain a proof about stochastic MDPs and the latest results that we were able to get. We provide a more detailed summary of our modifications in a comment to the AC. We will continue to evaluate our method. Thanks so much for your time and consideration.", "title": "latest results"}, "Hymsf4xvg": {"type": "rebuttal", "replyto": "BJA7mlaBl", "comment": "We updated our revision to contain a proof about stochastic MDPs and the latest results that we were able to get. We provide a more detailed summary of our modifications in a comment to the AC. We will continue to evaluate our method. Thanks so much for your time and consideration.", "title": "latest results"}, "rysaBQTUg": {"type": "rebuttal", "replyto": "SJ8uwSGVx", "comment": "We thank the reviewer for the encouraging feedback and suggestions.\n\nWe updated the submission and included material regarding stochastic MDPs.\n\nAbout qualitative results:\nWe are in the process of generating side-by-side videos to best assess where the proposed penalties yield to improvements and where the proposed penalties harm. Stability of DQN and our proposed extension is about the same. We will include standard deviations to demonstrate this.\n\nAbout extensions:\nWe plan to extend this work to D-DQN, DDPG, NAF and other methods. We plan to provide some of them for the final version. We will provide other results in an extended report once they are ready.\n\nJudging from the learning curves it looks like the algorithm is still improving after 10M frames. By how much is hard to assess at this point in time.", "title": "Re: review"}, "rJQ34QTUe": {"type": "rebuttal", "replyto": "BJhbTXKEx", "comment": "We thank the reviewer for the encouraging feedback and suggestions.\n\nAbout deterministic vs stochastic MDPs:\nWe updated the submission and added a discussion about stochastic MDPs to the appendix.\n\nAbout theoretical analysis:\nA theoretical analysis should follow the arguments of Q-learning. Usage of non-convex Q-functions likely makes any proofs hard though. We can add a discussion if really desired.\n\nAbout Retrace/Tree backup:\nAt this point we haven\u2019t compared to Retrace or Tree backup by Precup et al. Since there is no public implementation of Retrace/Tree backup, as suggested by other reviewers, we will compare our method to several other baselines, such as n-step Q-learning. It is also worth noting that in the Retrace/Tree backup paper, the Q*(\\lambda) algorithm seems to not work that well on Atari games. We will add those results as soon as they become available. Please keep in mind that we have some resource constraints.", "title": "Re: Review"}, "BJmkf7pUx": {"type": "rebuttal", "replyto": "Ske_zvGNl", "comment": "We thank the reviewer for the feedback and suggestions.\n\nAbout convergence:\nSince we are using non-convex functions, a convergence result would not be applicable. Note that this is generally true for all DQN approaches, and ours is no different.\n\nAbout bounds:\nNote that the max over the different lower bounds is not necessarily attained for close by steps (k=1). Particularly at the beginning of training the tightest bounds are actually obtained for large values of k. Intuitively this can be understood when considering high rewards in a maze environment that have been propagated to MDP states a few locations away. Hence we disagree with R1s claim that the maximum lower bound arises from neighboring states, i.e., the max is L_j,1.\n\nNote that L_j,0 is the term used in the standard DQN cost function, i.e., y_j = L_j,0, and we aim at regressing to that value. In fact we could include it as an additional constraint, e.g., as both a lower bound and an upper bound. This would only change the relative weight between the terms however, hence no need to further pursue this direction.\n\nUsage of a target network to compute y_j = L_j,0 is common practice. Hence it seems intuitive to follow the same idea for the bounds.\n\nAbout empirical evaluation:\nWe decided to focus on a comparison with vanilla DQN because the proposed technique can be combined with almost all of the later improvements, e.g., n-step Q-learning, actor-critic methods, double-DQN etc. In addition, we felt it is important to assess the differences to the core technique on a large number of tasks. Therefore we conduct experiments on a variety of different benchmarks as opposed to cherry-picking a few examples. We think using few benchmarks and many algorithms is just as reasonable as using many benchmarks and few algorithms. Both is unfortunately not possible given our current resource constraints. We are in the process of performing additional experiments which we will add as they become available.", "title": "Re: Intriguing idea"}, "rklc53r8l": {"type": "rebuttal", "replyto": "SJxEv2rLg", "comment": "Thank you for all your and other reviewers' insightful comments. We hope to address them and improve our paper.\nWe have been working on the combination of our method and other useful techniques to check whether they are complementary. We have been also implementing other baselines, such as n-step Q learning. Due to the high computational demand of these experiments, we will try to include as many results as possible in the upcoming revision. In addition, we will also include discussion and justification of our approach for stochastic MDPs.", "title": "Yes, we have been working on the new version"}, "BJA7mlaBl": {"type": "rebuttal", "replyto": "rJ8Je4clg", "comment": "This is a very interesting paper about a new way of doing faster reward propagation in reinforcement learning. My one major worry about the paper is that the experiments don\u2019t show that the main new idea is what actually leads to the improvement over DQN/D-DQN.\n\nThe proposed final objective in Equation 4 augments the standard Q-learning loss with two terms. Each of the additional terms is a one-sided L2 loss between the estimated Q-value and a certain n-step return.\n\nn-step returns and lambda returns have long been used to speed up reward propagation in reinforcement learning. The difference is that they have been previously used with the standard (two-sided) L2 loss. Their benefits have been shown in the classical RL literature (see [1], [2]) and more recently with deep architectures (see [3], [4], [5]). For example, [4] showed clear benefits from using n-step returns on Atari and TORCS and Retrace(lambda) [5] significantly outperformed DQN on Atari.\n\nSince this paper is really proposing a different way of using n-step returns, I think it\u2019s important to compare it to the standard ways of using them. Have you done any comparisons to DQN with n-step returns as targets or any other return-based method? It would also be good to empirically show what the effect of including the full return R_j into the lower bound is. It is only glossed over in the experiments but seems like it could be very important.\n\n[1] \u201cLearning from Delayed Rewards\u201d, Christopher Watkins (1989)\n[2] \u201cGeneralization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding\u201d, Richard S. Sutton (1996)\n[3] \u201cHigh-Dimensional Continuous Control Using Generalized Advantage Estimation\u201d, John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel (2015)\n[4] \u201cAsynchronous Methods for Deep Reinforcement Learning\u201d, Volodymyr Mnih, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, Koray Kavukcuoglu (2016)\n[5] \u201cSafe and Efficient Off-Policy Reinforcement Learning\u201d, R\u00e9mi Munos, Tom Stepleton, Anna Harutyunyan, Marc G. Bellemare (2016)", "title": "Missing comparisons"}, "BJ5FGclEg": {"type": "rebuttal", "replyto": "S1qk6Zl4x", "comment": "In the paper, we assume that the environmental dynamics are deterministic, i.e., the new state and reward are solely decided by the current state and action. The upper bounds and lower bounds are then also deterministic. For stochastic MDPs, we can still formulate a similar constrained optimization problem but with constraints that hold for the expected value of the environmental dynamics. Then we can still apply the quadratic penalty method to remove these constraints and formulate the objective function with expectations in the penalty functions. In this way, we apply the Jensen's inequality to move the environmental expectation operator outside of the quadratic function and show that one can apply stochastic gradient descent algorithm to minimize the relaxed upper bound of the objective function. Indeed, the deterministic objective function in the current paper can be seen as an empirical upper bound of the objective function in the stochastic setting. In this way, we can apply the optimality tightening to stochastic MDPs. We will incorporate this discussion and the corresponding mathematical derivations in our next revision.", "title": "Thank you for your question. Yes, we have the following justification of our method for stochastic environments."}, "S1qk6Zl4x": {"type": "review", "replyto": "rJ8Je4clg", "review": "I apologize for posting a question at this late juncture. But I was wondering about the meaning of the bounds in stochastic environments. For example, if either the reward function or the state transition is stochastic, then the bounds proposed in this paper would no longer be valid bounds. \n\nEmpirically, this may not matter. But I was wondering whether the authors have any thoughts on this. Thanks!In this paper, the authors proposed a extension to the DQN algorithm by introducing both an upper and lower bound to the optimal Q function. The authors show experimentally that this approach improves the data efficiency quite dramatically such that they can achieve or even supersede the performance of DQN that is trained in 8 days. \n\nThe idea is novel to the best of my knowledge and the improvement over DQN seems very significant. \n\nRecently, Remi et al have introduced the Retrace algorithm which can make use of multi-step returns to estimate Q values. As I suspect, some of the improvements that comes from the bounds is due to the fact that multi-step returns is used effectively. Therefore, I was wondering whether the authors have tried any approach like Retrace or Tree backup by Precup et al. and if so how do these methods stack up against the proposed method.\n\nThe author have very impressive results and the paper proposes a very promising direction for future research and as a result I would like to make a few suggestions:\n\nFirst, it would be great if the authors could include a discussion about deterministic vs stochastic MDPs. \n\nSecond, it would be great if the authors could include some kind of theoretically analysis about the approach.\n\nFinally, I would like to apologize for the late review.", "title": "Very late question.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJhbTXKEx": {"type": "review", "replyto": "rJ8Je4clg", "review": "I apologize for posting a question at this late juncture. But I was wondering about the meaning of the bounds in stochastic environments. For example, if either the reward function or the state transition is stochastic, then the bounds proposed in this paper would no longer be valid bounds. \n\nEmpirically, this may not matter. But I was wondering whether the authors have any thoughts on this. Thanks!In this paper, the authors proposed a extension to the DQN algorithm by introducing both an upper and lower bound to the optimal Q function. The authors show experimentally that this approach improves the data efficiency quite dramatically such that they can achieve or even supersede the performance of DQN that is trained in 8 days. \n\nThe idea is novel to the best of my knowledge and the improvement over DQN seems very significant. \n\nRecently, Remi et al have introduced the Retrace algorithm which can make use of multi-step returns to estimate Q values. As I suspect, some of the improvements that comes from the bounds is due to the fact that multi-step returns is used effectively. Therefore, I was wondering whether the authors have tried any approach like Retrace or Tree backup by Precup et al. and if so how do these methods stack up against the proposed method.\n\nThe author have very impressive results and the paper proposes a very promising direction for future research and as a result I would like to make a few suggestions:\n\nFirst, it would be great if the authors could include a discussion about deterministic vs stochastic MDPs. \n\nSecond, it would be great if the authors could include some kind of theoretically analysis about the approach.\n\nFinally, I would like to apologize for the late review.", "title": "Very late question.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SknNcV57l": {"type": "rebuttal", "replyto": "rJQuhCFme", "comment": "Thank you for your questions. Please see our answers.\n1. Using the quadratic penalty function method, the objective function can be different during training. When there were violated constraints, we added extra quadratic function(s) to the objective function, and thus the scale (or norm) of gradient can usually be larger. This might cause instability in gradient descent, e.g. RMSprop used in this paper. To make training more stable, we multiplied the gradient by 1/(1+\\lambda) or 1/(1+2*lambda) if only one bound was considered or two bounds were considered, respectively. We found that this rescaling technique improved the training in our experiments. We believe that there are other possible approaches to address this practical issue. We will make this clear in the next version.\n2. We compared our method to the vanilla DQN, which is the baseline considered in this paper. D-DQN was used as a reference here, not for comparison. We will change Table 1 in the next version.", "title": "Thanks for your questions. Please see our response below."}, "rJQuhCFme": {"type": "review", "replyto": "rJ8Je4clg", "review": "Hi,\n\nCould you please clarify the following:\n- What does this means precisely: \"Gradients are also rescaled so that their magnitudes are comparable with or without penalty.\"\n- In Table 1 why is the top right number in bold even if not the best result?\n\nThanks!In this paper, a Q-Learning variant is proposed that aims at \"propagating\" rewards faster by adding extra costs corresponding to bounds on the Q function, that are based on both past and future rewards. This leads to faster convergence, as shown on the Atari Learning Environment benchmark.\n\nThe paper is well written and easy to follow. The core idea of using relaxed inequality bounds in the optimization problem is original to the best of my knowledge, and results seem promising.\n\nThis submission however has a number of important shortcomings that prevent me from recommending it for publication at ICLR:\n\n1. The theoretical justification and analysis is very limited. As far as I can tell the bounds as defined require a deterministic reward to hold, which is rarely the case in practice. There is also the fact that the bounds are computed using the so-called \"target network\" with different parameters theta-, which is another source of discrepancy. And even before that, the bounds hold for Q* but are applied on Q for which they may not be valid until Q gets close enough to Q*. It also looks weird to take the max over k in (1, ..., K) when the definition of L_j,k makes it look like the max has to be L_j,1 (or even L_j,0, but I am not sure why that one is not considered), since L*_j,0 >= L*_j,1 >= ... >= L*_j,K. Neither of these issues are discussed in the paper, and there is no theoretical analysis of the convergence properties of the proposed method.\n\n[Update: some of these concerns were addressed in OpenReview comments]\n\n2. The empirical evaluation does not compensate, in my opinion, for the lack of theory. First, since there are two bounds introduced, I would have expected \"ablative\" experiments showing the improvement brought by each one independently. It is also unfortunate that the authors did not have time to let their algorithm run longer, since as shown in Fig. 1 there remain a significant amount of games where it performs worse compared to DQN. In addition, comparisons are limited to vanilla DQN and DDQN: I believe it would have been important to compare to other ways of incorporating longer-term rewards, like n-step Q-Learning or actor-critic. Finally, there is no experiment demonstrating that the proposed algorithm can indeed improve other existing DQN variants: I agree with the author when they say \"We believe that our method can be readily combined with other techniques developed for DQN\", however providing actual results showing this would have made the paper much stronger.\n\nIn conclusion, I do believe this line of research is worth pursuing, but also that additional work is required to really prove and understand its benefits.\n\nMinor comments:\n- Instead of citing the arxiv Wang et al (2015), it would be best to cite the 2016 ICML paper\n- The description of Q-Learning in section 3 says \"The estimated future reward is computed based on the current state s or a series of past states s_t if available.\" I am not sure what you mean by \"a series of past states\", since Q is defined as Q(s, a) and thus can only take the current state s as input, when defined this way.\n- The introduction of R_j in Alg. 1 is confusing since its use is only explained later in the text (in section 5 \"In addition, we also incorporate the discounted return R_j in the lower bound calculation to further stabilize the training\")\n- In Fig. S1 the legend should not say \"10M\" since the plot is from 1M to 10M", "title": "Small clarifications", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Ske_zvGNl": {"type": "review", "replyto": "rJ8Je4clg", "review": "Hi,\n\nCould you please clarify the following:\n- What does this means precisely: \"Gradients are also rescaled so that their magnitudes are comparable with or without penalty.\"\n- In Table 1 why is the top right number in bold even if not the best result?\n\nThanks!In this paper, a Q-Learning variant is proposed that aims at \"propagating\" rewards faster by adding extra costs corresponding to bounds on the Q function, that are based on both past and future rewards. This leads to faster convergence, as shown on the Atari Learning Environment benchmark.\n\nThe paper is well written and easy to follow. The core idea of using relaxed inequality bounds in the optimization problem is original to the best of my knowledge, and results seem promising.\n\nThis submission however has a number of important shortcomings that prevent me from recommending it for publication at ICLR:\n\n1. The theoretical justification and analysis is very limited. As far as I can tell the bounds as defined require a deterministic reward to hold, which is rarely the case in practice. There is also the fact that the bounds are computed using the so-called \"target network\" with different parameters theta-, which is another source of discrepancy. And even before that, the bounds hold for Q* but are applied on Q for which they may not be valid until Q gets close enough to Q*. It also looks weird to take the max over k in (1, ..., K) when the definition of L_j,k makes it look like the max has to be L_j,1 (or even L_j,0, but I am not sure why that one is not considered), since L*_j,0 >= L*_j,1 >= ... >= L*_j,K. Neither of these issues are discussed in the paper, and there is no theoretical analysis of the convergence properties of the proposed method.\n\n[Update: some of these concerns were addressed in OpenReview comments]\n\n2. The empirical evaluation does not compensate, in my opinion, for the lack of theory. First, since there are two bounds introduced, I would have expected \"ablative\" experiments showing the improvement brought by each one independently. It is also unfortunate that the authors did not have time to let their algorithm run longer, since as shown in Fig. 1 there remain a significant amount of games where it performs worse compared to DQN. In addition, comparisons are limited to vanilla DQN and DDQN: I believe it would have been important to compare to other ways of incorporating longer-term rewards, like n-step Q-Learning or actor-critic. Finally, there is no experiment demonstrating that the proposed algorithm can indeed improve other existing DQN variants: I agree with the author when they say \"We believe that our method can be readily combined with other techniques developed for DQN\", however providing actual results showing this would have made the paper much stronger.\n\nIn conclusion, I do believe this line of research is worth pursuing, but also that additional work is required to really prove and understand its benefits.\n\nMinor comments:\n- Instead of citing the arxiv Wang et al (2015), it would be best to cite the 2016 ICML paper\n- The description of Q-Learning in section 3 says \"The estimated future reward is computed based on the current state s or a series of past states s_t if available.\" I am not sure what you mean by \"a series of past states\", since Q is defined as Q(s, a) and thus can only take the current state s as input, when defined this way.\n- The introduction of R_j in Alg. 1 is confusing since its use is only explained later in the text (in section 5 \"In addition, we also incorporate the discounted return R_j in the lower bound calculation to further stabilize the training\")\n- In Fig. S1 the legend should not say \"10M\" since the plot is from 1M to 10M", "title": "Small clarifications", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryXRTxb7g": {"type": "rebuttal", "replyto": "Hyq5B0kQl", "comment": "Currently, we know the optimality tightening propagates reward faster thus achieves faster convergence on most of the games. However, as the mechanism under different games varies a lot, we are still studying on the intuition behind the result.", "title": "Thank you for your good question. Please see our answer below."}, "SJ4oclbXx": {"type": "rebuttal", "replyto": "B1wIpjRzl", "comment": "Yes, we strictly follow the Nature paper of Mnih et al. (2015) clipping the reward and temporal difference error term.", "title": "Please see our answer below."}, "Hyq5B0kQl": {"type": "review", "replyto": "rJ8Je4clg", "review": "Do you have any intuition or observations regarding which games benefit the most or the least from your proposed method?This paper proposes an improvement to the q-learning/DQN algorithm using constraint bounds on the q-function, which are implemented using quadratic penalties in practice. The proposed change is simple to implement and remarkably effective, enabling both significantly faster learning and better performance on the suite of Atari games.\n\nI have a few suggestions for improving the paper:\nThe paper could be improved by including qualitative observations of the learning process with and without the proposed penalties, to better understand the scenarios in which this method is most useful, and to develop a better understanding of its empirical performance.\n\nIt would also be nice to include zoomed-out versions of the learned curves in Figure 3, as the DQN has yet to converge. Error bars would also be helpful to judge stability over different random seeds.\n\nAs mentioned in the paper, this method could be combined with D-DQN. It would be interesting to see this combination, to see if the two are complementary. Do you plan to do this in the final version?\n\nAlso, a couple questions:\n- Do you think the performance of this method would continue to improve after 10M frames?\n- Could the ideas in this paper be extended to methods for continuous control like DDPG or NAF?", "title": "Qualitative observations?", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJ8uwSGVx": {"type": "review", "replyto": "rJ8Je4clg", "review": "Do you have any intuition or observations regarding which games benefit the most or the least from your proposed method?This paper proposes an improvement to the q-learning/DQN algorithm using constraint bounds on the q-function, which are implemented using quadratic penalties in practice. The proposed change is simple to implement and remarkably effective, enabling both significantly faster learning and better performance on the suite of Atari games.\n\nI have a few suggestions for improving the paper:\nThe paper could be improved by including qualitative observations of the learning process with and without the proposed penalties, to better understand the scenarios in which this method is most useful, and to develop a better understanding of its empirical performance.\n\nIt would also be nice to include zoomed-out versions of the learned curves in Figure 3, as the DQN has yet to converge. Error bars would also be helpful to judge stability over different random seeds.\n\nAs mentioned in the paper, this method could be combined with D-DQN. It would be interesting to see this combination, to see if the two are complementary. Do you plan to do this in the final version?\n\nAlso, a couple questions:\n- Do you think the performance of this method would continue to improve after 10M frames?\n- Could the ideas in this paper be extended to methods for continuous control like DDPG or NAF?", "title": "Qualitative observations?", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1wIpjRzl": {"type": "rebuttal", "replyto": "rJ8Je4clg", "comment": "May I ask if you used clip gradient in your algorithm like what Deepmind did?\n\n", "title": "About clip gradient"}, "rk6FJmCfx": {"type": "rebuttal", "replyto": "HJ1ae7sze", "comment": "It is interesting to know that the naive method leads to overestimation and does not perform as well as your approach. It appears that we should indeed impose more structural constraints derived from the Bellman equation. \n\nI am looking forward to your latest results!", "title": "Very interesting results"}, "HJ1ae7sze": {"type": "rebuttal", "replyto": "B1M84zOzx", "comment": "1. How many random seeds did you use for each game?\n- We used 4 random seeds for each game.\u2028\n2. How is the return $R_j$ used to improve the lower bound estimate?\n- $R_j$ provides an estimate for the lower bound $L_{j,\\infty}$. So we include it in our implementation as another lower bound. This is stated in the second paragraph of the experimental section 5: ``In addition, we also incorporate the discounted return $R_j$ in the lower bound calculation to further stabilize the training.\u2019\u2019 We\u2019ll clarify this in a revised version.\u2028\n3. Would a naive method using target $y_j = \\max(R_j, r_j + \\gamma \\max_a Q_{\\theta^-}(s_{j+1,a}) )$ work equally well? This method also seems to propagate sparse rewards across many time steps.\n- Indeed we tested this approach. It worked better than DQN on many games but not as good as the proposed method. In addition, we also noticed overestimation with this naive approach. \u2028\n4. How does the performance at 200M frames compare to DQN trained after 200M frames?\n- Due to limited computational resources, we were not able to run our methods on all Atari games for 200M frames. It would be interesting to compare them in the future.", "title": "Thanks for your interest. Please see our response below"}, "B1M84zOzx": {"type": "rebuttal", "replyto": "rJ8Je4clg", "comment": "The results are very interesting! The method is also simple and easy to implement.\n\nIt would be good to understand / elaborate the following technical details so that readers can understand more about your approach.\n1. How many random seeds did you use for each game?\n2. How is the return $R_j$ used to improve the lower bound estimate?\n3. Would a naive method using target $y_j = \\max(R_j, r_j + \\gamma \\max_a Q_{\\theta^-}(s_{j+1,a}) )$ work equally well? This method also seems to propagate sparse rewards across many time steps.\n4. How does the performance at 200M frames compare to DQN trained after 200M frames?\n\nYour feedback is greatly appreciated!", "title": "A few technical questions"}, "HydnfaQ-e": {"type": "rebuttal", "replyto": "HyMe55QWe", "comment": "1. Please refer to the method section in the Nature paper of Mnih et al. (2015): \u201cMore precisely, the agent sees and selects actions on every kth frame instead of every frame, and its last action is repeated on skipped frames.\u201d 50 million training frames/steps and k = 4 were used. Therefore, 200M frames in total were generated during training.\n\n2. In addition, since the games were run with 60Hz and the total approximate game experience is 38days (see again method section of Mnih et al. (2015)), one can easily infer that there were 200M \\approx 38days*86400sec/day*60frames/sec in total generated from the environment.\n\n3. Please check the Hyper-parameters section in the supplementary information of the double DQN paper (van Hasselt et al. (2015): ``Deep Reinforcement Learning with Double Q-learning\u2019\u2019): ``Training is done over 50M steps (i.e., 200M frames).\u2019\u2019", "title": "Please see our answer below."}, "HyMe55QWe": {"type": "rebuttal", "replyto": "rJ8Je4clg", "comment": "Mnih numbers are reported after 50M frames but this paper (Table S1) mentions it was after 200M. Where this number comes from?", "title": "incostinet comparision with Mnih 2015"}}}