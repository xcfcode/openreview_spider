{"paper": {"title": "Learning Compact Embedding Layers via Differentiable Product Quantization", "authors": ["Ting Chen", "Lala Li", "Yizhou Sun"], "authorids": ["iamtingchen@gmail.com", "lala@google.com", "yzsun@cs.ucla.edu"], "summary": "We propose a differentiable product quantization framework that can reduce the size of embedding layer in an end-to-end training at no performance cost.", "abstract": "Embedding layers are commonly used to map discrete symbols into continuous embedding vectors that reflect their semantic meanings. Despite their effectiveness, the number of parameters in an embedding layer increases linearly with the number of symbols and poses a critical challenge on memory and storage constraints. In this work, we propose a generic and end-to-end learnable compression framework termed differentiable product quantization (DPQ). We present two instantiations of DPQ that leverage different approximation techniques to enable differentiability in end-to-end learning. Our method can readily serve as a drop-in alternative for any existing embedding layer. Empirically, DPQ offers significant compression ratios (14-238x) at negligible or no performance cost on 10 datasets across three different language tasks.", "keywords": ["efficient modeling", "compact embedding", "embedding table compression", "differentiable product quantization"]}, "meta": {"decision": "Reject", "comment": "The presented paper gives a differentiable product quantization framework to compress embedding and support the claim by experiments (the supporting materials are as large as the paper itself). Reviewers agreed that the idea is simple is interesting, and also nice and positive discussion appeared. However, the main limiting factor is the small novelty over Chen 2018b, and I agree with that. Also, the comparison with low rank is rather formal: of course it would be of full rank , as the authors claim in the answer, but looking at singular values is needed to make this claim. Also, one can use low-rank tensor factorization to compress embeddings, and this can be compared. \nTo summarize, I think the contribution is not enough to be accepted."}, "review": {"BJg9ZaL2iB": {"type": "rebuttal", "replyto": "ByxFark3sB", "comment": "Dear reviewer,\n\nThanks for the prompt response!\n\nWe think it is reasonable that our method can achieve better task performance than full embedding in certain tasks/datasets, because DPQ can implicitly regularize the model with more efficient parameterization. We have observed this phenomenon consistently on some tasks (LM on PTB, Wikitext-2) with multiple runs and different random seeds (excluding the possibility of noises).\n\nThanks again for another nice suggestion. We manually checked the rank of the reconstructed embedding matrix with numerical method on the NMT/WMT-19(EN-DE) and it is indeed full rank (512 for 32000x512 matrix).\n\nWe firmly believe our work is novel and can make a positive contribution to the community. The major novelty is to **formulate discrete codes via Product Quantization and making it generally end-to-end differentiable**.\n\nIt only bears resemblance to (Chen et al, 2018b) in terms of using discrete codes to tackle the embedding compression problem. However our formulation and training techniques are very different. The product quantization formulation is far more flexible and efficient than (Chen et al, 2018b): it allows optimization with various approximation techniques (SX, VQ), and also allows one-pass end-to-end training whereas (Chen et al, 2018b) is still constrained by an extra distillation procedure. This new formulation led to SOTA empirical results by a large margin. Potentially it can also be applied beyond embedding layer compression (e.g. for dense or conv layers via end-to-end product quantization of weights).\n\nPlease feel free to let us know if there are any remaining concerns, we are happy to further clarify!", "title": "Thanks for the prompt response!"}, "r1eBRjEooB": {"type": "rebuttal", "replyto": "BJxbOlSKPr", "comment": "We have updated the paper with the following changes:\n\n    1) We provided detailed analytical comparisons between our work and Chen et al 2018b and other traditional compression techniques in Appendix F as suggested by Reviewer #1 and #2.\n    2) We added more empirical comparisons of our work with a broader set of baselines on more tasks in Appendix G as suggested by Reviewer #2 and #3.\n    3) We revised Theorem 1 and made some clarifications as suggested by Reviewer #3.\n    4) We added new results on BERT compression in Appendix H.\n", "title": "Revision"}, "BkeJQoNooH": {"type": "rebuttal", "replyto": "Hyl2WZpf5H", "comment": "Thank you for your time and the detailed comments! Please see below for clarifications and more empirical evidence.\n\n[Significance and Novelty]\n\nWe believe in the significance of embedding compression for inference/deployment time, because embedding parameters make up a large part of model parameters in a wide variety of models. For example, 95% of parameters in the medium-sized LSTM (refer to Paragraph 1 of Section 1) are embedding parameters; 99% in the MLP-based model for text classification, 22% in the BERT-base model, etc.\n\nTo our best knowledge, previous efforts offer up to ~50x compression ratios without performance loss, while our work achieve SOTA compression ratios of up to ~200x on 10 datasets across three different language tasks (we also have new results on BERT in Section H of the Appendix). We argue that these are empirically significant results.\n\nNovel contributions are made compared to Chen et al., 2018b's, and these contributions led to performance improvement by very large margins (Table 3 and 4). Their work and our work present very different methodologies. Here we list the major differences:\n\n    1) In their work, discrete codes are directly associated with each of the symbols; in this work, discrete codes are computed as outcome of product quantization. \n    2) Our formulation of discrete codes with product quantization allows us to derive two variants with different approximation techniques (softmax-based and vector quantization-based).\n    3) Our method uses a novel composition function (inspired by product quantization), which is much more efficient than before (smaller memory footprint and less computation time overhead, Figure 4).\n\n[Experiments and Why Discrete Codes (aka KD codes)]\n\nRe \u201cthere's been a lack of explanation as to why this should be done only through KD codes\u201d.\n\nFirst of all, we have added more comparisons to conventional methods in the Appendix G. Table 9 shows comparisons with scalar quantization, product quantization, low-rank factorization, as well as other discrete code baselines (Shu and Nakayama 2017 & Chen 2018b) for language modeling. Table 10 shows similar comparisons on text classification tasks. These results show that previous methods struggle to maintain task performance when trying to achieve good compression ratios, and our method DPQ outperforms them by large margins.\n\nThen, we provide more analysis on why our methods work better. Unlike traditional quantization techniques that accumulate and amplify quantization error, our method makes it end-to-end differentiable so that the neural nets can adapt to quantization error. DPQ also relates to factorization-based method, but DPQ can produce high-rank embedding table with sparse factorization, so it is more expressive (Theorem 1 in Section 2.1). More analysis with conventional approaches is in Appendix F.\n\nRe \u201cIt would be great to show the change in PPL according to the compression ratio of DPQ models.\u201d\n\nFigure 3, 7 and 8 show the trade-offs between task performance (PPL, BLEU or accuracy) and compression ratios for different sizes of K and D.\n\nRe \u201cCan we apply it to pre-trained models like BERT?\u201d\n\nYes. We added experiments on BERT (Appendix H). Without any hyperparameter tuning, DPQ could compress the embedding layer 37x with negligible loss in performance. \n\nRe \u201cDid you run all experiments just one time? There is no confidence interval.\u201d\n\nSome of our experiments are computationally expensive (e.g. days-long). However we did repeat experiments where resources allowed, e.g. the PTB language modeling experiments and the WMT\u201919 En-De experiments, and found that the results were stable (e.g. std=0.6 over 4 runs for PPL in PTB LM). In the paper, we follow recent evaluation protocol in these tasks (e.g. [Shu and Nakayama 2017, Vaswani et al 2017]) and left out confidence intervals.\n\n[Other comments]\n\nRe \u201cwhy the distilling in Chen et al., 2018b is a problem?\u201d\n\nDistillation leads to more computation cost and in practice a more complex pipeline. Training with distillation requires pre-training of the embedding layer, which means the same model has to be trained twice (more computation). We also have to maintain two embedding tables for distillation (more memory).\n\nRe \u201cIt is usually not necessary to train the entire embedding vector on GPU, so it would not be a big issue in the actual learning process.\u201d\n\nOur goal is to reduce the embedding table size at the inference/deployment stage. E.g. we would be able to compress big models so that they can be deployed on mobile devices. It is not the goal of this work to improve training of embeddings.\n\nWe hope we have provided better explanation and more evidence for the contribution of this work, and are happy to address any further concerns!", "title": "Response"}, "S1erwsNjoH": {"type": "rebuttal", "replyto": "Hyet4X5p_S", "comment": "Thank you for your time, careful evaluation and valuable suggestions!\n\n[Theorem 1]\n\nThank you for pointing this out! We have revised the statement of Theorem 1. Conditioned on B and the sub-matrices of V being full rank, the reconstructed matrix H would be full rank. The original conclusion stays unchanged: despite that DPQ uses fewer bits in its parameterization than the conventional full embedding, its resulting embedding matrix can still be full rank, which is more expressive than factorization methods via low-rank.\n\n[Experiments]\n\nWe added more comparisons and ablations in the revision which we hope will address the concerns.\n\n**Post-training embedding compression.** \nAs suggested by the reviewer, one could first train the full embedding model and then compress the full embedding table into discrete codes. There are two approaches:\n    1) Fix the discrete codes and re-train the model parameters. This is the \"pre-train\" method in Paragraph 2 of Section 3.1. Table 4 shows that the performance is not as good as DPQ.\n    2) Use the discrete codes and the reconstructed embeddings directly with the original model (we believe this is what the reviewer suggested). We tested this idea on the NMT task and presented the results in Table 11 in Appendix G. For the same compression ratios, task performance is notably worse than DPQ. This is most likely due to small approximation errors in the embedding layer accumulating over layers.\n\n**Decoupling K and V for DPQ-SX.**\nFigure 12 (Appendix G4) shows that with K=V in DPQ-SX, it incurs a tiny performance (PPL) loss, but still performs slightly better than DPQ-VQ in LM task. Intuitively, we don\u2019t think we should tie K and V for DPQ-SX, as K is used to compute the probability (using dot product as proximity metric) over elements on V. However, it seems the downstream model may be able to adapt to this change of parameterization (and therefore only a slight performance loss). \n\n**More baselines; more tasks.** \nIn Table 9 and 10 we show comparison between DPQ and more existing methods on LM and text classification tasks. Our methods outperforms baselines quite significantly and consistently.\n\n**Applying DPQ to BERT.**\nTo further verify our method we added experiments on BERT (Appendix H). Without any hyperparameter tuning, DPQ could compress the embedding layer 37x with negligible loss in performance. \n\n[Other]\n\nWe also revised the paper to incorporate the minor comments provided by the reviewer. We are happy to address any unresolved concerns or provide more clarifications!", "title": "Response"}, "B1eqa9EsoS": {"type": "rebuttal", "replyto": "r1lYKM7SKB", "comment": "Thanks for your time and the constructive feedback! While both this work and [1] are based on the idea of representing symbols with discrete codes, the two present very different methodologies. Here are the major differences:\n\n  1) In their work, discrete codes are directly associated with each of the symbols; in this work, discrete codes are computed as outcome of product quantization.\n  2) Our formulation of discrete codes with product quantization allows us to derive two variants with different approximation techniques (softmax-based and vector quantization-based).\n  3) Our method uses a novel composition function (inspired by product quantization), which is much more efficient than before (smaller memory footprint and less computation time overhead, Figure 4).\n  4) With these improvements, DPQ can be trained in a truly end-to-end fashion to achieve an order of magnitude higher compression ratios at negligible or no performance cost.\n\nWe have also elaborated these differences in our related work section. Thank you for this suggestion.\n\nRegarding the comparisons between DPQ-SX and DPQ-VQ, they represent two ways of approximating discrete code learning. Each has its advantages and drawbacks (Table 1). In our experiments, we found DPQ-SX performance marginable better than DPQ-VQ for more tasks/datasets, while DPQ-VQ is more computationally efficient during training. There are potential ways to improve compression results for DPQ-VQ in the future, so we believe both variants have their merits.", "title": "Response"}, "Hyet4X5p_S": {"type": "review", "replyto": "BJxbOlSKPr", "review": "This paper works on methods for compressed embedding layers for low memory inference, where the compressed embedding are learned together with the task-specific models in a differentiable end-to-end fashion. The methods in the paper build on a slight variant of the K-way D-dimensional discrete code representation proposed by Chen et al.. Specifically, the two methods in the paper are motivated by the idea that the K-way D-dimensional code can be viewed as a form of product quantization. The first proposed method (DPQ-SX) uses softmax-based approximation to allow for differentiable learning, while the second proposed methods uses clustering centroid-based approximation. Empirically, the authors demonstrate that the proposed methods for generating compressed embedding can achieve matching inference accuracy with the corresponding uncompressed full embedding across 3 NLP tasks; these proposed approach can outperform the pre-trained word embedding and the K-way D-dimensional code baselines in language modeling tasks.\n\nThis paper builds on an existing embedding representation approach---K-way D-dimensional code. But I think the perspective on viewing k-way D-dim approach as product quantization (which motivate the differentiable learning approach in the paper) is very interesting. Also I think the empirical performance of the proposed method is promising. I gave weak rejection because 1) the proof of theorem 1 is flawed; 2) The experiment might need additional validation to fully support the merit of the proposed methods.\n\nI list below the major concern / questions I have. I am happy to raise the score if the following questions are properly resolved in the rebuttal:\n\n1. Correct me if I am wrong, I think the proof of theorem 1 is wrong---if the integer based code book C is full rank , it does not necessarily imply the one-hot vector based code book B is full rank. E.g. assume K = 2 D = 2,\n\nC = [1 1;1 2;1 1; 1 2] is full-rank (rank = 2), but the corresponding B = [1 0 1 0 ; 1 0  0 1; 1 0 1 0 ; 1 0  0 1] is not full rank (rank < 4).\n\n2. As the proposed methods advocate training the inference-oriented compressed embedding together with the task models (such as translation models), I think the following naive baseline is necessary to fully evaluate the merit of the proposed approach: one can train the full embedding with the task model as usual, compress the task-specific full embedding using the K-way D-dim approach by Chen et al. or using the deep compositional code learning approach by Shu et al., and then use it for the inference. This provides an alternative way to use product quantization based approach for embedding with low inference memory, without training together with task models. Without this, I can not evaluate the necessity to use the train-together approach the author proposed.\n\n3. The proposed DPQ-SX approach performs better in the two proposed approaches. However this approach uses different K and V matrix where in the original K-way D-dim approach we have K = V. This makes it hard to say if the better performance is due to the decoupling of K and V, or because of the training method inspired from the product quantization perspective. It needs ablation study here.\n\n4. In Table 4, the authors only compare to baselines on the LM task, I am wondering how it compares to the the baselines on the other two translation and text classification models.\n\nFor improving the paper, the relatively minor comments are as the following:\n\n1. In equation 4, the partition function Z is not explicitly defined.\n\n2. In the second paragraph of section 3.1, it is not clear what exactly is the pre-trained embedding used as baseline.\n\n3. For better readability, it is better to inflate the caption of figure and tables to provide useful take-away message there.\n", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 3}, "r1lYKM7SKB": {"type": "review", "replyto": "BJxbOlSKPr", "review": "In this manuscript, authors improve the work in [1] by simplifying the reverse-discretization function. The empirical study demonstrates the effectiveness of the proposed algorithm.\nI\u2019m not familiar with the area. The differences between this work and [1] should be elaborated more in the related work, since they are closely related.\nBesides, for the technical part, DPQ-SX outperforms DPQ-VQ while the softmax approximation seems identical to that developed in [1].\n\n[1] ICML\u201918: Learning K-way D-dimensional Discrete Codes for Compact Embedding Representations", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 1}, "Hyl2WZpf5H": {"type": "review", "replyto": "BJxbOlSKPr", "review": "This paper considers the problem of having compact\u00a0yet expressive KD code for NLP tasks. The authors claim that the\u00a0proposed differentiable product quantization framework has better compression but similar performance compared to existing KD codes.The authors present two instances of the DPQ framework: DPQ-SX using softmax to make it differentiable, and DPQ-VQ using centroid based approximation. While DPQ-SX performs better in terms of performance and compression, DPQ-VQ has the advantage in scalability.\n\n- Significance\nIt's understandable that the size of the embedding is important, but there's been a lack of explanation as to why this should be done only through KD codes. Hence, it is doubtful how big the impact of the proposed framework is.\n\n- Novelty\nJust extending and making Chen et al., 2018b's distilling method to be differentiable has limited novelty.\n\n- Clarity\nThe paper is clearly written in most places, but there were some questions about the importance and logic of statements.\n\n- Pros and cons\nCompared to Chen et al., 2018b, there is no need to use expensive functions, and performance is better. But, the baseline consists only of algorithms using KD codes; there might be many disadvantages compared to other types of algorithms.\n\n- Detailed comments and questions\n1. It is true that the parameters for embedding make up a large part of the overall parameters, but I would like some additional explanation of how important they are to learning. It is usually not necessary to train the entire embedding vector on GPU, so it would not be a big issue in the actual learning process.\n2. In a similar vein, it would be nice to show which of the embedding vector size or the LSTM model size contributes significantly to\u00a0performance improvements.\u00a0If LSTM model size contributes more, the motivation would be weakened.\n3. It would be nice to add more baselines such as Nakayama 2017 as well as the standard compression/quantization methods used in other deep networks. And please explain why we should use KD codes to reduce embedding size. Also, why the distilling in Chen et al., 2018b is a problem?\n4.\u00a0Did you run all experiments just one time? There is no confidence interval.\n5. DPQ models have different compression ratios depending on the size of K and D. It would be great to show the change in PPL according to the compression ratio of DPQ models.\n6. Can we apply it to pre-trained models like BERT?", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 2}}}