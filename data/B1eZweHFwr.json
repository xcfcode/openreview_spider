{"paper": {"title": "Statistical Verification of General Perturbations by Gaussian Smoothing", "authors": ["Marc Fischer", "Maximilian Baader", "Martin Vechev"], "authorids": ["marcfisc@student.ethz.ch", "mbaader@inf.ethz.ch", "martin.vechev@inf.ethz.ch"], "summary": "We present a statistical certification method to certify robustness for rotations, translations and other transformations.", "abstract": "We present a novel statistical certification method that generalizes prior work based on smoothing to handle richer perturbations. Concretely, our method produces a provable classifier which can establish statistical robustness against geometric perturbations (e.g., rotations, translations) as well as volume changes and pitch shifts on audio data. The generalization is non-trivial and requires careful handling of operations such as interpolation. Our method is agnostic to the choice of classifier and scales to modern architectures such as ResNet-50 on ImageNet.", "keywords": ["adversarial robustness", "certified network", "randomised smoothing", "geometric perturbations"]}, "meta": {"decision": "Reject", "comment": "This paper proposes a smoothing-based certification against various forms of transformations, such as  rotations, translations. The reviewers have concerns on the novelty of the work and several technical issues. The authors have made efforts to address some of issues, but the work may still significantly benefit from a throughout improvement in both presentation and technical contribution."}, "review": {"We5cRt6sTb": {"type": "rebuttal", "replyto": "B1eZweHFwr", "comment": "An updated version of this paper was accepted at NeurIPS 2020 and can be found here: https://arxiv.org/abs/2002.12463", "title": "Published Version"}, "BkxnCnHooS": {"type": "rebuttal", "replyto": "SkeL435jtB", "comment": "> Non-isotropic covariance matrices\n\nWe have addressed this point in our response to all reviewers.\n\n> To certify this attack, given a base classifier, we can simply do, for example, grid search, on the low dimensional space to find the worst case with very good accuracy. And it should be able to give much better than the randomized smoothing method.\n\nWith interpolation scheme such as linear and bilinear interpolation, every legal floating-point parameter would produce a (potentially) different interpolation. Thus enumeration (even though theoretically possible as floats are countable) is infeasible. For the case of nearest-neighbor interpolation Pei et al. [1] investigated an enumerative approach.\n\n[1] Towards Practical Verification of Machine Learning: The Case of Computer Vision Systems, Pei et al., https://arxiv.org/abs/1712.01785", "title": "Response to Review #4"}, "HkloYnSosB": {"type": "rebuttal", "replyto": "S1l6RuK2Kr", "comment": "> How about the scenario when both rotation and translation exist?\n\nWe currently do not support this as a rotation concatenated with translation (and interpolation artifacts) does not commute with translation and rotation. We hope to address this issue in future work.\n\n> In experiments, can the authors consider the accuracy of the certified model under the adversarial setting, i.e., find the transformation that makes the deep nets performs worst?\n\nSimilar to Salman et al. [3] we could write an optimization problem to reflect this. Depending on the transformation the problem might be (piece wise) differentiable in the argument. However for rotations, prior research [1] found that randomly sampling perturbations is more effective. We will strive to include experimental results for this in the next version of the paper.  \n\n> Randomized Nets and Manifold Interpolation by Wang\n\nWe were unaware of this line of work and thank the reviewer of the pointer. Similar to Cohen et al. [2] this addresses norm-ball perturbations. We added it to our related work. \n\n[1] Exploring the Landscape of Spatial Robustness, Engstrom et al., ICML 2019, https://arxiv.org/abs/1712.02779\n[2] Certified Adversarial Robustness via Randomized Smoothing, Cohen et al. ICML 2019, https://arxiv.org/abs/1902.02918\n[3] Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers, Salman et al., https://arxiv.org/abs/1906.04584", "title": "Response to Review #2"}, "r1l0Tjrsjr": {"type": "rebuttal", "replyto": "r1x1Xg40tH", "comment": "> Concerns about contribution and novelty of dealing with interpolation noise & Non-isotropic Sigma\n\nWe have addressed these points in our response to all reviewers.\n\n> How are $\\sigma_i$ vs $\\sigma_r$ chosen?\n\nTo certify for large angles $\\gamma$, in Equation (2) we want as large values for $\\sigma_i$ and $\\sigma_r$ as possible. The largest possible values for $\\sigma_i$ are dictated by the current results in smoothing for L2-perturbations. By Table 6 in Appendix G of Salman et al. [1] we choose 0.75, as this provides a good trade-off between robustness and accuracy.\nThe choice of $\\sigma_r$ matters less than the choice of as $\\sigma_i$ large values don\u2019t decrease the accuracy as much. As we are only certifying values of $\\gamma$ up to $\\pm 8$ degrees, $\\sigma_r = 5$ was a good trade-off between accuracy and certifiability.\n\n> Audio experiments missing\n\nWe will add audio experiments in the next iteration of the paper.\n\n[1] Provably Robust Deep Learning via Adversarially Trained Smoothed Classifiers, Salman et al., 2019, https://arxiv.org/abs/1906.04584", "title": "Response to Review #3"}, "rkx4_jSsoS": {"type": "rebuttal", "replyto": "B1eZweHFwr", "comment": "We thank all reviewers for their comments and valuable feedback. We have addressed the individual reviewer comments below and now address common concerns here.\n\n> Impact of work\n\nAll reviewers have questioned the novelty of the presented work. We would like to address this by pointing out that the problem of these general perturbations is interesting to the research community [1 - 4]. Dealing with interpolation artifacts is a key challenge in achieving robustness to geometric perturbations of images in our and other works [3, 4]. We believe our approach is a suitable and non-straight-forward way to approach the problem.\n\n> Non-isotropic covariance matrices\n\nCohen et al.\u2019s [5] original approach requires only a single (co)variance that is the same for all dimensions. For the perturbations discussed in this work (specifically in section 4) a diagonal covariance matrix is required. Since Theorem 3.2 is the same of diagonal and full covariance matrices we stated it is such. However. for perturbations parameterized by multiple (non-noise) parameters the off-diagonal entries can be used to encode dependencies between the parameters, such as the translation in the x direction and the translation in the y direction. While the diagonal version can proof safety for L2-balls in the parameter space, the off-diagonal entries admits general ellipsoids. \n\n\n[1] Exploring the Landscape of Spatial Robustness, Engstrom et al., ICML 2019, https://arxiv.org/abs/1712.02779\n[2] Geometric Robustness of Deep Networks: Analysis and Improvement, Kanbak et al., CVPR 2018, https://arxiv.org/abs/1711.09115\n[3] Certifying Geometric Robustness of Neural Networks, Balunovic et al., to appear in NIPS 2019, https://www.sri.inf.ethz.ch/publications/balunovic2019geometric\n[4] Towards Practical Verification of Machine Learning: The Case of Computer Vision Systems, Pei et al., https://arxiv.org/abs/1712.01785\n[5] Certified Adversarial Robustness via Randomized Smoothing, Cohen et al. ICML 2019, https://arxiv.org/abs/1902.02918", "title": "Common Response"}, "SkeL435jtB": {"type": "review", "replyto": "B1eZweHFwr", "review": "This paper applied the framework of randomized smoothing classifier proposed by Cohen et al. to certify the adversarial attacks other than pixel change, including image rotating, brightness change in RGB space, Volume and pitch shifts for audio.\n\nCertifying general adversarial attack is an interesting direction. However, I do not believe this paper is qualified for publishing in ICLR. Below please find my comments:\n\nDifferent from change-pixel attack, the certification for rotation/brightness change in image classification, volume and pitch change in audio perturbations is much easier. The studied perturbation can be parameterized by a very low dimension vector (i.e. one dimension (angle and volume) for image rotation and volume change, 3 dimensions (brightness for each channel) for brightness perturbation). To certify this attack, given a base classifier, we can simply do, for example, grid search, on the low dimensional space to find the worst case with very good accuracy. And it should be able to give much better than the randomized smoothing method.\n\nOne contribution of this paper is in that: Theorem 3.2 is valid for random smoothing using gaussian with general covariance matrix. However, the effectiveness of using a general covariance matrix is not studied. In the experiment section, I find only isotropic ones are used.", "title": "Official Blind Review #4", "rating": "3: Weak Reject", "confidence": 2}, "S1l6RuK2Kr": {"type": "review", "replyto": "B1eZweHFwr", "review": "In this paper, the authors generalize the randomized smoothing type of robustness certification to handle many types of attacks beyond norm-based attacks, e.g., geometric perturbation, volume change, pitch shifts on audio data. At the core of the proposed generalization is using some interpolation which I think is quite straightforward.\n\n1. How about the scenario when both rotation and translation exist?\n\n2. In experiments, can the authors consider the accuracy of the certified model under the adversarial setting, i.e., find the transformation that makes the deep nets performs worst?\n\n3. Besides randomized smoothing on the input images, recently Wang et al showed that randomize the deep nets can\nalso improve the deep nets and they gave it a nice theoretical interpretation. Here is the reference: Bao Wang, Binjie Yuan, Zuoqiang Shi, Stanley J. Osher. ResNets Ensemble via the Feynman-Kac Formalism to Improve Natural and Robust Accuracies, arXiv:1811.10745, NeurIPS, 2019\n\n4. Interpolation idea has been used in improving robustness of deep nets, for example:\n1). Bao Wang, Xiyang Luo, Zhen Li, Wei Zhu, Zuoqiang Shi, Stanley J. Osher. Deep Neural Nets with Interpolating Function as Output Activation, NeurIPS, 2018 \n2). Bao Wang, Alex T. Lin, Zuoqiang Shi, Wei Zhu, Penghang Yin, Andrea L. Bertozzi, Stanley J. Osher. Adversarial Defense via Data Dependent Activation Function and Total Variation Minimization, arXiv:1809.08516, 2018 \n3). B. Wang, S. Osher. Graph Interpolating Activation Improves Both Natural and Robust Accuracies in Data-Efficient Deep Learning, arXiv:1907.06800\n\nIn sum, this paper studies the problem of certifying a broad class of adversarial attacks which is decent, however, the novelty is quite limited. Please address my questions during rebuttal.", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 3}, "r1x1Xg40tH": {"type": "review", "replyto": "B1eZweHFwr", "review": "Summary:\n\nThis paper introduces a new smoothing algorithm which produces classifiers with certified robustness against adversarial perturbations. In particular, the authors are interested in settings where the input vectors need not be robust simply to \\ell_2 perturbations, for which previous authors (e.g., Cohen et al.) have already introduced a \"randomized smoothing\" classifier which possesses the desired properties. Instead, motivated by the desire to certify robustness to other families of transformations, the authors propose to introduce a smoothing transform that operates in an additive way on the underlying parameter space (rather than on the raw input vectors, as in the case of \\ell_2 randomized smoothing). The authors provide details for a few examples from image and audio data processing, and then show experimental results on ImageNet data.\n\nComments:\n\nMy assessment of this paper is that the level of novelty is relatively low. On the technical side, I do not believe this paper makes any significant contributions. The authors themselves note that the proof of the main result, Theorem 3.2, closely parallels the proof of Cohen et al. Algorithmically, the proposals of the authors are also minor tweaks of the algorithms in Cohen et al., since the types of perturbations considered in this paper are essentially single-parameter problems. (As a question: The authors prove Theorem 3.2 where the randomized smoothing operation adds possible nonisotropic Gaussian noise in the parameter space. However, it seems that in the examples, the noise added is isotropic -- it is not clear how the authors are choosing \\sigma_i in relation to \\sigma_r for the interpolation example. Can the authors clarify this, and give examples where smoothing with nonisotropic noise would actually be useful?)\n\nThe only real tweak that the authors introduce when presenting their smoothing algorithm is to introduce a small correction term in order to deal with interpolation/rounding errors. However, I do not think that this change is particularly novel.\n\nAs a final comment, I think it would have been interesting for the authors to include some simulations with audio data as well, since this constitutes one of the main motivating applications discussed by the authors.", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 2}}}