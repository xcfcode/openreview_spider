{"paper": {"title": "Mind the Gap when Conditioning Amortised Inference in Sequential Latent-Variable Models", "authors": ["Justin Bayer", "Maximilian Soelch", "Atanas Mirchev", "Baris Kayalibay", "Patrick van der Smagt"], "authorids": ["~Justin_Bayer1", "~Maximilian_Soelch1", "~Atanas_Mirchev1", "~Baris_Kayalibay1", "~Patrick_van_der_Smagt1"], "summary": "We show how a common model assumption in amortised variational inference with sequential LVMS leads to a suboptimality and how to prevent it.", "abstract": "Amortised inference enables scalable learning of sequential latent-variable models (LVMs) with the evidence lower bound (ELBO). In this setting, variational posteriors are often only partially conditioned. While the true posteriors depend, e.g., on the entire sequence of observations, approximate posteriors are only informed by past observations. This mimics the Bayesian filter---a mixture of smoothing posteriors. Yet, we show that the ELBO objective forces partially-conditioned amortised posteriors to approximate products of smoothing posteriors instead. Consequently, the learned generative model is compromised. We demonstrate these theoretical findings in three scenarios: traffic flow, handwritten digits, and aerial vehicle dynamics. Using fully-conditioned approximate posteriors, performance improves in terms of generative modelling and multi-step prediction.", "keywords": ["variational inference", "state-space models", "amortized inference", "recurrent networks"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper studies how suboptimal conditioning sets create\nsuboptimal variational approximations in variational inference with amortization in state space models. \nWhile the point made about the role of the conditioning set is not a new one, the point was carried out further and \nmore clearly in this paper than previous works. Addressing a couple of issues would \nmake the paper stronger:\n\n- Really boiling down in the experiments to know for what models/data\n  the \"full\" approach would add value would provide concrete guidance\n  to the community.\n\n\n- Notation choices in the paper are rough. For example, Appendix A.2\n  reads like a type mismatch since the w on the left is a function of\n  z but is also equal to a function of z and C.\n\n\n- Adding a more detailed description of the complement of C in the\n  main text"}, "review": {"4SiYSFzX3r0": {"type": "review", "replyto": "a2gqxKDvYys", "review": "**Summary**\nThis paper investigates the effect of partial conditioning on amortized inference in variational auto-encoders, focusing specifically on sequential data sources where it is common practice to have a posterior that is factorized in such a way that conditioning is partial (usually only conditioning on past signals in the sequence). Given a true posterior that is conditioned on the entire observed datapoint, the authors discuss the effect of having an approximate posterior that is only conditioned on part of the input. As the approximate posterior cannot adapt to the part of the input that is left out of the conditioning, the evidence lower bound becomes less tight, due to the larger KL divergence between the approximate posterior and the true posterior. The authors compare this to the work by Cramer et al. [1], where the distinction was made between having a restricted family of possible distributions for the approximate posterior (approximation gap) and the gap between an amortized approximate posterior with an inference network shared for all datapoints and a non-amortized approximate posterior that is optimized for each datapoint separately (amortisation gap). They argue that partial conditioning leads to a third type of gap which is distinct of the aforementioned inference gaps. Through an example with discrete observations the authors derive that when the true posterior is conditioned on the full data, and the approximate posterior is only partially conditioned, the optimal approximate posterior is something akin to a product of true posteriors over the unconditioned information, and not a mixture where the left out information is marginalized out. Through a 1D example they show that this could lead to overly sharp posteriors that have high densities in regions where the true posterior has very low density. \nAs the authors also state, several studies have shown that full conditioning on future observations results in negligible performance gains. However, the authors conjecture that this is because those results were mainly found on problems where the conditioning issue was not (or less) relevant. \nThe authors demonstrate potential performance gains on 3 datasets where conditioning on future information could be helpful: unmanned aerial vehicle trajectories from the Blackbird dataset, a sequential version of MNIST where the rows of a picture correspond to sequential observations, and a selection of a traffic flow dataset. They perform log likelihood estimates and prefix-sampling to determine the effect of conditioning (partially or fully) on future observations.\n\n\n**Pros**\n- The idea behind the effect of conditioning on the amortization procedure is clearly explained.\n- The exposition that explains that the optimal approximate posterior could correspond to something akin to a product of distributions instead of a mixture is interesting.\n- On the datasets that were selected by the authors, the benefit of full conditioning versus partial conditioning is visible in the quantitative results (log likelihood estimates).\n\n**Cons**\n- The authors argue that the conditioning gap is a distinct gap from the amortization gap that was discussed by Cramer et al. [1]. It is not clear to me why these two gaps are distinct/independent, I would say the conditioning gap is part of the amortization gap introduced by Cramer et al. since the way that conditioning is handled in amortized inference is essential to the gap between the amortized and non-amortized approximate posterior. For instance, in the example of the univariate gaussian in section 3.2, where would the amortization gap from Cramer et al. fit in as a separate gap? The amortization gap can be large because the conditioning is incomplete, or because of the limited flexibility of the neural network mapping from conditioning to parameters. Such a limited flexibility could reach the same type of error as partial conditioning. \n- The effect of the narrow posteriors for partially conditioned approximate posteriors due to it being a product of distributions and not a mixture is not clear in the experiments, even though the authors do hint that this is observed in the qualitative prefix sampling experiments. The sample prefix experiments are furthermore very hard to judge, especially for the traffic flow dataset. The authors draw conclusions from these plots that I can\u2019t confirm by looking at the plots. For instance, with respect to the traffic flow samples the authors state that \u201cthe partially conditioned model concentrates too much\u2026\u201d. It seems to me like it concentrates about the same amount as the full model, and I don't see how it\u2019s \u201ctoo much\u201d as the dashed line is usually among the predictions. I think the authors are incentivized to find this conclusion because they try to argue in figure 1 that products of distributions concentrate too much argument.\n- As the authors state, previous work showed that on popular datasets conditioning on future information has little gains. Even though the authors find datasets where gains can be made, these datasets are not incredibly convincing that this is actually a widespread problem for sequential VAEs. The lack of overlap between datasets that related work is evaluated on  (such as the gym or mujoco datasets or natural speech waveform datasets and polyphonic music datasets) and datasets that this work is evaluated on and the fact that the datasets of this paper are particularly small or artificial makes me a little concerned that the problems need to be cherry picked for the proposed conditioning to have an actual effect. For instance, although the MNIST example is an obvious example where conditioning on future information could help, it is fairly artificial. The authors argue that the gym and mujoco datasets have deterministic dynamics (and therefore shouldn\u2019t suffer from partial conditioning), but do not explain why waveform datasets or polyphonic music datasets are not suitable to study this problem. Together with the fact that related work is discussed but not compared against empirically, this makes it hard to place this work in context with related work and to judge its relevance. \n- I would expect more results on the influence of the sneak-peak parameter k. On the traffic flow dataset the authors suggest that the full model (with largest possible k) can perform worse on the test set than a model with intermediate k because the intermediate-k model already contains sufficient future information. This could be investigated if results were compared for models with more values of k, and a leveling off of performance gains for increasing k could confirm this conjecture.\n\n\n**Minor comments**\n- In section 5 there is a lot of referring to section 5 itself in the middle of sentences, which breaks the flow and seems unnecessary. See for instance the first paragraph of section 5.\n- Are you using statically binarized mnist or dynamically binarized mnist?\n\n\n[1] Cramer et al. inference suboptimality in variational autoencoders. https://arxiv.org/abs/1801.03558\n", "title": "review ", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "PTVlliGXV3": {"type": "rebuttal", "replyto": "a2gqxKDvYys", "comment": "We would like to thank the reviewers for their thoughtful and constructive feedback. Our response will first tackle two general points raised in several reviews before briefly addressing each of your reviews individually. You can find the specific rebuttals as responses to your reviews. We also invite you to read the rebuttals given to your fellow reviewers.\n\n\n**Originality and Contribution**\n\nWe extend on where we see our contribution vs. what is commonly known in the literature. \n\nWe acknowledge that others have pointed out a discrepancy between the true posterior and the approximate posteriors typically used in the literature. Similarly, the approximation gap and the amortisation gap are common knowledge. In combination, it may indeed seem somewhat obvious that partial conditioning leads to suboptimal inference.\n\nAt the same time, we are not aware that partial conditioning has been questioned anywhere in the literature to a larger degree than, e.g., the choice of variational family. In fact, model names like \u201cDeep Kalman Filters\u201d or \u201cDeep Variational Bayes Filters\u201d imply that (approximate) Bayesian filters are learned. Our work shows that this is incorrect---a stronger result than expectable inference suboptimality that is apparently not obvious. In this light, we want to push back on the notion that we were merely fleshing out the obvious.\n\nOur contribution is to go beyond a demonstration of expectable inference suboptimality. We provide theoretical and empirical evidence as well as strong intuitions for partial conditioning. It is as important as the choice of variational family or architecture of the inference network. Neither a more expressive variational family nor a more powerful network architecture can fix the problem! We want to raise awareness of this issue among researchers and hence explicitly provide intuitions to guide future design.\n\n\n**Choice of Data Sets**\n\nSome of you have inquired about our choice of non-standard data sets, since previous work saw little benefits of smoothing variants over filtering counter parts.\n\nThe presence of a conditioning gap is a property of the system that generated the data. In section 3.3., we detail two common cases where it does not surface. There is good reason to believe that previous papers focus on such data sets, as we have argued in the paper.\n\nWhy are the cases where smoothing is only as good as filtering so common in the literature then? One reason might be that we overstate the problem and it is not that dramatic at all.  Another might just be survivorship bias: papers that show bad results are less likely to be published. Due to its tendency to focus on partially-conditioned approximate posteriors, the community has also focused on data sets where this has little effect. We study three quite diverse data sets where the problem is present, and believe each of them to be as relevant as previously studied data for benchmarking variational sequence models.\n\n\n**Summary of changes**\n\nWe changed the submission in the following places:\n\n - We added a clarification to our contribution in the introduction, based on your feedback.\n - We added a paragraph about alternative divergences as pointed out by reviewer 1.\n - We updated the definition of the conditioning gap, eq. (6) and surrounding sentence, to be properly defined for a data set rather than a subset.\n - We added missing literature that was pointed out by several reviewers.\n   - Added reference to \u201cVariational Autoencoder with Arbitrary Conditioning\u201d by Ivanov, Oleg and Figurnov, Michael and Vetrov, Dmitry P.\n    - Added reference to \u201cPhoto-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\u201d by Ledig et al.\n    - Added reference to \u201cLearning and Querying Fast Generative Models for Reinforcement Learning\u201d by Buesing et al.\n     - Added reference to \u201cTemporal Difference Variational Auto-Encoder\u201d by Gregor et al.\n - We have addressed all your \u201cminor\u201d comments.\n - We further included minor cosmetic changes for increased readability.\n - We added comment on binarisation of MNIST.", "title": "General reply"}, "PnNNT1t20PG": {"type": "rebuttal", "replyto": "fd6aa0I-ns", "comment": "Thank you for your positive review! \n\nWe will answer your specific questions here. Please also consider the general rebuttal given to all reviewers as a top-level reply to this submission\u2019s thread.\n\n> It would be helpful to point practitioners to this related work, as one option to consider given that the KL divergence learns products of posteriors and this may not be a desirable feature of a divergence for an application.\n\nThe KL is motivated by the standard VI observation that argmax_q ELBO =  argmin_q posterior KL. You raise a very interesting point here. We had not considered alternative objectives. We believe this is a promising avenue for future research. We added a short discussion to the manuscript.\nWe also point you to our discussion of originality and anticipated impact in the reply to all reviews. \n\nWe would love to hear how you think the paper can be improved to a clear accept in your opinion.\n", "title": "Reply"}, "ZOeV3kdLIq5": {"type": "rebuttal", "replyto": "QO_JhhZfI9W", "comment": "Thank you for your positive review. As per your suggestion, we now added a discussion w.r.t. the related work you pointed out. Beyond that, we would much appreciate detailed feedback as to the necessary improvements that would merit a \"clear accept\" rather than a \"good paper\" review.\nPlease also consider the general rebuttal given to all reviewers as a top-level reply to this submission\u2019s thread.\n", "title": "Reply"}, "jYWYmqr_Usk": {"type": "rebuttal", "replyto": "4SiYSFzX3r0", "comment": "Thank you for your detailed and constructive review. We are happy that you found our exposition clear and interesting, and that you consider the experiments to back up our findings quantitatively. \n\nWe will answer your specific questions here. Please also consider the general rebuttal given to all reviewers as a top-level reply to this submission\u2019s thread.\n\n> It is not clear to me why these two gaps are distinct/independent\n\nThe short answer is that the approximation gap is defined on a per-sample basis, while the conditioning gap is derived for all samples, see eq. (6) in section 3.1 for the definition. For a single sample, the conditioning gap does not exist. It only emerges as soon as inference has to compromise over many samples. The effect of that compromise on the ELBO is the conditioning gap.\n\nAs you correctly say, the learning can be hampered by errors of the inference network (amortisation gap) or missing inputs (conditioning gap). From this practical lens, both could be viewed as two sides of the same medal. This is a wide definition of the amortisation gap encompassing everything that makes the inference network miss q*. \n\nWe cannot arrive at an equation such as \u201camortisation gap = conditioning gap + something\u201d, because the LHS is per-sample and the RHS involves an expectation over all samples. This would require a redefinition of the amortisation gap. \n\nThe amortisation gap measures how much the neural net deviates from the mathematically optimal solution. The conditioning gap is different. The problem is neither the network capacity, nor a particular sample x, it\u2019s the foul compromise between all samples sharing the same C, but not necessarily the same ~C. The conditioning gap measures how much the shared optimal solution misses all the individual optimal solutions. Unlike with the amortisation gap, the target distribution has changed, so that even if the neural network adheres perfectly, the result is not desirable.  \n\nWe thus face two distinct phenomena, and it is worth studying them separately, as they require different remedies.\n\n> In the example of the univariate gaussian in section 3.2, where would the amortisation gap from Cramer et al. fit in as a separate gap?\n\nFor *any x*, we can get a better q than omega. But not for *all x* of them at the same time.\nThe q's are coupled if they share the same C, even though they differ in ~C. Hence, improving it for one ~C will make it worse for others. Notice that there is no amortisation gap here, because we can write down the solution in closed form, that is w_a(z). \n> The authors draw conclusions from these plots that I can\u2019t confirm by looking at the plots.\n\nTo convince you otherwise, we would like to draw your attention to the figure showing the prefix sampling for traffic flow, figure 4. \n\n\nThe phenomenon is most clear in the second column.\nHere the fully conditioned model supports, roughly speaking, two hypotheses: \none of a traffic jam, where the speed drops, and,\none without a traffic jam, where the speed stays constant. \nThe semi-conditioned model does so as well, although to a lesser degree.\n\nBeing able to maintain several qualitatively different hypotheses of the future is essential to stochastic models. We can clearly see that conditioning more helps with that. The other columns show\u2013more or less\u2013the same. We did not cherry-pick those plots.\n\nMind also that the plots are backed up by quantitative evaluations.\n\n> problems need to be cherry picked\n\nWe address this in the reply to all reviewers.\n\n> I would expect more results on the influence of the sneak-peak parameter k.\n\nWe acknowledge that this could be an interesting ablation study. We did not consider it central to the contribution of the paper and thus spared it.\n\n> Are you using statically binarized mnist or dynamically binarized mnist?\n\nWe binarized the data before training in a consistent matter over all experiments.\n", "title": "Individual Response"}, "3aW-a1yPE6x": {"type": "rebuttal", "replyto": "YkO3GmSn5Q3", "comment": "Thank you for your positive and constructive review. \n\nYou mention a performance gap between state-space and auto-regressive models. Advocating for or against any of these was beyond our scope for this submission. We believe both have their merits depending on the application. Notably, SSMs are still a wide-spread tool in the engineering disciplines. If anything, we want to understand if our findings can explain some of the performance gap, but by no means claim to close it.\n\nRegarding DKF, you are right in pointing out that eq. 3 hints at a faithful posterior approximation. Yet, all four models in section 5.2 drop at least z_t-1. We have updated our overview table accordingly. We have further added the two publications pointed out to you as related work in the appropriate parts of the paper.\n\nWe further point you at our reply to all reviewers, where we clarify our contribution in relation to common knowledge in the field.\n", "title": "Reply"}, "fd6aa0I-ns": {"type": "review", "replyto": "a2gqxKDvYys", "review": "I enjoyed this paper, and think it provides a valuable contribution to sequental latent variable modeling of time series data.\n\nSpecifically, this paper addresses the issue of conditioning in using variational inference to fit sequential latent variable models to data. In addition to potential errors from an amortisation gap or approximation gap, a conditioning gap is identified, where a variational distribution that is not conditioned on all possible information (previous timesteps' observations and latent variables) underperforms. \n\nThis seems like an 'obvious' insight, but I think that is a strength of this paper. It clearly shows why previous work falls short of using all available information to get good performance, through a simple theoretical analysis. Further, empirically the work demonstrates how to correct for the conditioning gap.\n\nI anticipate that through the publication of this paper at ICLR, the authors of future papers in this area will need to be careful in conditioning. This will benefit the research community as a whole, and lead to higher-quality variational approximations and papers.\n\nOne nit:\n\n- although the optimal variational approximation may not be ideal in the theoretical study here, in Section 3.1, I think a discussion of why the KL divergence was used in this study is warranted. For example, there are other divergence measures that do not suffer from the issues presented here  (c.f. https://dataspace.princeton.edu/handle/88435/dsp01pr76f608w).  It would be helpful to point practitioners to this related work, as one option to consider given that the KL divergence learns products of posteriors and this may not be a desirable feature of a divergence for an application.", "title": "Good contribution", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "QO_JhhZfI9W": {"type": "review", "replyto": "a2gqxKDvYys", "review": "Summary:\nThe paper considers the problem of Bayesian inference with partially conditioned variational posterior. Namely, this work describes the phenomena of ill-behaved variational posterior for the case of partially observed data. The paper's main theoretical finding is that the partially conditioned variational posterior behaves like a product of experts, resulting in a degenerate solution. Speaking intuitively, the true posterior can be seen as a mixture of distributions: the sum over the unobservable variable. At the same time, the optimal variational posterior mixes as a product of distributions. Clearly, the product of densities hardly depicts features of the mixture since a near-zero value of a single member is enough for zeroing out the product's density.\nNevertheless, such models are successfully applied in some cases, and the authors explain why this theoretically perspectiveless construction could work in practice. The answer is quite straightforward: the partially conditioned variational posterior works only when the unobserved variables do not affect the true posterior. Finally, the authors strengthen their theoretical studies with neat experimental studies.\n\nReview:\nIt is hard to write a useful review for this paper since the authors clearly have thought through many aspects of their work. I find this paper to be a useful piece, both theoretically and practically.\n\nMy only suggestion would be to include several works into consideration. The problem of partial observability is also important for generative image models [1,2]. I don't propose to perform a model comparison, but I think the reader would benefit if you could relate your result with similar works from CV. For instance, why other models avoid this degenerate solution while still conditioning partially. Or how one can possibly escape difficulties when full conditioning is not possible on the test stage.\n\nMinor comments:\n1. page 1, section 2.1. I think by \"minimization of eq. 1\" the authors mean maximization of the marginal likelihood.\n\nReferences:\n1. Ivanov, Oleg, Michael Figurnov, and Dmitry Vetrov. \"Variational autoencoder with arbitrary conditioning.\" arXiv preprint arXiv:1806.02382 (2018).\n2. Ledig, Christian, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken et al. \"Photo-realistic single image super-resolution using a generative adversarial network.\" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4681-4690. 2017.", "title": "neat and useful theoretical result supported with practical examples", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "YkO3GmSn5Q3": {"type": "review", "replyto": "a2gqxKDvYys", "review": "The paper reviews the issue of partial conditioning of the amortized posterior in sequential latent variable models, typically state-space models trained with a VAE-style loss, but where the posterior used is the filtering rather than smoothing posterior. The author show that training a model with posterior with missing information can lead to a gap in estimating both the posterior and the corresponding model. They show the benefits of using the correct posteriors in simple examples.\n\nOverall, the paper is well written, but its originality is on the low end; a large number of papers describing state-space VAE like models make explicit that the filtering posterior is technically suboptimal compared to the smoothing one; few see benefits in actually using the smoothing posterior (note that [1] derives a valid ELBO using only a one-step smoothing update). The derivation of the shared approximate posterior is standard variational inference derivations, but it is nice to see it explicitly written, and contrasted with the optimal posterior. The paper frequently gives nice intuitions behind various facts (mixture vs product of expert and the gating effect, when is an imperfectly conditioned expert enough, etc.). \n\nThe univariate Gaussian example is a good toy problem to understand the issue at hand. The numerical examples are selected to highlight the benefits of smoothing; they are interesting but perhaps not particularly challenging or surprising, and in relatively short sequences it makes sense that peeking or smoothing would benefit over filtering.  \n \n\nOverall, I am still inclined to accept the paper, as it investigates more clearly and makes more explicit knowledge that is usually treated as folklore and footnotes in other papers. \u02c6It does not really offer methods for making smoothing posteriors actually learn more powerful models than filtering posteriors on complex datasets, nor does it technically demonstrate that SOTA state-space models have their performance limited through the information of the posterior (note that state-space models still typically underperform autoregressive ones). \n\n\nMinor:\n-Table 1: The Deep Kalman Filter, in some of its instantiations, has an empty \\bar{C_t}; they explicitly condition z_t on z_tm1 and x_\\geq t (see equation 3). See also [2] for another state space model that considers both the smoothing and filtering posteriors (as others, they note no benefit to the smoothing posterior).\n\n-Equation 5: Technically the left hand side should be the function w, not the particular value w(z) (note z is a bound variable on one side of the equation and bound on the other side). I understand what the authors mean, but it looks strange as it is.\n\n\n[1] Gregor et. al, \"Temporal Difference VAE\"\n[2] Buesing et al., \"Learning and Querying Fast Generative Models for Reinforcement Learning\"", "title": "Review", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}