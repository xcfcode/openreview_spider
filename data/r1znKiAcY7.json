{"paper": {"title": "Few-shot Classification on Graphs with Structural Regularized GCNs", "authors": ["Shengzhong Zhang", "Ziang Zhou", "Zengfeng Huang", "Zhongyu Wei"], "authorids": ["17210980007@fudan.edu.cn", "15300180085@fudan.edu.cn", "huangzf@fudan.edu.cn", "zywei@fudan.edu.cn"], "summary": "", "abstract": "We consider the fundamental problem of semi-supervised node classification in attributed graphs with a focus on \\emph{few-shot} learning. Here, we propose Structural Regularized Graph Convolutional Networks (SRGCN), novel neural network architectures extending the well-known GCN structures by stacking transposed convolutional layers for reconstruction of input features. We add a reconstruction error term in the loss function as a regularizer. Unlike standard regularization such as $L_1$ or $L_2$, which controls the model complexity by including a penalty term depends solely on parameters, our regularization function is parameterized by a trainable neural network whose structure depends on the topology of the underlying graph. The new approach effectively addresses the shortcomings of previous graph convolution-based techniques for learning classifiers in the few-shot regime and significantly improves generalization performance over original GCNs when the number of labeled samples is insufficient. Experimental studies on three challenging benchmarks demonstrate that the proposed approach has matched state-of-the-art results and can improve classification accuracies by a notable margin when there are very few examples from each class.", "keywords": ["Graph Convolutional Networks", "Few-shot", "Classification"]}, "meta": {"decision": "Reject", "comment": "A new regularized graph CNN approach is proposed for semi-supervised learning on graphs.  The conventional Graph CNN is concatenated with a Transposed Network, which is used to supplement the supervised loss w.r.t. the labeled part of the graph with an unsupervised loss that serves as a regularizer measuring reconstruction errors of features. While this extension performs well and was found to be interesting in general by the reviewers,  the novelty of the approach (adding a reconstruction loss),  the completeness of the experimental evaluation, and the presentation quality have also been questioned consistently. The paper has improved during the course of the review, but overall the AC evaluates that paper is not upto ICLR-2019 standards in its current form.\n"}, "review": {"S1eCA4l927": {"type": "review", "replyto": "r1znKiAcY7", "review": "This paper proposes to regularize the training of graph convolutional neural networks by adding a reconstruction loss to the supervised loss. Results are reported on citation benchmarks and compared for increasing number of labeled data.\n\nThe presentation of the paper could be significantly improved. Details of the proposed model are missing and the effects of the proposed regularization w.r.t. other regularizations are not analyzed.\n\nMy main concerns are related to the model design, the novelty of the approach (adding a reconstruction loss) and its experimental evaluation.\n\nDetails / references of the transposed convolution operation are missing (see e.g. https://ieeexplore.ieee.org/document/7742951). It is not clear what the role of the transposed convolution is in that case. It seems that the encoder does not change the nodes nor the edges of the graph, only the features, and the filters of the transposed convolution are learnt. If the operation is analogous to the transposed convolution on images, then given that the number of nodes in the graph does not change in the encoder layers (no graph coarsening operations are applied), then learning an additional convolution should be analogous (see e.g. https://arxiv.org/pdf/1603.07285.pdf Figure 4.3.). Could the authors comment on that?\n\nDetails on the pooling operation performed after the transposed convolution are missing (see e.g. https://arxiv.org/pdf/1805.00165.pdf, https://arxiv.org/pdf/1606.09375.pdf). Does the pooling operation coarsen the graph? if so, how is it then upsampled to match the input graph?\n\nFigure X in section 2.1. does no exist.\n\nSupervised loss in section 2.2.1 seems to disregard the sum over the nodes which have labels.\n\n\\hat A is not defined when it is introduced (in section 2.2.2), it appears later in section 2.3.\n\nSection 2.2.2 suggests that additional regularization (such as L2) is still required (note that the introduction outlines the proposed loss as a combination of reconstruction loss and supervised loss). An ablation study using either one of both regularizers should be performed to better understand their impact. Note that hyper-parameters chosen give higher weight to L2 regularizer.\n\nSection 3 introduces a bunch of definitions to presumably compare GCN against SRGCN, but those measures of influence are not reported for any model.\n\nExperimental validation raises some concerns. It is not clear whether standard splits for the reported datasets are used. It is not clear whether hyper-parameter tuning has been performed for baselines. Authors state \"the parameters of GCN and GAT and SRGCN are the same following (Kipf et al; Velickovic et al.)\". Note that SRGCN probably has additional parameters, due to the decoder stacked on top of the GCN. Reporting the number of parameters that each model has would provide more insights. Results are not reported following standards of running the models N times and providing mean and std. Moreover, there are no results using the full training set.", "title": "presentation could be significantly improved, details are missing, validation is not compelling ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkxvGahOnm": {"type": "review", "replyto": "r1znKiAcY7", "review": "I appreciate the author response and additional effort to provide comparison with MoNet. I have raised my rating by 1 point. It should be noted that the edits to the revision are quite substantial and more in line of a journal revision. My understanding is that only moderate changes to the initial submission are acceptable.\n\n-----------------------------------------------\n\nThe paper introduces a new regularization approach for graph convolutional networks. A transposed GCN is appended to a regular GCN, resulting in a trainable, graph specific regularization term modelled as an additional neural network.\n\nExperiments demonstrate performance en par with previous work in the case where sufficient labelled data is available. The SRGCNs seem to shine when only few labelled data is available (few shot setting).\n\nThe method is appealing as the regularization adapts to the underlying graph structure, unlike structure-agnostic regularization such as L1.\n\nUnclear why the results are not compared to MoNet (Monti et al. 2017) which seems to be the current state-of-the-art for semi-supervised classification of graph nodes.\n\nOverall, well written paper with an interesting extension to GCN. The paper is lacking a comprehensive evaluation and comparison to latest work on graph neural networks. The results in the few shot setting are compelling.", "title": "interesting extension to GCNs, somehwat lacking a comprehensive evaluation", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rkli05svnm": {"type": "review", "replyto": "r1znKiAcY7", "review": "Edited: I raised the score by 1 point after the authors revised the paper significantly.\n\n--------------------------------------------\n\nThis paper proposes a regularization approach for improving GCN when the training examples are very few. The regularization is the reconstruction loss of the node features under an autoencoder. The encoder is the usual GCN whereas the decoder is a transpose version of it.\n\nThe approach is reasonable because the unsupervised loss restrains GCN from being overfitted with very few unknown labels. However, this paper appears to be rushed in the last minute and more work is needed before it reaches an acceptable level.\n\n1. Theorem 1 is dubious and the proof is not mathematical. The result is derived based on the ignorance of the nonlinearities of the network. The authors hide the assumption of linearity in the proof rather than stating it in the theorem. Moreover, the justification of why activation functions can be ignored is handwavy and not mathematical.\n\n2. In Section 2.2 the authors write \"... framework is shown in Figure X\" without even showing the figure.\n\n3. The current experimental results may be strengthened, based on Figures 1 and 2, through showing the accuracy distribution of GAT as well and thoroughly discussing the results.\n\n4. There are numerous grammatical errors throughout the paper. Casual reading catches these typos: \"vertices which satisfies\", \"makes W be affected\", \"the some strong baseline methods\", \"a set research papers\", and \"in align with\". The authors are suggested to do a thorough proofreading.\n\n", "title": "Idea is reasonable; work is preliminary", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SkghBg9YCX": {"type": "rebuttal", "replyto": "rkli05svnm", "comment": "We thank the reviewer for the constructive comments and have revised the paper significantly.\n1) We have greatly simplified the mathematics in section 3. We reorganized the material and simplified the notations that causing much confusion before. In particular, we removed the notion of influence distribution of Xu et al., as we find that this notion is not necessary for our purpose. We think that the revised version provides a clearer and more mathematical explanation on why shallow GCN is not sufficient for few-shot learning and why standard regularization doesn\u2019t help.\n\n2) We have made a thorough revision and proofreading to eliminate grammatical errors throughout the paper. We are sorry for causing much trouble in the first version. \n\n3) In order to strengthen the experimental results, we revised the experimental part. We have added MoNet (suggested by Reviewer 2) as another baseline and provide standard deviation of accuracy in all the experimental results as suggested. Some of the results are as follows, and the rest is in the revised paper.\n-----------------------------------------------------------------------------------------------------------------\n                                                                       Cora\n-----------------------------------------------------------------------------------------------------------------\n                           10                        20                       30                        40                      50\n-----------------------------------------------------------------------------------------------------------------\nGCN       44.69 +/- 8.62    57.84 +/- 7.89    65.42 +/- 5.86    67.74 +/- 4.56   72.64 +/- 3.47\n-----------------------------------------------------------------------------------------------------------------\nMoNet  43.92 +/- 8.61    56.20 +/- 7.48    62.08 +/- 5.35    65.43 +/- 4.08   70.04 +/- 3.55\n-----------------------------------------------------------------------------------------------------------------\nGAT      35.78 +/- 12.17  50.45 +/-12.35   59.58 +/- 8.33    62.35 +/- 5.24    67.72 +/- 4.25\n-----------------------------------------------------------------------------------------------------------------\nSRGCN 50.04 +/- 11.73  64.18 +/- 7.11    70.13 +/- 4.25     71.63 +/- 4.03   76.00 +/- 2.64\n-----------------------------------------------------------------------------------------------------------------\nIn addition, we provide the accuracy distribution of GAT and MoNet. Their box plots are shown in Section 5.2 and distribution histograms are presented in the appendix due space constraint. Furthermore, we have provided more detailed discussions on the experimental results. \n\n4) We also add results on standard splits. In this setting, SRGCN is still better than GCN and MoNet and is only slightly inferior to GAT, while being arguably much simpler than GAT. The results are as follows (the experimental results except SRGCN are copied from previous work) .\n----------------------------------------------------------------------------\n                      Cora              Citeseer            Pubmed\n-----------------------------------------------------------------------------\nGCN        81.4 +/- 0.5     70.9 +/- 0.5        79.0 +/- 0.3\n-----------------------------------------------------------------------------\nMoNet    81.7 +/- 0.5             ---                78.8 +/- 0.3\n-----------------------------------------------------------------------------\nGAT         83.0 +/- 0.7     72.5 +/- 0.7        79.0 +/- 0.3\n-----------------------------------------------------------------------------\nSRGCN    82.3 +/- 0.6     71.8 +/- 0.4        79.0 +/- 0.3\n-----------------------------------------------------------------------------\n\nThese new experiments confirm our previous claims that our SR regularization could improve accuracy significantly for few-shot learning. The results on standard splits shows that it is also an effective regularization method for general purpose.\n", "title": "Response to Reviewer3"}, "S1l0cpFtCm": {"type": "rebuttal", "replyto": "HkxvGahOnm", "comment": "Thanks for the constructive comments and feedback. We have revised the paper accordingly. Major revisions are summarized as follows.\n1) We have added MoNet as another baseline as suggested. We have also added the standard deviation of accuracy in all the experimental results as suggested by reviewer 3. The experimental results of MoNet on task 1 on Cora and Citeseer are as follows (see our revised paper for more results on MoNet)\n-----------------------------------------------------------------------------------------------------------------\n                                                                        Cora\n-----------------------------------------------------------------------------------------------------------------\n                          10                        20                       30                      40                       50\n-----------------------------------------------------------------------------------------------------------------\nMoNet  43.92 +/- 8.61    56.20 +/- 7.48   62.08 +/- 5.35   65.43 +/- 4.08   70.04 +/- 3.55\n-----------------------------------------------------------------------------------------------------------------\nSRGCN 50.04 +/- 11.73   64.18 +/- 7.11  70.13 +/- 4.25   71.63 +/- 4.03   76.00 +/- 2.64\n-----------------------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------------------\n                                                                     Citeseer\n-----------------------------------------------------------------------------------------------------------------\n                            10                        20                        30                         40                     50\n-----------------------------------------------------------------------------------------------------------------\nMoNet   37.58 +/- 7.02      45.36 +/- 7.61    54.43 +/- 6.79    57.22 +/- 5.59    59.94 +/- 4.78\n-----------------------------------------------------------------------------------------------------------------\nSRGCN  48.84 +/-10.76      57.99 +/- 7.09    64.04 +/- 6.47    66.60 +/- 3.00    67.72 +/- 2.17\n-----------------------------------------------------------------------------------------------------------------\nFrom the results, our SRGCN also outperforms MoNet by a large margin. Compared with other baselines, MoNet is generally worse than GCN in task 1 but is more competitive in task 2.\n\n2) We also add results on standard splits. In this setting, SRGCN is still better than GCN and MoNet and is only slightly inferior to GAT, while being arguably much simpler than GAT. The results are as follows (the experimental results except SRGCN are copied from previous work) .\n----------------------------------------------------------------------------\n                      Cora                Citeseer                Pubmed\n-----------------------------------------------------------------------------\nGCN       81.4 +/- 0.5         70.9 +/- 0.5          79.0 +/- 0.3\n-----------------------------------------------------------------------------\nMoNet   81.7 +/- 0.5                 ---                  78.8 +/- 0.3\n-----------------------------------------------------------------------------\nGAT        83.0 +/- 0.7         72.5 +/- 0.7          79.0 +/- 0.3\n-----------------------------------------------------------------------------\nSRGCN   82.3 +/- 0.6         71.8 +/- 0.4          79.0 +/- 0.3\n-----------------------------------------------------------------------------\nThese new experiments confirm our previous claims that our SR regularization could improve accuracy significantly for few-shot learning. The results on standard splits shows that it is also an effective regularization method for general purpose.\n\n3) We have greatly simplified the mathematics in section 3. We reorganized the material and simplified the notations that causing much confusion before. In particular, we removed the notion of influence distribution of Xu et al., as we find that this notion is not necessary for our purpose. We think that the revised version provides a clearer and more mathematical explanation on why shallow GCN is not sufficient for few-shot learning and why standard regularization doesn\u2019t help.\n\n", "title": "Response to Reviewer2"}, "HJePTS9YRQ": {"type": "rebuttal", "replyto": "HyGXiScY0X", "comment": "10) ##Clarification on data splits##\nIn the standard splits, the label rate is relatively high, so for few-shot experiments, we use random splits. For each training size, we test each model on 50 random splits and report the average accuracy and the accuracy distribution. In the revision, we also report standard deviations as suggested. In order to strengthen the experimental results, we revised the experimental part. We have added MoNet as another baseline. Some of the results are as follows; and see the revised paper for more results.\n-----------------------------------------------------------------------------------------------------------------\n                                                                       Cora\n-----------------------------------------------------------------------------------------------------------------\n                           10                        20                       30                        40                      50\n-----------------------------------------------------------------------------------------------------------------\nGCN       44.69 +/- 8.62    57.84 +/- 7.89    65.42 +/- 5.86    67.74 +/- 4.56   72.64 +/- 3.47\n-----------------------------------------------------------------------------------------------------------------\nMoNet  43.92 +/- 8.61    56.20 +/- 7.48    62.08 +/- 5.35    65.43 +/- 4.08   70.04 +/- 3.55\n-----------------------------------------------------------------------------------------------------------------\nGAT      35.78 +/- 12.17  50.45 +/-12.35   59.58 +/- 8.33    62.35 +/- 5.24    67.72 +/- 4.25\n-----------------------------------------------------------------------------------------------------------------\nSRGCN 50.04 +/- 11.73  64.18 +/- 7.11    70.13 +/- 4.25     71.63 +/- 4.03   76.00 +/- 2.64\n-----------------------------------------------------------------------------------------------------------------\n\n11)##Results on standard splits##\nIn the revision, we also report experimental results on standard splits. In this setting, SRGCN is still better than GCN and MoNet and is only slightly inferior to GAT, while being arguably much simpler than GAT. The results are as follows (the experimental results except SRGCN are copied from previous work).\n----------------------------------------------------------------------------\n                      Cora                Citeseer                Pubmed\n-----------------------------------------------------------------------------\nGCN       81.4 +/- 0.5         70.9 +/- 0.5          79.0 +/- 0.3\n-----------------------------------------------------------------------------\nMoNet   81.7 +/- 0.5                 ---                  78.8 +/- 0.3\n-----------------------------------------------------------------------------\nGAT        83.0 +/- 0.7         72.5 +/- 0.7          79.0 +/- 0.3\n-----------------------------------------------------------------------------\nSRGCN   82.3 +/- 0.6         71.8 +/- 0.4          79.0 +/- 0.3\n-----------------------------------------------------------------------------\nThese new experiments confirm our previous claims that our SR regularization could improve accuracy significantly for few-shot learning. The results on standard splits shows that it is also an effective regularization method for general purpose.", "title": "Response to Reviewer1 (Part 2/2)"}, "HyGXiScY0X": {"type": "rebuttal", "replyto": "S1eCA4l927", "comment": "We thank the reviewer for the comments and we have revised the paper thoroughly. We are sorry for causing confusion and misunderstandings on some technical issues. We make the following clarifications.\n1) As noted by the reviewer, convolution operator in GCN is actually quite different from convolution on images. The encoder does not change the nodes nor the edges of the graph, only the features. In particular the dimensionality of the output feature space is much lower. \n\n2)##comment on transposed graph convolution## \nOur main idea is to reconstruct the original features and use the reconstruction errors as a regularization, so the output feature vectors in the low-dimensional space needs to be transformed back into the original space; in particular, the dimensionality needs to be lifted to match the original space. Therefore, one can think of the transposed GCN as a GCN being reversed, which also does not change the graph, but only the features.\nTo illustrate this, let\u2019s consider a simple one-layer GCN as an example. Now the output of GCN is Z=\\sigma(\\hat{A} XW), where X is the input feature matrix and W is a trainable weigh matrix. To reconstruct X from Z, the transposed GCN is applied on Z: X\u2019=\\sigma(\\hat{A}^T Z W\u2019^T). Here \\hat{A} is symmetric, so the transpose operator could be omitted and W\u2019 is another trainable weight matrix. To make the dimensionality of X\u2019 and X the same, W and W\u2019 must have the same size and the linear transformation W\u2019 also needs to be transposed before applying on Z. \n\n3) ##Clarification on pooling##\nThe pooling method used here follows e.g. https://arxiv.org/pdf/1806.03536.pdf, https://arxiv.org/abs/1706.02216, which is different from pooling in CNN for images. Here, we simply use the entry-wise max function: max(X, \\hat{A}X), which also does not change the graph, but only the features. So, it is more like an activation function and doesn\u2019t coarsen the graph. We followed previous work and called this pooling. We are sorry for causing confusion. Now we have made the pooling more explicit in the revised paper and hope that this resolves the concern of the reviewer. \n\n4) In our original paper, the supervised loss in section 2.2.1 is ambiguous. So, we have revised it in the new version.\n\n5) We have added the definition \\hat A in section 2.2.2 as suggested by the reviewer. \n\n6) ##comment on L2 regularization##\nBoth GCN and GAT use a L2 regularizer in all their experiments. But as demonstrated by our empirical and theoretical results (section 3 in the new version), L2 regularizer is not enough to handle few-show learning. Our main contribution is a new regularization. The experiments show that the performance of GCN + L2 + new regularizer significantly outperforms GCN + L2. So we think it is fair enough to say that our new regularizer is highly effective.\nWe emphasize that we don\u2019t mean to replace standard regularization methods. \n\n7) ##Simplified analysis##\nWe have greatly simplified the mathematics in section 3. We reorganized the material and simplified the notations that causing much confusion before. In particular, we removed the notion of influence distribution of Xu et al., as we find that this notion is not necessary for our purpose. We think that the revised version provides a clearer and more mathematical explanation on why shallow GCN is not sufficient for few-shot learning and why standard regularization doesn\u2019t help.\n\n8) ##comment on higher weight for L2 regularizer##\nFor the weights of the regularizers, one should see that higher weight doesn\u2019t mean the corresponding regularizer is more important. In our case, although a higher weight is given to the L2 regularizer compared with the reconstruction loss (0.0005 vs 0.0001), the overall effect of the reconstruction loss is much stronger. The main reason that the weight of the reconstruction regularizer is smaller is that the reconstruction loss is a function of all feature vectors, whose total size is n*d (n is the number of nodes and d is the number of input features). n*d is typically much larger than the number of parameters in our setting. In particular, after training, 0.0001*reconstruction loss is always much larger than 0.0005*L2.\n\n9) ##Hyperparameters##\nNote the encoder and decoder of SRGCN are two symmetric GCNs and we use the same set of hyper-parameters for them, which are exactly the same as suggested by Kipf et al. The only additional parameter SRGCN has is the weight of the reconstruction loss, which is set to 0.0001 in all experiments. All the parameters were explicitly listed in section 5.1.\n", "title": "Response to Reviewer1 (Part 1/2)"}}}