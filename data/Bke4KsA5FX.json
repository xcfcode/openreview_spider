{"paper": {"title": "Generative Code Modeling with Graphs", "authors": ["Marc Brockschmidt", "Miltiadis Allamanis", "Alexander L. Gaunt", "Oleksandr Polozov"], "authorids": ["mabrocks@microsoft.com", "miallama@microsoft.com", "algaunt@microsoft.com", "polozov@microsoft.com"], "summary": "Representing programs as graphs including semantics helps when generating programs", "abstract": "Generative models forsource code are an interesting structured prediction problem, requiring to reason about both hard syntactic and semantic constraints as well as about natural, likely programs. We present a novel model for this problem that uses a graph to represent the intermediate state of the generated output. Our model generates code by interleaving grammar-driven expansion steps with graph augmentation and neural message passing steps. An experimental evaluation shows that our new model can generate semantically meaningful expressions, outperforming a range of strong baselines.", "keywords": ["Generative Model", "Source Code", "Graph Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper presents an interesting method for code generation using a graph-based generative approach.  Empirical evaluation shows that the method outperforms relevant baselines (PHOG).\n\nThere is consensus among reviewers that the methods are novel and is worth acceptance to ICLR."}, "review": {"SyxeYOdu3Q": {"type": "review", "replyto": "Bke4KsA5FX", "review": "The paper proposes a code completion task that given the rest of a program, predicts the content of an expression. This task has similarity to code completion tasks in the code editor of an IDE. The paper proposes an interesting problem, but the paper would benefit if writing and evaluation are significantly improved.\n\nThe work builds on prior research by Allamanis et al. 2018b that performs such completions of single variables by picking from the variables in the scopes. The difference here is that portions of parse trees are predicted as opposed to a single variables, where the algorithm from the prior research is used to predict single variables.\n\nWriting-wise the paper is hard to read on the technical part with many unclear details and this portion needs a good amount of extra explanations. The Epsilon set includes triples which are not described and need understanding equation (2). The first element of this triple is an edge label <edge>($a$, $v$) where $a$ is an AST and $v$ is a node. Thus, edges of the graph end up between entire ASTs and nodes? While I can see how could this make sense, there is certainly lack of explanation going on here. Overall, this part is hard to parse and time-consuming to understand except at high level. Furthermore, the text has many functions without signatures and they seem to be used before they are defined (e.g. getRepresentation).\n\nTechnically, the approach also seems very similar to N3NN by Parisotto et al, ICLR 2017. There should be more elaboration on what is new here. Otherwise, the novelty of the paper really is just combining this work with Allamanis et al. 2018b.\n\nIn terms of evaluation, the task seems to be on a different set of expressions than the one explained in the exposition. How many expressions where there in the evaluation programs and how many were chosen to evaluate on and based on what criteria. It seems from the exposition that expressions with field accessed and function calls are not possible to be generated, but then some completions show method calls. How much of the full task is actually solved? In particular, several of the cited prior works solve specific problems like constants that are ignored here.\n\nThe evaluation is mostly an ablation studies of the proposed approach by removing edges from the final idea. \nBesides this, the paper also introduces a new dataset for showcasing the technique and does not report sizes and running times, essentially not answering basic questions like what is the trade-off between the different techniques. Comparison to actual prior works on similar tasks is also lacking (some TODO is left in the paper), but there is the claim that existing neural techniques such as seq2seq perform \"substantially worse\". I guess the authors have extra experiments not included for lack of space or that the evaluation was not ready at submission time.\n", "title": "Overall good ideas, but not quite ready", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HJl5-iMxAX": {"type": "rebuttal", "replyto": "SyxeYOdu3Q", "comment": "Thank you again for your valuable and detailed feedback. We will add the content of the additional answers we've given in the comments here to the next revision of the paper. Concretely, we will provide more details on (1) the selection of samples in our dataset as well as how we infer the grammar (this will clarify the issue of method calls as well), (2) the exact meaning of lastUse in Alg. 2, with a note on the relationship to Allamanis et al. 2018 (3) the relation to R3NN. Are there any other open questions you had that we overlooked?\n\nFinally, as many of the points you raised in your initial review have been clarified / resolved, would you consider raising your rating for the updated version of our paper?", "title": "Further questions?"}, "Byen_drhpX": {"type": "rebuttal", "replyto": "H1eHUfG36m", "comment": "lastToken, lastUse, lastSibling and parent return unique nodes. While you are right that lastUse could be understood to return all locations in which a variable may have been used in an execution (which would indeed require several edges), we mean the lexically last occurrence of the node, which is uniquely determined. This is a simplification of the approach of Allamanis et al. (2018), though we are not stating this explicitly in our submission. We will clarify this in the next revision.\n\n[The actual source code used to compute is posted anonymously on http://paste.debian.net/hidden/7f6ba717/ now]\n\n\n\"inheritedAttr\" returns the node corresponding to the inherited attribute of a node, e.g., for node 10 in step 8 of Fig. 2, inheritedAttr(10) would return 0. We will clarify this in the next revision as well.\n\n\nThank you for your very detailed questions, they do help as us a lot to identify parts of the paper that are insufficiently precise.", "title": "More answers"}, "Ske3KQBnT7": {"type": "rebuttal", "replyto": "rkxpzWG36m", "comment": "On choosing expressions:\n\nWe are greedily picking the largest allowed expression from the ASTs that we consider. So for example, from \"if ((boolVar || x > y) && UserDefinedFoo(x + y + z - 1))\", we select \"boolVar || x > y\" and \"x + y + z - 1\" and no other subexpressions.\n\n\nOn vocabulary size:\nIn the graph-expansion setting, there is no classical decoder vocabulary. There is a grammar that we infer from the expressions observed in the training data. This yields rules such as \"|Expr| -> ! |Expr|\", \"|Expr| -> |Expr| + |Expr|\", \"|Expr| -> |Expr|.Equals(|Expr|\", where |Expr| is a non-terminal. That inferred grammar has 222 expressions in total for our dataset, and includes the built-in functions applicable to the datatypes we support.\n\nSome non-terminals are treated specially, as discussed in the paper. Concretely, |Variable| is expanded using pickVariable from Eq. 4, and |${Type}Literal| is expanded using pickLiteral from Eq. 5. The vocabularies used in pickLiteral have size 50, i.e., we pick from the 50 most common integer/character/string literals observed in the training data.\nThe copying part of the pickLiteral has access to all tokens in the context that are not language keywords (\"for\", \"public\", etc). Anecdotally, we can report that this makes no significant difference -- this masking of keywords/nonterminal nodes in the context was disabled by a bug for some experimental runs without negative effects.\n\n\nOn generating functions:\nSee above - the limited number of functions have dedicated grammar rules, i.e., \"|Expr|.Equals(|Expr|)\" is treated analogous to generating \"|Expr| + |Expr|\".\n\n\nOn determining \"user-definedness\" of methods:\nAs we only support methods, there is a straightforward check. When we observe \"var.Method(${args})\", we check if \"var\" is of an allowed type; this implies that Method is implemented in the type and not by the user. If the arguments ${args} are also in our fragment of the language, we include the full expression in the dataset; the inferrence of grammar rules from the observed ASTs then yields a rule |Expr| -> |Expr|.Method(...)\".\n\nThis is actually not completely correct, as C# has an extension mechanism by which methods can be added to existing type. However we found this to be seldomly used on the types we are restricting ourselves to, though this leads to a handful of user-defined extension methods occuring in our grammar (e.g., there is \"|Expr| -> |Expr|.VirtualPathToDbPath()\")", "title": "More answers"}, "HJeYe4p9p7": {"type": "rebuttal", "replyto": "SyxeYOdu3Q", "comment": "[Second part of reply, as we were over 5000 chars]\n\n> In terms of evaluation, the task seems to be on a different set of \n> expressions than the one explained in the exposition. How many \n> expressions where there in the evaluation programs and how many were \n> chosen to evaluate on and based on what criteria.\n\nThis is discussed in the first paragraph of Sect. 5:\n- \"all expressions of the fragment that we are considering (i.e.,\n   restricted to numeric, Boolean and string types, or arrays of such\n   values; and not using any user-defined functions)\"\n- \"343974 samples overall [...] ~100k samples generated from 114 projects\n   into a 'test-only' [...] remaining data we split into\n   training-validation-test sets (60-20-20)\"\n\nWe are not sure what additional information you are asking for here. Could you please elaborate?\n\n> It seems from the exposition that expressions with field accessed and \n> function calls are not possible to be generated, but then some \n> completions show method calls.\n\nWe exclude _user-defined_ functions but allow the built-in functions (and fields) of the considered data types, which primarily include string manipulation/tests (\"Substring\", \"IndexOf\", etc.) and generic functions such as \"Equals\" or the \"Length\" field of arrays. These built-in function calls are added as new productions in the underlying C# expression grammar.\n\n> In particular, several of the cited prior works solve specific \n> problems like constants that are ignored here.\n\nWe do handle constants (i) by generation from a vocabulary and (ii) by copying from context (cf. \"Choosing Productions, Variables & Literals\" and equation (5)); what other problem do you have in mind here?\n\n> The evaluation is mostly an ablation studies of the proposed approach \n> by removing edges from the final idea. Besides this, the paper also \n> introduces a new dataset for showcasing the technique and does not \n> report sizes and running times, essentially not answering basic \n> questions like what is the trade-off between the different techniques.\n\nWe will update the paper to include additional statistics about the experiments (for example, how many epochs were needed to train to convergence, how long an epoch takes on our dataset). Are there any specific statistics that you are interested in besides runtime?\n\n> Comparison to actual prior works on similar tasks is also lacking \n> (some TODO is left in the paper), but there is the claim that existing \n> neural techniques such as seq2seq perform \"substantially worse\".\n\nA seq2seq baseline achieves 21.8% accuracy (28.1% accuracy in the 5 most probable results returned by beam search) on the test dataset. The perplexity is very high 87.5, primarily driven by uncertainty about generating variables in generated expressions. On the test-only dataset, these are 10.8% accuracy (16.8% @5) and perplexity 130.5.\nWe have also updated the paper with the PHOG results.", "title": "Clarification and answers to review questions (Part 2)"}, "Skly0QT9pm": {"type": "rebuttal", "replyto": "SyxeYOdu3Q", "comment": "Thanks for the thorough review. We will try to improve the writing to make the parts of the paper you found hard to follow easier to read.\n\n> The work builds on prior research by Allamanis et al. 2018b that \n> performs such completions of single variables by picking from the \n> variables in the scopes. The difference here is that portions of parse \n> trees are predicted as opposed to a single variables, where the \n> algorithm from the prior research is used to predict single variables.\n\nWe want to point out that this is not quite precise -- the model from Allamanis et al. 2018b applies a much more complex analysis to identify the correct variable, introducing speculative data flow edges (i.e., \"how would the graph look like if a certain variable were used in this location\"). Our method is much more simple, and is more akin to using a pointer network to select a variable available in scope.\n\n> Writing-wise the paper is hard to read on the technical part with many \n> unclear details and this portion needs a good amount of extra \n> explanations. The Epsilon set includes triples which are not described \n> and need understanding equation (2). The first element of this triple \n> is an edge label <edge>($a$, $v$) where $a$ is an AST and $v$ is a \n> node.\n\nYou seem to be confusing functions $F(a, v)$ and edges $(u, label, v)$, which are not the same thing and should not be used interchangeably. A function like \u201cparent(a, v)\u201d returns a node from the partial AST $a$ (in this case, the parent node of $v$). An edge is a triple of \u201c(node, label, node)\u201d, explicitly defined in the paragraph right above Eq. (2). The edge labels and function names are (generally) not shared.\n\nWe apologize for this confusion. We have updated the paper with a new Notation paragraph that formally defines the constituents of edges triples and functions.\n\n> Thus, edges of the graph end up between entire ASTs and nodes?\n\nNo, edges are always between nodes. The functions like parent(a, v) return a /node/ from the partial AST $a$ (e.g. in this case, the parent node of $v$), and should not be confused with edge types.\n\n> Furthermore, the text has many functions without signatures and they \n> seem to be used before they are defined (e.g. getRepresentation).\n\nWe indeed stripped the text of explicit signatures for space reasons (as we felt they were implicitly defined anyway), but we remedied this somewhat in the new revision. We have added a Notation paragraph to explain the used functions, which we hope is sufficient. If you feel more context is needed, we can also include explicit signatures everywhere.\n\n> Technically, the approach also seems very similar to N3NN by Parisotto \n> et al, ICLR 2017. There should be more elaboration on what is new \n> here.\n\n[We assume this was a typo, and you refer to R3NNs] The core difference is that R3NNs use only the tree structure. While their up-then-down recursion scheme allows information sharing between different sibling subtrees in principle, no explicit domain knowledge is integrated to directly connect relevant parts of the tree. Our core contribution is to show how to integrate richer domain knowledge directly into the model.\n\nUsing a R3NNs in a generative procedure also implies a quadratic computational cost, as each partial tree is traversed twice at each expansion step (summing up to roughly \\sum_{1 < i < V} 2i = V^2 + V), whereas our sequential graph propagation requires only a linear pass over all nodes in our graph, where each node in the expansion tree is visited at most twice (once for inherited and once for synthesized attributes).\n\n[On a side note, the authors of the R3NN paper have communicated to us privately that training the model was extremely hard, requiring careful tuning of hyperparameters to ensure convergence to a reasonable state.\nIn contrast, our model required almost no hyperparameter tuning to get good results, and we only did a cursory exploration to create the experiments in the paper]", "title": "Clarification and answers to review questions"}, "S1ggSfa567": {"type": "rebuttal", "replyto": "Hyl3dbvq2X", "comment": "Thank you for your kind review!\n\n> The authors have a qualitative evaluation section describing the\n> differences in errors made by various methods. Making this more\n> quantitative by categorizing the errors and computing their frequency\n> would be quite interesting.\n\nWe thought about this as well, but we found it hard to automatically categorize errors beyond considering syntax, type and non-typing semantic errors. We have not reported the numbers for syntax errors in this paper, as all models produce syntactically valid expressions in over 99% of the cases. If you have ideas for metrics that are effectively computable, we are happy to provide additional experimental data. \n", "title": "Error categorization"}, "S1gbGzT5aX": {"type": "rebuttal", "replyto": "rkeutuqqnm", "comment": "Thank you for your careful review and kind comments. We hope that we can further improve our submission with your feedback.\n\n> 1, I think it would be great to provide more statistics of the\n> proposed dataset, e.g., the average number of tokens, the average size\n> of ASTs. \n\nAs a reminder, there are 344k samples in the dataset overall. There are on average 4.32 (stddev 3.80) tokens per expression to generate [2 tokens: 88k, 3 tokens: 116k, 4 tokens: 38k, 5 tokens: 32k, 6 tokens: 21k, 7 tokens or more: 49k]. The generated trees on average have 3.69 (stddev 3.06) production steps. The dataset is clearly dominated by simple expressions such as \u201cx > y\u201d and \u201cx[y]\u201d (we have filtered out single-variable expressions), but longer expressions are included often as well (cf. Sect. 2 for more details on the selection process). We can also report that we have successfully extended the model to generate whole blocks of statements in a different research project.\n\n> 2, Given the dynamic nature of the graph generation process, I am\n> curious about the efficiency of the proposed method. It would be great\n> to provide some run time information. Also, since recurrent networks\n> are heavily used throughout the model, I wonder how difficult the\n> training process is.\n\nRegarding performance: Training is relatively efficient, as we know the target expansion graph and can thus compute the representations of all nodes in the expansion graph in one go. While that computation is relatively easy to parallelize, its length is the length of the longest path in the target expansion graph. We currently cap this at 50 during training (which excludes only few examples in our dataset). Combined with the computationally relatively expensive GGNN-based encoder, training on a K80 processes around ~25 samples/s, compared to about ~60 samples/s just for the encoder. \n\nAs we needed to implement beam search at test time, we have decided to entirely forego batching (and, indeed, GPU usage) at test time, and have essentially implemented Alg. 1 from the paper directly in Python, pruning back the set of beams after each expansion step. For this, \u201cgetRepresentation\u201d is a lazy implementation of Eq. (2), computing node representations by message passing \u201con demand\u201d. We made no efforts to optimize this implementation, instead aiming for simplicity to avoid bugs. An implementation in a dynamic computation graph framework such as TF Eager or PyTorch should be able to significantly outperform our code.\n\nWe will report precise runtime statistics in the supplementary material in our next revision. We also plan to release our implementation.\n\n> 3, It would be great to also compare the log likelihood on the test\n> set.\n\nThe perplexity shown in Table 1 is directly proportional to the log likelihood in the test set, albeit normalized per token. Could you please elaborate on what you had in mind and how this differs from the results in Table 1?\n\n> 4, It is unclear from the paper that whether authors use a pre-trained\n> GGNN as encoder or train the encoder end-to-end with the decoder from\n> scratch.\n\nThe full network is trained end-to-end with no pretraining. We will clarify this in the text.\n\n> 5, It would be great to improve figure 2 as it is not easy to read.\n> Maybe draw another graph to illustrate the temporal evolution of AST?\n\nWe\u2019re sorry that this figure is not legible. Given the suggested page limit we wanted to make this explanation as concise as possible. In the new version, we redrew this figure as a sequence of multiple minifigures that show the evolution of the propagation.\n(This has extended the content length beyond the recommended 8 pages, but we agree with you that clarity of this illustration is more important.)\n", "title": "Answers to review questions"}, "rygKSlpcp7": {"type": "rebuttal", "replyto": "Bke4KsA5FX", "comment": "We have updated our submission taking some of the reviewer's feedback into account and hope that this improves its clarity. Primarily, we have made the following changes:\n- Experiments: We have included results for the PHOG and seq2seq baselines in the paper.\n- Notation: We have slightly improved the notation in Alg. 2 and Eq. (2) and added a paragraph giving an overview of used notation in Sect. 3.\n- Visualization of tree expansion: We have replaced Fig. 2 by a step-by-step version that should be easier to follow for the reader.\n\nIn the next revision, we plan to reflect the remaining feedback and make the following changes:\n- Statistics about dataset and (runtime) performance of decoders.\n- Experiments with a graph2seq baseline model.\n\n", "title": "Overview of changes in first revision"}, "rkeutuqqnm": {"type": "review", "replyto": "Bke4KsA5FX", "review": "In this paper, authors propose a conditional generative model which predicts the missing expression given the surrounding code snippet. Authors represent programs as graphs and use some off-the-shelf encoder to obtain representations for all nodes. Inspired from the attribute grammar, authors augment every node in AST with two new nodes which contain inherited and synthesized information. Based on GGNN, a grammar-driven decoder is further proposed to sequentially generate the AST and the corresponding program. Authors also propose a large dataset which is built from open sourced projects. Experimental results on this dataset show that the proposed method achieves better predictive performance compared to several recent work. \n\nStrength:\n\n1, The problem this paper tries to tackle, i.e., building generative models of code, is very challenging and of great significance. \n\n2, The overall model is a novel and successful attempt to incorporate the structure information of the program into neural networks. I think it will be inspiring for other machine learning based programming applications.\n\n3, The results are very promising and impressive, especially given the large size of the proposed dataset. For example, the top 5 accuracy of predicting correct expression on unseen projects is 57%.\n\nWeakness:\n\n1, I think it would be great to provide more statistics of the proposed dataset, e.g., the average number of tokens, the average size of ASTs. \n\n2, Given the dynamic nature of the graph generation process, I am curious about the efficiency of the proposed method. It would be great to provide some run time information. Also, since recurrent networks are heavily used throughout the model, I wonder how difficult the training process is. \n\n3, It would be great to also compare the log likelihood on the test set.\n\n4, It is unclear from the paper that whether authors use a pre-trained GGNN as encoder or train the encoder end-to-end with the decoder from scratch.\n\n5, It would be great to improve figure 2 as it is not easy to read. Maybe draw another graph to illustrate the temporal evolution of AST?\n\nOverall, I think this paper has made a great progress towards neural modelling of programs and recommend it to be accepted for ICLR.\n", "title": "Novel Model for Programs and Impressive Results", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hyl3dbvq2X": {"type": "review", "replyto": "Bke4KsA5FX", "review": "The paper introduces a 'code generation as hole completion' task and associated dataset, ExprGen. The authors proposed a novel extension of AST code generation which uses what they call Neural Attribute Grammars. They show the proposed method does well on this task, compared to ablations of their model (which are similar to previous AST approaches).\n\nThe task and dataset are interesting, and the comparison of the proposed method to baselines seems thorough. \n\n*Details to Improve*\nThe authors have a qualitative evaluation section describing the differences in errors made by various methods. Making this more quantitative by categorizing the errors and computing their frequency would be quite interesting.", "title": "Interesting task and dataset.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJeXntTY37": {"type": "rebuttal", "replyto": "Bke4KsA5FX", "comment": "The authors of the non-neural PHOG model have now run additional experiments on the dataset used in our paper. Note that their model is a language model and thus only takes code \"left of\" the hole to fill into account, and that their framework is fairly generic and does not have special modeling of which variables are in scope etc. Hence, the results of their model are only a lower bound of what their model could achieve on this task with suitable extensions; e.g., it would be possible to extend their formalism to also take code after the hole to fill into account.\n\nBearing these limitations in mind, their results (i.e., their row in Table 1) are as follows:\nOn the \"Test\" dataset:\n Acc@1: 34.8%\n Acc@5: 42.9%\nOn the \"Test-only\" dataset:\n Acc@1: 28.0%\n Acc@5: 37.3%", "title": "PHOG experimental results"}}}