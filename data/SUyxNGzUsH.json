{"paper": {"title": "VilNMN: A Neural Module Network approach to Video-Grounded Language Tasks", "authors": ["Hung Le", "Nancy F. Chen", "Steven Hoi"], "authorids": ["~Hung_Le2", "~Nancy_F._Chen1", "~Steven_Hoi2"], "summary": "We propose VilNMN, a novel neural module network approach for video-grounded language tasks, which achieves strong performance in both video QA and video-grounded dialogue tasks. ", "abstract": "Neural module networks (NMN) have achieved success in image-grounded tasks such as question answering (QA) on synthetic images. However, very limited work on NMN has been studied in the video-grounded language tasks. These tasks extend the complexity of traditional visual tasks with the additional visual temporal variance.  Motivated by recent NMN approaches on image-grounded tasks, we introduce Visio-Linguistic Neural Module Network (VilNMN) to model the information retrieval process in video-grounded language tasks as a pipeline of neural modules. VilNMN first decomposes all language components to explicitly resolves entity references and detect corresponding action-based inputs from the question. Detected entities and actions are used as parameters to instantiate neural module networks and extract visual cues from the video. Our experiments show that VilNMN can achieve promising performance on two video-grounded language tasks: video QA and video-grounded dialogues. ", "keywords": ["neural modular networks", "video-grounded dialogues", "dialogue understanding", "video understanding", "video QA", "video-grounded language tasks"]}, "meta": {"decision": "Reject", "comment": "The authors propose a neural module based approach for reasoning about video grounding.  The goal is to provide performance and interpretability.  Unfortunately, the reviewers found the paper opaque, the results confusing, and expressed repeated concerns about the novelty, fairness of comparisons and concerns that the surprising results were not sufficiently well justified by the paper (or the author's response)."}, "review": {"D_mav22ZWd-": {"type": "review", "replyto": "SUyxNGzUsH", "review": "Description:\n\nThis paper introduces the Visio-Linguistic Neural Module Network (VilNMN) consisting of a pipeline of dialogue and video understanding neural modules. Motivated by Hu et al. (2017), Kottur et al (2017), this paper extends the NMNs on video tasks for interpretable neural models. The model explicitly resolves entity references (dialog understanding) and detects actions from videos (video understanding) for response generation. Experiments show that NMNs achieve competitive results on AVSD (video-dialog) and TGIF-QA (video-QA) benchmarks. \n\nStrengths:\n- New modules for video understanding (\u201cwhere\u201d, \u201cwhen\u201d) have been proposed. A step towards interpretability of compositional neural networks \n- Ablation studies have been provided to understand the importance of each module in the VILNMN model.  \n- The breakdown of relative CIDEr/BLEU (Figure 7 in supplementary) for different context and video length is interesting. \n- SOTA results on TGIF-QA (Video QA) while competitive results on AVSD (dialog task). \n\nWeaknesses:\n- Availability of code is not discussed which is essential for reproducibility. \n- It would help to specify whether sentence vs corpus level BLEU was used for evaluation\n- Human evaluation is not provided. Limitations of automatic metrics and their reliability in language generation have been discussed repeatedly. See (Reiter and Belz, 2009; Novikova et al., 2017; Reiter, 2018). \n- Table 6 denotes that the evaluation results decrease with longer video or larger context modeling which is the main focus of the paper.   \n- It might be argued that this approach would not generalize. How would this model scale when the dialog becomes challenging (in terms of disfluencies, ellipses or alignment, topic switch, etc apart from co-reference; see Haber et al 2019 Photobook dataset for brief summary of dialog phenomena)? Similarly when the videos become more complex, would action recognition suffice? \n- CorefNMN (Kottur et al 2017) was designed specifically for co-reference resolution in the dialog. The paper could be similarly improved by explicitly motivating the specific utility of the NMN modules compared to high-level description- eg. to capture co-reference (dialog) and action recognition (video). \n- An analysis of the AVSD dataset would help understand the importance of the dialog context in the dataset - focus of one of the modules in this paper. (See Agarwal et al 2020 study for Visual dialog)\n\nQuestions:\n- Since AVSD is also posed as a retrieval task (Alamri et al. 2019), have the authors evaluated the system on ranking based metrics?  \n- Could the authors clarify why accuracy is also not reported for CountQA in Table 4? \n- It would help to explicitly mention the neural modules previously defined and the novel modules, eg how the \u201cfind\u201d module differs from Kottur et al. 2018. \n- Have the authors experimented with the dialog-based modules from Kottur et al. 2018 - eg. \u201crefer\u201d module? \n- In Fig 2 (as well as the main text), it would help to clarify if the underlying text encoder is shared for the dialog history, question, and caption. \n- How are the audio signals incorporated in the VilNMN mentioned in Table 2? Pardon if I missed this. \n- Have the authors experimented with pre-trained weights (decoders)? \n\nSuggestions/Comments:\n- Previously Johnson et al. (2017a); Hu et al. (2017), Kottur et al. (2018) have all explored NMN for visio-linguistic tasks (such as VQA, Visual dialog), the nomenclature \u201cVisio-Linguistic Neural Module Network (VLNMN)\u201d seems too broad. Something on the grounds of \u201cActionNMN/ActNMN\u201d would do justice to the work. \n- Model descriptions of ablations in Table 3 could be improved for clarity. \n- Implementation details could be further specified - the framework, all other hyperparameters to ease reproducibility. \n\n--------------------------------------------------------------------------------------------------------------------------------------------------------\nPost Rebuttal update:\n\nI would like to thank the authors for answering the questions. I believe that an updated version addressing all the concerns in detail will find its place in other future conferences. Original rating is maintained.\n\n\n", "title": "Adapting Neural Module Network (NMN) for Video dialog and QA", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "MuS5HYVnPwx": {"type": "review", "replyto": "SUyxNGzUsH", "review": "This paper studies the language grounding aspect of video-language problems. It proposes a Neural Module Network (NMN) for explicit reasoning of visually-grounded object/action entities and their relationships. The proposed method is demonstrated to be somewhat effective in the audio-visual dialogue task and has been shown superior to existing works on video QA. Overall, the paper is motivated clearly and is delivered with good clarity. The followings need to be clarified.\n\ni) The proposed model demonstrates impressive results on TGIF-QA but without any insightful justification. Since the questions in TGIF-QA are short and usually do not involve complicated reasoning, intuitively, a heavy reasoning scheme might not necessarily pay off. Please clarify the performance gain and possible reasons. Also, \"soft label programs\" lack the necessary context (and should be in bold instead in Tab. 4).\n\nii) Including intense model variants in the main result table (Tab. 2) gives this paper a somewhat unfair advantage, especially when the best performing method on each metric comes from different model variants. The validation set (from both AVSD and TGIF-QA) is supposed to serve the purpose of model architecture search and ablation studies. Besides, the underlines in the lower part of Tab. 2 should go to method VGD-GPT2.\n\n========== Post-Rebuttal ==========\n\nConcerns on paper/results clarity still persist. Lowering my rating to 5.", "title": "Interesting idea; results need further clarification", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "W1x0DGRphM0": {"type": "rebuttal", "replyto": "gIovmw9vbz", "comment": "We thank R2 for the detailed feedback. We appreciate that the reviewer found this work an interesting method using NMN in complex video-grounded language tasks. Please find below our response:\n\n* Concern #1:\n\u201chow would the \"when\" module be able to localize the same object/entity over time given no additional supervision? I think object tracking would greatly be beneficial in this case.\u201d \n\nIn our current approach, we pass the outputs from the \u2018where\u2019 modules applied on all entities/objects into the next \u201cwhen\u201d module. One motivation for this is that most of the question utterances in dialogues are usually a single sentence length. Each sentence often contains multiple subjects/objects but less than one action-related phrase. As suggested by the reviewer, object tracking might improve the reasoning performance of the reasoning steps between \u2018when\u2019 and \u2018where\u2019 modules in this case. We will try to investigate this direction in the future.\n\n* Concern #2: \n \u201cWhy the authors only used ResNet features instead of using similar features as in AVSD?\u201d\n\nThank you for your suggestion! We did not have enough time to report the results using an object-based feature counterpart. We reported the performance using ResNet-152 to fairly compare with the baseline models. For completeness, we will try to report similar results using features such as F-RCNN in the final version. \n\n* Concern #3: \n\u201cMore analysis on the results on the TGIF-QA\u201d \n\nAbout the results on the TGIF-QA benchmark, our performance gain can be explained as it comes from the linguistic bias decoded as reasoning structure programs. The decoded programs allow more precise neural (attention) operations between detected entities/actions and the video features. Specifically, in TGIF-QA, the questions are designed to follow a fixed question type distribution with certain question templates. This makes it much easier for the question parser to parse program structures. In our experiments, using exact-match accuracy of parsed programs vs. label programs to gauge performance of the question parser, our question parser can achieve a performance 81% to 94% accuracy in TGIF-QA vs. 41-45% in AVSD. \n\n\n* Concern #4: \n\u201c...using average pooling over object proposals is a bad idea as object appearance may vary very little over time in a video and the average pooling would smash the temporal information in the video.\u201d\n\nThank you for your feedback. We added a discussion on the use of object tracking in this part in the related work (Section 2). Compared to a solution using an object tracking mechanism, our method adopts a multi-step interaction framework between the space-time information in video with entity-action detected in text.\n\n* Concern #5: \nOther questions: \n\n1. \u201cHow did the authors subsample F frames/clips from a video?\u201d\n\nWe sampled F frames/clips with a striding of 8 frames from a video. \n\n2. Typos and presentation issues:\n\nThank you for your feedback! We fixed some of the errors and will thoroughly check again in the final version. \n", "title": "Response to Reviewer 2"}, "OZ3Uh8fUkO": {"type": "rebuttal", "replyto": "D_mav22ZWd-", "comment": "We thank R1 for the valuable feedback. We appreciate that the reviewer found this work \u201ca step towards interpretability of compositional neural networks.\u201d Please find below our response:\n\n* Concern #1: \n\u201cIt might be argued that this approach would not generalize.\u201d\n\nThank you for your suggestion on the potential dialogue phenomena. In this work, we constraint our models to dialogues and/or question-answering problems with videos as the common grounding information. We design our reasoning structures following simple thought processes such as \u2018find\u2019, \u2018when\u2019, and \u2018where\u2019 to extract information about entities and actions. We think that the simplicity of the method makes it easier to generalize to different scenarios of dialogues and videos. We hope to extend this method in the future and investigate specific challenges in dialogues and/or videos in other scenarios. \n\n* Concern #2: \n\u201cThe specific utility of the NMN modules compared to high-level description.\u201d \n\nIn Table 1, we included an overview of the NMN modules with their intended functionality in our NMN model pipeline. The high-level description of modules are to address information retrieval in time-space dimensions through the corresponding 'when' and 'where' module. \n\n* Concern #3: \n\u201cHuman evaluation is not provided\u201d\n\nWe noted that human evaluation is more reliable than automatic metrics in language generation tasks. However, in the AVSD benchmark, it has shown that the human score is closely correlated with automatic metric, as shown in the Dialogue Technology Challenge 7 (https://arxiv.org/abs/1901.03461). This is due to the simpler problem setting in the benchmark, closely similar to question-answering problems. \n\n* Concern #4: \n\u201cAvailability of code is not discussed which is essential for reproducibility.\u201d\n\nWe will make the code available for reproducibility, including details of training and hyper-parameters. \n\n*Concern #5: \nOther questions:\n\n1. \u201cIt would help to specify whether sentence vs corpus level BLEU was used for evaluation\u201d:\n\nWe follow the evaluation used in the previous baseline models. The BLEU score is computed by sentence level, between generated responses and 6-reference ground truth. \n\n2. \u201cCould the authors clarify why accuracy is also not reported for CountQA in Table 4?\u201d \n\nWe follow the evaluation method from previous baseline models and report the loss value, computed as the mean squared error between outputs and labels. \n\n3. \u201cHow are the audio signals incorporated in the VilNMN mentioned in Table 2?\u201d\n\nThe audio signals are incorporated using the same method with some modification to make it compatible to temporal-only features. We included further details in Appendix A. \n\n4. \u201cAn analysis of the AVSD dataset would help understand the importance of the dialog context in the dataset - focus of one of the modules in this paper\u201d\n\nThank you for the suggestion! We tried to use the ablation results to explain the importance of dialogue context. Specifically, in Appendix C, we reported the results of generated responses at different turn positions (hence, different sizes of dialogue context). \n\n5. \u201cSince AVSD is also posed as a retrieval task (Alamri et al. 2019), have the authors evaluated the system on ranking based metrics?\u201d \n\nThank you for your suggestion. We will try to experiment on this setting and report in the final version. As we noted, the retrieval task dataset is currently only available in train and validation splits (https://video-dialog.com/). \n\n6. \u201cHave the authors experimented with pre-trained weights (decoders)?\u201d\n\nOther than using models to extract visual features, we did not experiment with pre-trained weights on decoders. It is an interesting direction and we will try to include the results in the final version.  \n\n7. Writing and presentation issues: \n\nThank you for all the feedback! We have improved the manuscripts based on some of your comments and will thoroughly check again in the final version, including considering changing our method name to another term.  \n", "title": "Response to Reviewer 1"}, "vAJMbdqxcmo": {"type": "rebuttal", "replyto": "HkG_DfE0r24", "comment": "We thank R3 for the detailed feedback. We are glad that the reviewer found our approach \u201ca novel neural modular network for video grounding tasks, which can provide interpretable intermediate reasoning outcomes and show the model robustness.\u201d Please find below our response: \n\n* Concern #1: \n\u201cI am wondering how you define the modular space?\u201d\n\nWe design our modular space on the principle of time-space/spatio-temporal dimension in video. The spatial information in video can be defined as information regarding entities, which inspires us for a \u2018where\u2019 neural module. The temporal information in video contains information regarding changes from one frame to another. This is a key difference from traditional neural modular network models and we extend the prior approaches with a \u2018when\u2019 module. While the reasoning structure is simple, it is easier to adapt to different problem scenarios, especially in real world domains such as the AVSD benchmark. Our experiments show that predicted reasoning structures can provide additional bias to generate better dialogue responses. In our qualitative results, we demonstrated both failure and success examples that output responses can be explained by the predicted reasoning structures (Figure 5 and Appendix D).\n\n* Concern #2: \n\u201cHow do you train the program generation tasks from language?\u201d\n\nWe train the program generation tasks through the question parsers. We obtained supervision labels for this task using available linguistic parsers. We then train the parsers as a sequence-to-sequence task, with source sequences as the text inputs (dialogue context, question of the current turn) and the target sequence is the program. Please refer to Section 3.3. and Appendix A for more details. \n\n* Concern #3: \n\u201cHow do you determine the hyper-parameters \\alpha and \\beta?\u201d\n\nThe hyper-parameters are fine-tuned during training and we selected the best combination using the model response generation performance on the validation split. \n", "title": "Response to Reviewer 3"}, "BhbSlmT2nzQ": {"type": "rebuttal", "replyto": "MuS5HYVnPwx", "comment": "We thank the R4 for all the feedback. We are glad that R4 mentioned \u201cthe paper is motivated clearly and is delivered with good clarity.\u201d Please find below our response: \n\n* Concern #1: \n \u201cThe proposed model demonstrates impressive results on TGIF-QA but without any insightful justification\u201d: \n\nEven though the questions in TGIF-QA are short, they still contain a compositional structure (i.e. entities, actions) that previous approaches are not explicitly trained to learn. In our work, the models are trained to detect the compositionality in questions explicitly. Since TGIF-QA questions follow a very specific question type distribution (count, action, transition, and frameQA), the question structures are simpler and easier to learn than AVSD. Using exact-match accuracy of parsed programs vs. label programs as a metric, our question parser can achieve a performance 81% to 94% accuracy in TGIF-QA vs. 41-45% in AVSD. The higher accuracy in decoding a reasoning structure leads to better performance in predicting the output answers. \n\n* Concern #2: \n\u201cIncluding intense model variants in the main result table (Tab. 2) gives this paper a somewhat unfair advantage, especially when the best performing method on each metric comes from different model variants.\u201d\n\nThe model variants in Table 2 are differentiated by the data settings but the architecture is the same. In the AVSD benchmark, multiple types of data/modalities are considered (visual, audio) and we presented the performance based on different combinations of modalities. For a comparison, we also presented all baseline model performance based on their data settings as well. \n\n* Concern #3: \nTypos and presentation issues:\n\nThank you for your feedback! We fixed some of the errors and will thoroughly check again in the final version. \n", "title": "Response to Reviewer 4 "}, "gIovmw9vbz": {"type": "review", "replyto": "SUyxNGzUsH", "review": "Summary:\n\nThe paper studies the application of neural module network to video-grounded language tasks. They propose a method dubbed Visio-Linguistic Neural Module Network (VilNMN) to retrieve spatio-temporal information in a video through a linguistic-based parsed program. In particular, VilNMN first extracts entity references and their corresponding actions in linguistic cues. This information is then being used to locate relevant information in the visual cue to arrive at the correct answer. The proposed method is evaluated on two large scales benchmarks AVSD and TGIF-QA, demonstrating competitive performance with state-of-the-art methods.\n\nComments (Technical, Major Flaws of this paper): \n\n(1) Overall, it is interesting to see neural module network works in such a complex setting as in video-grounded language tasks and I appreciate the efforts of the authors trying to explain the method as detailed as possible. \n\n(2) The use of the mathematical symbols in the paper is very confusing. Normally, upper cases (capital letters) are used to denote matrices/sets of vectors while lower cases (non-capital letters) are often used to denote vectors.\n\n(3) How did the authors subsample F frames/clips from a video? In my understanding, there are a lot of frames in a video are blurry or distorted so if you sample them in a random manner, it would greatly affect the performance of object detection (Faster RCNN in this case). Please elaborate more on this.\n\n(4) Subsection 3.1, in the description of \"when\" module: A_when,i is a vector, not a matrix. Same in the supplementary document. In addition, I am wondering since there are many objects/entities in a video, how would the \"when\" module be able to localize the same object/entity over time given no additional supervision? I think object tracking would greatly be beneficial in this case.\n\n(5) In the experiments on TGIF-QA:\n- Why the authors only used ResNet features instead of using similar features as in AVSD? Spatial-based level features are fine in terms of computations but are less intuitive as I expect the object-based features counterpart and the linguistic entity references represent things at the same level of abstraction. \n- Honestly, I am skeptical about the results on the TGIF-QA datasets as the gap between VilNMN and the existing methods is very significant. From Table 4, it looks like motion features (optical flow, C3D, and the likes) play a role in tasks containing repetition of actions (such as count and action). I am not sure the reason why VilNMN without the use of any of those motion features could manage to outperform the existing methods with large margins?\n- Please provide more analysis on the results on the TGIF-QA as in my understanding even when a model correctly links entities in a question with their visual representations, it does not guarantee that it can arrive at correct answers. For example, in counting task, I would say most of the questions have the same parsed program. How does your model work in this scenario?\n\n\nSome other concerns:\n\n(6) At the end of the related works, the authors wrote \"In video represented as sequence of images,...., e.g though average pooling, resulting in potential loss of information\". This is a big assumption as using average pooling over object proposals is a bad idea as object appearance may vary very little over time in a video and the average pooling would smash the temporal information in the video. If one can properly model object tubelets via object tracking, what written in the paper wouldn't make sense.\n\n(7) Section 3.2: The sentence \"...calculated as softmax scores between an entity P_i and each token in dialogue history\" is mathematically incorrect. What drives the attention weights? Softmax is a function applying over a set of elements.\n\nI would be okay to raise the rating if the authors sufficiently address my concerns during the rebuttal phase.", "title": "I am glad to see the progress in applying NMN in video-grounded language tasks but the paper could be improved on its current form.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HkG_DfE0r24": {"type": "review", "replyto": "SUyxNGzUsH", "review": "The author present a novel neural moudalar network  for video grounding tasks, which can provide interpretable intermediate reasoing outcomes and show the model robustness. \nThis model achieves competitive results on AVSD datasets and state-of-the-art performance on TGIF-QA datasets, which demonstrates the effectiveness of the model design.\n\nDetailed comments are listed in the following\n\u2022 The novelity of the NMN is limited in this paper. The similari idea have been used in many previous literatures. I am wondering that how you define the modular space? Is there any prinpicle guidelines to design module like \"find, summarize, when, describe\"?\n\u2022 The reasoning struture in this papar is simple. The module \"find, when, where\" are more like signal detectors. There is no reasoning structure for how to get the final response. (in this paper, just fuse the detected information to get the final answer by a response decoder). So this methods cannot reveal the inner correlation between final response and detected visual/language entities.\n\u2022 [Question] How do you train the program generation tasks from language? Is there any groundtruth program structure annotation to supervise this? How do you determine the hyper-parameters \\alpha and \\beta?\n\u2022 The paper is written pooly and some expressions are confusing, like \"Different from..., our model are trained to fully genrate the parameters of components in the text\". The parameter here refer the input of each module, which is different from model parameters.", "title": "This paper present a novel NMN approach to solving video-grounding lanuguage tasks, which decompses all language into entity references and detect corresponding action-based visual feature, then instantiate NMN with those inputs to get the final response.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}