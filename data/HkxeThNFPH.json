{"paper": {"title": "Safe Policy Learning for Continuous Control", "authors": ["Yinlam Chow", "Ofir Nachum", "Aleksandra Faust", "Edgar Duenez-Guzman", "Mohammad Ghavamzadeh"], "authorids": ["yinlamchow@google.com", "ofirnachum@google.com", "sandrafaust@google.com", "duenez@google.com", "mgh@fb.com"], "summary": "A general framework for incorporating long-term safety constraints in policy-based reinforcement learning", "abstract": "We study continuous action reinforcement learning problems in which it is crucial that the agent interacts with the environment only through safe policies, i.e.,~policies that keep the agent in desirable situations, both during training and at convergence. We formulate these problems as {\\em constrained} Markov decision processes (CMDPs) and present safe policy optimization algorithms that are based on a Lyapunov approach to solve them. Our algorithms can use any standard policy gradient (PG) method, such as deep deterministic policy gradient (DDPG) or proximal policy optimization (PPO), to train a neural network policy, while guaranteeing near-constraint satisfaction for every policy update by projecting either the policy parameter or the selected action onto the set of feasible solutions induced by the state-dependent linearized Lyapunov constraints. Compared to the existing constrained PG algorithms, ours are more data efficient as they are able to utilize both on-policy and off-policy data. Moreover, our action-projection algorithm often leads to less conservative policy updates and allows for natural integration into an end-to-end PG training pipeline. We evaluate our algorithms and compare them with the state-of-the-art baselines on several simulated (MuJoCo) tasks, as well as a real-world robot obstacle-avoidance problem, demonstrating their effectiveness in terms of balancing performance and constraint satisfaction.", "keywords": ["reinforcement learning", "policy gradient", "safety"]}, "meta": {"decision": "Reject", "comment": "The paper is about learning policies in RL while ensuring safety (avoid constraint violations) during training and testing. \n\nFor this meta review, I ignore Reviewer #3 because that review is useless. The discussion between the authors and Reviewer #1 was useful.\n\nOverall, the paper introduces an interesting idea, and the wider context (safe learning) is very relevant. However, I also have some concerns.\nOne of my biggest concerns is that the method proposed here relies heavily on linearizations to deal with nonlinearities. However, the fact that this leads to approximation errors is not being acknowledged much. There are also small things, such as the (average) KL divergence between parameters, which makes no sense to me because the parameters don't have distributions (section 3.1). \n\nIn terms of experiments, I appreciate that the authors tested the proposed method on multiple environments. The results, however, show that safety cannot be guaranteed. For example, in Figure 1(c), SDDPG clearly violates the constraints. The figures are also misleading because they show the summary statistics of the trajectories (mean and standard deviation). If we were to look at individual trajectories, we would find trajectories that violate the constraints. This fact is brushed under the carpet in the evaluation, and the paper even claims that \"our algorithms quickly stabilize the constraint cost below the threshold\". This may be true on average, but not for all trajectories. A more careful analysis and a more honest discussion would have been useful. In the robotics experiment, I would like to understand why we allow for any collisions. Why can't we set $d_0=0$, thereby disallowing for collisions. The threshold in the paper looks pretty arbitrary.  Again, the paper states that  \"Figure 4a and Figure 4b show that the Lyapunov-based PG algorithms have higher success rates\". This is a pretty optimistic interpretation of the figure given the size of the error bars. \n\nThere are some points in the conclusion, I also disagree with:\n1) \"achieve safe learning\": Given that some trajectories violate the constraints, \"safe\" is maybe a bit of an overstatement\n2) \"better data efficiency\": compared to what?\n3) \"scalable to tackle real-world problems\": I disagree with this one as well because for all experiments you will need to run an excessive number of trials, which will not be feasible on a real-world system (assuming we are talking about robots).\n\nOverall, I think the paper has some potential, but it needs some more careful theoretical analysis (e.g., effect of linearization errors) and some better empirical analysis. \n\nAdditionally, given that the paper is at around 9 pages (including the figures in the appendix, which the main paper cites), we are supposed to have higher standards on acceptance than an 8-pages paper.\n\nTherefore, I recommend to reject this paper."}, "review": {"z4I2liw_9W": {"type": "rebuttal", "replyto": "HkxeThNFPH", "comment": "Dear ICLR-2020 Program Chairs,\n\nThank you for all your efforts in organizing a conference at the scale of ICLR. \n\nWe are writing to you in regards to our submission entitled: \"Safe Policy Learning for Continuous Control\", which was rejected despite receiving an average score of 6.67. We present a rebuttal to the meta-review, and kindly ask for reconsideration.  \n\nThe paper received scores of 8,6,6. The two reviewers, who gave the paper score of 6, R1 and R2, had high confidence (having published one or two papers in this area), and the one with a score of 8, R3, had low confidence. During the rebuttal phase (ended on November 15th, 2019), we responded to each reviewer\u2019s questions in detail and addressed most of their concerns. In particular, after our responses and an exchange with one of the knowledgeable reviewers (R1), she/he seemed to be pleased with our response and maintained her/his vote for acceptance. \n\nHowever, the meta-review, released on December 19th, 2019, effectively ignored all three reviews/scores, offered another independent review, and decided to reject the paper without giving us an opportunity to address her/his concerns. This poses two issues.\n\nFirst, the resulting review process for this paper was inconsistent with the ICLR review guidelines and philosophy, and that placed our paper in a disadvantaged position. The AC begins the meta-review by saying \u201cI ignore R3 because that review is useless\u201d. All the reviews were posted on Nov. 5th, over a month and a half ago. It is the AC\u2019s responsibility to read the reviews and if she/he finds a review \u201cuseless\u201d (in her/his words) to invite another reviewer or write a review her/himself. To provide the authors with enough feedback and time to respond to the issues raised in this new review, and to treat all the papers equitably, the additional reviews should be made available before the end of the rebuttal phase. None of this was done by this particular AC. This is especially important in a conference like ICLR that uses OpenReview and essentially advocates for more discussions between authors and reviewers (then other conferences with the standard review process). \n\nSecond, it is very important to note that the AC\u2019s rejection recommendation was not based on a sudden discovery of a fundamental flaw in the paper, missed by the reviewers, such as error in proof for a theoretical paper that would nullify all the conclusions and contributions, or a discovery of similar work. Such a case would warrant a sudden rejection. Instead, this is a practical, algorithmic paper, and all the issues raised by the AC are subjective in nature, based on her/his reading the paper and the evidence and experiments we provided to support our algorithms. Moreover, three reviewers, two of them experts in the area, did not have the same read of our results as the AC.\n\nIn fact, out of 303 papers with scores of 6.67 or higher, only ten were rejected. The rejections were due to close similarity with other work (1, 2), unresolved reviewers\u2019 concerns (3, 4, 5, 6, 7, 8). And, in one case (9), AC posted a review before November 15, giving authors an opportunity to respond, which they did. Our paper is the only rejected paper with a score of 6.67+ purely on the basis of a meta review without a chance to respond. Since the issues raised in the meta-review are new and not brought up by the three reviewers, we address them separately in the tab: \"Response to the meta-reviews\" , as we would if they were brought up during the discussion period.\n\nWe, the authors of the ICLR submission entitled \u201cSafe Policy Learning for Continuous Control\u201d, are all active members of the ML community, have published papers and served as reviewers and ACs at top ML venues for several years. We all share, enforce, and applaud a rigorous and fair review process, and know firsthand how difficult these decisions can be, especially when the field is growing this rapidly. From that perspective, the review of our paper seems like an unusual situation that warrants a closer look.\n\nGiven the circumstances of receiving a meta-review that a) overrules all reviewers\u2019 unanimous recommendations of acceptance (and/or weak acceptance), b) raises new concerns without giving us the opportunity to respond, and c) the new concerns are subjective in nature and not due to a discovery a foundational flaw in the paper, we kindly ask the program chairs and/or the senior area chair to take into consideration our response to the meta-review and reevaluate the decision. Thank you all for your consideration.\n\n\nBest Regards, \nAuthors of ICLR Paper: Safe Policy Learning for Continuous Control", "title": "Issues with the Reviewing Process of ICLR Paper: Safe Policy Learning for Continuous Control"}, "Uki8rPQdN3": {"type": "rebuttal", "replyto": "HkxeThNFPH", "comment": "Since the issues raised in the meta-review are new and not brought up by the three reviewers, we address them here, as we would if they were brought up during the discussion period.\n\n\u201cKL divergence between parameters\u201d: This is a shorthand notation on the KL divergence between policies, which is defined in the end of section 2.1 \n\n\u201cConstraint violation for some trajectories\u201d, \"achieve safe learning\": The notion of safety studied in this paper is based on the EXPECTED cumulative constraint cost that lies below a threshold. This is defined in Section 2, which was motivated detailedly in the second and third paragraphs of the introduction section when CMDPs are introduced. We never claimed that our methods guarantee constraint satisfaction for any trajectory. Our experiments corroborate the results of our proposed algorithm on achieving safe learning (under our defined notion of safety).  \n\n\u201cMore honest discussions\u201d:  In Section 3, we explicitly describe how the Lyapunov-based PG is derived, including how function approximation and linear approximation are involved. In Section 4, Section 5, Appendix D and E, we provide details on the experimental results on both the MuJoCo and the robot navigation tasks, including comparisons with CPO, the SOTA method (also see response to Reviewer 1 for more details), clear descriptions on how additional safeguard policy update techniques are required in the presence of function approximation (see Appendix C.4 and Appendix D.1 for details). In appendix A to C, we provide discussions and derivations on the baseline Lagrangian algorithm, the Lyapunov approach, and the propositions in the paper. Instead of giving a subjective comment, it would be helpful if the meta-reviewer elaborated on which angle would he/she think our paper is lacking \u201chonest discussions\u201d?\n\n\u201cSafety guarantees\u201d: That is a valid comment. However providing explicit safety guarantees is difficult in most model-free methods with function approximations. While the original Lyapunov-approach (from Chow et al., NIPS 2018) for MDP planning does have explicit safety guarantees, such guarantees are no longer valid for both value-based RL and policy gradient. That being said, similar issues do exist in other model-free RL baselines such as Lagrangian method and CPO, because with function approximation, keeping track of whether the constraint is satisfied requires studying the \u201csize\u201d of this function space (either via VC dimension or Radamacher complexity). In practice, even if we analyze such a bound, it would be of theoretical interest and by most practical means it\u2019s overly-conservative to implement the exact algorithm (similar to TRPO). On the other hand, in this work we empirically showed that the Lyapunov-based algorithms achieved good performance in terms of balancing learning and safety guarantees in both synthetic and real-world examples, and we believe such a contribution has important implications to RL applications. We did include similar discussions (see Appendix C.4 and D.1) in the paper and defer the mathematical derivations of safety guarantees as future work. \n\n\u201cAllow for any collisions in robot experiments\u201d: We clearly described the problem formulation of our robot experiment in the second paragraph of Section 5, in which constraint is based on collision energy (which depends on collision speed). To reduce the conservativeness of the safe RL algorithms, we allow the robot to brush off objects (such as wall) but do not allow it to ramp into obstacles that damage the system for safety reasons.\n\n\u201cRobotics experiment, Threshold in the paper looks pretty arbitrary\u201d: This threshold is based on the maximum allowable collision energy of the Fetch robot. \n\n\"Better data efficiency\": Compared with existing SOTA such as CPO, our Lyapunov-based safe RL approach is more general and can be applied in off-policy RL algorithms. Discussions can readily be found in the last paragraph of Section 4.\n\n\"Scalable to tackle real-world robotics problems\": Our algorithm is trained on the robot simulator (see Figure 5 for details) and is only deployed in the real-world environment for evaluation, which does not require a large amount of real-world trials.\n", "title": "Response to the meta-reviews"}, "B1gP-eEjoB": {"type": "rebuttal", "replyto": "BJxbkV6YjS", "comment": "Thank you for your additional comments/feedback. We have incorporated the changes you suggested in your initial review, except those regarding rewriting Sections 2 and 3.1, in the new version of the paper (and we will fix the page limit once everything is updated). We will improve the readability of these two sections and make them more self-contained in the final version of the paper. Below is our response to your two new comments. \n\n\nCPO and Linesearch\n\nWe agree with the reviewer that (backtracking) linesearch can help with enforcing the constraints and improving the balance between constraint satisfaction and performance. Yet, unless we have access to the true gradient in SGD-based policy gradient (PG) methods, such as TRPO, CPO, and our Lyapunov-based algorithms, linesearch still cannot guarantee constraint satisfaction. This is why CPO, a TRPO-based algorithm, in addition to linesearch, uses other techniques, such as safe-guard policy update and cost shaping (see Section 6.2 and 6.3 of the CPO paper https://arxiv.org/abs/1705.10528), to enforce constraint satisfaction. It is important to note that both linesearch and these additional techniques are agnostic to CPO and can be used together with our Lyapunov PG algorithms. Therefore, for the sake of clarity and fairness in comparing different algorithms, we decided to remove all the techniques used to improve the performance/constraint satisfaction balance from the algorithms, and conduct our experiments with their vanilla version. In order for all methods to be consistent in terms performance-feasibility tradeoff, for fixed learning rates, we run grid-search with the same intervals to choose the best learning rate for each method. \n\nWe would like to emphasize that the vanilla version of CPO is equivalent to our Lyapunov-based PG algorithm with \\theta-projection (as shown in the paper). We would leave further improving our Lyapunov-based algorithms by adding techniques such as line search, safe-guard policy update, cost shaping, etc., and experimenting with them in larger scale problems for future work. \n\n\nHalfCheetah constraint:\n\nWe agree with the reviewer that joint torque limit is an intuitive constraint. In fact, both joint torque limit and velocity constraints are well-motivated and have been shown to be useful in different applications (please find more details in Section 3 of https://arxiv.org/pdf/1904.12901.pdf). While the joint torque limit is an action-based constraint, which can alternatively be enforced by specific policy parameterizations (e.g., see Appendix C of the SAC paper https://arxiv.org/abs/1801.01290), the velocity constraint is a kinematic state-based constraint that depends on the state transitions of the MDP. Aside from tackling specific robot control problems, the objective of the benchmark MuJoCo (including HalfCheetah) experiments is to demonstrate/compare the effectiveness of different safe RL algorithms in solving CMDPs. Therefore, we decide to test our algorithms with the latter choice because it cannot be easily formulated without the CMDP framework.", "title": "Additional response to your concerns"}, "BJgnXsffsr": {"type": "rebuttal", "replyto": "BJxv60VsYH", "comment": "We thank the reviewer for providing useful feedbacks. Please find the itemized feedback to your questions/comments below: \n\nValue-based algorithm in intro: \nWe originally refer to the Lyapunov approach in Chow\u201918, which primarily relies on the discrete action space assumption (that is key for defining the LP formulation of the feasibility set F_L). It is true that value-based methods can also be used in continuous action RL, although it is less common than policy-based method because solving the max-Q problem can be computationally complex. To avoid these misnomers, we will add more descriptions in the introduction section regarding value-based RL for continuous control in the final paper.\n\nSection 2: \nWe mostly adopt the descriptions of Lyapunov functions from Chow\u201918. We made it clear in the introduction section that using Lyapunov functions to guarantee safety in RL is not a novel contribution of this paper. To improve the readability of this section, we will rewrite the descriptions in this section and add the missing references in the final paper. \n\npi_0:\nExistence of a feasible \\pi_0 can be checked/ensured by minimizing the cumulative constraint cost function. If the corresponding optimal policy satisfies the constraint, one can simply use that as \\pi_0. Otherwise, the problem has an empty feasibility set. This argument can be found in Chow\u201918, and we will add these descriptions about pi_0 in the final paper.\n\nSection 3.1:\nWe will modify the flow of presentation and streamline the mathematical expressions to improve the readability of this section in the final paper.\n\nFigure 6: \nFigure 6a shows the average success rate over 100 tasks (randomly sampled start and goal robot positions). A task is considered successful if the robot reaches the goal, regardless of the constraint. This is difficult because the robot navigates without the map, relying only on its noisy sensors. Figure 6b shows the average cumulative cost over those 100 tasks. Specifically, for each task we report the constraint experienced over the entire trajectory, and average that over the 100 trials. We will clarify that in the caption of the Figure.\n\nExperiments:\nHalfCheetah Safe: In this experiment our constraint is on the speed of the Cheetah. Restricting the joint torque is also an alternative constraint that we tried (but we omit their numerical results for the sake of brevity). We only apply one of the constraints in each experiment. Empirically, policies trained in these two types of constraints have similar performance, while intuitively these two cases have different meanings (bounding the total speed versus bounding the total torque).\n \nComparison with CPO without linesearch: \nHere we choose to compare the Lyapunov-based approaches with CPO without linesearch is mainly because linesearch in TRPO (and CPO) is generally a technique (that is agnostic to the choice of RL algorithms) to ensure constraint satisfaction as well as performance improvement. It is not only limited to improving the performance of TRPO/CPO. While linesearch can also be used in the Lyapunov-based policy gradient algorithm, it is not a part of the original algorithm. Therefore, for the sake of fair comparisons we remove linesearch in CPO during evaluation. Comparisons with linesearch will be left as future work.\n\nUnclear statements:\nDDPG v.s. PPO: Here the term covariate shift means that the training data is generated by a policy that is different from the current policy that is being trained, i.e., the RL training is done in an off-policy fashion. \n\nSec 5, second paragraph:\nThe robot\u2019s actions are two-dimensional vectors. First dimension is the robot's desired linear velocity (speed at which the robot should go straight). The second dimension is the robot's angular velocity - speed at which the robot should turn. Both velocity vectors are applied on the center of the mass of the robot. This is commonly known in robot kinematics literature as twist (https://en.wikipedia.org/wiki/Robot_kinematics). We will clarify that in the final paper.\n\nMinor points: \nThank you for catching that. We will correct the typos in the final paper.\n", "title": "Thank you. Please find our response below."}, "rJg94tfGsr": {"type": "rebuttal", "replyto": "HkgUCdPEcB", "comment": "We thank the reviewer for appreciating our work of deriving a novel, Lyapunov-based approach to enforce safety in reinforcement learning (RL) algorithms, and the effort of making these algorithms work in practice (in both the MuJoCo benchmark experiments and the indoor robot navigation example).  \n", "title": "Thank you."}, "rke7PtGfjB": {"type": "rebuttal", "replyto": "S1lJDVGRtB", "comment": "We thank the reviewer for appreciating our work in terms of novelty, theory, and experiments. ", "title": "Thank you"}, "BJxv60VsYH": {"type": "review", "replyto": "HkxeThNFPH", "review": "\nSummary:\n\nAuthors propose ideas to perform safe RL in continuous actions domain with modifications to Policy Gradient (PG)  algorithms via either constraining the policy parameters or constraining the actions selected by PG with a surrogate/augmented state dependent objective. The paper is well motivated and the experiments (although I have some reservations about the setup) demonstrate efficacy of the proposed method.  \n\nReview:\n--> Introduction\nI do not agree with the statement that value function based algorithms are restricted to discrete action domains, especially when you rely on  \u201cignoring function approximation errors\u201d for some of your claims. \n\nAgain in switching from value function to PG is true for traditional RL/Control theory but this is not valid here. ( your methods rely on Q(s, a) which is action-value function, or the constraint in equation 3 is integral of Q over all actions which would be value-function in traditional definition )\nNote: this is explained very well towards the end in the Appendix B, but this is a review of the paper and not Appendix B or C. \n\n\n--> Section 2\nsection 2.3 I would strongly advise the authors to rewrite this, this section reads like it was copied as is from the reference [Chow et al 2018].  especially the way Lyapunov function is defined. And the language and arguments are almost same. Some sentences cite the reference but conclusions drawn on these are not cited, are you claiming that these conclusions are original from this paper ?\n\nIt is not clear to me how the feasibility of initial pi_0 is ensured ? Did I miss this somewhere ?\n\n\n\u2192 Section 3\n\nSection 3 is pleasant to read and very easy to understand, however, same cannot be said of the\nsection 3.1. I had to spend significant time reading 3.1 and I am still not sure I have understood it very well. \n\nExperiments:\n\nI don\u2019t think halfCheetah-Safe is actually actually an useful experiment, Limiting the joint torques is perfectly understandable, just limiting speed and getting smooth motion could just be an artifact of the simulation environment. Are both constraints applied simultaneously (torque and speed) ? It is unclear from the text. \n\nI am not sure CPO without linesearch is actually a fair comparison. Line search may actually deem of the actions unsafe and therefore I would presume original CPO do be less prone to constraint violation than the proposed modification in your experiments. Again PPO is more heuristic than TRPO which makes it hard to compare like for like. PPO might give higher rewards but constraint violations may increase as well. An important point for Safe-RL I feel.\n\nFigure 6 \nCan you be more specific as what the figure 6 is showing ? Constraint ? is this constraint violation count? or cumulative sum of constraint slack over the whole trajectory ?\n \n\nNot part of assessment :\n\n\nUnclear Statements:\n\nPage 7, DDPG Vs PPO: explain clearly what you mean by  \u201ccovariate shift\u201d or remove the statement altogether. \n\nPage 7, section 5 second paragraph, \u201cThe actions are the linear \u2026. center of mass\u201d I couldn\u2019t understand this ? What do you mean by actions are velocity ?\n\n\n\nMinor points (Language, Typos):\n\npage 3, last paragraph,  Chow et al. is repeated, I can see why this happens there but suggest editing to avoid this. [This is also in intro paragraph, there it is just a typo and should be rectified]\n\nFigure 6: Captions labels are incorrect.\n\n\n\n\n\n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "S1lJDVGRtB": {"type": "review", "replyto": "HkxeThNFPH", "review": "The paper presents a technique for extending existing reinforcement learning algorithms to ensure safety of generated trajectories in terms of not violating given constraints.\n\nI have very little knowledge of this area and as a result was not able to evaluate the paper thoroughly. However, the problem addressed is certainly a very important one and based on my high-level understanding of the concepts involved the approach seems sensible. The experiments are clear and well designed, showing the trade-off between performance and safety.", "title": "Official Blind Review #3", "rating": "8: Accept", "confidence": 1}, "HkgUCdPEcB": {"type": "review", "replyto": "HkxeThNFPH", "review": "This is a very complete submission. There is a novel analysis,\nsimulations, as well as some results on real data. The authors propose\nLyapunov-based safe RL algorithms that can handle\nproblems with large or infinite action spaces, and return safe\npolicies both during training and at\nconvergence. As far as I can tell the approach is novel, makes sense,\nand requires a lot of technical innovations. I was impressed with the\nmethod and the analysis behind the method. The incorporation of the\nLyapunov idea from control theory makes a great deal of sense in this\napplication. However, it is not trivial to get from using this tool to\na working method.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 3}}}