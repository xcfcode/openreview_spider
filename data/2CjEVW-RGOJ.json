{"paper": {"title": "SkipW: Resource Adaptable RNN with Strict Upper Computational Limit", "authors": ["Tsiry Mayet", "Anne Lambert", "Pascal Leguyadec", "Francoise Le Bolzer", "Fran\u00e7ois Schnitzler"], "authorids": ["mayet.tsiry@gmail.com", "anne.lambert@interdigital.com", "pascal.leguyadec@interdigital.com", "francoise.lebolzer@interdigital.com", "~Fran\u00e7ois_Schnitzler1"], "summary": "Skip-Window is a method to allow recurrent neural networks (RNNs) to trade off accuracy for computational cost during the analysis of a sequence while keeping a strict upper computational limit.", "abstract": "We introduce Skip-Window, a method to allow recurrent neural networks (RNNs) to trade off accuracy for computational cost during the analysis of a sequence. Similarly to existing approaches, Skip-Window extends existing RNN cells by adding a mechanism to encourage the model to process fewer inputs. Unlike existing approaches, Skip-Window is able to respect a strict computational budget, making this model more suitable for limited hardware. We evaluate this approach on two datasets: a human activity recognition task and adding task. Our results show that Skip-Window is able to exceed the accuracy of existing approaches for a lower computational cost while strictly limiting said cost.", "keywords": ["Recurrent neural networks", "Flexibility", "Computational resources"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors did a good job responding to reviewer concerns.   While the reviewers still consider the method described in the paper to not be especially novel, at least one is impressed by the practicality.  imo the authors' attention detailed ablations and analysis post-review makes the paper worth including in the conference."}, "review": {"uYj3g5Bw5gW": {"type": "review", "replyto": "2CjEVW-RGOJ", "review": "This submission presents an extension of SkipRNN, Skip-Window, that splits input sequences into windows of length L from which only K samples can be used. This guarantees that the computational budget is never exceeded. Skip-Window implemented this inductive bias by predicting L updating probabilities in parallel at the beginning of each window. L needs to be set prior to training, whereas K can be modified at test time. The model is evaluated in two tasks, namely a synthetic adding task and human activity recognition. Authors report latency and energy consumption in small platforms, showing the impact of this research direction in real applications.\n\nMy main concern regarding the proposed architecture is that it limits the types of skipping patterns that can be discovered -- whereas the original SkipRNN can in principle learn any such pattern. For instance, Skip-Window cannot produce the skipping patterns shown in Figure 6 in Campos et al. (2018) unless K=L or L=1 (in both cases, Skip-Window reduces to SkipRNN). This can even be seen in the results for the adding task, where Skip-Window is actually *not* solving the task for most values of K (c.f. Figure 4). Recall that the output distribution has a variance of 0.166, and Campos et al. define solving the task as achieving an MSE two orders of magnitude below such variance. The reason for this is that Skip-Window with 5<K<L will miss the second marker in some sequences, as it needs to guess its position -- which is random within the second half of the sequence. For K<5, there\u2019s a chance that Skip-Window will miss the first marker as well. As a sanity check, I suggest that authors report the percentage of first and second markers that are missed in the adding task as a function of K. Plotting the MSE of an LSTM or GRU that skips inputs randomly, and varying the fraction of skipped inputs as in Campos et al., would also provide context for the presented error rates.\n\nDespite the imposed constraints in terms of skipping patterns, Skip-Window can be useful in tasks where the input signal can be downsampled more uniformly. After all, the adding task is a challenging problem for RNNs that skip input samples as missing one of the markers will massively increase the error rate. This potential is shown in the human activity recognition (HAR) results. However, I believe that more experimental evaluation is needed before this paper can be published at ICLR. First, the input sequences for HAR are extremely short (32 timesteps), which makes it difficult to draw strong conclusions. Second, since the Skip-Window architecture limits the types of skipping patterns that the model can discover, it is difficult to claim that Skip-Window is a generic RNN architecture unless experiments on more domains are reported (e.g. sequential MNIST, NLP).\n", "title": "Official Review", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "lr5ezuCEch4": {"type": "review", "replyto": "2CjEVW-RGOJ", "review": "The authors proposes a new framework skipW to strictly limit the computation in RNN. \n\nPros: \n1. The idea of strictly limiting the computation in RNN is new.\n2. The summary of related works is clear.\n\nCons:\n1. The motivation why the authors wants to enforces the strict constraint on the number of updates is unclear:\na. What if a consequent subsequence in a sequence is important? Then limiting processing only the K of L elements will omit this subsequence.\nb. Playing with lambda in equation (11) may also give you a tradeoff in computation and model performance and it will not omit the consequent subsequence example I give in (a).\n2. Is the training harder if we use less inputs and use error gradients than the training with the whole sequence? Although the computation time is less for a forward process, the training may be harder and take more time. This detail needs to be included.\n3. ThrRNN did one experiment in MNIST. I would like to see the comparison on the MNIST dataset.\n\nClarity:\n1. The paper is clearly written in general.\n\nOriginality:\n1. As far as I know, the paper is novel.\n\nMinor:\n1. It somehow sounds mysterious to me that the SKipW model can learn the adding task. The $\\tilde{u}_{W,t+1}$ only takes inputs at the beginning of the length L subsequence. It may not see the inputs where the marker is. Then how can the model learn to not to skip the inputs where the marker is?\n", "title": "Interesting paper, but lack of motivation", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "LK601scLtSB": {"type": "review", "replyto": "2CjEVW-RGOJ", "review": "Summary:\nThe paper proposes Skip-Window or SkipW an abstraction encapsulating RNN cells to actively skip updates similar to some earlier works like Skip-RNN, Skim-RNN, and ThrRNN. The novelty of the method comes is in having control over the total updates to control the overall computational budget compared to previous methods which didn't provide deterministic upper bounds and varied depending on the inputs. The idea is very simple and straightforward and can be looked at as a logical extension to the Skip-RNN line of work combined with a windowed approach on time series as used in ShaRNN (Dennis et al., NeurIPS 2019). The entire time series is divided into Windows of length L (which is a tunable parameter) and each window has a precomputed (from the final hidden state of the previous window) per-time step (update inside the window) importance vector which can be used as an indicator to update or not to update following the binarization as done in previous methods. The strict sparsification of this per-window importance vector to have only K non-zeros per window helps reduce compute to an upper bound ratio of K/L. The method further uses another threshold term over the sparsified importance vector to control finer budget requirements if needed. The experimentation is done on 2 tasks HAR-2D-Pose (with 32 time steps) and Adding task with 50 timesteps. The evaluation shows that Skip-Window shows good performance./accuracy compared to previous flexible RNNs with a reduction in the total number of updates. Finally, the impressive part of the paper is the real-world evaluation on Jetson Nano with a more complex workflow involving pose estimation from images for HAR-2D-Pose. \n\nPros:\n1. Simple and elegant idea. SkipWindow solution is in theory generalizable to multiple RNNs without much hassle. The abstraction could be thought of as an independent outer layer over RNNs similar to most other works in the space. \n2. Related work section is very thorough and the claims are grounded.\n3. The architecture is easy to understand along with the aspects of training\n4. The experimentation on both datasets reveals interesting insights and showcase the advantage of SkipW over other methods in both accuracy and computational budget. \n5. I highly commend the deployment experiments and evaluating it with complex workflow and showing how skipping updates and inputs can help compute latency and resources.\n6. Plots are very clean and appendix and ablation are good. \n\nWeaknesses:\n1. While I feel the idea works well and is elegant. It still is a combination of a couple of known techniques (mentioned in summary) which limits the novelty somewhat. But that doesn't stop the method from being useful.\n2. Figure 2 is not required or needs redesigning as the equations were much clearer than the figure and the gates just made it hard to understand. \n3. I understand while K/L upper bound is guaranteed, the authors want to use thr for finer control, I just feel, it might not be required for super long sequences. - Just a comment.\n4. My major issues are with experiments. While I like the work on HAR-2D-Pose and agree it is a good choice of dataset. I also want to see the success of Skip W on long term dependency tasks. I wouldn't count adding tasks as it can be trivially solved using good initialization (Henaff et al., ICML 2016). While I appreciate the efforts, I would like to see results on at least one or two (at least two preferred but I understand the time limitations, so one good dataset also works for the discussion phase) more real-world datasets to check the generalization. It can be Keyword spotting, phoneme detection, or even noisy-CIFAR. Something around 100 timesteps or more would be great\n\nI am asking the authors to add these things during rebuttal or give reasoning corresponding to it. This is a key question to be addressed during the discussion. Note that I do not need deployment for these datasets but would like to see these numbers in the paper for more datasets to make the paper stronger. The reasoning for the long-range is because the gains would be more profound there than in 32 time steps. \n\n5. While the K/L based on importance makes sense per window. Sometimes, RNNs tend to identify signatures randomly. I suggest the authors add a comparison with random K choice instead of top K choice per window and have multiple variants of it like periodic sampling or all inputs being from start or end (these are deterministic for prediction time). This experiment will help us determine if the importance vector actually contributes to the decision making on skipping updates. The rationale behind this comes from the action recognition literature where people have shown that random sub-sampling works decently well compared to intelligent sub-sampling. Another thing to add here is because the importance vector per window is chosen based on the previous window it might not be optimal and maybe random selection might work fine. If random sampling (even not so random like periodic etc.,) works well, then I am not sure about using importance vectors anymore.\n\nWithout these baselines, it is hard to argue otherwise. I strongly suggest authors pursue this to make the claims more solid. This is another thing that needs to be addressed during the discussion.\n\nDecision:\nEven though the novelty is slightly limited, I like the idea for its ease, decoupled natured and potential generalizability along with the control on computational budgets. I appreciate the authors for ablation and on-device experiments which are very thorough. The only issue I found with the paper is the experimentation and baselines. I want to see at least one long term dependency task (real-world) and baselines that evaluate if the importance vector is even needed with the simple sampling strategies among each window. \n\nI am very much willing to increase the score based on discussion and the improvements on the experimentation front.\n--------------------------------------------------------------------\nEdit after rebuttal and discussion.\n\nI thank the authors for extra experimentation to showcase the effectiveness of SkipW. While most reviewers here agree that the novelty is limited (that doesn't stop it from being useful), I strongly think the impact due to SkipW will be translated to the real-world. There has been some discussion on the datasets, which I agree are not extensive making the initial experimentation weak. However, the new experiments compensate to an extent and I would like to recommend a weak acceptance with a score of 6 (I am still between 6 and 7, waiting for other reviewers to pitch in). ", "title": "A simple and effective idea which can have real-world impact - can benefit from stronger experimentation", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "CgeXAF-cMsI": {"type": "rebuttal", "replyto": "GBRo-u_ZNew", "comment": "1. Done\n2. We have added some points to the IMDB figure. For MNIST, we have not been able to produce better SkipRNN models. Unfortunately, at some point both SkipW and SkipRNN stop learning and basically skip all inputs. We have added some failure examples in Table 3.\n3. The number of inputs processed is only controlled indirectly through $\\lambda$ for both methods. This makes it difficult to train methods processing a similar number of inputs. Please do note that the table contains fewer data points than the figure. We now realize this is misleading and will correct that. In particular, there is a SkipW model achieving 96.01% accuracy and 15.07% input processed, which is really close to the SkipRNN model at 96.1% and 15.6%\n4. Note that we provide a non-sequential random sampling baseline for MNIST (71% accuracy at 3.2% inputs).\n5. Thank you! SkipW retains a significant computational advantage over the variants not using the importance vectors, because SkipW can process fewer than K inputs in every window. We have included two additional figures (34 and 35) using the actual number of inputs processed rather than K/L. While not as good to compare accuracy, we new realize they are needed to correctly reflect the tradeoffs. We expect this advantage to be even bigger on MNIST and adding task, as there are windows without any information in these tasks.", "title": "couple of answers"}, "LoNMjbs16id": {"type": "rebuttal", "replyto": "JuhI11i5Z6u", "comment": "Thank you for your clarifications. \n\n**Q1.3: more experiments**\n\nWe have included new experiments on sequential MNIST and IMDB, a a NLP tasks. We unfortunately lacked time to use one of the data sets you suggested. While these two tasks were not part of your suggestions, we chose them for two reasons. First, two reviewers requested each of these two experiments. Then, theses tasks were analyzed in the SkipRNN paper. This gives us a point of comparison, a sanity check to verify our results and make sure we are optimizing hyper parameters reasonably well for both SkipRNN and SkipW. We have achieved better tradeoffs for SkipRNN in our experiments than in the original paper. We are therefore confident that the comparison is fair. In addition, in both tasks sequences are much longer than 100: 781 for MNIST and 200 for imdb. SkipW achieves tradeoffs similar to SkipRNN on MNIST, and slightly worse on IMDB. MNIST also provides another interesting comparison between the skip patterns learned by both methods. We also included a comparison to recent feature selection results (for MNIST), showing that SkipW achieves result close to some of them. \n\n**Q1.4: random or deterministic sampling to assess the interest of importance vectors**\nOn that front, we are happy to report that we have included random and periodic baselines in the main experimental section, and several additional baselines in Appendix H. We have performed three sets of \nexperiments on HAR to study the sampling mechanism:\n- a comparison to random sampling, periodic sampling and sampling the first K elements of a window. SkipW is better than these approaches.\n- a comparison to alternative sampling mechanisms: rather than a weight vector, we tried random sampling and using the first or last elements of a window. SkipW is also better most of the time, showing the interest of importance vectors.\n- replacing TopK by another select_K mechanism. Here we only tried using the first elements of the window (above the threshold). The performance is similar to TopK.\n\nWe believe these experiments make the paper (even) more interesting. We hope you will share our sentiment!", "title": "additional baselines and experiments included"}, "aE1omPFbijg": {"type": "rebuttal", "replyto": "5wx3Xe3wuG7", "comment": "We think we are in agreement about the merit of each method, although we would like to correct that SkipW also dynamically react to the inputs, but slower than skipRNN. On adding task (Figure 15), after the second marker has been detected on adding task, SkipW stops sampling inputs. But this only starts at the beginning of the next window whereas skipRNN does it immediately, a drawback of SkipW for sure, though SkipW is still better.\n\nWe think this discussion has been very interesting and raised several good points. We have decided to incorporate the essence of it in the paper. We hope you are fine with this. \nSpecifically, the main experimental section of the paper now mentions adding task failures and we have added some perspectives to the discussion on skip patterns. \nIn the appendix (C.3), on adding task, as it is particularly well suited to compare skip patterns, we have tried to summarize the present discussion. Please let us know what you think and whether something is missing.\n\nWe have added a random sampling baseline on Adding Task and HAR. In both tasks, both SkipW and SkipRNN beat these baselines. \nIn addition, we have included a discussion of failures on adding task and additional experiment on sequential MNIST and imdb, a NLP task.", "title": "discussion incorporated in paper and requested experimental results in paper"}, "Xe-I97s0X_N": {"type": "rebuttal", "replyto": "2CjEVW-RGOJ", "comment": "Dear reviewers, thank you again for the discussion. We have added additional experiments. Please let us know whether these address your concerns.\n\nChangelog:\n- Better highlighting that SkipW is interesting both for the tradeoffs achieved and the bounded computational cost\n- Random selection baseline for Adding task (requested by R2)\n- Random and deterministic baselines for HAR (requested by R2 and R1)\n- Convergence speed comparison (requested by R3)\n- Additional experiments (requested by all):\n   - sequential MNIST (length 784, requested by R3 and R2) \n   - IMDB (length 200, NLP requested by R2 and R4) \n- Additional baselines:\n   - random sampling baselines on HAR and adding task (R3, R2 and R1)\n   - variations of the sampling mechanism (R1)", "title": "Thank you for your reviews"}, "GElbeNLo1yF": {"type": "rebuttal", "replyto": "5DfAcHKCQHu", "comment": "Thank you for your answer. We agree that for a given K, SkipW limits the number of updates per window and that skipRNN does not have such a limitation. We will state this more explicitly. However, such a constraint is mandatory to satisfy computational constraints on portions of the sequence, including on the whole sequence. So we argue that this limitation is one reason why SkipW is interesting. In addition, we would like to raise two points:\n- In our opinion, the example in the answer and in particular the sentence \"SkipW will not be able to discover such scheme\" do not represent how SkipW works. To make sure there is no misunderstanding, we would like to clarify. As you mentioned in the summary, K is selected at test time. \n\t- During training, there is no restriction on the value of K and no restriction on the patterns SkipW can explore and therefore it can discover any skipping scheme. In your example, we expect that SkipW will correctly identify that it must sample the first 10% of the sequence. This example actually corresponds to the first marker of adding task. Figure 13 in the appendix illustrates that SkipW has correctly learned that it must sample the first 10% of the sequence and then stop (until the second marker can appear). SkipRNN has not learned it must stop (SkipRNN is labeled as ThrRNN - ThrRNN with a constant $thr=0.5$ is SkipRNN. Also something we will make clearer). \n\t- During inference, K is selected to fit a computational constraint. That is a system / user choice. In practice, this restricts the type of skip patterns used while the constraint is enforced. But again, this has no influence on training. In your example, SkipW would indeed stop sampling some inputs in the first 10% of the sequence if a strict constraint is applied at the beginning. If no strict constraint is applied (K=L) or if the constraint does not forbid it, SkipW would sample the first 10% of the sequence and then stop. This can be observed for adding task in figures 6 and 15. \n- We would like to stress that, despite this limitation, we achieve better or comparable tradeoffs than/to skipRNN. \n\nA comparison to random sampling is indeed a good idea. We also agree that training is fast for adding task, but our time and computational power is limited. At the moment we focus on new datasets, as this has been requested by all reviewers and we expect this to be the most time consuming task. We apologize that this delays our answers somewhat. As an alternative to empirical random sampling on adding task, we propose to use the theoretical best average error such a strategy can achieve. As you pointed out, the variance of the task is 0.166 as the variance of each value is 1/12. A model performing perfect summation of the values that are not skipped will therefore achieve an average error of P(skip) * 1/12 * number of markers. We have updated Figure 4 of our paper with this metric. Both skipRNN and SkipW is well below this limit. We have also added the proportion of markers skipped (Figure 12).", "title": "Skipping patterns & random sampling and markers missed on adding task"}, "AQObtQ-qHOY": {"type": "rebuttal", "replyto": "LK601scLtSB", "comment": "Thank you for your review. We were very excited about the results of our approach on real platforms. We are happy you share our enthusiasm!\n\n**Q1.1: While I feel the idea works well and is elegant. It still is a combination of a couple of known techniques (mentioned in summary) which limits the novelty somewhat. But that doesn't stop the method from being useful.**\n\nWe think that our approach is not the same as ShaRNN (Dennis et al., NeurIPS 2019). Both use the notions of windows of inputs. However, ShaRNN focuses on parallelizing computation by processing each window independently before aggregating them using a higher level RNN. Our approach computes an importance vector for the inputs in each window but otherwise processes the non-skipped inputs sequentially. \n\n**Q1.2: changing thr might not be required**\n\nAs you noted this is not the focus of our work. However we felt that readers may be curious what the impact of changing thr would be. Would you suggest removing it completely?\n\n**Q1.3: more experiments**\n\nWe agree, are working on them and hope to have additional results available soon.\n\n**Q1.4: random or deterministic sampling to assess the interest of importance vectors**\n\nAnother good point. While additional experiments would be nice indeed, we can already bring some answer to this question and plan to incorporate them in the paper. Campos et al. (2018) have compared their skipRNN approach to random sampling RNNs on multiple data sets. skipRNN significantly outperforms random sampling RNNs. Considering that skipW performs either better than or as well as skipRNN, we believe skipW will outperform random sampling. \nFurthermore, the importance vector in skipW is also used to discard unnecessary inputs and to allow the model to operate below the computational upper bound. This can lead to significant computational savings without impacting accuracy. So this can save a lot of energy as well. Of course a random sampling ratio could be chosen to maintain accuracy. However, as the number and locations of inputs processed in skipRNN and skipW are dynamically chosen based on the sequence processed, we think that random sampling will never reach the same sampling efficiency. This is also supported by the random sampling experiments mentioned in the previous paragraph.\nWe would be very happy to hear your thoughts about these two points.", "title": "We agree. Note that the importance vector is also used to discard useless inputs and save energy."}, "Le_3Nb-5tAc": {"type": "rebuttal", "replyto": "AvbdZBrqud", "comment": "Thank you for your review.\n\n**Q4.1: Figure 2 and equations 5-10 seem to be a bit unclear. What does \\hat{u}_{W,t}^{K} denote.**\n\nWe think there no \\hat notation in our paper, so we will assume you meant \\tilde. If not, please do correct us.\n\\tilde{u}{W} is indeed computed once at the beginning of an L sized window. But in order to change the trade off during the processing of this same window, this vector is available at every time step and denoted by \\tilde{u}{W,t}, where we refer to the time step rather than the window index as in figure 1. We thought this was clearer, but we will clarify.\nTherefore at each time step t, \\tilde{u}_{W,t}^{K} is the current L-vector values which will depend on the current threshold and the current K parameter.\n\nIf the threshold and K parameter are considered fixed for a window duration, the vector \\tilde{u}{W} can be topked and fbinarized and therefore \\tilde{u}_{W,t}^{K} is necessary. \n\n**Q4.2: In Figures 3 and 4 what is input processed % ?**\n\n*input processed %* in figure 3 and 4 refers to the percentage of the inputs that the RNN models do not skip and process. This is a measure of the computational cost of the model.\n\n**Q4.3: For example, in Figure 3 and in the case of L=8, the setup K=1 implies K/L = 0.125 , but the K=1 point for SkipW is at less than 10% mark on the xaxis.**\n\nYou are right. K/L is an upper bond on the number of inputs processed. But unimportant inputs are also discarded by the f_binarize function. So the actual number of inputs processed can be lower than K/L.\n\n**Q4.4: Furthermore, how is it that K=[3-8] all have roughly the same input processed % ? Its the same case for K=[1-4] and K=[3-16]**\nThe number of inputs processed is low to start with because of the f_binarize function. As there are already few inputs processed with K=L, lowering K has not much impact in this case as the upper bound of K/L is already larger than the average input processed with K=L.\n\n**Q4.5: In Figure 4, the number of points for ThrRNN seem to be much more than SkipW.**\n\n Indeed, ThrRNN adapts using the threshold (which can take any value), while the K parameter of skipW can only take integer values in {1,..,L}. However, as the upper bound is K/L, only K enables to match a constraint. thrRNN therefore does not allow to strictly limit computation. Using the threshold can also be done in skipW. This allows finer control but brings no additional benefit with respect to the constraint. You can see the impact of changing thr in skipW in figure 9 in the appendix, where modifying thr provides a similar  level of control as in thrRNN, but from more trade-off points corresponding to the different K values.\n\n**Q4.6: Figure 6 seems to imply that the inputs in p3 and p4 can be more easily dropped as compared to p1 and p2.**\n\nWe are not sure we understand the question, so please follow up if we miss the mark. In this figure, inputs in p_1 and p_3 are completely analyzed for high K and increasingly skipped when K decreases. About half the inputs in p_4 are analyzed for high K, and increasingly skipped when K decreases. Most inputs in p_2 are skipped, not matter the value of K. We believe this behavior is due to the position of the markers (in p_1 and p_3) and does not reflect a tendency of SkipW. \n\n\n**Q4.7: Figure 7 seems to be a bit unclear.**\n\nThe accuracy and average number of inputs processed that are reported are average values on a set of 5751 test sequences. Figure 7 analyzes the impact of changing thr and k inside a sequence, so on the fly as inputs go on. For example, the points (accuracy and computational cost) in the figure where x axis = 50% correspond to changing from (thr=0,4;K=8) to (thr=0,65;K=1) in the middle of every sequence out of 5751, so after seeing or skipping half of the inputs. In other words, the first half of each sequence is processed using (thr=0,4;K=8) and the second half using (thr=0,65;K=1). A prediction is only computed at the end of the sequence. This is true for every sequence. Values plotted at x axis = 30% corresponds to changing the thr and K after one third of every sequence and so on.\n\n**Q4.8: It seems that the system will under utilize the available computational capacity. This might not be desirable as any loss in accuracy might have been reduced if all available computational capacity was used.**\n\nIndeed, the system may under utilize the available computational capacity, but this should have little or no impact on accuracy. While we do want skipW to be flexible and be able to operate under a strict computational constraint, we also want to train a model that will use as little inputs as possible, while maintaining accuracy. This saves energy and we think there is no point in processing inputs if it is not useful. This is related to your question 4.4: for high values of K (figure 3 and 5), the same number of inputs are skipped so as not to waste resources. ", "title": "Here are some clarifications"}, "fioXuB4YsR7": {"type": "rebuttal", "replyto": "uYj3g5Bw5gW", "comment": "Thank you for your review.\n\n**Q2.1: My main concern regarding the proposed architecture is that it limits the types of skipping patterns that can be discovered -- whereas the original SkipRNN can in principle learn any such pattern**\nWe believe this statement and several elements in the associated paragraph are not correct. We argue and would like to discuss that the following statement is more accurate.\n*Both the proposed architecture and skipRNN can in principle learn any skipping pattern. Unlike skipRNN, the proposed architecture can, during inference, forbid some skipping patterns to avoid exceeding a computational limit. In practice, both the proposed architecture and skipRNN have some limits on the skipping patterns that can be learned.*\n\nOur architecture can be used with any K. Our architecture can be used with K=L and efficiently skip inputs. There is therefore no restriction on the types of skipping patterns learned and used by our architecture, as in skipRNN. In theory, SkipW can solve any task. This can for example be seen in Figure 4, where skipW solves adding task for K=10=L.\nOn a more technical side, K=L or L=1 are not the only configurations allowing skipW to reproduce the skip patterns of figure 6 in Campos et al. (2018). The longest sequence without skip in that figure seems to be of length 14. In theory, SkipW can achieve such skipping patterns for any K>=7 and L >= K, by examining the last K inputs of one window and the first K inputs of the following window. That being said we acknowledge that for any K<L, a high enough frequency will be problematic.\n\nBy lowering K, skipW can be used to match a computational constraint by forbidding some skip patterns. If the optimal skip pattern is forbidden, skipW can in theory fall back to another good skip pattern. In other words, skipW prevents some patterns only when computational limitations would not allow such patterns to be analyzed. Our experiments suggest it works in practice. Sometimes there is no impact on accuracy (figure 5, K changing from 5 to 2), sometimes there is (figure 5, K changing from 2 to 1). Whether the decrease in accuracy is acceptable or not depends on the task. However, without skipW, there is no possibility to match the constraint. This is the real improvement over skipRNN. For any non-trivial task, we agree that, with L large enough and K small enough, skipW will fail. \n\nBut this is no the only improvement: skipW can also achieve better accuracy / computational trade-off than skipRNN. When K=L, skipW does not reduce to skipRNN. The skip mechanisms are different, and lead to different trade-offs. In practice, skipRNN seems to be more limited than skipW in the skip patterns it can learn. For example, skipRNN is unable to skip long sequences of inputs. This is visible in our experiments but also in Campos et al. (2018). For example, in figure 6 of Campos et al. (2018), skipRNN samples multiple inputs at the beginning as you pointed out but also samples individual inputs several times throughout the sequence even though these inputs bring no information because of aliasing. This is also visible in other figures. Because skipRNN is dragged down by its inability to completely stop sampling where there is no information, when trying to further reduce the number of inputs processed by increasing lambda, skipRNN stops analyzing interesting inputs. We suspect that skipW has some limitations as well, but in our experiments, skipW achieves comparable or better results in terms of accuracy and computational cost trade offs than skipRNN. So we argue that the potential limitations of skipW are in practice less penalizing than or at least comparable to the limitation of skipRNN.\n\nWe think that the points above could be emphasized better in the paper. We plan to incorporate them. Thank you for provoking this discussion.\n\nWe also agree that additional experiments would be a great addition to the paper. We are working on them at the moment and hope to fulfill some of your suggestions before the end of the discussion period.\n\nAdding a baseline randomly skipping inputs in adding task is however easy, as Campos et al. (2018) provide these results. The task fails almost immediately, even when skipping only 2% of inputs. For some perspective on other data sets, please see our answer to Q1.4 of reviewer 1.", "title": "Main concern is not correct: skipW can discover any skip pattern and restrict patterns to match constraint"}, "V4NEexNGFfA": {"type": "rebuttal", "replyto": "lr5ezuCEch4", "comment": "Thank you for your review. We would like to address some of your concerns. \n\n**Q3.1: The motivation why the authors wants to enforces the strict constraint on the number of updates is unclear: a. What if a consequent subsequence in a sequence is important? Then limiting processing only the K of L elements will omit this subsequence. b. Playing with lambda in equation (11) may also give you a tradeoff in computation and model performance and it will not omit the consequent subsequence example I give in (a).**\na) You are correct: the model might omit some important inputs if more than K of these are present in a subsequence of L inputs. Whether the decrease in accuracy is acceptable or not depends on the task. However, without skipW, there is no possibility to match the constraint and we argue that lowering accuracy is often better than canceling the task. Please also see our answer to reviewer 2. Regarding (b): Playing with lambda allows to obtain an offline trade off that will be fixed. If we understood your comment correctly, this is similar to skipRNN (although the skip mechanisms are different). While skipRNN works well, we would like to stress that, in our experiments,  skipRNN is actually worse than or comparable to skipW. We believe skipRNN has trouble skipping a large number of inputs and cannot properly exploit the structure of the problem, as shown in the detailed experiments on adding task. So we argue that limiting the ability to analyze all elements is at least as good as skipRNN inability to skip a large number of inputs. Furthermore, unlike skipRNN, skipW allows the model to adapt to changing constraints on devices with limited resources, which is the main motivation of our work. \n\n**Q3.2: Is the training harder if we use less inputs and use error gradients than the training with the whole sequence?**\n During training, to process inputs in batch, we use multiplication by u_t and (1 - u_t) rather than actually skipping inputs. This allows the use of matrix operations on  mini-batches of inputs, which makes training faster than processing sequences one by one and skipping inputs. We will add these details.\n\n**Q3.3: It sounds mysterious to me that the SKipW model can learn the adding task**\nWhile training, the K parameter can be set to L, therefore the model is able - in regard of the lambda parameter - to use as much inputs as it likes in order to minimize the loss. While training, it learns to minimize the loss, to skip inputs in the irrelevant part of the sequence and spot the markers. More precisely, skipW can be used with any K. When K<L, some skip patterns are forbidden and the model falls back to other skip pattern.", "title": "Strict constraints allow the model to run on limited and shared computational resources"}, "AvbdZBrqud": {"type": "review", "replyto": "2CjEVW-RGOJ", "review": "\nThis work introduces Skip-Window (SkipW), an approach that allows RNNS to have improved computational efficiency at the cost of accuracy. SkipW adds a procedure to existing RNN cells that allows them to process fewer inputs while remaining in a strict computational budget. This work demonstrates the benefits of SkipW through experiments on multiple data sets.\n\nThis work proposes a structured procedure to process fewer inputs during inference while abiding by a computational budget. Each skip seems to be calculated over a window of inputs in a sequence, thereby minimizing inter and intra sequence variability in computation. This work does seem to have merit in a practical setting where the availability of computational resources can vary. The ability to manage dynamic budget, though not totally novel (ThrRNN also has the thr param), does provide an advantage with SkipW being perhaps more flexible with both K and thr usage.\n\nThis work empirically demonstrates the benefit of SkipW  over several baselines on two data sets. There are, however, some points that need to be addressed.\n\n\n\n\n\nFigure 2 and equations 5-10 seem to be a bit unclear. Figure 1 and the first two paragraphs of section 3 seem to imply that  \\hat{u}_{W} is calculated once as an L length vector at the beginning of an L sized window.   Here \\hat{u}_{W}^{K}  are the selected K entries for the whole L sized window. Therefore what does \\hat{u}_{W,t}^{K}  denote. \n\nIn Figures 3 and 4 what is input processed % ? Is it K/L. For example, in Figure 3 and in the case of L=8, the setup K=1 implies K/L = 0.125 , but the K=1 point for SkipW is at less than 10% mark on the xaxis. Furthermore, how is it that K=[3-8] all have roughly the same input processed % ? Its the same case for K=[1-4] and K=[3-16]  \n\n\nIn Figure 4, the number of points for ThrRNN seem to be much more than SkipW. This seems to imply much more finer control over computational cost for ThrRNN as compared to SkipW (more coarser control). What are the authors' thoughts about this?\n\n\nFigure 6 seems to imply that the inputs in p3 and p4 can be more easily dropped as compared to p1 and p2. What are the  authors' thoughts about the connection of this ease of dropping inputs to the various attention mechanisms that are available in literature. Furthermore, is it the case that later inputs will typically always have more tendency to be dropped in SkipW ?\n \n\nFigure 7 seems to be a bit unclear. Is the accuracy being depicted the relative accuracy as compared to thr=0.4 and K=8? Furthermore, how is the accuracy being calculated as the thr,K are changing on the fly as time/sequences go on? This might imply that different thr,K values see different data? Is the same exact sample being used to calculate the accuracy? \n\n\nIt seems that L-K denotes the minimum number of inputs that will be skipped (as there is a further 'binarize' that runs on the K inputs). Therefore, it seems that the system will under utilize the available computational capacity. This might not be desirable as any loss in accuracy might have been reduced if all available computational capacity was used. What are the authors' thoughts about this?\n\n\nPerhaps a text processing task in NLP would have made the results stronger as in practical scenarios, this is one of the common modalities where RNNs are used.", "title": "Interesting work", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}}