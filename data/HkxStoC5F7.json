{"paper": {"title": "Meta-Learning Probabilistic Inference for Prediction", "authors": ["Jonathan Gordon", "John Bronskill", "Matthias Bauer", "Sebastian Nowozin", "Richard Turner"], "authorids": ["jg801@cam.ac.uk", "jfb54@cam.ac.uk", "bauer@tue.mpg.de", "nowozin@google.com", "ret26@cam.ac.uk"], "summary": "Novel framework for meta-learning that unifies and extends a broad class of existing few-shot learning methods. Achieves strong performance on few-shot learning benchmarks without requiring iterative test-time inference.   ", "abstract": "This paper introduces a new framework for data efficient and versatile learning. Specifically:\n1) We develop ML-PIP, a general framework for Meta-Learning approximate Probabilistic Inference for Prediction. ML-PIP extends existing probabilistic interpretations of meta-learning to cover a broad class of methods. \n2) We introduce \\Versa{}, an instance of the framework employing a flexible and versatile amortization network that takes few-shot learning datasets as inputs, with arbitrary numbers of shots, and outputs a distribution over task-specific parameters in a single forward pass. \\Versa{} substitutes optimization at test time with forward passes through inference networks, amortizing the cost of inference and relieving the need for second derivatives during training.\n3) We evaluate \\Versa{} on benchmark datasets where the method sets new state-of-the-art results, and can handle arbitrary number of shots, and for classification, arbitrary numbers of classes at train and test time. The power of the approach is then demonstrated through a challenging few-shot ShapeNet view reconstruction task.", "keywords": ["probabilistic models", "approximate inference", "few-shot learning", "meta-learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes a decision-theoretic framework for meta-learning. The ideas and analysis are interesting and well-motivated, and the experiments are thorough. The primary concerns of the reviewers have been addressed in new revisions of the paper. The reviewers all agree that the paper should be accepted. Hence, I recommend acceptance."}, "review": {"S1l5yrVqPS": {"type": "rebuttal", "replyto": "HyxBBF9MDB", "comment": "Thank you for your comment!\n\nSummary: \n\nTo provide a fair comparison we use standard choices for architecture and other parameters across different meta-learning methods.\n\nDetailed Response:\n\nFor many works in the meta-learning community (especially for image classification as the main testbed) it is often difficult to disentangle gains due to improved feature extractors or improved meta-learning methodology. Depending on the data modality, stronger or weaker feature extractors may exist. To assess the quality of the meta-learner and not of ever-deeper architectures, many works utilise the same or very similar architecture (4 layer convnet) as a feature extractor, see our discussion on page 8. Note that this protocol for evaluating meta-learning methods is in line with e.g., MAML, Prototypical networks, Matching nets, and others. \n\nBauer et al 2017 (https://arxiv.org/abs/1706.00326, https://openreview.net/forum?id=r1DPFCyA-) and, more recently, Chen et al 2019 (https://openreview.net/forum?id=HkxLXnAcFQ) discuss the role of deeper feature extractors in few-shot learning scenarios. Both works find that results for the same meta-learning methodology vary hugely depending on the feature extractor used; the variability due to feature extractor is often larger than that due to meta-learner. \n\nAs our work proposes a new meta-learner, we -- in line with many other meta-learning works -- chose to compare on the simple standard architecture on which we indeed achieve SotA.  Comparing performance of one algorithm using a ResNet to another using the simpler convnet network is an apples-to-oranges comparison. As you note, the results we report for SNAIL are taken from their ablation study, where they investigate SNAIL performance with this simpler architecture. In this setting, Versa significantly outperforms SNAIL, and this is an important data-point.", "title": "To provide a fair comparison we use standard choices for architecture and other parameters across different meta-learning methods."}, "H1ghXV0kgN": {"type": "rebuttal", "replyto": "ByldnORAJV", "comment": "Thanks for the follow up questions.\n\nThe experiments with 1 task per batch yield the following result on 5-way 5-shot learning with Versa: (66.75 + / - 0.9)%. This is within the error bars of the current numbers in the paper, and above Prototypical networks trained and tested on 5-way (65.77 + / - 0.7)%. Further, this result was achieved quickly in response to this discussion without any tuning of the optimization hyper-parameters (learning rate etc.) to this new setting. Based on past experience with Versa, we are confident that this performance can be further improved. \n\nWe will similarly investigate the performance in the 1-shot setting, and report final numbers for the next iteration of the paper. We would be happy to include the numbers achieved with both Versa training procedures above, as well as the numbers achieved by both training procedures achieved by Proto-nets in Table 3, along with a thorough discussion about the differences induced between the two procedures in the case of Versa and Prototypical networks.\n\nWe do disagree with your statement that the two settings (5-way classification with 4 tasks per batch and 20-way classification) are the same. A 20-way classification task is not equivalent to four 5-way classification tasks, namely in the independence assumptions induced over the predictive distributions by the different normalizing constants. This is consistent with the fact that, in contradiction to what your claims would imply, Versa's performance on the meta-test set is not impacted by reducing the number of tasks per batch, while prototypical networks significantly benefits from higher-way training. \n\nFinally, several members of our team are away (or are about to depart) for the holidays. So, whilst we're very happy to discuss these matters further, we will only be able to do so after the Christmas break. We would be very happy for you to reach out to us directly at this time so that we can continue discussion in a more direct way with a quicker response time. Thank you again for your input, and happy holidays.", "title": "Numbers using prototypical networks matched training protocol"}, "SygBAooCkE": {"type": "rebuttal", "replyto": "B1gDhDl0y4", "comment": "Thanks for the follow up question!\n \nExisting comparisons:\n- As is shown in Section 4, prototypical networks is a special case of Versa, allowing for a direct comparison of the two when the training procedure is equivalent.\n- Our training procedure (and that of other methods presented in Table 3) is equivalent to the \"matched training condition\" in prototypical nets (i.e., train and test \"way\" are the same).\n- Experimental protocols (and in particular, makeup of the mini-batches) were found not to have a substantial effect on the performance of Versa. The experimental protocols described in the paper (number of tasks per batch, size of test sets, etc') closely follow that of MAML. See below for more details.\n- For example, we ran Versa with a meta-training batch of 1 (directly equivalent to the prototypical network setup), and found the final accuracy is within error-bars of the result of the submission, and still above the errors bars of prototypical networks trained and tested on 5-way.\n- We therefore consider the comparisons presented to be direct and fair.\n\nComparisons using higher way training\n- We have not investigated the effects of training with a higher way than testing.\n- This changes the objective function due to the normalisation constant in the softmax e.g. this would have a single normaliser for all 20 classes if considered together, versus separate ones for each of the tasks if the classes were split into 4 tasks of 5 classes each.\n- This is the key difference between these two training conditions and is not something Versa currently exploits.\n- We agree that it would be interesting to test whether using this modified objective improved Versa and indeed whether the same idea could lead to improvements in other methods too.\n\nMore details on the training protocol\n- For both Omniglot and miniImageNet, experiments demonstrated that the performance of Versa on the meta-test set is not sensitive to the number of tasks per batch during training.\n- As such, the experimental protocol (number of tasks per batch and the number of query points per batch) for both miniImageNet and Omniglot were chosen to match the MAML protocol.\n- We also ran the experiments with meta-training batch of size 1, which is directly equivalent to the prototypical network setup. \n- The performance in these experiments was very similar to our best results (within error bars), and significantly better than what is reported by prototypical nets for the same setup.\n- In summary the final performance was not found to be particularly sensitive to these choices.\n- Arguably, this is to be expected as this is equivalent to selecting the size of the mini-batch in conventional learning. In particular, our model makes an independence assumption across tasks (given \\theta).\n- As prototypical networks are a special case of Versa (with the amortization network set to identity around the mean encoding), we expect similar findings to hold for this model.", "title": "Yes, the comparison is fair for the following reasons."}, "HyxhLmE6JN": {"type": "rebuttal", "replyto": "r1eb9eOnyN", "comment": "Thank you for your question!\n\nThe prototypical networks paper proposes a number of different experimental protocols. One of these protocols trains on higher \"way\" than what is ultimately used for testing. This is detailed in table 5 in Appendix B of the prototypical networks paper. There you will see that the number you are quoting is achieved by training the system to perform 20 way classification, and then testing it on 5 way classification (final row of the table). This experimental protocol differs from that used by all other methods. When matching experimental protocols are used, i.e., training and testing on 5-way classification, prototypical networks achieve the numbers we quote in our ICLR submission (row 10 of their table). \n\nThe discrepancy between the experimental protocols was pointed out to us by readers of the arxiv version of our paper who suggested reporting numbers from the same experimental protocol. Hence the differing numbers between this submission and the arxiv version. You are correct in pointing out that in the unconstrained setting prototypical networks achieve better performance, but we are of the opinion that a fairer comparison is made when all methods use the same experimental protocol. The note in our submission saying \"The tables include results for only those approaches with comparable training procedures ...\" was intended to clarify this, but we will clarify this more explicitly in the final version if it is accepted. \n\nFinally, note that even the improved prototypical networks number (68.20 \u00b1 0.66%) is within error bars of the Versa number (67.37 \u00b1 0.86%).", "title": "Prototypical Nets propose several experimental protocols"}, "rJe6s67O6m": {"type": "rebuttal", "replyto": "HkxStoC5F7", "comment": "Dear Reviewers,\n\nMany thanks for your detailed comments and suggestions. We really appreciate the time and effort you have put into reading our paper. Your comments are both insightful and constructive, and we believe have contributed to improving the quality of our paper.\n\nWe have uploaded a revised version of the paper, incorporating your comments and suggestions. Below, we address each of your reviews individually.\n", "title": "Thank you for your reviews and comments"}, "r1xx7y4dTQ": {"type": "rebuttal", "replyto": "r1llzOxFhX", "comment": "\u201cThe important aspect of the algorithm is the context independence assumption between posteriors of different classes for learning weights. \u2026 The idea sounds great, but I am skeptical of the justification behind the independence assumption which, as per its justifications sounds contrived and only empirical.\u201d\n\nWe thank the reviewer for imploring us to think more carefully about this point. We share the concern that providing only an empirical justification for the context independent assumption is slightly troubling. We have therefore considered this more carefully, and have found that there is a principled justification of this design choice, which is best understood through the lens of density ratio estimation [i, ii]. \n\nResults from Density Ratio Estimation [i, ii] show that an optimal softmax classifier learns the ratio of the densities\n\n    Softmax(y=k | x) = p(x | y=k) / Sum_j p(x | y=j)\n\nassuming equal a priori probability for each class. Our system follows his optimal form by setting:\n\n            log p(\\tilde{x} | y=c) proportional h_theta ( \\tilde{x})^T w_c\n\nwhere w_c ~ q_phi (w | {x_n ; y_n=c} ) for each class in a given task. Here {(x_n, y_n)} are the few-shot training examples, and $\\tilde{x}$ is the test example. This argument states that under ideal conditions (i.e., we can perfectly estimate p(y=c | x) ), the context-independent assumption is correct, and further motivates our design.\n\nWe have amended the paper to include this argument (see Appendix B). We thank the reviewer for pointing to this important issue, and we hope that this alleviates some of their concerns.\n\n[i] - S. Mohamed. The Density Ratio Trick. The Spectator (Blog). 2018\n[ii] - M. Sugiyama, T. Suzuki, and T. Kanamori. Density ratio estimation in machine learning. 2012\n", "title": "On justification for the context-indepence assumption"}, "Hye5i0QOpQ": {"type": "rebuttal", "replyto": "Byxh3JK9nQ", "comment": "\u201cIt would have been good if some of the experiments could be moved into the main paper. \u2026 the structure and organization of the paper could be improved by moving some of the methodological details and experimental results in the appendix to the main paper.\u201d\n\nWe agree that a significant portion of interesting content has been relegated to the appendix in our submission. Much of this, of course, has to do with space constraints. However, we have addressed this in the revised version in line with your suggestions by (i) moving the appendix containing the toy-data experimentation to the main body of the paper (see Section 5.1), and (ii) moving some methodological details from the appendix in to the experiments section (see Section 5).\n\n\u201cIt would have been good if there was some validation of the time-performance of the model as one motivation of meta-learning is rapid adaptation to a test-time task. \u201c\n\nWe strongly agree that the issue of performance timing is of great interest, and it is useful and important to validate this experimentally. We were originally hesitant to add any timing results as code released with research papers is often optimized for correctness as opposed to speed. That said, we measured the test time performance of both MAML (as implemented in the authors'  publicly available repository at https://github.com/cbfinn/maml) and Versa in 5-shot 5-way experiments on mini-ImageNet, using the same architectures for both. We found Versa to achieve 5x speed up compared to MAML, while achieving significantly better accuracy (see Table 3). We have amended the paper to include this experimental data (see Section 5.2 for details). We believe this data demonstrates the performance gains achieved by relieving the need for test time optimization procedures.\n", "title": "More experiments to main text + timing experiments"}, "rkxV4A7_am": {"type": "rebuttal", "replyto": "Syewq7hpoX", "comment": "\u201cWhich of the competitors (if any) use the same restricted model setup (inference only on the top-layer weights)?\u201d\n\nTo the best of our knowledge, almost all the competing methods adapt the entire network for new tasks. We have amended the paper to clarify this point (see Section 5.2).\n\n\u201cDo competitors without train/test split also get k_c + 15 points, or only k_c points?\u201d\n\nTo the best of our knowledge, all methods we compare to use train/test splits, with the exception of the VI methods referenced in Table 1. The VI methods used the same number of observations at train time (i.e., the data available to all methods was identical).\n\n\u201cThe main paper does not really state what the model or the likelihood is [in the ShapeNet experiments]. From F.4 in the Appendix, this model does not have the form of your classification models, but psi is input at the bottom of the network. Also, the final layer has sigmoid activation. What likelihood do you use?\u201d\n\nThe terseness of the ShapeNet model details was a result of space constraints. We have amended the paper to include additional explanatory details (see Section 3). You are correct in observing that psi plays a different role from the classification case, namely as an input to the image-generator. The likelihood we used is Gaussian, the sigmoid activation ensures that the mean is between 0 and 1, reflecting the constraints on pixel-intensities. Your observation that using top-layer weights would allow us to perform exact inference is very insightful. We decided to use an architecture that passed the latent parameters underlying each shape instance through multiple non-linearities, but it would be very interesting to compare to the simpler baseline that you suggest. As this is a significant undertaking, we will leave it to future work,\n\n\u201cReal Bayesian inference would just see features h_theta(x) as inputs, not the x's. Why not simply feed features in then? \u2026 Be clear how it depends on theta (I think nothing is lost by feeding in the h_theta(x)).\u201d\n\nThank you for suggesting this cleaner way of presenting our work. We agree with your observations on the input to the inference network. We have amended Fig. 2 accordingly, and have improved the descriptions in Section 3.\n\n\u201cThe marginal likelihood has an Occam's razor argument to prevent overfitting. Why would your criterion prevent overfitting?\u201d\n\nThe mechanism preventing overfitting in our criterion is the meta train / test splits, which explicitly encourages the model to generalize from the training observations to the test data. Methods based on held-out sets, like cross validation, are known to favor models which are more complex than those favoured by Bayesian model comparison [i, ii]. However, as is empirically demonstrated in the experimental section, our proposed criterion consistently outperformed variational objectives.\n\n\u201cIt is quite worrying that the prior p(psi | theta) drops out of the method entirely. Can you comment more on that?\u201d\n\nThis is a subtle point that we view as both a feature and a bug. It is a feature in the sense that a prior is learned implicitly through the sampling procedure (as is shown for example in the simple Gaussian experiment -- see Section 5.1). This can be compared to VI, for example, where the prior enters through a KL regularization term which often favours underfitting. It is a bug if, for example, the user has a priori knowledge about the parameters that they would like to leverage. In this case, it could be possible to use synthetic training data to incorporate such knowledge into the scheme.  However, for the predictive purposes explored in this work, we did not find that the lack of prior posed an issue.\n\n\n[i] - C. E. Rasumessen and Z. Ghahramani. Occam\u2019s razor. 2001.\n[ii] - I. Murray and Z. Ghahramani. A note on the evidence and Bayesian Occam\u2019s razor. 2005.\n", "title": "Responding to your review"}, "Byxh3JK9nQ": {"type": "review", "replyto": "HkxStoC5F7", "review": "This paper proposes both a general meta-learning framework with approximate probabilistic inference, and implements an instance of it for few-shot learning. First, they propose Meta-Learning Probabilistic inference for Prediction (ML-PIP) which trains the meta-learner to minimize the KL-divergence between the approximate predictive distribution generated from it and predictive distribution for each class. Then, they use this framework to implement Versatile Amortized Inference, which they call VERSA. VERSA replaces the optimization for test time with efficient posterior inference, by generating distribution over task-specific parameters in a single forward pass. The authors validate VERSA against amortized and non-amortized variational inference which it outperforms. VERSA is also highly versatile as it can be trained with varying number of classes and shots.\n\nPros\n- The proposed general meta-learning framework that aims to learn the meta-learner that approximates the predictive distribution over multiple tasks is quite novel and makes sense.\n- VERSA obtains impressive performance on both benchmark datasets for few-shot learning and is versatile in terms of number of classes and shots.\n- The appendix section has in-depth analysis and additional experimental results which are quite helpful in understanding the paper.\n\nCons\n- The main paper feels quite empty, especially the experimental validation parts with limited number of baselines. It would have been good if some of the experiments could be moved into the main paper. Some experimental results such as Figure 4 on versatility does not add much insight to the main story and could be moved to appendix.\n- It would have been good if there was some validation of the time-performance of the model as one motivation of meta-learning is rapid adaptation to a test-time task. \n\nIn sum, since the proposed meta-learning probabilistic inference framework is novel and effective I vote for accepting the paper. However the structure and organization of the paper could be improved by moving some of the methodological details and experimental results in the appendix to the main paper. \n", "title": "A novel meta-learning framework", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1llzOxFhX": {"type": "review", "replyto": "HkxStoC5F7", "review": "This paper presents two different sections:\n1. A generalized framework to describe a range of meta-learning algorithms.\n2. A meta-learning algorithm that allows few shot inference over new tasks without the need for retraining. The important aspect of the algorithm is the context independence assumption between posteriors of different classes for learning weights. This reduces the number of parameters to amortize during meta-training. More importantly, it makes it independent of the number of classes in a task, and effectively doing meta-training across class inference instead of each task. The idea sounds great, but I am skeptical of the justification behind the independence assumption which, as per its justifications sounds contrived and only empirical. \n\nOverall, I feel the paper makes some progress in important aspects of meta-learning.", "title": "Review for Meta-Learning Probabilistic Inference for Prediction", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "Syewq7hpoX": {"type": "review", "replyto": "HkxStoC5F7", "review": "Summary:\nThis work tackles few-shot (or meta) learning from a probabilistic inference viewpoint. Compared to previous work, it uses a simpler setup, performing task-specific inference only for single-layer head models, and employs an objective based on predictive distributions on train/test splits for each task (rather than an approximation to log marginal likelihood). Inference is done amortized by a network, whose input is the task training split. The same network is used for parameters of each class (only feeding training points of that class), which allows an arbitrary number of classes per task. At test time, inference just requires forward passes through this network, attractive compared to non-amortized approaches which need optimization or gradients here.\n\nIt provides a clean, decision-theoretic derivation, and clarifies relationships to previous work. The experimental results are encouraging: the method achieves a new best on 5-way, 5-shot miniImageNet, despite the simple setup. In general, explanations in the main text could be more complete (see questions). I'd recommend shortening Section 4, which is pretty obvious.\n\n- Quality: Several interesting differences to prior work. Well-done experiments\n- Clarity: Clean derivation, easy to understand. Some details could be spelled out better\n- Originality: Several important novelties (predictive criterion, simple model setup, amortized inference network). Closely related to \"neural processes\" work, but this happened roughly at the same time\n- Significance: The few-shot learning results are competitive, in particular given they use a simpler model setup than most previous work. I am not an expert on these kind of experiments, but I found the comparisons fair and rather extensive\n\nInteresting about this work:\n- Clean Bayesian decision-theoretic viewpoint. Key question is of course whether\n   an inference network of this simple structure (no correlations, sum combination\n   of datapoints, same network for each class) can deliver a good approximation to\n   the true posterior.\n- Different to previous work, task-specific inference is done only on the weights of\n   single-layer head models (logistic regression models, with shared features).\n   Highly encouraging that this is sufficient for state-of-the-art few-shot classification\n   performance. The authors could be more clear about this point.\n- Simple and efficient amortized inference model, which along with the neural\n   network features, is learned on all data jointly\n- Optimization criterion is based on predictive distributions on train/test splits, not\n   on the log marginal likelihood. Has some odd consequences (question below),\n   but clearly works better for few-shot classification\n\nExperiments:\n- 5.1: Convincing results, in particular given the simplicity of the model setup and\n   the inference network. But some important points are not explained:\n   - Which of the competitors (if any) use the same restricted model setup (inference\n      only on the top-layer weights)? Clearly, MAML does not, right? Please state this\n      explicitly.\n   - For Versa, you use k_c training and 15 test points per task update during\n      training. Do competitors without train/test split also get k_c + 15 points, or\n      only k_c points? The former would be fair, the latter not so much.\n- 5.2: This seems a challenging problem, and both your numbers and reconstructions\n   look better than the competitor. I cannot say more, based on the very brief\n   explanations provided here.\n   The main paper does not really state what the model or the likelihood is. From\n   F.4 in the Appendix, this model does not have the form of your classification\n   models, but psi is input at the bottom of the network. Also, the final layer has\n   sigmoid activation. What likelihood do you use?\n   One observation: If you used the same \"inference on final layer weights\" setup\n   here, and Gaussian likelihood, you could compute the posterior over psi in closed\n   form, no amortization needed. Would this setup apply to your problem?\n\nFurther questions:\n- Confused about the input to the inference network. Real Bayesian inference would\n   just see features h_theta(x) as inputs, not the x's. Why not simply feed features in\n   then?\n   Please do improve the description of the inference network, this is a major\n   novelty of this paper, and even the appendix is only understandable by reading\n   other work as well. Be clear how it depends on theta (I think nothing is lost by\n   feeding in the h_theta(x)).\n- The learning criterion based on predictive distributions on train/test splits seem\n   to work better than ELBO-like criteria, for few-shot classification.\n   But there are some worrying aspects. The marginal likelihood has an Occam's\n   razor argument to prevent overfitting. Why would your criterion prevent overfitting?\n   And it is quite worrying that the prior p(psi | theta) drops out of the method\n   entirely. Can you comment more on that?\n\nSmall:\n- p(psi_t | tilde{x}_t, D_t, theta) should be p(psi_t | D_t, theta). Please avoid a more\n   general notation early on, if you do not do it later on. This is confusing\n", "title": "Few-shot learning, based on amortized inference network for parameters of logistic regression head models. Uses learning criterion based on predictive distributions on train/test splits. Extensive comparison, achieves state-of-the-art despite simpler setup than many competitors", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}