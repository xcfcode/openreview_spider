{"paper": {"title": "Few-Shot Learning with Graph Neural Networks", "authors": ["Victor Garcia Satorras", "Joan Bruna Estrach"], "authorids": ["vgsatorras@gmail.com", "bruna@cims.nyu.edu"], "summary": "", "abstract": "We propose to study the problem of few-shot learning with the prism of inference on a partially observed graphical model, constructed from a collection of input images whose label can be either observed or not. By assimilating generic message-passing inference algorithms with their neural-network counterparts, we define a graph neural network architecture that generalizes several of the recently proposed few-shot learning models. Besides providing improved numerical performance, our framework is easily extended to variants of few-shot learning, such as semi-supervised or active learning, demonstrating the ability of graph-based models to operate well on \u2018relational\u2019 tasks.", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "All reviewers agree that the proposed method is novel and experiments do a good job in establishing its value for few-shot learning. Most the concerns raised by the reviewers on experimental protocols have been addressed in the author response and revised version."}, "review": {"BJIp_k0xM": {"type": "review", "replyto": "BJj6qGbRW", "review": "This paper proposes to use graph neural networks for the purpose of few-shot learning, as well as semi-supervised learning and active learning. The paper first relies on convolutional neural networks to extract image features. Then, these image features are organized in a fully connected graph. Then, this graph is processed with an graph neural network framework that relies on modelling the differences between features maps, \\propto \\phi(abs(x_i-x_j)).  For few-shot classification then the cross-entropy classification loss is used on the node.\n\nThe paper has some interesting contributions and ideas, mainly from the point of view of applications, since the basic components (convnets, graph neural networks) are roughly similar to what is already proposed. However, the novelty is hurt by the lack of clarity with respect to the model design.\n\nFirst, as explained in 5.1 a fully connected graph is used (although in Fig. 2 the graph nodes do not have connections to all other nodes). If all nodes are connected to all nodes, what is the different of this model from a fully connected, multi-stream networks composed of S^2 branches? To rephrase, what is the benefit of having a graph structure when all nodes are connected with all nodes. Besides, what is the effect when having more and more support images? Is the generalization hurt?\n\nSecond, it is not clear whether the label used as input in eq. (4) is a model choice or a model requirement. The reason is that the label already appears in the loss of the nodes  in 5.1. Isn't using the label also as input redundant?\n\nThird, the paper is rather vague or imprecise at points.  In eq. (1) many of the notations remain rather unclear until later in the text (and even then they are not entirely clear). For instance, what is s, r, t. \n\nThe experimental section is also ok, although not perfect.  The proposed method appears to have a modest improvement for few-shot learning. However, in the case of  active learning and semi-supervised learning the method is not compared to any baselines (other than the random one), which makes conclusions hard to reach.\n\nIn general, I tend to be in favor of accepting the paper if the authors have persuasive answers and provide the clarifications required.", "title": "Official Reviewer 2", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1_szu5xM": {"type": "review", "replyto": "BJj6qGbRW", "review": "This paper introduces a graph neural net approach to few-shot learning. Input examples form the nodes of the graph and edge weights are computed as a nonlinear function of the absolute difference between node features. In addition to standard supervised few-shot classification, both semi-supervised and active learning task variants are introduced. The proposed approach captures several popular few-shot learning approaches as special cases. Experiments are conducted on both Omniglot and miniImagenet datasets.\n\nStrengths\n- Use of graph neural nets for few-shot learning is novel.\n- Introduces novel semi-supervised and active learning variants of few-shot classification.\n\nWeaknesses\n- Improvement in accuracy is small relative to previous work.\n- Writing seems to be rushed.\n\nThe originality of applying graph neural networks to the problem of few-shot learning and proposing semi-supervised and active learning variants of the task are the primary strengths of this paper. Graph neural nets seem to be a more natural way of representing sets of items, as opposed to previous approaches that rely on a random ordering of the labeled set, such as the FCE variant of Matching Networks or TCML. Others will likely leverage graph neural net ideas to further tackle few-shot learning problems in the future, and this paper represents a first step in that direction.\n\nRegarding the graph, I am wondering if the authors can comment on what scenarios is the graph structure expected to help? In the case of 1-shot, the graph can only propagate information about other classes, which seems to not be very useful.\n\nThough novel, the motivation behind the semi-supervised and active learning setup could use some elaboration. By including unlabeled examples in an episode, it is already known that they belong to one of the K classes. How realistic is this set-up and in what application is it expected that this will show up?\n\nFor active learning, the proposed method seems to be specific to the case of obtaining a single label. How can the proposed method be scaled to handle multiple requested labels?\n\nOverall the paper is well-structured and related work covers the relevant papers, but the details of the paper seem hastily written.\n\nIn the problem set-up section, it is not immediately clear what the distinction between s, r, and t is. Stating more explicitly that s is for the labeled data, etc. would make this section easier to follow. In addition, I would suggest stating the reason why t=1 is a necessary assumption for the proposed model in the few-shot and semi-supervised cases.\n\nRegarding the Omniglot dataset, Vinyals et al. (2016) augmented the classes so that 4,800 classes were used for training and 1,692 for test. Was the same procedure done for the experiments in the paper? If yes, please update 6.1.1 to make this distinction more clear. If not, please update the experiments to be consistent with the baselines.\n\nIn the experiments, does the \\varphi MLP explicitly enforce symmetry and identity or is it learned?\n\nRegarding the Omniglot baselines, it appears that Koch et al. (2015), Edwards & Storkey (2016), and Finn et al. (2017) use non-standard class splits relative to the other methods. This should probably be noted.\n\nThe results for Prototypical Networks appear to be incorrect in the Omniglot and Mini-Imagenet tables. According to Snell et al. (2017) they should be 49.4% and 68.2% for miniImagenet. Moreover, Snell et al. (2017) only used 64 classes for training instead of 80 as utilized in the proposed approach. Given this, I am wondering if the authors can comment on the performance difference in the 5-shot case, even though Prototypical Networks is a special case of GNNs?\n\nFor semi-supervised and active-learning results, please include error bars for the miniImagenet results. Also, it would be interesting to see 20-way results for Omniglot as the gap between the proposed method and the baseline would potentially be wider.\n\nOther Comments:\n\n- In Section 4.2, Gc(.) is defined in Equation 2 but not mentioned in the text.\n- In Section 4.3, adding an equation to clarify the relationship with Matching Networks would be helpful.\n- I believe there is a typo in section 4.3 in that softmax(\\varphi) should be softmax(-\\varphi), so that more similar pairs will be more heavily weighted.\n- The equation in 5.1 appears to be missing a minus sign.\n\nOverall, the paper is novel and interesting, though the clarity and experimental results could be better explained.\n\nEDIT: I have read the author's response. The writing is improved and my concerns have largely been addressed. I am therefore revising my rating of the paper to a 7.", "title": "Novel idea for few-shot learning", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "By7ixJ9eG": {"type": "review", "replyto": "BJj6qGbRW", "review": "This paper studies the problem of one-shot and few-shot learning using the Graph Neural Network (GNN) architecture that has been proposed and simplified by several authors. The data points form the nodes of the graph with the edge weights being learned, using ideas similar to message passing algorithms similar to Kearnes et al and Gilmer et al. This method generalizes several existing approaches for few-shot learning including Siamese networks, Prototypical networks and Matching networks. The authors also conduct experiments on the Omniglot and mini-Imagenet data sets, improving on the state of the art.\n\nThere are a few typos and the presentation of the paper could be improved and polished more. I would also encourage the authors to compare their work to other unrelated approaches such as Attentive Recurrent Comparators of Shyam et al, and the Learning to Remember Rare Events approach of Kaiser et al, both of which achieve comparable performance on Omniglot. I would also be interested in seeing whether the approach of the authors can be used to improve real world translation tasks such as GNMT. ", "title": "Good paper with interesting approach", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SkzW_xPbG": {"type": "rebuttal", "replyto": "r1_szu5xM", "comment": "\n\n> In the experiments, does the \\varphi MLP explicitly enforce symmetry and identity or is it learned?\n\nThe \\varphi(a,b) = MLP(abs(a-b)) explicity enforces symmetry due to the absolute value.\nThe identity is easily learned since the input to the MLP will always be the same vector (a vector of zeros) when a==b. \nI have rewritten this line to clarify which property is enforced and which one is easily learned.\n\n\n> Regarding the Omniglot baselines, it appears that Koch et al. (2015), Edwards & Storkey (2016), and Finn et al. (2017) use non-standard class splits relative to the other methods. This should probably be noted.\n\n- From Koch et al. (2015) we are using the results from Vinyals et al. (2016) reimplementation that is using the common class splits. I added a note explaining it in the caption of the table.\n\n- I checked again the paper from Finn et al. (2017). Based on what I read, they are using the same configuration splits, 1200 training, 423 testing and augmented by multiples of 90 degrees. I add the paper url at the end of this answer. Same for Edwards & Storkey (2016). Correct me if I am wrong.\n\n\n> The results for Prototypical Networks appear to be incorrect in the Omniglot and Mini-Imagenet tables. According to Snell et al. (2017) they should be 49.4% and 68.2% for miniImagenet. Given this, I am wondering if the authors can comment on the performance difference in the 5-shot case, even though Prototypical Networks is a special case of GNNs?\n\nIn order to use the same evaluation procedure across papers, all results are evaluated using the same K-way q-shot conditions for both training an test, in other words, a network that for example evaluates a 20-way 1-shot experiment has been trained on 20-way 1-shot tasks. This was the evaluation procedure presented by (Vinyals et al.) and followed by later works. In Prototypical Networks these results are reported in the Appendix. (Mishra et al.) is also reporting these results from (Snell et al) in their comparison. We chose to use the same evaluation procedure across papers.\n\n\n> Moreover, Snell et al. (2017) only used 64 classes for training instead of 80 as utilized in the proposed approach.\n\nWe modified it, in the results of the last update, the network is trained with the 64 training classes. The 16 validation classes are only used for early stopping and parameter tuning.\n\n\n> For semi-supervised and active-learning results, please include error bars for the miniImagenet results. Also, it would be interesting to see 20-way results for Omniglot as the gap between the proposed method and the baseline would potentially be wider.\n\nWe added the error bars.\n\n\n> Other Comments:\n> In Section 4.2, Gc(.) is defined in Equation 2 but not mentioned in the text.\n\nSolved\n\n\n> In Section 4.3, adding an equation to clarify the relationship with Matching Networks would be helpful.\n\nDone\n\n\n> I believe there is a typo in section 4.3 in that softmax(\\varphi) should be softmax(-\\varphi), so that more similar pairs will be more heavily weighted.\n\nSolved\n\n\n> The equation in 5.1 appears to be missing a minus sign.\n\nSolved\n\n\nWe improved Mini-Imagenet results by regularizing better (using dropout and early stopping.)\n\n\n- Finn et al. (2017) https://arxiv.org/pdf/1703.03400.pdf\n- Edwards & Storkey (2016) https://arxiv.org/pdf/1606.02185.pdf\n", "title": "Answer part 2"}, "r1XiADHmz": {"type": "rebuttal", "replyto": "ByG5VLHQG", "comment": "\nHi Parnav, thank you for the clarification.\n\nI thought 32x32 was the resolution of the attentive patch instead of the image size, but that was wrong, sorry for that. I edited my first comment in order to avoid confusing future readers. Even though, we didn't add the paper due to other differences in the data augmentation and the embedding network now commented in the first answer.", "title": "Answer to Clarification"}, "HkmKSsqzz": {"type": "rebuttal", "replyto": "By7ixJ9eG", "comment": "\nThank you for the review,\n\n\n> I would also encourage the authors to compare their work to other unrelated approaches such as Attentive Recurrent Comparators of Shyam et al, and the Learning to Remember Rare Events.\n\nWe added the results from \"Learning to Remember Rare Events\" to our table. Regarding \"Attentive Recurrent Comparators\" we didn't add it due to some differences in the data augmentation method (translation, mirroring and shearing) and the use of Wide Resnets as feature extractors.\n\n\n> I would also be interested in seeing whether the approach of the authors can be used to improve real world translation tasks such as GNMT. \n\nI guess the Graph Neural Network could replace the attentive module for Neural Machine Translation systems that search for parts over the source sentence. Maybe it could exploit more complex relationships among the words of the input sentence. I don't know if it has been already tried.\n\n\nNext, I list some of the main modifications that we have done to the paper:\n\n- We updated Omniglot results using the same data augmentation protocol than other papers and results are now more competitive with the state of the art.\n\n- We considerably improved Mini-Imagenet results by regularizing better (using dropout and early stopping.)\n\n- We improved the writing, figures and we corrected some typos.\n\n", "title": "Answer"}, "HkH8YlDZz": {"type": "rebuttal", "replyto": "BJIp_k0xM", "comment": "\nFirst of all, thank you for the review and comments.\n\n\n> As explained in 5.1 a fully connected graph is used (although in Fig. 2 the graph nodes do not have connections to all other nodes). If all nodes are connected to all nodes, what is the different of this model from a fully connected, multi-stream networks composed of S^2 branches?\n\nThe graph is defined as fully connected, but the connection weights are different among nodes. In other words, the adjacency matrix $A$ is not a binary matrix formed by 0s and 1s, instead of that, every value of the adjacency matrix $A[i,j]$ is a Real number that ranges from 0 to 1. These values are computed by the network at every layer before applying each Graph Convolution.\n\nOur Graph Neural Network (GNN) performs a particular operation on each node of the Graph and another operation is applied on the neighbors of that node that are averaged using the weights of the Adjacency matrix. This behaviour is easier to represent unfolding the equation 2 from the paper: \n\nx^{k+1} = Gc(x^{k}) = \u03c1(I\u00b7x^{k}\u00b7\\theta_1^{k} + A\u00b7x^{k}\u00b7\\theta_2^{k}).\n\nWhere $k$ is the layer index; $x^{k}$ are the nodes structured into a 2-dimensional (Number_nodes by Number_features) matrix; $I$ is the identity matrix; $A$ is the adjacent matrix; and \\theta_1^{k} and \\theta_2^{k} are the vectors of learnable parameters of dimensionality (Number_features,) for these two operations.\n\nThe matrices $I$ and $A$ are called operators, in our paper are represented with the following term  $\\mathcal{A} = {A^{k}, I}$\n\nOnce it has been clarified, we can notice that the mechanism of a GNN is different from a multi-stream network composed by S^2 branches. In a GNN the same operation is convolved over the nodes of a layer and at every node the information of the neighbors is aggregated based on the Adjacency matrix, the parameters to learn at every layer will only be theta_1 and theta_2. Based on my knowledge about multi-stream networks, the number of parameters to handle S^2 branches would be radically larger, the mechanism to aggregate the information from the other nodes would also be different. I hope this explanation clarified this point.\n\nRegarding Fig. 2 I updated it connecting all the nodes. In the previous version I had removed some of the connections to clearly see how they change at every layer, but it was probably misleading.\n\n\n> To rephrase, what is the benefit of having a graph structure when all nodes are connected with all nodes. Besides, what is the effect when having more and more support images? Is the generalization hurt?\n\nWe can distinguish to main benefits of the GNN:\n1) A different metric from the euclidean is learned at every layer. \n2) GNN can handle contextual information by aggregating information from the other nodes to the current one based on the weights of the adjacency matrix learned in (1).\n\nAbout the generalization, the number of parameters of the GNN is independent from the number of support images, therefore, increasing the number of support images will not overparametrize the network avoiding the risk to overfit.\n\n\n> Second, it is not clear whether the label used as input in eq. (4) is a model choice or a model requirement. The reason is that the label already appears in the loss of the nodes in 5.1. Isn't using the label also as input redundant?\n\nIn a few-shot task we have: 1) A support subset of labeled images and 2) An unlabeled image to classify. We input (1) the label of the subset of labeled images and we predict (2) the label of the image to classify which appears in the loss. Therefore it is a model requirement for the few-shot scenario. We modified this explanation to be clearer. In the semi-supervised scenario we just input the label for some of the samples, therefore in this case, it is only a requirement for the labeled samples.\n\n\n> Third, the paper is rather vague or imprecise at points.  In eq. (1) many of the notations remain rather unclear until later in the text (and even then they are not entirely clear). For instance, what is s, r, t. \n\nWe have rewritten this section in order to be clearer. \n\n\n> The experimental section is also ok, although not perfect.  The proposed method appears to have a modest improvement for few-shot learning. However, in the case of  active learning and semi-supervised learning the method is not compared to any baselines (other than the random one), which makes conclusions hard to reach.\n\nWe updated the Omniglot results using the same data augmentation protocol than other papers and results are more competitive with the state of the art now.\nWe also improved Mini-Imagenet results by better regularizing the network using dropout and early stopping.", "title": "Answer "}, "Hy1Xgqbfz": {"type": "rebuttal", "replyto": "HyGQQPbMM", "comment": "Thanks again for your comment. \n\nWe apologize, there is a typo in the equation (and in the previous answer). The subscript \"l\" is indexing over output feature maps, not over input feature maps. We will fix this in the document. \n\nBest,\n\nAuthors", "title": "typo in the answer"}, "HyGQQPbMM": {"type": "rebuttal", "replyto": "r1CZZbezf", "comment": "\nThanks for reading the paper,\n\n\n> What do d_k and d_k+1 stands for? \n\nAs you said, they are the point dimension for the k-stage and k+1-stage.\n\n\n> If they are the point dimension for the k-stage and k+1-stage how can you iterate over the [l] index (which goes from 1 to d_k+1) for both?\n\nYou can see that x^{(k)}$ and $\\theta^{(k)}$ are also indexed by $k$, therefore, this $k$ is also changing the value of [l] which also depends on $k$. In practice we just mean that at every layer the input dimensionality is d_k, and the output is d_k+1, same as in CNNs.\n\n\n> Why you use a concatenation? is it suppose to represent the 1 in the generator family? Shouldn't it be added instead of concatenated?\n\nThe concatenation is independent from the Graph operation, we are just concatenating the input layer to the new outputs, this is applied to other types of Neural Networks. It was introduced by \"Densely Connected Convolutional Networks\", https://arxiv.org/abs/1608.06993\n\n\n", "title": "Answer"}, "Syc4dgwWz": {"type": "rebuttal", "replyto": "r1_szu5xM", "comment": "\nFirst of all thank you for the review and comments.\n\n\n> Regarding the graph, I am wondering if the authors can comment on what scenarios is the graph structure expected to help? In the case of 1-shot, the graph can only propagate information about other classes, which seems to not be very useful.\n\nI would like to point out two main strengths of the GNN method that we are proposing:\n1) A different metric from the euclidean is learned at every layer. \n2) GNN can handle contextual information by aggregating information from the neighbor nodes based on the weights of the adjacency matrix learned in (1).\n\nBased on our experiments, propagating information from other classes seems more useful when the number of classes is larger than one, specifically we notice the largest improvement in Mini-Imagenet 5-way 5-shot. But the metric that is learned at every layer also provides strong results for the case of one-shot, specially we notice it in the Omniglot dataset in 5-way 1-shot and 20-way 1-shot. The Graph structure also allows us to run in the semi-supervised and active learning scenarios.\n\n\n> Though novel, the motivation behind the semi-supervised and active learning setup could use some elaboration. By including unlabeled examples in an episode, it is already known that they belong to one of the K classes. How realistic is this set-up and in what application is it expected that this will show up?\n\nOne example application that comes to my mind for the semi-supervised scenario would be building a face recognition system from Facebook profiles. A user may have uploaded dozens of images with other people and maybe just some of the pictures are labeled. It could be possible to use all the faces from the pictures that the user uploaded even if they are not labeled together with the few labeled ones to build a few-shot classifier for that user. \n\nI hope the open community will be able to find new and better applications.\n\n\n> For active learning, the proposed method seems to be specific to the case of obtaining a single label. How can the proposed method be scaled to handle multiple requested labels?\n\nIn the proposed method, we are uncovering one of the labels choosing from a softmax distribution in a particular layer. It would be possible to uncover multiple labels by choosing more than one, it woud also be possible to uncover multiple labels at multiple layers. Our main aim here was to test out how feasible is to learn to do Active learning from the classification loss using an end to end structure like a GNN instead of using handcrafted methods like Uncertainty Sampling. Scaling it to larger datasets can be hard to optimize, we leave it for the future and we present it as a prove of concept.\n\n\n> In the problem set-up section, it is not immediately clear what the distinction between s, r, and t is.\n\nWe have rewritten this section more accurately.\n\n\n> Regarding the Omniglot dataset, Vinyals et al. (2016) augmented the classes so that 4,800 classes were used for training and 1,692 for test. Was the same procedure done for the experiments in the paper? If yes, please update 6.1.1 to make this distinction more clear. If not, please update the experiments to be consistent with the baselines.\n\nThanks a lot for this point. Our data augmentation implementation was only for the training classes. We updated the paper results implementing the data augmentation also on the test classes as it is done in other works and the results are now better and more competitive with the state of the art.\n\nThe answer continues in the following message \"Answer part 2\"", "title": "Answer part 1"}}}