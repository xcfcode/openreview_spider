{"paper": {"title": "Hallucinations in Neural Machine Translation", "authors": ["Katherine Lee", "Orhan Firat", "Ashish Agarwal", "Clara Fannjiang", "David Sussillo"], "authorids": ["katherinelee@google.com", "orhanf@google.com", "agarwal@google.com", "clarafy@berkeley.edu", "sussillo@google.com"], "summary": "We introduce and analyze the phenomenon of \"hallucinations\" in NMT, or spurious translations unrelated to source text, and propose methods to reduce its frequency.", "abstract": "Neural machine translation (NMT) systems have reached state of the art performance in translating text and are in wide deployment.  Yet little is understood about how these systems function or break.  Here we show that NMT systems are susceptible to producing highly pathological translations that are completely untethered from the source material, which we term hallucinations.  Such pathological translations are problematic because they are are deeply disturbing of user trust and easy to find with a simple search.  We describe a method to generate hallucinations and show that many common variations of the NMT architecture are susceptible to them. We study a variety of approaches to reduce the frequency of hallucinations, including data augmentation, dynamical systems and regularization techniques, showing that data augmentation significantly reduces hallucination frequency. Finally, we analyze networks that produce hallucinations and show that there are signatures in the attention matrix as well as in the hidden states of the decoder.", "keywords": ["nmt", "translate", "dynamics", "rnn"]}, "meta": {"decision": "Reject", "comment": "Strengths\n\n-  Hallucinations are a problem for seq2seq models, esp trained on small datasets\n\nWeankesses\n\n- Hallucinations are known to exists, the analyses / observations are not very novel \n\n- The considered space of hallucinations source (i.e. added noise) is fairly limited, it is not clear that these are the most natural sources of hallucination and not clear if the methods defined to combat these types would generalize to other types. E.g., I'd rather see hallucinations appearing when running NMT on some natural (albeit noisy) corpus, rather than defining the noise model manually.\n\n-  The proposed approach is not particularly interesting, and may not be general. Alternative techniques (e.g., modeling coverage) have been proposed in the past. \n\n-  A wider variety of language pairs, amounts of data, etc needed to validate the methods. This is an empirical paper, I would expect higher quality of evaluation.\n\nTwo reviewers argued that the baseline system is somewhat weak and the method is not very exciting. \n\n\n"}, "review": {"HJe5CxUsAX": {"type": "rebuttal", "replyto": "SJgtmx4oRm", "comment": "Thank you for taking an interest in our work! We're happy to answer your questions. \n\n1. Before we embarked on this project, we had no idea how easy it was to generate so many hallucinations. This is an easy test for practitioners to perform on their models to compare the ease of generating hallucinations with different models. We expect practitioners to modify the algorithm to suit their needs. For example, they could change the criteria for what is a hallucination and/or perturb with every token to discover if there were tokens that more reliably cause hallucinations.\n\n2. We discovered that you don\u2019t always need the reference translation! We found that the attention matrix and initial state of the decoder when performing a hallucination are significantly different than a normal translation. You can monitor your attention matrix and norm of the initial state of the decoder to help identify hallucinations. However, we didn\u2019t know, before doing this experiment, that these markers existed. We keep using the reference translations in algorithm 1 because it requires as much access to the models training pipeline (for most models) as the attention matrix or initial state of the decoder and is simple to reason about. In models with other architectures, perhaps these markers will also change. \n\n3. Yes, please see figure 5 and the description on page 8 of the process we used to identify warped attention matrices.\n\n4. We see no reason why not (though the work is yet to be done). Using a word based MT system would be similar in practice. The words would be indexed and a sentence converted to a sequence of indices. Then, perturbing with tokens would mean adding an index to that sequence and performing the translation. \n", "title": "Thank you for reading! We're happy to expand:"}, "ryxxt8I9RX": {"type": "rebuttal", "replyto": "rkg3VT4q0Q", "comment": "The transformer_big are too large to feasibly run many experiments on.  However, at your request, we are currently finishing a batch of transformer_base models (each takes about a week).  We will include the results of these models in our manuscript.", "title": "Yes, we will."}, "rJedhZhYA7": {"type": "rebuttal", "replyto": "H1lf5bhKA7", "comment": "- Why is your baseline so weak?\n\nWe have chosen to use small models for the sake of large scale analysis - note each data point comes from 10 separately trained NMT models. Additionally, we are interested in the recurrence of machine translation models and use a dynamical systems approach in one section of the paper. Jacobian computations are notoriously computationally expensive, and it would be technically infeasible to compute Jacobians on larger models (we would run out of memory). Even still, a single Jacobian computation takes around 20 minutes on CPU with Autograd. To compute the Jacobian for a larger model for 2999 sentences, even after parallelizing, would make scientific exploration infeasible. By keeping our models small, we can test over a thousand of models and explore more hypotheses. That being said, for the size of our models, our baseline BLEU score (25.6 with beam search) is reasonable as models above 30 BLEU very quickly become complicated due to the use of hard-to-analyze techniques such as model ensembling, etc. \n\nWe aren\u2019t sure what you mean by \"present results on the clean dataset with your baseline.\" Our baselines are using the original clean WMT En-De train, dev and test sets. \n\nDifferent NMT systems have different values. In production, one might value stability of translations and knowing when the model is hallucinating and balance that tradeoff with accuracy. We study both how to stabilize models and detect, via the attention matrix, when the model is hallucinating. \n\n- Your algorithm is very brute-forcey. \n\nYes it is simple :-). Our perturbation pipeline is favorable because of its simplicity and ease of adaptability. Yes, our perturbation pipeline could alter the meaning of sentences. However, one would not expect a completely different translation by adding a word, such as \u2018and\u2019, and this is precisely what we document. Previous work shows reastic perturbations (like typos) (such as: https://arxiv.org/abs/1711.02173) that do not semantically alter the meaning of sentences, but the example perturbed translations given in those texts are not as drastically different as we show. Further, we have provided examples of perturbations that do not alter semantic meaning, but still result in hallucinations (for example: all punctuation added to the beginning or end of a sentence, or adding \u201cund\u201d (German for \u201cand\u201d) as shown above). We aim to provide a framework and categorize a phenomenon that can help improve robustness of translation systems through identifying and understanding where the model slips up. \n\nAs you may have different values for the model or system you\u2019re building, we welcome you to try other types of perturbations (as you seem to have) and even modify the criteria for what is a hallucination based on your own understanding of your models (shift the threshold, include perplexity, change the weights of the adjusted BLEU score). We provide a simple setup, and make a case for the community to explore stability, hallucinations, attention, and the decoder\u2019s dynamics, as you have already begun to do. ", "title": "pt 2."}, "H1lf5bhKA7": {"type": "rebuttal", "replyto": "SyeoGAwkAX", "comment": "Thank you for your interest and feedback. With this paper, we introduce and document a novel phenomenon. We hope the community will do as you have done and explore stability, hallucinations, and the internals of their models. \n\nRespectfully, we strongly reject the implication that our methodology is flawed and are confident in our findings. There may be any number of reasons why your results differ from ours, from minor bugs to simple hyper parameter differences. We find the phenomenon of hallucinations to be robust over all our experiments, which include over a thousand models with many hyperparameter settings, random seeds, and architectural variants.\n\nBelow, we answer each question:\n\n- Why did you use BPE tokens?\n\nWe trained all our models with BPE/WPM because it is the common standard in NMT research and production [Google NMT (Wu et al. 2016) uses word-piece model, Transformer (Vasvani et al. 2017) and its derivatives for WMT-17, 18 are using byte-pair encoding (Bojar et al. 2017-18).]. We chose to also use BPE for perturbations to stay consistent with the model (we don\u2019t re-tokenize the sentence after perturbing it), and allows us to test a mixture of word and character perturbations. That being said, our methodology is much closer to perturbing models with full words as the majority of tokens we perturb with are either full words, punctuation or single characters. Of the tokens we chose as perturbing tokens, 75% of all common tokens, and 37% of all rare tokens are full words. The vast majority of rare tokens (~80%) are single Chinese, Korean, or Arabic characters. In our text, we give examples of realistic perturbations with both full words, for instance inserting \u201cund,\u201d the German word for \u201cand\u201d (taken from figure 5 (attention matrix), and punctuation. Here are two examples:\nOriginal input: In der medizinisch-behandelten Gruppe l\u00f6ste sich im Vergleich dazu der Diabetes vollst\u00e4ndig nur bei einem Prozent und teilweise bei nur etwa zwei Prozent.\nOriginal translation: In the medical and treatment group , the diabetes solved the total only at a per cent and in some cases only two per cent of the diabetes solved .\nReference: In the medically-treated group , by comparison , diabetes resolved completely in only 1 percent and partially in only about 2 percent .\nPerturbed input: und In der medizinisch-behandelten Gruppe l\u00f6ste sich im Vergleich dazu der Diabetes vollst\u00e4ndig nur bei einem Prozent und teilweise bei nur etwa zwei Prozent .\nTranslated perturbed: The company has been able to provide a new and more efficient solution for the company .\n\nOriginal input: Gauselmann w\u00fcnscht sich , dass die Mitgliedschaft im Schachclub und auch freundschaftliche Kontakt zum Tennisclub &quot; Rot-Wei\u00df &quot; als Ausdruck seiner Verbundenheit mit der Kurstadt gesehen wird .\nOriginal translation: Gauselmann wants to see that membership in the chess club and also friendly contact with the tennis club &quot; Rot-Wei\u00df &quot; is seen as an expression of his commitment to the city city .\nReference: Gauselmann wants his membership of the chess club as well as his friendly contact with the &quot; Red-white &quot; tennis club to be seen as an expression of his ties with the spa town .\nPerturbed input: . Gauselmann w\u00fcnscht sich , dass die Mitgliedschaft im Schachclub und auch freundschaftliche Kontakt zum Tennisclub &quot; Rot-Wei\u00df &quot; als Ausdruck seiner Verbundenheit mit der Kurstadt gesehen wird .\nTranslated perturbed: The Memory of the Science of the Science of the Science of the Science of the Science of the Town Square , the Cathedral , is a new and most popular place .\n\nWe give further examples of in section 8.3 of the appendix. \n\n\n- Using BPE tokens.\n\nWe append, prepend, replace, etc. as tokens appear in the vocabulary. The vocabulary includes words, subword tokens, and characters. \"und\" for example, appears as both \"und\" (word) and \"und@@\" (subword) in the vocabulary. We're sorry this was misleading. \nFurther, since we do not re-tokenize after adding perturbations, the NMT model will always see \u201cund@@ Guten morgen\u201d and never \u201c<UNK> morgen.\u201d In this case, whether a token is a subword or a full word token should not be more informative of how likely a sentence is to hallucinate than the stability of that particular token.", "title": "Thank you for your feedback. pt 1."}, "ryeoD08d0X": {"type": "rebuttal", "replyto": "rJgDGTh_2X", "comment": "Thank you for your feedback! We're glad you find this exploration interesting.\n\nWe've given some thought to how hallucinations compare to adversarial examples. Like adversarial examples, hallucinations illustrate a form of instability in models which can be useful to understand why the model behaves a particular way and help propose ideas for improving stability and generalizability of models. One difference is that we aren't looking for worst case perturbations (or to perturb an input to a particular result), nor do we use gradient based methods. We show it is simple to find a perturbation that causes such a divergent hallucination. So we have similar motivations, but go about it in different ways. ", "title": "Thank you for your feedback."}, "rkxlNRL_C7": {"type": "rebuttal", "replyto": "rygelC8dR7", "comment": "It is difficult to perform exactly algorithm 1 on translation systems like Google Translate. Our analysis requires knowing the vocabulary the model used during training, but production systems are typically trained on datasets that aren't publicly available. We invite researchers who train and serve production systems to test their systems with our methodology.\n\nTo explain further why we study a smaller neural network module in insolation, we first agree that production systems output better translations than research systems. Competition submissions also output better translations than research systems. However, our goal is to analyze and quantify a phenomenon we observed. Successful translation products deploy additional safeguards to reduce malformed outputs that are sometimes part of the model (as described above) and sometimes software, including overwriting and fixing outputs that have bad-publicity or are generally malicious. For Google Translate in particular, many examples have been logged/blogged (eg. https://motherboard.vice.com/en_us/article/j5npeg/why-is-google-translate-spitting-out-sinister-religious-prophecies and https://twitter.com/hashtag/neuralempty?src=hash We cite the former in our paper). Competition submissions also employ auxiliary techniques on top of the base model which complicates training and decoding in ways that are not well understood. For example, why does back-translation help so much? What does it change in the trained model? Thus, to hope to study the phenomenon, we remove these additional techniques from the bare NMT model. Our paper quantifies this study and documents our attempts to reduce and understand it. We believe that today's NMT systems, at the core of translation products and competition submissions are prone to hallucinations.\n\nStudying smaller models allowed us to tractably study many variants of our canonical model. For the size we investigated, the models used in our experiments achieve a competitive, average BLEU score of 25.6 (we previously reported the greedy BLEU score and not beam search, which the community typically reports). An RNN based NMT model (with 4x larger vocabulary and 4x bigger dimensionality compared to our models) is expected to reach 28 BLEU score ball-park as indicated here (https://github.com/tensorflow/nmt#wmt-german-english, on which our implementations are based) on the particular test set (newstest16) we\u2019ve used. Since the WMT challenge does not require a single model, all systems with a BLEU score of 30+ incorporate additional techniques on top of the bare NMT architecture. For instance, 2016 WMT German-English winning system with a 38.2 BLEU score, uses back-translation (a data augmentation technique for MT), model ensembles, and rescores with massive Language Models on top of large Neural Networks. These additional techniques would have made studying hallucinations overly complex and masks attempts to tease out root causes of this phenomenon.\n\nSmaller models also allowed us to explore a dynamical systems perspective. We were unable to compute the Jacobian of the hidden states of the decoder on a larger model because it simply could not fit in memory. At this size, we can feasibly compute the Jacobian, dh(t)/dh(s), but it still takes around 20 minutes per Jacobian we wish to compute. Even after parallelization, computing Jacobians for at least 2999 sentences pushes us to the edge of reasonable scientific exploration. With a model larger in any dimension, we would not have been able to do this analysis.\n\nYour suggestion to study coverage is interesting. While the coverage method proposed by Tu et al. ACL'16 is nontrivial to incorporate into our systems and to test, we did a separate coverage test by providing a coverage penalty during beam search decoding (as used in Google NMT) and found that they hallucinated on average 49.4%. For reference, the canonical model decoded with beam search hallucinates on average 48.2% and greedy decoding hallucinates on average 73.3%. It appears that adding coverage did not decrease hallucinations. However, adding coverage to beam search did impact the BLEU score and lowered the BLEU score from 25.6 to 22.3.\n\nThank you for giving us your feedback. We hope you will consider our goals and motivations while evaluating our work.", "title": "pt 2"}, "rygelC8dR7": {"type": "rebuttal", "replyto": "HkgrJLDq3m", "comment": "Thank you for your feedback. As per your suggestions, we added the following additional models and experiments, resulting in the following changes.\n* The BLEU scores we reported in the paper are with greedy decoding. Since the NMT community frequently reports BLEU with beam search, we have updated our paper to reflect this. Our canonical model achieves a competitive BLEU score of 25.6 on newstest16 (https://github.com/tensorflow/nmt#wmt-german-english). We now report this in the paper.\n* We added a Transformer model to our results. \n* We perturbed the Transformer model to hallucinate and found that it hallucinates on average (over ten random seeds) 16.6% of the time (there exists a token such that 16.6% of source sentences can be made to a hallucinate). We expand on and give a discussion of the Transformer model we used in the paper.\n* You are right that coverage would be an interesting model variant to look at. We ran a coverage study and found that coverage with beam search hallucinates on average 49.4% of the time, whereas beam search hallucinates 48.2% of the time and greedy decoding hallucinates 73.3% of the time. We expand more on why we chose this version of coverage below.\n* Finally, we correlated BLEU score to perturbation percentages and did not see a decrease in perturbation percentage as BLEU score increased.  We have added an additional figure to show this. The data shows that there is a correlation coefficient of 0.33 between BLEU score and hallucination percentage. This correlation should be interpreted cautiously because we haven\u2019t exhaustively explored the full space of models.\n\nOur paper is an analysis paper. Our goals are to quantify the phenomenon of hallucinations and explore what this tells us about training and using NMT models. To make these goals technically feasible, we extract the core NMT neural network model from the layers of techniques and fail-safes in production systems and SoTA-level competition entries. To make these goals technically tractable, we scale down the bare model which allows us to study as many hyperparameters, architectural variants, and random seeds over the thousand+ models we studied. Results we find on small models are not irrelevant. Our canonical models train to an average BLEU score of 25.6, competitive for its size, and we show that an increase in BLEU score does not correlate to a decrease in the percentage of hallucinations. In the next comment, we'll expand further on our decisions.\n", "title": "Thank you for your feedback. We've run additional experiments to address your questions. pt 1."}, "Byg0UoU_RX": {"type": "rebuttal", "replyto": "BJxe2Ed9nQ", "comment": "Thank you! At your request, we have updated figure 4 to make it more clear what each part represents. We've also added to the caption to explain what the differences between the two attention matrices are. Below, we've expanded more on the questions you've raised.\n\n1. We chose subword tokens (segmented with byte pair encoding) from our source language (German) vocabulary so we never have a noisy word that\u2019s unseen in the training set. We\u2019ve described how we develop our source, subword vocabulary in section 3 at the bottom of page 3. We chose these tokens as representative of the distribution of tokens: The specific tokens we\u2019ve chosen are based on one of four types of subword tokens: common, rare, mid-frequency, and punctuation tokens. We first sorted our vocabulary of subword tokens by frequency, then formed the following groups:\n    a. Common tokens: the 100 most common tokens\n    b. Rare tokens: the 100 least common tokens\n    c. Mid-frequency tokens: After removing common and rare tokens from our sorted vocabulary of subword tokens, we sample 100 random tokens.\n    d. Punctuation tokens: All punctuation marks that exist in the vocabulary.\n    (This selection process is described in the first paragraph of section 4.)\n    Since we use BPE encoding, which segments words into sub-word units depending on their frequencies (character level co-occurrences to be precise), unseen words are never treated as UNK tokens. If a word does not appear in the training set, the BPE algorithm will segment it into the sub-words or characters that appear in our final vocabulary instead of using the UNK token.\n\n2. Here is a further explanation of the difference in the upper right of figure 4 compared to the upper left.\nThe attention matrix shows the attention weight applied to each input token in the source sentence (x-axis) as the model decodes and outputs the translated sentence (y-axis). On the upper left, we show the attention matrix of an unperturbed translation. We see weight is applied to most of the input source tokens. On the upper right, we show the attention matrix of the same source sentence, but with a perturbation at the beginning (\u2018und\u2019) that causes the translation to hallucinate. We observe that weight is applied to very few input source tokens throughout translation, which is highly atypical and indicative of a broken translation.\n", "title": "Thank you for your feedback! We've made some clarifications."}, "BJxe2Ed9nQ": {"type": "review", "replyto": "SkxJ-309FQ", "review": "I think this paper conducts several interesting analysis about MT hallucinations and also proposes several different ways of reducing this effect. My questions are as follows:\n\n* I am very curious about how do you decide the chosen noisy words. I am also wondering what is the difference if you do choose different noisy words. Another thing, if the noisy words are unseen in the training set, will it be treated as \"UNK\"?\n* Can you highlight what is changed in the upper right side of fig.4? It would be great if you include gloss in the figure as well.", "title": "interesting analysis", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HkgrJLDq3m": {"type": "review", "replyto": "SkxJ-309FQ", "review": "\tMy major concern about the work is that the studied model is quite weak. \n\t\"All models we present are well trained with a BLEU score of at least 20.0 on the test set, a reasonable score for 2-layer models with 256 hidden units.\" \n\t\"We then used the WMT De!En 2016 test set (2,999 examples) to compute the hallucination percentage for each model.\"\n\tI checked the WMT official website http://matrix.statmt.org/matrix. It shows that the best result was a BLEU score of 40.2, which was obtained at 2016. The models used in this work are about 20.0, which are much less than the WMT results reported two years ago. Note that neural machine translation has made remarkable progress in recent two years, not to mention that production systems like Google translator perform much better than research systems. Therefore, the discoveries reported in this work are questionable. I strongly suggest the authors to conduct the studies base on the latest NMT architecture, i.e., Transformer.\n\t\n\tFurthermore, I checked the examples given in  introduction in Google translator and found no hallucination. So I'm not sure whether such hallucinations are really critical to today's NMT systems. I'd like to see that the study on some production translation systems, e.g., applying Algo 1 to Google translator and check its outputs, which can better motivate this work.\n\t\n\tFor the analysis in Section 6.1, if attention is the root cause of hallucinations, some existing methods should have already address this issue. Can you check whether the model trained by the following work still suffers from hallucinations?\nModeling Coverage for Neural Machine Translation, ACL 16.", "title": "about the models", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJgDGTh_2X": {"type": "review", "replyto": "SkxJ-309FQ", "review": "The authors introduce hallucinations in NMT and propose some algorithms to avoid them. \nThe paper is clear (except section 6.2, which could have been more clearly described) and the work is original. \nThe paper points out hallucination problems in NMT which looks like adversarial examples in the paper \"Explaining and Harnessing Adversarial Examples\". So, the authors might want to compare the perturbed sources to the adversarial examples.\nIf analysis is provided for each hallucination patten, that would be better. \n", "title": "Adversarial examples in NMT", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}