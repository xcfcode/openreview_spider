{"paper": {"title": "Memory Representation in Transformer", "authors": ["Mikhail Burtsev", "Yurii Kuratov", "Anton Peganov", "Grigory V. Sapunov"], "authorids": ["~Mikhail_Burtsev1", "yurii.kuratov@phystech.edu", "apeganov@nvidia.com", "~Grigory_V._Sapunov1"], "summary": "Transformer can be trained to use memory for solving natural language processing tasks.", "abstract": "Transformer-based models have achieved state-of-the-art results in many natural language processing tasks. The self-attention architecture allows transformer to combine information from all elements of a sequence into context-aware representations. However, information about the context is stored mostly in the same element-wise representations. This might limit the processing of properties related to the sequence as a whole more difficult. Adding trainable memory to selectively store local as well as global representations of a sequence is a promising direction to improve the Transformer model. Memory-augmented neural networks (MANNs) extend traditional neural architectures with general-purpose memory for representations. MANNs have demonstrated the capability to learn simple algorithms like Copy or Reverse and can be successfully trained via backpropagation on diverse tasks from question answering to language modeling outperforming RNNs and LSTMs of comparable complexity. In this work, we propose and study few extensions of the Transformer baseline (1) by adding memory tokens to store non-local representations, (2) creating memory bottleneck for the global information, (3) controlling memory update with dedicated layer. We evaluate these memory augmented Transformers and demonstrate that presence of memory positively correlates with the model performance for machine translation and language modelling tasks. Augmentation of pre-trained masked language model with memory tokens shows mixed results for tasks from GLUE benchmark. Visualization of attention patterns over the memory suggest that it improves the model's ability to process a global context.", "keywords": ["transformer", "memory augmented networks"]}, "meta": {"decision": "Reject", "comment": "The paper studies three kinds of memory-augmented Transformers, focusing on one (the MemTransformer, which adds [MEM] tokens to a document.)  This is a nice clean extension of Transformers and a topic well worth investigating.  Unfortunately, the experimental results were considered unconvincing:\n\n - The baselines were relatively weak\n - The experimental setting was unusual (eg only 10 epochs)\n - The experiments did not show consistent improvement\n\nOverall the paper was considered below acceptable quality for ICLR.\n"}, "review": {"_0S1DsO1_kU": {"type": "review", "replyto": "g1KmTQhOhag", "review": "> Summary: The paper proposes to study three formulations (MemTransformer, MemCtrl and MemBottleneck) of memory-augmented self-attention transformers, and investigate the influence of adding memory tokens to the model to its overall performance. The authors claim via some experiments on MT and LM that memory augmentation is able to improve the canonical transformer models.\n\n-------------------------\n\nPost-rebuttal thoughts:\n\nWhile the rebuttal of the authors did clarify some points, I think it is undeniable that the empirical results presented in the current version of the paper are still far from being convincing enough to claim this form of memory augmentation is a useful addition to the Transformers. The visualization of the memory augmentation via attention map is also too handwavy. \n\nWhile I appreciate the authors' effort to address my questions in the rebuttal phase, I will keep my current score.\n\n-------------------------\n\n**My general opinion**: \n\nI personally think that memory augmentation is a simple but potentially useful technique that explicitly adds memory to a deep network, which, as the authors mentioned in their paper, is not a new topic. Memory augmented networks have been around in many deep models, especially sequence models. However, I find this paper still not strong enough for me to recommend its acceptance in its current form, mainly because 1) the formulations are rather incremental; and 2) the empirical results are too weak to demonstrate (convincingly) the usefulness; and 3) I found some experimental settings uncommon and empirical analysis too handwavy. My detailed question and comments are below.\n\n-------------------------------------\n\nI have some detailed questions/comments.\n\n1. In the MemBottleneck formulation, does the update rule of $A^\\text{seq}$ take $X^\\text{mem}$ or $H^\\text{mem}$? Figure 1d seems to suggest that the memory update at a layer $i$ precedes the update on the sequence tokens. In addition, what is the difference between $H$ and $X$? Are they not the same sequence (i.e., the $H$ from a layer is passed in as $X$ of the next layer)?\n\n2. In the MemBottleneck subsection, the paper claims that its cost \"scales linearly with the size of the input sequence $O(N)$\" instead of $O(N^2)$. But if the hypothesis of the authors are correct, that these memory modules serve to collect and \"re-distribute\" the sequence information, shouldn't one generally expect the memory size to be proportionate to the sequence length? For example, I certainly would expect that a sequence of length 8000 would certainly require a different memory size than a sequence length 40. The point is, one cannot simply assume \"the size of the memory is constant\", just like one cannot assume \"the size of the sequence is constant\". The more accurate way, for instance, is to say the complexity is $O(NM)$ with $M$ being the memory size.\n\n3. My major concern with this paper is with its various empirical results and experimental settings:\n   - i) In machine translation, how exactly does the encoder-decoder self-attention work in the three proposed Mem settings? E.g., does the decoder queries attend to $X^\\text{mem}$ only (of the encoder output), or also the $X^\\text{seq}$?\n   - ii) How does MemCtrl models perform in the small setting of Table 1? (They seem to be slightly better on the larger setting)\n   - iii) Overall, I found the numbers reported for WMT'14 de-en in Table 1 **too low** for me to convince of anything. Even though the *much more commonly used setting is WMT'14 en-de*, it should be overall easy to get a >29 BLEU score with a base Transformer model (e.g., [1,2] got >31 for the base Transformer). I am not sure why the authors halted their experiment at 20 epochs and 10 epochs (how many training steps are these though?). Hence, it's not clear to me whether these minor improvements in the below-expectation BLEU scores are truly indicative of the final performance of these models. \n    - iv) The paper says no beam search was used. Why? What are the results if you use beam_size = 5?\n    - v) The MemBottleneck Skip Transformer is really just a weak form of a typical Transformer, except that you initialized the first layer of the hidden units to 0, and downsample the sequence from the length of $X^\\text{seq}$ to the length of $X^\\text{mem}$. So it's not a surprise to me that \"MemBottleneck Skip Transformer 20\" is significantly better than MemBottleneck 10/20. \n    - vi) The various ablation studies the authors made in the paper, in my opinion, exactly suggest that the usefulness of these memory augmentations are in doubt. For example, the huge gap between MemBottleneck and canonical transformer, as well as the negative gap between MemBottleneck 10 and MemBottleneck 20, I think is good evidence of this. Another example is the Table 2, where inference time with a larger memory size even degrades the performance. I didn't find a satisfactory explanation from the authors on this.\n    - vii) What are the values in Table 3? Are they BLEU scores? Why are they <1?\n    - viii) The paper says that \"BLEU score of MemTransformer 10 with 5 `[mem]` tokens shows it is still able to translate with acceptable quality\". This is false. 7 BLEU score is **A LOT** of difference. If you look at the generation result, they are qualitatively very different.\n    - ix) How exactly did you use the memory augmentation for a causal/decoder Transformer for the language modeling task in Table 3? This is not explained in detail in the paper, but how did you handle the potential information leakage problem (i.e., we don't want future tokens to flow back to the past)? If you simply do it in the same way as in encoders of MT by appending it to the sequence, then you can either only *read* from the memory without writing (if you append to the start of the sequence), or only *write* to it without reading (if you append to the end of the sequence). I might be missing something here, and hopefully the authors can clarify.\n    - x) The fact that there's no consistent improvement in Table 5 of the memory-augmented Transformer on the base Transformer is very concerning to me. Apparently, sometimes BERT-base is better than many of the MemTransformers.\n\n4. One potentially interesting study that I think the authors can look into is the memory slot permutation. In a certain sense, the memory slots have the same representation embedding `[mem]`, and are mainly differentiated (among themselves) by their respective position embedding. However, it also seems from Figure 2 that when writing to memory, the original order of the sequence tokens is not necessarily preserved. So what if, in the intermediate layers of the MemTransformer, you permute the order of the memory slots? Do you expect that to affect the performance of the MemTransformer?\n\n5. The other issue I found about the paper is that, although this is an empirical paper, there are still a bunch of not well-supported claims (i.e., handwavy claims) which the authors should have looked into. For example:\n    - i) In section 3.1: \"This can be due to the more complex architecture of the MemBottleneck that has twice more layers in the encoder part\". But there's no formal investigation of this in the paper... for example, does an 8-layer MemTransformer (which has \"twice as many layers as the 4-layer MemBottleneck) also perform badly?\n    - i) In the caption of Figure 2: \"Activity in the left top corner that involves first four tokens might indicate fusion of neighbour vectors by pairwise summation of `[mem]` tokens.\" This sounds like a posterior explanation... and it's not clear to me what this \"fusion\" is doing and why it's doing this. How is this helping MemTransformer? Does it occur in every head in every layer? Does it happen even if you only use memory size 5? These questions should be answered if you would like to claim this phenomenon.\n    - ii) In the \"memory to memory attention\" paragraph: \"If the diagonal attention is sharp, then corresponding memory vectors are added to themselves, so their content is amplified and became more error-tolerant.\" Do you have support for the error tolerance claim? Is it still \"amplified\" even if there is a LayerNorm immediately afterward, which would shrink things back anyway (i.e., $\\text{LN}(x)$ and $\\text{LN}(x+x)$ should be the same)?\n\n-------------------------------------\n\nSome minor points that didn't impact the score:\n\n1. In the intro section, 'results in \"blurring\"' ---> it's not clear to me what this means. Please clarify.\n2. In the very beginning of section 3, the authors used $N=4$ to refer to number of layers. But $N$ has been used to refer to sequence length before (e.g., when saying $O(N)$). \n\n- [1] https://www.ijcai.org/Proceedings/2020/0534.pdf\n- [2] https://arxiv.org/pdf/2010.07638v1.pdf\n", "title": "Review from R5: Interesting exploration but the method and empirical studies are relatively weak", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "eqhju1iena": {"type": "rebuttal", "replyto": "g1KmTQhOhag", "comment": "We appreciate comments and suggestions from all reviewers and looking forward to use some ideas in further research. Unfortunately, we were not able to finish all our experiments to reproduce stronger baseline in time. Still, we believe that results already presented in the paper are valuable for a progress in the field and ask reviewers to reconsider scores.", "title": "Thanks to all reviewers for their effort spent on discussion of our contribution"}, "5odj3wkP4Lc": {"type": "rebuttal", "replyto": "EkBx88idgqh", "comment": "Agree with your point that weak baseline is a common concern among all reviewers. We will try to address it during rebuttal period.", "title": "will try to address the common concern."}, "eIGJfSohl18": {"type": "rebuttal", "replyto": "EU7IPF3ynXx", "comment": ">  I'm quite confused. The authors on one hand agree that there's a significant deviation with a change in the number of [mem] tokens, at the same time insist on using \"gradually\". Sorry, but if the performance drops from 25.07 to 7.87 with a change in memory size then it is not gradual! \n\nBy gradual we mean that decrease in performance correlates with reduction of memory capacity, full 10 [mem] tokens - 25.07, 5 [mem] - 18.22, 2 [mem] - 15.91, 0 [mem] - 11.75. We will reformulate our statement to avoid further possible misunderstanding.\n\n> See below for my criticism on MT.\n\nPlease, find our opinion on this matter in response to the R5 - \nhttps://openreview.net/forum?id=g1KmTQhOhag&noteId=uJm31kKvY0t", "title": "Response"}, "uJm31kKvY0t": {"type": "rebuttal", "replyto": "Iv5wHNNoCDh", "comment": "The primary focus in our work was not on improving machine translation or language models per se but to study how general purpose memory slots might be utilized by Transformer architecture. This is why we not agree that hacks **should** be included. We took the model from the library to minimize implementation bias and minimally changed it. We run the both versions for the same hyperparameters and the same number of training steps then report numbers and discuss what happen with memory representation. It would be nice to improve sota by small modification of baseline but it is not always the case. \n\nFor example, XLNet [3] brings quite minimal empirical improvement over the BERT (Table 1, p.7) and it  is hard to find substantial number of XLNet based solutions on the GLUE and SuperGLUE  leaderboards. Was it worth publishing XLNet paper if it is so weak on GLUE\\SuperGLUE? \n\nAnyway, we will try to reproduce stronger baseline till the end of rebuttal period and update results in the paper.\n\n[3] https://arxiv.org/pdf/1906.08237.pdf", "title": "win in numbers is not ony value for research"}, "sfmSW-_b85H": {"type": "review", "replyto": "g1KmTQhOhag", "review": "The paper presents transformer extensions with global memory.  The basic idea is to augment the input sequence with special memory token [mem].  The authors explore three variants: (a) MemTransformer - just the augmented sequence, (b) MemCtrlTransformer - separate params for memory update, and (c) MemBottleneck - Bottleneck of memory used to update token representations. The authors show gains over vanilla transformer/pretrained baseline for machine translation, language modeling, and the GLUE setup. \n\nI think the idea in general is interesting but it's not particularly novel. The authors cite a lot of recent works in this area, and a particularly similar approach has been explored by Gupta et al in \"GMAT: Global Memory Augmentation for Transformers\" (authors should cite this work). The main problem with the paper is that the empirical results are quite weak and don't show a consistent trend with regards to utility of memory. \n\nApart from the machine translation results, it's hard to make a case that the gains on GLUE and language modeling are significant. For the MT results, the baseline results look too weak and the authors don't cite any prior work. Through a quick scan of MT papers, I found papers reporting +6 BLEU scores on the same task and the same architecture (see https://arxiv.org/pdf/1904.09324.pdf).  This is concerning because the baseline model seems highly undertuned. \n\nAmong the three variants, the authors only provide results for MemTransformer on language modeling and GLUE. \nFor machine translation, some of the entries in Table 1 are missing, and the small and base model don't have the same entries.\n\nOther minor comments:\n* What do the authors mean by 2000 validation \"segments\"? Are the segments just sentences? Is it the standard dev set? If not, then why not? If it's the standard dev set, why not refer to it at as WMT-14 DE-EN dev set.\n* The authors don't specify the number of attention heads in main text. I presume it's 8, as mentioned in appendix.\n* In Table 1, the authors add a result with \"MemCtrl Shared Transformer\" which they say is the MemCtrl model with shared parameters. Isn't this the same as MemTransformer?\n* The authors say in conclusion that \"the speed of training positively correlates with the memory size\". They don't provide any evidence for this, and if they train for the same number of epochs then increasing memory size should only increase the training time. \n* Again in conclusion, the authors say that \"the memory controller learned by the model degrades only gradually when memory size is changed during inference.\" That is clearly not true given the results in Table 2. \n\nSuggested text edits:\n* Section 1 - \"They have unspecific [mem] tokens that can store global or copy of local information.\" -> Rephrase this/Expand on this. \n* Section 2.2 - \"It minimally changes the original model architecture.\" -> There's no architecture change. Get rid of this line.\n* Section 3 - \" if the opposite is not stated.\" -> \"if otherwise stated.\"\n* Section 3.2 - \"Kovaleva et al. (Kovaleva et al., 2019) \" -> Use \\citet\n\n\n", "title": "Interesting Idea But Weak Empirical Results", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HPWpv_MHQqQ": {"type": "rebuttal", "replyto": "2TdaDWGUP77", "comment": "> It seems from the experiments that little practical gains can be expected, since MemTransformer never outperforms the transformer baseline in terms of accuracy or efficiency.\n\nThis statement contradicts experimental evidence. Results presented in the paper demonstrate that MemTransformer outperforms the baseline for the majority of tasks we tested. \n\n>Motivation is not clear: the proposed approach doesn\u2019t seem to bring any interesting practical gain, nor does it answer any interesting research question.\n\nThe motivation is well aligned with a lot of other modern research like Star-Transformer, ETC, Big Bird, Longformer, and others which do use a combination of global and local attention to improve Transformer architecture. Our research also contributes to the area of memory augmented networks, to our knowledge  there are no publications that study transformer modifications in the framework of memory augmentation.\n\n>Last but not least, the experimental setting is flawed: e.g., the MT experiment cuts training at a suspiciously early stage, presumably contributing to the bad performance.\n\nIt\u2019s true that the models could be trained longer, and it would probably give us even better results. Yet, 20 epochs do already show a difference, and in terms of the number of updates it is not really small. In general, our results clearly demonstrate that simple augmentation of input with memory tokens improves performance in a variety of tasks from machine translation to classification.\n\n>I\u2019m not against discussing related works in the introduction. But the current version is probably trying to scramble too much stuff into intro, making it tedious to read.\n\nYou\u2019re right, it\u2019s worth separating this part into a \u2018Related work\u2019 section.\n\n>I suggest walking through the transformer architecture in a more self-consistent way. I would be very surprised if one without much hands-on experience of transformers can understand what\u2019s going on in section 2.\n\nUnfortunately, the size limit of the submission does not allow us to add an introduction to the transformer architecture. We gave a short mathematical formulation of the transformer architecture which is typical for many papers (say, https://arxiv.org/abs/2006.04768 or https://arxiv.org/abs/2003.05997).\n\n>The baselines\u2019 performance in the MT experiments is far worse than what we usually see. I\u2019m guessing this is because the training stops too early. Can the authors explain why they choose to do so? If limited computation is the concern, I recommend the authors to work with smaller datasets instead but use a more convincing settings. Same for other experiments.\n\nActually, the results on the WMT DE-EN are not \u2018far worse than what we usually see\u2019. We should note that we use DE-EN, and not EN-DE dataset. The latter is more common, yet the former is used as well. We chose DE-EN because it is easier to demonstrate that the results are of reasonable quality and to help readers better understand the examples in the paper (just because English is the common ground).  In our experiments 12-layer Transformer (baseline taken from official TF tutorial - https://www.tensorflow.org/tutorials/text/transformer ) achieved 24.65 BLEU after 10 epochs, which is in the top-10 results according to the (see https://paperswithcode.com/sota/machine-translation-on-wmt2014-german-english). To make our results more interpretable, we did not introduce any additional hacks and extensions to the baseline for improving it\u2019s scores.\n\n>The experiments do not show any practical gain from MemTransformer. The paper talked about its linear complexity in input sequence length. This paper would be stronger if it can present convincing experiments and show that MemTransformer can have a better efficiency in practice.\n\nWhile the results are not SoTA, MemTransformer demonstrates improvements compared to baselines. So, adding [mem] tokens to BERT-base model improved its performance on 6 / 9 tasks as shown in the Table 5. On WMT-14 DE-EN MemTransformer models clearly outperform the Transformer baseline.\n\n>Section 3.2 would be much more clear and interesting if it opens with the research questions it\u2019s trying to answer.\n\nThank you. We will try to formulate the research question for the Section 3.2 to make interpretation of results easier. ", "title": "Thanks for comments."}, "ZQdtZPw9n3": {"type": "rebuttal", "replyto": "3glSHz0IRDb", "comment": ">The main claim of the paper is that presence of memory tokens positively correlates with the model performance. At the same time, adding memory tokens increases the model\u2019s capacity, making the comparisons with the baseline unfair. The more rigorous approach would be to compare to a baseline with proportionally increased dimensions as having memory tokens may be akin to having additional space to store sequence token representations.\n\nThank you for the suggestion. We will perform comparison with the baseline you proposed and include results in the next version of this or other upcoming paper.\n\n>The use of global tokens (like [CLS] token in Longformer as mentioned in the paper) is a well-known technique that undermines the paper\u2019s novelty.\n\nThere is a principal difference between [mem] tokens and other service tokens like [CLS]. [CLS] output is directly included in the loss function for the next sentence classification, and tokens like [SEP] directly related to the training task. On the other hand, [mem] tokens are general purpose placeholders for any representation. There is no \u201ctarget\u201d value for a [mem] token, so its output doesn\u2019t included in the loss function directly. \n\nAs well, while [CLS] token is used for a long time, to the best of our knowledge there are no works that perform an analysis of the attention patterns emerged when using specifically this token. \n\n>The authors claim that models\u2019 quality positively correlates with the memory size. However, this statement contradicts the findings in Table 1 and Table 5. The authors briefly discuss the lack of trainability in section 3.1. Still, the authors do not specify conditions under which adding more memory will benefit the model quality.\n\nThe statement does not contradict the findings in these tables. MemTransformers consistently produce better results.\n\n>Memory lesions experiments (table 2). Memory size change during inference seems to lead to drastic drops in model quality. At the same time, the models are not trained to perform in the absence of memory. What if the models are trained on different memory sizes from the beginning with memory size sampled per-batch?\n\nThank you for this idea. We think that such noisy training might increase robustness of the model to the variation in the size of memory during inference. \n\n>Memory extension experiments (table 3). Does the model forget how to work with smaller memory sizes during the memory extension process? E.g., what will memory lesions results look like for the memory extension model?\n\nThis is an intersecting research question. It can be quite easily tested without extended experiments. We will include results of such lesioning in the next version of the paper.\n\n>Are any of the models for the GLUE benchmark trained using a memory extension training scheme?\n\nNo, but it is a good idea to test.\n\n>It\u2019s not clear if all mem tokens use the same embeddings or if they use separate embeddings (e.g., [mem1], [mem2], etc)\n\nAll mem tokens use the same embeddings but positional encoding for every [mem] token was different.\n\n>MemBottleneck results lack comparison with other extensions that lower the computational cost (O(N), O(N log N) extensions, etc)\n\nThat\u2019s correct. We did not target lowering the model computational complexity in the paper, so we do not perform comparisons with those models.\n\n>Relative positional embeddings seem to work better with memory tokens, are there any more experiments with relative positional embeddings except for the ones in table 5?\n\nThese are the only results using relative positional embeddings. It\u2019s an interesting direction to explore in the future .\n\n>Not all results in Table 1 are symmetrically available for both small and base models. \u2026 No results for MemBottleneck and MemCtrl in table 5. At the moment, both MemBottleneck and MemCtrl look weak; having more results for those extensions may help.\n\nTrue. We did not have a computational budget to perform all the comparisons and less promising MemCtrl and MemBottleneck was dropped from some experiments. However, the experiments conducted give us an understanding of the relative performance improvements/degradations.\n\n>The authors can add FLOPS/params count to tables 1 and 5 to make a comparison of the models' more convenient.\n\nWe will add this to the updated version of the paper.", "title": "Thanks for insightfull comments and ideas to test."}, "tJgLAeTKe5Q": {"type": "rebuttal", "replyto": "sfmSW-_b85H", "comment": ">I think the idea in general is interesting but it's not particularly novel. The authors cite a lot of recent works in this area, and a particularly similar approach has been explored by Gupta et al in \"GMAT: Global Memory Augmentation for Transformers\" (authors should cite this work). \n\nGMAT appeared the same time we published our paper on arxiv, we were not aware of it when working on the paper. We agree, we should mention it in the updated paper. GMAT is clearly related to our approach, yet it is different in almost all the details: attention type, general architecture, set of tasks. GMAT paper also doesn\u2019t perform any analysis of memory patterns.\n\n>The main problem with the paper is that the empirical results are quite weak and don't show a consistent trend with regards to utility of memory.\n\nMemTransformer modifications outperformed baselines in machine translation, language modeling and in 6 out of 9 GLUE tasks. We regard this as a consistent trend for improvement with small but significant gains.\n\n>Apart from the machine translation results, it's hard to make a case that the gains on GLUE and language modeling are significant. For the MT results, the baseline results look too weak and the authors don't cite any prior work. Through a quick scan of MT papers, I found papers reporting +6 BLEU scores on the same task and the same architecture (see https://arxiv.org/pdf/1904.09324.pdf). This is concerning because the baseline model seems highly undertuned.\n\nWe do not target the SoTA on MT benchmark as our goal is to estimate the improvements over the baseline. In our experiments 12-layer Transformer (baseline taken from official TF tutorial - https://www.tensorflow.org/tutorials/text/transformer ) achieved 24.65 BLEU after 10 epochs, which is in the top-10 results according to the (see https://paperswithcode.com/sota/machine-translation-on-wmt2014-german-english). To make our results more interpretable, we did not introduce any additional hacks and extensions to the baseline for improving it\u2019s scores.\n\n>Among the three variants, the authors only provide results for MemTransformer on language modeling and GLUE. For machine translation, some of the entries in Table 1 are missing, and the small and base model don't have the same entries.\n\nTrue. We did not have a computational budget to perform all the comparisons. However, the experiments conducted give us an understanding of the relative performance improvements/degradations.\n\n> What do the authors mean by 2000 validation \"segments\"? Are the segments just sentences? Is it the standard dev set? If not, then why not? If it's the standard dev set, why not refer to it at as WMT-14 DE-EN dev set.\n\nYou are right, \u201csegments\u201d are sentences from standard WMT-14 DE-EN validation set obtained from https://www.tensorflow.org/datasets/catalog/wmt14_translate#wmt14_translatede-en. Corrected in the text.\n\n> The authors don't specify the number of attention heads in main text. I presume it's 8, as mentioned in appendix.\n\nThis information is provided in the footnote to the sentence describing the transformer setup at the beginning of the Section 3 (p.4) as a variable $h$. \n\n>In Table 1, the authors add a result with \"MemCtrl Shared Transformer\" which they say is the MemCtrl model with shared parameters. Isn't this the same as MemTransformer?\n\nIn  MemCtrl Shared all 6 layers of the memory control subnetwork have the same parameters. In MemCtrl  every layer of the memory control subnetwork has its own parameters. This is described in the last two sentences of the first paragraph of Section 3.1. (p. 5).\n\n>The authors say in conclusion that \"the speed of training positively correlates with the memory size\". They don't provide any evidence for this, and if they train for the same number of epochs then increasing memory size should only increase the training time.\n\nHere \u201ctraining time\u201d was assumed to represent a number of training steps, so the same performance can be achieved faster compared to baseline. This is equivalent to the better quality after training for the same number of steps. We\u2019ve edited this sentence in conclusion to clarify this.\n\n> Again in conclusion, the authors say that \"the memory controller learned by the model degrades only gradually when memory size is changed during inference.\" That is clearly not true given the results in Table 2.\n\nThat depends on an interpretation of graduality. The fact is that the stronger deviation in the number of [mem] tokens during the inference leads to worse performance. \n\n>Suggested text edits \n\nThank you for suggestions, we updated the paper to address them.", "title": "Thanks for comments and suggestions. "}, "xoXUSLTLpkO": {"type": "rebuttal", "replyto": "7b_VWzn-bf", "comment": "> The idea is not entirely new; Memory augmented networks have been proposed in previous work and the paper does not differentiate the method from related work very well.\n\nThat\u2019s true the idea is not entirely new, yet there is very little research focused on investigating how the memory is utilized in transformers. To the best of our knowledge, no baselines such as proposed in the paper were investigated before. If you know a particular work we should mention, let us know, please.\n\n>The paper is not clear about how to obtain the memory input token.  According to the paper, the memory input token seems like a static vector; however, a memory augmented network can retrieve a dynamic vector for global information, with the ability to read and write to the memory.\n\nIn the second paragraph of section 2.2. (bottom of p.3) we introduce and formally define [mem] tokens. We extend vocabulary with extra service [mem] token, which is similar to [SEP] token heavily used in BERT-like models. The [mem] tokens serve as placeholders which allow transformer to process arbitrary vectors dynamically. This is a core idea of the paper and our results demonstrate that after training dynamic updates of these placeholders are consistent with writing, reading and processing operations.\n\n> The naive Mem Transformer Layer is counter intuitive, as there is little differentiation between the memory layer and the sequence layers. How the memory layer contains additional global information is not explained.\n\nIn the case of naive Mem Transformer Layer (fig. 1b), there is no differentiation between the memory and sequence layers as described in the Section 2.2 (p.3). Both [mem] tokens and sequence tokens are processed by the same vanilla transformer layers according to eq. (1) and eq. (2) (p.3).\n\n> Figure 1c is confusing, are the arrows crossed or not?\n\nIn the case of MemCtrl Transformer, there are two different layers. The first one updates representations of sequence tokens, and the second updates representations of [mem] tokens. Both layers attend to the same concatenation of sequence and memory (sec. 2.3 p.4), this concatenation of attention input is depicted by crossing arrows.\n\n>-The formulation of memory representation and sequence representation are exactly the same for the MemCtrl Transformer, which again is very counter intuitive. It is intuitive that we should augment the sequence transformer layer with additional global information, however, we might not want the other way. The memory layers should be updated at coarser granularity.\n\nMemory representation is the same for all modifications we propose in our paper. The difference lies in the processing of these representations. We are not imposing any specific function on the memory tokens such as global or local or any other representation, the idea is that the transformer should learn memory representation that helps to solve the main task. It is not clear what do you mean by coarser granularity, so we can not address this in our reply.\n\n> -The motivation for a memory network like proposed in the paper is not clearly stated. How the memory network can capture long-term dependencies and information is not clearly explained. The design of variants of Memory Transformers seems very arbitrary.\n\nDisagree. We demonstrate attention patterns and extensively discuss how transformer can accumulate and process information in memory. Our selection of Memory Transformer modifications to study is very straightforward and explained in the first lines of every dedicated section (sections 2.2, 2.3, 2.4). Specifically, the Simple MemTransformer (sec. 2.2. p.3) is the simplest possible memory extension of the baseline transformer. In the MemTransformer memory and sequence are updated by layers with the same parameters but if processing of memory requires specific computation then it is better to train dedicated subnetwork for this. Thus in the MemCtrl we introduce  a separate sublayer for memory processing. In the MemBottleneck we block direct exchange of information between representations of elements of a sequence to test capabilities of the transformer to learn operations for  processing of global information in memory only.\n\n> -The paper also lacks a more detailed comparison with related work, like Transformer-XL, Star-Transformer, Longformer, ETC, etc.\n\nWe will add more detailed comparison to these architectures to the final version of the paper. \n", "title": "Explaining main concepts of the paper"}, "RtfA1wbIIq8": {"type": "rebuttal", "replyto": "_0S1DsO1_kU", "comment": "> In the MemBottleneck formulation, does the update rule of  $A^{seq}$ take $X^{mem}$ or $H^{mem}$? \n\nYou are right that $H^{mem}$  is passed to the next layer as $X^{mem}$ . To make it less confusing we will update equations in Section 2.4.\n\n>The more accurate way, for instance, is to say the complexity is with being the memory size.\n\nThank you for your suggestions. Your formulation is more accurate and will use it in the paper.\n\n>In machine translation, how exactly does the encoder-decoder self-attention work in the three proposed Mem settings?\n\nWe keep the decoder intact, so it attends to concatenation of memory and sequence representations. We will update paper to make this clear.\n\n>How does MemCtrl models perform in the small setting of Table 1?\n\nWe compared MemCtrl variants to MemTransformer and baseline only over the base sized models. We expect that MemCtrl with shared weights should have performance similar or better then MemTransformer.\n\n> Overall, I found the numbers reported for WMT'14 de-en in Table 1 too low for me to convince of anything. \n\nYou are right that  WMT\u201914 en-de is more common, but we chose de-en because it is easier to demonstrate that the results are of reasonable quality and to help readers better understand the examples in the paper (just because English is the common ground).  In our experiments 12-layer Transformer (baseline taken from official TF tutorial - https://www.tensorflow.org/tutorials/text/transformer ) achieved 24.65 BLEU after 10 epochs, which is in the top-10 results according to the (see https://paperswithcode.com/sota/machine-translation-on-wmt2014-german-english).\n\n\n> The paper says no beam search was used. Why? \n\nTo make our results more interpretable and fair, we did not introduce any additional hacks and extensions for improving scores of the baseline and our code derived from it.\n\n> The MemBottleneck Skip Transformer is really just a weak form of a typical Transformer.. .So it's not a surprise to me that \"MemBottleneck Skip Transformer 20\" is significantly better than MemBottleneck 10/20. \n\nIt is not so obvious for us, because MemBottleneck without skip connection still might learn to pass representations of sequence tokens without modifications through the layers of the encoder.\n\n >What are the values in Table 3? Are they BLEU scores? Why are they <1?\n\nValues multiplied by 100 will represent BLEU. To be corrected.\n\n>The paper says that \"BLEU score of MemTransformer 10 with 5 [mem] tokens shows it is still able to translate with acceptable quality\". This is false. 7 BLEU score is A LOT of difference. \n\nWe wanted to say that translation still frequently makes some sense (and not just random sequence) even for such low values of BLEU. This will be reformulated.\n\n> How exactly did you use the memory augmentation for a causal/decoder Transformer for the language modeling task in Table 3? \n\nFor language modeling we added [mem] tokens after the sequence for fixed positional embeddings and after the sequence for the relative positional embeddings. When [mem] tokens were prepended at the beginning of the sequence they were masked to prevent reading from the future and can be only used to process content of the [mem] and sequence tokens from the previous segment of Transformer XL input. In the case of appending at the end of the sequence, [mem] tokens were able to append for sequence from previous and current segments as well as [mem] tokens from the previous segment.\n\n> The fact that there's no consistent improvement in Table 5 of the memory-augmented Transformer on the base Transformer is very concerning to me. \n\nThe BERT memory augmentation differed significantly from other experiments, because memory augmentation was performed for an already pre-trained model. So, it seems that fine tuning for memory usage on the small task is not sufficient to have large enough gains in performance.\n\n>So what if, in the intermediate layers of the MemTransformer, you permute the order of the memory slots? \n\nFig. 2d suggests that order of memory slots matters because reading operation is performed over a block of nearby [mem] tokens. So, we think that permutation of memory slots in the intermediate layers should disrupt normal memory processing. But this can be only verified by experiments.\n\n>The other issue I found about the paper is that, although this is an empirical paper, there are still a bunch of not well-supported claims (i.e., handwavy claims) which the authors should have looked into.\n\nMajority of such statements are devoted to possible interpretation of phenomena observed in results of experiments. This is why we tried to frame them as discussion but maybe we're not very successful due to incorrect wording. The purpose of that discussion is to draw readers\u2019 attention to some interesting features of the results but not completely explain them. Indeed, many of them require further research to be transformed from conjectures to solid statements.\n", "title": "Thanks for in depth review and research questions raised"}, "2TdaDWGUP77": {"type": "review", "replyto": "g1KmTQhOhag", "review": "This paper proposes to augment transformer architectures with memory components. The high-level idea is to use multiple special \u201cmem\u201d tokens as additional inputs. Depending how the \u201cmem\u201d tokens\u2019 representations interact with the true input sequence, three variants are studied: (a) in MemTransformer [\u201cmem\u201d, input] attends to [\u201cmem\u201d, input]; (b) MemCtrl is similar, but uses a separately parameterized module to calculate the \u201cmem\u201d representations; (c) in MemBottleneck, \u201cmem\u201d attends to [\u201cmem\u201d, input], and [input] attends only to \u201cmem.\u201d Experiments with machine translation, language modeling, and fine-tuning on the GLUE benchmark are conducted.\n\nOverall the presentation of the paper could be significantly improved. For example, it lays out the technical sections in a way that assumes the readers have hands-on expertise with transformer architectures. Further, the motivation is not clear to me. It seems from the experiments that little practical gains can be expected, since MemTransformer never outperforms the transformer baseline in terms of accuracy or efficiency. This is probably fine if the paper studies a set of clear research problems with well-designed experiments and answers some interesting questions. But it is not the case. Last but not least, the experimental setting is flawed: e.g., the MT experiment cuts training at a suspiciously early stage, presumably contributing to the bad performance. In sum I do not think the paper is ready for ICLR.\n\nPros:\n- Studying the transformers through the memory perspective is interesting.\n\nCon:\n- Writing can be significantly improved.\n- Motivation is not clear: the proposed approach doesn\u2019t seem to bring any interesting practical gain, nor does it answer any interesting research question.\n- Experimental design is flawed, making the conclusions less convincing.\n\nDetailed comments:\n- I\u2019m not against discussing related works in the introduction. But the current version is probably trying to scramble too much stuff into intro, making it tedious to read.\n- I suggest walking through the transformer architecture in a more self-consistent way. I would be very surprised if one without much hands-on experience of transformers can understand what\u2019s going on in section 2.\n- The baselines\u2019 performance in the MT experiments is far worse than what we usually see. I\u2019m guessing this is because the training stops too early. Can the authors explain why they choose to do so? If limited computation is the concern, I recommend the authors to work with smaller datasets instead but use a more convincing settings. Same for other experiments.\n- The experiments do not show any practical gain from MemTransformer. The paper talked about its linear complexity in input sequence length. This paper would be stronger if it can present convincing experiments and show that MemTransformer can have a better efficiency in practice.\n- Section 3.2 would be much more clear and interesting if it opens with the research questions it\u2019s trying to answer.", "title": "Review", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "3glSHz0IRDb": {"type": "review", "replyto": "g1KmTQhOhag", "review": "The authors propose augmenting transformer architectures by adding trainable memory via introducing special memory tokens to the input sequence. They also explore different architecture extensions (memory bottleneck, dedicated layer for memory updates). The authors\u2019 main hypothesis is that adding memory to Transformer-based architectures should result in better model performance. They present BLEU scores on WMTF-14 DE-EN translation and GLUE benchmark results to support that.\n\nPros:\n+ Good qualitative analysis of attention patterns that emerge when memory tokens are added\n+ A practically viable memory extension training scheme that can be used to fine-tune models with added memory tokens\n\nCons:\n- The main claim of the paper is that presence of memory tokens positively correlates with the model performance. At the same time, adding memory tokens increases the model\u2019s capacity, making the comparisons with the baseline unfair. The more rigorous approach would be to compare to a baseline with proportionally increased dimensions as having memory tokens may be akin to having additional space to store sequence token representations. Some of the attention patterns (write-to-memory -> store -> read-from-memory) indicate that this may be the reason for the improved model scores.\n- The use of global tokens (like [CLS] token in Longformer as mentioned in the paper) is a well-known technique that undermines the paper\u2019s novelty.\n- The authors claim that models\u2019 quality positively correlates with the memory size. However, this statement contradicts the findings in Table 1 and Table 5. The authors briefly discuss the lack of trainability in section 3.1. Still, the authors do not specify conditions under which adding more memory will benefit the model quality.\n \nGeneral comments/questions:\n* Memory lesions experiments (table 2). Memory size change during inference seems to lead to drastic drops in model quality. At the same time, the models are not trained to perform in the absence of memory. What if the models are trained on different memory sizes from the beginning with memory size sampled per-batch?\n* Memory extension experiments (table 3). Does the model forget how to work with smaller memory sizes during the memory extension process? E.g., what will memory lesions results look like for the memory extension model?\n* Are any of the models for the GLUE benchmark trained using a memory extension training scheme?\n* It\u2019s not clear if all mem tokens use the same embeddings or if they use separate embeddings (e.g., [mem1], [mem2], etc)\n* MemBottleneck results lack comparison with other extensions that lower the computational cost (O(N), O(N log N) extensions, etc)\n* Relative positional embeddings seem to work better with memory tokens, are there any more experiments with relative positional embeddings except for the ones in table 5?\n* Not all results in Table 1 are symmetrically available for both small and base models. This table lacks the results for MemCtrl Transformer for small models and lacks the results for MemBottleneck Transformer for base models which makes it harder to assess the viability of those architecture extensions. \n* No results for MemBottleneck and MemCtrl in table 5. At the moment, both MemBottleneck and MemCtrl look weak; having more results for those extensions may help.\n* The authors can add FLOPS/params count to tables 1 and 5 to make a comparison of the models' more convenient.\n \nOn rating:\n \nAlthough extending transformer architectures with memory tokens looks like a practically viable way to improve models\u2019 quality, the experiments are not rigorous enough to confirm this is due to the memory mechanism and not because of the increased model capacity, which can be achieved by a simple model scaling. The well-documented use of global tokens in literature also diminishes the novelty of this paper.\n", "title": "Good qualitative analysis of attention patterns on memory tokens but weaker experimental results", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "7b_VWzn-bf": {"type": "review", "replyto": "g1KmTQhOhag", "review": "#### Summary:\nThe paper brought up an interesting limitation of a Transformer network, that information about the context is stored mostly in the same element-wise representation, which might limit the processing of properties related to global information. This work proposed adding memory tokens to store non-local representations and creating memory bottleneck for the global information. The evaluation shows positive results adding memory to a Transformer network.\n\n#### Weakness:\nThe idea is not entirely new; Memory augmented networks have been proposed in previous work and the paper does not differentiate the method from related work very well. \n\n-The paper is not clear about how to obtain the memory input token. According to the paper, the memory input token seems like a static vector; however, a memory augmented network can retrieve a dynamic vector for global information, with the ability to read and write to the memory. \n\n-The naive Mem Transformer Layer is counter intuitive, as there is little differentiation between the memory layer and the sequence layers. How the memory layer contains additional global information is not explained. \n-Figure 1c is confusing, are the arrows crossed or not?\n\n-The formulation of memory representation and sequence representation are exactly the same for the MemCtrl Transformer, which again is very counter intuitive. It is intuitive that we should augment the sequence transformer layer with additional global information, however, we might not want the other way. The memory layers should be updated at coarser granularity. \n\n-The paper is poorly written with many errors in the paper. Please proof read the paper before submission. \n\n#### Detailed feedbacks:\nThe reviewer finds this paper hard to read as there is not a flow of story in the paper. The layout of the paper and the design of experiments seems very arbitrary. \n\n-The motivation for a memory network like proposed in the paper is not clearly stated. How the memory network can capture long-term dependencies and information is not clearly explained. The design of variants of Memory Transformers seems very arbitrary. \n\n-The paper also lacks a more detailed comparison with related work, like Transformer-XL, Star-Transformer, Longformer, ETC, etc.\n\n-\"global\"->''global'' (make sure it looks correct in latex). \n\n-\".Surprisingly\"->\". Surprisingly\"\n\n-Please improve the quality of writing and ask native speakers to proof read the paper.\n", "title": "Idea is not new and empirical results are not strong.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}