{"paper": {"title": "Multi-way Encoding for Robustness to Adversarial Attacks", "authors": ["Donghyun Kim", "Sarah Adel Bargal", "Jianming Zhang", "Stan Sclaroff"], "authorids": ["donhk@bu.edu", "sbargal@bu.edu", "jianmzha@adobe.com", "sclaroff@bu.edu"], "summary": "We demonstrate that by leveraging a multi-way output encoding, rather than the widely used one-hot encoding, we can make deep models more robust to adversarial attacks.", "abstract": "Deep models are state-of-the-art for many computer vision tasks including image classification and object detection. However, it has been shown that deep models are vulnerable to adversarial examples. We highlight how one-hot encoding directly contributes to this vulnerability and propose breaking away from this widely-used, but highly-vulnerable mapping. We demonstrate that by leveraging a different output encoding, multi-way encoding, we can make models more robust. Our approach makes it more difficult for adversaries to find useful gradients for generating adversarial attacks. We present state-of-the-art robustness results for black-box, white-box attacks, and achieve higher clean accuracy on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN when combined with adversarial training. The strength of our approach is also presented in the form of an attack for model watermarking, raising challenges in detecting stolen models.", "keywords": ["Adversarial Defense", "Robustness of Deep Convolutional Networks"]}, "meta": {"decision": "Reject", "comment": "This paper proposes a method for improving robustness to black-box adversarial attacks by replacing the cross-entropy layer with an output vector encoding scheme. The paper is well-written, and the approach appears to be novel. However, Reviewer 4 raises very relevant concerns regarding the experimental evaluation of the method, including (a) lack of robustness without AT in the whitebox case (which is very relevant as we still lack good understanding of blackbox vs whitebox robustness) (b) comparison with Kannan et al and (c) lack of some common strong attacks. Reviewer 1 echoes many of these concerns."}, "review": {"HJg8pq5KJ4": {"type": "rebuttal", "replyto": "SyexE_84yE", "comment": "\n\nWe thank the reviewer for their clarification as we had misinterpreted \u201call possible attacks\u201d in the reviewer\u2019s second comment. \n \nWe agree with the reviewer that evaluating a defense method against all possible attacks under a threat model and reporting the minimum accuracy is a very strong and important setting. Comparisons indicate that our method continues to perform better than state-of-the-art, as we approach the lower bound (minimum). We can further clarify this point in the final version of the paper. \n \nHowever, the goals of a defense method also include making an adversary computationally inefficient. Therefore, a defense that makes low-cost attacks ineffective is also valuable. In terms of the computational cost, the SPSA attacks are computationally much more expensive than the PGD attacks in order to estimate gradients. The SPSA attack involves at most ~2.4 *10^6 -step (batch size = 8192 * iteration = 300) forward passes per image and the n-step PGD attack involves n iterations of backward passes per image. In this work, we demonstrate the benefits of our proposed target encoding, RO, over the conventional 1ofK encoding. This includes added robustness against low-cost attacks over 1ofK encoding. \t", "title": "Response to AnonReviewer4 (Round 3)"}, "HylsrpYNC7": {"type": "rebuttal", "replyto": "HklcygBth7", "comment": "\n\nThank you for your review.\n\n1. and 2. In response to the reviewer\u2019s request, we perform additional experiments with models that we denote as \u2018RO_softmax\u2019 (suggestion 1) and \u20181ofK_MSE\u2019 (suggestion 2). We train \u2018A_RO_softmax\u2019, \u2018A_1ofK_MSE\u2019, \u2018C_RO_softmax\u2019, and \u2018C_1ofK_MSE\u2019 models, and evaluate the models under FGSM attacks from \u20181ofK\u2019 substitute models. We also report Pearson correlation coefficients of the sign of the input gradients between the substitute and the target models as explained in Section 4.1.1. The results are reported in Appendix A of the manuscript. Models with \u2018RO_Softmax\u2019 and \u20181ofK_MSE\u2019 are less robust to attacks from \u20181ofK\u2019 compared to \u2018RO\u2019 and have higher correlations between \u20181ofK\u2019 models.\n\n4. Adversarial robustness on ImageNet is still an open problem.  Even using the conventional 1ofK encoding, it is hard to generalize to clean data when a model is trained with adversarial training [1]. Tsipras et al. adversarially train a model with L-infinity=0.05 on Restricted ImageNet, a smaller subset of ImageNet, and the clean accuracy is decreased by ~70% compared to standard training (without adversarial training). \n\nIn addition, adversarial training on ImageNet is very costly.  Kannan et al. [2] managed to perform adversarial training on ImageNet with 53 GPUs. Due to the computational constraints, most of the papers at ICLR 2018 on adversarial robustness do not experiment with ImageNet.  (e.g., Madry et al. ICLR 2018, Buckman et al ICLR 2018, Na et al. ICLR 2018).\n\nWe would like to emphasize that scaling-up adversarial training for ImageNet is beyond the scope of this work, but we obtain promising generalizability on smaller scale datasets with adversarial training.\n\n[1] Tsipras, Dimitris, et al. \"Robustness may be at odds with accuracy.\" arXiv preprint arXiv:1805.12152 (2018).\n[2] Kannan, Harini, Alexey Kurakin, and Ian Goodfellow. \"Adversarial Logit Pairing.\" arXiv preprint arXiv:1803.06373(2018).\n\n3. We present the mean and standard deviation of five runs here:\n \nMNIST: 93.3 (+/- 0.98), 96.5 (+/- 0.31), 98.8 (+/- 0.12)\nSVHN: 45.2 (+/- 1.30), 56.7 (+/- 0.47), 90 (+/- 0.44)\nCifar100: 21.2 (+/- 0.25), 42.6 (+/- 0.20), 60.0 (+/- 0.27)\nCifar10: 52.4 (+/- 0.33), 65.8 (+/- 0.13), 86.5 (+/- 0.17)\n\n\n\n", "title": "Response to AnonReviewer1"}, "rylyK57E14": {"type": "rebuttal", "replyto": "HJl11lvcAQ", "comment": "\n\nAlthough it is very last minute, we appreciate the interest of the reviewer in our work. As per the reviewer\u2019s request, we ran experiments that utilize the Carlini-Wagner loss and we report the results below in point 7.\n\n\n1. \nWe motivate (in Figure 1) and results demonstrate (in Table 1) that a large output dimensionality is key to the performance of RO. In the appendix, we show that RO_softmax and 1ofK_MSE are not as effective as our RO in decorrelating the gradients (see Table 6, 7), and we reveal a very consistent relationship between the gradient decorrelation and the robustness against black box attacks (see Table 6-10). RO_softmax and 1ofK_MSE only have dimension K=10 (vs. 2000 for RO) in the output layer, which limits their gradient space (see Section 3). Note that increasing the output dimension in RO will not encourage an increase of the norm as the RO vectors are on a unit sphere. The benefit of RO comes from a less constrained gradient space leading to better decorrelation of the gradients.\n\n\n4. \nThe results in Table 2 directly answer this question. We show that our method achieves added robustness on FGSM white-box attacks (untargeted) in Table 2 without adversarial training, as well as higher robustness than vanilla adversarial training on PGD white-box attacks (untargeted) when combined with adversarial training.\n\n\n5 and 6.\nThe 54% accuracy on white-box attacks reported in Table 4 is obtained in the same setting as Madry et al [3], where only 7 iterations are performed for generating the adversarial attacks. The 47.8% accuracy reported in the rebuttal is obtained by increasing the iterations to 1000, which is a much more challenging attack setting than that of Madry et al. We now provide a similar evaluation for Cifar-100 and SVHN on 1000 iterations:\n\n                     MNIST    Cifar-10    Cifar-100    SVHN\nMadry [3]        92.5       45.2           13.4           32.7\nOurs                94.2        47.8           27.5           41.4\n\nThe 49% accuracy we reported in the rebuttal is about a query-based attack [5], and 1000 images are tested, following the setting of [9]. We have run the experiment multiple times and find that the results are consistently performing better than Madry et al [3]. \n\nEven the state-of-the-art defense, Madry et al. [3], achieves 0% classification accuracy on MNIST against the other white-box attacks in [10]. Developing a universal defense method for all types of attacks is an open problem and is out of the scope of our paper. We focus on gradient-based attacks and generate the white-box attacks with the same configuration used in [3] for a fair comparison, showing the improved robustness of our RO method under the same settings.\n\n\n7. \nIn response to the reviewer\u2019s request, we perform additional experiments on white-box attacks. We evaluate our method on (a) PGD attacks from CW loss and (b) Decision-based attacks [11] (which do not exploit knowledge of the model\u2019s loss function).\n\n(a) We use a CW loss which minimizes the distance between an output (y) and the ground-truth (t_i) vector while maximizing the distance between the output and the nearest vector (t_j) to the output (y), where i is not equal to j and t_i and t_j are from the codebook. We use the CW loss to train a model and generate PGD attacks. We measure the classification accuracies on 1000-step PGD white-box attacks, black-box attacks from [3], and clean data of Cifar-10. \n\n\t                          White-box    black-box     clean\nMadry et al. [3]             45.2            64.2              87.3          \nOurs (CW loss)             50.5            69.4              90.9\n\n(b) We use the Foolbox implementation [12] of the Decision-based attacks, and evaluate 1000 samples on MNIST and set a threshold of 1.5 following [10, 11]. Please note that these attacks do not exploit knowledge of the model\u2019s loss function. We obtain a white-box attack accuracy of 42.7%, while Madry et al. obtain an accuracy of 35.4%.\n\nWe found that our model consistently performs better than [3] on the PGD attacks from CW loss and the Decision-based attacks. These attacks do not weaken our claim that RO encoding performs better than 1ofK encoding. \n\n\n\n[3] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. ICLR, 2018.\n[10] Schott, Lukas, et al. \"Towards the first adversarially robust neural network model on MNIST.\" CoRR, abs/1805.09190(2018). (https://arxiv.org/abs/1805.09190)\n[11] Brendel, Wieland, Jonas Rauber, and Matthias Bethge. \"Decision-based adversarial attacks: Reliable attacks against black-box machine learning models.\" ICLR 2018\n[12] Rauber, Jonas, Wieland Brendel, and Matthias Bethge. \"Foolbox v0. 8.0: A python toolbox to benchmark the robustness of machine learning models.\" arXiv preprint arXiv:1707.04131(2017).\n", "title": "Response to AnonReviewer4 (Round 2)"}, "ryxDez-80m": {"type": "rebuttal", "replyto": "rJges_jka7", "comment": "\n\nThank you for your review.\n\nWe agree that optimizing the output encoding to maximize robustness to attack is not sufficiently explored in deep models and is an interesting future direction for our research.\n", "title": "Response to AnonReviewer3"}, "BklQfl94RQ": {"type": "rebuttal", "replyto": "SylVL979pQ", "comment": "\n\nThank you for your review.\n\nLike [1] and [2], we did not consider the threat model where an adversary has the ability to send queries to the target model. Since our submission shows the vulnerability between models when the same 1ofK encoding is used and introduces a new output encoding, the black-box threat model in our paper is designed to test whether it is useful to hide the output encoding (codebook) from the adversary. To be specific, our experiments are designed to test the efficacy of our proposed encoding when the adversary assumes the conventional 1ofK encoding is used. Therefore, we are more interested in how the black-box attacks from a model with 1ofk encoding are transferable to the model with our proposed encoding. In this context, we also present the watermarking attack that further demonstrates how adversarial examples are transferable when different output encodings are used.\n\nIn response to the reviewer\u2019s request, we test query-based attacks. Since we use a different output encoding and loss function, the existing implementations of SPSA and the Decision Attack cannot be directly applied to our model for technical reasons (library wrappers). Considering the time we need to modify the implementation and validate the code, we instead use another publicly available query-based attack [3], which are more powerful than [5, 6]. Due to the time complexity (at most 1.2x10^5 forward passes per image), we tried the first 1000 images in the test set for Cifar-10. Please note that [4] also perform their attacks on 1000 images of Cifar-10 for evaluation. We obtain an accuracy of 49.10%, while the pre-trained model of [1] obtain an accuracy of 45.6%. \n\n[1] Madry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" ICLR 2018.\n\n[2] Kannan, Harini, Alexey Kurakin, and Ian Goodfellow. \"Adversarial Logit Pairing.\" arXiv preprint arXiv:1803.06373(2018).\n\n[3] NATTACK: A Strong and Universal Gaussian Black-Box Adversarial Attack, ICLR 2019 submission.\n\n[4] Uesato, Jonathan, et al. \"Adversarial risk and the dangers of evaluating against weak attacks.\" arXiv preprint arXiv:1802.05666 (2018).\n\n[5] Athalye, Anish, Nicholas Carlini, and David Wagner. \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.\" arXiv preprint arXiv:1802.00420(2018).\n\n[6] Ilyas, Andrew, et al. \"Black-box Adversarial Attacks with Limited Queries and Information.\" arXiv preprint arXiv:1804.08598(2018).\n", "title": "Response"}, "HJx1u75VAm": {"type": "rebuttal", "replyto": "HJgz2qXQ0X", "comment": "\n\n5. In response to the reviewer\u2019s request, we test query-based attacks. Since we use a different output encoding and loss function, the existing implementations of SPSA and the Decision Attack cannot be directly applied to our model for technical reasons (library wrappers). Considering the time we need to modify the implementation and validate the code, we instead use another publicly available query-based attack [5], which are more powerful than [2, 6]. Due to the time complexity (at most 1.2x10^5 forward passes per image), we tried the first 1000 images in the test set for Cifar-10. Please note that [9] also perform their attacks on 1000 images of Cifar-10 for evaluation. We obtain an accuracy of 49.10%, while the pre-trained model of [3] obtain an accuracy of 45.6%. \n\nWe also demonstrate robustness when we increase the number of iterations used to generate PGD white-box attacks to 100 and 1000. Here, we report our classification accuracy and compare it to results from the publicly released model of Madry et al. [3]. For MNIST, we obtain an accuracy of 94.47% and 94.21%, while Madry et al. [3] obtain an accuracy of 92.53% and 92.45% at 100 and 1000 iterations, respectively. For Cifar-10, we obtain an accuracy of 47.94% and 47.80%, while Madry et al. obtain an accuracy of 45.35% and 45.23% at 100 and 1000 iterations, respectively. We observe that the increase in the order of magnitude of the number of iterations used to generate the attack does not significantly impact our classification accuracy, and maintains a higher accuracy compared to Madry et al. [3]. The results we report in the submission use 7 iterations for Cifar-10 and 40 iterations for MNIST, which was also used in reporting the results in Madry et al. [3], Buckman et al. [7], and Kannan et al. [8]. \n\nIn addition, we validate that iterative attacks perform better than one-step attacks, white-box attacks are more powerful than black-box attacks, and unbounded attacks achieve 100% success, all of which are desirable properties of defenses in regard to obfuscated gradients [2].\n\n6. At the time of submission, the ALP [8] paper had not been retracted yet. We do not know clear reasons why they retracted the paper but ALP increases robustness against gradient-based white-box attacks over [3]. The work mentioned by the reviewer (https://openreview.net/forum?id=Bylj6oC5K7) shows that ALP achieves higher accuracy against 1000-step PGD attacks compared to [3] (ALP:48.34% vs. Madry et al: 45.15%). In addition, Mosbach et al. [4] also show added robustness for ALP against gradient-based attacks over [3]. Our evaluation is consistent with these observations.\n\nAside from ALP, as stated in our original manuscript, our method performs better than [3]. We are happy to omit the comparison to ALP if it is questionable. \n\n7. We use the same loss function, Mean Squared Error loss, for both training a model and generating adversarial examples following works using gradient-based attacks [3,7,8]. \n\n[1] Carlini, Nicholas, and David Wagner. \"Defensive distillation is not robust to adversarial examples.\" arXiv preprint arXiv:1607.04311 (2016).\n\n[2] Athalye, Anish, Nicholas Carlini, and David Wagner. \"Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples.\" arXiv preprint arXiv:1802.00420(2018).\n\n[3] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. ICLR, 2018.\n\n[4] Mosbach, Marius, et al. \"Logit Pairing Methods Can Fool Gradient-Based Attacks.\" arXiv preprint arXiv:1810.12042(2018).\n\n[5] NATTACK: A Strong and Universal Gaussian Black-Box Adversarial Attack, ICLR 2019 submission.\n\n[6] Ilyas, Andrew, et al. \"Black-box Adversarial Attacks with Limited Queries and Information.\" arXiv preprint arXiv:1804.08598(2018).\n\n[7] Buckman, J., Roy, A., Raffel, C., and Goodfellow, I. Thermometer encoding: One hot way to resist adversarial examples. ICLR, 2018.\n\n[8] Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial Logit Pairing. arXiv preprint arXiv:1803.06373.\n\n[9] Uesato, Jonathan, et al. \"Adversarial risk and the dangers of evaluating against weak attacks.\" arXiv preprint arXiv:1802.05666 (2018).\n", "title": "Response to AnonReviewer4 (Part 2)"}, "rJgXI7cVCX": {"type": "rebuttal", "replyto": "HJgz2qXQ0X", "comment": "\n\nThank you for your review.\n\n1. In response to the reviewer, we measure the correlation of gradients between all convolutional layers of the different models. We first compute the gradients of the loss with respect to intermediate features of Conv1 and Conv2. Then, we compute the Pearson correlation coefficient of the sign of the gradients with respect to such intermediate features between models. For further comparison, we train models \u2018A\u2019_1ofK\u2019 and \u2018A\u2019_RO\u2019, that are independently initialized from \u2018A_1ofK\u2019 and \u2018A_RO\u2019. This correlation analysis is reported in Appendix B of the manuscript. We find that the correlations of Conv1 and Conv2 between \u20181ofK\u2019 models are much higher than those of \u2018RO\u2019 models. In addition, even though \u2018RO\u2019 models used the same output encoding, they are not highly correlated. We also find that the correlations between \u2018RO\u2019 and \u20181ofK\u2019 are low. \n\nIn response to the reviewer\u2019s request, we perform additional experiments with a model that we denote as \u2018RO_softmax\u2019. We train \u2018A_RO_softmax\u2019, \u2018C_RO_softmax\u2019 models, and evaluate the models under FGSM attacks from \u20181ofK\u2019 substitute models. We also report Pearson correlation coefficients of the sign of the input gradients between the substitute and the target models as explained in Section 4.1.1. The results are reported in Appendix A of the manuscript. Models with \u2018RO_Softmax\u2019 and \u20181ofK_MSE\u2019 are less robust to attacks from \u20181ofK\u2019 compared to \u2018RO\u2019 and have higher correlations between \u20181ofK\u2019 models.\n\n2. Using a higher dimension (>K) results in a less constrained gradient space compared to that of 1ofK encoding. [1] addresses the importance of the norm in the input to the softmax layer (Section 3 of [1]) so that the norm has a massive impact on the softmax output. However, we do not use a softmax and our loss is determined by the relative difference between the output and the ground-truth vector which is not related to the norm of the output. Therefore the explanation in [1] is not directly applicable to our setup.\n\n3. We demonstrate watermarking evasion to further demonstrate that our proposed approach decorrelates model predictions on watermarked images. We interpret a watermarked image used to deliberately cause a misclassification as an adversarial example. *When the encoding of the substitute and target models is different, adversarial examples become less transferable.*\n\n4. Table 3 was intended to only show the results of Black-box attacks without adversarial training. The purpose of presenting the White-box (1ofK) result is to show how transferable Black-box attacks (1ofK) are for reference. We remove the row of White-box (1ofK), together with its referenced text, in order to avoid confusion. For the White-box (RO) without adversarial training, we achieve 0.1% classification accuracy on untargeted attacks and 74.7% attack success rate on targeted attacks for MNIST. Without adversarial training, the RO model is still vulnerable to untargeted attacks and shows improvements on targeted attacks. This indicates that our model does not break gradient descent. Considering the space constraints, we decided to instead demonstrate the main argument: RO gives better performance than 1ofK under White-box attacks with adversarial training as reported in Table 4. \n", "title": "Response to AnonReviewer4 (Part 1)"}, "rklWcg9EAQ": {"type": "rebuttal", "replyto": "BJg8CsX5aQ", "comment": "\n\nThank you for your review.\n\n1. This can be any loss function that measures a distance between the vectors. We use the Mean Squared Error Loss as described in the paper.\n\n2. t_RO is the RO ground-truth vector for a class in the C_RO codebook.\n\n3. \\beta is empirically determined to be 1000 as stated in Section 4.1. \n\n4. Yes, we are working on releasing the code.", "title": "Response"}, "B1gVc3FEAm": {"type": "rebuttal", "replyto": "SJggFnGg2m", "comment": "\n\nThank you for your review.\n\nTable 3 was intended to only show the results of Black-box attacks without adversarial training. The purpose of presenting the White-box (1ofK) result is to show how transferable Black-box attacks (1ofK) are for reference. We remove the row of White-box (1ofK), together with its referenced text, in order to avoid confusion.\n \nFor the White-box (RO) without adversarial training, we achieve 0.1% classification accuracy on untargeted attacks and 74.7% attack success rate on targeted attacks for MNIST. Without adversarial training, the RO model is still vulnerable to untargeted attacks and shows improvements on targeted attacks. However, the main argument is that RO gives better performance than 1ofK under White-box attacks with adversarial training as reported in Table 4.\n", "title": "Response"}, "H1eM2aF4RQ": {"type": "rebuttal", "replyto": "B1xpAhx63m", "comment": "\n\nThank you for your review.\n\nIn response to the reviewer, we measure the correlation of gradients between all convolutional layers of the different models. We first compute the gradients of the loss with respect to intermediate features after Conv1 and Conv2. Then, we compute the Pearson correlation coefficient of the sign of the gradients with respect to such intermediate features between models. For further comparison, we train models \u2018A\u2019_1ofK\u2019 and \u2018A\u2019_RO\u2019, that are independently initialized from \u2018A_1ofK\u2019 and \u2018A_RO\u2019. This correlation analysis is reported in Appendix B of the manuscript. We find that the correlations of Conv1 and Conv2 between \u20181ofK\u2019 models are much higher than those of \u2018RO\u2019 models. In addition, even though \u2018RO\u2019 models used the same output encoding, they are not highly correlated. We also find that the correlations between \u2018RO\u2019 and \u20181ofK\u2019 are low. \n", "title": "Response to AnonReviewer2"}, "HJgz2qXQ0X": {"type": "review", "replyto": "B1xOYoA5tQ", "review": "This paper argues that the vulnerability of classifiers to (black-box) adversarial attacks stems from the use of a final cross-entropy layer trained on one-hot labels. The authors propose replacing this layer by encoding each label as a high-dimensional vector and then training the classifier to minimize the L2 distance of the classifier output from the encoding of the correct class. While the approach is interesting and the paper well-written, both the motivation and the experimental evaluation is insufficient. Hence I consider it below the ICLR bar.\n\nI find the approach weakly motivated. The argument in Figure 1 is very hand-wavy with no clear experimental or theoretical support. The authors argue that cross-entropy with one hot labels causes gradient correlation in the last layer and this propagates all the way through the network (bottom of page 3) but there are no experiments supporting this conjecture.\nMoreover, the approach is not fundamentally different from standard networks with cross-entropy training. One can consider adding an extra layer (with number of neurons equal to the encoding vector dimensions) and keeping the weights of these neurons fixed (the output weights are essentially the encoding dictionary). Then training with cross-entropy is increasing the inner product with these vectors. This is qualitatively very similar to the proposed approach of this paper. Is there a benefit from explicitly considering the encoding vectors? \n\nMoreover, why is the length of the encoding vectors important from a conceptual point of view? As far as I can tell, this is simply encouraging the output of the network to be large in norm. This could be leading to gradient masking, similar to the phenomena observed for defensive distillation.\n\nI find the proposed approach to watermark evasion interesting. However I consider it orthogonal to the rest of the results so it is hard to consider it as a contribution to the main point of the paper.\n\nFigure 3 is missing white-box evaluation of RO classifiers. Is this on purpose? It is important to understand if the claimed improvement in robustness actually stems from RO rather than mostly from combining it with adversarial training.\n\nThe authors report an increase in white-box adversarial robustness. However, I don't believe that the evaluation of their method is thorough. There are plenty of examples by now where PGD has not been sufficient to evaluate the ground-truth robustness of a model. This can distort the relative robustness of different approaches. Given that the increase from baselines in white-box robustness is relatively small (<10% for most datasets) a much more thorough evaluation is required to conclusively demonstrate the benefit of this method. For instance, applying the SPSA attack from Uesato et al. (2018, https://arxiv.org/abs/1802.05666) or a variant of the CW (https://arxiv.org/abs/1608.04644) attack adapted to the particular method used.\nAs an additional point of concern emphasizing this issue, the authors present the results of Kannan et al. (2018) as state-of-the-art. So far, there is no conclusive evidence about ALP improving the robustness of neural networks beyond adversarial training. The original paper was found to be not as robust as claimed and retracted from NIPS. A similar paper reporting ALP to improve robustness in smaller datasets (CIFAR10) was submitted to ICLR (https://openreview.net/forum?id=Bylj6oC5K7) but was withdrawn after the authors performed additional experiments. The fact that the authors find the approach of Kannan et al. (2018) to offer an increase over the robustness of Madry et al. (2017) thus raises concerns about the reliability of the evaluation.\n\nOther comments:\n-- When the authors perform PGD, what is exactly the loss it is applied on? Is it clear that this is the optimal loss to use when attacking RO classifiers?", "title": "An interesting approach but insufficient evaluation and motivation", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJges_jka7": {"type": "review", "replyto": "B1xOYoA5tQ", "review": "Authors proposes new method against adversarial attacks. Paper is organized well and easy to follow. Basically, authors notice that gradients of a deep neural network when one hot encoding is used can be highly correlated and hence can be used in the design on an attack. Authors supply extensive experimental evidence to support their method. Those experiments shows significant amount of gains compared to baselines. Although proposed method is neat, I believe it has room to be improved. One question which is bothering me is: Given that one hot encoding is not optimal, can one find optimal (highly resistant to any attack) encoding? One may use evolutionary computing to empirically analyse such a encoding or one may come up an existence/non-existence proof (I am not expert in the field however I guess ecoc field should have investigates similar problems) of such encoding. ", "title": "Review of Multi-way Encoding for Robustness to Adversarial Attacks  ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B1xpAhx63m": {"type": "review", "replyto": "B1xOYoA5tQ", "review": "This paper argues that a random orthogonal output vector encoding is more robust to adversarial attacks than the ubiquitous softmax. The reasoning is as follows:\n\n1. different models that share the same final softmax layer will have highly correlated gradients in this final layer\n2. this correlation can be carried all the way back to the input pertubations\n3. the use of a multi-way encoding results in a weaker correlation in gradients between models\n\nI found (2) to be a surprising assumption, but it does seem to be supported by the experiments. These show a lower correlation in input gradients between models when using the proposed RO encoding. They also show an increased resiliance to attack in a number of different settings. \n\nOverall, the results seem to be impressive. However, I think the paper would be a lot stronger if there was a more thorough investigation of the correlation between gradients in all layers of the models. I did not find the discussion around Figure 1 to be very compelling, since it is only relevant to the encoding layer, while we are only interested in gradients at the input layer. The correlation numbers in Table 2 are unexpected and interesting. I would like to see a deeper investigation of these correlations.\n\nI am not familiar with the broader literature in this area, so giving myself low confidence.\n", "title": "Novel approach to classification for resiliance against adversial attacks, supported by multiple experiments.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "HklcygBth7": {"type": "review", "replyto": "B1xOYoA5tQ", "review": "This work proposes an alternative loss function to train models robust to adversarial attacks. Specifically, instead of the common sparse, N-way softmax-crossentropy loss, they propose to minimize the MSE to the target column of a random, dense orthogonal matrix. I believe the high-level idea behind this work is that changing the target labelspace is a more effective means of defending against adversarial attacks than modifying the underlying architecture, as the loss-level gradients will be strongly correlated across all architectures in the latter scenario.\n\nPros:\n-Paper was easy to follow\n-Using orthogonal encodings to decorrelate gradients is an interesting idea\n-Benchmark results appear promising compared to prior works\n\nCons:\n-This work claims that their RO-formulation is fundamentally different from 1-of-K, but I'm not completely sure that's true. One could train a classification model where the final fully connected layer (C inputs K output logits) were a frozen matrix (updates disabled) of K orthogonal basis vector (ie, the same as the C_{RO}) codebook they propose. The inputs to this layer would probably have to be L2 normalized, and the output logits would then proceed through a softmax-crossentropy layer. Would this be any less effective than the proposed scheme?\n-Another baseline/sanity test that should probably included is how does the 1-of-K softmax/cross-entropy compare with the proposed method where encoding length l = k and the C_{RO} codebook is just the identity matrix? \n-Some of the numbers in Table 4 are pretty close. Since the authors are replicating Kannan et al, it would be best to included error bars when possible to account for differences in random initializations.\n-It is unclear the extent to which better classification performance on the clean input generalizes to datasets such as ImageNet\n\nOverall, I think the results are promising, but I'm not fully convinced that similar results cannot be achieved using standard cross-entropy losses with 1-hot labels.", "title": "Promising results, but could use some more experiments", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "Skx3FZMps7": {"type": "rebuttal", "replyto": "BJl1qeldjm", "comment": "As suggested, we set the number of iterations used to generate PGD white-box attacks to 100 and 1000. Here, we report our classification accuracy and compare it to results from the publicly released model of Madry et al. [1]. For MNIST, we obtain an accuracy of 94.47% and 94.21%, while Madry et al. [1] obtain an accuracy of 92.53% and 92.45% at 100 and 1000 iterations, respectively. For Cifar-10, we obtain an accuracy of 47.94% and 47.80%, while Madry et al. obtain an accuracy of 45.35% and 45.23% at 100 and 1000 iterations, respectively. We observe that the increase in the order of magnitude of the number of iterations used to generate the attack does not significantly impact our classification accuracy, and maintains a higher accuracy compared to Madry et al. [1]. The results we report in the submission use 7 iterations for Cifar-10 and 40 iterations for MNIST, which was also used in reporting the results in Madry et al. [1], Buckman et al. [2], and Kannan et al. [3]. \n\n[1] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. ICLR, 2018.\n\n[2] Buckman, J., Roy, A., Raffel, C., and Goodfellow, I. Thermometer encoding: One hot way to resist adversarial examples. ICLR, 2018.\n\n[3] Kannan, H., Kurakin, A., and Goodfellow, I. Adversarial Logit Pairing. arXiv preprint arXiv:1803.06373.", "title": "Results"}}}