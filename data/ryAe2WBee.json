{"paper": {"title": "Multi-label learning with semantic embeddings", "authors": ["Liping Jing", "MiaoMiao Cheng", "Liu Yang", "Alex Gittens", "Michael W. Mahoney"], "authorids": ["lpjing@bjtu.edu.cn", "15112085@bjtu.edu.cn", "11112191@bjtu.edu.cn", "gittens@icsi.berkeley.edu", "mmahoney@stat.berkeley.edu"], "summary": "The SEM approach to multi-label learning models labels using multinomial distributions parametrized by nonlinear functions of the instance features, is scalable and outperforms current state-of-the-art algorithms", "abstract": "Multi-label learning aims to automatically assign to an instance (e.g., an image or a document) the most relevant subset of labels from a large set of possible labels. The main challenge is to maintain accurate predictions while scaling efficiently on data sets with extremely large label sets and many training data points. We propose a simple but effective neural net approach, the Semantic Embedding Model (SEM), that models the labels for an instance as draws from a multinomial distribution parametrized by nonlinear functions of the instance features. A Gauss-Siedel mini-batch adaptive gradient descent algorithm is used to fit the model. To handle extremely large label sets, we propose and experimentally validate the efficacy of fitting randomly chosen marginal label distributions. Experimental results on eight real-world data sets show that SEM garners significant performance gains over existing methods. In particular, we compare SEM to four recent state-of-the-art algorithms (NNML, BMLPL, REmbed, and SLEEC) and find that SEM uniformly outperforms these algorithms in several widely used evaluation metrics while requiring significantly less training time.\n", "keywords": ["Supervised Learning"]}, "meta": {"decision": "Reject", "comment": "This is largely a well written paper proposing a sensible approach for multilabel learning that is shown to be effective in practice. However, the main technical elements of this work: the model used and its connections to basic MLPs and related methods in the literature, the optimization strategy, and the speedup tricks are all familiar from prior work. Hence the reviewers are somewhat unanimous in their view that the novelty aspect of this paper is its main shortcomings. The authors are encouraged to revise the paper and clarify the precise contributions."}, "review": {"B1TxXz1Ee": {"type": "rebuttal", "replyto": "ryODwThGl", "comment": "Eq.(11) is used to calculate the probability that the current instance is assigned the j-th label. By ranking the probabilities along all labels, we can select the top labels for the current instance. Several methods have been proposed in the literature for choosing an appropriate number of labels to predict, but due to space considerations and because that question is orthogonal to the question of ranking the labels, to compare the baselines and our methods, we use the known number of true labels to select the appropriate number of predicted labels in all methods.\n\nJean et al\u2019s paper was published in ACL 2015 for the problem of machine translation, where the input and output neurons represent words from different languages. Similar to our marginalization method, Jean et al. only consider a subset of target words for each source word, but the sampling strategies are different. In SEM, we sample the subset of negative labels for each instance with probability proportional to their frequency of occurrence in all training data, which can be easily and efficiently obtained. However, the approach of Jean et al re-builds a target vocabulary for each source sentence or builds a common candidate list for multiple source sentences, which is time-consuming.\n\nIn experimental sections 4.3 (small label sets) and 4.4 (large label sets), we discuss the choice of the main parameter, r, the latent space dimensionality; a longer discussion of the impact of the choice of r is given in the supplementary material (section A). Throughout we use a minibatch size of 200; for small label sets we use 30 epochs of training, and for large label sets we use 10 epochs of training. These values were selected because they make training efficient across a range of data sets.", "title": "response"}, "S1XCCWJNl": {"type": "rebuttal", "replyto": "S1RmDv17g", "comment": "We will correct the taxonomy of linear vs non-linear as you pointed out. \n\nNNML, a previous deep neural network approach showed that, if training time is disregarded, then neural networks can outperform linear algebraic approaches for multi-label learning. Unfortunately, this approach does not apply to large data sets in time comparable to the state-of-the-art linear algebraic approaches. Similarly, ADIOS shows that deep learning can outperform linear approaches, but no comparison of the scalability of ADIOS vs that of the baseline methods is given. In general, it is not hard to believe that as the reviewer suggests, deep nonlinear networks should outperform the current state-of-the-art methods used for multi-label learning. However, the baseline methods were carefully designed to provide good performance with fast training, precisely because the training time limits the size of the data sets to which multi-label learning methods can be fruitfully applied.\n\nThe key point of our paper is that we provided a neural network approach that beats state-of-the-art multi-label learning methods both in terms of performance metrics *and* training time, so can be used in practice on truly large data sets. The question of whether deep learning can reach a similarly competitive ratio of performance and speed is open.", "title": "response"}, "r1v8ibyVl": {"type": "rebuttal", "replyto": "HkiQ9qk7g", "comment": "As demonstrated in the paper introducing SLEEC (Bhatia et al. 2015), FastXML (Prabhu & Varma, 2014), etc., extreme multi-label classification data sets exhibit positive label sparsity in that each data point has only a few positive labels associated with it. We believe the P@K metric is a practically important metric for comparison of the methods as it focuses on the accurate prediction of the few positive labels per data point rather than the vast number of negative labels.\n\nWe mention several principal label space transformation methods in the introduction, but due to space constraints, we could not compare to all methods that have been proposed for large-scale multi-label learning. In Section 4, we explain our choice to omit comparisons to ML-CSSP (Bi & Kwok, 2013) and CPLST (Chen & Lin, 2012), which is a principal label space transformation method: prior work has shown SLEEC has better performance (as mentioned by the reviewer, in the P@K metric). Further, the authors of SLEEC argue that \u201ctechniques such as CS, CPLST, ML-CSSP, 1-vs- All could only be trained on the small data sets given standard resources.\u201d\n\nFor the baseline NN model, NNML (Nam et al. 2014), we used the codes provided by the authors, which do use AdaGrad.\n\nThanks for pointing us toward ADIOS. We note that ADIOS does not provide timing results, and uses deep learning. The contribution of our work is proof that shallow neural networks can outperform state-of-the-art linear algebraic methods both in terms of performance metrics as well as, counterintuitively, training time on large multi-label learning problems. It seems that ADIOS outperforms the baseline methods in terms of performance metrics, but timing results are not shown (and it is reasonable to expect that one pays a large cost in terms of training time for this performance). It is future work to determine to what extent ADIOS or other deep-learning methods can maintain a performance gap over baseline methods while having competitive learning times. ", "title": "response"}, "HkiQ9qk7g": {"type": "review", "replyto": "ryAe2WBee", "review": "Principle labels space transformation and ML-CSSP are valid competitors even though the authors decided to omit them. Previous work (\"SLEEC\", \"FastXML\") is better than these approaches only on P@K measure, not on MacroF measure. The ICML 2016 paper \"ADIOS: Architectures Deep in Output Space\" is a also not cited.\n\nAlso, did you train the baseline NN model with adagrad?\nThe paper proposes a semantic embedding based approach to multilabel classification. \nConversely to previous proposals, SEM considers the underlying parameters determining the\nobserved labels are low-rank rather than that the observed label matrix is itself low-rank. \nHowever, It is not clear to what extent the difference between the two assumptions is significant\n\nSEM models the labels for an instance as draws from a multinomial distribution\nparametrized by nonlinear functions of the instance features. As such, it is a neural network.\nThe proposed training algorithm is slightly more complicated than vanilla backprop.  The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear. \n\nThe paper is well written and the main idea is clearly presented. However, the experimental results are not significant enough to compensate the lack of conceptual novelty. \n\n\n", "title": "prior work and experiments.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJfuJGINx": {"type": "review", "replyto": "ryAe2WBee", "review": "Principle labels space transformation and ML-CSSP are valid competitors even though the authors decided to omit them. Previous work (\"SLEEC\", \"FastXML\") is better than these approaches only on P@K measure, not on MacroF measure. The ICML 2016 paper \"ADIOS: Architectures Deep in Output Space\" is a also not cited.\n\nAlso, did you train the baseline NN model with adagrad?\nThe paper proposes a semantic embedding based approach to multilabel classification. \nConversely to previous proposals, SEM considers the underlying parameters determining the\nobserved labels are low-rank rather than that the observed label matrix is itself low-rank. \nHowever, It is not clear to what extent the difference between the two assumptions is significant\n\nSEM models the labels for an instance as draws from a multinomial distribution\nparametrized by nonlinear functions of the instance features. As such, it is a neural network.\nThe proposed training algorithm is slightly more complicated than vanilla backprop.  The significance of the results compared to NNML (in particular on large datasets Delicious and EUrlex) is not very clear. \n\nThe paper is well written and the main idea is clearly presented. However, the experimental results are not significant enough to compensate the lack of conceptual novelty. \n\n\n", "title": "prior work and experiments.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1RmDv17g": {"type": "review", "replyto": "ryAe2WBee", "review": "Re the prior work description, why are approaches such as Tai & Lin (2010) and Lin et al (2014), which use PCA, not considered to assume a low rank? The performance of PCA-based algorithms often depends on the eigenvalues decaying rather quickly, which is mostly equivalent to a low-rank assumption.\n\nSimilarly, why kernelize the output of SEM instead of using a deeper nonlinear network to produce the predictions to be used?This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network. This in and of itself is not novel, nor is the idea of optimizing this by adagrad. Though it's weird that the paper explicitly derives the gradient and suggests doing alternating adagrad steps instead of the more standard adagrad steps; it's unclear whether this matters at all for performance. The main trick responsible for increasing the efficiency of this model is the candidate label sampling, which is done in a relatively standard way by sampling labels proportionally to their frequency in the dataset.\n\nGiven that neither the model nor the training strategy is novel, it's surprising that the results are better than the state-of-the-art in quality and efficiency (though non-asymptotic efficiency claims are always questionable since implementation effort trades off fairly well against performance). I feel like this paper doesn't quite meet the bar.", "title": "Questions", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJ20qUlVx": {"type": "review", "replyto": "ryAe2WBee", "review": "Re the prior work description, why are approaches such as Tai & Lin (2010) and Lin et al (2014), which use PCA, not considered to assume a low rank? The performance of PCA-based algorithms often depends on the eigenvalues decaying rather quickly, which is mostly equivalent to a low-rank assumption.\n\nSimilarly, why kernelize the output of SEM instead of using a deeper nonlinear network to produce the predictions to be used?This paper proposes SEM, a simple large-size multilabel learning algorithm which models the probability of each label as softmax(sigmoid(W^T X) + b), so a one-layer hidden network. This in and of itself is not novel, nor is the idea of optimizing this by adagrad. Though it's weird that the paper explicitly derives the gradient and suggests doing alternating adagrad steps instead of the more standard adagrad steps; it's unclear whether this matters at all for performance. The main trick responsible for increasing the efficiency of this model is the candidate label sampling, which is done in a relatively standard way by sampling labels proportionally to their frequency in the dataset.\n\nGiven that neither the model nor the training strategy is novel, it's surprising that the results are better than the state-of-the-art in quality and efficiency (though non-asymptotic efficiency claims are always questionable since implementation effort trades off fairly well against performance). I feel like this paper doesn't quite meet the bar.", "title": "Questions", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryODwThGl": {"type": "review", "replyto": "ryAe2WBee", "review": "- How different is the proposed marginal fitting algorithm with [Jean et al, arxiv 1412.2007]?\n- When using for prediction, it seems eq 11 would produce 1 label on average per example; what if the number of labels is higher in expectaction?\n- How were the hyper-parameters chosen for the proposed method (no validation set?)\nThe paper presents the semantic embedding model for multi-label prediction.\nIn my questions, I pointed that the proposed approach assumes the number of labels to predict is known, and the authors said this was an orthogonal question, although I don't think it is!\nI was trying to understand how different is SEM from a basic MLP with softmax output which would be trained with a two step approach instead of stochastic gradient descent. It seems reasonable given their similarity to compare to this very basic baseline.\nRegarding the sampling strategy to estimate the posterior distribution, and the difference with Jean et al, I agree it is slightly different but I think you should definitely refer to it and point to the differences.\nOne last question: why is it called \"semantic\" embeddings? usually this term is used to show some semantic meaning between trained embeddings, but this doesn't seem to appear in this paper.\n", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyqLAIg4x": {"type": "review", "replyto": "ryAe2WBee", "review": "- How different is the proposed marginal fitting algorithm with [Jean et al, arxiv 1412.2007]?\n- When using for prediction, it seems eq 11 would produce 1 label on average per example; what if the number of labels is higher in expectaction?\n- How were the hyper-parameters chosen for the proposed method (no validation set?)\nThe paper presents the semantic embedding model for multi-label prediction.\nIn my questions, I pointed that the proposed approach assumes the number of labels to predict is known, and the authors said this was an orthogonal question, although I don't think it is!\nI was trying to understand how different is SEM from a basic MLP with softmax output which would be trained with a two step approach instead of stochastic gradient descent. It seems reasonable given their similarity to compare to this very basic baseline.\nRegarding the sampling strategy to estimate the posterior distribution, and the difference with Jean et al, I agree it is slightly different but I think you should definitely refer to it and point to the differences.\nOne last question: why is it called \"semantic\" embeddings? usually this term is used to show some semantic meaning between trained embeddings, but this doesn't seem to appear in this paper.\n", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}