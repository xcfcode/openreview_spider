{"paper": {"title": "Latent Variables on Spheres for Sampling and Inference", "authors": ["Deli Zhao", "Jiapeng Zhu", "Bo Zhang"], "authorids": ["zhaodeli@gmail.com", "jengzhu0@gmail.com", "zhangbo@xiaomi.com"], "summary": "", "abstract": "Variational inference is a fundamental problem in Variational AutoEncoder (VAE). The optimization with lower bound of marginal log-likelihood results in the distribution of latent variables approximate to a given prior probability, which is the dilemma of employing VAE to solve real-world problems. By virtue of high-dimensional geometry, we propose a very simple algorithm completely different from existing ones to alleviate the variational inference in VAE. We analyze the unique characteristics of random variables on spheres in high dimensions and prove that Wasserstein distance between two arbitrary data sets randomly drawn from a sphere are nearly identical when the dimension is sufficiently large. Based on our theory, a novel algorithm for distribution-robust sampling is devised. Moreover, we reform the latent space of VAE by constraining latent variables on the sphere, thus freeing VAE from the approximate optimization of posterior probability via variational inference. The new algorithm is named Spherical AutoEncoder (SAE). Extensive experiments by sampling and inference tasks validate our theoretical analysis and the superiority of SAE.", "keywords": ["variational autoencoder", "generative adversarial network"]}, "meta": {"decision": "Reject", "comment": "This paper proposes to improve VAE/GAN by performing variational inference with a constraint that the latent variables lie on a sphere. The reviewers find some technical issues with the paper (R3's comment regarding theorem 3). They also found that the method is not motivated well, and the paper is not convincing. Based on this feedback, I recommend to reject the paper."}, "review": {"BJl_-qJEiH": {"type": "rebuttal", "replyto": "H1e_Hdw6tr", "comment": "\nQ1: \u201cAn important claim in this paper is that the proposed approach \u201calleviates variational inference in VAE\u201d. However, this requires clarification as well as theoretical/empirical justifications\u201d\nQ2: \u201cMoreover, why and how would Theorem 2 justify improved inference when projecting latent samples onto a hypersphere?\u201d\nA1 and A2: These two questions and related comments might be due to our inappropriate use of \u201calleviate\u201d and the extensive meaning of inference beyond probability. Actually, there is no posterior inference and any priors involved in our SAE algorithm. It is the vanilla autoencoder subject to the spherical constraint shown in equation (10). So we said \u201cthus freeing VAE from the approximate optimization of posterior probability via variational inference\u201d and \u201cOur algorithm is geometric and free from posterior probability optimization\u201d. Indeed, \u201calleviates variational inference in VAE\u201d is an inappropriate use in this scenario. We will correct this in the revised version. \n\nBesides, we use \u201cinference\u201d to refer to inferring (obtaining) z from the encoder, not only for \u201cvariational\u201d inference or \u201cprobabilistic\u201d inference. This might cause misunderstanding with habitual thinking in this field. This misunderstanding might be avoided by using \u201cgeometric inference\u201d. We will note this meaning clearly in the revised version.\n\n\nQ3: \u201cHowever, in VAE we do not expect the posterior to match the prior perfectly, as this would result in useless data representations or inference.\u201d \nA3: We  understand your viewpoint about the model distribution and the prior distribution . \u201cmatch the prior perfectly\u201d does not mean the point-to-point correspondence. We refer to fitting distributions. The word \u201cmatch\u201d is also used in Wasserstein autoencoder (https://arxiv.org/abs/1711.01558), which is the same scenario to ours.\n\nQ4: \u201cTheorem 2 (on the convergence of the Wasserstein distance (W2) on high dimensional hyperspheres) does not seem to hold if, for instance, P and P\u2019 are empirical distributions with overlapping supports.\u201d\nQ5: \u201cFurther, even when the above Theorem holds, the W2 distance may be relatively high since it is proportional to the square root of the number of samples.\u201d\nA4: To make our theory much easier to understand, we directly gave the computational definition of Wasserstein distance in (8) and (9) rather than its original integral form. Thus, Theorem 2 is the direct result by substituting the conclusion of Lemma 1 into (8). It is very easy. About the correctness of Lemma 1, please refer to the elegant proof at http://faculty.madisoncollege.edu/alehnen/sphere/hypers.htm.\n\nMost theorems only hold under some conditions. Both Lemma 1 and Theorem 2 need a basic condition. The condition is that the points are drawn from spheres at RANDOM. To satisfy the condition, we use the operation of centerization in our SAE algorithm, which is motivated from central limit theorem in probability. \n\nIn fact, it is straightforward to design the case to deny Lemma 1 and Theorem 2 if we bypass the condition. For instance, let Z1 be the set sampled from the spherical part in the open positive orthant and Z2 sampled from the spherical part in the open negative orthant. The third set Z3 is derived from Z2 by the small perturbation. Both Lemma 1 and Theorem 2 do not hold for the dataset { Z1, Z2, Z3}. But such samping violates the randomness needed. For SAE, the centerization is used to prevent such cases. \n\nA5: \u201cthe W2 distance may be relatively high since it is proportional to the square root of the number of samples.\u201d is correct. However, it is logically wrong to use it to deny our theory, because all the W2 distances between two arbitrary random datasets still converge to be the same constant in Theorem 2 when the number of samples increases. The conclusion still holds in our paper.\n\nQ6: \u201cImprove experiments by including more datasets and baselines (e.g., hyperspherical VAE [1]), as well conduct more targeted experiments to give more insights regarding the effect of the L2 normalization on inference and generation. \u201d\nA6: We failed to get the convergent results of hyper-Spherical VAE (S-VAE) on FFHQ faces of size 128x128. So we did not compare it in the current version. We are now running it on MNIST. The results will be updated in the revised version within several days.\n", "title": "To Reviewer #3"}, "B1xPFFJ4jH": {"type": "rebuttal", "replyto": "BJeMRX6atB", "comment": "\nQ1: \u201cThen, to clarify the algorithm, it seems necessary to provide the formulation of objective functions.\u201d,  \u201cIs the objective still valid or reasonable even it is derived from the equation (10) without posterior inference?\u201d\nA1: The objective function will be provided in the revised version. It is the reconstruction loss || x - \\tilde{x} || subject to the spherical constraint on z (equation (10)). There are no posterior inference and no KL-divergence involved in our algorithm. It is very simple. \n\nQ2: \u201cHow does the objective change when centerization and spherization are applied to the GAN?\u201d\nA2:  There is no extra objective when applied to GANs. Only centerization and spherization are needed. \n\nQ3: \u201cCompared with using von Mises-Fisher distribution in the vanilla VAE, the advantage of the proposed method is not clear. To my understanding, the main difference seems to be whether using lower bound with posterior inference or deterministic framework without such approximation. However, there are no theoretical or empirical results to show the benefit of the proposed method.\u201d\nA3: This might be the misunderstanding caused by that we didn\u2019t explicitly write the objective function in the paper. We explain this in Q1. Our SAE algorithm is essentially different from S-VAE (hyper-Spherical VAE). The S-VAE is established on the principle of VAE. So, S-VAE has the drawbacks posed by VAE such as the approximation of posterior inference, the prior dependence, and the reparameterization trick for random variables. But SAE is distribution-agnostic with respect to Wasserstein distance, which is rigorously guaranteed by Theorem 2. \n\nActually, we failed to get the convergent results of S-VAE on FFHQ faces of size 128x128. We are now running it on MNIST. The results will be updated in the revised version within several days.\n\n\nQ4: \u201cCompare to ProGAN and StyleGAN, is the contribution of the paper to applying centerization to GAN and centerization and spherization to autoencoder?\u201d\nA4: Both GAN and autoencoder need to use centerization and spherization on random variables. For ProGAN and StyleGAN, the authors empirically applied spherization on z in their code, which motivated our work. We also made it clear in the context of equation (3).  \n\nQ4: \u201cWhat dimension do you use as latent dimension in the experiments?\u201d\nA4: We followed the experimental setting of StyleGAN. The 512-dimensional latent vectors are used for StyleGAN, VAE, and SAE on the face datasets including FFHQ and CelebA. For MNIST, we take the 10-dimensional latent codes. \n\nQ5: \u201cDoes the choice of prior distribution affect the experimental results? If so, is there any compatible reason with the intuition of SAE?\u201d\nA5: This question might be another misunderstanding caused by Q1. Actually, there are no any priors involved in SAE during training. We used different priors to test the robustness of SAE and VAE after training was completed. We will make it clear in the revised version.\n", "title": "To Reviewer #2"}, "Byek4o59or": {"type": "rebuttal", "replyto": "rJx2slSKDS", "comment": "The revised version has been updated. We revised the submission from the following eight aspects according to Reviewers' advice.\n\n1. We explicitly wrote the objective function of SAE in equation (11). The reconstruction loss and the spherical constraint in equations (10) and (11) are all operations in SAE. There are no variational inference, no probabilistic optimization, and no priors involved in SAE during training.\n\n2. To make the meaning of the word \"inference\" clear, we named the inference in SAE as the spherical inference. As opposed to the variational inference, the spherical inference \u00a0is deterministic during training. But the decoder of SAE is rather robust to various priors for sampling after training. We made this expression clear in the paper to avoid misunderstanding with the variational inference.\n\n3. We added the discussion for Wasserstein autoencoder, adversarial autoencoder, and beta-VAE in the related work.\n\n4. We also compared VAE and SAE on CelebA. \u00a0We need to note that the quality of generated faces by VAE and SAE will be both improved if we use the face images of only cropping facial parts for the experiments. But such data are not sufficient to test the robustness of the algorithms against the variations of the entire facial features.\n\n5. We compared hyper-Spherical VAE (S-VAE) with SAE on MNIST using the official code at https://github.com/nicola-decao/s-vae-tf.\n\n6. We provided the visualization results of latent codes from VAE, S-VAE, and SAE on CelebA and MNIST. This visualization clearly shows the superiority of the spherical inference in SAE.\n\n7. We re-arranged images in Figure 5 to save more space for the new contents. The experimental results in this Figure are kept the same as the previous version.\n\n8. We corrected the typos, polished the writing, \u00a0and made the paper more readable.\n\nAll the complementary experimental results were attached in Appendix.", "title": "About the revised version"}, "HylUhjk4iB": {"type": "rebuttal", "replyto": "HJga0cRjYr", "comment": "\nQ1: \u201cis this a variant of the Wasserstein auto-encoder?\u201d\nA1: Our SAE algorithm is not a variant of WAE proposed in the following paper.\n\nWasserstein Auto-Encoders\nhttps://arxiv.org/abs/1711.01558\n\nWAE minimized Wasserstein distance between the model distribution and the prior distribution. The algorithm reduces to adversarial learning via a discriminator in the latent space, which is similar to the following paper\n\nAdversarial Autoencoders\nhttps://arxiv.org/abs/1511.05644\n\nLike VAE, both Wasserstein autoencoder and adversarial autoencoder need a prior distribution to match. However, there is no loss imposed on the latent space to optimize for SAE. SAE does not need priors either. The object function is the reconstruction loss || x - \\tilde{x} || with the spherical  constraint shown in equation (10). It is much simpler than Wasserstein autoencoder.\n\nIn order to elucidate the unique property of random variables on spheres, we leveraged Wasserstein distance to derive Theorem 2. The Wasserstein distance here serves to establish a circumstance that the algorithm with the spherical constraint can be distribution-agnostic. We did not use Wasserstein distance for computation in SAE.  \n\nBoth Wasserstein autoencoder and adversarial autoencoder are very interesting and inspiring algorithms. We like these two works very much.\n\n\nQ2: \u201cthe image quality of VAE (CelebA) is not that bad in other VAE papers, maybe tuning the \\beta-VAE can also achieve the same quantitative and qualitative results.\u201d\nQ3: \u201cCan you visualize the latent space (z) for the CelebA dataset, also comparing with the results from VAE?\u201d\nA2: For data factors, the image quality of VAE depends on the image size and the image diversity. For face images, the large image size and more backgrounds in the image will make the data difficult to fit. We used the more challenging data of FFHQ available at  https://github.com/NVlabs/stylegan and the image size we used is 128x128. \n\nA3: We are conducting the experiment on CelebA of size 64x64 according to your advice. We will update the results when this complementary experiment is completed.\n", "title": "To Reviewer #1"}, "HJga0cRjYr": {"type": "review", "replyto": "rJx2slSKDS", "review": "This paper proposed an interesting idea that by regularizing the structure of the latent space into a sphere, we can free VAE from variantional inference framework.\n\nHowever, here are several concerns about this paper:\n1. is this a variant of the Wasserstein auto-encoder?\n2. the image quality of VAE (CelebA) is not that bad in other VAE papers, maybe tuning the \\beta-VAE can also achieve the same quantitative and qualitative results.\n3. Can you visualize the latent space (z) for the CelebA dataset, also comparing with the results from VAE?", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "H1e_Hdw6tr": {"type": "review", "replyto": "rJx2slSKDS", "review": "Summary\n\nThis paper considers the L2 normalization of samples \u201cz\u201d from a given prior p(z) in Generative Adversarial Netowks (GAN) and autoencoders. The L2 normalization corresponds to projecting samples onto the surface of a unit-hypersphere. Hence, to attempt to justify this normalization, the authors rely on some already established results regarding high dimensional hyperspheres. In particular, the focus is on the fact that, the Euclidean distance between any given point on a hypersphere and another randomly sampled point on the hypersphere tends to a constant, when the number of dimensions goes to infinity. This result is then used to show that the Wasserstein distance between two arbitrary distributions on a hypersphere converges to a constant when the number of dimensions grows. Based on this result, the authors claim that projecting the latent samples onto the surface of a hypersphere would make GAN less sensitive to the choice of the prior distribution. Moreover, they claim that such normalization would also benefits inference, and that it addresses the issue of variational inference in VAE.\n\nMain comments. \n\nThis paper is hard to follow and requires substantial improvements in terms of writing, owing to several grammatical and semantic issues. Moreover, there is a lack rigor; some important claims are supported neither by experiments nor by theoretical analysis. Experiments in the main paper are also weak. I can therefore not recommend acceptance. My detailed comments are below.\n\n- An important claim in this paper is that the proposed approach \u201calleviates variational inference in VAE\u201d. However, this requires clarification as well as theoretical/empirical justifications.\n- In the introduction, it is stated that generated samples from VAE may deviate from real data samples, because \u201cthe posterior q(z|x) cannot match  the prior p(z) perfectly\u201d. However, in VAE we do not expect the posterior to match the prior perfectly, as this would result in useless data representations or inference. Generation issues in VAE may rather be explained by the fact that, in this context we optimize a lower bound on the KL-divergence between the empirical data distribution and the model distribution. The latter objective does not penalize the model distribution if it puts some of its mass in regions where the empirical data distribution is very low or even zero.\n- Theorem 2 (on the convergence of the Wasserstein distance (W2) on high dimensional hyperspheres) does not seem to hold if, for instance, P and P\u2019 are empirical distributions with overlapping supports. Further, even when the above Theorem holds, the W2 distance may be relatively high since it is proportional to the square root of the number of samples.\n- Moreover, why and how would Theorem 2 justify improved inference when projecting latent samples onto a hypersphere?\n- Please consider revising the following statement in the introduction: \u201cThe encoder f in VAE approximates the posterior q(z|x)\u201d. The encoder \u201cf\u201d in VAE parametrizes the variational posterior.\n- Some typos,\n\t- Abstract, \u201c\u2026 by sampling and inference tasks\u201d  -- \u201con sampling \u2026\u201d\n\t- Introduction second paragraph after eq 2. \u201c\u2026 it also causes the new problems\u201d \u2013 \u201c \u2026 causes new problems\u201d\n\t- Section 2.1, \u201cFor convenient analysis \u2026\u201d \u2013 \u201cFor a convenient \u2026\u201d \n\t- Second paragraph after Theorem 1. \u201c\u2026 perform probabilistic optimizations \u2026 \u201d \u2013 \u201c\u2026 optimization \u2026\u201d \n\t- Section 5.2, second paragraph. Is it Figure 9?\n\nThe main recommendations I would make are as follows.\n- Consider revising the paper to improve its writing.\n- Provide rigorous theoretical analysis and discussions to support the main claims. \n- Improve experiments by including more datasets and baselines (e.g., hyperspherical VAE [1]), as well conduct more targeted experiments to give more insights regarding the effect of the L2 normalization on inference and generation. \n\n[1] Davidson, Tim R., et al. \"Hyperspherical variational auto-encoders.\" UAI, 2018.\n", "title": "Official Blind Review #3", "rating": "1: Reject", "confidence": 4}, "BJeMRX6atB": {"type": "review", "replyto": "rJx2slSKDS", "review": "This paper proposes a novel autoencoder algorithm, named Spherical AutoEncoder (SAE). In this paper, the authors argue that the sphere structure has good properties in high-dimensional. To leverage the properties, proposed algorithm centerizes latent variables and projects them onto unit sphere. To show the empirical performance of the proposed approach, the authors perform image reconstruction and generation using FFHQ dataset and MNIST dataset.\n\nComments:\nI think the proposed approach, using spherical latent space, is interesting and make sense.\n\n- As mentioned in section 3.2, the proposed algorithm is reduced to standard autoencoder since it is free from posterior inference. Then, to clarify the algorithm, it seems necessary to provide the formulation of objective functions.\n- Is the objective still valid or reasonable even it is derived from the equation (10) without posterior inference?\n- How does the objective change when centerization and spherization are applied to the GAN?\n- Compared with using von Mises-Fisher distribution in the vanilla VAE, the advantage of the proposed method is not clear. To my understanding, the main difference seems to be whether using lower bound with posterior inference or deterministic framework without such approximation. However, there are no theoretical or empirical results to show the benefit of the proposed method. If theoretical or empirical results with reasonable intuition is provided, it will make the proposed algorithm more valuable.\n\nQuestions:\n- Compare to ProGAN and StyleGAN, is the contribution of the paper to applying centerization to GAN and centerization and spherization to autoencoder?\n- What dimension do you use as latent dimension in the experiments?\n- Does the choice of prior distribution affect the experimental results? If so, is there any compatible reason with the intuition of SAE?\n\nTypo:\nUnder equation (10) in page 5: \\tilde{z} should be \\hat{z}.\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 2}, "r1lf20oUFB": {"type": "rebuttal", "replyto": "S1gC86mHYr", "comment": "Thanks for your very insightful comments, Alex.\n\n1) About covering\n\nThe case you raised is really challenging. We choose the vector centerization to afford randomness on the sphere. But how well this operation enforces z_i to cover the spherical surface as uniformly as possible is an important topic to study for spherical autoencoder (SAE).\n\nThe vector centerization presented in our paper does have flaw. For example, the points on the spherical surface falling into the open positive orthant (z_i > 0) cannot be sampled. Considering that there are all 2^512 orthants in R^512, however, the open positive orthant only takes 1/2^512 part of the whole sphere. So, the negative effect is nearly trivial.\n\nWe also figured out another way of sampling on the sphere to circumvent this problem. For any {z_1,...,z_i,...,z_n} drawn from arbitrary distributions, we can first project them on the sphere by z_i <-- z_i/norm(z_i). The projected points probably lie on some specific regions on the sphere.  Then we can randomly rotate these points on the sphere by a series of orthogonal matrices that are obtained by orthogonalizing random matrices via Gram\u2013Schmidt process. In this way, we can get {z_1,...,z_i,...,z_n} that distributes randomly on the sphere as long as the rotation manipulations are sufficient.\nHowever, this method is not friendly to end-to-end learning for autoencoder. We do not use it in this paper.\n\nThe vector centerization and spherization is the simplest way we get to realize our idea, even though it is not perfect. What is most important is that it is very easy to use in the end-to-end architecture of autoencoder.\n\n2) About inductive bias\n\nTheorem 2 tells that SAE is distribution-agnostic with respect to Wasserstein distance. In other words, it has distributional inductive bias. However, it is very inspiring about your conjecture \"Perhaps the inductive bias of the neural network makes this type of issue unlikely\".\n\nActually, your conjecture leads to the connection between random variables on the sphere and universal approximation theorem (UAT).  We also think that this is an alternative way of further exposing the deep reason why the simple spherical constraint can outperform the traditional variational inference. You may refer to the following paper, if interested.\n\nSpherical approximate identity neural networks are universal approximators\n Zarita Zainuddin, Saeed Panahian Fard\nICNC, 2014\n\nYour thoughts are quite inspiring. We will consider the topics you raised seriously for our future work.", "title": "about covering and inductive bias"}}}