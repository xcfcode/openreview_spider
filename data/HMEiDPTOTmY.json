{"paper": {"title": "Later Span Adaptation for Language Understanding", "authors": ["Rongzhou Bao", "Zhuosheng Zhang", "hai zhao"], "authorids": ["~Rongzhou_Bao1", "~Zhuosheng_Zhang1", "~hai_zhao1"], "summary": "", "abstract": "Pre-trained contextualized language models (PrLMs) broadly use fine-grained tokens (words or sub-words) as minimal linguistic unit in pre-training phase. Introducing span-level information in pre-training has shown capable of further enhancing PrLMs. However, such methods require enormous resources and are lack of adaptivity due to huge computational requirement from pre-training. Instead of too early fixing the linguistic unit  input as nearly all previous work did, we propose a novel method that combines span-level information into the representations generated by PrLMs during fine-tuning phase for better flexibility. In this way, the modeling procedure of span-level texts can be more adaptive to different downstream tasks. In detail, we divide the sentence into several spans according to the segmentation generated by a pre-sampled dictionary. Based on the sub-token-level representation provided by PrLMs, we enhance the connection between the tokens in each span and gain a representation with enhanced span-level information. Experiments are conducted on GLUE benchmark and prove that our approach could remarkably enhance the performance of PrLMs in various natural language understanding tasks.", "keywords": []}, "meta": {"decision": "Reject", "comment": "The paper proposes to combine the span-level information into a phrase-level representation in the fine-tuning phrase for pre-trained language models.  The phrases are pre-defined in a dictionary.  Experiments show improvements in various downstream tasks in the GLUE benchmark.  It's a borderline paper.  Various concerns were raised by the reviewers, for example, the relation with the SpanBERT method in pre-training phrase and the significance of the results.  The authors addressed most of the concerns but the reviewers were not fully convinced.  In general, I think it is an interesting paper with good motivation and results.  Hope it can be improved (e.g. more experiments on SpanBERT) and accepted in another conference."}, "review": {"Vh-GsVS0GyA": {"type": "rebuttal", "replyto": "HMEiDPTOTmY", "comment": "Dear Reviewers and AC,\nWe sincerely thank the reviewers for their detailed comments. Reviewers (R2) noted that our method is novel for (a) incorporating span information only during fine-tuning and (b) developing a new way to segment sentences. At the same time, our idea is recognized as well motivated and easy to understand. We have incorporated all the feedback into the newly uploaded version. We briefly summarize the major updates as follows, and please refer to the responses to each reviewer for more detailed explanations:\n1.\tIn Section 5.4, we have added the experiments with stronger PrLMs, including RoBERTa and SpanBERT. The results show that our method can consistently improve the performance of various PrLMs. \n2.\tIn Section 5.1, we have added a set of ablation experiments using a pre-trained chunker provided by NLTK to segment the sentences. The results show that our method can process the data twice as fast as the pre-trained chunker and achieve better performance.\n3.\tIn Section 5.5, we have added the experiments on NER task. Results show that our approach can also improve the performance of PrLMs on token-level tasks.\n4.\tIn Appendix, we report the average results on GLUE benchmark dev sets. We also add the significance test, and the results show that the improvements of our method are significant comparing to the baseline.\n\nThanks again for your waiting. Please refer to the latest version of the paper for detailed modification. We hope the response and revision can address the concerns. If you have any further comments or suggestions, we will be happy to respond.\n", "title": "Revision Summary"}, "1_KqmZPQR0V": {"type": "rebuttal", "replyto": "iBuyelfEcm", "comment": "Thanks a lot for your insightful comments. Please see our response below.\n\n**About the literature discussion**\nWe have cited this paper and added a discussion in Section 5.2. Since the tasks (and methods) between the two works are very different, as you mentioned the novelty of our work is not affected.\n\n**About stronger baseline**\nFollowing your advice, we have added the experiments with stronger PrLMs, including RoBERTa, in Section 5.4. Results show that our approach can consistently improve the PrLMs.\n\n\n**About entity-based task**\nFollowing your advice, we have added the experiments on NER task in Section 5.5. Results show that our approach can also improve the performance of PrLMs on token-level tasks.\n\n**bout pre-trained chunker**\nFollowing your advice, in Section 5.1, we have added a set of ablation experiments using a pre-trained chunker provided by NLTK to segment the sentences. The results show that a pre-trained chunker can indeed achieve better performances than random segmentation. However, our approach can achieve an even better result than pre-trained chunker, and at the same time be more convenient. Compared with pre-trained chunker, our method processes the data one time faster.\n\n", "title": "Response to reviewer 4"}, "FUKq4Leem-t": {"type": "rebuttal", "replyto": "NbG-UyXv4aD", "comment": "Thanks for your interest and constructive comments!\n\n**About the significancy of the results**\nAccording to your advice, we report the average results on GLUE benchmark dev sets in Appendix. The improvement of 1.6 percent by average is remarkable. Furthermore, we also add the significance test in appendix, results show that the improvements of our method are significant comparing to baseline. In addition, our approach can gain remarkable promotion on certain tasks. We add the experiments with stronger PrLMs, including RoBERTa, in Section 5.4, we can improve the task of RTE by 4.0 percent. This result further proves the efficacy of our method.\n\n**About the application of our method to SpanBERT**\nAccording to your advice, we add the experiments with SpanBERT, in Section 5.4. The results show that we can further improve the performance of SpanBERT. Such result implies that our method take advantages of the span-level information in a different manner comparing to PrLMs pre-trained with span-level information, which makes our method distinguished comparing to previous works. It is really interesting to find out the origin of this difference, we will do more researches about this topic in the future.", "title": "Response to reviewer 1"}, "lo9isb8gzkU": {"type": "rebuttal", "replyto": "2u6trT873YO", "comment": "Thanks a lot for your careful  review. Please see our response below.\n\n**About the main contribution and advantages of this work**\nIt is true that previous work such as SpanBERT innovatively pointed out the effectiveness of employing span-level information in PrLMs. Meanwhile, our method remains competitive for the following reasons:\n- **Convenience**:  Previous works incorporate span-level in pre-training, which demands tremendous time and resources. Comparatively, Our method can be directly applied during fine-tuning. The comparison between incorporation of span-level information in pre-training and LaSA is showed below:\n| Methods |   Time  | Resource |\n| :-:| :-: | :-:|\n| pre-training| 32 days | 32 Volta V100 |\n| LaSA | 12 hours max | 2 Titan RTX |\n-  **Task-adaptive**:  For methods that incorporate span-level information in pre-training, the utilization of span-level information is fixed for every downstream task. In our method, the extra module designed to incorporate span-level information is trained during the fine-tuning, resulting in a more dynamically adaptation to different downstream tasks as discussed in Section 4.4.\n-  **Flexible to PrLMs**: We add the experiments with stronger PrLMs in Section 5.4. Our approach can be generally applied to various PrLMs including RoBERTa and SpanBERT.\n-  **Novelty**\uff1aAs shown by the newly added experiments in Section 5.4, our approach can further improve the performance of PrLMs pre-trained with span level information (e.g. SpanBERT). Such result implies that we our method utilizes the span-level information in a different manner comparing with PrLMs pre-trained with span-level information, which makes our method distinguished comparing with previous works.\n\n**About the improvement**\nSince the PrLMs are very strong baselines, the improvement of one percent is already remarkable. Furthermore, we added the significance test in Appendix, results show that the improvements of our method are significant comparing to the baseline. In addition, our approach can gain a remarkable margin on certain tasks. As you can find in Section 5.5, by applying our method to RoBERTa, which is already a very strong baseline, we can improve the task of RTE by 4.0 percent. This result further proves the efficacy of our method.\n\n**About the typo**\nThanks a lot for your careful review, the mistake is corrected.\n", "title": "Response to reviewer 3"}, "0z6KDEnImIL": {"type": "rebuttal", "replyto": "k5WqL5d-eHM", "comment": "Thanks so much for your constructive feedbacks. Please see our response below.\n\n**About the complexity in fine-tuning**\nYes, our method introduces an extra module for down-stream tasks, the complexity, as well as the computation, is very close to the baseline PrLMs. There are two factors that may influence the complexity and training time: 1) model complexity and 2) sentence segmentation during pre-processing. The total numbers of our model and the baseline are very close \u2013 only about 3% extra parameters comparing to the adopted PrLMs. The preprocessing of sentence segmentation is also very fast, about 730 sentences per second per CPU. The processing is parallelizable  and does not affect the whole data processing time. Considering the consistent performance gains over PrLMs on every downstream task, the extra complexity would be acceptable.\n        \nFurthermore, for methods that incorporate span-level information in pre-training, the utilization of span-level information is fixed for every downstream task. Whereas, in our method the extra module designed to incorporate span-level information is trained during the fine-tuning, which can be more dynamically adapted to different downstream tasks as discussed in Section 4.4.\n\n**Whether improving the performance on the same aspects comparing with SpanBERT**\nWe add the experiments with stronger PrLMs, including SpanBERT, in Section 5.4. The results show that we can further improve the performance of SpanBERT by adopting our later span adaption method. The result implies that our method might take advantages of the span-level information in a different manner compared with PrLMs pre-trained with span level information, such as SpanBERT. It is really interesting to find out the origin of this difference, we will do more researches about this topic in the future.\n\n**About token-level tasks**\nFollowing your advice, we have added the experiments on NER task in Section 5.5. The results show that our approach can also improve the performance of PrLMs on token-level tasks.\n\n\n\n\n", "title": "Response to Reviewer 2 "}, "lE-u0jVX3aG": {"type": "rebuttal", "replyto": "HMEiDPTOTmY", "comment": "Dear reviewers,\n\nThanks for all your reviews and waiting. We are working on the revision accordingly and will update the paper with a detailed response soon. Please keep tuned!", "title": "We will update the paper and response soon. Please keep tuned!"}, "NbG-UyXv4aD": {"type": "review", "replyto": "HMEiDPTOTmY", "review": "Summary:\n \nThe paper introduced a fine-tuning approach which adapts the subword level, span level and sentence level to the target tasks. The span segmentation is done by using a pre-trained n-gram statistical model.  Empirical studies on GLUE benchmark show that the proposed approach consistently improves the performance of BERT. \n \nPros:\n \n1. The idea of leveraging a n-gram model for pre-trained language model fine-tuning is interesting.\n2. The proposed method is well motivated and easy to understand.\n \nCons:\n\nMy concern is the significancy of the results. Since the improvement of the proposed method is marginal (~ 1% avg) and fine-tuning BERT on GLUE tasks might have high variance, authors should report the mean and std over multiple runs.\n \nQuestions:\n\nIf the model is pre-trained with span level information (e.g., spanBERT), will the proposed method outperform normal fine-tuning?\n", "title": "Is the experimental results significant?", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "k5WqL5d-eHM": {"type": "review", "replyto": "HMEiDPTOTmY", "review": "This paper presents an approach to incorporate span information in pre-trained language models like BERT during fine-tuning. In the proposed approach, the segmentation of a sentence is obtained according to a pre-sampled n-gram dictionary. The fine-grained representation in a same span within the segmentation is aggregated to a span-level representation using a CNN model. These span-level representations are further aggregated using a CNN model to generate sentence-level representation. The experiments show that the proposed model can achieve similar performance gain as other span-based language models which includes span information during pre-training.    \n\nThe paper is well written and the proposed method is novel. The novelty of the method is in mainly including span information only during fine-tuning and also the way spans are identified in a sentence. The authors conducted an extensive set of experiments with ablation and results show that the proposed idea is effective and attains similar gains as other span-based language model. \n\nIt is not clear how this model helps in practice since it is achieving similar performance as other models like SpanBERT. It is true that this model doesn\u2019t introduce complexity while pre-training but pre-training is a one-time process and even if pre-training is slower, it does not affect downstream tasks. However, the proposed model introduces complexity in the downstream tasks and may affect training time in the downstream task.\n \nThe authors claim that the proposed model perform similar to models like SpanBERT but it is not clear if both models are improving on similar aspects. Some fine-grained analysis of results with these models might be insightful.\n\nAnother concern I have is that the proposed model can only work in the scenarios where sentence-level representation is required. However, there are tasks like named-entity recognition (NER) where word level representation is needed. ", "title": "An interesting work on incorporating span information in BERT like models during fine-tuning", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "iBuyelfEcm": {"type": "review", "replyto": "HMEiDPTOTmY", "review": "This paper introduces a new mechanism based on span-based representation grouping for better language model finetuning. Specifically, firstly, the paper proposes to utilize an n-gram splitter to segment the text into several segments. Secondly, a hierarchical network is built based on a three-level network (subword level->span level->sentence level). Using these tasks to perform fine-tuning on top of a pre-trained masked language modeling shows improvements than directly fine-tuning on the pre-trained language models. By conducting experiments on the GLUE benchmark with the BERT-base and BERT-large models, the results are improved. The general idea is simple and easy to understand. However, I have several concerns which are list as follows,\n\n1. Combining span-level information has been widely studied in the literature[1] (not cited). Although the tasks between the two papers are different, the novelty of the paper is limited. Besides, it is also worth discussing between Sec 5.2 and [1]. \n2. The experiments are conducted on BERT-base and BERT-large models. However, the two models are weak baselines. I would like to see the empirical results on stronger models, like RoBERTa related models.\n3. As the paper focuses on training span representations, it is necessary to perform experiments on entity-based tasks. The experiments setup may refer to the paper [2].\n4. The paper proposed to use the n-grams, it is also necessary to have a comparison with the pre-extracted spans using an off-the-shell chunker.\n\n\nI also have the following questions:\nIf a pretrained chunker is utilized to extract chunks, what are the results like?\nIn section 5.2, does the variant of n-grams affect the results?\n\n\n\nFor these reasons, I do not recommend acceptance of this paper.\n\n\n\n\n[1] Toshniwal et al. A Cross-Task Analysis of Text Span Representations, ACL RepL4NLP-2020\n[2] Yamada et al. LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention, EMNLP 2020\n\n\n", "title": "A simple, useful and easy to understand idea. However, the paper lacks novelty and the empirical results may not be convincing.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "2u6trT873YO": {"type": "review", "replyto": "HMEiDPTOTmY", "review": "Previous works reveal that span-level information can enhance the performance of PrLMs if they are used in pre-training. However, the existing methods require enormous resources and lack adaptivity. To this end, the paper proposes a method that combines span-level information into the representations generated by PrLMs during the fine-tuning phase. To combine span-level information, the paper first breaks a sentence into various span components. Then, an accumulated representation with enhanced span-level information is built based on the sub-token-level representation provided by PrLMs. The experimental results on the GLUE benchmark show that the proposed method improves the performance of PrLMs. The main contribution of this paper is the introduction of generating the span components via a pre-sampled dictionary. Overall, the proposed method is not novel since similar methods or ideas have been widely used in NPL.\n.\n\nConcerns:\n\n1. Employing span-level information in PrLMs was proposed in previous works such as SpanBERT. The paper presented a way that combines span-level information into the representations generated by PrLMs during the fine-tuning phase. However, as shown in Table 2, the proposed method is not better than SpanBERT. So, what are the advantages of the proposed method.\n2. As shown in Table 1, compared with the baselines, the proposed method does not bring remarkable promotion.\n\nMinor comments: \n1. The subscript \u201cj\u201c should be \u201c1\u201d in the second line of Formula (1).\n", "title": "enhancing the performance of PrLMs by span-level information", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}