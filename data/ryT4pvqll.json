{"paper": {"title": "Improving Policy Gradient by Exploring Under-appreciated Rewards", "authors": ["Ofir Nachum", "Mohammad Norouzi", "Dale Schuurmans"], "authorids": ["ofirnachum@google.com", "mnorouzi@google.com", "schuurmans@google.com"], "summary": "We present a novel form of policy gradient for model-free reinforcement learning with improved exploration properties.", "abstract": "This paper presents a novel form of policy gradient for model-free reinforcement learning (RL) with improved exploration properties. Current policy-based methods use entropy regularization to encourage undirected exploration of the reward landscape, which is ineffective in high dimensional spaces with sparse rewards. We propose a more directed exploration strategy that promotes exploration of under-appreciated reward regions. An action sequence is considered under-appreciated if its log-probability under the current policy under-estimates its resulting reward. The proposed exploration strategy is easy to implement, requiring only small modifications to the standard REINFORCE algorithm. We evaluate the approach on a set of algorithmic tasks that have long challenged RL methods. We find that our approach reduces hyper-parameter sensitivity and demonstrates significant improvements over baseline methods. Notably, the approach is able to solve a benchmark multi-digit addition task. To our knowledge, this is the first time that a pure RL method has solved addition using only reward feedback.", "keywords": ["Reinforcement Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes a nice algorithm for improved exploration in policy search RL settings. The method essentially optimizes a weighted combination of expected reward (essentially the REINFORCE objective without an entropy regularizer) plus a term from the reward augmented maximum likelihood objective (from a recent NIPS paper), and show that the resulting update can be made with a fairly small modification to the REINFORCE algorithm. The authors show improved performance on several sequential \"program-like\" domains like copying a string, adding, etc.\n \n I'm recommending this paper for acceptance, as I think the contribution here is a good one, and the basic approach very nicely offers a better exploration policy than the typical Boltzmann policy, using a fairly trivial modification to REINFORCE. But after re-reading I'm less enthusiastic, simply because the delta over previous work (namely the RAML paper) doesn't seem incredibly substantial. None of the domains in the experiments seem substantially challenges, and the fact that it can improve over REINFORCE isn't necessarily amazing.\n \n Pros:\n + Well-motivated (and simple) modification to REINFORCE to get better exploration\n + Demonstrably better performance with seemingly less hyperparameter tunies\n \n Cons:\n - Delta over RAML work isn't that clear, essentially is just a weighted combination between REINFORCE and RAML (though in RL context)\n - Experiments are good, but not outstanding relative to simple baselines"}, "review": {"HymfcMD5l": {"type": "rebuttal", "replyto": "H1bRgy6Fx", "comment": "We've updated the paper to include a discussion on the similarities and differences with Reward-Weighted Regression.  We hope this will clear up the confusion while giving proper credit to previous work.", "title": "Paper Updates"}, "B1QoVE6Kg": {"type": "rebuttal", "replyto": "H1bRgy6Fx", "comment": "Thanks for the comment. The objective studied in the paper is different from reward-weighted regression (RWR) even though it looks similar.\n\nThe RWR objective is formulated as\n\nO_{RWR} = \\sum_a [exp(r(a) / t) * \u03c0(a)]  ,\n\nwhereas the RAML objective is\n\nO_{RAML} = \\sum_a [exp(r(a) / t) * log \u03c0(a)]  .\n\nNot only are these two objectives different, but also they are motivated differently, and we believe that the RAML objective has a more reasonable formulation (see RAML vs. RWR papers). From an algorithmic point of view, the two training algorithms are different.  RAML (and UREX below) use standard gradient descent methods while RWR is usually optimized via an EM algorithm.\n\nThat said, even though UREX builds on RAML, its objective is different:\n\nO_{UREX} = \\sum_a [r(a) * \u03c0(a) + t * exp(r(a) / t)  log \u03c0(a)]\n\nIn practice UREX requires importance sampling, whereas RWR does not take this into account.\n\nNevertheless, we agree that the paper should have cited RWR to contrast the UREX and RAML objectives with RWR and to provide a better context for the algorithmic development. We were planning to include a reference to RWR in the camera-ready version. Thank you for the reminder.", "title": "Not quite the same"}, "H1bRgy6Fx": {"type": "rebuttal", "replyto": "ryT4pvqll", "comment": "\nIt has recently come to my attention that the objective proposed by the authors in this paper has in fact already been studied in the literature under the name 'reward-weighted regression', from e.g. ICML 2007 [1]. This has spawned several other works using the same objective (e.g. [2]). One can examine for instance the objective proposed in Section 3.4 from [2], from ICANN 2008.\n\nWhile this paper has already been accepted to ICLR, it would be beneficial for the authors to at least cite these works (and other related works) so that readers are aware of the previous origins of this idea.\n\n[1] http://machinelearning.org/proceedings/icml2007/papers/98.pdf\n[2] http://people.idsia.ch/~daan/papers/icann08.pdf", "title": "Same objective as reward-weighted regression"}, "ByoSfASIg": {"type": "rebuttal", "replyto": "ryT4pvqll", "comment": "We thank all the reviewers for their thoughtful comments.  We have made several updates to the paper in response:\n\n-- We have made adjustments to the introduction to make it clear that the motivation of the paper is twofold: (1) to introduce an RL algorithm that improves the performance and the exploration behavior of REINFORCE (2) to improve the behavior of RL methods on algorithmic tasks.\n\n-- In Section 6, we have made it clear that we only use total-episode reward for UREX and MENT.\n\n-- For more results, we have revised the paper to include an appendix with a simple bandit-like task with a large action space. \n", "title": "Paper Updates"}, "HJFzG0H8g": {"type": "rebuttal", "replyto": "SkmRSsZ4g", "comment": "Thank you for your thoughtful review.  Below are our responses:\n\n- The only drawback of the paper is to restrict the evaluation of this algortihm to algorithmic problems that are specific\n\nWe focused on algorithmic environments since such environments are deterministic, simple to understand, and unlike video games and robotics do not require sophisticated tuning of perception and motor control modules. In addition, they have large action spaces with sparse rewards where a reasonable policy needs to predict an appropriate long sequence of actions to obtain any non-negligible reward. \n\n- it would be easy to test the proposed model onto other standard RL problems. This would clearly help to make the article stronger and I greatly encourage the authors to add some other tasks in their paper.\nWe have revised the paper to include an appendix with results on a simple bandit-like task with a large action space. In addition, we plan to evaluate UREX on video games and robotics as a future work.", "title": "Responses"}, "S166ZCB8x": {"type": "rebuttal", "replyto": "SJCrPi-El", "comment": "Thank you for your thoughtful review.  Below are our responses:\n\n- the focus in the introduction on algorithmic tasks may be a double-edged sword\u2026.\nIt is true that we had two motivations, and the proposed policy gradient modification is far more general than just being applicable to algorithm induction.  However, we think algorithm induction is an important problem in its own right that also provides an excellent test-bed for research on exploration issues, since, as mentioned above, it greatly diminishes the role of perception and motor control (which can quickly overwhelm development and experimentation).  We will revise the introduction to make these considerations clearer and that the choice of algorithmic tasks is not necessary to the success of the algorithm. We have also included an additional bandit task in the appendix.\n\n- in the introduction you say the reward is sparse\u2026.  I'd move or mention this in section 6.\nGood point. We will mention this in section 6, as suggested.\n\n- approach seems quite sensible to tau being in the same range as logpi(a|h), but you only try tau=0.1 for UREX. I'm not sure I understand nor agree with this experimentation choice.\nWe found that tau=0.1 worked well for UREX across the entire range of our experiments.  Such a fixed choice can only put UREX at a comparative disadvantage, yet it still allowed UREX to produce better results in our experiments. We do not assert that this value of tau is always suitable - some experimentation is always needed in principle. Clearly, tau must change if the rewards are scaled by some constant for example.\n\n- an alternative to grid search is random search (Bergstra&Bengio, 2012).... \nGiven that we only modified at most three hyper-parameters, we found it feasible and more appropriate to do a full grid search.\n\n- I'm ambivalent to claiming that an algorithm is robust to hyperparmeters simply because it performs better on the selected hyperparameter range.... Couldn't it be that MENT needs different hyperparameters?\nOf course proving robustness to hyperparameters is challenging, but we selected the range of hyperparameters after experiments with several algorithms and tasks. Generally, hyperparameters outside the selected range performed poorly for all of the techniques. We were surprised to see that UREX performs well on a larger subset of hyperparameters compared to the baselines, and we reported the behavior in the paper. We believe this indicates more robustness for the current tasks, but more experiments could be beneficial.\n\n- I see why matching 1/tau with logpi is the obvious choice, but it implies a very strong prior: that the reward (to a factor of 1/tau) lies in the same space as the log policy. One point of failure...\nYes, our formulation assumes that two action sequences with the same reward should have the same probability in the optimal policy.  If under the current model, a short sequence is more probable than a longer sequence with equal reward, then the gradient updates would push the policy to put more probability mass on the longer sequence which is aligned with our assumption.  Indeed, we do not see this as a failure case.  In some tasks, it may be intrinsically better to prefer a shorter sequence, all else being equal, which should be reflected in the reward function.\n\n- It might have been good to also compare with methods explicitly trying to explore better with value-functions\u2026.\nWhile this would be interesting, we focused on policy methods and double Q-learning.\n\n- \u2026 tau plays a major role in this method, but there is little analysis on its effect on experiments.\nWhile we observed that choosing a good value of tau is important, we found the same value of tau to work reasonably well across all of the tasks.  We leave more experimentation for a more thorough analysis to future work.\n\n- I think that the paper could have a more convincing empirical argument, even if it is for toy tasks.\nFor more results, we have revised the paper to include an appendix with a simple bandit-like task with a very large action space. \nDo you have particular empirical statistics in mind to help showcase the effectiveness of comparing reward and log probability under the policy?\n\n- I do think that this paper would benefit from more (or cleverer) experimentation, as well as demonstrating more explicitly the impact of the method on exploration.\nWe have considered quantitatively measuring an algorithm\u2019s \u201cexploration\u201d, but it is hard to know what to measure as a proxy to quantify an algorithm\u2019s exploration behavior.  For example, if our proxy is entropy of the policy distribution, then entropy regularization would be rated highly under that measurement. The objective measurement we came up with is whether an algorithm can solve a task that is hard for other algorithms. Accordingly, our experiments show that UREX performs better than the alternatives, and we associate this performance edge to better exploration.\n", "title": "Responses"}, "HkWOWASIe": {"type": "rebuttal", "replyto": "HkW7wkvNe", "comment": "Thank you for your thoughtful review.  Below are our responses:\n\n- It is not very clear what is the main motivation of the paper\u2026 Authors should make it clear which is the main motivation\nIt is true that we had two primary motivations: first, to introduce an RL algorithm that improves the performance and the exploration behavior of the commonly used policy gradient; and second, to improve the behavior of RL algorithms on program induction. Generally RL has had minimal success in program induction, yet we believe there is a big potential for its positive impact on program induction that has not yet been fully realized. We\u2019ve revised the paper to make this clearer.\n\n- Is the main motivation better exploration for policy gradient methods? If so, authors should have benchmarked their algorithm with standard reinforcement learning tasks.\nWe believe that program induction is a good test-bed for fundamental RL research because such environments are deterministic, simple to understand, and unlike video games and robotics do not require sophisticated tuning of perception and motor control modules. Program induction tasks are already particularly challenging, and, as mentioned in the paper, the problem of learning to add two numbers has not been previously solved by a model-free RL approach. Even with maximum likelihood approaches, sophisticated neural nets were shown to solve the addition task only in the past few years. That said, given the existing literature on video games, we are also intending to evaluate UREX on video games as a future work.\n\n- While there is a huge body of literature on improving REINFORCE, authors have considered a simple version of REINFORCE on a non-standard task and say that UREX is better.\nWhat specific variants of policy gradient do you have in mind that are applicable to our problems?\n\n- If the main motivation is improving the performance in algorithm learning tasks, then the baselines are still weak.\nWe believe our paper presents the state-of-the-art results in learning algorithms that only use the reward feedback and no prior knowledge of the environment.\n\n- Also the action space is too small.\nWe note that in the episodic tasks that we consider, a reward is not associated with individual actions but with a sequence of actions. Thus, the \u201caction space\u201d consists of action sequences (a_1, \u2026, a_t) which grow exponentially in depth. Since the action sequences we encounter in these problems are quite large (at least 30 time steps in all tasks at the hardest setting), accordingly, these environments exhibit sparse rewards in very large action spaces.\n", "title": "Responses"}, "BJuatAT7g": {"type": "rebuttal", "replyto": "rkLPwRp7x", "comment": "Q: What do RAML and RL stand for in O_RL and O_RAML?\nA: RL is \"reinforcement learning\" and is the objective for maximizing expected reward with an optional entropy regularizer.\nRAML is reward-augmented maximum likelihood (see https://arxiv.org/abs/1609.00150).\n\nQ: In REINFORCE, what b(h)/baseline are you using?\nA: We estimate the average coefficient of grad(log \\pi) empirically by sampling 10 trajectories.  We use this empirical estimate as the baseline.\n\nFrom the paper:\n\"For MENT, we use the 10 samples to subtract the mean of the coefficient of d/d\u03b8 log \u03c0\u03b8(a | h) which includes the contribution of the reward and entropy regularization.\"", "title": "Responses"}, "SkD4mcKml": {"type": "rebuttal", "replyto": "Sy1KsnSme", "comment": "Thanks for your question.  You should note that the reward function r( . ) is not a function of policy, but rather a function of an action sequence.  We define the optimal policy as that which gives an action sequence a^(k) a probability proportional to exp(a^(k) / t).\n\nWe would also like to clarify that (10) is merely the standard weight formula used in importance sampling: in importance sampling, a sample sequence a^(k) is always drawn from the proposal distribution \\pi_\\theta; the point of the weighting is to correct for the difference between the probability of a^(k) under the proposal distribution versus the probability of a^(k) under the target distribution.\n\nHopefully this helps - let us know if you have further questions.", "title": "Response"}, "HJy8M5KXl": {"type": "rebuttal", "replyto": "S1DxmKJml", "comment": "Thank you for the comments.  Responses are included below.  Please let us know if you have any additional concerns.\n\nQ: ... lacks of clarity since the authors are mixing arguments concerning reinforcement learning in general, and ... learning algorithms\nA: We conduct the experiments on some challenging algorithmic tasks, but our framework is applicable to generic RL problems such as Atari games. Nothing in the presented technique is specific to learning algorithms.  However, we chose to focus on algorithms since they are relatively easy to describe/analyze and come in encapsulated and relatively short episodes.\n\nQ: ... the underlying MDP that can be with stochastic transitions or deterministic ones. In Section 3, second paragraph, the transition between states seems to be deterministic -- s_t+1 = f(s_t,a_t). So I am not sure that the algorithm proposed here is specific or not to deterministic problems.\nA: The algorithm is also applicable to stochastic environments. The transition between the states is only deterministic given a latent random variable h, thus stochasticity in the transitions is encapsulated in h. One can reduce MDPs to such a formulation by using the latent variable h to encode all of the random seeds used within the stochastic transitions. Similarly, we also model the reward signal as a deterministic function of an action sequence given the latent variable h. We will make this more clear in the paper.\n\nQ: Concerning the experimental part, here also the choice of solving algorithms problem is very specific, while the conclusion of the paper is about general RL... could you please provide more details ?\nA: We believe that algorithmic environments provide a nice and simple testbed for new RL algorithms, and obtaining state-of-the-art results on tasks like addition and binary search is significant. That said, we are looking into extending the proposed framework to games and robotics applications as future work.\n\nQ: ... the influence of the use of curriculum learning is not analyzed. Is it possible to provide results with and without curriculum learning ? ...\nA: In the OpenAI Gym library, curriculum learning is implicit in the environment.  Thus, we opted to leave the environment intact and used the curriculum as the OpenAI Gym defines it. Even with curriculum learning, the reward is quite sparse on most tasks considered. For example, on the BinarySearch task, even at the beginning of the curriculum (n = 32), on average, the agent must get a sequence of 10 actions correct to obtain a nonzero reward.  When there are 12 options for each action, it is clear that any non-zero reward is difficult to find, let alone the optimal policy.\n", "title": "responses"}, "Sy1KsnSme": {"type": "rebuttal", "replyto": "ryT4pvqll", "comment": "Hi,\n\nI feel the weight \"w_\\tau(a^k|h)\" proposed in Equation (10) is quite crucial to model the *under appreciation*. Equation (10) is linked to the optimal policy distribution proposed by Equation (5). The weight should be proportional  to \u03c0*/\u03c0_\\theta, and \u03c0* is proposed in Equation (5) based on the rewards given optimal policy. But apparently, in Equation (10), the reward term \"r\" used is from the underlying policy, which is not optimal. Otherwise, it's not quite possible to compute that for gradient update. \n\nHope you could clarify on this. Thank you!", "title": "Optimal Reward vs. Actual Reward"}, "S1DxmKJml": {"type": "review", "replyto": "ryT4pvqll", "review": "The main topic of the paper lacks of clarity since the authors are mixing arguments concerning reinforcement learning in general, and arguments for the particular task of learning algorithms. It makes some points unclear to me:\n\nThe difference between general RL and learning algorithms concerns the nature of the underlying MDP that can be with stochastic transitions or deterministic ones. In Section 3, second paragraph, the transition between states seems to be deterministic -- s_t+1 = f(s_t,a_t). So I am not sure that the algorithm proposed here is specific or not to deterministic problems. Could you please improve the description on that point ?\n\nConcerning the experimental part, here also the choice of solving algorithms problem is very specific, while the conclusion of the paper is about general RL... could you please provide more details ?\n\nAt last, the influence of the use of curriculum learning is not analyzed. Is it possible to provide results with and without curriculum learning ? Particularly, you argue that your method is a good way to deal with reward sparsity, while curriculum learning seems to be in your experiments a way to reduce the sparsity of the reward at the beginning of the process... Dealing with really sparse reward (without curriculum learning) could help to better understand if your method helps. The paper proposes a new algorithm based on REINFORCE which aims at exploring under-appreciate action sequences. The idea is to compare the probability of a sequence of actions under the current policy with the estimated reward. Actions where the current policy under-estimate the reward will provide a higher feedback, thus encouraging exploration of particular sequences of actions. The UREX model is tested on 6 algortihmic RL problems and show interesting properties in comparison to the standard regularized REINFORCE (MENT) model and to Q-Learning.\n\nThe model is interesting, well defined and well explained. As far as I know, the UREX model is an original model which will certainly be useful for the RL community. The only drawback of the paper is to restrict the evaluation of this algortihm to algorithmic problems that are specific while it would be easy to test the proposed model onto other standard RL problems. This would clearly help to make the article stronger and I greatly encourage the authors to add some other tasks in their paper. ", "title": "pre-review questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SkmRSsZ4g": {"type": "review", "replyto": "ryT4pvqll", "review": "The main topic of the paper lacks of clarity since the authors are mixing arguments concerning reinforcement learning in general, and arguments for the particular task of learning algorithms. It makes some points unclear to me:\n\nThe difference between general RL and learning algorithms concerns the nature of the underlying MDP that can be with stochastic transitions or deterministic ones. In Section 3, second paragraph, the transition between states seems to be deterministic -- s_t+1 = f(s_t,a_t). So I am not sure that the algorithm proposed here is specific or not to deterministic problems. Could you please improve the description on that point ?\n\nConcerning the experimental part, here also the choice of solving algorithms problem is very specific, while the conclusion of the paper is about general RL... could you please provide more details ?\n\nAt last, the influence of the use of curriculum learning is not analyzed. Is it possible to provide results with and without curriculum learning ? Particularly, you argue that your method is a good way to deal with reward sparsity, while curriculum learning seems to be in your experiments a way to reduce the sparsity of the reward at the beginning of the process... Dealing with really sparse reward (without curriculum learning) could help to better understand if your method helps. The paper proposes a new algorithm based on REINFORCE which aims at exploring under-appreciate action sequences. The idea is to compare the probability of a sequence of actions under the current policy with the estimated reward. Actions where the current policy under-estimate the reward will provide a higher feedback, thus encouraging exploration of particular sequences of actions. The UREX model is tested on 6 algortihmic RL problems and show interesting properties in comparison to the standard regularized REINFORCE (MENT) model and to Q-Learning.\n\nThe model is interesting, well defined and well explained. As far as I know, the UREX model is an original model which will certainly be useful for the RL community. The only drawback of the paper is to restrict the evaluation of this algortihm to algorithmic problems that are specific while it would be easy to test the proposed model onto other standard RL problems. This would clearly help to make the article stronger and I greatly encourage the authors to add some other tasks in their paper. ", "title": "pre-review questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJ4hl2azl": {"type": "rebuttal", "replyto": "rkQ1n_jfx", "comment": "Thanks for the comments.  \n\nQ: It's not clear in section 6 what is the action to output a character. Is it an extra action in the action set? A different controller? \nA: In the OpenAI Gym's interface, used in the experiments, at each time step the agent needs to provide an action represented by a tuple \"(move, write, out)\". In this tuple, move is a discrete variable for the direction of the move, write is a boolean as to whether emit an output or not, and out is a discrete variable for the character to be emitted. Thus, a single action taken by the agent encapsulates both pointer movements and output emissions. We've updated the paper to make this clearer.\n\nQ: How does training speed compare for the suggested tasks? Is the convergence speed affected by the task in a similar way across models?\nA: On the tasks that both MENT and UREX solve such as Copy and DuplicatedInput, we did not observe a substantial difference in training speed. Even within a single method and a single task there is a wide range of training iterations for different random restarts, making it hard to make a definitive conclusion.  For example, on the ReversedAddition task, some runs solve the task within 20,000 steps, while others solve it in 30,000 to 40,000 steps.  However, for completeness, we have updated the paper to include plots of average reward over training for the two methods on the six tasks.\n\nQ: You hypothesize that UREX does smarter exploration. Intuitively this seems true. Is it something that you have observed experimentally? Could you reproduce e.g. Figure 3 (or a similar measure of \"exploration\") for other tasks?\nA: Because we were able to solve some difficult algorithmic tasks using UREX that were not solved just by using MENT, we concluded that UREX performs smarter exploration.\nYes, we do observe the basic behavior exhibited by Figure 3 in most of the successful trials.  We've included another plot in the paper that again shows the basic behavior of importance weights going to 0.\n\n", "title": "A few responses"}, "rkQ1n_jfx": {"type": "review", "replyto": "ryT4pvqll", "review": "-It's not clear in section 6 what is the action to output a character. Is it an extra action in the action set? A different controller? \n-How does training speed compare for the suggested tasks? Is the convergence speed affected by the task in a similar way across models?\n-You hypothesize that UREX does smarter exploration. Intuitively this seems true. Is it something that you have observed experimentally? Could you reproduce e.g. Figure 3 (or a similar measure of \"exploration\") for other tasks?\n\nThanks!\n\noverview:\nThis work proposes to link trajectory log-probabilities and rewards by defining under-appreciated rewards. This suggests that there is a linear relationship between trajectory rewards and their log-probability which can be exploited by measuring the resulting mismatch.\nThat is, when an action sequence under-appreciates its reward, its log-probability is increased.\nThis method is a simple modification to the well-known REINFORCE method, requiring only one extra hyperparameter \\tau, and intuitively provides us with a better exploration mechanism than \\epsilon-greedy or random exploration.\nThe method is tested on algorithmic environments, and compared to entropy-regularized REINFORCE and double Q-learning, and performs equally or better than those two baselines, especially in more complex environments.\n\n\nremarks:\n- the focus in the introduction on algorithmic tasks may be a double-edged sword. It is an interesting domain to test your hypothesis and benchmark your method. At the same time, it distracts the reader from the (IMO) generality of the proposed method.\n- in the introduction you say the reward is sparse, in section 6, on tasks 1-5, you then say there is a reward at each correct emission, i.e. each time step. This is only 'corrected' to end-of-episode-reward in section 7.4, after having discussed results. I'd move or mention this in section 6.\n- approach seems quite sensible to tau being in the same range as logpi(a|h), but you only try tau=0.1 for UREX. I'm not sure I understand nor agree with this experimentation choice.\n- an alternative to grid search is random search (Bergstra&Bengio, 2012). It may illustrate better hyperparameter robustness, and allow you to explore more in the same number of experiment.\n\nopinion:\n- An interesting approach to policy-gradient, to be sure. It tackles the very important question of \"how should agents explore?\"\n- I'm ambivalent to claiming that an algorithm is robust to hyperparmeters simply because it performs better on the selected hyperparameter range. All you really show is that it performs well some amount of time when the hyperparams lay in that range. Couldn't it be that MENT needs different hyperparameters? (Just being devil's advocate here)\n- I see why matching 1/tau with logpi is the obvious choice, but it implies a very strong prior: that the reward (to a factor of 1/tau) lies in the same space as the log policy. One point of failure I see (but correct me if I'm wrong) is that as the length of the trajectory grows the reward is expected to grow linearly, so short ways to get some reward will be less explored than long ways of getting the same reward, creating an imbalance unless the reward is shaped such that shorter trajectories get more reward (which is only the case in task 6).\n- It might have been good to also compare with methods explicitly trying to explore better with value-functions (e.g. prioritized experience replay, Schaul et al 2015)\n- At the risk of repeating myself, tau plays a major role in this method, but there is little analysis on its effect on experiments.\n\n\nThe methodology and reasoning is clearly explained and I think this paper communicates its message very well.\nThat message is novel, albeit a minor modification to a well-known algorithm, it is well motivated and, I think, a welcome addition to literature concerning exploration in RL.\nThe experiments are chosen accordingly, and results seem to reflect the hypothesis of the authors.\n\nI realize the tyranny of extensive experimentation and the scarcity of time, but I do think that this paper would benefit from more (or cleverer) experimentation, as well as demonstrating more explicitly the impact of the method on exploration.\nReading this paper convinced me that measuring mismatch between a trajectory's observed reward and its probability given the current policy is a clever (and well motivated) thing to do. Yet, I think that the paper could have a more convincing empirical argument, even if it is for toy tasks.", "title": "A few questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJCrPi-El": {"type": "review", "replyto": "ryT4pvqll", "review": "-It's not clear in section 6 what is the action to output a character. Is it an extra action in the action set? A different controller? \n-How does training speed compare for the suggested tasks? Is the convergence speed affected by the task in a similar way across models?\n-You hypothesize that UREX does smarter exploration. Intuitively this seems true. Is it something that you have observed experimentally? Could you reproduce e.g. Figure 3 (or a similar measure of \"exploration\") for other tasks?\n\nThanks!\n\noverview:\nThis work proposes to link trajectory log-probabilities and rewards by defining under-appreciated rewards. This suggests that there is a linear relationship between trajectory rewards and their log-probability which can be exploited by measuring the resulting mismatch.\nThat is, when an action sequence under-appreciates its reward, its log-probability is increased.\nThis method is a simple modification to the well-known REINFORCE method, requiring only one extra hyperparameter \\tau, and intuitively provides us with a better exploration mechanism than \\epsilon-greedy or random exploration.\nThe method is tested on algorithmic environments, and compared to entropy-regularized REINFORCE and double Q-learning, and performs equally or better than those two baselines, especially in more complex environments.\n\n\nremarks:\n- the focus in the introduction on algorithmic tasks may be a double-edged sword. It is an interesting domain to test your hypothesis and benchmark your method. At the same time, it distracts the reader from the (IMO) generality of the proposed method.\n- in the introduction you say the reward is sparse, in section 6, on tasks 1-5, you then say there is a reward at each correct emission, i.e. each time step. This is only 'corrected' to end-of-episode-reward in section 7.4, after having discussed results. I'd move or mention this in section 6.\n- approach seems quite sensible to tau being in the same range as logpi(a|h), but you only try tau=0.1 for UREX. I'm not sure I understand nor agree with this experimentation choice.\n- an alternative to grid search is random search (Bergstra&Bengio, 2012). It may illustrate better hyperparameter robustness, and allow you to explore more in the same number of experiment.\n\nopinion:\n- An interesting approach to policy-gradient, to be sure. It tackles the very important question of \"how should agents explore?\"\n- I'm ambivalent to claiming that an algorithm is robust to hyperparmeters simply because it performs better on the selected hyperparameter range. All you really show is that it performs well some amount of time when the hyperparams lay in that range. Couldn't it be that MENT needs different hyperparameters? (Just being devil's advocate here)\n- I see why matching 1/tau with logpi is the obvious choice, but it implies a very strong prior: that the reward (to a factor of 1/tau) lies in the same space as the log policy. One point of failure I see (but correct me if I'm wrong) is that as the length of the trajectory grows the reward is expected to grow linearly, so short ways to get some reward will be less explored than long ways of getting the same reward, creating an imbalance unless the reward is shaped such that shorter trajectories get more reward (which is only the case in task 6).\n- It might have been good to also compare with methods explicitly trying to explore better with value-functions (e.g. prioritized experience replay, Schaul et al 2015)\n- At the risk of repeating myself, tau plays a major role in this method, but there is little analysis on its effect on experiments.\n\n\nThe methodology and reasoning is clearly explained and I think this paper communicates its message very well.\nThat message is novel, albeit a minor modification to a well-known algorithm, it is well motivated and, I think, a welcome addition to literature concerning exploration in RL.\nThe experiments are chosen accordingly, and results seem to reflect the hypothesis of the authors.\n\nI realize the tyranny of extensive experimentation and the scarcity of time, but I do think that this paper would benefit from more (or cleverer) experimentation, as well as demonstrating more explicitly the impact of the method on exploration.\nReading this paper convinced me that measuring mismatch between a trajectory's observed reward and its probability given the current policy is a clever (and well motivated) thing to do. Yet, I think that the paper could have a more convincing empirical argument, even if it is for toy tasks.", "title": "A few questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}