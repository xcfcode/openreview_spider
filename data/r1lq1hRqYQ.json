{"paper": {"title": "From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following", "authors": ["Justin Fu", "Anoop Korattikara", "Sergey Levine", "Sergio Guadarrama"], "authorids": ["justinjfu@eecs.berkeley.edu", "kbanoop@google.com", "svlevine@eecs.berkeley.edu", "sguada@google.com"], "summary": "We ground language commands in a high-dimensional visual environment by learning language-conditioned rewards using inverse reinforcement learning.", "abstract": "Reinforcement learning is a promising framework for solving control problems, but its use in practical situations is hampered by the fact that reward functions are often difficult to engineer. Specifying goals and tasks for autonomous machines, such as robots, is a significant challenge: conventionally, reward functions and goal states have been used to communicate objectives. But people can communicate objectives to each other simply by describing or demonstrating them. How can we build learning algorithms that will allow us to tell machines what we want them to do? In this work, we investigate the problem of grounding language commands as reward functions using inverse reinforcement learning, and argue that language-conditioned rewards are more transferable than language-conditioned policies to new environments. We propose language-conditioned reward learning (LC-RL), which grounds language commands as a reward function represented by a deep neural network. We demonstrate that our model learns rewards that transfer to novel tasks and environments on realistic, high-dimensional visual environments with natural language commands, whereas directly learning a language-conditioned policy leads to poor performance.", "keywords": ["inverse reinforcement learning", "language grounding", "instruction following", "language-based learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper generated a lot of discussion (not all of it visible to the authors or the public). \n\nR1 initially requested reasonable comparisons, but after the authors provided a response (and new results), R1 continued to recommend rejecting the paper simply because they personally did not find the manuscript insightful. Despite several requests for clarification, we could not converge on a specific problem with the manuscript. Ungrounded gut feelings are not grounds for rejection. \n\nAfter an extensive discussion, R2 and R3 both recommend accepting the paper and the AC agrees. Paper makes interesting contributions and will be a welcome addition to the literature. "}, "review": {"ByxaK7Xt2X": {"type": "review", "replyto": "r1lq1hRqYQ", "review": "Summary:\n\nThis paper proposes learning reward functions via inverse reinforcement learning (IRL) for vision-based instruction following tasks like \"go to the cup\". The agent receives the language instruction (generated via grammar templates) and a set of four images (corresponding to four cardinal directions) from virtual cameras mounted on the agent as input at every time step and its aim is either to 1. navigate to the goal location (navigation task) or 2. move an object from one place to another (pick task). \n\nThe really interesting part in this paper is learning reward functions such that they generalize across different tasks and environments (e.g. indoor home layouts). This differentiates it from the standard IRL setting where reward functions are learnt and then policies optimized on this reward function on the *same* environment. \n\nIn order to generalize across tasks and environments a slight modification to the max-ent IRL gradient equations are made: 1. Similar to meta-learning the gradient is taken with respect to multiple tasks (in a sampling-based manner) and 2. Making the reward function a function of not just states and actions but also language context. The overall algorithm (Algorithm 1) is simple and the critical step of computing an optimal policy to compute the IRL gradient is done by assuming that one has access to full state and dynamics and essentially running a planner on the MDP. This assumption is not unreasonable since in a simulator one has access to the full dynamics and can hence one can compute the optimal trajectories by planning. \n\nExperiments are presented on the SUNCG dataset of indoor environments. Two baselines are presented: One using behavior cloning (BC) and an oracle baseline which simply regresses to the ground truth reward function which is expected to be an upper bound of performance. Then DQN is used (with and without reward shaping) using the learnt reward functions to learn policies which are shown to have better performance on different tasks. \n\nComments and Questions:\n\n- The paper is generally well-written and easy to understand. Thanks!\n\n- The idea of using IRL to learn generalizable reward functions to learn policies so as to aid transfer between environments in such vision-language navigation tasks is interesting and clearly shows benefits to behavior cloning.\n\n- One of my main concerns (and an interesting question that this paper naturally raises) is how does this approach compare to imitation learning (not vanilla behavior cloning which is straight-up supervised learning and has been theoretically and empirically shown to have worse performance due to distribution shifts. See Ross and Bagnell, 2011, Ross, Gordon, Bagnell 2012 (DAgger, Ross and Bagnell 2014 (AggreVate), Chang et al., 2015 (LOLS), etc). If the same budget of 10 demonstrations per environment is used via DAgger (where say each iteration of DAgger gets say 2 or 3 demonstrations until the budget is exhausted) how does it compare? Note online version of DAgger has already been used in similar settings in \"Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\" by Anderson et al, CVPR 2018. The main difference from their setting is that this paper considers higher level tasks instead of taking as input low-level turn-by-turn language inputs. \n\n- The following papers are relevant and should be cited and discussed:\n\"Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\" by Anderson et al, CVPR 2018.\n\n\"Embodied Question Answering\", Das et al, CVPR 2018.\n\nUpdate:\n------------\nAfter looking at other reviews and author rebuttals to all reviews I am raising my grade. \n", "title": "Comparison to Imitation Learning (not just naive BC)!", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJx-ECNJRQ": {"type": "rebuttal", "replyto": "SkgNkygonm", "comment": "Thank you for the review. We have responded to the concerns raised below by adding a comparison to another recent work that learns from raw images, clarifying the complexity of our task, and clarifying the contributions. We would appreciate it if the reviewer could revisit their review and let us know if any other concerns remain.\n\n\u201cWhat then is the implication on how their proposed IRL algorithm is designed differently? How would the algorithm of MacGlashan et al. (2015) empirically perform as compared to the authors' proposed approach?\u201d\nWe added a comparison to another work (Bahdanau et al. (2018)) in Table 1 in Section 6.2, and to a modified version of GAIL (Ho et. al 2016), to provide a point of comparison. To our knowledge, this approach is closest in terms of problem statement, in that it also operates on raw RGB images. We unfortunately cannot compare to MacGlashan et al., due to a difference in the basic problem assumptions. The work of MacGlashan et al. require predefined  notions of \u201ccolors\u201d, \u201cobjects\u201d, \u201crooms\u201d as well as semantic relationships between entities such as \u201croomIsRed\u201d or \u201cobjectInRoom\u201d to be given to the algorithm. In contrast, our method operates directly from raw text and image observations, making it easier to apply in practice without domain-specific engineering. We believe MacGlashan et. al is incomparable as it operates on significantly stronger assumptions, but we hope the baselines we have included are sufficient.\n\n\u201cI view the technical contributions of this work to be at best incremental; it does not seem to address any significant technical challenge to be able to integrate the various known tools in their work. I am not able to learn as much as i would have liked from this paper.\u201d\nAs noted by R3,  the main technical challenge we address is to show the effectiveness of transferring reward over transferring policies across environments. Most works in IRL train and test within the same environment and do not concern themselves with generalization across environments. We provide a conceptually simple approach to a well-studied and important problem (language-based instruction following) that is relatively hyperparameter stable (we have included an ablation study over architectures) and easy to train. To our knowledge no previous works have applied deep inverse reinforcement learning to instruction-following tasks. \n\n\u201cConsidering the use of deep learning... larger-scale environments/tasks, as opposed to simulated environments in this paper.\u201d\n\nPertaining to the scale of our dataset, it contains 1413 tasks, and we use 10 demonstrations per task (14130 demonstrations total). Each task on average contains ~1200 states (for a total of ~1.7m states). As a comparison \u201cVision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments\u201d Anderson et. al. uses  7,189 demonstrations. Wulfmeier et. al reports 25,000 demonstrations used, but we could not find the number of states in their tasks.\n\nSUNCG is a commonly used dataset because it contains relatively realistic scenes. It is not a trivial or toy simulated task, but is commonly used for 3D visual navigation research in complex scenes (for example, FollowNet, Shah et. al 2018,  MINOS, Savva et. al. 2017). We believe that our tasks based on SUNCG are more realistic than many prior works on instruction following.\n", "title": "Thank you for your comments"}, "Sygm9a4y0m": {"type": "rebuttal", "replyto": "rkxKz3BtnQ", "comment": "Thank you for the detailed review. We have run additional experiments as requested, and respond to the concerns raised below.\n\n\u201cFirstly, is not very clear whether the argument made in the abstract  \u201cdirectly learning a language-conditioned policy leads to a poor performance\u201d is justified or not \u201c\nThis claim is supported by our experiments against the behavioral cloning baseline. We have also added an additional comparison against DAgger (table 1, section 6.2), which is the strategy used by several previous works. For this baseline, we regress directly onto the optimal actions at every single state (which we compute using an oracle), and thus we believe this is an upper-bound on performance for a policy-based method. This baseline is stronger than supervised learning from language and demonstration pairs, as used by papers mentioned in the related work, and for methods such as DAgger and active learning, because every single state in the environment is part of the training set.\n\n\u201cIs it possible to compare against some recent work such as Tung et al 2018 or Bahdanau et al 2018? Otherwise, it is not very clear whether the proposed approach is the state-of-the-art or not.\u201d\nWe have included a baseline against Bahdanau et. al. 2018 and a variant which uses an exact policy solver, as the original Bahdanau paper uses an RL solver as an inner-loop for policy optimization, rather than planning/dynamic programming. The results are shown in Table 1 in Section 6.2. Our method achieves substantially better performance compared to Bahdanau et. al 2018 in this comparison.\n\n\u201cThirdly, using the panorama image as observation seems not a practical choice. Is it possible to provide some ablation studies or discussions on the performance over number of views? \u201c\nWe have updated section 6.1 to include discussion of the choice of panoramic images.  In our experiments, we did notice a significant degradation in performance without the panorama observations, because the agent can sometimes pick/place up objects without seeing the goal location, which confuses the reward regressor. In practice, however, a panorama image can be obtained on a robot by having multiple cameras mounted on a robot (which many robots already have). Due to time constraints, we did not have a chance to run this ablation study, but we will include it in the final version.\n\n\u201cFinally, the architecture design is not well-justified. Why not using pre-trained image classifiers (or DQN encoder) as feature extractor (or finetune the model from pre-trained image classifier)?\u201d\nWe have updated appendix A to show an ablation study over architectures. We did not find that changing the architecture led to significant performance differences, and thus we believe the architecture is not the main bottleneck in performance. We decided to pick a simple architecture which at the same time performed well.\n\n\u201cThe actual resolution (32 x 24 x 3) in the paper looks a bit unusual. \u201c\nWe did not see a performance difference using different resolutions (we tried 32x24 and 64x64) so we used smaller images for computational speed. We have included our numbers in the appendix, section A. Our resolution was based on parameters used in previous work on a similar environment (Shah et. al 2018).\n", "title": "We have included additional experiments as requested"}, "BkgTwpVkCm": {"type": "rebuttal", "replyto": "ByxaK7Xt2X", "comment": "Thank you for your comments and pointing out additional related work. We agree that the main contribution of the paper was to demonstrate the effectiveness of reward learning and its ability to transfer better across visually complex environments than other imitation learning approaches. We have also included a baseline against DAgger and additional baselines against GAIL-based approaches in table 1, section 6.2.\n\n\u201cOne of my main concerns \u2026. is how does this approach compare to imitation learning\u201d\nOur original behavioral cloning baseline (table 1, section 6.2) runs supervised learning to match the optimal policy (computed via planning/DP) at all states, and not from demonstrations. We believe this provides an upper bound on the performance of any policy-based method, as it is receiving expert advice at every state (in contrast with DAgger-based approaches which only receive this at queried states), and thus distribution shift is not a large concern. Perhaps the name \"behavioral cloning\" - we have relabeled this baseline as \u201coptimal policy cloning\u201d.\n\nWe have updated the related work section to include additional discussion of the works the reviewer has mentioned mentioned. Anderson et. al , as mentioned, concentrates on lower level tasks and policy-based learning. We believe our policy-learning baseline is the strongest baseline we could make for that class of methods and thus provides an accurate comparison, since imitation learning methods do not typically assume access to optimal actions at each state. This is unrealistic, but we believe it provides an upper bound on the performance of any direct imitation method. Das et. al. is also certainly related although they consider a different task based on question-answering. Both of these methods also adopt policy-learning approaches, rather than learning rewards.", "title": "Thank you for your constructive comments"}, "SkgNkygonm": {"type": "review", "replyto": "r1lq1hRqYQ", "review": "This paper applies IRL to the cases of multiple tasks/environments and multimodal input features involving natural language (text) and vision (images). It is interesting to see the better performance of their proposed approaches with language-conditioned rewards over language-conditioned policies. The paper is written well.\n\nI view the technical contributions of this work to be at best incremental; it does not seem to address any significant technical challenge to be able to integrate the various known tools in their work. I am not able to learn as much as i would have liked from this paper.\n\nConsidering the use of deep learning that can handle highly complex images and text, the practical significance of this work can be considerably improved by grounding their work in real-world context and/or larger-scale environments/tasks, as opposed to simulated environments in this paper. See, for example,\n\nM. Wulfmeier, D. Rao, D. Z. Wang, P. Ondruska, and I. Posner, Large-scale cost function learning for path planning using deep inverse reinforcement learning, The International Journal of Robotics Research, 2017. \n\nThe authors say that \"The work of MacGlashan et al. (2015) requires an extensively hand-designed, symbolic reward function class, whereas we use generic, differentiable function approximators that can handle arbitrary observations, including raw images.\" What then is the implication on how their proposed IRL algorithm is designed differently? How would the algorithm of MacGlashan et al. (2015) empirically perform as compared to the authors' proposed approach?\n\n\n\nMinor issues\n\nPage 2: a comparison to in Section 6 to as an oracle?\nPage 3: What is rho_0?\nPage 7: In order compare against?\nPage 7: and and indicator on?", "title": "Limited technical merit and significance", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkxKz3BtnQ": {"type": "review", "replyto": "r1lq1hRqYQ", "review": "Paper Summary: This paper studies the inverse reinforcement learning problem for language-based navigation. Given the panorama image as its observation, language embedding as its goal, a deep neural network architecture is proposed to predict the reward function from the input observation and goal. Maximum causal entropy IRL has been adopted to learn such language-conditioned reward function. This paper used the SUNCG environment for experiments and designed two tasks (navigation and pick-and-place) for evaluation.\n\n==\nNovelty & Significance:\nThis paper studies a very interesting topic in reinforcement learning and the problem has potential usage when training robot agent in the real world.\n\n==\nQuality:\nOverall, reviewer feels that the experimental results are not very strong. Some of the points are not clearly presented. \n\nFirstly, is not very clear whether the argument made in the abstract \u201cdirectly learning a language-conditioned policy leads to a poor performance\u201d is justified or not. Please clarify this point in the rebuttal.\n\nSecondly, Table 1 and Table 2 can only be treated as ablation studies. The \u201creward-regression\u201d is not a baseline but more about a oracle model. \nIs it possible to compare against some recent work such as Tung et al 2018 or Bahdanau et al 2018? Otherwise, it is not very clear whether the proposed approach is the state-of-the-art or not.\n\nThirdly, using the panorama image as observation seems not a practical choice. Is it possible to provide some ablation studies or discussions on the performance over number of views? \n\nFinally, the architecture design is not well-justified. Why not using pre-trained image classifiers (or DQN encoder) as feature extractor (or finetune the model from pre-trained image classifier)? The actual resolution (32 x 24 x 3) in the paper looks a bit unusual. \n\nOne more thing, the url provided in the paper directs to an empty project page. \n\nIf these concerns can be addressed in the rebuttal, reviewer is happy to re-evaluate (e.g., raise scores) this work.\n", "title": "Interesting paper, but results are not very convincing.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}