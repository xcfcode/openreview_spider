{"paper": {"title": "Meta Back-Translation", "authors": ["Hieu Pham", "Xinyi Wang", "Yiming Yang", "Graham Neubig"], "authorids": ["~Hieu_Pham1", "~Xinyi_Wang1", "~Yiming_Yang1", "~Graham_Neubig1"], "summary": "Use meta learning to teach the back-translation model to generate better back-translated sentences.", "abstract": "Back-translation is an effective strategy to improve the performance of Neural Machine Translation~(NMT) by generating pseudo-parallel data. However, several recent works have found that better translation quality in the pseudo-parallel data does not necessarily lead to a better final translation model, while lower-quality but diverse data often yields stronger results instead.\nIn this paper we propose a new way to generate pseudo-parallel data for back-translation that directly optimizes the final model performance.  Specifically, we propose a meta-learning framework where the back-translation model learns to match the forward-translation model's gradients on the development data with those on the pseudo-parallel data. In our evaluations in both the standard datasets WMT En-De'14 and WMT En-Fr'14, as well as a multilingual translation setting, our method leads to significant improvements over strong baselines. ", "keywords": ["meta learning", "machine translation", "back translation"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes a meta-learning-based technique to learn how to back-translate (generate a synthetic source-language translation of an observed target-language sentence) for the purpose of better optimising a source-to-target translation model. \n\nThe approach is an interesting novel angle to jointly training the translation model and the back-translation component. Compared to techniques like UNMT and DualNMT, the approach offers reduced training time and a simpler formulation with fewer trainable components (and fewer hyperparameters). \n\nDuring the discussion phase the authors provided additional insight, clarifications, and results that improved our perception of the paper. I would personally appreciate if the authors would update their paper with the clarifications they made to points raised by R2, R3, and R4, especially on the details about meta-validation, the discussion about memory footprint, and the additional results on UNMT (and variants). "}, "review": {"mCXrEu0Shv5": {"type": "rebuttal", "replyto": "xJNkSBrOSDR", "comment": "Thank you for the positive comment and for increasing the score. It means a lot to us.\n\nWe will take some time to try your suggestion on pre-training and continual training. Specifically, we'll pre-train a shared encoder and a decoder into both languages until convergence, and then we will continue to train on the denoising objective and the two-round back-translation objective.\n\nPlease note that pre-training a shared encoder translation model in both directions and then continuing to train the converged model with both the denoising objective and the two-round back-translation objective will lead to a significantly slower training time compared to UNMT, which is already slower than MetaBT.\n\nWe will update you about the result when we get it.", "title": "Let's try pre-training"}, "quN69SIwL6K": {"type": "review", "replyto": "3jjmdp7Hha", "review": "The paper proposed an interesting approach for back-translation. The idea is to update both forward model and backward models during training. The forward model is updated in a standard way using synthetic samples generated from the backward model. The backward model is updated using gradient of the forward model on meta-validation dataset (i.e., parallel data). Evaluation shows that Meta BT performs better than offline BT ((Edunov et al., 2018)  in both multilingual setup and standard  en-fr, en-de translation direction. In general, I find the approach is nice.\nWhile the paper compared Meta-BT with offline BT (i.e., MLE as mentioned in the paper) and DualNMT, I think these two baselines are not sufficient to verify the claims made by the paper.\n\nThe paper claims that MetaBT allows to update the backward model unlike offline-BT and avoids expense of multiple iterations in iterative BT. Note that back-translation can be done on-the-fly (Artetxe et al., 2018, Conneau and Lample, 2019). Online back translation allows updating both forward and backward models using monolingual data during training. Thus, I think online BT should be a baseline for appropriate comparison. While MetaBT avoids multiple iterations of iterative BT, the evidence is not provided in the paper in terms of training time for MetaBT. The additional complexity of MetaBT lies in the update of the backward model per mini-batch and the computation of the Jacobian-vector products. As the author already mentioned that MetaBT has a large memory footprint, thus it\u2019s slower to do one update in MetaBT. I wonder how MetaBT performs in comparison with iterative BT with 2 iterations with respect to BLEU scores and training time.\n\nCan the authors provide similar plots in Figure 2 for En-De and in Figure 3 for En-Fr? With respect to MetaBT avoids overfitting. I think it would be nice to have some analysis on the samples generated by the backward model. In comparison to offline BT, does the backward model in MetaBT generate more diverse output?\n\nWith respect to the presentation of the paper, I think Figure 1 is a bit confusing to read. I was hoping to get the main idea from Figure 1 but it didn\u2019t help at all.\n\n\n\n\n**References**\n\nMikel Artetxe, Gorka Labaka, Eneko Agirre, Kyunghyun Cho. Unsupervised Neural Machine Translation. ICLR 2018. \nGuillaume Lample, Alexis Conneau. Cross-lingual Language Model Pretraining. NeurIPS 2019\n\n\n**Post-response update**\nThanks authors for extra effort on semi-supervised experiments. I decided to increase the score to 6. \n\n", "title": "Missing some baselines", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "CjcN8lbVvvF": {"type": "rebuttal", "replyto": "FMrasOOJVZi", "comment": "Your interpretations of the **bolded arrows** are all correct. We are very grateful that you engage in discussing with us, even on the details :-)\n\nJust to make everything crystal clear, in your comment you wrote *\"both forward and backward are **first** trained on parallel data\"*. The word *\"**first**\"* should refer to the updating order *in a training iteration*, and not that the forward and backward models are first pre-trained to convergence on parallel data and then are trained on monolingual data. This follows the UNMT paper's descriptions. Specifically, at the end of Page 7: *\"their training **alternates** between denoising, backtranslation and, additionally, maximizing the translation probability of these parallel sentences\"*", "title": "Your interpretations are correct"}, "piOW1BnmBHd": {"type": "rebuttal", "replyto": "3jjmdp7Hha", "comment": "Thanks the reviewers for giving us a lot of good feedback. We hope the rebuttal addressed your concerns about our paper, and we would love to answer and discuss any other questions you have!", "title": "We'd like to address any more questions about the paper!"}, "2aRXecuN6Np": {"type": "review", "replyto": "3jjmdp7Hha", "review": "summary:\n\nThis paper applies techniques from meta-learning to derive and end-to-end update rule for a workflow involving backtranslation, specificically maximizing translation performance of the forward model, while updating the backward model to produce backtranslations that are maximally useful to improve the forward model's quality (as measured on a meta validation set). The approach is evaluated on WMT EN-DE and EN-FR and compared against a simple sampling strategy for backtranslation, and dual learning. In addition, the paper considers a multilingual setup where the translation direction is low-resource, and the initial backtranslation model is trained on a mix of parallel data from the language pair of interest, as well as auxiliary data with a high-resource, related source language. \n\nstrengths:\n\n+ the idea of end-to-end optimization of the backward model to maximally benefit training of the forward model is novel and interesting.\n\n+ the mathematical derivation of the objective function is sound.\n\nweaknesses:\n\n- the empirical evaluation is not very convincing. Important information is missing, baselines are inexplicably weak, and some other simple baselines are missing. Specifically:\n\n -- what data sets do you use for meta validation? Is it one (or several) of the newstest sets? Since you're actually learning gradients on this data set, rather than just using it for early stopping or hyperparameter choice, I think you should consider using that data directly for training as one of the baselines. For example, Oravecz et al. (2019) report a 1.5-2 BLEU gain from fine-tuning on newstest2008-2017 for EN-DE in their submission to WMT.\n\n -- for the Edunov et al. 2018 baseline (sampling for back-translation), there is a gap of >6 BLEU between the best result they report (35 BLEU) and yours (28.73 BLEU). Some of this may be explainable by the fact that you use Transformer base rather than Transformer large, but even relatively speaking, Edunov et al. (2018) observe an improvement of ~4 BLEU with BT, while this paper only reports ~2. Authors discuss memory limitations as the reason why the did not train on Transformer Big, but they could conceivably use gradient accumulation to enable training even on limited hardware. Other reasons for the score discrepancy deserve discussion (for example, are BLEU scores reported tokenized or not?)\n \n -- please provide more detail how the high-resource data was used for the various baselines. You mention that you follow settings from previous work (Neubig & Hu, 2018; Wang et al., 2019a), but these actually use different techniques. I gather that you use some type (which?) of transfer learning for the forward models. Do you also apply these techniques to the backward models, and if so, is the initial model for meta back-translation initialized differently?\n  \n -- the dual learning baseline bears some conceptual similarity to the proposed objective in that both backward and forward model are continuously improved. I'm surprised to see that it leads to worse performance than the baseline trained on parallel data only, but there is too little information to build trust in this result. Which language model was used for the additional rewards? What beam size (or sample size) was used for the backtranslation? What monolingual data was used, only target-languge data or also source-language data? If the latter, which data was used?\n \n- I find the title problematic. I can see how it was coined as a combination of meta-learning and backtranslation, but it's misleading to a reader who doesn't know this intention. If we take \"meta\" to roughly express some self-referentiality (meta-learning is learning to learn; a meta-analysis is an analysis of analyses), is meta-backtranslation the backtranslation of backtranslations?\n\nrecommendation:\n\nOverall, I vote for rejection. I like the core idea and could see this work being published eventually, but feel that the empirical evaluation needs to be strengthened before I would recommend acceptance. There are open questions about the evaluation setup, and if authors can answer these questions, this will let me better judge the empirical rigour.\n\ntypos:\n\nmulltilingual -> multilingual\n\n\n**post-response update**:\n\nsome of my concerns about the evaluation were resolved in the author response (for example regarding the meta-validation datasets) , and I have slightly increased my rating.", "title": "interesting idea; concerns about evaluation", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "FcA5kiIgF3N": {"type": "rebuttal", "replyto": "quN69SIwL6K", "comment": "We have finished our experiments with UNMT. In particular, we trained a Transformer-Base model to translate En->De. We use the Semi-supervised mode of UNMT which was described at the end of Page 7 in Artetxe et al., (2018). We let the model learn on all parallel data in WMT\u201914 En-De. Additionally, we use the same amount of monolingual data in English for the denoising objective and the on-the-fly back-translation objective.\n\nWe attain a BLEU score of 28.76 for WMT\u201914 En->De which is in the same ballpark with back-translation using sampling (SampleBT, 28.73 from Table 1 in our paper).\n\nMost importantly, this indicates that MetaBT achieves +1.63 BLEU higher than UNMT, even though UNMT uses monolingual English data while MetaBT does not. We will continue to analyze this result going forward towards the final version of the paper, but we believe that at least for now our result demonstrates MetaBT, whose sole purpose is to improve the quality of the forward translation model En->De, is being successful in this endeavor.\n\nThere are also a few auxiliary negative points about UNMT compared to MetaBT:\n* First, UNMT needs 2 rounds of back-translation, 2 rounds of denoising, and 1 round of supervised training, specifically:\nnoisy En **-> En**\nnoisy De **-> De**\nmono En -> infer De **-> mono En**\nmono De -> infer En **-> mono De**\nparallel En **-> parallel De**\nAll the **bolded arrows** are trained. Compared to UNMT, MetaBT only has:\nparallel En **-> parallel De**\nmono De -> **infer En -> mono De**\n\n* Second, we measured the step time of two methods and found that for the same batch size, UNMT is 1.4x slower than MetaBT, even though MetaBT keeps two models in memory and has to perform two back-propagation passes.", "title": "Comparison with Unsupervised NMT"}, "tg7gdd2n3_E": {"type": "rebuttal", "replyto": "quN69SIwL6K", "comment": "We thank R3 for the comments and the suggestions for the baselines.\n\n**[Comparing to On-the-fly Back-translation]** Of the two baselines R3 suggested, we find that:\n\n- Cross-lingual Language Model Pretraining (CLM; Lample and Conneau, 2019) uses a pre-training step which is not used in MetaBT. Pre-training is certainly a reasonable method for using available data, but it is different in nature than back-translation methods such as MetaBT, so we decided to focus our experimental comparison on the most related techniques (which are also widely used in state-of-the-art translation systems). We also hypothesize that due to the different nature of these techniques the gains from them are likely to stack to some extent. Thus, we did not compare against CLM.\n- Unsupervised Neural Machine Translation (UNMT; Artetxe et al., 2018). We are implementing UNMT to compare with MetaBT on WMT\u201914 EnDe and EnFr, using the same Transformer-Base model and training data with MetaBT. We are working on running this experiment, and will try to update you with the results in a few days if the experiments finish within the time frame for authors/reviewers discussion (apologies if they do not -- the time frame is a bit tight).\n\n**[Training time]** Using 128 TPUv2 with the Transformer-Base model, our implementation MetaBT takes about 20 hours to train from scratch to convergence. In comparison, on the same hardware, the same model takes 18 hours to train on WMT-EnDe + monolingual BT (it takes 4 hours to train on only the WMT EnDe parallel data, but obviously the results are much worse). Therefore, as we wrote in the paper, MetaBT takes a bit longer than one iteration of back-translation, but is faster than two iterations of iterative back-translation.\n\n**[Extra Plots about MetaBT\u2019s Behaviors on WMT EnDe & EnFr]** We provide the extra plots here. We will add them to the final version of the paper. Please find the figures here:\n* Figure 2 for en-de: https://pasteboard.co/JA1P6uz.png\n* Figure 3 for en-fr: https://pasteboard.co/JA1Q0e0.png and https://pasteboard.co/JA1Qi4C.png\n\n**[Does the backward model in MetaBT generate more diverse outputs?]** Yes. We have presented this analysis for our multilingual experiments in Fig. 4. The figure shows that MetaBT does learn to generate outputs that have more words from the low-resource language. These outputs are more diverse than that of the vanilla backward model, which mostly generates sentences from the high-resource language.\n\n*References*\n\nMikel Artetxe, Gorka Labaka, Eneko Agirre, Kyunghyun Cho. Unsupervised Neural Machine Translation. ICLR 2018.\n\nGuillaume Lample, Alexis Conneau. Cross-lingual Language Model Pretraining. NeurIPS 2019.\n", "title": "Response to reviewer 3"}, "5z7TfVWMRe": {"type": "rebuttal", "replyto": "2aRXecuN6Np", "comment": "We thank R4 for raising the detailed review, and clarifications about the missing details in the paper. Please find the answers to your questions below. We will include all these missing details in the future revision of our paper.\n\n**[Meta-validation datasets]** We remove 3,000 pairs of parallel training data from WMT EnDe / EnFr training datasets to use as the meta-validation set. For our baselines, we do not withhold these 3,000 pairs of parallel sentences, and so our baselines already use this data. This makes the comparison fair, as R4 suggested.\n\n**[Comments on Results for SamplingBT]** Our BLEU scores are calculated tokenized using moses tokenizer; we will clarify this in the paper. We suspect that the gap between NoBT and SampleBT baseline (MLE in the paper) is smaller than observed by Edunov et al. (2018) because of the small capacity of the Transformer-Base model. Perhaps a larger model is required to fit the 260M sentences in the German monolingual dataset that we and Edunov et al. (2018) both use.\n\nAnother piece of evidence in our paper that supports this theory about small capacity is the fact that this improvement becomes smaller for our WMT EnFr experiments. Since the WMT EnFr dataset is larger than WMT EnDe, our small model becomes more underfit.\n\nUsing small models does not invalidate our results, as we fairly use the same setting for all of our baselines. That said, we have acknowledged in the paper that somewhat larger memory footprint is currently a downside of MetaBT. In the future, we hope to alleviate this issue with better techniques, and it is also likely that the development of better hardware will reduce the issues somewhat as well.\n\n**[Use Gradient Accumulation to Train Larger Models]** Thank you for the insightful suggestion. In fact, we have thought about gradient accumulation when working on the paper. That said, we found that gradient accumulation is not trivial to use together with auto-differentiation to update the backward model, but we will certainly consider it for future experiments. \n\nSpecifically, to accumulate gradients from multiple steps, in our framework TensorFlow, we need to use a tf.while_loop and need to aggregate the intermediate gradients in a memory block. Auto-differentiation computations apparently cannot propagate through this aggregation process, resulting in a None gradient for the backward model. A more viable approach for us would be model parallelism, but the technique is still not quite ready at the moment.\n\n**[More details on the Neubig & Hu. (2018) and Wang et al. (2019a)]** We are sorry about the confusion here. We mostly follow Neubig & Hu (2018)\u2019s most simple method (the \u201cBi\u201d method) of simply using the concatenation of low-resource language and its most related high-resource language for training both the forward and the backward model. We will clarify this in a revised version of the paper. \n\n**[Comments on DualNMT]** We were surprised that DualNMT\u2019s results ended up being very weak too. To provide more details:\n- For WMT EnDe/EnFr, we use a separate Language Model (LM) for each language English, German, and French. These LMs are 6-layered Transformer-Base models, similar to the encoder of our translation Transformers. Each LM is pre-trained on the corresponding language\u2019s monolingual data, for which we use the Newscrawl data between 2012-2018. All LMs use the same SentencePiece tokenization with the translation models (note that this means that we have different English LMs for WMT EnDe and WMT EnFr). During DualNMT, we use a sample size of 1, due to memory constraints.\n- We found DualNMT to be very slow and memory inefficient. In particular, DualNMT requires that we store in memory two translation models (one for each direction) plus two LMs (one for each language). The interactions between these models also require their intermediate computations to be kept in memory. As a result, we needed to halve the global batch size compared to the other methods.\n- DualNMT can perhaps be improved if we increase the LMs\u2019 capacity. We suspect so, because we observe that our LMs are still underfitting the monolingual training data (their training PPL is in the ballpark of 50). However, larger LMs will not fit in memory together with two translation models.\n- For our multilingual low-resource experiments, we use the same BT sampling setup for both DualNMT and MetaBT to ensure the fairness of comparison. The LMs are 6-layered Transformer-Base models, and they are pretrained on the monolingual data from the low-resource language. We suspect that DualNMT does not perform well in this setting because the LMs are not trained on enough data.\n\n**[Title]** We thank R4 for understanding the intentions behind our title and method name. For better clarity, in our revision, we will extend the title to better reflect MetaBT\u2019s intention. For example: \u201cMeta Back-Translation: Generating Effective Pseudo-Parallel data with Back-Translation and Meta-Learning\u201d.", "title": "Response to reviewer 4"}, "oOOJaK7K6UO": {"type": "rebuttal", "replyto": "wizsFpQYH8L", "comment": "We thank R1 for the overall positive comments and suggestions.\n\n**[Clarity of motivation]** To reiterate the introduction somewhat, the main goal of our method is to directly optimize the backward model so that it produces data that are most effective to train the forward model. Because of this, it is a bit of a mischaracterization that our method is attempting to improve the quality of the backward model itself, but rather we are trying to improve the positive effect that these translations have on improving training of the forward model. Because of this, the only true measure of whether our model has succeeded is if the forward translation quality improves, and our experimental results demonstrate that this is the case. This is a new way of thinking about back translation and proposing this new way of thinking about back translation is one of the major contributions of this paper. As a corollary to this, we are also not aware of any relevant citations that assess back-translation quality from this standpoint, but if you can point out any particularly relevant ones we would be happy to cite them.  \n\n**[Analysis of how the BT model is improved]** While forward translation model improvements are the only true indicator of whether our proposed method has succeeded, Figure 4 does show one piece of analysis: MetaBT learns to generate sentences that are closer to the low-resource language, although the initial backward model is trained to mostly generate the related high-resource language. This could also be viewed as one variety of \u201cdomain shift\u201d as mentioned by in the review, if we view the two different input languages as different domains.\n", "title": "Response to reviewer 1"}, "hK_IDXiM07": {"type": "rebuttal", "replyto": "x1IcJIMoO4d", "comment": "We thank R2 for the overall positive comments and suggestions.\n\n**[Meta-validation datasets]** We removed 3,000 pairs of parallel training data from WMT EnDe / EnFr training datasets to use as the meta-validation set. Note that for our baselines, we do not withhold these 3,000 pairs of parallel sentences, and so our baselines already use this data. This makes the comparison fair, as R4 suggested.\n\nWe thank the reviewer for pointing out the extra references. We will include them as well as the Xia et al. paper in the related work.\n", "title": "Response to reviewer 2"}, "x1IcJIMoO4d": {"type": "review", "replyto": "3jjmdp7Hha", "review": "The paper describes a method to improve NMT training with backtranslation.\n\nRather than using a fixed t->s model to translate target monolingual data in order to augment the training set for the s->t model, the proposed approach first pretrains the t->s model as usual then jointly trains it with the forward s->t model using a meta-learning approach: the s->t model is trained on the syntetic backtranslated data and a \"meta-validation\" loss is computed on a paralled dataset, which is used to update the t->s model using REINFORCE.\nThe approach is similar to the DualNMT model by Xia et al, but rather than updating on monolingual data based on LM and reconstruction scores, it uses a reward based on the cross-entropy on parallel data.\nThe paper also proposes a way to adapt this method to a multi-lingual setting.\n\nExperiments are performed on WMT-14 En->De and En->Fr, and on 4 IWSLT-2018 language pairs. The authors report small but consistent improvements. Additional analyses are also reported.\n\nOverall the method seems valid, although it is described at a very high level and no code release is mentioned. In my experience successfully implementing RL-based method is strongly dependent on getting hyperparameters and implementation details right, so it could be hard to reproduce this work without the code or a more detailed description. Also it's not entirely clear what is being used as \"meta-validation\" data here, I suppose it's all the parallel training data, but the paper doesn't make it clear.\n\nMinor issues: the \"Tagged backtranslation\" paper by Caswell et al. 2019 contrasts the claim that improvements with sampling backtranslation are due to increased diversity. It should be referenced as relevant work. The Xia et al., 2016 Dual NMT paper is referenced multiple times in the text but not in the bibliography section", "title": "Review", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "wizsFpQYH8L": {"type": "review", "replyto": "3jjmdp7Hha", "review": "The paper presents an extension of the back-translation method which provides a means of leveraging monolingual data in NMT where the quality of the data generated by the back-translation model is controlled through a meta learning regime that trains the BT model jointly with the actual translation model.\nThe method is an interesting application of meta-learning to NMT and worth seeing the results. It is a well-written paper with a sound description of the method and evaluation. The motivation is the main weakness and a better discussion and comparison to related work would help clarify the applicability of the method.\n\nComments\n- The motivation for the method is supported by two claims, the first one related to the upper bound on quality of the BT model trained on the parallel data, and the second on the quality of the pseudo-parallel data generated by the BT model used then to train the actual NMT model together with the original parallel data. There is on the other hand not any empirical support in either claim that these create a weakness, thus, grounding the motivation for the paper. \nIn the former case, the method does not really modify in any matter the quality of the original parallel data, hence the discussion is irrelevant. In the latter case, especially, when using the pseudo-parallel data, there are many factors not discussed that affect the quality of the pseudo-parallel data generated by the BT model other the diversity, such as source domain drift and eg. generation of translationese. It is indeed limited the literature on this topic although doesn't validate that any claim should be supported either by references or within the study included in the paper. The second reference in this context is referred to as slow and expensive but isn't the meta learning also making the model much slower and more costly in a similar sense?\n- Can the authors analyze or discuss how well the backward model is improving and how does it distinguish from the previous methods?\n", "title": "An interesting application of meta-learning in NMT, but with an unclear motivation", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}