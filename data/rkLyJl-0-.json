{"paper": {"title": "Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks", "authors": ["Shankar Krishnan", "Ying Xiao", "Rif. A. Saurous"], "authorids": ["skrishnan@google.com", "yingxiao@google.com", "rif@google.com"], "summary": "We describe a practical optimization algorithm for deep neural networks that works faster and generates better models compared to widely used algorithms.", "abstract": "Progress in deep learning is slowed by the days or weeks it takes to train large models. The natural solution of using more hardware is limited by diminishing returns, and leads to inefficient use of additional resources. In this paper, we present a large batch, stochastic optimization algorithm that is both faster than widely used algorithms for fixed amounts of computation, and also scales up substantially better as more computational resources become available. Our algorithm implicitly computes the inverse Hessian of each mini-batch to produce descent directions; we do so without either an explicit approximation to the Hessian or Hessian-vector products. We demonstrate the effectiveness of our algorithm by successfully training large ImageNet models (InceptionV3, ResnetV1-50, ResnetV1-101 and InceptionResnetV2) with mini-batch sizes of up to 32000 with no loss in validation error relative to current baselines, and no increase in the total number of steps. At smaller mini-batch sizes, our optimizer improves the validation error in these models by 0.8-0.9\\%. Alternatively, we can trade off this accuracy to reduce the number of training steps needed by roughly 10-30\\%. Our work is practical and easily usable by others -- only one hyperparameter (learning rate) needs tuning, and furthermore, the algorithm is as computationally cheap as the commonly used Adam optimizer.", "keywords": ["Deep Learning", "Optimization"]}, "meta": {"decision": "Accept (Poster)", "comment": "Pros:\n+ Clearly written paper.\n+ Easily implemented algorithm that appears to have excellent scaling properties and can even improve on validation error in some cases.\n+ Thorough evaluation against the state of the art.\n\nCons:\n- No theoretical guarantees for the algorithm.\n\nThis paper belongs in ICLR if there is enough space.\n"}, "review": {"ByPYAMtgG": {"type": "review", "replyto": "rkLyJl-0-", "review": "The paper proposes a new algorithm, where they claim to use Hessian implicitly and are using a motivation from power-series. In general, I like the paper.\n\nTo me, Algorithm 1 looks like some kind of proximal-point type algorithm. Algorithm 2 is more heuristic approach, with a couple of parameters to tune it.  Given the fact that there is convergence analysis or similar theoretical results, I would expect to have much more numerical experiments. E.g. there is no results of Algorithm 1. I know it serves as a motivation, but it would be nice to see how it works.\n\nOtherwise, the paper is clearly written.\nThe topic is important, but I am a bit afraid of significance. One thing what I do not understand is, that why they did not compare with Adam? (they mention Adam algorithm soo many times, that it should be compared to).\n\nI am also not sure, how sensitive the results are for different datasets? Algorithm 2 really needs so many parameters (not just learning rate). How \\alpha, \\beta, \\gamma, \\mu, \\eta, K influence the speed? how sensitive is the algorithm for different choices of those parameters?\n\n\n ", "title": "[UPDATED] Interesting idea, nice experiments, good motivation but lacks theoretical understanding", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hy5t5WIxG": {"type": "review", "replyto": "rkLyJl-0-", "review": " \nThis paper presents a new 2nd-order algorithm that implicitly uses curvature information, and it shows the intuition behind the approximation schemes in the algorithms and also validates the heuristics in various experiments.  The method involves using Neumann Series and Richardson iteration to avoid Hessian-vector product in second order method for NN.  In the actual performance, the paper presents both practical efficiency and better generalization error in different deep neural networks for image classification tasks, and the authors also show differences according to different settings, e.g., Batch Size, Regularization.  The numerical examples are relatively clear and easy to figure out details.\n\n1. While the paper presents the algorithm as an optimization algorithm, although it gets better learning performance, it would be interesting to see how well it is as an optimizer.  For example, one simple experiment would be showing how it works for convex problems, e.g., logistic regression.  Realistic DNN systems are very complex, and evaluating the method in a simple setting would help a lot in determining what if anything is novel about the method.\n\n2. Also, for deep learning problems, it would be more convincing to see how different initialization can affect the performances. \n\n3. Although the authors present their algorithm as a second order method at beginning, the final algorithm is kind of like a complex momentum SGD with limited memory.  Rather than simply throwing out a new method with a new name, it would be helpful to understand what the steps of this method are implicitly doing.  Please explain more about this.\n\n4. It said that the algorithm is hyperparameter free except for learning rate.  However, it is hard to see why there is no need to tune other hyperparameters, e.g., Cubic Regularizer, Repulsive Regularizer.  The effect/sensitivity of hyperparameters for second order methods are quite different than hyperparameters for first order methods, and it is of interest to know how hyperparameters for implicit second order methods perform.\n\n5. For Section 4.2, the well know benefit by using large batch size to train models is that it could reduce training time and epochs.  However, from Table 3, there is no such phenomenon.  Please explain.\n\n", "title": "See below.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BkEmOSvef": {"type": "review", "replyto": "rkLyJl-0-", "review": "Summary: \nThe paper proposes Neumman optimizer, which makes some adjustments to the idealized Neumman algorithm to improve performance and stability in training. The paper also provides the effectiveness of the algorithm by training ImageNet models (Inception-V3, Resnet-50, Resnet-101, and Inception-Resnet-V2). \n \nComments:\nI really appreciate the author(s) by providing experiments using real models on the ImageNet dataset. The algorithm seems to be easily used in practice. \n\nI do not have many comments for this paper since it focuses only in practical view without theory guarantee rigorously. \n\nAs you mention in the paper that the algorithm uses the same amount of computation and memory as Adam optimizer, but could you please provide the reason why you only compare Neumann Optimizer with Baseline RMSProp but not with Adam? As we know, Adam is currently very well-known algorithm to train DNN. Do you think it would be interesting if you could compare the efficiency of Neumann optimizer with Adam? I understand that you are trying to improve the existing results with their optimizer, but this paper also introduces new algorithm.  \n\nThe question is that, with the given architectures and dataset, what algorithm should people consider to use between Neumann optimizer and Adam? Why should people use Neumann optimizer but not Adam, which is already very well-known? If Neumann optimizer can surpass Adam on ImageNet, I think your algorithm will be widely used after being published.  \n \nMinor comments:\nPage 3, in eq. (3): missing \u201c-\u201c sign\nPage 3, in eq. (6): missing \u201ctranspose\u201d on \\nabla \\hat{f}\nPage 4, first equation: O(|| \\eta*mu_t ||^2)\nPage 5, in eq. (9): m_{k-1}\n", "title": "Neumann Optimizer: A Practical Optimization Algorithm for Deep Neural Networks", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "ByNx4IpXf": {"type": "rebuttal", "replyto": "rkLyJl-0-", "comment": "We thank all the reviewers for their feedback. Before we address individual comments, we would like to mention some key themes in this paper that seem to have been lost mainly due to our presentation in the experiments section. \n\n(1) Training deep nets fast (in wall time/parameter updates) without affecting validation performance is important. Previous attempts to scale up, using large batch size and parallelization, hit limits which we avoid. For example, using 500 workers computing gradients in parallel, we can train Resnet-V1-50 to 76.5% accuracy in a little less than 2 hours. In contrast, in Goyal et al. \u201cAccurate, large minibatch SGD: Training Imagenet in 1 hour.\u201d, the maximum batch size was 8000 (equivalent to 250 workers), and in You et al. \u201cScaling SGD batch size to 32k for imagenet training\u201d, there is a substantial 0.4-0.7% degradation in final model performance.\n\n(2) Our method actually achieves better validation performance (~1% better) compared to the published best performance on image models in multiple architectures.\n", "title": "Overall response to reviewer's comments"}, "Hkg9QITXG": {"type": "rebuttal", "replyto": "rkLyJl-0-", "comment": "We have made the following changes in the new version of the paper that we uploaded. \n\nWe have added some new experiments \n\n(1) Comparison to Adam in Figure 1,\n(2) Multiple Initializations in Appendix D, and \n(3) A Stochastic Convex Problem in Appendix B, along with small edits suggested by reviewers.\n", "title": "Changes made to the updated paper"}, "BJXgmLamM": {"type": "rebuttal", "replyto": "HJ1hYPI0-", "comment": "Thanks for your interest and detailed feedback, Boris. We\u2019ve incorporated most of your feedback, and hope to answer some of your questions below:\n\n1. We\u2019ve added the small calculation for this in Section 3.1.\t\n\n2. A couple of things are going on here:\ni) We allow the mini-batch to vary in algorithm 2. This is a pretty significant change (we like to think of solving a stochastic bootstrap style subproblem instead of deterministic ones).\nii) We change the notation to offset the w_t (so that w_t in Algorithm 2 actually correspond to w_t + \\mu m_t in Algorithm 1). This is a pure notational change, and has no effect on the iteration -- we could also have done the same thing for Algorithm 1 (i.e., we could have unrolled m_t as the sum of gradients).\n\n3. It seems somewhat insensitive to period of the resets, but the resets are necessary, especially at the start.\n\n4. The coefficients we have for m_{t-1} and d_t aren\u2019t a convex combination, and additionally, we subtract an extra \\eta d_t from the update in Line 11 (this subtraction is correct, and somewhat surprising...you accidentally identified it as a typo below). It\u2019s somewhat difficult to reinterpret as momentum.\n\n5. We have not tried on large models without batch normalization. Since most convolutional architectures include batch norm, we had not thought to have experiments along this axis.\n6. By default, all the models we used include weight decay -- so Figure 5 (the ablation experiments) should give you an idea of what happens if you use weight decay and not cubic + repulsive.\n\nTypos:\nWe have all except (5) and (6) -- thank you! (5) and (6) are actually correct -- it definitely looks a little strange, but what we\u2019ve done is to keep track of offset variables w_t + \\mu m_t.\n", "title": "Combined response to Boris Ginsburg's comments"}, "SkFuMIpXM": {"type": "rebuttal", "replyto": "Hy5t5WIxG", "comment": "Thank you AnonReviewer3 for your thoughts and comments: we address your comments below and hope to clear up one misconception (caused by poor labelling of Table 3):\n\n1. We have added an experiment in Appendix B to show the results on a synthetic logistic regression problem. We compared the Neumann optimizer with SGD, Adam and a Newton algorithm for varying batch sizes. Our method outperforms SGD and Adam consistently, and while Newton\u2019s method descends to a better loss, it comes at a steep per-step cost. We believe there are other large batch methods like Nesterov and SVRG that might get to lower losses than our method. However, none of these algorithms perform well on training a deep neural net. \n\n2. We've included an Appendix D with a new experiment illustrating that different initializations and trajectories of optimization all give the same quality model output (for the Inception V3 model).\n\n3. We're not quite sure what the reviewer is looking for here: it seems that Section 2 gives a derivation of the method: the method is implicitly inverting the Hessian (which is convexified after regularization) of a mini-batch. Our algorithm crucially differs from standard momentum in that gradient evaluation occurs at a different point from the current iterate (in Algorithm 1), and we are not applying an exponential decay (a standard momentum update would blow up if you did this).\n\n4. We agree that it is of interest to further study the sensitivity to hyperparameters. The results that we have hold for ImageNet, but also for CIFAR-10 and CIFAR-100 with no change in hyperparameters, so we think that the results are likely to carry over to most modern CNN architectures on image datasets -- the hyperparameter choice will likely work out of the box (much like the beta_1, beta_2 and epsilon parameters in Adam). We agree that there appears to be quite a few hyperparameters, but \\alpha and \\beta are regularization coefficients, so they have to be roughly scaled to the loss; \\gamma is a moving average coefficient and never needs to be changed; \\mu is dependent only on time, not the model; finally, training is quite insensitive to K (as mentioned in Section 3.2). Thus, the only hyperparameter that needs to be specified is the learning rate \\eta, and that does determine the speed of optimization.\n\n5. The epochs listed in Table 3 are total epochs (i.e., total sum of all samples seen by all workers), so using twice as many workers is in fact twice as fast (we've updated the table to clarify this). We're a little concerned that we were not clear on the significance of the experimental results: our algorithm scales up to a batch size of 32000 (beating state-of-the-art for large batch training), and we obtain linear speedups across this regime i.e., we can run 500 workers, in 1/10th the time that it takes the usual 50 worker baseline. We think of this as the major contribution of our work.\n", "title": "Combined response to AnonReviewer3's comments"}, "ByNdWUp7M": {"type": "rebuttal", "replyto": "ByPYAMtgG", "comment": "Thank you AnonReviewer2 for your comments. Here are our responses:\n\nWe have added a number of new experiments, including (1) Solving a stochastic convex optimization problem (where the Neumann optimizer is far better than SGD or Adam), (2) Comparisons with Adam on Inception-V3 (see below) and (3) Multiple runs of the Neumann algorithm on Inception-V3 showing that the previous experiments are reproducible.\n\nTo the comment about running Algorithm 1: we\u2019ve run it on stochastic convex problems before, where it performs much better than either SGD or Adam. On deep neural nets, our earlier experience with similar \u201ctwo-loop\u201d algorithms (i.e., freeze the mini-batch, and perform substantial inner-loop computation) lead us to the conclusion that Algorithm 1 would most likely not perform very well at training deep neural nets. The main difficulty is that the inner loop iterations \u201coverfit\u201d to the mini-batch. As you mentioned, this is meant to purely motivational for Algorithm 2. \n\nAdam achieves similar (or worse) results to the RMSprop baselines (Figure 1): in comparison to our Neumann optimizer, the training is slower, the output model is lower quality, and the optimizer scales poorly. When training with Adam, we observed instability with default parameters (especially, epsilon). We changed it to 0.01 and 1.0 and have two runs which show dramatically different results. Our initial reason for not including comparisons to Adam was that we wanted to use standard models and training parameters (i.e., the Inception and Resnet papers use RMSprop).\n\nWe think that the significance in our paper lies in the strong experimental results:\n1. Significantly improved accuracy in output models (using a small number of workers) over published baselines -- i.e., just switching over to our optimizer will increase accuracy by 0.8-0.9%.\n2. Excellent scaling behaviour (even using a very large number of workers).\nFor example, our experimental results for (2) are strictly stronger than those in the literature for large batch training.\n\nThe results that we have hold for ImageNet, but also for CIFAR-10 and CIFAR-100 with no change in hyperparameters, so we think that the results are likely to carry over to most modern CNN architectures on image datasets -- the hyperparameter choice will likely work out of the box (much like the beta_1, beta_2 and epsilon parameters in Adam). We agree that there appears to be quite a few hyperparameters, but \\alpha and \\beta are regularization coefficients, so they have to be roughly scaled to the loss; \\gamma is a moving average coefficient and never needs to be changed; \\mu is dependent only on time, not the model; finally, training is quite insensitive to K (as mentioned in Section 3.2). Thus, the only hyperparameter that needs to be specified is the learning rate \\eta, and that does determine the speed of optimization.\n", "title": "Combined response to AnonReviewer2's comments"}, "HJLT-LpXG": {"type": "rebuttal", "replyto": "BkEmOSvef", "comment": "Thank you AnonReviewer1 for your feedback and comments.\n\nWe ran a new set of experiments comparing Adam, RMSprop and Neumann (Figure 1). Adam achieves similar (or worse) results to the RMSprop baselines: in comparison to our Neumann optimizer, the training is slower, the output model is lower quality, and the optimizer scales poorly. When training with Adam, we observed instability with default parameters (especially, epsilon). We changed it to 0.01 and 1.0 and have two runs which show dramatically different results. Our initial reason for not including comparisons to Adam was that we wanted to use standard models and training parameters (i.e., the Inception and Resnet papers use RMSprop).\n\nWe hope that practitioners will consider Neumann over Adam for the following reasons:\n- Significantly higher quality output models when training using few GPUs.\n- Ability to scale up to vastly more GPUs/TPUs, and overall decreased training time.\n\nWe\u2019ve incorporated your minor comments -- thanks again!\n", "title": "Combined response to AnonReviewer1's comments"}, "rJc1-IpmG": {"type": "rebuttal", "replyto": "rkXFpDlzf", "comment": "On the Resnet-V1, each P100 had 32 examples, so 32000 corresponds to 1000 GPUs. This is updated in Table 3 now.\n\nTo your question about small batches -- we ran the algorithm in asynchronous mode ( mini-batches of 32, with 50 workers doing separate mini-batches); the final output was quite a bit worse in terms of test accuracy (76.8% instead of 79.2%). It\u2019s not clear whether it\u2019s the Neumann algorithm with batch size 32 or the async that causes the degradation though.  So at the least algorithm doesn\u2019t blow up with small batches, but we haven\u2019t explored this setting enough to say anything conclusive.\n", "title": "Details on our experiments"}}}