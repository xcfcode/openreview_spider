{"paper": {"title": "Probing BERT in Hyperbolic Spaces", "authors": ["Boli Chen", "Yao Fu", "Guangwei Xu", "Pengjun Xie", "Chuanqi Tan", "Mosha Chen", "Liping Jing"], "authorids": ["~Boli_Chen1", "~Yao_Fu3", "kunka.xgw@taobao.com", "chengchen.xpj@taobao.com", "chuanqi.tcq@alibaba-inc.com", "chenmosha.cms@alibaba-inc.com", "~Liping_Jing3"], "summary": "We propose a Poincar\u00e9 probe for finding syntax and sentiments from BERT in hyperbolic spaces.", "abstract": "Recently, a variety of probing tasks are proposed to discover linguistic properties learned in contextualized word embeddings. Many of these works implicitly assume these embeddings lay in certain metric spaces, typically the Euclidean space. This work considers a family of geometrically special spaces, the hyperbolic spaces, that exhibit better inductive biases for hierarchical structures and may better reveal linguistic hierarchies encoded in contextualized representations. We introduce a $\\textit{Poincar\u00e9 probe}$, a structural probe projecting these embeddings into a Poincar\u00e9 subspace with explicitly defined hierarchies. We focus on two probing objectives: (a) dependency trees where the hierarchy is defined as head-dependent structures; (b) lexical sentiments where the hierarchy is defined as the polarity of words (positivity and negativity). We argue that a key desideratum of a probe is its sensitivity to the existence of linguistic structures. We apply our probes on BERT, a typical contextualized embedding model. In a syntactic subspace, our probe better recovers tree structures than Euclidean probes, revealing the possibility that the geometry of BERT syntax may not necessarily be Euclidean. In a sentiment subspace, we reveal two possible meta-embeddings for positive and negative sentiments and show how lexically-controlled contextualization would change the geometric localization of embeddings. We demonstrate the findings with our Poincar\u00e9 probe via extensive experiments and visualization.", "keywords": ["Hyperbolic", "BERT", "Probe", "Syntax", "Sentiment"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper introduces a new method to probe contextualized word embeddings for syntax and sentiment properties using hyperbolic geometry. The paper is written well and relevant to the ICLR community. Reviewers highlight that the proposed Poincar\u00e9 probe offers solid results, extensive experiments that support the benefits of the approach, and proposes a new approach to analyze the geometry of BERT models. The revised version clarified various concerns of the initial reviews and improved the manuscript (comparison to Euclidean probes, low dimensional examples, new results on edge length distributions etc.). Overall, the paper makes valuable contributions to probing contextualized word embeddings and the majority of reviewers and the AC support acceptance for its contributions. Please revise your paper to take feedback from reviewers after rebuttal into account (especially to further improve clarity and discussion of the method)."}, "review": {"PGvac6_6PWd": {"type": "review", "replyto": "17VnwXYZyhH", "review": "This paper proposes probes based on hyperbolic embedding spaces, and compares them to the behaviour of Euclidean probes from recent work. The main result is that these probes allow for better recovery of syntactic properties of sentences from contextualized word embeddings compared to context-independent ones, when comparing them to euclidean probes. Similar results are presented  on sentiment analysis, even though no results are presented for context-independent word embeddings.\n\nOn the whole the paper is well-written with great visualizations. My main concern is that the findings are not strong enough at this point. As per the introduction, the first main finding is that the results indicate the possibility that hyperbolic models help us construct more sensitive probes. But how do we know a probe is not too sensitive or not sensitive enough? E.g. looking at the results in table 1, how do we know that the differences in the scores when using contextualised embeddings are for a good reason? Some of the differences are small, especially in the case of the depth probe. Similarly, for the second finding about BERT might be encoding information in non-Euclidean way, how do we know this is the case? One way would be to somehow modify BERT and make it \"more Euclidean\", and then this would help strengthen the hypothesis. But now as it stands, it is interesting but rather speculative.\n\nSome other points:\n- Statistical significance would help in reporting the results as many differences are small in Table 1. Also confidence intervals in the sentence length experiment would be nice.\n- It would be better to explain the scores used in Table 1, instead of referring to Hewitt and Manning\n- For syntactic parsing it is argued that the probe shouldn't be a parser, but in the sentiment analysis the probe is a better parser than the baseline model used. Shouldn't the probe not be a good sentiment analysis model in itself?\n- In the ends of section 5, the referring labels for subfigures in figure 6 are off.\n\nPost-author response: I appreciate the extra experiment on GLoVe vs Linear on sentiment analysis, it is what I was asking for. I have raised my score in response to that, as well as the additional reporting on the results. The discussion in Bayesian terms was interesting, I think it would help. Nevertheless, I was thinking is it that it should be possible to construct embeddings that have known syntactic vs semantic properties. E.g. one could increase/decrease the context size, perhaps to extreme values in the case of models such as GLoVe. And then we could actually have a much stronger prior. If the paper is accepted, I think such an experiment would be very informative.", "title": "Official Blind Review #3 - edited after author response", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "OYTHTm-3K2r": {"type": "rebuttal", "replyto": "vkO5cKM4U60", "comment": "We thank the reviewer for the detailed comments, here are our responses:\n\n### Updated description of the hypothesis and the Poincar\u00e9 probe\n* First, we note that there could be a misunderstanding as the reviewer is discussing the isometric embedding of the d-1 dimensional hyperbolic space in the d dimensional Euclidean space. We want to clarify that:\n    * We are not trying to find an isometry between the hyperbolic and the Euclidean (original BERT space), we are trying to find an (approximately) isometry between the hyperbolic space and the raw tree space where the elements in this space are nodes and the distance is number of edges from one node to another. \n    * So a formal statement of the hypothesis here is: (a) whether there exists a Poincar\u00e9 ball (transformed from the original BERT embedding space with simple parameterization) that encodes dependency trees (in the same vein with Hewitt and Manning 2019), and further (b) whether such Poincar\u00e9 probes are more sensitive to the existences of deeper syntax (as the property induced from its own inductive bias, also discussed in our response to Reviewer 3). \n* More detailed explanation of probe design. We note that our probe is now: $q = Q exp_0(Ph)$, below we explain each term in detail: \n    * $exp_0$ and $Ph$: the use of the exponential map at the origin follows previous work in Genea et. al. (2018), Mathieu et. al. (2019). The primary reasons are (a) simpler mathematical formulation; (b) simpler optimization. Consequently, $Ph$ is a vector in the tangent space of the origin of a Poincar\u00e9 ball and $P$ is a Euclidean-to-Euclidean mapping from the BERT original space (assumed to be Euclidean) to the tangent space. Though our choice follows previous works, we do agree that a more in-depth study, either from a geometric perspective or from a linguistic perspective, is worth investigating. \n    * The second mapping $Q$ is also primarily for optimization reason: we empirically observe that the numerics would be unstable without this mapping (note that the numerical stability is still an open issue for hyperbolic deep learning, Nickel and Kiela 2017, Becigneul and Ganea 2019) and it essentially maps the embeddings from one Poincar\u00e9 ball to another. Also note the reviewer 1 and 4 pointed out this mapping could possibly make the comparison to the Euclidean counterparts not that fair, so we updated the Euclidean probe to make a more fair comparison (see details in responses to them) and the results are basically consistent with the previous results.\n\n### Low dimensional example\n* Thank you for your suggestion. We have added examples about changes of shapes and localizations of probed trees with regard to the curvature of the Poincar\u00e9 ball in the updated paper (Figure 18-20, Table 10). \n* Basically, as the curvature of the Poincar\u00e9 ball changes to be closer to -1, the trees would be more close and stretched to the edge of the ball with increasing syntax recovering performance. \n\n### Interpretation of the hyperbolic subspace under the context of BERT transformer network and further comments\n\n* First, we assume that the BERT original space is (or embedded in) a Euclidean space. The contextualized word embeddings within this space lay in certain special manifolds whose structures and meanings we are currently not aware. We note that these manifolds should encode more information than syntax. \n* Then our Poincar\u00e9 distance probe maps the embeddings manifolds (which we know little about) to a Poincar\u00e9 ball, under which the tree distances are approximately equal to the squared hyperbolic distance (similar arguments apply to the depth probe).\n*  This Poincar\u00e9 probe essentially disentangles syntactical information out of the mixed information encoded in the original embedding, so we also call the Poincar\u00e9 ball a syntactical subspace. \n* Although we know more about the properties of the Poincar\u00e9 ball, we do agree that the properties of the embedding manifold in the original BERT embeddings space, how the information flow and transform along these spaces, and how BERT access these information in a hyperbolic (or non-hyperbolic) way should be more deeply investigated. We leave them to future work. \n\n### References\n* Octavian-Eugen Ganea, Gary Be\u0301cigneul and Thomas Hofmann. *Hyperbolic Neural Networks*. NeurIPS 2018\n* Emile Mathieu, Charline Le Lan, Chris J. Maddison, Ryota Tomioka, Yee Whye Teh. *Continuous Hierarchical Representations with Poincar\u00e9\u0301 Variational Auto-Encoders*. NeurIPS 2019 \n* Gary Be\u0301cigneul and Octavian-Eugen Ganea. *Riemannian Adaptive Optimisation Methods*. ICLR 2019 \n* Maximilian Nickel and Douwe Kiela. *Poincar\u00e9 Embeddings for Learning Hierarchical Representations*. NeurIPS 2017\n", "title": "Response to reviewer 2"}, "fJHv4_ntYy5": {"type": "rebuttal", "replyto": "PGvac6_6PWd", "comment": "We thank the reviewer for the detailed opinions, here are our responses:\n\n### Clarification on probe sensitivity \n\n* The reason that we want a sensitive probe is that, it gives an accurate estimate of the (intractable) real syntactical capability of contextualized embeddings, which is crucial for the community to assess what these embeddings have learned and what they can do. \n* We would like to take this chance and discuss how to define a sensitive probe in the most rigorous notion by doing a Bayesian style thought experiment under an ideal setting:\n    * Suppose we can manually construct two sets of baseline embeddings E0 and E1 and _strictly control_ that the UUAS for E0 to be exactly 25 (= little syntax information) and the UUAS for E1 to exactly 80 (= rich syntax information).\n    * Then we construct a probe to and test the UUAS from that probe. We would like the outputs to be close to 25 and 80 respectively. In this ideal setting, we can exactly measure the sensitivity by the error to the true score. \n    * Such ideal setting is based on the assumption that we know the true UUAS from the very beginning. However it is _currently impossible_ to construct such two sets of embeddings and strictly control the UUAS. Also such golden UUAS prior information is _intractable_ for the ELMo0 and BERTbase7 embeddings, i.e.,we can never know exactly what the numbers are, otherwise we can use the errors to measure sensitivity. \n    * So we _approximate the unknown prior with belief supported by strong evidence_: we hold strong belief that ELMo0 should contain as little syntax as possible, and BERTbase7 should have strong syntax information. Such belief comes from abundant theoretical and empirical evidences about BERT's syntactical capability (and ELMo0's incapability). Note that such belief is also strongly yet implicitly hold by Hewitt and Manning (2019), and serves as a foundation of their work.\n    * In our setting, since there are also abundant theoretical and empirical evidences showing the Euclidean probe's incapability of modeling trees, i.e., there should be more tree information that cannot be correctly revealed in the Euclidean space. To correctly recover the tree information, we need a probe with better inductive bias for trees, this is why we propose the Poincar\u00e9 probe. \n    * The Poincar\u00e9 probe gives lower UUAS estimate on ELMo0 (meaning its not a better parser) and higher UUAS on BERTbase 7 (Table 1), where the major gain is from deeper trees (Figure 2 C). We further highlight additional statistics  (Figure 3 in the updated paper) showing the edge length distribution of the Poincar\u00e9 probe is closer to the ground truth distribution than its Euclidean counterpart.\n* All these inductive bias, theoretical properties, and empirical results serves as strong evidence that Poincar\u00e9 probe reveals more syntactical information for deeper trees encoded in BERT embeddings (i.e., being more sensitive to the existence of deeper syntax). So we hold strong belief that BERT is underestimated by Euclidean probes. \n* Yet we choose to be careful and not to conclude that BERT is geometrically hyperbolic. Current evidence in this paper may make this hypothesis stronger, but not strong enough to make us conclusive (as is also discussed in section 4.3). \n* We indeed do not aim to be conclusive about what geometry of BERT should be. Acknowledging the fact that the community is still far from thoroughly understanding contextualized embeddings (and more general blackbox neural networks), our goal is to use as scientifically rigorous language as we can to describe our exploration and findings of the Poincar\u00e9 probe, and inspire the community to find more, e.g., to study contextualized embeddings from a more geometric perspective, or to explore more about hyperbolic geometry like using them to construct better parsers. \n\n### Interpretation of scores on Table 1\n\n* We note that the depth probing task itself is not as challenging as distance because the later is to estimate pairwise relation while the former is unary, especially for the NSpr metrics (same argument holds for the original Hewitt and Manning 2019). The UUAS should be the most representative metrics as it measures the reconstruction of the whole tree. \n* The highlight of Table 1 is that Poincar\u00e9 probe gives lower UUAS for ELMo0 than the Euclidean (where we hold strong belief that these embeddings should contain little syntax), meaning that it does not form a better parser (as opposed to the GRU in Appendix A, which effectively make the model a parser rather than a probe); it also gives higher UUAS for BERTbase7 where the major gain is from longer sentences and deeper trees (Figure 2C and Figure 3), which reveals deeper syntactical capability of BERT. ", "title": "Response to reviewer 3 (1/2)"}, "wPs-SUTWwZL": {"type": "rebuttal", "replyto": "4yFfqozK15s", "comment": "### Changing Euclidean probe to two linear transformations with a non-linearity \n* We agree that extending the Euclidean probe to two layers would make it more fair comparing to the Hyperbolic Probe. The additional results are in Table 9 in the updated Appendix. \n* Specifically, if we further add Sigmoid non-linearity to the two-layer Euclidean probe, the UUAS score on BERTbase7 would be 84.5 (larger than Poincar\u00e9 = 83.7). However its UUAS score on the baseline ELMo0 also increases to 29.8 (larger than the previous one-layer Euclidean = 26.8, also larger than Poincar\u00e9 = 25.8). \n* Our interpretations on the results of two-layer Euclidean probe with non-linearity are: \n    * Although it gives higher UUAS than Poincar\u00e9 for BERTbase7, it also increases the UUAS on the baseline ELMo0 (while a sensitive probe should give lower UUAS for ELMo0), meaning its parsing capability is indeed increased with the sigmoid function.\n    * The sigmoid function essentially changes the underlying geometric structure since it concentrates all values into [0, 1], so it may transform the original space into another space with (currently) unclear geometry. While we do think there are many fine-grained nuance under each space that definitely worth more digging, we believe these results do not necessarily indicate that our Poincar\u00e9 probe is less sensitive than the Euclidean counterpart. \n* Additionally, we also include the results on (a) two-layer Euclidean probe without non-linearity (per reviewer 4's request) / with tanh/ ReLU (b) probing results of all these probes applied to a new RANDOM baseline embeddings (where the probed embedding is just a randomly sampled matrix). We have:\n  * Our Poincar\u00e9 probe shows better sensitivity than the two-layer Euclidean probe without non-linearity  (see detailed discussions in the response to reviewer 4). \n  * The results for two-layer Euclidean probes with tanh and ReLU show that they also behave more like a parser. \n  * Arguments similar to probing results on the ELMo0 baseline apply to results on the RANDOM baseline. \n* In conclusion, we would say that a two-layer Euclidean probe is more like a parser, rather than a more sensitive probe. ", "title": "Response to reviewer 1 (2/2)"}, "Sxt1NklPxP8": {"type": "rebuttal", "replyto": "W_0oI5YMVT", "comment": "We thank the reviewer for the detailed opinions, here are our responses:\n\n### Adding an additional linear transformation to the Euclidean probe\n\n* We agree that the second projection makes the configuration of the Poincar\u00e9 probe different than the Euclidean one. And yes from the equations, this projection is redundant. We do it primarily for optimization purpose (as you have noticed) because the numerics at the boundary of the Poincar\u00e9 ball might be unstable and inaccurate during gradient descent (a common issue for hyperbolic models, Nickel and Kiela 2017, Ganea et. al. 2018; also the same reason for GRU). \n* We have added a new experiment about additional linear transformation in the Euclidean probe. The new results are (detailed in updated Appendix Table 9): \n    * Euclidean two layers (new results): BERTbase7: UUAS 79.9, DSpr 0.84 | ELMo0: UUAS 26.7, DSpr 0.44\n    * Euclidean one layer (Table 1 in the paper): BERTbase7: UUAS 79.8, DSpr 0.85 | ELMo0: UUAS 26.8, DSpr 0.44\n    * Hyperbolic (Table 1 in the paper): BERTbase7: UUAS 83.7, DSpr 0.88 | ELMo0: UUAS 25.8, DSpr 0.44\n* We see that adding new linear layer to the Euclidean probe barely changes its performance and rank of the probes still holds.\n* Additionally, we have changed the Hyperbolic probe to one linear transformer, the results on BERTbase7 do decrease to: 80.2 UUAS, 0.87 DSpr. \n    * Though it is still higher than the two Euclidean probes, we would not conclude the margin is large enough\n    * However we further note this is primarily for optimization reasons as mentioned above, i.e., in this setting, the Poincar\u00e9 probe are not optimized to its full potential. \n* Additionally, we have also added non-linearity to the Euclidean probes (per Reviewer 1's request, detailed in the responses to them). \n    * There are two consequences after doing it: (a) this would effectively change the geometric structure of the Euclidean probe, making it not strictly Euclidean (b) the probing scores under this setting would also increase for the ELMo0 embeddings (which we do not expect). \n    * Our conclusion is that adding non-linearity to Euclidean probes would make it more like a parser, rather than a more sensitive probe.\n* Despite these additional results, we would like to highlight that our primary results hold when comparing a Euclidean probe with two linear transformations with the Poincar\u00e9 probe. \n\n### Clarity\n\n* Thank you for pointing this out. We use the PTB dataset with Stanford Dependency formalism. Spearman correlation means how the recovered distance/ depth correlate to the golden label. We have updated more details in the section 4.1 of the new version. \n* The numbers reported in Table 1 and Figure 2 are from two models on two objectives separately (code config/example*.yaml, task = distance or depth or both). We do train a single model with both two objectives added together (results in Appendix Table 4, slightly worse than separate training). The primary reason for joint training is for better visualization: we want to see trees with roots close to the origin and see how it expands to the edge of the space.  This kind of illustration would help us better understand the geometric structure. Figure 1 and 3 in the main paper and all tree visualizations in the appendix are from joint training. \n\n### References\n\n* Maximillian Nickel and Douwe, Kiela. *Poincar\u00e9 Embeddings for Learning Hierarchical Representations*. NeurIPS 2017\n\n* Octavian Ganea and Gary Becigneul and Thomas Hofmann. *Hyperbolic Neural Networks*. NeurIPS 2018", "title": "Response to reviewer 4"}, "bfrZCEmEasI": {"type": "rebuttal", "replyto": "PGvac6_6PWd", "comment": "### Probe sensitivity for sentiment analysis\n* We believe there is a misunderstanding of what baseline means in this setting: \n    * The baselines for probe sensitivity are NOT models, they are the embeddings being probed by these models.\n    * A reasonable baseline here should be a set of embedding without sentiment information. Again we use the LINEAR embedding baseline. \n* And we indeed did not add such baseline in the original version. So we have added new results in Table 7 (Updated Appendix). The accuracy is 48.4 for both Euclidean and Poincar\u00e9 probes for the baseline LINEAR embedding, meaning that they are doing nearly random guess (near 50 accuracy) and neither of them forms a classifier. \n\n### Statistical Significance \nThank you for asking, below is the table of p-values: \n\n|             | Distance  |       |            |       |         |         |\n|-------------|-----------|-------|------------|-------|---------|---------|\n|             | Euclidean |       | Poincar\u00e9   |       | p-value |         |\n| Method      | UUAS      | DSpr. | UUAS       | DSpr. | UUAS    | DSpr.   |\n| ELMo0       | 26.8      | 0.44  | 25.8       | 0.44  | 1.3e-7  | 0.41    |\n| Linear      | 48.6      | 0.58  | 45.7       | 0.58  | 1.1e-6  | 0.18    |\n| ELMo1       | 77.0      | 0.83  | 79.8       | 0.87  | 5.4e-3  | 7.7e-4  |\n| BERTbase7   | 79.8      | 0.84  | 83.7       | 0.88  | 9.9e-10 | 4.6e-8  |\n| BERTlarge15 | 82.1      | 0.86  | 85.1       | 0.89  | 6.5e-10 | 4.4e-10 |\n| BERTlarge16 | 81.9      | 0.87  | 85.9       | 0.90  | 1.6e-9  | 3.2e-12 |\n\n|             | Depth     |       |            |       |         |        |\n|-------------|-----------|-------|------------|-------|---------|--------|\n|             | Euclidean |       | Poincar\u00e9   |       | p-value |        |\n| Method      | Root %    | NSpr. | Root %     | NSpr. | Root %  | NSpr.  |\n| ELMo0       | 54.2      | 0.55  | 53.5       | 0.49  | 0.010   | 9.2e-5 |\n| Linear      | 2.9       | 0.26  | 4.5        | 0.26  | 0.37    | 0.52   |\n| ELMo1       | 86.5      | 0.86  | 88.4       | 0.87  | 1.5e-6  | 3.9e-3 |\n| BERTbase7   | 88.2      | 0.87  | 91.3       | 0.88  | 7.2e-3  | 0.013  |\n| BERTlarge15 | 89.0      | 0.88  | 91.1       | 0.88  | 7.4e-3  | 0.043  |\n| BERTlarge16 | 89.6      | 0.88  | 91.7       | 0.89  | 6.3e-5  | 5.0e-3 |\n\nWe see most of the results hold for the UUAS metrics (the most prominent metrics as discussed previously). \n\n### Explaining scores in Table 1 and referring labels for subfigures in figure 6\nThank you for pointing this out, we have updated the corresponding section 4.1 and 5.2 in the updated paper. ", "title": "Response to reviewer 3 (2/2)"}, "4h-PBMC3Lbb": {"type": "rebuttal", "replyto": "4yFfqozK15s", "comment": "We thank the reviewer for the detailed opinions, here are our responses:\n\n### Cases where Euclidean and Poincar\u00e9 are different and what do we learn from them\n* We would draw the reviewer's attention on the following aspects showing the differences of the two probes: \n    * Top sentiment words from the two probes (Table 3): though we have a lot of attention on the syntax task, firstly we note that in the sentiment task, these words are not cherry-picked, they are strictly ranked by the probes. This would serve as the first case showing the differences between the two probes where the Poincar\u00e9 probe gives more words align with human intuition.\n    * (New results): Randomly sampled sentences (Figure 11 in the updated Appendix): the current qualitative comparison in the main paper Figure 4 (previously Figure 3 in old version) is indeed cherry-picked for illustration purpose. So we add 12 randomly sampled sentences with different length in the Appendix as a more detailed qualitative analysis. We roughly observe that the major differences are about edges spanning long sentence chunks, which motivate our next new experiment.\n    * (New results): following the previous observation that the differences of the two probes are on the long edges, we compare the edge length distributions of golden trees and trees produced from the two probes. The results are updated in Figure 3 in the paper main body as a bar chart. Here we give the detailed statistics of the number of edges recovered by these probes (order by edge length in 1-5, 6-10, 11-15, 16-20, >20): \n        * Ground truth: 30420 (90.30%), 2124 (6.30%), 695 (2.06%), 255 (0.75%), 192 (0.56%)\n        * Poincar\u00e9: 30752 (91.29), 1931 (5.73%), 642 (1.90%), 217 (0.64%), 144 (0.42%)\n        * Euclidean: 30778 (91.36%), 2001 (5.94%), 612 (1.81%), 183 (0.54%), 112 (0.33%)\n        * We see that both two probes underestimate the number of long edges, which verifies that deeper trees are indeed more challenging to discover. yet the distribution of the Poincar\u00e9 probe is closer to the ground truth than the Euclidean probe while the later the more biased towards short edges. These results would serve as another evidence that the Poincar\u00e9 probe better recovers deeper syntax. \n    * UUAS w.r.t. sentence length in Figure 2C: to interpret the results, we would emphasize that it is the absolute values that reveal where the performance gain comes from: the Poincar\u00e9 probe consistently better recovers the syntactical information (higher UUAS) on long edges than the Euclidean counterpart. These results also align with our new experiments showing that the Poincar\u00e9 probe gives a closer edge length distribution than the Euclidean counterpart. \n* Combining these four differences, we would advocate using our Poincar\u00e9 probes to the practitioners for its sensitivity to the existence of deeper syntax:\n    * A major reason that we want a more sensitive probe is that it gives a more accurate estimate of the underlying syntactical capability of contextualized embeddings, which is crucial for the community to assess what these embeddings have learned and what they can do. It would also help the community to study the geometric structure of contextualized embeddings from a hyperbolic machine learning perspective. \n    * Furthermore, the sensitivity for longer edges would be particularly important from a linguistic perspective because: (a) longer edges reveal more complicated dependency structures in sentences (b) longer edges are also the most often places where existing models make wrong predictions, as is commonly observed in parsing and structured prediction literature, which further leads to decades of targeted works. \n    * We also believe that in our implementation, the Poincar\u00e9 probe is just as easy to use as the Euclidean counterpart (partially due to its own simplicity). In the attached Jupiter notebook we further provide abundant visualization. We would encourage the audience to try it out. ", "title": "Response to reviewer 1 (1/2)"}, "W_0oI5YMVT": {"type": "review", "replyto": "17VnwXYZyhH", "review": "## Summary\n\nThis work examines some of the syntactic and semantic information present in contextual word embeddings by training probes for dependency parsing and sentiment classification. The probes take the form of a low-dimensional projection of the word embeddings, and obtain the dependency parse and sentiment by considering the distance between the pairs of embeddings or between the embeddings and a class embedding respectively.\n\nThis is closely related to the approach of (Hewitt and Manning, 2019), whose experimental setting this work reproduces, but the innovation here consists in using hyperbolic rather than Euclidean distances: the embedding space is identified with the tangent space at the origin and projected to the Poincare ball using its exponential map, allowing the probe to use the hyperbolic distance.\n\nThe hyperbolic version of the probes consistently out-perform the Euclidean one, which is encouraging, and the authors provide some useful visualizations of the learned projections. However, the experiments fail to account for one possibly relevant difference between their Hyperbolic and Euclidean setting. The paper is also at times difficult to follow on its own, as it relies a bit too much on cited work.\n\n## Clarity\n\nEven if the experimental setting is the same as (Hewitt and Manning, 2019), a quick summary would be welcome here: currently, the name of the parsing dataset is not even mentioned, nor are the meanings of the Spearman correlation metrics (the abbreviations are not self-explanatory).\n\nThere are also some open questions: for parsing, do you train a single model with both objectives? From reading the code, it looks like you alternate doing an epoch of each. Is that right? How does that compare to learning both individually? \n\n## Correctness\n\nMy main issue with the comparison lies in the second projection which the hyperbolic (but not the Euclidean) method uses, as defined in Equation (3)\n\nFirst, doing the projection in the hyperbolic space is redundant. Remember that the Mobius matrix-vector simply applies the logarithmic map at the origin followed by a linear transformation in the tangent space followed by the exponential map at the origin. Hence, Equations (2) and (3) could simply be summarized as:\n$$q_i = \\exp_0 (Q P h_i) $$\n\n(Same remark for the GRU in the appendix.)\n\nWhile this would be equivalent to having a single projection matrix if we could do global optimization, given the geometry of the loss function the hyperbolic setting with its additional parameter might be easier to optimize than the Euclidean one. In particular, this probably accounts for the difference in sentiment classification with fixed label embeddings.\n\nThankfully, the difference in the same task with learned embedding seems to indicate that there is more to the result, and I would consider increasing my score if all the results hold with either an additional linear transformation in the Euclidean setting or a single projection in the hyperbolic.\n", "title": "A good argument for taking the curvature of the space into account for probes, but the comparison might not be entirely fair and the experimental setting description needs to be more complete.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "vkO5cKM4U60": {"type": "review", "replyto": "17VnwXYZyhH", "review": "#### Summary:\n\nIn the same vein as Hewitt & Manning 2019, the authors present an extremely lightly parametrized \u201cprobe\u201d model to determine the presence of syntactic structure in the embedding space of BERT models. While Hewitt & Manning examine the Euclidean distance between linearly transformed token embeddings and its correlation with parse tree distance and depth, this work examines a different distance function based on distances in hyperbolic space. They find that this distance measure, for an equivalent or lesser number of parameters, better reproduces the syntactic properties. This suggests that the BERT model may operate simply, but on a non-Euclidean manifold, in order to work with syntactic information.\n\n\n#### Reasons for score:\n\n\nThe paper presents a simple method to examine syntactic structure in contextual embedding models, as a nice extension of the seminal work on Euclidean probing, and should be directly of interest to the ICLR audience. The paper seems to be well-executed experimentally, with a variety of analyses.\n\nHowever, I would like to see the authors give more geometric rigor to their model and argument, and clarify the hypothesis which they are testing. There is a lot of appeal to intuition about hyperbolic space and its ability to encode trees, but it is not clear what that has to do with their proposed probe in a concrete way. \n\nThe paper would be greatly improved by a simple 1 to 3 dimensional example of their proposed probe and how it could discover an embedded submanifold with hyperbolic structure.\n\n#### Positives:\n\n- This is a mathematically (and superficially conceptually), simple generalization of the work of Hewitt & Manning, which offers improvements to and new avenues for exploration of these probing ideas.\n- The work convincingly shows that their probe offers better performance, especially in UUAS, compared to Euclidean probes, which is quite interesting.\n- Generally, I believe that there is a good idea here, but it would be much improved by additional mathematical rigor.\n\n#### Comments/Concerns:\n\n- It is hard to understand this paper without having read Hewitt & Manning, it could do with being a bit more self-contained. For example, the metrics like \u201cDSpr\u201d, etc, should be explained.\n\n- It is not clear what hypothesis is being tested here. What does it mean for the embedding space to have a hyperbolic structure in this sense? The Nash embedding theorem ensures the existence of an isometric embedding of d-1 dimensional hyperbolic space in d dimensional Euclidean space, but that embedding can hardly be accomplished in general by the simple linear transform + exponential map + moebius multiplication described here.\n\n- In this vein, there should be much more rigor as to describing what exactly the Poincar\u00e9 probe is doing. The paper talks about mapping to \u201cthe tangent space\u201d. What tangent space is this? Is it the tangent space of a Poincar\u00e9 ball at the origin? Why is this a reasonable thing to do if the goal is to discover hyperbolic structure? When should this be expected to work?\n\n- An example in low dimensions with visuals of a hyperbolic submanifold, and a Poincar\u00e9 probe interacting with it, should feature prominently.\n\n- Along these same lines, it would be very helpful for the paper to give an example of how the neural network might access this information on a hyperbolic submanifold, though this may be beyond the scope of the work.\n\n- In the introduction, the authors note the ability of hyperbolic space to encode exponentially larger volumes vs Euclidean space, but of course the Poincar\u00e9 ball they are examining is embedded in Euclidean space.  What does this mean concretely in the context of the BERT transformer network?\n\n#### Minor comments:\n\n- middle of page 7, \u201cRiemannianAdam\u201d is missing the space ", "title": "Simple, interesting, could stand more rigor ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "4yFfqozK15s": {"type": "review", "replyto": "17VnwXYZyhH", "review": "This paper proposes probing BERT representations by projecting them into a Poincare subspace. The proposed approach is used to probe ELMO and BERT for both syntax and sentiment in comparison with the conventional Euclidean probes.\n\nI am ambivalent about this paper. On the positive side, I think that it is a quite solid work, with extensive experimentation, additional supporting results in the appendix, and an accompanying code that can be used to reproduce results and obtain additional visualizations. The paper is also well written and the authors are rigorous when discussing their results rather than trying to oversell. \n\nOn the negative side, I have some reservations about the relevance of this study. What do we learn from it? It is true that the Poincare probes obtain generally higher scores than the Euclidean probes, but it doesn't look like they lead to any new insight about how BERT works. If the message here is that Poincare probes are more appropriate than their Euclidean counterparts, I would have liked to see instances were Euclidean probes lead to erroneous or at least different conclusions when compared to Poincare probes. In the absence of that, we can expect that practitioners will stick with Euclidean probes given that they are simply easier to use.\n\nMoreover, I am not sure if the comparison with Euclidean probes is entirely fair. If my understanding is correct, the Poincare probes learn two linear transformations (P and Q), whereas Euclidean probes learn a single one. Unless I am missing something, it could be that the Poincare probes obtain higher scores simply because the transformation they are learning is more expressive, and not because of the underlying geometric space. In order to test this hypothesis, I think that the authors should try learning two linear transformations for Euclidean probes, with a non-linearity like ReLU in between.\n\nFinally, I feel that some of the analyses did not follow a systematic methodology and some of the interpretations seem subjective and possibly questionable. For instance, I don't see any clear difference between the Poincare and Euclidean probes when it comes to the sentence length (Figure 2C), except for very short sentences. For sentence length > 12 the curves look very similar to me, except that the absolute values for Poincare are higher. Similarly, the visualizations, although interesting, provide a rather anecdotal evidence, in particular since the examples seem to be cherry-picked.", "title": "Review", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}