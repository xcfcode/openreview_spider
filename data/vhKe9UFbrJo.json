{"paper": {"title": "Relating by Contrasting: A Data-efficient Framework for Multimodal Generative Models", "authors": ["Yuge Shi", "Brooks Paige", "Philip Torr", "Siddharth N"], "authorids": ["~Yuge_Shi1", "~Brooks_Paige1", "~Philip_Torr1", "~Siddharth_N1"], "summary": "", "abstract": "Multimodal learning for generative models often refers to the learning of abstract concepts from the commonality of information in multiple modalities, such as vision and language. While it has proven effective for learning generalisable representations, the training of such models often requires a large amount of related multimodal data that shares commonality, which can be expensive to come by. To mitigate this, we develop a novel contrastive framework for generative model learning, allowing us to train the model not just by the commonality between modalities, but by the distinction between \"related\" and \"unrelated\" multimodal data. We show in experiments that our method enables data-efficient multimodal learning on challenging datasets for various multimodal VAE models. We also show that under our proposed framework, the generative model can accurately identify related samples from unrelated ones, making it possible to make use of the plentiful unlabeled, unpaired multimodal data.", "keywords": ["Deep generative model", "multi-modal learning", "representation learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes an efficient method to train generative models on multimodal data using a contrastive approach. Usually training such models requires significant training data to be able to learn patterns. The authors propose a variational autoencoder approach that enables multimodal learning of models using a data-efficient approach, and shows the effectiveness of the approach on challenging datasets.\n\nThe authors have mostly addressed the feedback of the reviewers and done some of the necessary changes to the paper (e.g., adding more results and missing related work). They should make sure to address any lingering concerns about the paper, mentioned by the reviewers in their post-rebuttal feedback."}, "review": {"FmrYMYAcnBU": {"type": "review", "replyto": "vhKe9UFbrJo", "review": "Summary\n\nThe paper proposes a contrastive multimodal generative model framework for including unrelated datapoints as well as related datapoints in the learning of the multimodal model. Using such a contrastive formulation, the paper shows that the proposed model outperforms previous work on four desiderata for multimodal generative models. \n\nStrengths\n+ The paper is a novel take on contrastive learning in a probabilistic generative modeling framework, which is interesting\n+ The paper reports extensive experimental results\n\nWeaknesses\n\nWhy not directly use some energy based model with a contrastive loss to learn such multimodal generative models? It feels somewhat odd to both normalize p(X, Y) and also require unaligned examples as supervision. With an energy based model one can do joint generation, conditional generation (coherence) and synergy studies. A discussion on this would be really helpful!\n\nA minor point, one could also consider the case where \u201cr\u201d is latent, that would actually bring up a very relevant case in vision and language where one crawls the web and the vision and language pairs may be apriori related or unrelated (but it is not known if they are or not). Also, intuitively, why is it that using unrelated data makes us believe that the models should be more data efficient? It would be great to give more intuition on that point. \n\nEqn. 2: The way the equation is written down (given the motivation above) it is not clear that the sum in the second term should be inside the log. Why was this choice made, instead of a more standard contrastive learning approach of summing over the distance with the negative examples (which would make the sum outside the log?). Also, paragraph 1 page 4 is very confusing as the model is talked about as generating when training but as I understand there is no generation when training (only computation of likelihoods?). \n\nAppendix B proof: It appears that the proof might be incorrect as the sum over X that we have in Eqn. 4 is not over all X but over a sample of N examples randomly chosen. It then appears incorrect to claim that this is the same as PMI (as opposed to some kind of an approximation?). \n\nBaseline explanation: It is not clear how the proposed approach is applied to the JMVAE objective (Table. 1), which adds additional terms to the regular ELBO. Are those additional terms also used? Clarification on that would be very helpful. \n\nOverall, it would be very useful to get more insights on why the proposed approach is expected to yield a better multimodal generative model based on the four criteria. Is the idea that generative models without explicitly being given negative supervision are optimistic in which datapoints they consider to be related? How does contrastive training affect \"coverage\" metric as discussed in Vedantam et.al. (Triple ELBO)? Essentially, any insight on when one should and should not do contrastive training would be useful to add to the paper.\n\n**POST REBUTTAL**\nI read through the other reviews and the author rebuttal. I thank the authors for addressing all of my concerns in the rebuttal. Overall, this is an interesting contribution in the multimodal VAE space and would make for a good poster at the conference, and I am happy to keep my original rating for the paper. \n\nIn terms of energy based models here is one recent work which comes to mind which might be useful to extend to multimodal settings: \nDu, Yilun, Shuang Li, and Igor Mordatch. 2020. \u201cCompositional Visual Generation and Inference with Energy Based Models.\u201d arXiv [cs.CV], April. https://arxiv.org/abs/2004.06030.\n(note the quality of generations in Fig. 5)", "title": "Interesting contribution on Multimodal VAEs", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "kMP17hFgPuM": {"type": "review", "replyto": "vhKe9UFbrJo", "review": "The paper tries to minimize the difference of the PMI between related and unrelated pairs of multimodal data but arrives at a very different objective with many approximations. It can be plugged into existing VAE based methods and improve learning performance and data efficiency. My major concern is about the derivation and the connection between motivation and the final objective.\n\nClarity and correctness:\n\nThe presentation of Section 3 should be further clarified. In particular, I wonder about the validness of Hypothesis 3.1. First, it is not clear to me which joint distribution is used in the definition of PMI in the hypothesis? It is not specified and I guess it is the model distribution after training according to the derivation in Appendix A. It is kind of strange because we make a hypothesis about the model after training and optimize the model to reach the hypothesis. \n\nThe derivation is also unclear, according to Eqn. (6), the objective should be the difference between both joint and marginal distributions. I'm not fully convinced by simply ignoring the difference between the marginal ones. I note that the authors mention that the marginal term is not relevant to the relatedness but ignoring it is not optimizing the difference between the PMI, right?\n\nEqn. (6) to Eqn. (2) is also unclear, why we optimize log sum p(negative) instead of optimizing sum log p(negative)? log sum negative is quite like an approximate of log marginal, which seems to contradict the discussion under Eqn. (6). \n\nThe resulting final objective (4) does not obviously connect to the original motivation: maximizing the difference between the PMI values of related and unrelated pairs. I checked Appendix B while the authors claim that it approximately optimizes the joint loglikelihood with an IPM regularization. I have several questions: i) how does this connect to the original motivation? ii) how does the approximation in Eqn. (7) affect training? iii) can we directly optimize Eqn. (7) instead of Eqn. (4)?\n\nThe estimate of the final objective also involves unknown approximation. In particular, the paper does not use CUBO and IWAE properly to form a valid bound of the objective. \n\nWith so many approximations, I can hardly figure out the key factors of the proposed method and say about the technical contribution.\n\nExperiments:\n\nI'm not an expert in multimodal learning. The paper mainly focuses on comparison with VAE based methods. The evaluation metrics are not commonly used but chosen following a related work. The paper claims that the main empirical contribution is its data efficiency, which seems to be verified.\n\n===========\n\n**after rebuttal**\n\nSome of the issues are clarified. I updated my score to 5.\n", "title": " concerns about the derivation and the connection between motivation and the final objective.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "3N2u89YxcaJ": {"type": "rebuttal", "replyto": "ikD19jtuhlP", "comment": "We updated the manuscript again based on Reviewer 4's additional feedback with the following changes:\n\n1. Further clarification on the two estimators considered for term 2 of Eqn. 4--- one a valid bound with high variance (CUBO) the other a non-bound with arbitrarily low variance (IWAE) at the end of sec 3.2;\n2. Highlight connection between Hypothesis 3.1 and the method used in label propagation experiments to provide empirical support to our hypothesis.\n\nWe thank the reviewer again for the helpful feedback!", "title": "Summary of additional revisions based on R4 feedback"}, "6DhNJQGo9TS": {"type": "rebuttal", "replyto": "ypGcocpl_Sz", "comment": "We thank the reviewer for additional feedback! Below are our response to the three questions raised:\n\n**Q1.**\nAs we stated, what motivated our work is our interests in studying the effect of applying contrastive loss to generative models. Therefore, conceptually, we consider relatedness similar to how it is considered in the contrastive loss literature: that the two inputs should have conceptual commonality. More formally, we characterised relatedness with the difference between pointwise mutual information (PMI) of related vs. unrelated pairs in Hypothesis 3.1. As for the \"real mechanism\" of relatedness, it should defer in different multimodal scenarios: for MNIST-SVHN it is digits, for language-vision dataset it is the object of interest.\n\nAppendix A and B are not how we characterise relatedness, but rather, provide intuition to our final objective.\n\n**Q2.**\nWe thank the reviewer for re-investigating Sec 3.2 and we are glad that we could clarify the misunderstanding. We will update the manuscript to highlight the fact that we have 2 versions of objective: one a valid bound with high variance and the other a non-bound with arbitrarily low variance.\n\n**Q3.**\nHypothesis 3.1 can indeed be validated. In fact, the label propagation experiments in Sec 4.5 provide quite strong empirical evidence to our claim. We find for a generative model trained with contrastive objective, the difference between PMI for related pairs and unrelated pairs is a good indicator of relatedness, enabling label propagation by thresholding the PMI estimated by the trained model. This shows that Hypothesis 3.1 is true for a well-trained parametrised generative model.\n\nWe thank the reviewer for enquiring about this connection; we will update our manuscript to highlight this and make our motivations clearer.\n", "title": "Response to additional questions"}, "ikD19jtuhlP": {"type": "rebuttal", "replyto": "vhKe9UFbrJo", "comment": "We would like to thank the reviewers for their helpful suggestions. Based on the feedback we received, we have updated our manuscript with the following changes:\n- Added qualitative results for our experiments in Appendix G and H, including:\n    - Generative results (reconstruction, cross generation and joint generation);  **(R1)**\n    - Marginal likelihood tables evaluated with IWAE (K=1000); **(R1)**\n    - Sample per class histogram of joint generation to showcase generation diversity.  **(R1)**\n- Added additional related work discussing weakly supervised methods; **(R1)**\n- - Clarified in related work that previously seen contrastive approaches are not just good at \"specific tasks\", but also perform well at generalising to different downstream tasks; **(R1)**\n- Added reason and relevant citations for choosing $\\log \\sum$ over $\\sum \\log$ for our objective in Eqn 2; **(R2, R4)**\n- Clarified that Figure 4 only contains natural images; **(R2)**\n- Included discussion on possible adaptation of our method for GANs in Section 5; **(R3)**\n- Updated Appendix B to clarify that the PMI term in Eqn. 7 is an approximation; **(R2, R4)**\n- Clarified the purpose of Hypothesis 3.1 --- the hypothesis should hold true for *true generative models*, and our goal is to learn a parametric generative model that matches this behaviour; **(R4)**\n- Clarified the omitting of margin `m` in methodology. **(R1)**\n\nWe hope that these updates improves the clarify of our paper.", "title": "Summary of revisions based on rebuttal feedback"}, "1lW7Xibv2Ce": {"type": "rebuttal", "replyto": "sdfoXW7T79", "comment": "> \"Relatedness\" in this work is based on log-likelihoods computed in sample space. However, contrasting in latent space might provide a more meaningful measure of \"relatedness\". Have you considered computing the objective (or at least the part that estimates the \"unrelatedness\") using latent representations instead?\n\nYes, we have indeed considered doing this, however the latent contrastive approach has the following problems:\n1. **Modelling perspective:** The motivation of applying contrastive loss on the latent space is to minimise the distance of latents from the same class of different modalities. However, it would be unreasonable to say that the entire latent code should be close to each other when the two samples are related, since latent of each modality contains style information private to each domain. One thing that would be reasonable try is to first explicitly split the latent into private and shared subspaces and put the contrastive constraint on the shared subspace only. We will investigate this in our future work;\n1. **Empirical perspective:** If we add contrastive loss on the latent space as a regulariser for VAE learning, we must consider hyperparameters such as weighting and the choice of distance; one would imagine that the optimal hyperparameter will also change when the regulariser is applied to different multi-modal VAEs and different dataset, making it difficult to use.\n\n\n> In Section 4.5 it is not quite clear how you estimate the \"optimal threshold\". What measure is this threshold based on? Do the results in Figure 8 depend on this threshold and would it be reasonable to show an ablation across different threshold values?\n\nThe threshold is computed with the PMI estimate, as stated at the bottom of page 7. This made intuitive sense to us since the motivation of the objective (as per hypothesis 3.1) is to maximise the difference of PMI between related and unrelated pair. We apologise for the confusion and will make this clearer in our updated draft.\n\nIn addition to PMI threshold, we have also experimented with using joint marginal likelihood and it didn't work nearly as well -- we will include this result as an ablation study in the updated manuscript.\n\n\n> In the paragraph above Equation (2) you mention the margin. It is not clear why is not used later on. E.g., Is it an additional hyperparameter in the model?\n\nApologies for the confusion! We omitted the margin m in our objective following common practice in this sort of max-margin scheme. We will clarify this in updated manuscript.\n\n\n> In Hypothesis 3.1., you use the pointwise mutual information (PMI) and state that PMI(x, y) > PMI(x, y'). As far as I understand, PMI is zero when two outcomes are independent, but it can be negative when two outcomes are not independent, which would violate the \"uncontroversial assumption\". Alternatively, maybe one can use the mutual information between two random variables instead of PMI between outcomes?\n\n\nWe simply claim PMI(related pair) > PMI(unrelated pair). We intend this claim primarily as a measure of dependence, not correlation. This should not depend on the sign of the PMI, which can be anything when densities are involved. \n\nTheoretically speaking using mutual information here would have similar effect, however it requires taking the expectation under the model which is difficult.\n\n> The related work section could benefit from a short paragraph on weakly-supervised learning, which seems to be an important theme in the present paper.\n\nIndeed! Thank you for the suggestion, we will cite relevant weakly-supervised learning paper in our related work in our updated manuscript.\n\n\n> Overall, it is a good paper with strong empirical results for the sample efficiency argument. Yet, I tend towards a weak accept, due to the one-sided evaluation. I will be happy to adjust my score if the paper presents the generative results more transparently in the experiments.\n\nWe certainly understand your concerns -- we'd like to re-iterate that the approach in this paper does not harm generative performance. We will include the following results to make our generative performance more transparent:\n- Generative results for MNIST-SVHN;\n- Marginal likelihood tables for generation;\n- Histogram of number of examples generated from samples from prior for each class to show diversity of joint generation.", "title": "Response to Reviewer 1 (2/2)"}, "SI7lyTMl-pM": {"type": "rebuttal", "replyto": "kMP17hFgPuM", "comment": "> I wonder about the validness of Hypothesis 3.1. First, it is not clear to me which joint distribution is used in the definition of PMI in the hypothesis? It is not specified and I guess it is the model distribution after training according to the derivation in Appendix A. It is kind of strange because we make a hypothesis about the model after training and optimize the model to reach the hypothesis.\n\nHypothesis 3.1 is defined for the true generative model, and our goal is to learn a parametric model that targets this true generative process by setting up the model for multimodal data and learning such a model with a concomitant objective.\nWe will update the manuscript to ensure that this is made clear; thank you for pointing this out!\n\n> The derivation is also unclear, according to Eqn. (6), the objective should be the difference between both joint and marginal distributions. I'm not fully convinced by simply ignoring the difference between the marginal ones. I note that the authors mention that the marginal term is not relevant to the relatedness but ignoring it is not optimizing the difference between the PMI, right?\n\nYes, you are correct in saying that we are not optimising the difference between PMI. As we stated in our paper, we are only optimising the components in PMI difference that are relevant to relatedness (or, to say the least, involves two different modalities), since learning relatedness is our goal. The $log p(y') - log p(y)$ term we discarded in PMI difference (term 2 of Eqn. 6) involves only one modality (functionally, it captures the typicality within modality) and is therefore not relevant to our goal of learning relatedness.\n\n\n> Eqn. (6) to Eqn. (2) is also unclear, why we optimize log sum p(negative) instead of optimizing sum log p(negative)? log sum negative is quite like an approximate of log marginal, which seems to contradict the discussion under Eqn. (6).\n\nWe justify the choice of the log sum instead of sum log form of objective with the following three arguments:\nNote that Eqn. 2 can be rewritten as $log p(x,y)/\\sum_y' p(x,y')$, which can be seen as the logarithm of the ratio between the positive term and the sum of negative terms. This form of contrastive loss is commonly seen in literature of self-supervised contrastive learning (e.g. [1-3]); a consequence of `LogSumExp` being a smooth approximation to the `max` function [4].\n\nMoreover, from Jensen's inequality, the proposed approach (sum outside the log) is upper bounded by our formulation (`LogSumExp`). We chose our formulation simply as a matter of convention.\n\n[1] Representation Learning with Contrastive Predictive Coding, https://arxiv.org/abs/1807.03748\n\n[2] Learning deep representations by mutual information estimation and maximization, https://arxiv.org/abs/1808.06670\n\n[3] A Simple Framework for Contrastive Learning of Visual Representations, https://arxiv.org/abs/2002.05709\n\n[4] A Metric Learning Reality Check, https://arxiv.org/abs/2003.08505\n\n", "title": "Response to Reviewer 4 (1/2)"}, "4c7a0Fl2fOw": {"type": "rebuttal", "replyto": "FmrYMYAcnBU", "comment": "> Why not directly use some energy based model with a contrastive loss to learn such multimodal generative models? It feels somewhat odd to both normalize p(X, Y) and also require unaligned examples as supervision. With an energy based model one can do joint generation, conditional generation (coherence) and synergy studies. A discussion on this would be really helpful!\n\nWhile it is indeed true that contrastive methods have been typically used to train energy-based models, typically Restricted Boltzmann Machines (RBMs), such models have typically not seen much application in complex domains such as image/language. Was there perhaps some particular model you had in mind that would apply in this instance? We'd be happy to explore such a model.\n\n> One could also consider the case where \u201cr\u201d is latent, ...\n\nIndeed. Our primary interest here was the effect of adapting a contrastive objective for generative models like VAEs. We discovered that doing so is functionally similar to capturing an _implicit_ relatedness latent variable whose effect manifests through maximising (related) or minimising (unrelated) the likelihood; an observation we subsequently leverage for experiments that bootstrap off of available labelled related data.\nAn explicit relatedness latent is an interesting direction to explore and we will be studying this in our future work. Thank you for the suggestion!\n\n> Eqn. 2: The way the equation is written down (given the motivation above) it is not clear that the sum in the second term should be inside the log. Why was this choice made, instead of a more standard contrastive learning approach of summing over the distance with the negative examples (which would make the sum outside the log?).\n\n\nNote that Eqn. 2 can be rewritten as $log p(x,y)/\\sum_{y'} p(x,y')$, which can be seen as the logarithm of the ratio between the positive term and the sum of negative terms. This form of contrastive loss is commonly seen in literature of self-supervised contrastive learning (e.g. [1-3]); a consequence of `LogSumExp` being a smooth approximation to the `max` function [4].\n\nMoreover, from Jensen's inequality, the proposed approach (sum outside the log) is upper bounded by our formulation (`LogSumExp`). We chose our formulation simply as a matter of convention.\n\n[1] Representation Learning with Contrastive Predictive Coding, https://arxiv.org/abs/1807.03748\n\n[2] Learning deep representations by mutual information estimation and maximization, https://arxiv.org/abs/1808.06670\n\n[3] A Simple Framework for Contrastive Learning of Visual Representations, https://arxiv.org/abs/2002.05709\n\n[4] A Metric Learning Reality Check, https://arxiv.org/abs/2003.08505\n\n\n> Also, paragraph 1 page 4 is very confusing as the model is talked about as generating when training but as I understand there is no generation when training (only computation of likelihoods?).\n\nIndeed, there are no generations in the examples. We intend Figure 4 to highlight the fact that an image need not be meaningful for the likelihood to be minimised, and thereby provide an intuition on why the minimising log likelihood term in our objective can be overpowering.\n\nWe will revise the narrative and clarify this aspect in a better manner.\n\n\n> Appendix B proof: It appears that the proof might be incorrect as the sum over X that we have in Eqn. 4 is not over all X but over a sample of N examples randomly chosen. It then appears incorrect to claim that this is the same as PMI (as opposed to some kind of an approximation?).\n\n\nAppendix B is intended to provide a more intuitive understanding of the objective in Eqn (4), through an alternate interpretation. It is indeed intended as an approximation as denoted in Eqn (7). We will make this clearer in the updated manuscript.\n\n> Baseline explanation: It is not clear how the proposed approach is applied to the JMVAE objective (Table. 1), which adds additional terms to the regular ELBO. Are those additional terms also used?\n\nYes, the auxiliary terms of JMVAE are also used and we only replaced the ELBO term by our contrastive ELBO in Eqn 4.", "title": "Response to Reviewer 2 (1/2)"}, "LtiseySSMBa": {"type": "rebuttal", "replyto": "FmrYMYAcnBU", "comment": "> It would be very useful to get more insights on why the proposed approach is expected to yield a better multimodal generative model based on the four criteria. Is the idea that generative models without explicitly being given negative supervision are optimistic in which datapoints they consider to be related?\n\n\nIn a manner of speaking, yes.\n\nThe four evaluation criteria we adopt from Shi et al. 2019 characterise the extent and effectiveness with which the latent variable captures information from the observed modalities and disentangles shared and modality-specific information.\n\nNon-contrastive approaches (e.g. Shi et al. 2019, Wu & Goodman 2018) only ever observe positive examples of relatedness, and are thus limited in their ability to implicitly capture what things ought not to be related. Contrastive methods, following the max-margin style of objective, can leverage unrelated data to better capture and disentangle representations, leading to better models as evaluated by the criteria, as observed in our experimental results.\n\n\n> How does contrastive training affect \"coverage\" metric as discussed in Vedantam et.al. (Triple ELBO)?\n\nThe multimodal scenario considered in Vedantam et al. is a visual-attribute one: one modality is typically visual, such as human faces, and the other is a one hot attribute vector (e.g. gender or hair colour). Since these attribute labels are well-defined and categorical, it is easy to evaluate \"coverage\" . However, in the case of more generic multimodal data as considered here, it is unclear what would constitute \"coverage\". This was one of the primary reasons we evaluated on the four metrics (section 4.2) originally proposed in Shi et al. (2019).\n\n\n> Any insight on when one should and should not do contrastive training would be useful to add to the paper.\n\nAs we show in our experimental results (Table 1 & 2, Figure 6), contrastive approach performs better than original models no matter the percentage of data used, indicating that it is always beneficial to adopt such a learning scheme even when more related data is available. However, there are two factors that broadly dictate if one can/ought to employ a contrastive approach:\n\n1. Complexity of data and number of modalities:\n\n   While it is relatively straightforward to extend the 2-modality case discussed here by\n   considering the pointwise total correlation (PTC) instead of PMI, and evaluating the\n   appropriate number of marginals for the negative terms of the contrastive loss, such\n   approaches can quickly run into practical difficulties with the amount of computation\n   required. A similar argument can apply for more complex data like videos.\n\n   In saying this, we would also like to note that when the model is chosen sensibly it is\n   entirely possible to keep the computational cost quite low --- the most complicated\n   experiment ran on MMVAE can be trained on a 4GB GPU within a few hours.\n\n2. Pervasiveness of 'relatedness'\n\n   Constructing the contrastive loss relies on the assumption that randomly sampled\n   pairs/tuples are typically unrelated. Were this not the case, the max-margin approach\n   can lead to poor representations.\n\nIf both factors can be circumvented, using the contrastive objective is preferable.", "title": "Response to Reviewer 2 (2/2)"}, "krH0u4mSTds": {"type": "rebuttal", "replyto": "SyvZxQCei1I", "comment": "> One weakness that I find is a lack of comparison against the GAN based models. Can the authors do such a comparison? Or at least discuss why and how GAN based models could be integrated with the current approach?\n\nThank you for the suggestion and positive appraisal.\n\nWe didn't cover GANs as we were primarily interested in the representation-learning aspects of the multimodal VAE rather than the quality of generations of the model. One could potentially extend this idea to GANs by employing an additional discriminator that evaluates relatedness between generations across modalities, but this is beyond the scope of our work here. We can however include a discussion of GANs and their applicability in this context to the updated manuscript.", "title": "Response to Reviewer 3"}, "pMMj_V_MNPW": {"type": "rebuttal", "replyto": "kMP17hFgPuM", "comment": "> The resulting final objective (4) does not obviously connect to the original motivation: maximizing the difference between the PMI values of related and unrelated pairs. I checked Appendix B while the authors claim that it approximately optimizes the joint loglikelihood with an IPM regularization. I have several questions:\n> - how does this connect to the original motivation? how does the approximation in Eqn. (7) affect training? can we directly optimize Eqn. (7) instead of Eqn. (4)?\n\nWhat motivated our work is our interests in studying the effect of applying contrastive loss to generative models, and from the intuition that under the true data-generating distribution the PMI should be higher for related than for unrelated pairs. Section 3.1 describes our thought process which leads us from this starting point to the objective defined in Eqn. (4).\n\nThis brings us to Appendix B. Appendix B is not a derivation of our objective, but rather an observation which should help us better understand it. As you point out, there are several approximations in section 3.1 --- we too would like to understand better the resulting objective in Eqn. (4), and do so by relating it to a maximum likelihood objective regularized by an approximation to the PMI. We never optimize Eqn. (7) directly, which is intractable due to the presence of the marginal likelihood terms; though, we agree that it could be interesting as future work to think about other approaches which approximate and optimize this directly.\n\nWe will clarify this in a revision to the manuscript.\n\n\n> The estimate of the final objective also involves unknown approximation. In particular, the paper does not use CUBO and IWAE properly to form a valid bound of the objective.\n\nThis is not true. We went to significant lengths to formulate and evaluate a valid bound to the contrastive objective. You can find details about this in Section 3.2 and all of our experiments.\n\nIn particular, we describe in item 2 of Section 3.2 that by employing the CUBO upper bound estimator for term 2 of Eqn. 4, the approximation **is** a proper upper-bound of the objective.\nWe also formulate an alternative, using the IWAE estimator for both terms of Eqn. 4 (item 1 of Section 3.2). While this means we no longer have a bounded approximation, it has the desirable quality of being an arbitrarily tight and low-variance estimator of the marginal likelihood.\nIn our empirical evaluation we showcased the pros and cons of using these two different approximations with the `cI-*` (IWAE) and `cC-` (CUBO) models.\n\nNote that it is common practice to approximate the intractable marginal likelihood in VAEs, and our proposal of having a contrastive objective doesn't change that.\n\n\n> With so many approximations, I can hardly figure out the key factors of the proposed method and say about the technical contribution.\n\nWhile we sympathise with the reviewer's comments about approximations, the only place where approximation appears in the derivation of our objective is the marginal likelihood approximation described in Section 3.2, which is a property of the VAE, not of our method. Other approximations only appear in Appendix A and B, which are intended to provide intuitions for our proposed objective.\n\nOur technical contribution is the novel application of contrastive framework to VAE objective with careful consideration of optimisation, which we believe, with the positive empirical evaluations, has been demonstrated to be valid and effective.\n", "title": "Response to Reviewer 4 (2/2) "}, "ea-zPxlBhTK": {"type": "rebuttal", "replyto": "sdfoXW7T79", "comment": "> My main objection is with regards to claim (C1), which states that the new objective improves multimodal learning. The problem is that the choice of metrics might be too one-sided, measuring only the \"relatedness\" in terms of ground truth labels, but not the generative quality (of a generative model). As such, the used metrics (linear classification accuracy; classification accuracy of a pretrained classifier on generated samples) ignore the diversity of generated samples.\n\nThis is indeed a valid concern. We would like to clarify that our models trained using contrastive methods do not suffer from mode collapse, and the joint generation of the models do cover a wide diversity of examples. We will reassure the readers of this in the updated manuscript by adding/further highlighting the following components:\n\n- We will add qualitative results of the model's reconstructions and joint/cross generations (in fact, we had already included these as part of ablation studies for gamma on Page 15, but we will make these qualitative results a standalone section so that it can be easily found);\n- To showcase the diversity of the joint generation, we will include a histogram computing the percentage of each class generated from prior samples;\n- To ensure the readers that there is no mode collapse, we will include a table with figures of marginal likelihoods for both original and contrastive multi-modal VAEs.\n\nWe would also like to point out the following: a model that performs well on \"synergy\" and \"cross coherence\" is very unlikely to suffer from mode collapse. This is because the classifier used to compute these scores are trained using *real data*, not *generated data* as the reviewer suggested. Therefore, if the model was indeed generating noise, it would be very unlikely that the classifier still matches the noise to the correct class.\n\n> The ablations in the appendix indicate a trade-off between the \"relatedness\" metrics and generative quality, suggesting that the proposed objective yields a looser ELBO (Figure 13) and results in a worse generative performance (Figure 12). If such a trade-off exists, I would recommend presenting it more transparently in the experiments and phrasing the respective claim more conservatively. Further, all previous multimodal VAEs report log-likelihood values, so it seems natural to provide these numbers, too.\n\nWe will make the suggested changes in our updated manuscript to reflect that. Thank you for the suggestion.\n\n\n> In the related work section, it is not clear how the authors conclude that contrastive methods are limited to \"specific tasks\" and how the proposed model overcomes this limitation. If the statement refers to the manual design of a contrastive objective, I would argue that previous work includes experiments showing that learned representations still generalize to other downstream tasks, such as classification and object detection. If the authors refer to generative tasks in particular, this should be stated more clearly.\n\nBy \"specific tasks\" we do indeed refer to a comparison to generative tasks and we will clarify this in the updated paper.\n\n\n> In the experiments of Section 4.3, do I understand correctly that when using e.g. 20% of the data, you do not use any of the remaining 80% as \"unrelated\" samples for contrasting?\n\nYes, that is correct.\n\n> In the description of the datasets you mention that multiple pairings are used to create a dataset of \"related\" samples. Have you also tried to reduce the number of pairings instead of taking percent of the dataset? Does this make a difference with respect to classification, coherence, or generative performance?\n\nYes, we have experimented with this. For two subsets of the original dataset that are of the same cardinality, the model performs better when the reduction of size is due to fewer pairings, compared to a lower percentage taken from each modality. This is because in the former case the model sees more variation in each modality.\n\nIt is indeed interesting to consider this alternate way of reducing the dataset size as it casts light on an important question to be asked of the data --- whether it is easier to obtain relatedness through algorithmic pairing, or through manual annotation.\n\nIf we consider this from a semi-supervised learning perspective (in MNIST-SVHN case), typically the fewer labels used for each modality the better, in which case the method used in our paper makes more practical sense; however if we consider the language-vision scenario where no explicit labels can be assigned for each example, then to reduce the size of the dataset one would have to reduce the number of pairings.", "title": "Response to Reviewer 1 (1/2) "}, "sdfoXW7T79": {"type": "review", "replyto": "vhKe9UFbrJo", "review": " The paper proposes a contrastive objective that (1) minimizes the distance between \"related\" samples while (2) maximizing the distance between randomly paired samples. Existing multimodal VAEs optimize (1) via different multimodal ELBOs. The novelty lies in the optimization of (2) which can further benefit from unimodal samples for which no \"related\" samples of the other modality are available---this can be viewed as a semi-supervised approach for weakly-supervised multimodal data.  For the estimation of (2), the paper experiments with two different estimators, IWAE and CUBO.\n\nThe paper claims three contributions:\n- (C1) improve multimodal learning of existing multimodal VAEs by extending them with objective (2)\n- (C2) improve sample efficiency of multimodal VAEs by extending them with objective (2)\n- (C3) further improve sample efficiency by bootstrapping \"related\" samples from a larger pool of unimodal samples\n\nOverall, the empirical results support the claims of improved sample efficiency, but there might be a potential problem with claim (C1), due to the one-sided selection of metrics used for evaluation.\n\n\n## Strong points\n\nThe paper tackles a practical problem (sample efficiency) with a\nstraightforward solution (leverage unpaired data) that can be easily\nimplemented and seems to work well across a family of models.\n\nStrong empirical results demonstrating improved sample efficiency. For the\nconsidered datasets, the proposed objective improves sample efficiency \nsignificantly, requiring only 20% of the dataset to reach a\nperformance comparable to not using the additional objective. Additional\nresults showing that the sample efficiency can further be improved by a\nprocedure for bootstrapping paired samples from a set of unpaired samples.\nThis is an interesting idea that has been explored in semi-supervised learning,\nbut the present paper applies it to a weakly-supervised, multimodal data.\n\nThe paper nicely connects the objectives used in multimodal VAEs to contrastive\nlearning. It extends the different ELBOs used in multimodal VAEs with an\nadditional objective for minimizing the similarity of randomly paired samples.\n\n\n## Weak points\n\nMy main objection is with regards to claim (C1), which states that the new\nobjective improves multimodal learning. The problem is that the choice of\nmetrics might be too one-sided, measuring only the \"relatedness\" in terms of\nground truth labels, but not the generative quality (of a generative model). As\nsuch, the used metrics (linear classification accuracy; classification accuracy\nof a pretrained classifier on generated samples) ignore the diversity of\ngenerated samples. For example, a model could achieve perfect scores by\ngenerating a single \"canonical\" image of the corresponding class, akin to mode\ncollapse. I understand that generative quality is not the paper's primary goal,\nbut it should not be overlooked, since otherwise, it begs the question of why to\nuse decoders at all and not just apply contrasting in latent space. The\nablations in the appendix indicate a trade-off between the \"relatedness\"\nmetrics and generative quality, suggesting that the proposed objective yields a\nlooser ELBO (Figure 13) and results in a worse generative performance (Figure\n12). If such a trade-off exists, I would recommend presenting it more\ntransparently in the experiments and phrasing the respective claim more\nconservatively.  Further, all previous multimodal VAEs report log-likelihood\nvalues, so it seems natural to provide these numbers, too.\n\nIn the related work section, it is not clear how the authors conclude that contrastive methods are limited to \"specific tasks\" and how the proposed model overcomes this limitation. If the statement refers to the manual\ndesign of a contrastive objective, I would argue that previous work includes\nexperiments showing that learned representations still generalize to other\ndownstream tasks, such as classification and object detection. If the authors\nrefer to generative tasks in particular, this should be stated more clearly.\n\nOverall, it is a good paper with strong empirical results for the sample\nefficiency argument. Yet, I tend towards a weak accept, due to the one-sided\nevaluation. I will be happy to adjust my score if the paper presents the\ngenerative results more transparently in the experiments.\n\n\n## Questions\n\nIn the experiments of Section 4.3, do I understand correctly that when using\ne.g. 20% of the data, you do not use any of the remaining 80% as \"unrelated\"\nsamples for contrasting? Put differently, I understand that you only use\nrandomly paired samples among the 20%-subset for that purpose, correct?\n\nIn the description of the datasets you mention that multiple pairings are used\nto create a dataset of \"related\" samples.  Have you also tried to reduce the\nnumber of pairings instead of taking $n$ percent of the dataset? Does this make\na difference with respect to classification, coherence, or generative\nperformance?\n\n\"Relatedness\" in this work is based on log-likelihoods computed in sample\nspace.  However, contrasting in latent space might provide a more meaningful\nmeasure of \"relatedness\". Have you considered computing the objective (or at\nleast the part that estimates the \"unrelatedness\") using latent representations\ninstead?\n\nIn Section 4.5 it is not quite clear how you estimate the \"optimal threshold\".\nWhat measure is this threshold based on? Do the results in Figure 8 depend \non this threshold and would it be reasonable to show an ablation across\ndifferent threshold values?\n\nIn the paragraph above Equation (2) you mention the margin $m$.  It is not\nclear why $m$ is not used later on. E.g., Is it an additional hyperparameter in\nthe model?\n\n\n## Additional Feedback\n\nIn Hypothesis 3.1., you use the pointwise mutual information (PMI) and state\nthat PMI(x, y) > PMI(x, y'). As far as I understand, PMI is zero when two\noutcomes are independent, but it can be negative when two outcomes are not\nindependent, which would violate the \"uncontroversial assumption\".\nAlternatively, maybe one can use the mutual information between two random\nvariables instead of PMI between outcomes?\n\nThe related work section could benefit from a short paragraph on\nweakly-supervised learning, which seems to be an important theme in the present\npaper.\n", "title": "The paper deals with multimodal VAEs and addresses their problem of requiring lots of \"related\" (i.e., weakly-supervised) samples. To tackle the sample inefficiency, the paper proposes a contrastive objective. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyvZxQCei1I": {"type": "review", "replyto": "vhKe9UFbrJo", "review": "This work presents a generative model for multimodal learning. The paper maximizes or minimizes the pointwise mutual information between data from two modalities considering a novel random variable relatedness to dictates if data are related or not.  This is realized by casting multimodal learning as max-margin optimization with the contrastive loss for the objective. For the optimization, the paper considers the IWAE estimator. As per the experiments, the paper considers MNIST-SVHN and CUB Image-Captions dataset and perform evaluations across four metrics. Using the experiments, the paper demonstrates that the proposed approach improves multimodal learning, data-efficient learning, and label propagation. \n\n- The paper is well written and easy to follow.\n- The experimental design is praiseworthy as the paper considers different aspects in evaluating multimodal learning via a generative model. The optimization of the proposed approach is also carefully considered. \n- The experimental results across all three tasks demonstrate the efficacy of the proposed approach. \n- One weakness that I find is a lack of comparison against the GAN based models. Can the authors do such a comparison? Or at least discuss why and how GAN based models could be integrated with the current approach? \n", "title": "Multimodal VAE using contrastive-style objective.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}