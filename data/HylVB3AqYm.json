{"paper": {"title": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware", "authors": ["Han Cai", "Ligeng Zhu", "Song Han"], "authorids": ["hancai@mit.edu", "ligeng@mit.edu", "songhan@mit.edu"], "summary": "Proxy-less neural architecture search for directly learning architectures on large-scale target task (ImageNet) while reducing the cost to the same level of normal training.", "abstract": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. 10 4 GPU hours) makes it difficult to directly search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize proxy tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present ProxylessNAS that can directly learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6\u00d7 fewer parameters. On ImageNet, our model achieves 3.1% better top-1 accuracy than MobileNetV2, while being 1.2\u00d7 faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design.", "keywords": ["Neural Architecture Search", "Efficient Neural Networks"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper integrates a bunch of existing approaches for neural architecture search, including OneShot/DARTS, BinaryConnect, REINFORCE, etc. Although the novelty of the paper may be limited, empirical performance seems impressive. The source code is not available. I think this is a borderline paper but maybe good enough for acceptance.\n"}, "review": {"rkx-525OJ4": {"type": "rebuttal", "replyto": "S1x0oTiHR7", "comment": "Thanks for your questions. Please see responses below.\n\n>>> \u201cWhat was the search time on CIFAR-10 in GPU hours? For Proxyless-R and Proxyless-G?\u201d\n\nThe search time depends on the size of the backbone architectures (e.g., number of blocks). For example, when searching with 54 blocks, it takes around 4 days on a single GPU for both Proxyless-R and Proxyless-G. When searching with fewer blocks (e.g. 8 blocks), it takes less than 1 day. \n\n>>> \u201cIs Batch Normalization in training or evaluation mode when optimizing architecture parameters?\u201d\n\nThe batch normalization is in the training mode.\n\n>>> \u201cFor REINFORCE, what do you use as optimization metric on validation set for architecture parameters on CIFAR-10? Normal loss, like cross entropy or actually misclassification rate?\u201d\n\nWe use the misclassification rate. Normal loss, like cross entropy, may also be a feasible optimization metric\n\n>>> \u201cFor REINFORCE, do you use any kind of baselining? Do you use multiple architecture samples per update?\u201d\n\nThe baseline is the moving average of previous mean metrics with a decay of 0.99. And we update every 8 samples.\n", "title": "Re: Further questions"}, "Hke4jK9OkE": {"type": "rebuttal", "replyto": "H1x6eyDe0X", "comment": "Apologize for the mistake. The correct one is setting \"replacement=False\". Beta2 is set to be the default value in Pytorch (i.e., 0.999). As for network parameters, we use SGD optimizer with Nesterov momentum 0.9 and cosine learning rate schedule.", "title": "Re: replacement=True or False?"}, "H1eNgOayAX": {"type": "rebuttal", "replyto": "rkle4Pwda7", "comment": "Hi Robin, \n\nThanks for your interest in our work and your detailed questions. \n\n>>> Response to \"Rescaling architecture parameters\" \nYour understanding of the gradient-based updates is correct.  \nAs for sampling two paths according to the multinomial distribution, we use \"torch.multinomial()\". And by setting \"replacement=False\", the same path will not be chosen twice. \n\n>>> Response to \"Adam optimizer for architecture parameters\" \nWe also consider it would be problematic to use the adaptive gradient averages for this case where most of the paths are not chosen. So we set beta1 to be 0 in the Adam optimizer.  Sampling multiple times before making an Adam update step is a nice idea. We will try it later. Thanks for your suggestion. \n", "title": "Responses to implementation questions"}, "rJe1QITpR7": {"type": "rebuttal", "replyto": "rJl4Qn5KAQ", "comment": "Thank you for your helpful feedback. We have revised our paper according to your suggestion.\n\n>>> \u201cin the new mobile phone results you have presented there is a network that actually has better latency with slightly worse accuracy, which makes it hard to compare\u201d\n\n2.6% top-1 accuracy improvement on ImageNet is significant. To achieve the same accuracy, MobileNetV2 needs 2x latency (143ms v.s. 78ms). Please see Figure 4.\n\n>>> \u201cIt would be nice to actually have a table showing the strengths/weaknesses along these axes for all of these methods\u201d\n\nThanks for your suggestion. We will add the table to our paper. \n\nModel\t                             Top-1\t  Top-5\tLatency\tHardware-Aware\t  No-Proxy\tNo-Repeating\tTime\tMemory\nMobilenetV1\t                      70.6\t   89.5\t 113ms\t              -\t                         -\t                  No\t                    -\t               -\nMobilenetV2\t                      72.0\t   91.0\t  75ms\t              -\t                         -\t                  No\t                    -                  -\nNASNet-A\t                      74.0\t   91.3\t 183ms\t            No\t               No                      No                  10^4  \t   10^1\nAmoebaNet-A\t              74.5\t   92.0\t 190ms\t            No\t               No\t                  No\t                 10^4         10^1\nDarts\t                              73.1\t   91.0\t      -\t                    No\t               No\t                  No\t                 10^2\t   10^2\nMnasNet\t                      74.0\t   91.8\t  79ms\t            Yes\t               No\t                  No\t                 10^4    \t  10^1\nProxylessNAS (mobile)      74.6\t   92.2\t  78ms\t            Yes\t               Yes\t                  Yes                 10^2    \t  10^1\n\n>>> \u201cprecisely define what is novel about the method\u201d and \u201cemphasize exactly the empirical contribution\u201d\n\nWe summarize our contributions as follows:\n\n> Methodologically,\na) We provided a new path-level pruning perspective for NAS.\n\nb) We proposed a gradient-based approach (Section 3.3.1) to handle non-differentiable hardware objectives (e.g. latency), making them differentiable by introducing regularization loss.\n\nc) We proposed a path-level binarization approach to address the high memory consumption issue of differentiable NAS. Notably, different from BinaryConnect that binarizes each weight, our path-level binarization approach binarizes the entire path.\n\n> Empirically,\na) We significantly reduced the cost of memory/compute for the training of large over-parameterized networks and thereby scaled to large-scale datasets (ImageNet) without proxy and repeating blocks.\n\nb) We studied specialized neural network architectures for different hardware architectures and showed its advantage, raising people\u2019s awareness of specializing neural network architectures for hardware.\n\nc) We achieved strong empirical results on both CIFAR-10 and ImageNet. On different hardware platforms (GPU, CPU and mobile phone), our models not only significantly outperform previous state-of-the-arts, but also peer submissions.\n\nWe sincerely thank your feedback and hopefully have cleared your concerns.\n", "title": "Thanks for your helpful feedback."}, "HyxWxsxaCQ": {"type": "rebuttal", "replyto": "HJelAtlcRm", "comment": "Thank you for your reply and detailed suggestion. We have uploaded a revision of our paper and removed the number of search space size. ", "title": "Thanks for your further feedback. We have revised the paper accordingly."}, "BklS-ur9h7": {"type": "review", "replyto": "HylVB3AqYm", "review": "The algorithm described in this paper is part of the one-shot family of architecture search algorithms. In practice this means training an over-parameterized architecture, of which the architectures being searched for are sub-graphs. Once this bigger network is trained it is pruned into the desired sub-graph. The algorithm is similar to DARTS in that it it has weights that determine how important the various possible nodes are, but the interpretation here is stochastic, in that the weight indicates the probability of the component being active. Two methods to train those weights are being suggested, using REINFORCE and using BinaryConnect, both having different trade offs.\n\n- (minor) *cumbersome* network seems the wrong term, maybe over-parameterized network?\n- (minor) I do not think that the size of the search space a very meaningful metric\n\nPros:\n- Good exposition\n- Interesting and fairly elegant idea\n- Good experimental results\n\nCons\n- tested on a limited amount of settings, for something that claims that helps to automate the creation of architecture. I think this is the main shortcoming, although shared by many NAS papers\n- No source code available\n\nSome typos:\n\n- Fo example, when proxy strategy -> Fo*r* example\n- normal training in following ways. -> in *the* following ways\n- we can then derive optimized compact architecture.", "title": "Interesting idea for efficient NAS that gives state-of-the-art results (on limited datasets)", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "SJlO7KpVp7": {"type": "rebuttal", "replyto": "HkxEMyl33m", "comment": "We sincerely thank you for your comprehensive comments and constructive advices.\n\n>>> Response to \u201ccombination of existing methods\u201d: \nThanks for your kind advice on organizing the paper to make our contributions more clear. Here, we would like to emphasize our contributions:\n\na) Our proxy-less NAS is the first NAS algorithm that directly learns architectures on the large-scale dataset (e.g. ImageNet) without any proxy. We also solved an important problem improving the computation efficiency of NAS as we reduced the computational cost (GPU hours and GPU memory) of NAS to the same level as normal training. Moreover, the GPU memory requirement of our method keeps at O(1) complexity rather than grows linearly with the number of candidate operations O(N)  [3, 4]. Therefore, our method can easily support a large candidate set while DARTS and One-Shot cannot. \t\n\nb) Our proxy-less NAS is the first NAS algorithm that breaks the convention of repeating blocks in neural architecture design. From Alexnet and VGG to ResNet and MobileNet, manually designed CNNs used to repeat blocks within the same stage. Previous NAS works keep the tradition as otherwise the searching cost will be unaffordable. Our work breaks the constraints, and we found this is actually a stereotype that needs to be corrected. \n\nThe new interesting design patterns, found by our method, can provide new insights for efficient neural architecture design. For example, people used to stack multiple 3x3 convs to replace a single large kernel conv, as this uses fewer parameters while keeping a similar receptive field. But we found this pattern may not be proper for designing efficient (low latency) networks: Two 3x3 depthwise separable convs actually run slower than a single 5x5 depthwise separable conv.  Our GPU model, shown in Figure 4, incorporates large kernel convs and aggressively pools at early stages to shrink network depth. Then the model chooses computation-expensive operations at low-resolution stages. It also tends to choose computation-expensive operations in the first block within each stage where the feature map is downsampled.  As a consequence, our GPU model can outperform previous SOTA efficient architectures in accuracy performances (e.g. 3.1% higher top-1 than MobileNetV2), while running faster than them (e.g. 1.2x faster than MobileNetV2). Such patterns cannot be found by previous NAS, as they optimize on proxy task and force blocks to share structures.\n\nc) Our method builds upon methods from two communities (one-shot architecture search from NAS community and Pruning/BinaryConnect from model compression community). It is the first time to incorporate ideas from the model compression community to the NAS community and we also provide a new path-level pruning perspective for one-shot architecture search. Moreover, we provide a unified framework for both gradient-based updates and REINFORCE-based updates. \n\nd) Our proxy-less NAS achieved very strong empirical results on two most representative benchmarks (i.e. CIFAR and ImageNet). On CIFAR-10, our optimized model reached 2.08% error rate with only 5.7M parameters, outperforming previous state-of-the-art architecture (AmeobaNet-B with 34.9M parameters). On ImageNet, we searched specialized neural network architectures for three different platforms (GPU, CPU and mobile phone). With latency constraints, our optimized models also achieved state-of-the-art results (3.1% higher top-1 accuracy while being 1.2x faster on GPU and 2.6% higher top-1 accuracy with similar latency on mobile phone, compared to MobileNetV2). \n\nBesides, we directly optimize the latency, rather than an inaccurate proxy (i.e. FLOPs). It\u2019s an important concept that low FLOPs doesn\u2019t translate to low latency. All our speedup numbers are reported with real measured latency. We believe both our efficient search methodology and the resulting efficient models have big industry impact. ", "title": "proxy-less NAS is an important contribution that breaks many conventions and stereotypes of neural architecture design.  It's not a combination of existing methods."}, "rJxuErCmTm": {"type": "rebuttal", "replyto": "BklS-ur9h7", "comment": "We sincerely thank you for the detailed comments on our paper. We have revised the paper and fixed the typos accordingly.\n\n>>> Response to \u201climited amount of tested settings\u201d: \nAs our proxy-less NAS has reduced the cost to the same level of normal training (100x more efficient on ImageNet), it is of great interest for us to apply proxy-less NAS to more settings and datasets. However, for this work, considering the resource constraints and time limits, we have strong reasons to believe that our experiment settings are sufficient:\n\na) Our experiments are conducted on two most representative benchmarks (CIFAR and ImageNet). It is in line with previous NAS papers and also makes it possible to compare our method with previous NAS methods. We also experimented with 3 different hardware platforms and observed consistent latency improvement over previous work. \n\nb) Moreover, on the challenging ImageNet classification task, we have conducted architecture search experiments under three different settings (GPU, CPU and Mobile) while previous NAS papers mainly transfer learned architectures from CIFAR-10 to ImageNet without conducting architecture search experiments on ImageNet [1, 2]. \n\n>>> Response to \u201cno source code available\u201d: \nReviewer 2 also has similar requests, based on the concern on our strong empirical results. Our pre-trained models and the evaluation code are provided in the following anonymous link: https://goo.gl/QU3GhA. Besides, we have also uploaded the video visualizing the architecture search process: https://goo.gl/VAzGJs. We plan to open source our project upon publication.\n\n>>> Response to \u201cthe size of the search space is not a very meaningful metric\u201d: \nThis might be a misunderstanding. We do not intend to use the size of our search space as a metric for comparison; instead, it is an important reason why our accuracy is much better than previous NAS methods. Previous NAS methods forced different blocks to share the same structure and only explored a limited architecture space (e.g. 10^18 in [2] and 10^10 in [3]). Our method, breaking the constraints, allows all of the blocks to be specified and has much larger search space (i.e. 10^547).\n\n[1] Zoph B, Vasudevan V, Shlens J, Le QV. Learning transferable architectures for scalable image recognition. CVPR 2018.\n[2] Liu H, Simonyan K, Yang Y. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055. 2018.\n[3] Bender G, Kindermans PJ, Zoph B, Vasudevan V, Le Q. Understanding and simplifying one-shot architecture search. ICML 2018.", "title": "Proxyless NAS enables efficient and direct search on different tasks and hardware platforms"}, "BkxbEO0XpX": {"type": "rebuttal", "replyto": "rJl-2uUshQ", "comment": "We sincerely thanks for the detailed feedback. Our pre-trained models and the evaluation code are provided in the following anonymous link for verifying our results: https://goo.gl/QU3GhA. We have also made a video to visualize the architecture search process: https://goo.gl/VAzGJs. We would like to release the entire codebase upon publication. \n\n>>> Response to \u201cperformances are too good to be true\u201d: \nWe consider the comment as a compliment rather than a drawback. There are several reasons for our good results:\na) Our proxy-less NAS *directly* learns on the *target* task while previous NAS methods *indirectly* learn on *proxy* tasks. For example, on CIFAR-10, DARTS [1] conducted architecture search experiments with 8 blocks due to their high memory consumption and then transferred the learned block structure to a much larger network with 20 blocks. This indirect optimization scheme would lead to suboptimal results while our proxy-less NAS does not suffer from this problem. \n\nb) We broke the convention in neural architecture design by *not* repeating the same building block structure. Our method explores a much larger architecture space compared to previous NAS methods (10^547 vs 10^18). Furthermore, our method has much larger block diversity and is able to learn preferences at different positions in the architecture.\n \nFor example, our optimized neural network architectures for GPU, CPU and mobile phone prefer to choose more computation-expensive operations (e.g. 7x7 MBConv6) for the last few stages where the resolution of feature map is low. They also prefer to choose more computation-expensive operations in the first block within each stage where the feature map is downsampled. We consider the ability to learn such patterns which are absent in previous NAS papers also helps to improve our results.\n\n>>> Response to \u201cDPP-Net and NAO citations\u201d: \nApologize for the typo and missing a relevant paper in our reference part. We have fixed typo and added a reference to \u201cNeural Architecture Optimization\u201d. Thanks for pointing out our mistakes.\n\n[1] Liu H, Simonyan K, Yang Y. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055. 2018.\n", "title": "Models on all platforms have been open sourced. Reproducible experiment verified on 3 different platforms. "}, "S1lC-BtXpX": {"type": "rebuttal", "replyto": "HkxRDmY7am", "comment": "Thanks for your interest in our work. The evaluation code and pretrained models are accessible at https://goo.gl/QU3GhA. We also made a video to visualize the architecture search process at https://goo.gl/VAzGJs . You are welcome to validate the performance. The entire codebase will be released upon publication.\n\nOur implementation is repeatable and reproducible. We used the same code base to search CPU/GPU/Mobile models. On all three platforms the performance consistently outperformed previous work, thanks to our Proxyless NAS enables searching over a large design space efficiently.\n", "title": "We have uploaded the evaluation code and pretrained models"}, "B1lXIuW-CQ": {"type": "rebuttal", "replyto": "S1xBSKTN6m", "comment": "We have added the results for Proxyless-G on ImageNet to the paper (please see Table 6 in Appendix D). We find that without taking latency as a direct objective, Proxyless-G has no incentive to choose computation-cheap operations. Consequently, it designs a very slow network that has 158ms latency on mobile phone. After rescaling the network using depth multiplier [1, 2], the latency of the network reduces to 83ms. However, this model can only achieve 71.8% top-1 accuracy on ImageNet which is 2.8% lower than Proxyless-R. Therefore, as discussed in our previous responses, it is essential to take latency which is non-differentiable as a direct optimization objective. And REINFORCE-based approach provides a solution to this problem.\n\nBeside REINFORCE, we have recently designed a differentiable approach to handle the non-differentiable objectives (please see Appendix D). Specifically,  we propose the latency regularization loss based on our proposed latency prediction model (please see Appendix C). The key to the latency regularization loss is an observation that the expected latency of a mixed operation is actually differentiable w.r.t. architecture parameters. Therefore, by incorporating the expected latency into the loss function as a regularization term, we are able to directly optimize the trade-off between accuracy and latency. Further details are provided in Appendix D. \n\n[1] Sandler, Mark, et al. \"MobileNetV2: Inverted Residuals and Linear Bottlenecks.\" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.\n[2] Tan, Mingxing, et al. \"Mnasnet: Platform-aware neural architecture search for mobile.\" arXiv preprint arXiv:1807.11626 (2018).", "title": "We have added the results for Proxyless-G on ImageNet. And we also include a new differentiable approach to handle non-differentiable objectives (i.e. latency)."}, "SkeDbIW-CX": {"type": "rebuttal", "replyto": "HylVB3AqYm", "comment": "Hi all,\n\nWe have uploaded a revision of our paper with the following new methods and stronger experiment results:\n\na) \u201cEconomical alternative to mobile farm\u201d. In Appendix C, we introduce an accurate latency prediction model and remove the need of building an expensive mobile farm infrastructure [1] when learning specialized neural network architectures for mobile phone. We add new experiment results on the mobile setting, where our model achieves state-of-the-art top-1 accuracy on ImageNet under mobile latency constraints. \n\nb) \u201cMake latency differentiable\u201d. In Appendix D, we present a *differentiable* approach to handle the non-differentiable objectives (i.e. latency in our case). Specifically,  we propose the latency regularization loss based on our proposed latency prediction model. By incorporating the predicted latency of the network into the loss function as a regularization term, we are able to directly optimize the trade-off between accuracy and latency. We also add new experiments on ImageNet to justify the effectiveness of the proposed latency regularization loss.  \n\n[1] Tan, Mingxing, et al. \"Mnasnet: Platform-aware neural architecture search for mobile.\" arXiv preprint arXiv:1807.11626 (2018).", "title": "Paper revision: new methods and new experiment results"}, "S1xBSKTN6m": {"type": "rebuttal", "replyto": "SJlO7KpVp7", "comment": "\n>>> Response to \u201ccomparison with One Shot and DARTS\u201d: \nApologize for the unclear explanation for this experiment. We will revise this part to make it more clear. \n\nAll of three methods are evaluated under the same condition except DARTS [3]. Same as the original paper, DARTS *has to* use a smaller scale setting for learning architectures due to the high memory consumption. So for DARTS, the first cell structure setting is chosen to fit the network into a single GPU to learn cell structure. Then we evaluated the learned cell structure on two larger settings by repeatedly stacking it, same as the original DARTS paper [3]. \n\nFor our method, since we solved the high memory consumption issue via binarized path, our method can directly learn architectures under both small-scale and large-scale settings with *limited* GPU memory. As it is one of the key advantages of our method over previous NAS methods, we consider it reasonable to keep such differences. \n\n>>> Response to \u201cadd results for Proxyless-G on ImageNet\u201d: \nThanks for suggesting this new experiment. We have launched this experiment and will add the results to the paper.\n\nHowever, it is important to take latency as a *direct* objective when learning specialized neural network architectures for a platform. Otherwise, NAS would fail to make a good trade-off between accuracy and latency. For example, NASNet-A [1] and AmoebaNet-A [2] has shown compelling accuracy results compared to MobileNetV2 1.4 with similar number of parameters and FLOPs. But they are optimized without the awareness of the latency, their measured latencies on mobile phone are much worse than MobileNetV2 1.4 (see below). Therefore, we employ REINFORCE to directly optimize the non-differentiable objective (i.e. latency).\n\nModel\t\t\t\tParams\t        FLOPS\t        Top-1\tMobile latency\nMobileNet V2 1.4\t\t6.9M\t\t585M\t\t74.7\t\t143ms\nNASNet-A\t\t\t5.3M\t\t564M\t\t74.0\t\t183ms\nAmeobaNet-A\t\t5.1M\t\t555M\t\t74.5\t\t190ms\n\n[1] Zoph B, Vasudevan V, Shlens J, Le QV. Learning transferable architectures for scalable image recognition. CVPR 2018.\n[2] Real E, Aggarwal A, Huang Y, Le QV. Regularized evolution for image classifier architecture search. arXiv preprint arXiv:1802.01548. 2018.\n[3] Liu H, Simonyan K, Yang Y. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055. 2018.\n[4] Bender G, Kindermans PJ, Zoph B, Vasudevan V, Le Q. Understanding and simplifying one-shot architecture search. ICML 2018.", "title": "We made Apple-to-Apple comparison. Our advantage on memory saving is clear."}, "HkxEMyl33m": {"type": "review", "replyto": "HylVB3AqYm", "review": "\nThis paper addresses the problem of architecture search, and specifically seeks to do this without having to train on \"proxy\" tasks where the problem is simplified through more limited optimization, architectural complexity, or dataset size. The paper puts together a set of existing complementary methods towards this end, specifically 1) Training \"cumbersome\" networks as in One Shot and DARTS, 2) Path binarization to address memory requirements (optimized using ideas in BinaryConnect), and 3) optimizing a non-differentiable architecture using REINFORCE. The end result is that this method is able to find efficient architectures that achieve state of art performance with fewer parameters, can be optimized for non-differentiable objectives such as latency, and can do so with smaller amounts of GPU memory and computation.\n\nStrengths\n\n + The paper is in general well-written and provides a clear description of the methods.\n\n + Different choices made are well-justified in terms of the challenge they seek to address (e.g. non-differentiable objectives, etc.)\n\n + The results achieve state of art while being able to trade off other objectives such as latency\n\n + There are some interesting findings such as the need for specialized blocks rather than repeating blocks, comparison of architectures for CPUs vs. GPUs, etc. \n\nWeaknesses\n \n - In the end, the method is really a combination of existing methods (One Shot/DART, BinaryConnect, use of RL/REINFORCE, etc.). One novel aspect seems to be factorizing the choice out of N candidates by making it a binary selection. In general, it would be good for the paper to make clear which aspects were already done by other approaches (or if it's a modification what exactly was modified/added in comparison) and highlight the novel elements.\n\n - The comparison with One Shot and DARTS seems strange, as there are limitations place on those methods (e.g. cell structure settings) that the authors state they chose \"to save time\". While that consideration has some validity, the authors should explicitly state why they think these differences don't unfairly bias the experiments towards the proposed approach.\n\n - It's not clear that the REINFORCE aspect is adding much; it achieves slightly higher parameters when compared against Proxyless-G, and while I understand the motivation to optimize a non-differentiable function in this case the latency example (on ImageNet) is never compared to Proxyless-G. It could be that optimized the normal differentiable objective achieves similar latency with the smaller number of parameters. Please show results for Proxyless-G in Table 4.\n\n - There were several typos throughout the paper (\"great impact BY automatically designing\", \"Fo example\", \"is build upon\", etc.)\n\n In summary, the paper presents work on an interesting topic. The set of methods seem to be largely pulled from work that already exists, but is able to achieve good results in a manner that uses less GPU memory and compute, while supporting non-differentiable objectives. Some of the methodological issues mentioned above should be addressed though in order to strengthen the argument that all parts of the the method (especially REINFORCE) are necessary. ", "title": "Interesting combination of existing methods and good performance", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJl-2uUshQ": {"type": "review", "replyto": "HylVB3AqYm", "review": "It seems the authors propose an efficient method to search platform-aware network architecture aiming at high recognition accuracy and low latency. Their results on CIFAR-10 and ImageNet are surprisingly good.  But it is still hard to believe that the author can  achieve 2.08% error rate with only 5.7M parameter on CIFAR10 and 74.5% top-1 accuracy on ImageNet with less GPU hours/memories than prior arts.\n\nGiven my concerns above, the author must release their code and detail pipelines since NAS papers are difficult to be reproduced. \n\nThere is a small typo in reference part:\nJing-Dong Dong's work should be DPP-Net instead of PPP-Net (https://eccv2018.org/openaccess/content_ECCV_2018/papers/Jin-Dong_Dong_DPP-Net_Device-aware_Progressive_ECCV_2018_paper.pdf)\nand I think this paper \"Neural Architecture Optimization\" shoud be cited.", "title": "Solid work with convincing results", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "rJeFYtGK27": {"type": "rebuttal", "replyto": "HylVB3AqYm", "comment": "Hi all,\n\nOur efficient algorithm allows us to specialize neural network architectures for different devices easily. Recently, we extended our proxyless NAS to the mobile setting and achieved SOTA result with mobile latency constraint (< 80ms latency on Pixel 1 phone) as well. The following is our current results on ImageNet (Device: Pixel 1. Batch size: 1. Framework: TF-Lite):\n\nModel\t\t\t\tTop-1\tTop-5\tMobile latency\nMobileNet V1\t\t70.6\t\t89.5\t\t113ms\nMobileNet V2\t\t72.0\t\t91.0\t\t75ms\nNASNet-A\t\t\t74.0\t\t91.3\t\t183ms\nAmeobaNet-A\t\t74.5\t\t92.0\t\t190ms\nMnasNet\t\t\t74.0\t\t91.8\t\t76ms\nMnasNet (our impl.)\t74.0\t\t91.8\t\t79ms\nProxyless NAS (ours)\t74.6\t\t92.2\t\t78ms\n\nThe detailed architectures of our searched models and their learning process are provided in the following anonymous link:\nhttps://drive.google.com/open?id=1nut1owvACc9yz1ZPqcbqoJLS2XrVPp1Q", "title": "New experiment results on mobile phone"}}}