{"paper": {"title": "Communicating Hierarchical Neural Controllers for Learning Zero-shot Task Generalization", "authors": ["Junhyuk Oh", "Satinder Singh", "Honglak Lee", "Pushmeet Kohli"], "authorids": ["junhyuk@umich.edu", "baveja@umich.edu", "honglak@umich.edu", "pkohli@microsoft.com"], "summary": "", "abstract": "The ability to generalize from past experience to solve previously unseen tasks is a key research challenge in reinforcement learning (RL). In this paper, we consider RL tasks defined as a sequence of high-level instructions described by natural language and study two types of generalization: to unseen and longer sequences of previously seen instructions, and to sequences where the instructions themselves were previously not seen. \nWe present a novel hierarchical deep RL architecture that consists of two interacting neural controllers: a meta controller that reads instructions and repeatedly communicates subtasks to a subtask controller that in turn learns to perform such subtasks. To generalize better to unseen instructions, we propose a regularizer that encourages to learn subtask embeddings that capture correspondences between similar subtasks. We also propose a new differentiable neural network architecture in the meta controller that learns temporal abstractions which makes learning more stable under delayed reward. Our architecture is evaluated on a stochastic 2D grid world and a 3D visual environment where the agent should execute a list of instructions. We demonstrate that the proposed architecture is able to generalize well over unseen instructions as well as longer lists of instructions.", "keywords": ["Reinforcement Learning", "Deep learning"]}, "meta": {"decision": "Reject", "comment": "The paper looks at how natural language instructions can be decomposed into sub-tasks for as-yet-unseen new tasks\n hence the zero-shot generalization, which is considered to be the primary challenge to be solved. \n The precise problem being solved by the original paper is not clearly expressed in the writing. This left some reviewers asking for comparisons, while the authors note that for the specific nature of the problem being solved, they could not seen any particular methods that is known to be capable of tackling this kind of problem. The complexity of the system and simplicity of the final examples were also found to be contradictory by a subset of the reviewers, although this is again related to the understanding of the problem being solved.\n \n With scores of 3/4/5/7, the ideas in this paper were appreciated by a subset of reviewers. \n At the time of the writing of this metareview, the authors have posted a fairly lengthy rebuttal (Jan 18) and significant revisions (Jan 18), with no further responses from reviewers as of yet. However, it is difficult for reviewers to do a full re-evaluation of the paper on such short notice. \n \n While the latest revisions are commendable, it is unfortunately difficult to argue strongly in favor of this paper at present."}, "review": {"H1JYrQpLg": {"type": "rebuttal", "replyto": "Sy1CLUdrx", "comment": "Thank you for the comment and pointing out relevant work.\nWe\u2019ve posted a common response to the all reviewers as a separate comment above.\nWe\u2019d appreciate it if you go through the common response as well as this comment.\n\n- Regarding \u201cit relies on many recent advances skillfully combined\u201d\nMany recent advances are indeed combined in our paper. We explain why each of technique is needed in the common response. In addition, we revised the paper so that readers can easily differentiate between our own idea and existing recent techniques.\n\n- Regarding dealing with SMDP structure in gradient update in temporal abstraction setting\nJust to clarify, the subtask controller is trained first and serves as a parameterized option for the meta controller. So, the two controllers are trained separately. The SMDP structure of the meta controller is implicitly determined by a binary variable c_t in the meta controller itself. The gradient update of the meta controller is also affected by this variable as shown in the Equation (10) in the appendix.\n\n- Regarding parameterized options and other related work\nThank you for pointing our relevant work. We included many of them in the \u201crelated work\u201d section.\n\n- Regarding the clarity of the paper (\u201cminor issues\u201d)\nThank you for the detailed comments! We reflected your comments in the revision.\nThe term \u201czero-shot learning/generalization\u201d means that the agent generalizes to new tasks \u201cwithout additional learning process\u201d. This term has been widely used in supervised learning problems where the model should predict previously unseen labels. As you mentioned, zero-shot learning is closely related to \u201clearning with priors\u201d. In order to make zero-shot learning possible, the model (or the agent) should learn prior knowledge about problems (or tasks) and infer the underlying goal given a new problem from the prior. In our work, this prior corresponds to two assumptions. The first assumption is that subtask arguments (e.g., \u2018visit / transform / pick up\u2019 v.s. \u2018cow / duck / stone \u2026\u2019) are independent of each other, and this is learned through analogy-making. The second assumption is that instructions should be executed sequentially, which is embedded in the structure of the meta controller. By learning or having such priors, the agent can successfully generalize to previously unseen and longer instructions.", "title": "Response to Reviewer 4"}, "BJRjE7aIg": {"type": "rebuttal", "replyto": "SJDFGUZEe", "comment": "Thank you for the comment.\nWe\u2019ve posted a common response to the all reviewers as a separate comment above.\nWe\u2019d appreciate it if you go through the common response as well as this comment.\n\n- Regarding the simplicity of the domain and the problem and \u201csimilar problems have been solved using simpler models\u201d: \nWe believe that our problem has unique challenges compared to traditional RL tasks. We have discussed these challenges with justification of our method in the common response above. We also added a new result from a 3D visual domain in the revision (Section 6.5). Please let us know if you have further comments or questions about this.  \n\n- Regarding \u201cPrevious work on planning with skills are ignored by the authors\u201d\nThank you for pointing out important prior work on planning with skills. We added and discussed this line of work [1, 2, 3] in the \u201crelated work\u201d section. To summarize, we found that the problem considered in the prior work is quite different from our problem. (i.e., their problems lack the challenges discussed in the common response.) Please let us know if there is still missing work.\n\n- Regarding \u201cGeneralization is limited to breaking a sentence into words\u201d\nAlthough we demonstrated generalization to unseen pairs of subtask arguments (two arguments) in the main experiment, the proposed analogy-making regularizer can be applied to any number of arguments. In addition, it can also be used for more complex generalization scenarios. For example, if objects should be handled in a different way given the same subtask (i.e., verb and noun are not independent), our analogy-making regularizer can be used to provide prior knowledge so that the agent can generalize to unseen target objects in a desired way without needing to experience them. This is discussed in Appendix B. \n\n- Regarding \u201cthe paper is difficult to read\u201d: \nThank you for the suggestions about the structure of our paper. We revised the paper by moving less important details to the appendix and by reorganizing the section structure. Just to clarify, Algorithm 1 and 2 tables are not algorithms in the sense that they describe the neural network architecture. Please let us know if there are still some sections that need to be removed or improved.\n\n[References]\n[1] George Konidaris, Andrew G. Barto. Building Portable Options: Skill Transfer in Reinforcement Learning, IJCAI 2007.\n[2] George Konidaris, Ilya Scheidwasser, Andrew G. Barto. Transfer in Reinforcement Learning via Shared Features, Journal of Machine Learning Research 2012.\n[3] Bruno Castro da Silva, George Konidaris, Andrew G. Barto, Learning Parameterized Skills, ICML 2012.", "title": "Response to Reviewer 3"}, "rkCpXm6Ue": {"type": "rebuttal", "replyto": "rJD_Y3GNg", "comment": "Thank you for the comment.\nWe\u2019ve posted a common response to the all reviewers as a separate comment above.\nWe\u2019d appreciate it if you go through the common response.\n\n- Regarding the complexity of our architecture:\nWe have clarified and justified why we need each component of our architecture in the common response. We also revised the paper so that readers can understand the system and its motivation more easily.\n\n- Regarding the domain:\nWe found that most of the existing RL benchmarks (e.g, Atari) are not flexible enough to define sequences of seen and unseen tasks. So, we chose to build our own tasks in a 2d grid-world as we aim to solve generalization problems rather than a particular domain. In addition, we also added a new experiment on a 3D visual domain in the current version of the paper.", "title": "Response to Reviewer 2"}, "HJcD7QpUg": {"type": "rebuttal", "replyto": "SJF9aBYBl", "comment": "Thank you for the review.\nWe\u2019ve posted a common response to the all reviewers as a separate comment above.\nWe\u2019d appreciate it if you go through the common response as well as this comment.\n\n- Regarding the justification of combining subtask embeddings in the subtask controller: \nThis is addressed in \u201cChallenge 1\u201d section in the common response.\n\n- Regarding the domain (2D grid world) does not represents a large-scale task: \nWe originally used the term \u201clarge-scale task\u201d to refer to a large number of sets of instructions, but we agree that the 2D grid world is not a large-scale environment. Since the term \u201clarge-scale task\u201d is ambiguous, we changed the wording (removed the use of \u201clarge-scale task\u201d) in the revision. We also added a new result from a more challenging 3D visual domain (Section 6.5) in the revision. \n\n- Regarding \u201cNo comparison to state-of-the-art alternatives\u201d: \nWe are not aware of state-of-the-art alternatives for our problem because we believe our problem is a new problem. We found that all the previous works are neither directly applicable to our problem nor comparable to our architecture because they lack the ability to deal with unseen instructions or partial observability induced by the instructions as described in the common response. For this reason (lack of state-of-the-art alternatives), we provided results from an ablation study by showing results with and without each idea of our methods (e.g., analogy-making, flat baseline) in the experiment. Please let us know any existing work or baselines that can be directly compared to our work, and we will consider adding them in our revision.\n\n", "title": "Response to Reviewer 1"}, "SJpbf76Il": {"type": "rebuttal", "replyto": "SJttqw5ge", "comment": "Dear reviewers,\n\nThank you for your valuable comments. \nWe have revised our paper by reflecting many comments from you.\nWe also added new results from a 3D visual domain to address concerns regarding simplicity of the 2D grid-world domain (Section 6.5).\n\nWe put a common response here as many of you raised similar questions/comments about complexity of our architecture and simplicity of the problem. We describe challenging aspects of our problem (not domain) and justify each component of our method. \nTo begin with, the complex components of our architecture are designed NOT for the domain BUT for other challenges that we describe below: (1) zero-shot generalization over unseen tasks, (2) partial observability induced by instructions, and (3) mapping from instructions to subtasks. \n\n- Challenge 1:  Zero-shot generalization over unseen tasks\nMost prior work on generalization in RL considers transfer learning where either the semantics of the task are fixed, or the agent is further trained on the target task. In contrast, our work considers \u201czero-shot\u201d generalization where the agent should solve previously unseen tasks \u201cwithout experiencing them beforehand\u201d. In this setting, unlike conventional transfer learning, the agent needs to be given a description of the task (e.g., instructions) as additional input in order to be able to generalize to unseen tasks. Generalization over task descriptions is rarely tackled except for the papers we discussed in the related work section (e.g., UVFA [6], Parameterized Skill [3]). In this type of approach, it is necessary to learn a representation of task description (i.e., subtask in our work). We used a neural network to learn a representation of the subtask, and the term \u201csubtask embedding\u201d means such a learned representation. \n\n-- Why analogy-making regularization?\nSimply learning the mapping from the subtask to the policy (or parameterized skill/option) does not guarantee that the learned mapping will generalize to unseen subtasks (i.e., \u201czero-shot\u201d subtask generalization). This is why we proposed the analogy-making regularizer that allows the agent to learn the underlying manifold of the subtask space so that the agent can successfully map even an unseen subtask to a good policy. In our main experiment, we used analogy-making to encourage the agent to learn that it should perform any actions (e.g., pick up) on any target objects (e.g., cow) in the same way. Note that this is just one way of using our analogy-making regularizer. It can also address more complex generalization scenarios (e.g., \u201cinteract with X\u201d) as discussed in Appendix B; in such cases, meaning of \u201cinteract\u201d changes depending on the target objects, and simple methods (e.g., concatenation of action and target object embeddings) will fail in this scenario.\n\n- Challenge 2: Partial observability induced by instructions\nMuch of the prior work on solving sequential RL tasks uses fully-observable environments [1, 2, 3, 5, 7] (i.e., a specific subtask to execute at each time step can be unambiguously inferred from the current observation). In contrast, our environment is \u201cpartially observable\u201d because the agent is given just a full list of instructions, but it\u2019s NOT given which instruction it has to execute at each time-step. In addition, the current observation (i.e., grid-world with objects) does not tell the agent which instruction to execute. Thus, the agent needs to \u201cremember\u201d how many instructions it has finished and decide when to move on to the next instruction. We chose this challenging setting motivated by the problem of a household robot that is required to execute a list of previously unseen instructions without human supervision that tells the robot what to do for every step. \n\n-- Why use memory?\nWe believe that our memory architecture is a much simplified version of Neural Turing Machines and has only the *minimal and necessary* components for dealing with partial observability described above. Without the memory component, there is no way for the agent to keep track of its progress on instruction execution. \n\n- Challenge 3: Mapping from instructions to subtasks \nEven though the meta controller is given a subtask controller that has pre-trained skills, the mapping from instructions to a sequence of skills (subtasks) is not trivial in our problem because of the following reasons: 1) stochastic events (i.e., randomly appearing enemy) require the agent to \u201cinterrupt\u201d the current subtask. 2) \u201cTransform/Pick up ALL\u201d instructions require the agent to repeat the same subtask for a while, and the number of repetition depends on the observation. Moreover, the reward signal is quite delayed due to this type of instruction. \n\n-- Why differentiable temporal abstraction? \nIn the meta controller, selecting a subtask at every time-step is inefficient and makes it harder to learn under delayed reward. It is known that \u201ctemporal abstraction\u201d provided by options can allow the meta controller to learn faster as the meta controller can use the temporally-extended actions in SMDP [8]. However, the meta controller cannot simply use the subtask termination signal provided by the subtask controller to define the time-scale of its actions due to the necessity of \u201cinterruption\u201d mechanism. Thus, we proposed a new way to \u201clearn\u201d the dynamic time-scale in the meta controller through neural networks. Although the agent can also deal with the challenges without such learned temporal abstraction in principle, we show empirically that the meta controller with our idea (learned temporal abstraction) performs significantly better than the baseline which updates the subtask at every time-step. \n\n- Justification of the use of other minor techniques\nThe three techniques listed below are existing methods that are applied to our problem in an appropriate way. Note that they are not designed to tackle the main challenges of our problem described above, but we used them to improve the overall results or stabilize training. We provided the reason why we used those techniques. \n\n-- Parameter prediction [9]: This approach has been shown to be effective for one-shot and zero-shot image classification problems. We used this technique because we also aim for zero-shot generalization over unseen tasks. \n-- Policy distillation [10]: This approach has been shown to be more efficient for multi-task policy learning. Although we found that policy distillation is not \u201cnecessary\u201d to train the subtask controller, it gives slightly better results than training from scratch. \n-- Generalized Advantage Estimation [11]: This is a recent state-of-the-art technique to combine bootstrapped values and Monte-Carlo return to improve the stability and efficiency of training.\n\n[References]\n[1] George Konidaris, Andrew G. Barto. Building Portable Options: Skill Transfer in Reinforcement Learning, IJCAI 2007.\n[2] George Konidaris, Ilya Scheidwasser, Andrew G. Barto. Transfer in Reinforcement Learning via Shared Features, Journal of Machine Learning Research 2012.\n[3] Bruno Castro da Silva, George Konidaris, Andrew G. Barto, Learning Parameterized Skills, ICML 2012.\n[4] Warwick Masson, George Konidaris, Reinforcement Learning with Parameterized Actions, AAAI 2016.\n[5] S. R. K. Branavan, Harr Chen, Luke S. Zettlemoyer, Regina Barzilay, Reinforcement Learning for Mapping Instructions to Actions, ACL 2009.\n[6] Tom Schaul, Daniel Horgan, Karol Gregor, David Silver. Universal Value Function Approximators, ICML 2015.\n[7] Satinder Pal Singh. Transfer of learning by composing solutions of elemental sequential tasks. Machine Learning, 8(3-4):323\u2013339, 1992.\n[8] Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1):181\u2013211, 1999.\n[9] J. Lei Ba, K. Swersky, S. Fidler, et al. Predicting deep zero-shot convolutional neural networks using textual descriptions. CVPR 2015.\n[10] A. A. Rusu, S. G. Colmenarejo, C. Gulcehre, G. Desjardins, J. Kirkpatrick, R. Pascanu, V. Mnih, K. Kavukcuoglu, and R. Hadsell. Policy distillation. ICLR 2016.\n[11] J. Schulman, P. Moritz, S. Levine, M. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. ICLR 2016.", "title": "Common response to all reviewers"}, "BkhwukRBx": {"type": "rebuttal", "replyto": "SJF9aBYBl", "comment": "Thank you for the review. We will post our full response soon.\nMay I ask what the state-of-the-art alternatives are?\nWe would like to implement and compare them with our method in the next revision if possible.", "title": "state-of-the-art alternatives"}, "B1R9sVrXl": {"type": "rebuttal", "replyto": "H15Vq2TMg", "comment": "HAMs and PHAMs are indeed relevant to our paper. We had referred to HAMs in the related work section of the section so will focus on the connections to PHAMs here.\nWhile there are conceptual connections between PHAMs and our work, there are some major differences which we discuss below.\n\n- Both our framework and PHAMs learn a modularized policy. Both works can, in principle,  combine learned modules to construct a new policy that can solve new large-scale tasks. However, the PHAM paper does not consider or discuss such a zero-shot generalization scenario, while it is the main emphasis of our work. \n\n- PHAMs also present \u201cparameterized\u201d subroutines. While these are similar to our subtask controller (which serves as a parameterized option), our framework goes much further by learning a representation of parameters (i.e., subtask embedding) through deep neural networks. We do this by proposing and evaluating the analogy-making regularizer that enables generalization to previously \u201cunseen\u201d subtasks (unseen parameters). To our knowledge, generalization to unseen parameters is not discussed in the PHAMs paper.\n\n- PHAMs use a programming language (e.g., LISP) to partially but explicitly specify the policy (e.g., action space for each abstract machine, interrupt condition), which effectively reduces the state-action space compared to the original MDP. In contrast, instructions in our paper (the counterpart of the programming language in PHAMs) do not explicitly specify the policy but require the agent to learn what policy to follow to execute the instruction. In addition, the background task that requires interruption is not specified in the instruction. The agent has to learn from trial and error from delayed feedback when and how to interrupt the current instruction. We also have instructions like \u201cPick up ALL eggs\u201d which requires the meta-controller to learn to repeatedly provide the same subtask arguments to the subtask controller until this instruction is successfully executed.  In this sense, instructions in our work are more like a partial description of tasks and goals,  while the programming language in PHAMs is a description of policy. \n\n- While PHAMs guarantee hierarchical optimality, our architecture does not guarantee it because we use a nonlinear function approximation (e.g., deep neural network, a complex form of regularization), which is necessary for generalization.\n\n- The idea of PHAMs can be actually integrated into our work by using a programming language (e.g., LISP) as instructions. This will provide us with richer task descriptions such as conditional statements and recursions. This would be a very interesting future work.\n\n- The key ideas in our paper (analogy-making, differentiable temporal abstraction) are also orthogonal to the idea of PHAMs.\n\nWe will also take a look at the other relevant work you mentioned and discuss them in the following revision.\nThank you for the constructive comment! ", "title": "Thank you for the constructive comment."}, "S1qOje77e": {"type": "rebuttal", "replyto": "SJgX36KMg", "comment": "1. The parameter of the convolution is \\varphi(g) and b. \\varphi(g) is a multi-layer perceptron taking the subtask arguments. Intuitively, a neural network takes subtask arguments (g) as input, and the output of the network becomes the parameter of the convolution (\\varphi(g)). This is a relatively new technique called \u201cparameter prediction\u201d for conditioning neural networks on some variables (i.e., subtask).\nFor the fully-connected layer, W\u2019 and W are two different weight matrices that are independent of the subtask. In this case, we again want to use \\varphi(g) as the weight matrix, but the size of the weight matrix tends to be large in the fully-connected layer. This is why we chose to use \u201cmatrix factorization\u201d to reduce the size of the \\varphi(g) by factorizing the whole weight into the three parts: W\u2019, diag(\\varphi(g)), W. You can find more details in Memisevic & Hinton (2010) paper.\nThe two above equations (convolution, fully-connected) are implicitly used in CNN(s_t; g) part in the following equation. The idea is to use such layers (parameters predicted by the subtask argument) throughout the whole network.\n* denotes convolution operation. We will include the definition of * in the following revision.\n\n2. We followed the location-based addressing mechanism used in Neural Turing Machines (Graves et al., 2014). Each dimension of l_t means shifting the memory pointer by either -1, 0, or 1, because we apply convolution operation using l_t as a filter to the memory pointer (p_t = l_t * p_{t-1}).\n\n3. We have a single MLP taking subtask arguments as input. Subtask arguments are represented as one-hot vectors. We trained this MLP only on the training set of subtasks described in Table 5 in the appendix E. The MLP is expected to generalize to new subtasks that are only observed at test time. \n\n4. Yes, as you say, one could possibly use \\beta for the update decision (c_t = \\beta). However, this approach would result in an \u201copen-loop\u201d policy where the meta controller blindly waits until the subtask termination. The problem is that this approach will not be able to handle stochastic events in our environment. For example, if an enemy appears, then the optimal policy should update the subtask even though the current subtask (say \"pick up diamond\") is not finished (c_t=1, \\beta = 0). This is indeed learned by our meta controller. On the other hand, if the termination signal is used as the update decision (c_t = \\beta), the agent should wait until \u201cpick up diamond\u201d is finished (which might be too late to deal with the stochastic event).\n\n5. We assume that a termination prediction is correct only if predictions are correct throughout the whole episode. We measured the termination prediction accuracy while following its learned policy. Our result shows that \u2018Concat+Analogy\u2019 performs comparable to \u2018Parameter+Analogy\u2019 in terms of the policy (action selection), but 'Concat+Analogy' struggles with detecting when the subtask is finished.\n\n6. The overall design of the flat controller is similar to meta controller which enables it, in principle, to solve the whole task. Furthermore, it is also pre-trained on training subtasks by modifying the architecture based on domain-specific knowledge (This is described in Section 5.1). Nevertheless, the flat baseline learned a sub-optimal policy that treats \u2018Pick up/Transform X\u2019 as \u2018Pick up/Transform all X\u2019. In other words, it always removes all the target objects in the world given \u2018Pick up/Transform\u2019 instructions. It also often picks up or transforms target objects given \u2018Visit\u2019 task.  This sub-optimal strategy performs reasonably well on short sequences of instructions and on the forgiving environment (which was used for a part of curriculum learning), because target objects are re-generated whenever they do not exist in the world. This allows the flat controller to recover from its mistake. However, the flat controller fails in the original environment where target objects are not re-generated, because it tends to unnecessarily remove objects that can be targets in the future. \nThe flat controller has a single policy network that has to deal with the overall complex task (instruction execution + random events). On the other hand, in our architecture, the roles of two controllers are clearly separated, and the communication protocol (subtask arguments) allows these controller to interact with each other so that they can learn a complex closed-loop policy and generalize more easily when combined. \n\n7. We did not see any correlations that you mentioned, because all the agents had achieved relatively good performance at seen subtasks. For instance, even the agents without analogy-making regularization can actually solve all the training subtasks with high success rate (99%), but they do not perform well on unseen subtasks (60% success rate). We observed that they often pick up or transform target objects given unseen \u2018Visit\u2019 subtasks. They sometimes do not even reach target objects given unseen subtasks. \n\n8. We will enlarge the figures as you suggested.\n\nThank you for your valuable comments. \nWe will try to reflect your comments in the following revision. ", "title": "Thank you for the useful comments."}, "H15Vq2TMg": {"type": "review", "replyto": "SJttqw5ge", "review": "I think there's a strong connection between the proposed paper and earlier work by Ron Parr, David Andre, and Stuart Russell on programmable hierarchies of abstract machines (HAMs). Recall that in HAMs, the policy is specified as a series of abstract machine invocations where the only point at which learning occurs are specific decision pints. Programmable HAMs extends this paradigm to the case where the policy is specified by a more or less full programmable language, with macros, loops, recursion etc. Many of the stated objectives in this paper can be fairly easily accomplished in PHAMs, I think. Of course, the underlying ML technology in HAMs did not exploit the current deep Q network approach, and that is certainly a valuable thing to pursue. So, I wonder whether the proposed work can be seen as an offshoot of PHAMs, adapted to the deep RL setting? \n\nLater work by Marthi and Russell extended this paradigm to concurrent actions as well, so it's certainly an important thread of work to compare against (both theoretically and experimentally). \n\nIt's also worth pointing out that there is considerable work in genetic programming (Koza et al.) that explored the discovery of reusable procedures, function abstractions, recursion etc., based on evolutionary modification of LISP  like program abstractions. Again, the technology was different, but the underlying motivations are similar. \n\nThis is nice work, in any case, and well worth pursuing as a way of scaling deep RL. This paper can be seen as instantiating a famous paper by the founder of AI John McCarthy on learning to take advice (which was studied in depth by other later researchers, such as Jack Mostow in the card game Hearts). The idea is that the agent is given high level instructions on how to solve a problem, and must distill from it a low level policy. This is quite related to how humans learn complex tasks in many domains (e.g., driving, where a driving instructor may provide advice such as \"keep a certain distance from the car in front\"). \n\nA fairly complex neural deep learning controller architecture is used, although the details of this system are somewhat confusing in terms of many details that are presented. A simpler approach might have been easier to follow, at least initially. The experiments unfortunately are on a rather simplistic 2D maze, and it would have been worthwhile to see how the approach scaled to more complex tasks of the sort usually seen in deep RL papers these days (e.g, Atari, physics simulators etc.). \n\nNice overall idea, somewhat confusing description of the solution, and an inadequate set of experiments on a less than satisfactory domain of 2D grid worlds. ", "title": "Connections to programmable HAMs etc. ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJD_Y3GNg": {"type": "review", "replyto": "SJttqw5ge", "review": "I think there's a strong connection between the proposed paper and earlier work by Ron Parr, David Andre, and Stuart Russell on programmable hierarchies of abstract machines (HAMs). Recall that in HAMs, the policy is specified as a series of abstract machine invocations where the only point at which learning occurs are specific decision pints. Programmable HAMs extends this paradigm to the case where the policy is specified by a more or less full programmable language, with macros, loops, recursion etc. Many of the stated objectives in this paper can be fairly easily accomplished in PHAMs, I think. Of course, the underlying ML technology in HAMs did not exploit the current deep Q network approach, and that is certainly a valuable thing to pursue. So, I wonder whether the proposed work can be seen as an offshoot of PHAMs, adapted to the deep RL setting? \n\nLater work by Marthi and Russell extended this paradigm to concurrent actions as well, so it's certainly an important thread of work to compare against (both theoretically and experimentally). \n\nIt's also worth pointing out that there is considerable work in genetic programming (Koza et al.) that explored the discovery of reusable procedures, function abstractions, recursion etc., based on evolutionary modification of LISP  like program abstractions. Again, the technology was different, but the underlying motivations are similar. \n\nThis is nice work, in any case, and well worth pursuing as a way of scaling deep RL. This paper can be seen as instantiating a famous paper by the founder of AI John McCarthy on learning to take advice (which was studied in depth by other later researchers, such as Jack Mostow in the card game Hearts). The idea is that the agent is given high level instructions on how to solve a problem, and must distill from it a low level policy. This is quite related to how humans learn complex tasks in many domains (e.g., driving, where a driving instructor may provide advice such as \"keep a certain distance from the car in front\"). \n\nA fairly complex neural deep learning controller architecture is used, although the details of this system are somewhat confusing in terms of many details that are presented. A simpler approach might have been easier to follow, at least initially. The experiments unfortunately are on a rather simplistic 2D maze, and it would have been worthwhile to see how the approach scaled to more complex tasks of the sort usually seen in deep RL papers these days (e.g, Atari, physics simulators etc.). \n\nNice overall idea, somewhat confusing description of the solution, and an inadequate set of experiments on a less than satisfactory domain of 2D grid worlds. ", "title": "Connections to programmable HAMs etc. ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SJgX36KMg": {"type": "review", "replyto": "SJttqw5ge", "review": "Thank you for writing this nice paper. I have a few questions:\n\n1. Please explain the notation in section 3.1. What are the parameters of the convolution layer? only b? Is the fully connected layer using W twice? the term fully connected seems to be an abuse of notation, otherwise please explain. Do you use this notation afterwards? if so, where? Is * denotes the convolution operation(3.1) or an inner product(3.2.1)? \n2. Section 3.2.1 Why is I in R^3? \n3. Subtask arguments, do you have an MLP for every possible sub-task?  is there a difference between a train and a test sub-task? do you train an MLP for the test sub-tasks as well? \n4. 3.2.2, please highlight the difference between c_t and the subtask termination signal (\\beta). In particular, can you use the termination signal instead? what would be the impact of this? \n5. Table 1. The difference between Concat+analogy to parameter+analogy is more significant in the accuracy measure. How do you measure accuracy for the concat version? \n6. Can you please explain why the baseline(FLAT) models make a good comparison with your architecture? \n7. Can you provide details regarding the test tasks that were solved/unsolved by the different agents? For example, if the agent knows to solve the tasks \"pick diamond\" and \"transfer cow\", it is reasonable to assume that it will also be capable to \"transfer diamond\". Did you see such correlations? \n8. Figure 2, please increase it significantly in size and add more of the paper notation above it. \n\nI would like to suggest the authors to remove some of the technical details from the main text and include them in the appendix instead. While the paper presents many interesting contributions, it is slightly hard to follow. The paper presents a hierarchical DRL algorithm that solves sequences of navigate-and-act tasks in a 2D maze domain. During training and evaluation, a list of sub-goals represented by text is given to the agent and its goal is to learn to use pre-learned skills in order to solve a list of sub-goals. The authors demonstrate that their method generalizes well to sequences of varying length as well as to new combinations of sub-goals (i.e., if the agent knows how to pick up a diamond and how to visit an apple, it can also visit the diamond). \n\nOverall, the paper is of high technical quality and presents an interesting and non-trivial combination of state-of-the-art advancements in Deep Learning (DL) and Deep Reinforcement Learning (DRL). In particular, the authors presents a DRL agent that is hierarchical in the sense that it can learn skills and plan using them. The skills are learned using a differential temporally extended memory networks with an attention mechanism. The authors also make a novel use of analogy making and parameter prediction. \n\nHowever, I find it difficult to understand from the paper why the presented problem is interesting and why hadn't it bee solved before. Since the domain being evaluated is a simple 2D maze, using deep networks is not well motivated. Similar problems have been solved using simpler models. In particular, there is a reach literature about planning with skills that had been ignored completely by the authors. Since all of the skills are trained prior to the evaluation of the hierarchical agent, the problem that is being solved is much more similar to supervised learning than reinforcement learning (since when using the pre-trained skills the reward is not particularly delayed). The generalization that is demonstrated seems to be limited to breaking a sentence (describing the subtask) into words (item, location, action). \n\nThe paper is difficult to read, it is constantly switching between describing the algorithm and giving technical details. In particular, I find it to be overloaded with details that interfere with the general understanding of the paper. I suggest moving many of the implementation details into the appendix. The paper should be self-contained, please do not assume that the reader is familiar with all the methods that you use and introduce all the relevant notations. \n\nI believe that the paper will benefit from addressing the problems I described above and will make a better contribution to the community in a future conference. \n", "title": "Questions", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJDFGUZEe": {"type": "review", "replyto": "SJttqw5ge", "review": "Thank you for writing this nice paper. I have a few questions:\n\n1. Please explain the notation in section 3.1. What are the parameters of the convolution layer? only b? Is the fully connected layer using W twice? the term fully connected seems to be an abuse of notation, otherwise please explain. Do you use this notation afterwards? if so, where? Is * denotes the convolution operation(3.1) or an inner product(3.2.1)? \n2. Section 3.2.1 Why is I in R^3? \n3. Subtask arguments, do you have an MLP for every possible sub-task?  is there a difference between a train and a test sub-task? do you train an MLP for the test sub-tasks as well? \n4. 3.2.2, please highlight the difference between c_t and the subtask termination signal (\\beta). In particular, can you use the termination signal instead? what would be the impact of this? \n5. Table 1. The difference between Concat+analogy to parameter+analogy is more significant in the accuracy measure. How do you measure accuracy for the concat version? \n6. Can you please explain why the baseline(FLAT) models make a good comparison with your architecture? \n7. Can you provide details regarding the test tasks that were solved/unsolved by the different agents? For example, if the agent knows to solve the tasks \"pick diamond\" and \"transfer cow\", it is reasonable to assume that it will also be capable to \"transfer diamond\". Did you see such correlations? \n8. Figure 2, please increase it significantly in size and add more of the paper notation above it. \n\nI would like to suggest the authors to remove some of the technical details from the main text and include them in the appendix instead. While the paper presents many interesting contributions, it is slightly hard to follow. The paper presents a hierarchical DRL algorithm that solves sequences of navigate-and-act tasks in a 2D maze domain. During training and evaluation, a list of sub-goals represented by text is given to the agent and its goal is to learn to use pre-learned skills in order to solve a list of sub-goals. The authors demonstrate that their method generalizes well to sequences of varying length as well as to new combinations of sub-goals (i.e., if the agent knows how to pick up a diamond and how to visit an apple, it can also visit the diamond). \n\nOverall, the paper is of high technical quality and presents an interesting and non-trivial combination of state-of-the-art advancements in Deep Learning (DL) and Deep Reinforcement Learning (DRL). In particular, the authors presents a DRL agent that is hierarchical in the sense that it can learn skills and plan using them. The skills are learned using a differential temporally extended memory networks with an attention mechanism. The authors also make a novel use of analogy making and parameter prediction. \n\nHowever, I find it difficult to understand from the paper why the presented problem is interesting and why hadn't it bee solved before. Since the domain being evaluated is a simple 2D maze, using deep networks is not well motivated. Similar problems have been solved using simpler models. In particular, there is a reach literature about planning with skills that had been ignored completely by the authors. Since all of the skills are trained prior to the evaluation of the hierarchical agent, the problem that is being solved is much more similar to supervised learning than reinforcement learning (since when using the pre-trained skills the reward is not particularly delayed). The generalization that is demonstrated seems to be limited to breaking a sentence (describing the subtask) into words (item, location, action). \n\nThe paper is difficult to read, it is constantly switching between describing the algorithm and giving technical details. In particular, I find it to be overloaded with details that interfere with the general understanding of the paper. I suggest moving many of the implementation details into the appendix. The paper should be self-contained, please do not assume that the reader is familiar with all the methods that you use and introduce all the relevant notations. \n\nI believe that the paper will benefit from addressing the problems I described above and will make a better contribution to the community in a future conference. \n", "title": "Questions", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}