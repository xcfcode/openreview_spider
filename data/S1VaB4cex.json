{"paper": {"title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "authors": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "authorids": ["larsson@cs.uchicago.edu", "mmaire@ttic.edu", "greg@ttic.edu"], "summary": "", "abstract": "We introduce a design strategy for neural network macro-architecture based on self-similarity.  Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals.  These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers.  In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks.  Rather, the key may be the ability to transition, during training, from effectively shallow to deep.  We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures.  Such regularization allows extraction of high-performance fixed-depth subnetworks.  Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "The paper describes a novel and simple way of constructing deep neural networks using a fractal expansion rule. It is evaluated on several datasets (ImageNet, CIFAR 10/100, SVHN) with promising results which are on par with ResNets. As noted by the reviewers, FractalNets would benefit from additional exploration and analysis."}, "review": {"S1cqRHSPg": {"type": "rebuttal", "replyto": "ryKsOZQvl", "comment": "> only the technicality in the paper is the fractal network architecture with intuitive claims\n\nOur experiments demonstrate two novel technical capabilities:\n(1) The ability to train fractal networks to exhibit an anytime property.  As the rebuttal clarifies, residual networks do not have this property.\n(2) The ability to \u201cpeel-off\u201d the deepest column of such a fractal network for use as a stand-alone network with high performance.  The resulting network has the same connectivity as a plain network, yet performs substantially better than a plain network trained in isolation.\n\nBoth claims are backed by quantitative performance numbers in the respective experiments.\n\nFrom (2), we can extrapolate that the fractal structure is a training scaffold, with its purpose being to allow the network to evolve from being effectively shallow to deep over the course of training.\n\n> Authors state in the paper that \" In experiments, fractal networks **match the state-of-the-art\nperformance held by residual networks** on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks\"\n\nYes, at the same depth (34-layers), FractalNet-34 matches the performance of ResNet-34 C.  \n\n> Veit et al demonstrates systematical empirical support for their claims about analyzing residual networks (which seems to be on arxiv since May but still not cited on paper ).\n\nAs detailed in the \"Explanatory power\" section of our rebuttal, we disagree with [Veit et al]\u2019s conclusions, disagreement shared with, e.g.,  [Greff et al, arXiv:1612.07771, ICLR 2017 submission].\n\nSince [Veit et al]\u2019s goals (analyzing ResNet) differ from ours (demonstrating alternatives, understanding very deep networks in general), we opted to be non-confrontational.  We now agree that it is better to cite [Veit et al] and [Greff et al] and state our disagreement.  We will add the citations and points made in our rebuttal to the final version of the paper.\n\n> can not report results on dozens of layers\n\nWe are puzzled by this comment. Table 1: results for 20 and 40 layer FractalNets; Table 2: 34 layers; Table 3: 20, 40, 80, 160 layers; Table 4: 20 and 40 layer networks. That\u2019s \"dozens\".\n\n> also in Table 3, error increases as depth increase to 160 layers\n\nThis characterization is inaccurate.  Error decreases gradually from 10 through 80 layers, then only slightly increases when going from 80 to 160 layers.  This shows diminishing returns with increased depth, but also robustness to being too deep; extra depth does not break trainability.\n\n> As an answer to lack of comparison to DenseNet, authors argue in rebuttal that DenseNet cites FractalNet but this can not be a reason for the lack of comparison as DenseNet was published in August and well-known as holding state of art results as a resnet variant. \n\nThe rebuttal clearly stated that we are happy to add DenseNet to our comparison tables (and we will).\n\nNote: like the Wide Residual Networks paper, *at the time of ICLR submission*, the DenseNet paper did not include results on ImageNet (https://arxiv.org/abs/1608.06993v2 revised on Nov 29 added ImageNet results for DenseNet).\n\n> Authors state in rebuttal that \"Many of the variants only reported results on CIFAR/SVHN\u2026 it is a bit difficult to have already compared to results that did not exist at submission time.\" .  However there were clearly published ones, for example two results by Huang et al, 2016b on july (arxiv) ..\n\nTable 1 (CIFAR/SVHN) has always included results for (Huang et al, 2016b, Stochastic Depth).  \n\nOn ImageNet, (Huang et al, 2016b) state that their model fails to improve over a baseline ResNet.  See Table 1 and Section 4 in their latest version: https://arxiv.org/abs/1603.09382v3\n\n> Regarding the \"simplifying power\" claim of authors.  It is not very convincing that it is simplifying. how can it simplify with a harder training procedure with many parameters that can not scale as good as baselines?\n\nOur training procedure is exactly as simple as ResNet\u2019s: train a network with a single attached loss in an end-to-end manner via SGD.\n\nDrop-path is optional for FractalNet, analogous to the manner in which stochastic depth is optional for ResNet.  They can each be used as additional regularization, at the cost of more complicated training.  Drop-path, in addition, can optionally enforce the anytime property.\n\nWe also note that, in terms of parameters, choosing to extract the deepest FractalNet column as a stand-alone high-performance network approximately halves the number of parameters used at test time.\n\n> The rebuttal lists unsatisfactory answers with somehow manipulative arguments\n\nWe are not sure how to respond to this unfortunate characterization of our arguments.\n", "title": "Re: rebuttal response"}, "HJcpm-LIe": {"type": "rebuttal", "replyto": "S1VaB4cex", "comment": "We thank the reviewers for their time, but disagree with many points in the reviews.  We first address what we feel is the overall disconnect between the reviews and the contributions of the paper, then rebut specific points below.\n\nMachine learning research is a mix of engineering, mathematics, and science.  Complete focus on engineering can restrict one to the narrow view of judging work by counting parameters and fractions of a percent in accuracy.  The reviews unfortunately reflect this mode of thinking.  However, long-term progress also requires scientific advancement, including new ideas as well as experiments that enhance understanding by revealing simple principles underlying complex phenomena.\n\nFractalNet is a new idea with:\n\n(1) Explanatory power:\n\nAs stated in the abstract, our experiments show that for the success of very deep networks, the \"key may be the ability to transition, during training, from effectively shallow to deep\".  This is the property shared by ResNet and FractalNet.\n\nFurthermore, this observation is an important counterpoint to [Veit et al., NIPS 2016] which claims that: (a) ensemble-like behavior is key and (b) the fraction of available effectively short paths is somehow related to performance.  FractalNet provides a counterexample: we can extract a single column (plain network topology with one long path) and it alone (no ensembling) performs as well the entire network.  In FractalNet, the rest of the network is a training apparatus for transitioning from shallow to deep.  This strongly suggests that, with more careful analysis, the same may be true for ResNet.\n\nIn fact, unlike [Veit et al., NIPS 2016] our explanation is consistent with the view that very deep networks, such as ResNet and FractalNet, learn unrolled iterative estimation [Greff et al., Highway and Residual Networks learn Unrolled Iterative Estimation, arXiv:1612.07771 and ICLR 2017 submission].  In this view, the longest path is most important; the ensemble can be discarded after training.  Moreover, our Section 4.3 and Figure 3 provide insight into the dynamics of this learning process.\n\nWe believe that the research community values understanding these mechanisms and would benefit from the evidence our work injects into the debate.\n\n(2) Simplifying power:\n\nFractalNet shows how a simple design principle produces structure similar to the mishmash of hacks (hand-designed modules, manually-attached auxiliary losses at intermediate depths) required in prior architectures like Inception.  It also expands the set of simple tricks for building very deep networks to include an option other than residual connections.\n\n(3) Good performance:\n\nFractalNet matches a ResNet of equal depth in terms of performance on ImageNet.  Training either of these networks from scratch takes weeks on modern GPUs.  Yet, because FractalNet uses slightly more parameters or does not beat the absolute best ResNet variant (developed after 7 or so papers iterating on ResNet) on the smaller and less important CIFAR dataset, the reviews declare experiments unsatisfactory.  There is a line between demanding high experimental standards and strangling promising ideas; we are all for the former, but hope not to fall victim to the latter.\n\n(4) New capabilities:\n\nContrary to AnonReviewer2's claim, prior work does not demonstrate an anytime property for ResNet; see our specific response below.  FractalNet, in combination with drop-path training, provides a novel anytime output capability, which could prove useful in real-world latency sensitive applications.\n\n\n-----\nResponse to AnonReviewer1\n\nPlease see our comments about experiments above.\n\n> Therefore the empirical effectiveness of drop-path is not convincing too.\n\nDrop-path serves two purposes: (a) additional regularization in the absence of data augmentation and (b) regularization that allows the resulting network to have the anytime property.  Yes, additional data augmentation can compensate for drop-path if one only cares about (a), but drop-path is essential for and the only effective means of enabling anytime output.\n\n> DenseNets (Huang et al, 2016a) should be also included in the comparison\n\nDenseNet cites FractalNet as the original version of FractalNet (arXiv:1605.07648) was published on arXiv three months prior to the original version of DenseNet (arXiv:1608.06993).  We are happy to alert readers to subsequent work, but please keep in mind the historical sequence of development.\n\n> Table 1 has Res-Net variants as baselines however Table 2 has only ResNet.\n\nMany of the variants only reported results on CIFAR/SVHN.  For example, the Wide Residual Networks paper (arXiv:1605.07146), at the time of the ICLR deadline (November 5), did not include ImageNet results.  It was updated weeks later, on November 28, to include ImageNet results.  We can expand the table, but it is a bit difficult to have already compared to results that did not exist at submission time.\n\n> there is no improvement in SVHN dataset results and this is not discussed in the empirical analysis.\n\nSection 4.2 observes \"most methods perform similarly on SVHN\".  The errors rates on SVHN are so low as to be uninformative.  We believe SVHN is simply too easy of a dataset to be a challenge to any of the modern techniques in Table 1.  Much like MNIST, SVHN is now a useful sanity check, but not a differentiator.\n\n> Also, authors give a list of some improvements over Inception (Szegedy et al., 2015) but again these intuitive claims about effectiveness of these changes are not supported with any empirical analysis.\n\nOur point is merely to draw connections between the FractalNet architecture and some hand-designed components of Inception, suggesting a more fundamental explanation for why those particular hand-designed choices are effective.  Widespread adoption of ResNet by the community has displaced Inception, VGG, and other architectures, making ResNet the clear leader and appropriate target for experimental comparison.\n\n\n-----\nResponse to AnonReviewer2\n\nPlease see our comments about experiments and the main contributions of the paper above.\n\n> The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.\n\nWe are demonstrating feasibility of parameter reduction tricks for FractalNet.  Much of the iterative improvement in the ResNet variants themselves, developed over many papers, already has a component of optimizing the architecture to reduce parameters and/or computational load.\n\n> Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts.\n\nThis statement is incorrect.  Our model is not directly related to Inception.  It reproduces some local connectivity reminiscent of Inception modules, but the global connectivity structure is entirely different. Even locally, FractalNet has a recursive join structure which Inception modules lack.  As consequence of this is that at global scale, FractalNet contains many paths whose length range over many orders of magnitude (all powers of 2 up to the maximum depth).  In contrast, the shortest path through an Inception network grows linearly with depth (number of modules stacked).\n\n> As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance\n\nImageNet performance is a useful benchmark tool, not the end goal of network design.  Mass adoption of ResNet by the community suggests that simplicity and scalability are also legitimate concerns.  With regard to scalability in depth, the only networks previously demonstrated to easily extend to the 100-1000 layer regime rely on some form of residual connection (ResNet/Highway Networks).  We demonstrate a 160-layer FractalNet, placing it in this group.\n\nMoreover, FractalNet and ResNet/Highway all have a mechanism for evolving from being effectively shallow to effectively deep over the course of training. This shallow to deep evolution seems critical, but unfortunately the design of Inception prevents its effective depth from being less than the number of stacked Inception modules.\n\n> It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.\n\nWe disagree.  The lesioning experiments in Veit et al. demonstrate the opposite: ResNet does not have an anytime property.  If deleting a few layers, then yes, ResNet can recover.  However, Section 4.2 and Figure 5 of Veit et al, show that deleting 10 blocks of a 54 block ResNet increases CIFAR-10 error to 0.2, deleting 20 blocks pushes error above 0.5.  So one cannot maintain a reasonable error if halving the ResNet depth.  Compare this to our Table 4: subnetwork columns of one half (20) and even one fourth (10) the layers of a full FractalNet (40) maintain low error on CIFAR-100.  This robustness across many orders of magnitude in depth (and thus time) is required for a useful anytime property and is unique to FractalNet (with drop-path training).\n\n> The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.\n\nDrop-path is essential for and the only effective means of enabling anytime output.  The fact that it can contribute as a regularizer in other scenarios is a bonus.\n\n\n-----\nResponse to AnonReviewer3\n\n> the experimental evaluation is not convincing, e.g. no improvement on SVHN\n\nAs we replied to AnonReviewer1: Section 4.2 observes \"most methods perform similarly on SVHN\".  The errors rates on SVHN are so low as to be uninformative.  We believe SVHN is simply too easy of a dataset to be a challenge to any of the modern techniques in Table 1.  Much like MNIST, SVHN is now a useful sanity check, but not a differentiator.\n\n> number of parameters should be mentioned for all models for fair comparison\n\nWe can add this to the table, but please see our larger discussion of experiments above.\n\n> the effect of drop-path seems to vanish with data augmentation\n\nDrop-path is essential for and the only effective means of enabling anytime output.  The fact that it can contribute as a regularizer in the absence of data augmentation is a bonus.\n", "title": "Rebuttal"}, "BylglxHme": {"type": "rebuttal", "replyto": "HJYdQPeme", "comment": "Table 1 includes ResNet variants developed through a total of 7 different papers.  The Wide Residual Networks paper alone reports results for 10 different variants in its Table 4, and we are showing their best (28-layer) one here.  This wide ResNet also required many parameters (36.5M).  While CIFAR is a useful dataset, at some point such exploration devolves into training on the test set.  \n\nOur own architectural exploration on CIFAR is far more restrained.  Ours is the first paper introducing the fractal architecture idea; perhaps subsequent ones can further optimize it.  We set B=5 because that is a natural choice for 32x32 input images.  A series of 5 spatial reductions, each by a factor of 2, reduces a 32x32 spatial grid to a 1x1 spatial grid, so a classification layer can be attached without making any additional decisions about pooling.  Table 3 shows graceful scaling with any value for C, while Table 1 shows good performance with C=3 with a simple 20-layer design whose channel count progression mirrors that of VGG/ResNet.  The 40-layer design in Table 2 is our first effort at reducing parameters in a smart way while increasing depth.  We believe there is room for further exploration here as results appear stable for this variant.  We have not yet run the 40-layer without augmentation, but will do so to fill in those entries of Table 1.\n\nImageNet training, being more time consuming, is less susceptible to the implicit leakage of test into training; we also tie ResNet performance in a fair comparison on ImageNet (Table 2).\n", "title": "Re: parameters and comparison"}, "B1Dr1eSme": {"type": "rebuttal", "replyto": "H1R1WQyXe", "comment": "> Why is a novel adjective 'ultra-deep' needed to describe these networks?\n\nOur intention is to be descriptive, not to define a new class of networks.  We are open to suggestions for revising terminology.  We want to convey the fact that fractal networks scale gracefully as depth increases.  Like residual networks, and unlike plain networks, fractal networks of any depth are trainable; our experiments show scaling from 5 to 160 layers.  A subtitle of \"deep neural networks without residuals\" is not informative; it would just mean any network other than residual networks.  Our architectural design is one of the few demonstrated suitable for training beyond 100 layers.\n\nPlease note that [Veit et al., Residual Networks Behave Like Ensembles of Relatively Shallow Networks, NIPS 2016] use \u201cultra-deep\u201d in reference to residual networks, so this terminology is not original to our paper.\n\n> What was the main motivation for experiment design?\n\nCIFAR-100/10 and SVHN have served as standard benchmark datasets for evaluating a wide variety of proposed neural network architectures.  We centered our experiments on these standard tasks in order to facilitate comparison with an extensive body of prior work, as shown in Table 1.  Training on ImageNet requires significantly more computational resources, but given its importance, we provide a performance comparison of FractalNet and ResNet on it in Table 2.  Our remaining experiments focus on examining the properties of fractal networks: scaling with depth (Table 3), anytime output (Table 4), and behavior during training (Figure 3).\n\n> What knowledge does this paper add over Inception networks, which already have long and short paths (as authors note)?\n\n(1) Unlike Inception networks, fractal networks contain paths whose lengths span over many orders of magnitude (all powers of two within each repeated block).  The Inception architecture has some slightly shorter paths, but does not have this great diversity in path length.\n\n(2) A simpler construction rule.  We show that repeated application of the fractal expansion rule generates network architectures that exhibit both paths of all lengths and cross sections reminiscent of Inception modules.  This elucidates principles behind the otherwise ad-hoc nature of Inception modules.\n\n(3) No auxiliary losses.  The Inception architecture required manual introduction of auxiliary losses at intermediate levels of the network in order to facilitate training.  Fractal networks eliminate this hack and are instead trained via a single loss attached at the end of the network.\n\n(4) An anytime property when trained with drop-path regularization.  We introduce a novel regularization scheme (drop-path) and show that after training with it, fractal networks can be partially evaluated at test time to yield a quick answer.  That is, evaluating a short path (treating a column within a fractal network as a \u201cplain\u201d network) yields output with decent accuracy.  Evaluating a longer path (column) yields even higher accuracy.  For real-time systems or latency-sensitive applications, fractal networks could provide a stream of outputs with increasing accuracy up to some limit on compute time.  There is no analogue of such capability in either Inception networks or residual networks.", "title": "Re: Questions about nomenclature and evaluations"}, "H1R1WQyXe": {"type": "review", "replyto": "S1VaB4cex", "review": "Why is a novel adjective 'ultra-deep' needed to describe these networks?\n\nWhat was the main motivation for experiment design?\n\nWhat knowledge does this paper add over Inception networks, which already have long and short paths (as authors note)?\nThis paper proposes a design principle for computation blocks in convolutional networks based on repeated application of expand and join operations resulting in a fractal-like structure. \n\nThis paper is primarily about experimental evaluation, since the objective is to show that a residual formulation is not necessary to obtain good performance, at least on some tasks.\n\nHowever, in my opinion the evaluations in the paper are not convincing. The primary issue is lack of a proper baseline, against which the improvements can be clearly demonstrated by making isolated changes. I understand that for this paper such a baseline is hard to construct, since it is about a novel architecture principle. This is why more effort should be put into this, so that core insights from this paper can be useful even after better performing architectures are discovered.\nThe number of parameters and amount of computation should be used to indicate how fair the comparisons are between architectures. Some detailed comments:\n\n- In Table 1 comparisons to Resnets, the resnets from He et al. 2016b and Wide Resnets should be compared to FractalNet (in lieu of a proper baseline). The first outperforms FractalNet on CIFAR-100 while the second outperforms it on both. The authors compare to other results without augmentation, but did not perform additional experiments without augmentation for these architectures.\n\n- The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.\n\n- A proper comparison to Inception networks should also be performed for these networks. My guess is that the reason behind a seemingly 'ad-hoc' design of Inception modules is to reduce the computational footprint of the model (which is not a central motivation of fractal nets). Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts, one can easily simplify the Inception design to build a strong baseline e.g. by converting the concatenation operation to a mean operation among equally sized convolution outputs. As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance [1].\n\n- It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.\n\n- The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.\n\nOverall, it's not clear to me that the experiments clearly demonstrate the utility of the proposed architecture. \n\n[1] Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. \"Inception-v4, inception-resnet and the impact of residual connections on learning.\" arXiv preprint arXiv:1602.07261 (2016).\n", "title": "Questions about nomenclature and evaluations", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rkUFJWfNl": {"type": "review", "replyto": "S1VaB4cex", "review": "Why is a novel adjective 'ultra-deep' needed to describe these networks?\n\nWhat was the main motivation for experiment design?\n\nWhat knowledge does this paper add over Inception networks, which already have long and short paths (as authors note)?\nThis paper proposes a design principle for computation blocks in convolutional networks based on repeated application of expand and join operations resulting in a fractal-like structure. \n\nThis paper is primarily about experimental evaluation, since the objective is to show that a residual formulation is not necessary to obtain good performance, at least on some tasks.\n\nHowever, in my opinion the evaluations in the paper are not convincing. The primary issue is lack of a proper baseline, against which the improvements can be clearly demonstrated by making isolated changes. I understand that for this paper such a baseline is hard to construct, since it is about a novel architecture principle. This is why more effort should be put into this, so that core insights from this paper can be useful even after better performing architectures are discovered.\nThe number of parameters and amount of computation should be used to indicate how fair the comparisons are between architectures. Some detailed comments:\n\n- In Table 1 comparisons to Resnets, the resnets from He et al. 2016b and Wide Resnets should be compared to FractalNet (in lieu of a proper baseline). The first outperforms FractalNet on CIFAR-100 while the second outperforms it on both. The authors compare to other results without augmentation, but did not perform additional experiments without augmentation for these architectures.\n\n- The 40 layer Fractal Net should not be compared to other models unless the parameter reduction tricks are utilized for the other models as well.\n\n- A proper comparison to Inception networks should also be performed for these networks. My guess is that the reason behind a seemingly 'ad-hoc' design of Inception modules is to reduce the computational footprint of the model (which is not a central motivation of fractal nets). Since this model is directly related to the Inception module due to use of shorter and longer paths without shortcuts, one can easily simplify the Inception design to build a strong baseline e.g. by converting the concatenation operation to a mean operation among equally sized convolution outputs. As an aside, note that Inception networks have already shown that residual networks are not necessary to obtain the best performance [1].\n\n- It should be noted that Residual/Highway architectures do have a type of anytime property, as shown by lesioning experiments in Srivastava et al and Viet et al.\n\n- The architecture specific drop-path regularization is interesting, but is used along with other regularizers such as dropout, batch norm and weight decay and its benefit on its own is not clear.\n\nOverall, it's not clear to me that the experiments clearly demonstrate the utility of the proposed architecture. \n\n[1] Szegedy, Christian, Sergey Ioffe, and Vincent Vanhoucke. \"Inception-v4, inception-resnet and the impact of residual connections on learning.\" arXiv preprint arXiv:1602.07261 (2016).\n", "title": "Questions about nomenclature and evaluations", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}