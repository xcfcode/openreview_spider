{"paper": {"title": "Differentially Private Synthetic Data: Applied Evaluations and Enhancements", "authors": ["Lucas Rosenblatt", "Xiaoyan Liu", "Samira Pouyanfar", "Eduardo de Leon", "Anuj Desai", "Joshua Allen"], "authorids": ["~Lucas_Rosenblatt1", "~Xiaoyan_Liu1", "sapouyan@microsoft.com", "eddeleon@microsoft.com", "andesai@microsoft.com", "joshuaa@microsoft.com"], "summary": "We present both extensive benchmarking for state-of-the-art differentially private synthesizers and QUAIL, an ensemble-based modeling approach to generating differentially private synthetic data with high utility.", "abstract": "Machine learning practitioners frequently seek to leverage the most informative available data, without violating the data owner's privacy, when building predictive models. Differentially private data synthesis protects personal details from exposure, and allows for the training of differentially private machine learning models on privately generated datasets. But how can we effectively assess the efficacy of differentially private synthetic data? In this paper, we survey four differentially private generative adversarial networks for data synthesis. We evaluate each of them at scale on five standard tabular datasets, and in two applied industry scenarios. We benchmark with novel metrics from recent literature and other standard machine learning tools. Our results suggest some synthesizers are more applicable for different privacy budgets, and we further demonstrate complicating domain-based tradeoffs in selecting an approach. We offer experimental learning on applied machine learning scenarios with private internal data to researchers and practitioners alike. In addition, we propose QUAIL, a two model hybrid approach to generating synthetic data. We examine QUAIL's tradeoffs, and note circumstances in which it outperforms baseline differentially private supervised learning models under the same budget constraint.", "keywords": ["privacy", "differential privacy", "generative adversarial networks", "gan", "security", "synthetic data", "evaluation", "benchmarking", "ensemble"]}, "meta": {"decision": "Reject", "comment": "The paper surveys existing differentially private data synthesis\nmethods, and introduces an algorithm that learns both a generator and\na classifier in a differentially private mode.\n\nThe problem is highly timely and important. Results are promising.\n\nMain remaining concerns after discussion between the reviewers and the\nauthors are:\n\n- reason why the proposed scheme can give better classification\naccuracy, should be clarified more\n\n- unclarity on conclusions that can be drawn from the experiments. The\nrevised version has improved on this somewhat.\n\nOne explanation for the problems was suggested to be that the paper\ntries, at the same time, to both present a new method and be a\nsurvey. Is hard to do in a short paper, and as a result, the paper\nlacks focus. At the very least, more work is needed.\n\nThe authors are encouraged to continue their work on this\nimportant problem, and the review comments hopefully help in that.\n"}, "review": {"OvfhXQdVXjT": {"type": "rebuttal", "replyto": "_pfZlDfGDBn", "comment": "\"My first comment still stands...\"\n\nPoint taken, we appreciate the advice about how we could structure the paper or papers for more impact!\n\n\n\"I do agree that DP methods can SOMETIME outperform non-DP methods...\"\n\nIt is true that there are sometimes cases where na\u00efve use of holdouts, or improper regularization, causes vanilla regression to perform worse than differential privacy.  Another way to look at this is that differential privacy gives some degree of regularization \"for free\", in settings where privacy is important.\n\nIn practice, we have observed generalization benefits in 3 scenarios: 1) differential privacy applied to aggregate features before training a model, 2) differential privacy applied during the training process (e.g. DP-SGD), and 3) differentially private synthetic data.  Note that #3, differentially private synthetic data, is the setting of the Dwork/Hardt paper on reusable synthetic holdouts (the \"Thresholdout\" algorithm is interactive in that setting, but the same ideas apply).\n\nWe feel that this is an underexplored area of research.\n\n\n\"In figures 6 and 7, it seems like...\"\n\nThe simplest explanation is that the epsilon budget (3.0) was more than sufficient in training the supervised learning model for this specific task. As we showed in some of the experimentation added as part of our revision, there are some scenarios where the embedded DP learning model, with a fraction of epsilon used for the vanilla scenario, performs almost as well as the vanilla scenario. In this case, we believe QUAIL can help allocate excess epsilon.", "title": "Response"}, "p9lKhca24bb": {"type": "rebuttal", "replyto": "VyFJM9sTpk", "comment": "Thank you for the constructive feedback! We address your questions and concerns in the following comments.\n\nOverall Author Comments:\nThank you for reviewing our paper and providing feedback. \n\nWe believe that we provide two clear contributions: the QUAIL hybrid method and a survey of existing state of the art DP techniques (PATE and DPSGD) when applied to an extensive array of scenarios (5 publicly available datasets, 2 large internal datasets, on wide range of \\epsilon budget). \n\nWe note that QUAIL should not be directly compared to existing methods in that it is an enhancement, not a standalone method, and so can be applied to any existing DP synthetic data method in conjunction with any DP supervised learning method.\n\nSpecific Responses:\n\nReviewer comment: \u201cThere is no mention of \\delta used for (\\epsilon,\\delta) - differential privacy.\u201d\n\nAuthors: It is true that we failed to highlight the \\delta used in the main body of the paper. We do, however, list our model parameters in the appendix,  including our \\delta (\u201ctarget_delta\u201d) parameters, which were all set at 1e-05. We will highlight this accordingly in our final revision.\n\n\nReviewer comment: \u201cPlease use something else than \\delta for difference between performance measures as it can get confusing\u201d\n\nAuthors: This is a good point, and we will update our figures to use something else to express the difference in our final revision.\n\n\nReviewer comment: \u201cAs the version of GAN used in the paper is conditional, shouldn't the generator be differentially private as well? i.e. how are we protecting the privacy of the labels\u201d\n\nAuthors: Based on the post-processing property that any randomized mapping of a differentially private output, is also differentially private, the generator is guaranteed to be differentially private when the generator is trained to maximize the probability of D(G(z)) .  In CTGAN, the authors add the cross-entropy loss between conditional vector and produced set of one-hot discrete vectors into the generator loss. To guarantee differential privacy with the generator, we removed the cross-entropy loss when calculating generator loss.  Thus, the generator is differentially private as well. \n\nReviewer comment: \u201cThere is a mention of PATECTGAN performing better even compared to the non-noisy model trained on real data, how is this possible, please add some explanation.\u201d\n\nAuthors: Training supervised learning models on fully synthesized datasets has been observed to at times outperform the same supervised learning model on the real data. Dwork et al. suggested that it can reduce overfitting. They said: \u201cThe intuition is that if we can learn about the data set in aggregate while provably learning very little about any individual data element, then we can control the information leaked and thus prevent overfitting.\u201c Furthermore, in line with other recent literature, we concluded that, with fine tuning, high-quality synthetic data (like that generated by PATECTGAN at higher epsilons) can be more useful for pre-training than the real data that may not contain a more optimal distribution of edge cases. We will include a note on this in our revision \u2013 and in practice we often observe this phenomena.\n\n(See Dwork, Cynthia, et al. \"The reusable holdout: Preserving validity in adaptive data analysis.\" Science 349.6248 (2015): 636-638. See An Annotation Saved is an Annotation Earned: Using Fully Synthetic Training for Object Instance Detection, https://arxiv.org/abs/1902.09967. See McCormac et al. SceneNet RGB-D: Can 5M Synthetic Images Beat Generic ImageNet Pre-Training on Indoor)", "title": "Response to Reviewer 1"}, "U432pN7D1R": {"type": "rebuttal", "replyto": "umUTphXx83H", "comment": "Thank you for the constructive feedback! We address your questions and concerns in the following comments.\n\nOverall Author Comments:\nThank you for expressing your appreciation for our \u201cintegrity in presenting all the results no matter positive or negative\u201d and for our \u201ceffort in conducting experiments on quite a few datasets.\u201d We were motivated to conduct our extensive experiments and represent them faithfully after we observed a lack of similarly broad, candid experimentation on synthetic data in current literature.\n\nSpecific Responses:\n\nReviewer comment: \u201cThe main issue is that we cannot draw a clear conclusion from the experimental results. I would suggest taking one or two epsilon values, and look at the results in more detail to see if we can find any trend.\u201d\n\nAuthors: This is a useful suggestion. We have further investigated the epsilon value of 3.0 in our revision, especially in trying to peel back the QUAIL method to produce some justification for why it works. We have included our justification here, and new figures in the paper body:\n\u201cBy first assessing the TSNE clustering in Figures 8 and 9, we see that not only is the synthetic data produced by QUAIL very similar to the real data, but the accuracy of the labeling for the embedded model (in this case, DPLR) is also very similar. Further investigation into data scale revealed that the QUAIL method takes advantage of allocating excess epsilon when datasets are large. As datascale increases, the sensitivity of the differentially private model decreases and so less epsilon can be used more efficiently. Thus, we see that an exaggerated difference between DPLR embedded in QUAIL (with an epsilon of 2.4) and DPLR with an epsilon of 3.0 for a dataset of 20,000 samples. In this case, the embedded DPLR model accuracy suffers, and so does the learning utility of the produced synthetic data. Conversely, as we increase the data size to 50,000 and 100,000 samples, we see that the internal model (with epsilon 2.4) can match the performance of the vanilla model (with epsilon 3.0). Then, the synthetic dataset serves only to augment the performance by small but significant margin (in Figure 10, we see a bump of three percent to f1 score.\u201d\n\n\nReviewer comment: \u201cThe notations in the algorithm should be clearly explained/defined before the algorithm (for example, N and X), and some intuition can be added after the algorithm description. In the algorithm description itself, I think maybe it's clearer to defined the target dimension and the rest of the dimension separately, i.e. defining one sample as (feature, target) and later on we will have (synthetic feature, synthetic target). In the \"split\" part, I guess eps_C should be (1-p)*eps.\u201d\n\nAuthors: Thank you for the notes on the notation/format of the QUAIL algorithm. We have made them in our revision. If the review could clarify what they meant by \u201c, i.e. defining one sample as (feature, target) and later on we will have (synthetic feature, synthetic target),\u201d we can complete the changes for our final revision. \n\n\nReviewer comment: \u201cThe part below Theorem 3.1 might better be put into the experiment section than the algorithm section\u201d\n\nAuthors: Thank you, we have moved this in our revision.\n\n\nReviewer comment: \u201cIn quite a few figures, we see interesting trends like some algorithm can have worse utility as epsilon grows. So it might be important to report the standard deviation of the algorithm for readers to better understand what was going on. Also, the texts in the figures can be made larger\u201d\n\nAuthors: Thank you! In our final revision, we will add standard deviations in our figures, and increase the text size.\n\n\nReviewer comment: \u201cThe paper called the algorithm \"ensemble method\". I feel like ensemble means something specific in ML, and simply using two different models together doesn't quite seem like ensemble. Maybe I'm understanding something here but it should be better explained\u201d\n\nAuthors: This is a good point \u2013 your understanding is correct. It is slightly misleading to describe QUAIL as an \u201censemble\u201d method between two models as it doesn\u2019t do any traditional boosting, bagging, etc. In our revision, we have reworded and removed references to QUAIL as an ensemble, replacing it with \u201chybrid approach.\u201d", "title": "Response to Reviewer 2"}, "BVtiob0wwAg": {"type": "rebuttal", "replyto": "VJP8uDw3Ako", "comment": "Thank you for the constructive feedback! We address your questions and concerns in the following comments.\n\nOverall Author Comments:\nWe appreciate your interest in our proposed method. We encourage you to consider that our extensive experiments, especially in applied scenarios, with existing DP synthetic data (and modifications to existing methods) represent a valuable secondary contribution.\n \nSpecific Responses:\n\nReviewer comment: \u201cOne limitation of this manuscript is that the reason why the proposed scheme can give better classification accuracy is not discussed\u2026 One quick thought is that the proposed scheme preserves the cluster structure of the samples well and therefore the classification model trained with the resulting sample has good accuracy.\u201d\n\nAuthors: This is a useful suggestion, and something had explored in our experimentation. We have included an analysis of the \u201ccluster structure\u201d as part of our revision.\n\n\nReviewer comment: \u201cAlso, the reason why the RMSE of the regression model trained with this scheme is worse than other methods is not examined, either\u2026 In contrast, the metric structure behind the samples is not preserved well and therefore the regression model does not have good RMSE.\u201d\n\nAuthors: We believe we addressed this concern on page 7, in section 5. We wrote \u201cFor QUAIL-enhanced models, the RMSE is considerably larger than the real and other DP synthetic data. We attribute this to a weakness of the embedded regression model (DP Linear Regression) in QUAIL for this data scenario.\u201d We appreciate the reviewer\u2019s thoughts, and believe there could be some validity to their statement. However, we are confident in our assertion that the weakness of the DP Linear Regression model embedded in QUAIL is the root of the overall poor performance. We present results that suggest a more robust DP Linear Regression model embedded in QUAIL would lead to improved performance in our revision.\n\n\nReviewer comment: \u201cMinor: FIg 6 is in the Appendix, not in the main body. Also, many important claims (mainly in experimental results) are given with results in the Appendix. The main claim should be constructed with the contents in the main body. \u201c\n\nAuthors: This is a good point. We have move Figures 6 and 7 into the main body, as well as some other pertinent figures in our revision.\n", "title": "Response to Reviewer 3"}, "VyFJM9sTpk": {"type": "review", "replyto": "ABZSAe9gNeg", "review": "The paper proposes QUAIL, an ensemble of a generative model and a classifier, where both are trained with differential privacy, in order to generate a differentially private dataset (from the generative model) and a label vector (from the classifier). The paper further compared QUAIL with \"other\" differentially private generative models based on conditional GAN.\n\nMy main concern with the paper is that there is no clear contribution. It is hard to judge whether the main claim is that the paper presents a survey of the current differentially private generative models, in which case, the survey part is very short and not in-depth. Or is that the the paper proposes QUAIL, which in many cases performs worse than other generative models. My some other concerns are detailed below:\n\n- There is no mention of \\delta used for (\\epsilon,\\delta) - differential privacy\n- Please use something else than \\delta for difference between performance measures as it can get confusing\n- As the version of GAN used in the paper is conditional, shouldn't the generator be differentially private as well? i.e. how are we protecting the privacy of the labels?\n- There is a mention of PATECTGAN performing better even compared to the non-noisy model trained on real data, how is this possible, please add some explanation.\n", "title": "Review", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "umUTphXx83H": {"type": "review", "replyto": "ABZSAe9gNeg", "review": "The authors proposed QUAIL, an algorithm that uses a supervised model and a synthetic data model to generate synthetic data that is good for downstream tasks. It also shows some empirical evaluations of the algorithm.\n\nThe technical part, especially the experiments, might need some improvement. The main issue is that we cannot draw a clear conclusion from the experimental results. I would suggest taking one or two epsilon values, and look at the results in more detail to see if we can find any trend. Also, generating synthetic data under differential privacy is not an easy task, so I think it's ok to skip the regime where epsilon < 1, or try even larger epsilon to get a reasonable utility first.\nHowever, I would like to say that I very much appreciate the authors' effort in conducting experiments on quite a few datasets, and their integrity in presenting all the results no matter positive or negative.\n\nThe paper can be better organized, for example,\n- The notations in the algorithm should be clearly explained/defined before the algorithm (for example, N and X), and some intuition can be added after the algorithm description. In the algorithm description itself, I think maybe it's clearer to defined the target dimension and the rest of the dimension separately, i.e. defining one sample as (feature, target) and later on we will have (synthetic feature, synthetic target). In the \"split\" part, I guess eps_C should be (1-p)*eps.\n- The part below Theorem 3.1 might better be put into the experiment section than the algorithm section.\n- In quite a few figures, we see interesting trends like some algorithm can have worse utility as epsilon grows. So it might be important to report the standard deviation of the algorithm for readers to better understand what was going on. Also, the texts in the figures can be made larger.\n- The paper called the algorithm \"ensemble method\". I feel like ensemble means something specific in ML, and simply using two different models together doesn't quite seem like ensemble. Maybe I'm understanding something here but it should be better explained.\n", "title": "Need some improvement", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "VJP8uDw3Ako": {"type": "review", "replyto": "ABZSAe9gNeg", "review": "The proposed method works as follows. Given samples are partitioned into two parts; one is for classifier training and the other is for data synthesizer training. Both are trained in a differentially private manner. After training, the DP synthesizer generates samples and the DP classifier labels them so that the resulting samples can be used as training samples. By the post-processing theorems, the resulting are differentially private, which are published as synthesized samples.\n\nThe idea is interesting, simple, and unique. Also, experimental results demonstrate that the  models trained with the proposed method give s better F1 score compared to the existing methods. One limitation of this manuscript is that the reason why the proposed scheme can give better classification accuracy is not discussed. Also, the reason why the RMSE of the regression model trained with this scheme is worse than other methods is not examined, either. One quick thought is that the proposed scheme preserves the cluster structure of the samples well and therefore the classification model trained with the resulting sample has good accuracy. In contrast, the metric structure behind the samples is not preserved well and therefore the regression model does not have good RMSE. I am not sure this is correct or not, but anyway, I think further consideration on these issues will be interesting and needed for this type of experimental study to find a clue to improve data synthesization with DP guarantee.\n  \nMinor:\nFIg 6 is in the Appendix, not in the main body. Also, many important claims (mainly in experimental results) are given with results in the Appendix. The main claim should be constructed with the contents in the main body. \n\n \n\n\n\n\n\n", "title": "The idea is interesting and simple whereas no clear reason that the proposed method works well is not explained.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}