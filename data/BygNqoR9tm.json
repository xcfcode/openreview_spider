{"paper": {"title": "Sinkhorn AutoEncoders", "authors": ["Giorgio Patrini", "Marcello Carioni", "Patrick Forr\u00e9", "Samarth Bhargav", "Max Welling", "Rianne van den Berg", "Tim Genewein", "Frank Nielsen"], "authorids": ["patrinig@hotmail.com", "marcello.carioni@uni-graz.at", "patrickforre@gmail.com", "samarth.bhargav@student.uva.nl", "welling.max@gmail.com", "riannevdberg@gmail.com", "tim.genewein@de.bosch.com", "nielsen@lix.polytechnique.fr"], "summary": "", "abstract": "Optimal Transport offers an alternative to maximum likelihood for learning generative autoencoding models. We show how this principle dictates the minimization of the Wasserstein distance between the encoder aggregated posterior and the prior, plus a reconstruction error. We prove that in the non-parametric limit the autoencoder generates the data distribution if and only if the two distributions match exactly, and that the optimum can be obtained by deterministic autoencoders.\nWe then introduce the Sinkhorn AutoEncoder (SAE), which casts the problem into Optimal Transport on the latent space. The resulting Wasserstein distance is minimized by backpropagating through the Sinkhorn algorithm. \nSAE models the aggregated posterior as an implicit distribution and therefore does not need a reparameterization trick for gradients estimation. Moreover, it requires virtually no adaptation to different prior distributions. We demonstrate its flexibility by considering models with hyperspherical and Dirichlet priors, as well as a simple case of probabilistic programming. SAE matches or outperforms other autoencoding models in visual quality and FID scores. ", "keywords": ["generative models", "autoencoders", "optimal transport", "sinkhorn algorithm"]}, "meta": {"decision": "Reject", "comment": "The reviewers appreciated the contribution of combining Wasserstein Autoencoders with the Sinkhorn algorithm.\n\nYet R4 as well as the author of the WAE paper (Ilya Tolstikhin) both expressed concerns about the empirical evaluation.\n\nWhile R1-R3 were all somewhat positive in their recommendation after the rebuttal, they all have somewhat lower confidence reviews, as is also clear by their comments.\n\nThe AC decided to follow the recommendation of R4 as they were the most expert reviewer. The AC thus recommends to \"revise and resubmit\" the paper."}, "review": {"S1x4RtP6hQ": {"type": "review", "replyto": "BygNqoR9tm", "review": "Summary\n\nThis paper builds upon the recent Wasserstein Auto-Encoders; the main innovation is the use of the Sinkhorn distance between the prior and the aggregate posterior. This distance, is used as a differentiable and more tractable surrogate for the wasserstein distance, which is proposed as an alternative to heuristics (e.g. MMD) in Wasserstein-auto encoders.\n\nAlso, the authors provide some theoretical results complementing the Wasserstein Auto-Encoders papers and illustrate their results showing better or equal performance than with alternatives, particularly VAE and WAE.\n\nEvaluation.\nIt is certainly a good paper, and well written (but notice several typos). The experimental part is thorough which certainly plays in favor at evaluation time. Theoretical results are a significant contribution, but not quite deep.\n\n But what troubles me is that when I read the paper it was hard for me to understand what the contribution is. My guess on this is what I wrote in the Summary. And if that is the case, I think it is a rather marginal contribution: it would be desired to have a theory for why using the wassertein penalization is better than the ones proposed in the WAE paper (Recall, there, penalizations are introduced to get rid of a constraint that appears in the minimization paper). The authors argue that Sinkhorn does as good as wasserstein and I can believe that (but it would be good if authors could refer to recent results [1] regarding the quality of this approximation, which essentially say that sinkhorn does no do miracles) but the most fundamental question is why a wasserstein penalization is better than the ones proposed in WAE. If no such theory is available, it is hard for me to judge the paper as non-marginal.\n\nTheorem 3.1 is an attempt for such an explanation, but it was not clear at all for me why theorem 3.1 implies that wasserstein distance is the penalization to use. Theorem 3.1 only gives an upper bound. The authors may elaborate on this\n\n\nI hope my criticism is helpful for improving the current version of the manuscript.\n\n\n[1] http://proceedings.mlr.press/v75/weed18a/weed18a.pdf\n\n\n=== after rebuttal ===\n\nThe authors have addressed my concerns and I have updated my score accordingly.", "title": "Contributions should be better explained", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJlAP_3fAQ": {"type": "rebuttal", "replyto": "BygNqoR9tm", "comment": "We have uploaded a revised version of our submission and we kindly invite the reviewers to take it into consideration. Following the reviewers' feedback, we have improved Section 3 and clarified how one obtains our objective function in a principled manner. In particular, one can rewrite W(Px, Pg) by *an equality* with the sum of (p-th root of) reconstruction error and W(Qz, Pz). This is now Corollary 3.3, which follows from the two main Theorems. No such equality for a penalized reconstruction is known for WAE, and even more [A] argues that any soft constraint to enforce Qz =~ Pz yields a minimization of a lower bound.\n\n[A] Bousquet, Olivier, et al. \"From optimal transport to generative modeling: the VEGAN cookbook.\" arXiv preprint arXiv:1705.07642 (2017).", "title": "Updated paper"}, "B1gZb2ky07": {"type": "rebuttal", "replyto": "SJeod5c_6X", "comment": "EDIT: more comments on WGAN, heuristics and a correction about the FID scores \n===\n\nIn the updated submission, we are going to show more clearly how Theorem 3.1 is fundamental for the construction of our learning objective. We will also show that the provided bound \u2014 which uses a Wasserstein distance as latent space penalty \u2014 provides us with a tight bound for learning the generative model. On this regard, a variant of WAE that uses a WGAN as penalty would in fact optimize a similar objective. Although, an interesting point on this regard is raised in [A]: a penalty derived by a GAN would provide a *lower bound*, which is the opposite of what needed in the WAE/SAE minimization problem. Still, it is true that their relative quality could be measured experimentally; yet we are not aware of any published work on a WAE-WGAN and we believe it is unfair to require a novel combination of methods as a comparative baseline.\n\nAdditionally, as stated in a review below: we did not compare with a WAE-(W)GAN or ALI as we are interested in deepening the understanding and improving the quality of auto-encoders as in their min-min kind of objective -- this really follows WAE's motivations as well. Notice that we could also extend SAE with an adversarial cost in latent space, by learning the transportation cost c. We believe that only this extension of SAE would be a fair comparison with WAE-GAN. We will leave this direction for future work.\n\nWe call WAE-GAN/MMD heuristics relatively to the fact that we can justify the alternative choice of the Wasserstein distance due to Theorem 3.1. We will rephrase this argument. Moreover, when one introduces the WAE's objective by means of Lagrangian multipliers, optimization consists of a minimization of a lower bound of the problem [A].\n\nWe did not discuss Sinkhorn's convergence in terms of batch training. Yet previous work such as [B, and others referred in our paper] showed the empirical merits of minibatch training by backpropagating through the Sinkhorn. An analysis would be an interesting addition but we consider it outside the scope of this work.\n\nWe will improve the paper presentation in order to make these points clear:\n* Empirical lower bound for MMD: they are computed as the MMD between two (independent) set of N samples from the prior, where N is the same same of the test test. The number gives us a lower bound for the MMD between the aggregated posterior and prior. \n* Missing SAE with normal prior in table 2: while we could have reported those numbers for completeness, we only experimented with hypersphere in the section and still obtained competitive results. We don\u2019t see this as a limitation, as we consciously did not attempt to tune the prior as hyper-parameter in order to improve the results. Please notice that we use several shapes of priors with SAE in other experiments, also Normal (6.5).\n* Thanks for providing a pointer to [3], of which we were not aware. Following the paper perspective, we agree that the FID bias may be relevant in our experimental results. Time permitting (either during this review or for the final version), we will report FID scores on the MNIST test set and study their difference. On the use of a FID-LeNet score, we do not believe that a numerical comparison with other work on the MNIST FID score would be too insightful, due to the simplicity of the dataset. Our strategy and network for computing the FID were inspired by [C].\n\n[A] Bousquet, Olivier, et al. \"From optimal transport to generative modeling: the VEGAN cookbook.\" arXiv preprint arXiv:1705.07642 (2017).\n[B] Genevay, A., Peyr\u00e9, G., & Cuturi, M. (2017). Learning generative models with sinkhorn divergences. arXiv preprint arXiv:1706.00292.\n[C] Odena, Augustus, et al. \"Is Generator Conditioning Causally Related to GAN Performance?.\" arXiv preprint arXiv:1802.08768 (2018).", "title": "We thank the reviewer for the detailed analysis and for the several suggestions for improvement."}, "Sygi8JJkCX": {"type": "rebuttal", "replyto": "HJeduotupm", "comment": "Thank you for the insightful comments. \n\nWe agree that in general it would be very difficult to fit a distribution in high dimensions with a distribution coming from a low dimensional space using (continuous) neural networks. But note that the encoder maps from the high dimensional image space to the low dimensional latent space, which is a much easier task. So the feasible set of the constraints are not necessarily empty and much easier to approximate using neural networks. For example even though finding a neural network that fits a 2-dimensional Gaussian from 1-dimensional one is only possible in the high capacity limit of a neural network, fitting a 1-dimensional Gaussian from a 2-dimensional Gaussian only needs a projection and a linear map to match. \n\nIn response to the comment that \"WAE-MMD and SAE are *identical* up to\" the penalty terms: in the stated generality all constructions based on variational auto-encoders are identical up to the penalty term: reconstruction plus some form of matching (aggregated) posterior and prior plus other divergence quantities. In this regard, any WAE variant could be seen as implementation of the idea behind adversarial auto-encoders [A], up to the variations of the penalty term. Yet, WAE is built on an elegant and insightful derivation justified by the use of Wasserstein distances. We believe our theoretical derivation provides an even stronger motivation for the construction of a learning objective as we formulate it.  Furthermore, using the Wasserstein penalty allows us to give a theoretical bound on the error. In the updated submission we will analyse more in detail this bound, showing its tightness, and behaviour in the case of deterministic neural network encoders as model class as well.\n\n[A] Makhzani, Alireza, et al. \"Adversarial autoencoders.\" arXiv preprint arXiv:1511.05644 (2015).", "title": "On fitting distributions with continuous functions and novelty"}, "SJeod5c_6X": {"type": "review", "replyto": "BygNqoR9tm", "review": "The paper introduces a new cost function for training Wasserstein Autoencoders that combines reconstruction error with Sinkhorn distance on the latent space. Authors provide nice theoretical motivation, yet empirical results seem incremental and do not fully support the effectiveness of this approach.\n\nPros:\n- Theorem 3.1 (although trivial) provides motivation for optimizing Wasserstein distance in the latent space in WAEs.\n- Theorem 3.2 shows sufficiency of optimization over deterministic encoders in WAEs.\n- The proposed SAE virtually does not favor any prior and can preserve some aspects of geometry of the original space. \n\nCons:\n- It is unclear why Sinkhorn algorithm would provide better estimate of Wasserstein distance than e.g. adversarial WGANGP (which would be a variant of GAN-WAE). Sinkhorn convergence is discussed only in terms of sample size and  smoothing regularizer, not in the context of batch training. \n- Quantitative results are on par or marginally better than other methods, they also lack some comparisons (see details below).\n- There is no comparison to relevant models outside VAE scope, e.g. ALI [4]. \n\nThe novelty of this paper is combining WAEs with Sinkhorn algorithm. Overall, it has potential, but the proposed method would probably require clearer evaluation. \n\nDetailed issues:\n- Notation for posterior seems somewhat inconsistent and misleading, namely push-forward G#P_Z = P_G, while Q#P_X = Q_Z.\n- It is unclear why MMD or GAN losses on WAS's latent space are referred to as heuristics, each of these constitutes a divergence in the same way as the proposed Sinkhorn distance.\n- FID scores for MNIST are incomparable due to the use of own network; using LeNet has been proposed [3].\n- It is unclear what \u2018Empirical lower bounds\u2019 for MMD mentioned in Table 1. caption mean, as unbiased MMD estimator (e.g. [2]) is available. On the other hand, FID is known to be biased [3], so test-set FID should be provided for comparison.\n- Table 2. lacks comparison of SAE with normal prior even though a) authors note that MMDs are incomparable with different priors, b) SAEs is claimed to be prior-agnostic, c) in such setting MMD-WAE might be advantageous [1]. Again, no test-set FID scores.\n- Samples in Figure 2 too small.\n- MMD lacks citation (e.g. [2]).\n\nTypos:\np.6 line 3 construcetion -> construction\np.6 line 30 Hypersherical -> Hyperspherical \nP.8 line 1 this a sign -> this is a sign\n\n[1] Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Sch\u00f6lkopf. Wasserstein Auto-Encoders. ICLR 2018.\n[2] Arthur Gretton, Karsten M. Borgwardt, Malte J Rasch, Bernhard Sch\u00f6lkopf, and Alex J. Smola. A kernel two-sample test. The Journal of Machine Learning Research, 13, 2012a.\n[3] Miko\u0142aj Bi\u0144kowski, Dougal J. Sutherland, Michael Arbel, Arthur Gretton. Demystifying MMD GANs. ICLR 2018.\n[4] Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Arjovsky and Aaron Courville. Adversarially Learned Inference. ICLR 2017\n", "title": "Good motivation but empirical evidence shows limited improvements.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1ghcbxM6m": {"type": "rebuttal", "replyto": "rJxPy2YZp7", "comment": "Thank you for your comment. We would like to make some clarity on how we utilize the Sinkhorn algorithm in this work, which is similar to the insight made in [B]. The Sinkhorn composes O(L) operations, which are mostly linear and, more importantly, they are all differentiable. For the purpose of back propagation, we can interpret those as a stack of layers to compute the loss function for the encoder in latent space. We remark that there is no need of alternating optimization \u2014 as running the Hungarian algorithm would require. Instead, we differentiate through the Sinkhorn operations to compute an approximation of the Wasserstein distance in latent space.\n\n[B] Genevay, A., Peyr\u00e9, G., & Cuturi, M. (2017). Learning generative models with sinkhorn divergences. arXiv preprint arXiv:1706.00292.", "title": "Reply to AnonReviewer1"}, "S1e0fT3eaX": {"type": "rebuttal", "replyto": "BygNqoR9tm", "comment": "We thank the reviewers for the analysis of our work.\n\nWe see our theory, and in particular the statement in Theorem 3.1, as a fundamental contribution of our work, which goes beyond a simple extension of the WAE framework. The issues raised by the reviewers give us the opportunity for a more detailed comment in support of the use of the Wasserstein distance in latent space. We will work on a better presentation of the following arguments for the camera ready.\n\nRecall that WAE uses either a MMD or implements a discriminator which minimizes a JS-divergence, i.e symmetrized KL divergence. Instead, the use of a Wasserstein distance is relevant because:\n- Wasserstein is upper bounded by JS. See Theorem 2 in [A] . This property is often mentioned as Wasserstein inducing a weaker topology with respect to JS. While small JS implies small Wasserstein, the converse is not true and therefore minimizing Wasserstein can be beneficial in practice.\n- Sinkhorn takes the best of Wasserstein and MMD, and indeed can be characterized as interpolating between the two. See [B] and the more recent [C].\n\nThe use the Wasserstein distance in the latent space of a generative auto-encoder is theoretically justified by Theorem 3.1. The validity of Theorem 3.1 relies on the properties of the Wasserstein distance. In fact, the triangle inequality is only possible for metrics, not for general divergencies like KL.\n\nMoreover the bound in Theorem 3.1 is sharp: if one of the terms is zero then the inequality is actually equality. Furthermore, the Lipschitz constant inequality is also tight: if aggregated posterior and prior match, the actual term of the triangle inequality is zero and again equality follows \u2014 see also Theorem 3.3.\n\nThe arguments are abstract, but simple, and directly give a consistent optimization objective in one framework. In contrast, WAE makes and elegant and detailed analysis first, but finally resorts to Lagrange multipliers with ad-hoc divergences, therefore partially abandoning the framework of Optimal Transport.\n\nWe thank AnonReviewer3 for pointing out the analysis of [1]. We referred to [D] which also provide finite sample guarantees. We will update our references.\n\nRegarding the specific questions of AnonReviewer2, we did not compare with a WAE-GAN as we are interested in deepening the understanding and improving the quality of auto-encoders as in their min-min kind of objective -- this really follows WAE's motivations as well. Notice that we could also extend SAE with an adversarial cost in latent space, by learning the transportation cost c. We believe that only this extension of SAE would be a fair comparison with WAE-GAN. We will leave this direction for future work.\n\nThe computational overhead of running the Sinkhorn with respect to, e.g. MMD, is always relative to the neural network architecture. With CelebA and CIFAR10 we train high capacity networks and the cost of running the Sinkhorn is negligible. One can interpret its computation as stacking several (mostly linear) layers on top of the encoder, with a width that is proportional to the batch size. Experiments with very small \\epsilon, which can be expensive due to a large L, did not show a clear positive impact on generative performance of our models; thus we did not consider running the Sinkhorn with small entropy regularization. In summary, we did not observe large overhead, except when the neural network is very simple, such as in the toy experiments. We will include an empirical analysis in final version.\n\n\n[A] Arjovsky, M., Chintala, S., & Bottou, L. (2017). Wasserstein gan. arXiv preprint arXiv:1701.07875.\n[B] Genevay, A., Peyr\u00e9, G., & Cuturi, M. (2017). Learning generative models with sinkhorn divergences. arXiv preprint arXiv:1706.00292.\n[C] Feydy, J., S\u00e9journ\u00e9, T., Vialard, F. X., Amari, S. I., Trouv\u00e9, A., & Peyr\u00e9, G. (2018). Interpolating between Optimal Transport and MMD using Sinkhorn Divergences. arXiv preprint arXiv:1810.08278.\n[D] Weed, J., & Bach, F. (2017). Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance. arXiv preprint arXiv:1707.00087.", "title": "Reply to AnonReviewer2 and AnonReviewer3"}, "rJxPy2YZp7": {"type": "review", "replyto": "BygNqoR9tm", "review": "I tried to understand the paper it seems to me that the paper proposed a new objective function for learning an autoencoder based on Sinkhorn algorithm. Following the idea of Sinkhorn algorithm, the Sinkhorn autoencoder minimizes the W-distance between aggregated posterior and the data prior via integrating the objective of (original) autoencoder and Sinkhorn distance. This looks new, but I did not look insight to the detailed derivations. One thing that I do not like is that Sinkhorn distance needs to be optimized before optimizing the autoencoder distance, if I understand correctly.", "title": "summary", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJlBQj-a3X": {"type": "review", "replyto": "BygNqoR9tm", "review": "The paper proposes a new representation of Wasserstein AutoEncoder and provides the formal analysis of learning autoencoders with optimal transport theory. The proposed model, SAE, employs the constraints on the equality of prior and posterior latent spaces with a Sinkhorn distance. Moreover, the proposed model is also backed up with some theoretical guarantees.\n\nThe paper is well-written and easy to follow. The experimental results with different priors have demonstrated the effectiveness of the newly formulated model. However, it is not convinced that what is the advantages of the proposed model with WAE. Can the authors provide more insight and comparison with its counterpart, WAE?\n\nIn term of time-complexity, computing Sinkhorn distance in Alg 1 introduces computation overhead, especially with small \\epsilon. In compared with WAE, what is the computation overhead of the proposed model? Can the authors provide some theoretical analysis of time-complexity and experimental results?\n\nIn Table 2, there are only FID values for WAE with MMD cost. Can the authors show the numbers with WAE-GAN on these datasets?\n\nConclusion: The theoretical and experimental contributions are significant to publish at the venue.", "title": "Well-written and significant novelty paper ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}