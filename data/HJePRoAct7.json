{"paper": {"title": "Graph U-Net", "authors": ["Hongyang Gao", "Shuiwang Ji"], "authorids": ["hongyang.gao@tamu.edu", "sji@tamu.edu"], "summary": "We propose the graph U-Net based on our novel graph pooling and unpooling layer for network embedding.", "abstract": "We consider the problem of representation learning for graph data. Convolutional neural networks can naturally operate on images, but have significant challenges in dealing with graph data. Given images are special cases of graphs with nodes lie on 2D lattices, graph embedding tasks have a natural correspondence with image pixel-wise prediction tasks such as segmentation. While encoder-decoder architectures like U-Net have been successfully applied on many image pixel-wise prediction tasks, similar methods are lacking for graph data. This is due to the fact that pooling and up-sampling operations are not natural on graph data. To address these challenges, we propose novel graph pooling (gPool) and unpooling (gUnpool) operations in this work. The gPool layer adaptively selects some nodes to form a smaller graph based on their scalar projection values on a trainable projection vector. We further propose the gUnpool layer as the inverse operation of the gPool layer. The gUnpool layer restores the graph into its original structure using the position information of nodes selected in the corresponding gPool layer. Based on our proposed gPool and gUnpool layers, we develop an encoder-decoder model on graph, known as the graph U-Net. Our experimental results on node classification tasks demonstrate that our methods achieve consistently better performance than previous models.", "keywords": ["graph", "pooling", "unpooling", "U-Net"]}, "meta": {"decision": "Reject", "comment": "The authors supplied an updated paper resolving the most important reviewer concerns after the deadline for revisions. In part, this was due to reviewers requesting new experiments that take substantial time to complete.\n\nAfter discussion with the reviewers, I believe that if the revised manuscript had arrived earlier, then it should be accepted. Without the new results I would recommend rejecting since I believe the original submission lacked important experiments to justify the approach (inductive setting experiments are very useful).\n\nThe community has an interest in uniform application of the rules surrounding the revision process. It is not fair to other authors to consider revisions past the deadline and we do not want to encourage late revisions. Better to submit a finished piece of work initially and not assume it will be possible to use up a lot of reviewer time and fix during the review process.\n\nWe also don't want to encourage shoddy, rushed experimental work. However, the way we typically handle requests from reviewers that require a lot of work to complete is by rejecting papers and encouraging them to be resubmitted sometime in the future, typically to another similar conference.\n\nThus I am recommending rejecting this paper on policy grounds, not on the merits of the latest draft. I believe that we should base the decision on the state of the paper at the same deadline that applies to all other authors.\n\nHowever, I am asking the program chairs to review this case since ultimately they will be the final arbiters of policy questions like this."}, "review": {"Hyx1gmL61V": {"type": "rebuttal", "replyto": "S1elcFUKyN", "comment": "Sure, we totally understand. Since the update window has been closed at Nov 26, we created an anonymous link for an updated PDF of our paper with modification labeled by red color. We will update this version once it is enabled in the system. We appreciate if the reviewer can evaluate our work by considering this updated version of our paper.\n\nhttps://documentcloud.adobe.com/link/track?uri=urn%3Aaaid%3Ascds%3AUS%3A04840d73-14ec-4978-a4a9-c2c78e8a5dea\n", "title": "Updated PDF link"}, "S1lr33nVy4": {"type": "rebuttal", "replyto": "Sye-iMcEkN", "comment": "Thank you for your comments.\n\nSince we spent most of the rebuttal time on adding experiments of graph classification tasks, we have not got enough time to update the PDF of our paper (the paper modification deadline was Nov 26). We are working on a new version of the paper with all the updates that we claimed in comments, such as new experimental results and improved literature review. We will update our paper once PDF update is enabled in the system. We appreciate if the reviewer can evaluate our work by considering the additional information in the rebuttal part of this submission along with the original PDF file.", "title": "Rebuttal phase 3"}, "S1lM9Y5TAQ": {"type": "rebuttal", "replyto": "SyxFQ_z5AX", "comment": "Here is just a friendly clarification that we address reviewer's comments (for part 1 and part 2) together in the post below. Thank you.", "title": "Rebuttal phase 2 clarification"}, "S1eOKzDfA7": {"type": "rebuttal", "replyto": "BJxG0Xg9h7", "comment": "Thank you for your comments. \n\n\"-Some choices in the method seem rather arbitrary, such as the tanh non-linearity in \\tilde y. Could the authors elaborate on that? How important is the gating?\"\n\nThe choices are not arbitrary. In our experiments, the tanh outperforms other commonly employed gated operations. As we stated in Section 3.1 of our paper, the gated operations are very important for training the projection vector p: \u201cNotably, the gate operation makes the projection vector p trainable by backpropagation\u2026\u201d. \n\n\"-It would be interesting to analyze which nodes where selected by the pooling operators. Are those nodes close together or spread out in the previous graph?\"\n\nThank you for your suggestion. Due to time limitation, we will add some graph visualization in the final version of our paper.\n\n\"-...Have the authors tried other upsampling strategies analogous to the ones typically used for images (e.g. upsampling with nearest neighbors)?\"\n\nWe cannot try other up-sampling strategies since there are still no up-sampling methods for graphs currently. Most up-sampling operations on images need locality information such as deconvolution layer. But it is hard on graph data since numbers of neighing nodes are not fixed and they are not ordered.\n\n\"-When skipping information from the downsampling path to the upsampling path, is there a concatenation or a summation? How do both operations compare? (note that concatenation introduces many more parameters) How about only skipping only the indices (no summation nor concatenation)? This kind of analysis, as it has been done in the computer vision literature, would be interesting.\"\n\nWe use summation for skip connection. We compared these two skip connection strategies and found summation worked better. Summation operation can reduce the number of parameters compared to concatenation, which helps to avoid overfitting. Due to page limit, and these are not our main contribution, we did not put such information on the paper. When published, we will release our code, which includes all such implementation details.\n\n\"-What is the influence of the first embedding layer to reduce the dimensionality of the features?\"\n\nSince we worked on three citation network datasets, the initial feature vectors are bag-of-words representations, which is really sparse and has high-dimension. We use an embedding layer to reduce them into low-dimensional representations to avoid over-fitting. In practice, this layer is very important since it helps reduce parameters, thereby resulting in better generalization and performance.\n\n\"-How do the models in Table 2 compare in terms of number of parameters?\"\n\nWe didn\u2019t provide such kind of comparisons in our paper since baseline models did not report the number of parameters in their works. But in our models, the numbers of parameters are only around 20 thousand depending on the datasets.\n\n\"-What's the influence of imposing larger weights on self loop in the graph?\"\n\nImposing larger weights promotes the performance slightly. Since it is a very popular way in traditional machine learning methods on graph data, we did not provide analysis on this.\n\n\"-What about experiments in inductive settings?\"\n\nWe add experiments on graph classification tasks which are under inductive learning settings. The results are summarized in Table above. Our approaches achieve new state-of-the-art performance on graph classification tasks under inductive settings.\n\n\"-Please add references for the following claim \"U-Net models with depth 3 or 4 are commonly used...\"\"\n\nSure, we will add some. Thanks.\n\n\"-Please double check your references, e.g. in the introduction, citations used for CNNs do not always correspond to CNN architectures. \"\n\nSure, we will. Thanks.\n\n\"-The literature review could be significantly improved, missing relevant papers to discuss include:...\"\n\nThank you for providing these helpful references. I will add some related to the paper. But some references listed here are not very related to my work. For example, - Bruna et al. Spectral networks and locally connected networks on graphs, 2014 works on spectral networks. Also, Isola et al. Image-to-image translation with conditional adversarial networks, 2016 is a GAN work on images and is not related to my work. I will add some related works in the final version of our paper. Thanks.", "title": "Rebuttal to AnonReviewer1 (Part 2)"}, "SkgC0A0cRm": {"type": "rebuttal", "replyto": "SyxFQ_z5AX", "comment": "\u201cRegarding upsampling, it is worth mentioning that the choice of distributing the features of the tracked indices while keeping the other rows in the feature matrix to zero is a design choice. One could think of copying the features of one of the nearest neighbor instead (i.e. the features of the closest node -- in terms of number of hops -- selected during pooling), following the nearest neighbor upsampling commonly used in image-based architectures.\u201d\n\nThanks for your suggestion. That is an excellent point. Indeed, we already used the method of nearest neighbor in our model. In decoder part, we have a GCN layer after each gUnpool layer. In each gUnpool layer, the feature vectors of previously unselected nodes are initially set to zero. The following GCN layer will first do a feature summation from neighboring nodes, which are the nearest neighbors of one hop in the graph. This means the previously unselected nodes are actually initialized by their nearest neighbors, which actually followed your suggestions. Thanks again.\n\n\u201cRegarding the number of parameters, it is hard to assess whether improvement comes from the contributed architecture or an increased number of parameters without having access to a comparison in terms of number of parameters.\u201d\n\nThanks for your comments. Although no model reports their numbers of parameters, we can provide a comparison between our work and DiffPool based on our calculations. DiffPool achieved state-of-the-art performances on graph classification tasks as shown in the table above.\n\nDiffPool claimed that they employed a 12-layer network with 8 GCN layers, 2 DIFFPOOL layers, and 2 MLP layers. On small datasets like ENZYMES, the number of hidden dimensions used in GCN and MLP is set to 64 in GCN layers. For large datasets like D&D, this number is 128. \nIn each DIFFPOOL layer, they use a GraphSage model to generate an assignment matrix, which contains at least one GCN-like layer. By taking the case for small datasets, the number of parameters can be estimated as: 8* 64*64 (for GCN layers) + 2*64*64 (for DIFFPOOL layers) + 2*64*64 (for MLP) = 49152.\n\nFor our g-U-Net with the depth of 4, there are 9 GCN layers, each of which employs the hidden dimension of 48. There are 4 gPool layers, each of which contains 48 parameters. So, the total number of parameters is: 9*48*48 (for GCN layers) + 4*48 (for gPool layers) = 20928. From this calculation, the number of parameters of our model in graph classification tasks is far less than that of DiffPool. Notably, we employ the same model architecture for both small and large datasets. Actually, one advantage of our gPool layer is that it doesn\u2019t involve many trainable parameters, which reduces the risk of overfitting.\n\nAlso, we perform ablation study in Section 4.4 by comparing performances between g-U-Net and g-U-Net without using gPool and gUnpool layers. Without these layers, the performance decreases significantly (-2.3%) with only about 100 fewer parameters. This also demonstrates that the performance improvements are due to our proposed methods not the number of parameters.\n\n\u201cRegarding the literature review, all graph-related papers seem relevant to this work to me. The image-based papers are good literature to showcase existing upsampling methods that could have been taken into account (see previous comment related to nearest neighbor upsampling).\u201d\n\nThanks a lot for pointing out these. We will add these citations to corresponding parts in the final version of this work.", "title": "Rebuttal phase 2"}, "BkenHKxQAQ": {"type": "rebuttal", "replyto": "r1enduxQRQ", "comment": "Thank you very much.", "title": "Thank you"}, "rkglzX79nX": {"type": "review", "replyto": "HJePRoAct7", "review": "* I have revised my score upwards due to the authors response to my concerns --- particularly the addition of new results on graph classification. The original review remains here, and I respond to the author's response below. \n\nThe authors propose a new technique to add \u201cpooling\u201d and \u201cunpooling\u201d layers to a graph neural network (GNN). To deal with the lack of spatial locality in graphs, the downsampling operation relies on a learned scalar projection vector (which gives the \u201cscores\u201d for selecting different nodes). During upsampling, the model simple relies on storing the un-sampled adjacency matrix. Thorough experimental results on Cora, Citeseer, and Pubmed highlight the utility of the approach, with ablation studies isolating the importance of the pool/unpool operations.\n\nOverall, this is an interesting paper with the possibility of having a moderate impact within the area of GNNs/GCNs, and the method is clearly described. While there are a number of minor modifications made to the standard GCN model, which could potentially confound the results, the authors do provide a sensible ablation study to isolate the importance of their pool/unpool operations. The overall results on the three node classification datasets are also quite strong. \n\nThe primary shortcoming of this paper is that it only evaluates the model on three citation network datasets (Cora, Citseer, and Pubmed). While these datasets are now standard in the GCN/GNN community, they are very small, have few labeled examples, and it would greatly strengthen the paper to use a different dataset or two, e.g., the Reddit or PPI datasets from Hamilton et al. 2017 or the BlogCatolog dataset used in Grover et al. 2016 could be used for node classification. Or the authors could apply the proposed technique to graph classification or link prediction. In this reviewers opinion, it is very hard to judge the general utility of a method when results are only provided on these three very-specific datasets, where the performance differences between methods are now very marginal. \n\nIn a related point, while this work cites other approaches that apply pooling operations in graph neural networks (e.g., Ying et al. 2018, Simonovsky and Komodakis 2018), no comparisons are made against these approaches. One would suppose that these comparisons are not made because this paper only tests the graph U-net for node classification, but it would greatly strengthen this paper to add comparisons to these other pooling operations, e.g., for graph classification. Moreover, it is possible to define analogous unpooling operations for Ying et al. 2018 and Simonovsky and Komodakis 2018, similar to the unpooling operation used in this work (e.g., for Ying et al.\u2019s DiffPool you can just \u201cunpool\u201d to the previous graph and assign each node a feature corresponding to the weighted sum of the features of the assigned clusters). Of course, it would require significant work (e.g., experiments on graph classification or some modifications of existing approaches) to actually test whether the pool approach proposed here is actually better than those in Ying et al. 2018 and Simonovsky and Komodakis 2018, but such comparisons are necessary to demonstrate whether the pooling operation proposed here is an improvement over existing works, or whether the primary novelty is the combined application of pooling and unpooling in a node classification setting. \n\nAs another minor point, whereas unpooling operations can be used to define a generative model in the image setting, this is not the case here, as the unpooling operation relies on knowledge about the input graph (i.e., the model always unpools to the same connectivity structure). This is not necessarily a bad thing, but it could improve the paper to clarify this issue. ", "title": "An interesting paper that could benefit from more empirical comparisons", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJxkbxPGAQ": {"type": "rebuttal", "replyto": "rkglzX79nX", "comment": "Thanks for your comments. \n\n\"-The primary shortcoming of this paper is that it only evaluates the model on three citation network datasets (Cora, Citseer, and Pubmed). While these datasets are now standard in the GCN/GNN community, they are very small, have few labeled examples, and it would greatly strengthen the paper to use a different dataset or two, e.g., the Reddit or PPI datasets from Hamilton et al. 2017 or the BlogCatolog dataset used in Grover et al. 2016 could be used for node classification. Or the authors could apply the proposed technique to graph classification or link prediction. In this reviewers opinion, it is very hard to judge the general utility of a method when results are only provided on these three very-specific datasets, where the performance differences between methods are now very marginal. \"\n\nTo evaluate our proposed gPool method for graph classification tasks on other datasets, we add more experiments on several graph classification datasets, including D&D, Proteins, and Collab datasets, which are standard datasets employed in such experiment settings. D&D, Proteins, and Collab datasets contain 1178, 1113, and 5000 graphs and 284.32, 39.06, and 74.49 average numbers of nodes on each graph, respectively. The results are summarized in Table below. We can observe from the results that our proposed gPool method outperforms DiffPool [1] by margins of 1.79% and 1.43% on D&D and Proteins. Notably, the result obtained by DiffPool-DET on Collab is significantly higher than all other methods and the other two DiffPool models. On all three datasets, our model outperforms baseline models including DiffPool. In addition, DiffPool claimed that their training utilized auxiliary task of link prediction to stabilize model performance. But in our experiments, we only use graph labels for training without any auxiliary tasks to stabilize training.\n\n++++++++++++++++++++++++++++++++++++++++++++++\n___________________|____D&D__|__ PROTEINS__|__COLLAB__\n________PSCN_____|___ 76.27__|____75.00_____|____72.60____\n______DGCNN____|___ 79.37__|____76.26_____|____73.76____\n__ DiffPool -DET_|___ 75.47__|____75.62_____|____82.13____\n_DiffPool-NOLP_ |___ 79.98__|____76.22_____|____75.58____\n______DiffPool____|___ 80.64__|____76.25_____|____75.48____\n______g-U-Net____|___ 82.43__|____77.68_____|____77.56____\n\nDue to time constraint, we will add these results in the final version of our paper.\n\n\"-In a related point, while this work cites other approaches that apply pooling operations in graph neural networks (e.g., Ying et al. 2018, Simonovsky and Komodakis 2018), no comparisons are made against these approaches... \"\n\nThe work proposed in DiffPool can be used for constructing un-pool layers. Our proposed approaches are similar to regular pooling and un-pooling layers used on images and texts. We selected some important nodes to form a new graph using original edges. For DiffPool, the graph will become softly connected with every two nodes connected by a probability rate. In addition, our proposed pooling layer only involves a very small number of extra parameters, which are trainable projection vectors. While in DiffPool, a network is employed for each diff-pool layer to learn the assignment matrix. That may increase the risk of overfitting and make the training unstable. Actually, to stabilize training, DiffPool employs an auxiliary task of link prediction during training in graph classification tasks.\n\n\"-As another minor point, whereas unpooling operations can be used to define a generative model in the image setting, this is not the case here, as the unpooling operation relies on knowledge about the input graph (i.e., the model always unpools to the same connectivity structure). This is not necessarily a bad thing, but it could improve the paper to clarify this issue. \"\n\nSure, our proposed gUnpool layer corresponds to the regular un-pool layer used on images. The regular un-pool layer also needs the pooled position information in corresponding regular pooling layer to restore the original image structure. We will add this clarification in the final version of our paper.\n\n[1] Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L Hamilton, and Jure\nLeskovec. Hierarchical graph representation learning with differentiable pooling. The Thirty-second Annual Conference on Neural Information Processing Systems (NIPS), 2018", "title": "Rebuttal to AnonReviewer3"}, "ryeNpxwMRX": {"type": "rebuttal", "replyto": "HJxUcVxrhm", "comment": "Thank you for your comments. \n\n\"- It is not clear why the evaluation seem to only be done for the transductive learning settings. I understand that some of the previous work might have done that, but this application scenario is quite limited.\"\n\nThank you for your suggestions. We applied our proposed approaches to graph classification problems under inductive learning settings. We add more experiments on several graph classification datasets including D&D, Proteins, and Collab datasets, which are standard datasets employed in graph classification tasks under inductive learning settings. The results are summarized in Table below. It can be seen that our proposed pooling method outperforms DiffPool [1] by margins of 1.79% and 1.43% on two datasets.  Notably, the result reported by DiffPool-DET on Collab is significantly higher than other baselines and the other two DiffPool models. This demonstrates that our proposed methods can be applied to node classification and graph classification tasks under both transductive and inductive learning settings. \n\n++++++++++++++++++++++++++++++++++++++++++++++\n___________________|____D&D__|__ PROTEINS__|__COLLAB__\n________PSCN_____|___ 76.27__|____75.00_____ |____72.60____\n_______DGCNN___|___ 79.37__|____76.26_____ |____73.76____\n___DiffPool-DET_|___ 75.47__|____75.62_____ |____82.13____\n_DiffPool-NOLP_|___ 79.98__|____76.22_____ |____75.58____\n______DiffPool___|___ 80.64__|____76.25_____ |____75.48____\n______g-U-Net___|___ 82.43__|____77.68_____ |____77.56____\n\nDue to time limitation, we will add these results in the final version of our paper.\n\n\"- One concern about the g-pool operation is that it is not local: unlike e.g. max pool on 2D which produces local maxima, here the selection is done globally, which could lead to situations where the entire parts of the graph are completely ignored. \"\n\nYes, our gPool operation is performed on global scope instead of local. This is due to the fact that defining locality is very hard especially for pooling operations. Unlike grid-like data such as images and texts, there is no obvious rule to group some nodes into a local patch for pooling operations. In DiffPool [1], they also define a global pooling operation. The difference is that their method learns an assignment matrix to softly assign each node to nodes in the new graph. While our approach is more similar to the regular global k-max pooling operation.\n\nAlthough some parts of the graph are abandoned in a gPool layer, our proposed gUnpool layer and graph U-Net architecture will restore the graph structure in the decoder part for feature representation learning. Therefore, we don\u2019t need to worry about the loss of node information by employing gUnpool layer and graph U-Net architecture.\n\n\"- Another concern, which has been partially addressed in section 3.4 is that the connectivity is not really taken into account when downsampling the adjacency matrix. The solution which introduces previously non-existing edges and thus kind of modifies the original graph is not very satisfying. \"\n\nWhen performing pooling operation, the non-existing edges are introduced based on the fact that we employ GCN layers before our proposed gPool layers. Each GCN layer will aggregate one-hop neighboring nodes information for each node in the graph. This means that two nodes that are two hops away will have information communication. Based on this fact, we employ graph power of 2 to augment the graph connectivity to avoid isolated nodes in the graph.\n\nAlso, this method will partially solve the connectivity loss problem when down-sampling the graph. But it\u2019s hard to maintain original graph connectivity when we need to sample some important nodes out especially on sparsely-connected graphs.\n\n[1] Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L Hamilton, and Jure\nLeskovec. Hierarchical graph representation learning with differentiable pooling. The Thirty-second Annual Conference on Neural Information Processing Systems (NIPS), 2018", "title": "Rebuttal to AnonReviewer2"}, "rkgzTzwzCX": {"type": "rebuttal", "replyto": "BJxG0Xg9h7", "comment": "Thank you for your comments. \n\n\"-Given that the main contribution of the paper is the introduction of a pooling operation for graph structured data, it might be a good idea to evaluate the operation in a task that does require some kind of downsampling...\"\n\nTo evaluate our gPool layer on down-sampling-required tasks, we add more experiments on graph classification tasks under inductive learning settings on three standard datasets; those are D&D, Proteins, and Collab datasets with 1178, 1113, and 5000 graphs, respectively. The results including comparison with a state-of-the-art graph pooling method [1] are summarized in Table below. Our proposed methods outperform baseline models including DiffPool on two out of three datasets and achieve new state-of-the-art performances.  Notably, the result reported by DiffPool-DET on Collab is significantly higher than other baselines and the other two DiffPool models. \n\nNote that the primary contribution of our work is to develop both graph pooling and unpooling layers that together enable the development of graph U-nets. Evaluation of our methods on graph classification tasks only involve pooling layers, which is not a comprehensive evaluation of our proposed methods.\n\n++++++++++++++++++++++++++++++++++++++++++++++\n__________________|____D&D__|__ PROTEINS__|__COLLAB__\n________PSCN____|___ 76.27__|____75.00_____|____72.60____\n_______DGCNN__ |___ 79.37__|____76.26_____|____73.76____\n___DiffPool-DET_|___ 75.47__|____75.62_____|____82.13____\n_DiffPool-NOLP_|___ 79.98__|____76.22_____|____75.58____\n______DiffPool___|___ 80.64__|____76.25_____|____75.48____\n______g-U-Net___|___ 82.43__|____77.68_____|____77.56____\n\nDue to time constraint, we will add these results in the final version of our paper.\n\n\"-Authors claim that one of the motivations to perform their pooling operation is to increase the receptive field. It would be worth comparing pooling/upsamping to dilated convolutions...\"\n\nThanks for your suggestion. Dilated convolution is not defined on graphs since it is not clear how to define locality on graph data. Actually, regular convolution operations are not available on graph data. GCN only performs a linear transformation after a simple summation from neighboring nodes. To our knowledge, trainable filters on spatial dimension are not available on graph data. It\u2019s hard to compare with dilated convolutions on graph data.\n\n[1] Rex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L Hamilton, and Jure\nLeskovec. Hierarchical graph representation learning with differentiable pooling. The Thirty-second Annual Conference on Neural Information Processing Systems (NIPS), 2018", "title": "Rebuttal to AnonReviewer1 (Part 1)"}, "BJxG0Xg9h7": {"type": "review", "replyto": "HJePRoAct7", "review": "This paper proposes pooling and upsampling operations for graph structured data, to be interleaved with graph convolutions, following the spirit of fully convolutional networks for image pixel-wise prediction. Experiments are performed on node classification benchmarks, showing an improvement w.r.t. architectures that do not perform any downsampling/upsampling operations.\n\nGiven that the main contribution of the paper is the introduction of a pooling operation for graph structured data, it might be a good idea to evaluate the operation in a task that does require some kind of downsampling, such as graph classification / regression. Moreover, authors should compare to other graph pooling methods.\n\nAuthors claim that one of the motivations to perform their pooling operation is to increase the receptive field. It would be worth comparing pooling/upsamping to dilated convolutions to see if they have the same effect on the performance when dealing with graphs. \n\nSome choices in the method seem rather arbitrary, such as the tanh non-linearity in \\tilde y. Could the authors elaborate on that? How important is the gating?\n\nIt would be interesting to analyze which nodes where selected by the pooling operators. Are those nodes close together or spread out in the previous graph?\n\nThe proposed unpooling operation seems to be the same as unpooling performed to upsample images, that is using skip connections to track indices, by recovering the position where the max value comes from and setting the rest to 0. Have the authors tried other upsampling strategies analogous to the ones typically used for images (e.g. upsampling with nearest neighbors)?\n\nWhen skipping information from the downsampling path to the upsampling path, is there a concatenation or a summation? How do both operations compare? (note that concatenation introduces many more parameters) How about only skipping only the indices (no summation nor concatenation)? This kind of analysis, as it has been done in the computer vision literature, would be interesting.\n\nWhat is the influence of the first embedding layer to reduce the dimensionality of the features?\n\nHow do the models in Table 2 compare in terms of number of parameters?\n\u2028What's the influence of imposing larger weights on self loop in the graph?\n\nWhat about experiments in inductive settings?\n\nPlease add references for the following claim \"U-Net models with depth 3 or 4 are commonly used...\"\n\nPlease double check your references, e.g. in the introduction, citations used for CNNs do not always correspond to CNN architectures.\n\nThe literature review could be significantly improved, missing relevant papers to discuss include:\n- Gori et al. A new model for learning in graph domains, 2005.\n- Scarselli et al. The graph neural network model, 2009.\n- Bruna et al. Spectral networks and locally connected networks on graphs, 2014.\n- Henaff et al. Deep convolutional networks on graph-structured data, 2015.\n- Niepert et al. Learning convolutional neural networks for graphs, 2016.\n- Atwood and Towsley. Diffusion-convolutional neural networks, 2016.\n- Bronstein et al. Geometric deep learning: going beyond Euclidean data, 2016.\n- Monti et al. Geometric deep learning on graphs and manifolds using mixture model cnns, 2017.\n- Fey et al. SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels, 2017.\n- Gama et al. Convolutional Neural Networks Architectures for Signals Supported on Graphs, 2018.\nAs well as other pixel-wise architecture for image-based tasks such as:\n- Long et al. Fully Convolutional Networks for Semantic Segmentation, 2015.\n- Jegou et al. The one hundred layers tiramisu: fully convolutional densenets for semantic segmentation, 2016.\n- Isola et al. Image-to-image translation with conditional adversarial networks, 2016.\n- Zhao et al. Stacked What-Where auto-encoders, 2015.", "title": "interesting problem of pooling/upsampling graphs, experimental validation and literature review could be significantly improved", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJxUcVxrhm": {"type": "review", "replyto": "HJePRoAct7", "review": "Summary:\nThis paper introduces an encoder-decoder neural net architecture for arbitrary graphs. The core contribution is pooling and un-pooling operations for respectively graph down and up sampling.\n\nPros:\n+ U-Net like architectures indeed are very successful in vision applications, and having a model that was similar properties on graphs would be very useful.\n+ The paper is clearly written. \n+ I really liked the idea behind the pooling operation: it is simple, seems easy to implement efficiently, and generally makes sense (although see concerns below). \n+ The choice of the baselines is reasonable, and experimental results seem convincing. Ablation studies are also there.\n\nCons:\n- It is not clear why the evaluation seem to only be done for the transductive learning settings. I understand that some of the previous work might have done that, but this application scenario is quite limited.\n- One concern about the g-pool operation is that it is not local: unlike e.g. max pool on 2D which produces local maxima, here the selection is done globally, which could lead to situations where the entire parts of the graph are completely ignored. \n- Another concern, which has \fbeen partially addressed in section 3.4 is that the connectivity is not really taken into account when downsampling the adjacency matrix. The solution which introduces previously non-existing edges and thus kind of modifies the original graph is not very satisfying. \n", "title": "Good paper, clearly written and has some interesting ideas", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJxCB1HUs7": {"type": "rebuttal", "replyto": "HJxRweG0FQ", "comment": "Thank you for the interest in our work and references. We were aware of these work. But our work mainly focus on graph pooling and un-pooling operations, which are orthogonal to methods in these papers. We would like to add these references as needed in our final version.", "title": "Baselines"}, "rJesYj2atX": {"type": "rebuttal", "replyto": "SJgK6c3ptX", "comment": "Sure, I totally agree with you. We can do more experiments about this part. Very happy to have this great discussion with you.", "title": "possible further studies on gating"}, "Bygr-fsTYX": {"type": "rebuttal", "replyto": "rklHQEEpYX", "comment": "Hi, thank you for your appreciation and question. Actually, we have tried sigmoid, tanh and softmax. tanh performs the best. We have thought about reasons. There are some possible explanations. The values in y vector are the scalar projection values. The negative values do not mean they are negligible just because they are in the opposite direction of vector p. So if we do sigmoid, their corresponding node vectors will become trivial. And also tanh is zero centered, which facilitates the training of projection vectors. So we choose to use tanh for gate operation. Also the use of tanh can regularize node vectors such that they are in the same direction of projection vector p. We are not sure if this can help with the feature encoding. We may try to investigate this in the future. Hope these explanations can help you. Happy to have future discussion with you if any question. Thank you.", "title": "Why use tanh gating"}}}