{"paper": {"title": "MixKD: Towards Efficient Distillation of Large-scale Language Models", "authors": ["Kevin J Liang", "Weituo Hao", "Dinghan Shen", "Yufan Zhou", "Weizhu Chen", "Changyou Chen", "Lawrence Carin"], "authorids": ["~Kevin_J_Liang1", "~Weituo_Hao1", "~Dinghan_Shen1", "~Yufan_Zhou1", "~Weizhu_Chen1", "~Changyou_Chen1", "~Lawrence_Carin2"], "summary": "We propose MixKD, a distillation framework leveraging mixup for large-scale language models.", "abstract": "Large-scale language models have recently demonstrated impressive empirical performance. Nevertheless, the improved results are attained at the price of bigger models, more power consumption, and slower inference, which hinder their applicability to low-resource (both memory and computation) platforms. Knowledge distillation (KD) has been demonstrated as an effective framework for compressing such big models. However, large-scale neural network systems are prone to memorize training instances, and thus tend to make inconsistent predictions when the data distribution is altered slightly. Moreover, the student model has few opportunities to request useful information from the teacher model when there is limited task-specific data available. To address these issues, we propose MixKD, a data-agnostic distillation framework that leverages mixup, a simple yet efficient data augmentation approach, to endow the resulting model with stronger generalization ability. Concretely, in addition to the original training examples, the student model is encouraged to mimic the teacher's behavior on the linear interpolation of example pairs as well. We prove from a theoretical perspective that under reasonable conditions MixKD gives rise to a smaller gap between the generalization error and the empirical error. To verify its effectiveness, we conduct experiments on the GLUE benchmark, where MixKD consistently leads to significant gains over the standard KD training, and outperforms several competitive baselines. Experiments under a limited-data setting and ablation studies further demonstrate the advantages of the proposed approach.", "keywords": ["Natural Language Processing", "Representation Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This work explores the distillation of language models using MixUp for data augmentation. Distillation with MixUp seems to be novel in the narrow context of distilling language models, although it has been used before in different contexts as the reviewers point out. The results of the experimental validation are encouraging, and the application is valuable and of wide interest to the ICLR audience. I therefore recommend accepting this paper for a poster presentation."}, "review": {"LFc_qGo-k4Y": {"type": "review", "replyto": "UFGEelJkLu5", "review": "Nice paper. The main idea in this paper is to use a specific kind of data augmentation, Mixup (Manifold Mixup), in order to improve the effectiveness of the KD process and obtain better performing student models, especially in cases where not enough data is available on the target dataset and task.\n\nWhile the idea is interesting in itself, I think what is proposed in this paper, is very relevant to this paper (noisy student): https://arxiv.org/abs/1911.04252\n\nSo, methodologically it's not super novel, but it should also be taken into account that many of the developments in this area have happened very recently. Moreover, the paper still has an added value since it is applying these techniques for a different input modality and in a slightly different setup. \n\nSummary of the experiments:\n-12 layer BERT is fine tuned as the teacher (different fine-tuned teacher for each task).\n- Two different architectures are used for student models (6 layer BERT and 3 layer BERT).\n- Student models are initialised by copying the lower layers of the teachers.\n- The teacher is used to provide pseudo labels for the student model on the target dataset, while the target dataset is augmented with the manifold mixup approach on the embedding layer (Proposed Approach).\n- The proposed approach is compared with plain KD and no KD (just fine-tuning on target).\n \nMy Questions:\n- During KD, do you feed original examples from target to the teacher then apply mixup, or do you first apply mixup then feed the generated examples to the teacher and get the pseudo labels from the teacher?\n-Most importantly, It is not clear to me from the experiments whether there is still an advantage of doing this, if the teacher is trained well enough e.g., with the same type of augmentation (intuitively, if the teacher is trained well enough and KD process is well tuned, all the information that the model can gain by doing additional data augmentations should already be transferrable through the soft targets from the teacher.)\n\nSome positive point about the paper:\n- Ablation experiments are conducted to separate the gain from simple KD with KD+Mixup.\n\nSome of my concerns:\n- The improvements in the accuracies reported seem marginal and I think there are indication of the significancy of the results in the paper (e.g., mean and variance over several trials?).\n\nSome minor points:\n- Figure 2: Mixup and Non Mixup examples (circles and triangles) are not clearly differentiable (just hard to see).\n-I think the type of data augmentation applied is a special case of Manifold Mixup, so it would be nice to cite this paper as well:  https://arxiv.org/abs/1806.05236\n\n", "title": "Intuitive approach to improve the benefits gained from KD", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "EivEpQXvMYd": {"type": "rebuttal", "replyto": "i_NwoErzuu-", "comment": "From an intuition perspective: taking your suggestion of a \u201cvery small number of examples\u201d to the extreme, if there were just 2 samples, the student would get very limited information on the features learned by the teacher using just KD. Given neural networks\u2019 well documented ability to memorize samples, it\u2019s possible that the student may overfit. With data augmentation, however, the student also gets the teacher\u2019s predictions for the synthetic samples to learn from as well. For example, mixup can produce samples at the decision boundary (e.g. if $\\lambda=0.5$), which can yield insights into the teacher\u2019s decision process. Moreover, the intuition here is similar to how data augmentation helps NLP tasks in general, where the augmented examples can endow the learned model with stronger generalizability (especially when the number of training examples is relatively small).\n\nFrom an empirical perspective: note that we do compare against vanilla KD in Table 2, as well as PKD, which is a method that uses knowledge distillation across multiple layers. In general, we see better results on GLUE with our approach. We haven\u2019t had the chance yet to do a thorough comparison of the variance analysis between MixKD and KD, but it\u2019s something we\u2019ll look into.\n", "title": "Clarifications"}, "f9woLsKTtV": {"type": "rebuttal", "replyto": "x9WQNdDhu2e", "comment": "We would like to thank reviewer 4 for the thoughtful comments. Below we address the concerns mentioned in the review:\n\n**Importance of data augmentation for text data**\nWe push back on the notion that \"text data augmentation for knowledge distillation is not necessarily an important problem for real-world applications.\" Please note that our method focuses on the supervised problem of task-specific fine-tuning, not the pre-training stage. As such, we focus on labeled data, not unlabeled data. While we do agree that there is indeed a vast wealth of text data available in certain contexts, this is certainly not the case in general. There are many applications where data may be quite limited: a few examples include doctor notes for rare diseases, small business analytics (e.g emails, job descriptions), anomaly detection, low-resource languages, and few-shot learning. As such, we believe there is still great value in text data augmentation for real-world applications. \n\n**TinyBERT**\nWhile TinyBERT is certainly a relevant work, we caution against direct comparisons between the two. TinyBERT performs distillation in two-stages: pre-training and task-specific finetuning. MixKD only does the latter, which makes it significantly easier to use in practice, as large language model pre-training can be computationally expensive (16 TPUs for 4 days for BERT base). TinyBERT also uses a multilayer distillation strategy, while we again use a simpler approach of just distilling the logits. That said, these ideas from TinyBERT can be readily combined with MixKD to push our numbers higher. However, the goal of our paper was not necessarily to optimize the absolute state-of-the-art, but rather to prove that our simple approach leads to significant improvements in compressed model performance.\n\n**Metrics**\nIn the TinyBERT paper, we did not find the numbers for the setting where only the data augmentation module is applied to the baseline. Therefore, to make a fair comparison between MixKD and the data augmentation module proposed in TinyBERT, we use the released code of TinyBERT to generate the augmented samples and reproduce the corresponding experiments. \n\n**Efficiency of the data augmentation module**\nWe agree that the computation time for producing augmented samples does not matter that much, since we can do it offline. However, in TinyBERT, the ratio of augmented samples to original data points is set to 20, on average. This will greatly enlarge the training set size, thus taking much longer time to train relative to MixKD (where only one augmented sentence is generated from each original input). While longer training time due to data augmentation doesn't impact inference time performance, it does have significant consequences with respect to development time and iterability.\n", "title": "Reviewer 4 Response"}, "8omgKwtWJHP": {"type": "rebuttal", "replyto": "_DX0z-z_dsw", "comment": "Thank you for the positive review! To answer your concerns:\n\nC1. In practice, cross-entropy loss is often calculated based on the softmax output, with the ground truth as a one-hot vector. This means the loss will not exactly reach 0, thus the logarithm term in cross-entropy will always be bounded, leading to a bounded loss. In addition, we may also add a small value, 1e-7 for example, to all the outputs of softmax for stability. This also makes the cross-entropy loss bounded, without influencing the prediction too much.  \n\nC2. Thanks for the catch; we'll add it to our draft. We used $\\alpha_{SM}=1$ and $\\alpha_{TMKD}=1$. As shown in our hyperparameter sensitivity analysis in Figure 3, we didn't find the exact value of these hyperparameters to matter too much. \n\nC3. The reason we did our thorough ablation analysis on the dev set is because the labels are not publicly released for the test set. Calculating GLUE test performance requires submitting to the test server, which imposes a limit to how many submissions can be done.\n\nC4. Thanks for the suggestion; we will change it accordingly.\n\nC5. It's a t-SNE plot of the transformer features at the [CLS] token position. Sorry that the figure was too small. We'll expand it in our next draft.\n", "title": "Reviewer 3 Response"}, "x5tX7E9QkPG": {"type": "rebuttal", "replyto": "LFc_qGo-k4Y", "comment": "We're glad you enjoyed our paper! To answer your questions:\n\nQ1: The latter: we perform mixup of the samples at the word embedding level and then feed them to the teacher for pseudolabels. Please refer to equations 2 and 4 in our paper.\n\nQ2: The motivation stems from the notion that if there is a limited dataset, then the teacher can only generate a limited set of pseudolabels for the student to learn from. With only a limited set, even if the teacher learns a richer structure, the student may not have enough opportunities to learn it from the teacher. Performing data augmentations allows the teacher to provide more opportunities for the student to distill the teacher networks' knowledge. \n\nMean and variance: In our initial experiments, we found that especially on relatively larger datasets such as MNLI, QNLI and SST-2, the variance of the same model's performance is pretty small. Thus, the improvements from MixKD are consistent and demonstrate the advantages of our proposed approach. That said, it is a good idea to add variance analysis to our experiments.\n\nWe'll expand Figure 2 to make it clearer, and we're happy to add a reference to Manifold Mixup.\n", "title": "Reviewer 2 Response"}, "zKlq2EpMZY0": {"type": "rebuttal", "replyto": "YzU82qRBZcP", "comment": "We appreciate the positive review! Thank you for the miscellaneous suggestions; we'll incorporate them into our next draft.\n\nComputational costs: Good point, thanks for the suggestion. While the inference speed will depend on a few factors (hardware, dataset, batch size, etc.), for SST-2 with a batch size of 16:\t\n- $\\text{BERT}_{12}$ Teacher Inference Speed: ~115 samples/second\n- $\\text{BERT}_{6}$ Student Inference Speed: ~252 samples/second\n- $\\text{BERT}_{3}$ Student Inference Speed: ~397 samples/second \n- $\\text{BERT}_{12}$ Teacher Parameters: 109,483,778\n- $\\text{BERT}_{6}$ Student Parameters: 66,956,546\n- $\\text{BERT}_{3}$ Student Parameters: 45,692,930\n", "title": "Reviewer 1 Response"}, "X19tRXyCoz": {"type": "rebuttal", "replyto": "KNptB6n1M3", "comment": "Thanks for the comment. We would like to point out that for the theorem to hold, it is enough to ensure [x_i, x'_i] and [x_j, x'_j] to be independent, where *'* means augmented data. From our construction, this is easily satisfied. Regarding the number of augmented data, yes, it needs to be sufficiently large. That is why we need the condition of b in Theorem 3. In the practical case of large data, this will always be satisfied. We will make this point more clear.", "title": "not a concern"}, "sIE9ytToVXN": {"type": "rebuttal", "replyto": "pNCx8QMdJso", "comment": "Please note our way of constructing the augmented data described previously has already guaranteed that the augmented data are independent with each other.", "title": "independency"}, "8vuiKX0HF5c": {"type": "rebuttal", "replyto": "bw3PtpRfnDM", "comment": "Dear Mingyang,\n\nThanks for the comment. We agree augmented data samples should be independent from each other, which is used in proving Theorem 4 in Baxter 2000 (see Theorem 18 in Baxter 2000). \n\nIn our method, we can actually make this requirement satisfied by manipulating the construction of augmented data. For example, suppose we need m augmented data for training in one iteration, the augmented data can be constructed with 2m true data samples as: x'_i = \\lambda x_{2i-1} + (1 - \\lambda) x_{2i}. This ensures the independence between x'_1,...,x'_m. Across different iterations, the independence is also easy to verify: for any two x'_i, x'_j, p(x'_i)p(x'_j)=p(x'_i, x'_j) always holds. Thus the independence between augmented data samples can be easily satisfied during the whole training process. \n\nWe acknowledge our result does not apply to all data augmentations but will not cause practical issues. We will make this clear in our revision.", "title": "about independency"}, "I-CoWWYfmd6": {"type": "rebuttal", "replyto": "x6IvMLv-Lx_", "comment": "Hi Vikas, thanks for your interest in our work! We'd like to point out that our work doesn't explicitly consider semi-supervised learning, as it wasn't our main focus, though we certainly could. That said, we'll happily discuss the related work that you suggested in a future version.", "title": "Thanks for the comment!"}, "cSaTtU1-YeS": {"type": "rebuttal", "replyto": "SPj2uNmVGqb", "comment": "Dear Mingyang,\n\nThanks for your comments. Yes, we agree x_i and x'_i might not be iid. That is why we considered 3 cases. Case 3 (corresponding to Theorem 3) explicitly consider the non-iid case. Theorem 1 and 2 consider the case of existing a distribution such that x_i and x'_i are iid sample from it. The reason we consider this is that even though x_i and x'_i might be dependent, there may still exists a distribution whose iid samples coincide with x_i and x'_i.", "title": "thanks for your comment"}, "x9WQNdDhu2e": {"type": "review", "replyto": "UFGEelJkLu5", "review": "This paper applies mixup (Zhang et al., 2018) to augment training data to improve knowledge distillation in NLP tasks . Mixup was originally proposed to augment data for continuous data. To apply mixup to textual data, this paper applies mixup to the word/token embeddings instead of the tokens themselves. Some theoretical analysis has been done, and the experimental results show improved metrics over baseline methods such as DistillBERT.\n\n\n*Strong points*\n- This paper is well-written and very easy to follow.\n- Theoretical results have been provided and the experiment has been carefully done.\n\n*Weak points*\n- The proposed method is a direct application of mixup (Zhang et al., 2018). The overall idea is quite straightforward.\n- The empirical results of the proposed method are not impressive (worse than the metrics reported in TinyBERT (Jiao et al., 2019)?).\n- It is true that for benchmarks like GLUE, the datasets are quite small so that data augmentation is important for knowledge distillation. This is however not true for most real-world applications where we often have almost unlimited unlabelled data (e.g., from logs, extract from webs etc). In other words, text data augmentation for knowledge distillation is not necessarily an important problem for real-world applications.\n\n*Questions & Other comments*\n- In Table 3, the metrics for TinyBERT is different from what has been reported in TinyBERT (Jiao et al., 2019). Can you add more explanation here?\n- \u201cNotably, TinyBERT\u2019s data augmentation module is much less efficient than mixup\u2019s simple operation\u201d It is not an issue to have some computation overhead in data augmentation, as it is a one-off operation.\n", "title": "An interesting paper but not good enough", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "_DX0z-z_dsw": {"type": "review", "replyto": "UFGEelJkLu5", "review": "The paper proposes combining the MixUp data augmentation method with teacher-student distillation to improve the fine-tuned performance of BERT on benchmark NLP tasks (GLUE). The problem is important, well-motivated and of interest to a broad base of NLP researchers and practitioners. The paper is clear, and generally well-written, although the idea itself is not surprisingly novel (somewhat of a low-hanging fruit), from the experimental results, the method improves upon baselines, and so the real-world impact could be high, especially given its simple implementation.\n\nPros:\n* Well-written, solid experiments / ablations on a canonical benchmark (bonus points for reporting GLUE test set performance), some theoretical contributions.\n* Reports performance in labeled-data-limited setting (by subsampling GLUE dataset)\n\nCons:\n* In the theory, a bounded loss function is assumed, but in practice the unbounded cross-entropy is used (right?)\n* Optimal hyper-parameters $\\alpha_\\text{SM}$ and $\\alpha_\\text{TMKD}$ for \"MixKD\" on the GLUE dev test are not reported in the main text\n* \"MixKD\" includes a back-translation (BT) loss term as well as a student loss on mixup samples (SM). While the impact of each term is studied on the GLUE dev set, I'm interested in seeing its performance on GLUE test.\n* In the theorems, $\\delta$ should be re-introduced.\n* The embedding visualization section needs improvement. What is being visualized here? 2-dimensional logits / probabilities from the model or input embeddings projected down to 2D? Furthermore, it is hard to distinguish circles from triangles without zooming in 3-5x.", "title": "Review", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "YzU82qRBZcP": {"type": "review", "replyto": "UFGEelJkLu5", "review": "The paper describes a distillation framework that leans heavily on Mixup, an effective data augmentation technique that is typically applied to images, to achieve impressive results on a range of GLUE benchmarks (). By using SM+TMKD+BT, the student model (BERT_6) is able to capture 99.88% of the performance of the teacher model (BERT_12).\n\nThe authors provide good motivation for the problem of distilling large-scale language models and conduct extensive experiments across multiple GLUE benchmarks, including ablation and hyperparameter sensitivity studies, and the results are quite convincing. Figure 3 nicely demonstrates that the framework is not overly sensitive to any of its hyperparameters.\n\nQuestions:\n- Given that the paper motivates distillation as a means of reducing the computational cost of state-of-the-art language models (less power, less memory, lower latency), could you also describe the number of parameters that the student models use and their latency at inference time (vs. that of the teacher models)?\n\nWhile the paper is generally well-written, there are a number of small issues / inconsistencies:\n- Nit: Mixup is inconsistently capitalized / hyphenated.\n- Since Mixup is central to this distillation framework, it would be valuable to briefly describe the method in the body of the paper, before describing how it was adapted to the domain of language.\n- \u201cthat a a multi-task BERT model\u2026\u201d\n- \u201caround an training example\u201d\n- Nit: \u201cvicinal\u201d is an extremely rare word, so it might be worth using a more accessible word such as \u201cneighboring\u201d\n- Nit: \u201ctabular data were also shown, to demonstrate generality\u201d (remove comma)\n- Nit: \u201cthe extra word embeddings are mixuped with zero paddings\u201d (maybe \u201cmixed up\u201d is better? :))\n- In Table 1, for ease of reading, it would be good to describe in the caption the meaning of the first and second \u2018/\u2019 delimited number in each cell.\n- For Figure 2, please describe the dimensionality reduction method used to visualize the latent space of training data and augmented examples. The pattern looks compelling, but I don\u2019t see you describe anywhere how the data was projected. ", "title": "Extensive experiments demonstrating effectiveness of distillation framework for large-scale NLP models", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}