{"paper": {"title": "Structured Sequence Modeling with Graph Convolutional Recurrent Networks", "authors": ["Youngjoo Seo", "Micha\u00ebl Defferrard", "Pierre Vandergheynst", "Xavier Bresson"], "authorids": ["youngjoo.seo@epfl.ch", "michael.defferrard@epfl.ch", "pierre.vandergheynst@epfl.ch", "xavier.bresson@gmail.com"], "summary": "This paper introduces a neural network to model graph-structured sequences", "abstract": "This paper introduces Graph Convolutional Recurrent Network (GCRN), a deep learning model able to predict structured sequences of data. Precisely, GCRN is a generalization of classical recurrent neural networks (RNN) to data structured by any arbitrary graph. Such structured sequences can be series of frames in videos, spatio-temporal measurements on a network of sensors, or random walks on a vocabulary graph for natural language modeling.The proposed model combines convolutional neural networks (CNN) on graphs to identify spatial structures and RNN to find dynamic patterns. We study two possible architectures of GCRN, and apply the models to two practical problems: predicting moving MNIST data, and modeling natural language with the Penn Treebank dataset. Experiments show that exploiting simultaneously graph spatial and dynamic information about data can improve both precision and learning speed.", "keywords": ["Structured prediction"]}, "meta": {"decision": "Reject", "comment": "While graph structures are an interesting problem, as the reviewers observed, the paper extends previous work incrementally and the results are not very moving.\n \n pros\n - interesting problem space that has not been thoroughly explored\n cons\n - experimental evaluation was not convincing enough with the results.\n - the method itself is a small incremental improvement over prior papers."}, "review": {"S11WiDkPg": {"type": "rebuttal", "replyto": "B14KAcgVl", "comment": "Thanks for your review.\n\nThe Model1 that we used in PTB experiment uses graph convolution that gives filtered output of input. And the filtered output has activation on possible candidate nodes around given an input word node. Therefore, it is right that the input information propagation along with the edges.\n\nFor the moving MNIST experiment, the isotropic property of our graph convolution may not capture the fine structure of the image, but it is easier to capture the direction of its moving. We will update the paper by showing the learned filter and feature maps.", "title": "Answer for comment 3."}, "HyTakd0Ix": {"type": "rebuttal", "replyto": "Hkwsxob4g", "comment": "Thanks for your review.\n \nWe recently noticed that the paper Video Pixel Networks(Kalchbrenner et al., 2016).  And surprised by the results. However, in our experiment, the main purpose is to show that the GRCN works similar or better than the traditional convolutional method on 2D grid structure data. Therefore, we thought that it is not necessary to compare with state-of-the-art results on moving MNIST.\n\nFor the experiment on PTB, we compared with small settings that correspond to non-regularized LSTM experiment setting in Zaremba et al., 2014 to check the simplest setting first. We will conduct a further experiment on PTB with various configurations.\n", "title": "We will update experiment session."}, "Sy_yhwR8l": {"type": "rebuttal", "replyto": "r10x8yS4e", "comment": "Thanks for your review.\n\nThe main reason why we keep the same experimental settings of Zaremba et al.,on PTB experiment is for fair comparison and checking the difference. But I agree that we need to find our own best experimental setting and show it together. We will update the PTB results with best learning schedule and settings.", "title": "Best experimental settings for PTB"}, "HkR8ohyNg": {"type": "rebuttal", "replyto": "ryH1FYCGx", "comment": "Thanks for your questions!\n\n1. Model 2 is exactly what you describe. You are right that our spectral filters are isotropic, and thus do not distinguish between edges. All edges are indeed treated equally (they could be treated differently by having one graph per type of edge for example). This naturally makes sense on graphs with only one type of edge, like the vocabulary graph used in Section 5.2. On images however, it is not evident whether this is good or not. That's exactly what we wanted to test in Section 5.1: could our isotropic filters perform better than 2D filters on video prediction ? And the answer is yes. Moreover, our spectral filters require much less parameters than 2D filters for the same diameter, e.g. 5x5=25 corresponds to K=3, 9x9=81 corresponds to K=5.\n\n2. No particular reason. For a fair comparison with Shi et al. (2015), we used the same architecture as them, which is Model 2.\n\n3. On moving MNIST, we followed the same setup as Shi et al. (2015), only replacing the 2D convolution by a graph convolution. So the actual prediction is given by a 1x1 convolution on h_t. On PTB, the chosen word is the one with highest probability.", "title": "Clarifications"}, "SkH2Sh1Nl": {"type": "rebuttal", "replyto": "ByjvpGkQg", "comment": "Thanks for your questions!\n\n1. We used two representations for a word: (i) a word vector of dimension 200 and (ii) a one-hot code of dimension 10,000. (i) is only used for the model introduced by Zaremba et al. For our model, which takes as input a vocabulary graph of 10,000 nodes, we use (ii), which is then a signal on the vocabulary graph. For comparison, we also tested Zaremba et al. with (ii). We modified Table 2 to state the input representation for clarification.\n\n2. We added the number of parameters of each model in Table 2.\n\n3. Please see our answers to reviewer 3.", "title": "Answers about penn treebank experiment (section 5.2)"}, "rJ7GEnkVg": {"type": "rebuttal", "replyto": "SJjybsOGx", "comment": "\n1. Thank you for the question. We checked and realized that we did a mistake during the preparation of the datasets, we basically missed the end-of-sentence. The mistake propagated to the perplexity results of all models. We fixed it, re-ran the experiments, and rewrote significantly Section 5.2. We encourage the reviewer to look at it.\n\n2. We used two representations for the word x_t: (i) a word vector of dimension 200 and (ii) a one-hot code of dimension 10,000. (i) is only used for the model introduced by Zaremba et al. For our model, which takes as input a vocabulary graph of 10,000 nodes, we use (ii), which is then a signal on the vocabulary graph. For comparison, we also tested Zaremba et al. with (ii). We clarified this in the revision.\n\n3. Your're right: the graph is fixed, and the embedding is only used to construct the graph. The network's input is that fixed graph and a time-series of one-hot vectors.\n\n4. We did not tune the hyper-parameters but followed those of the small configuration of:\nhttps://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/models/rnn/ptb/ptb_word_lm.py\nwhich are based on the paper of Zaremba et al. (2014).\n\n5. The high perplexity was due to the mistake describe in 1. We are now able to reproduce Zaremba et al.\n\nThanks to your comment, we fixed a mistake and the presentation of that experiment should be clearer.", "title": "Answers about penn treebank experiment (section 5.2)"}, "ByjvpGkQg": {"type": "review", "replyto": "S19eAF9ee", "review": "I have few questions about the penn treebank experiment,\n\n1) I am not sure to understand what is the input used? Are you using the same input for the RNN/LSTM/GRU baselines and the Model1/Model2 variations?\n\n2) What is the number of parameters of each model?\n\n3) I also share the same interrogations than reviewers 3 for the model hyperparameters and the high perplexities of the baseline.This paper investigates the  modeling of graph sequences . Authors propose Graph Convolutional Recurrent Networks (GRCN)  that extends convLSTM  (Shi et al. 2015) for data having an unregular graph structure at each timestep. They replace the 2D convolution with a graph convolutional operator from (Defferrad et al., 2016).\nAuthors propose two variations of the GRCN model. In Model 1,  the graph convolution is only applied on the input data. In Model 2, the graph convolution  is applied on both  input data and the previous hidden states. They evaluate their approaches on two different tasks, video generation using the movingMNIST dataset and world-level language modelling using Penntreebank.\n\nOn movingMNIST authors show that their GRCN 2 improves upon convLSTM. However, they evaluate only with one-layer convLSTM, while Shi et al. report better results with 3 layers (also not as good as  GRCN) . It would be nice to evaluate GCRCN in that setting as well.\nWhile the authors show an improvement of GRCN relatively to convLSTM, GRCN on this task seems relatively weak compared to recent works such as the Video Pixel Networks (Kalchbrenner et al., 2016). It contradicts the claim that \"Model 2 has shown good performance in the case of video prediction\" in the conclusion.\n\nFor the Penntreebank experiments, author compares  their model 1 with FC-LSTM, with or without dropout. However, the results in (Zaremba et al., 2014) still seems different than the one reported here. In (Zaremba et al., 2014), they  reports a test perplexity of 78.4 for the large regularized LSTM in their table 1 which outperforms the score of the GRCN. Also, following works such as variational dropout or zoneout have since improve upon Zaremba results. Is there some differences in the experimental setting?  It would be nice to have results that are directly comparable to previous work.\n\n\nPros:\n- Interesting model, \nCons:\n- Overall, the proposed contribution is relatively incremental compared to (Shi et al. 2015) and (Defferrad et al., 2016). \n- Weak results of GRCN relatively to previous works in the experiments, that do not convince of the GRCN advantages.\n", "title": "Pre-review questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hkwsxob4g": {"type": "review", "replyto": "S19eAF9ee", "review": "I have few questions about the penn treebank experiment,\n\n1) I am not sure to understand what is the input used? Are you using the same input for the RNN/LSTM/GRU baselines and the Model1/Model2 variations?\n\n2) What is the number of parameters of each model?\n\n3) I also share the same interrogations than reviewers 3 for the model hyperparameters and the high perplexities of the baseline.This paper investigates the  modeling of graph sequences . Authors propose Graph Convolutional Recurrent Networks (GRCN)  that extends convLSTM  (Shi et al. 2015) for data having an unregular graph structure at each timestep. They replace the 2D convolution with a graph convolutional operator from (Defferrad et al., 2016).\nAuthors propose two variations of the GRCN model. In Model 1,  the graph convolution is only applied on the input data. In Model 2, the graph convolution  is applied on both  input data and the previous hidden states. They evaluate their approaches on two different tasks, video generation using the movingMNIST dataset and world-level language modelling using Penntreebank.\n\nOn movingMNIST authors show that their GRCN 2 improves upon convLSTM. However, they evaluate only with one-layer convLSTM, while Shi et al. report better results with 3 layers (also not as good as  GRCN) . It would be nice to evaluate GCRCN in that setting as well.\nWhile the authors show an improvement of GRCN relatively to convLSTM, GRCN on this task seems relatively weak compared to recent works such as the Video Pixel Networks (Kalchbrenner et al., 2016). It contradicts the claim that \"Model 2 has shown good performance in the case of video prediction\" in the conclusion.\n\nFor the Penntreebank experiments, author compares  their model 1 with FC-LSTM, with or without dropout. However, the results in (Zaremba et al., 2014) still seems different than the one reported here. In (Zaremba et al., 2014), they  reports a test perplexity of 78.4 for the large regularized LSTM in their table 1 which outperforms the score of the GRCN. Also, following works such as variational dropout or zoneout have since improve upon Zaremba results. Is there some differences in the experimental setting?  It would be nice to have results that are directly comparable to previous work.\n\n\nPros:\n- Interesting model, \nCons:\n- Overall, the proposed contribution is relatively incremental compared to (Shi et al. 2015) and (Defferrad et al., 2016). \n- Weak results of GRCN relatively to previous works in the experiments, that do not convince of the GRCN advantages.\n", "title": "Pre-review questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryH1FYCGx": {"type": "review", "replyto": "S19eAF9ee", "review": "\n1) is it fair to say that the proposed model is the same as that of Shi et al. (2015), except the convolutions are generalized to graph convolutions exactly as those in Defferrard et al. (2016), so that non-grid data is allowed? Except it seems that the model is not strictly more powerful as it treats all edges equally and can't distinguish between them, if my understanding is correct.\n\n2) Is there a reason why Model 1 is not evaluated on Moving MNIST?\n\n3) Can the authors clarify how the actual prediction is made, for example for Moving MNIST? I assume y is bernoulli based on linear projection of h_t?\nThe authors address the problem of modeling temporally-changing signal on a graph, where the signal at one node changes as a function of the inputs and the hidden states of its neighborhood, the size of which is a hyperparameter. The approach follows closely that of Shi et al. 2015, but it is generalized to arbitrary graph structures rather than a fixed grid by using graph convolutions of Defferrard et al. 2016. This is not a strict generalization because the graph formulation treats all edges equally, while the conv kernels in Shi et al. have a built in directionality. The authors show results on a moving MNIST and on the Penn Tree Bank Language Modeling task.\n\nThe paper, model and experiments are decent but I have some concerns:\n\n1. The proposed model is not exceptionally novel from a technical perspective. I usually don't mind if this is the case provided that the authors make up for the deficiency with thorough experimental evaluation, clear write up, and interesting insights into the pros/cons of the approach with respect to previous models. In this case I lean towards this not being the case.\n\n2. The experiment results section is rather terse and light on interpretation. I'm not fully up to date on the latest of Penn Tree Bank language modeling results but I do know that it is a hotly contested and well-known dataset. I am surprised to see a comparison only to Zaremba et al 2014 where I would expect to see multiple other results.\n\n3. The writing is not very clear and the authors don't make sufficiently strong attempt to compare the models or provide insight or comparisons into why the proposed model works better. In particular, unless I'm mistaken the word probabilities are a function of the neighborhood in the graph. What is the width of this graph? For example, suppose I sample a word in one part of the graph, doesn't this information have to propagate to the other parts of the graph along the edges? Also, it's not clear to me how the model can achieve reasonable results on moving MNIST when it cannot distinguish the direction of the moving edges. The authors state this but do not provide satisfying insight into how this can work. How does a pixel know that it should turn on in the next frame? I wish the authors thought about this more and presented it more clearly.\n\nIn summary, the paper has somewhat weak technical contribution, the experiments section is not very thorough, and the insights are sparse.\n", "title": "clarifications", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B14KAcgVl": {"type": "review", "replyto": "S19eAF9ee", "review": "\n1) is it fair to say that the proposed model is the same as that of Shi et al. (2015), except the convolutions are generalized to graph convolutions exactly as those in Defferrard et al. (2016), so that non-grid data is allowed? Except it seems that the model is not strictly more powerful as it treats all edges equally and can't distinguish between them, if my understanding is correct.\n\n2) Is there a reason why Model 1 is not evaluated on Moving MNIST?\n\n3) Can the authors clarify how the actual prediction is made, for example for Moving MNIST? I assume y is bernoulli based on linear projection of h_t?\nThe authors address the problem of modeling temporally-changing signal on a graph, where the signal at one node changes as a function of the inputs and the hidden states of its neighborhood, the size of which is a hyperparameter. The approach follows closely that of Shi et al. 2015, but it is generalized to arbitrary graph structures rather than a fixed grid by using graph convolutions of Defferrard et al. 2016. This is not a strict generalization because the graph formulation treats all edges equally, while the conv kernels in Shi et al. have a built in directionality. The authors show results on a moving MNIST and on the Penn Tree Bank Language Modeling task.\n\nThe paper, model and experiments are decent but I have some concerns:\n\n1. The proposed model is not exceptionally novel from a technical perspective. I usually don't mind if this is the case provided that the authors make up for the deficiency with thorough experimental evaluation, clear write up, and interesting insights into the pros/cons of the approach with respect to previous models. In this case I lean towards this not being the case.\n\n2. The experiment results section is rather terse and light on interpretation. I'm not fully up to date on the latest of Penn Tree Bank language modeling results but I do know that it is a hotly contested and well-known dataset. I am surprised to see a comparison only to Zaremba et al 2014 where I would expect to see multiple other results.\n\n3. The writing is not very clear and the authors don't make sufficiently strong attempt to compare the models or provide insight or comparisons into why the proposed model works better. In particular, unless I'm mistaken the word probabilities are a function of the neighborhood in the graph. What is the width of this graph? For example, suppose I sample a word in one part of the graph, doesn't this information have to propagate to the other parts of the graph along the edges? Also, it's not clear to me how the model can achieve reasonable results on moving MNIST when it cannot distinguish the direction of the moving edges. The authors state this but do not provide satisfying insight into how this can work. How does a pixel know that it should turn on in the next frame? I wish the authors thought about this more and presented it more clearly.\n\nIn summary, the paper has somewhat weak technical contribution, the experiments section is not very thorough, and the insights are sparse.\n", "title": "clarifications", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJjybsOGx": {"type": "review", "replyto": "S19eAF9ee", "review": "I have some questions about section 5.2\n\n1. It seems that you used the same dataset (and split) as in Zaremba et al. (2014). But in Zaremba et al. experiment there are 929k training words, 73k validation words, and 82k test words whereas you reported 887k/70k/78k. Can you explain the difference? \n\n2. I understand that the input at each time step x_t is a n x d matrix where n is the vocabulary size and d is the word vector dimensions. How did you construct x_t for language modelling? \n\n3. Because the graph is constructed based on word2vec, you can't fine tune the word embeddings, right? (otherwise the graph will change after each update)\n\n4. how did you tune the hyper-parameters for those models reported? Did you employ early-stop to halt all training after 8 epochs?\n\n5. Table 2 shows perplexities of those models. The LSTM has a much higher perplexity compared to the LSTM reported in Zaremba et al (263.85 vs 114.5 of non-regularized LSTM). Can you explain why? The paper proposes to combine graph convolution with RNNs to solve problems in which inputs are graphs. The two key ideas are: (i) a graph convolutional layer is used to extract features which are then fed in an RNN, and (ii) matrix multiplications are replaced by graph convolution operations. (i) is applied to language modelling, yielding lower perplexity on Penn Treebank (PTB) compared with LSTM. (ii) outperformed LSTM + CNN on the moving-MNIST.\n\nBoth two models/ideas are actually trivial and in line with the current trend of combining different architectures. For instance, the idea of replacing matrix multiplications by graph convolution is a small extension for Shi et al.\n\nRegarding to the experiment on PTB (section 5.2), I'm skeptical about the way the experiment carried out. The reason is that, instead of using the given development set to tune the models, the authors blindly used an available configuration which is for a different model.\n\nPros: \n- good experimental results\n\nCons:\n- ideas are quite trivial \n- the experiment on PTB was carried out improperly ", "title": "experiments on penn treebank", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r10x8yS4e": {"type": "review", "replyto": "S19eAF9ee", "review": "I have some questions about section 5.2\n\n1. It seems that you used the same dataset (and split) as in Zaremba et al. (2014). But in Zaremba et al. experiment there are 929k training words, 73k validation words, and 82k test words whereas you reported 887k/70k/78k. Can you explain the difference? \n\n2. I understand that the input at each time step x_t is a n x d matrix where n is the vocabulary size and d is the word vector dimensions. How did you construct x_t for language modelling? \n\n3. Because the graph is constructed based on word2vec, you can't fine tune the word embeddings, right? (otherwise the graph will change after each update)\n\n4. how did you tune the hyper-parameters for those models reported? Did you employ early-stop to halt all training after 8 epochs?\n\n5. Table 2 shows perplexities of those models. The LSTM has a much higher perplexity compared to the LSTM reported in Zaremba et al (263.85 vs 114.5 of non-regularized LSTM). Can you explain why? The paper proposes to combine graph convolution with RNNs to solve problems in which inputs are graphs. The two key ideas are: (i) a graph convolutional layer is used to extract features which are then fed in an RNN, and (ii) matrix multiplications are replaced by graph convolution operations. (i) is applied to language modelling, yielding lower perplexity on Penn Treebank (PTB) compared with LSTM. (ii) outperformed LSTM + CNN on the moving-MNIST.\n\nBoth two models/ideas are actually trivial and in line with the current trend of combining different architectures. For instance, the idea of replacing matrix multiplications by graph convolution is a small extension for Shi et al.\n\nRegarding to the experiment on PTB (section 5.2), I'm skeptical about the way the experiment carried out. The reason is that, instead of using the given development set to tune the models, the authors blindly used an available configuration which is for a different model.\n\nPros: \n- good experimental results\n\nCons:\n- ideas are quite trivial \n- the experiment on PTB was carried out improperly ", "title": "experiments on penn treebank", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}