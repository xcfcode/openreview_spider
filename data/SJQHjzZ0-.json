{"paper": {"title": "Quantitatively Evaluating GANs With Divergences Proposed for Training", "authors": ["Daniel Jiwoong Im", "He Ma", "Graham W. Taylor", "Kristin Branson"], "authorids": ["daniel.im@aifounded.com", "hma02@uoguelph.ca", "gwtaylor@uoguelph.ca", "kristinbranson@gmail.com"], "summary": "An empirical evaluation on generative adversarial networks", "abstract": "Generative adversarial networks (GANs) have been extremely effective in approximating complex distributions of high-dimensional, input data samples, and substantial progress has been made in understanding and improving GAN performance in terms of both theory and application. \nHowever, we currently lack quantitative methods for model assessment. Because of this, while many GAN variants being proposed, we have relatively little understanding of their relative abilities. In this paper, we evaluate the performance of various types of GANs using divergence and distance functions typically used only for training. We observe consistency across the various proposed metrics and, interestingly, the test-time metrics do not favour networks that use the same training-time criterion. We also compare the proposed metrics to human perceptual scores.", "keywords": ["Generative adversarial networks"]}, "meta": {"decision": "Accept (Poster)", "comment": "+ clearly written and thorough empirical comparison of several metrics/divergences for evaluating GANs, prominently parametric-critic based divergences.\n - little technical novelty with respect to prior work. As noted by reviewers and an anonymous commentator:  using an Independent critic for evaluation has been proposed and used in practice before.\n +  the contribution of the work thus lies primarily in its well-done and extensive empirical comparisons of multiple metrics and models    "}, "review": {"ryX_FSexG": {"type": "review", "replyto": "SJQHjzZ0-", "review": "Through evaluation of current popular GAN variants. \n  * useful AIS figure\n  * useful example of failure mode of inception scores\n   * interesting to see that using a metric based on a model\u2019s distance does not make the model better at that distance\nthe main criticism that can be given to the paper is that the proposed metrics are based on trained models which do not have an independent clear evaluation metrics (as classifiers do for inception scores). However, the authors do show that the results are consistent when changing the critic architecture. Would be nice to see if this also holds for changes in learning rates. \n * nice to see an evaluation on how models scale with the increase in training data.\n\nUsing an Independent critic for evaluation has been proposed and used in practice before, see \u201cComparison of Maximum Likelihood and GAN-based training of Real NVPs\u201d, Danihelka et all, as well as Variational Approaches for Auto-Encoding Generative Adversarial Networks, Rosca at all.\n\nImprovements to be added to the paper:\n   * How about overfitting? Would be nice to mention whether the proposed metrics are useful at detecting overfitting. From algorithm 1 one can see that the critic is trained on training data, but at evaluation time test data is used. However, if the generator completely memorizes the training set, the critic will not be able to learn anything useful. In that case, the test measure will not provide any information either. A way to go around this is to use validation data to train the critic, not training data. In that case, the critic can learn the difference between training and validation data and at test time the test set can be used. \n  * Using the WGAN with weight clipping is not a good baseline. The improved WGAN method is more robust to hyper parameters and is the one currently  used by the community. The WGAN with weight clipping is quite sensitive to the clipping hyperparameter, but the authors do not report having changed it from the original paper, both for the critic or for the discriminator used during training. \n  *  Is there a guidance for  which metric should be used? \n\nFigure 3 needs to be made a bit larger, it is quite hard to read in the current set up. ", "title": "Through evaluation of current popular GAN variants. ", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "H1uFgwqeM": {"type": "review", "replyto": "SJQHjzZ0-", "review": "This paper proposes using divergence and distance functions typically used for generative model training to evaluate the performance of various types of GANs. Through numerical evaluation, the authors observed that the behavior is consistent across various proposed metrics and the test-time metrics do not favor networks that use the same training-time criterion. \n\nMore specifically, the evaluation metric used in the paper are: 1) Jensen-Shannon divergence, 2) Constrained Pearson chi-squared, 3) Maximum Mean Discrepancy, 4) Wasserstein Distance, and 5) Inception Score. They applied those metrics to compare three different GANs: the standard DCGAN, Wasserstein DCGAN, and LS-DCGAN on MNIST and CIFAR-10 datasets. \n\nSummary:\n\u2014\u2014\nIn summary, it is an interesting topic, but I think that the paper does not have sufficient novelty. Some empirical results are still preliminary. It is hard to judge the effectiveness of the proposed metrics for model selection and is not clear that those metrics are better qualitative descriptors to replace visual assessment. In addition, the writing should be improved. See comments below for details and other points.\n\nComments:\n\u2014\u2014\n1.\tIn Section 3, the evaluation metrics are existing metrics and some of them have already been used in comparing GAN models.  Maximum mean discrepancy has been used before in work by Yujia Li et al. (2016, 2017)\n\n2.\tIn the experiments, the proposed metrics were only tested on small scale datasets; the authors should evaluate on larger datasets such as CIFAR-100, Toronto Faces, LSUN bedrooms or CelebA.\n\n3.\tIn the experiments, the authors noted that \u201cGaussian observable model might not be the ideal assumption for GANs. Moreover, we observe a high log-likelihood at the beginning of training, followed by a drop in likelihood, which then returns to the high value, and we are unable to explain why this happens.\u201d Could the authors give explanation to this phenomenon? The authors should look into this more carefully.\n\n4.\tIn algorithm 1, it seems that the distance is computed via gradient decent. Is it possible to show that the optimization always converges? Is it meaningful to compare the metrics if some of them cannot be properly computed?\n\n5.     With many different metrics for assessing GANs, how should people choose? How do we trust the scores? Recently, Fr\u00e9chet Inception Distance (FID) was proposed to evaluate the samples generated from GANs (Heusel et al. 2017), how are the above scores compared with FID?\n\nMinor Comments:\n\u2014\u2014\n1.\tWriting should be fixed: \u201cIt seems that the common failure case of MMD is when the mean pixel intensities are a better match than texture matches (see Figure 5), and the common failure cases of IS happens to be when the samples are recognizable textures, but the intensity of the samples are either brighter or darker (see Figure 2).\u201d\n", "title": "review", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H1C2pZplz": {"type": "review", "replyto": "SJQHjzZ0-", "review": "the paper proposes an evaluation method for training GANs using four standard distribution distances in literature namely:\n- JSD\n- Pearson-chi-square\n- MMD\n- Wasserstein-1\n\nFor each distance, a critic is initialized with parameters p. The critic is a neural network with the same architecture as the discriminator.\nThe critic then takes samples from the trained generator model, and samples from the groundtruth dataset. It trains itself to maximize the distance measure between these two distributions (trained via gradient descent).\n\nThese critics after convergence will then give a measure of the quality of the generator (lower the better).\n\nThe paper is easy to read, experiments are well thought out.\nFigure 3 is missing (e) and (f) sub-figures.\n\nWhen proposing a distance measure for GANs (which is a high standard, because everyone is looking forward to a robust measure), one has to have enough convincing to do. The paper only does experiments on two small datasets MNIST and CIFAR. If the paper has to convince me that this metric is good and should be used, I need to see experiments on one large-scale datset, such as Imagenet or LSUN. If one can clearly identify the good generators from bad generators using a weighted-sum of these 4 distances on Imagenet or LSUN, this is a metric that is going to stand well.\nIf such experiments are constructed in the rebuttal period, I shall raise my rating.", "title": "new evaluation metrics for GANs", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJBlWX6mM": {"type": "rebuttal", "replyto": "SJQHjzZ0-", "comment": "The revised version of the paper contains several additional experimental contributions motivated by the reviews we received. Though we have details of each of these in our responses to the reviews, we wanted to provide a short summary as follows:\n\n- Comparison with human perceptual scores (Correlation test between different metrics to human perceptual scores)\n- Evaluation on larger image data, LSUN bedrooms\n- Use of the (improved) WGAN with gradient penalty\n- Comparison to Fr\u00e9chet Inception Distance\n- Investigating the metrics as a means of detecting overfitting\n\nThank you for your interest!", "title": "Summary to the paper update."}, "SyqLlX6QM": {"type": "rebuttal", "replyto": "SJm-JvWfz", "comment": "Thank you for directing us to \u201cGANs for Biological Image Synthesis\u201d by Osokin et al. We have included references to this work in the revised paper. \n\nThank you!!\n", "title": "Thank you for directing us \u201cGANs for Biological Image Synthesis\u201d"}, "S1Gex7a7z": {"type": "rebuttal", "replyto": "H1C2pZplz", "comment": "Thank you for your review.\n\nAt the reviewer\u2019s suggestion, we conducted the same experiments on the LSUN bedroom dataset. We used 64 x 64 of 90,000 images to train GAN, LSGAN, WGAN and tested on another 90,000 unseen images. We evaluated using the LS score, IW distance, and MMD. We omitted the Inception score, because LSUN bedroom dataset contains just a single class and there is no pre-trained convolutional network available (inception score needs a pre-trained convolutional network). Samples from each model are also added in the appendix of the paper. Here is a summary of the results:\n\n                LS (higher the better)     IW (lower the better)    MMD (higher the better)\nGAN :     0.14614                             3.79097                         0.00708\nLSGAN:  0.173077                           3.36779                         0.00973\nWGAN :  0.205725                           2.91787                         0.00584\n\nAll three metrics agrees that WGAN has the best score. LSGAN is ranked second according to the LS score and IW distance, in contrast, MMD puts GAN in second place and LSGAN on third place. Nevertheless, in our more recent experiments added to the revised version of the paper, we showed that MMD score often disagrees with human perceptual score.\n\nIn summary, we applied our evaluation methods to larger images and the performance of IW and LS are consistent with what we observed on MNIST and CIFAR10. \n\nWe added these results to the paper.\n\n\nAdditionally, we would like to note that we added a comparison between the evaluation metrics to human perceptual scores. Please see the response (***) to Reviewer 1. \n\nThank you!!\n", "title": " Thank you for your review."}, "Hyexk7TmM": {"type": "rebuttal", "replyto": "H1uFgwqeM", "comment": "Thank you for your review!\n\nRegarding comment 1 : \nThe reviewer noted that MMD has been used in previous works by Yujia Li et al. [1,2]. Indeed, MMD has been proposed as a training objective in many previous works. Nevertheless, the goal of this paper was to consider different evaluation metrics for scoring GANs and test whether one type of GANs is statistically better than the other one under different metrics. Our claim of novelty is not in proposing a new metric but in evaluating GANs under many different metrics. We made an observation that many metrics have been used to train GANs but surprisingly have not been used to evaluate GANs at test time. Hence, we used those metrics with a critic network to evaluate GANs. Li et al. [1,2] have not employed MMD as a GAN evaluation metric at test time. In the paper, we systematically compared and ranked GANs under different metrics. \n\n\nRegarding comment 2 : Please see our response to Reviewer 3. \n\n\nRegarding comment 3 : The mentioned phenomenon for log-likelihood using AIS by [3] is interesting. However, we do not know why it has that behaviour and we believe that this is not within the scope of our paper to find out. It would be best to directly ask the authors of [3].\n\nRegarding comment 4: The reviewer asked about convergence guarantees using gradient descent. Note that gradient descent is employed widely in deep learning and optimizing the critic\u2019s objective (the distance) is exactly same as training a deep feedforward network with gradient descent. The scope of this comment falls under all deep learning methods and it is not specific to our paper. \n\nNote : We include the training curve of critics to show that at least the training curve converges (see Figure 26).\n\nRegarding comment 5: Please see our response to the review (Fr\u00e9chet Inception Distance for evaluating GANs) from Nov. 20th 2017. We have included substantial experimental results in the updated version of the paper.\n\n(***)\nMoreover, we added a comparison between the evaluation metrics and human perceptual scores. We showed which metrics are more statistically correlated with human perceptual scores.  This was done based on the Wilcoxon rank sum test & two-sided Fisher\u2019s test. The fraction of pairs on which each metric agrees with humans (higher the better):\n\n      Metric                      Fraction Agreed                  agreed # samples / # total samples\nInception score              0.862595                                            ( 113 / 131)\nIW Distance                    0.977099                                            (128 / 131)\nLS score                           0.931298                                            (122 / 131)\nMMD                                0.83261                                              (109 / 131)   \n\n\nIt shows that IW distance agreed the most with human perceptual scores, and then followed by LS, Inception score, and MMD.  \n\nAlso, here are the results of a two-sided Fisher\u2019s test (no multiple comparison correlation) that these fractions are the same or not:\n\nIS equals IW      : p-value = 0.000956\nIS equals LS      : p-value = 0.102621\nIS equals MMD  : p-value = 0.606762\nIW equals LS     : p-value = 0.136684\nIW equals MMD : p-value = 0.000075\nLS equals MMD : p-value = 0.020512\n\nOver all, it demonstrates that IW, LS are significantly more aligned with the perception scores than the Inception Score and MMD, p < 0.05. (See the Section 4.2.1 for details).\n\nWe have improved the quality of the writing in the revised paper.\nThank you!!\n\n\n[1] Yujia Li, Kevin Swersky and Richard Zemel.  Generative Moment Matching Networks. International Conference on Machine Learning (ICML), 2015\n[2] Yujia Li, Alexander Schwing, Kuan-Chieh Wang, Richard Zemel. Dualing GANs. https://arxiv.org/abs/1706.06216\n[3] Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov and Roger Grosse. On the Quantitative Analysis of Decoder-Based Generative Models.   ICLR, 2017. \n", "title": "Thank you for your review!"}, "S1mjkm67z": {"type": "rebuttal", "replyto": "ryX_FSexG", "comment": "Thank you for your review and thank you for directing us to \u201cComparison of Maximum Likelihood and GAN-based training of Real NVPs\u201d by Danihelka et al. We have included references to this work in the revised paper. \n\nRegarding the comment on WGAN with weight clipping :\nWe agree with the reviewer. We will include the clipping hyperparameter, which was 0.1. Additionally, we added experiments with the (improved) WGAN with gradient penalty as shown above. The results can be also found in the updated version of the paper.\n\n\n\nRegarding the comment on overfitting : \nWe agree with the reviewer\u2019s comments. As such, we experimented with the scenario proposed by the reviewer. We trained two critics on training data and validation data respectively and evaluated on test data from both critics. \n\nWe trained six GANs (GAN, LSGAN, WGAN_GP, DRAGAN, BEGAN, EBGAN) on MNIST and Fashion MNIST.  We trained these GANs with 50K training examples. At test time, we used 10k training and 10k validation examples for training the critics, and evaluated on 10k test examples. Here, we present the test scores from the critics trained on training and validation data :\n\nFashion-MNIST\n             LS score (trained on training data)  LS score (trained on validation data)             \nLSGAN          0.135 +- 0.0046                           0.136 +- 0.0074\nGAN \t      0.1638 +- 0.010                           0.1635 +- 0.0006\nDRAGAN      0.1638 +- 0.015                           0.1645 +- 0.0151\nBEGAN         0.1133 +- 0.042                           0.0893 +- 0.0095  \nEBGAN         0.0037 +- 0.0009                         0.0048 +- 0.0023\nWGAN_GP   0.000175 +- 0.0000876               0.000448 +- 0.0000862\n\nMNIST \n             LS score (trained on training data)  LS score (trained on validation data)             \nLSGAN          0.323 +- 0.0104                          0.352 +- 0.0143\nGAN \t      0.312 +- 0.010                            0.4408 +- 0.0201\nDRAGAN      0.318 +- 0.012                             0.384 +- 0.0139\nBEGAN         0.081 +- 0.016                             0.140 +- 0.0329  \nEBGAN         3.38e-6 +- 0.1.86e-7                   3.82e-6 +- 2.82e-7\nWGAN_GP   0.196 +- 0.006                             0.307 +- 0.0381\n\nNote that we also have the IW and FID evaluation on these models in the paper. For Fashion-MNIST, we find that test scores with critic trained on training and validation data are very close. Hence, we don\u2019t see any indication of overfitting. On the other hand, there are gaps between the scores for the MNIST dataset and the test scores from critics trained on validation set gives better performance than the ones that are trained on the training set. \n\n\n\nRegarding the comment on guidance towards a metric :\n\nIn terms of guidance on which metric to use, we recommend using the metric that is the closest to the human perceptual score! Thus, we added a comparison between the evaluation metrics to human perceptual scores. Please see the response (***) to Reviewer 1. \n\n\nThank you!!\n", "title": "Thank you for your review "}, "SJ7anZWlM": {"type": "rebuttal", "replyto": "ry8IOs_yM", "comment": "Thank you for directing us to FID paper.\n\nAs the reviewer stated, FID computes the distance between the Gaussian in Wasserstein-2 distance. The sufficient statistics of Gaussian comes from the first and second moment of the neural network features (convolutional neural network's feature maps). \n\nCompare to our proposed evaluation methods, FID has an advantage and disadvantage. The main advantage is the speed. The calculating of FID distance is much faster than our evaluation methods. The disadvantage of FID is that it only considers difference in first two moments of the samples, which can be insufficient unless the feature maps are Gaussian distributed. On the other hand, the four metrics that we consider does not make any assumptions about the distribution of the samples. \n\nWe agree with the reviewer on analyzing the experiments with FID! So, we have run experiments that includes FID:  We included the FID scores to Table 3, which shows the overall performance of DCGAN, LSGAN, WGAN on CIFAR10. We also evaluated on more recently proposed models, such as DRAGAN, BEGAN, EBGAN, WGAN_GP, based on the off-the-shelf package on MNIST and Fashion-MNIST.  The evaluation metric includes, LS, IW and FID. \n\n\nFor CIFAR10, the FID results agree with LS. The FID results are following:  \n         CIFAR10    FID\n          DCGAN : 0.112 +- 0.010; \n          W-DCGAN : 0.095 +- 0.003; \n          LS-DCGAN : 0.088 +- 0.008 \n(the smaller FID the better; see the Table 3 for other metric scores). According to FID, LS-DCGAN is the best among three models. \n\nFor MNIST, the three metrics, LS, IW, and FID, agree with each other on the rank as well. They all find that samples from DRAGAN is the best, then LSGAN, and so on. \n         MNIST       Metric        \n        DCGAN       IW: 0.111 +- 0.0074 LS: 0.4814 +- 0.0083  FID: 1.84 +- 0.15\n        EBGAN       IW: 0.029 +- 0.0026 LS: 0.7277 +- 0.0159  FID: 5.36 +- 0.32\n        WGAN GP  IW: 0.035 +- 0.0059 LS: 0.7314 +- 0.0194  FID: 2.67 +- 0.15\n        LSGAN        IW: 0.115 +- 0.0070 LS: 0.5058 +- 0.0117  FID: 2.20 +- 0.27\n        BEGAN       IW: 0.009 +- 0.0063 LS: -\t   \t\t            FID: 15.9 +- 0.48\n        DRAGAN    IW: 0.116 +- 0.0116 LS: 0.4632 +- 0.0247  FID: 1.09 +- 0.13\n\nFor Fashion-MNIST, LS, IW and FID agree on the rank of worst ones, but there are some subtle difference between LS and IW versus FID.   According to LS and IW, DRAGAN samples are ranked first and LSGAN samples are ranked second, and visa versa for FID. \n        Fashion-MNIST       Metric\n        DCGAN       IW: 0.69 +- 0.0057  LS: 0.0202 +- 0.00242 FID:   3.23 +- 0.34\n        EBGAN       IW: 0.99 +- 0.0001  LS: 2.2e-5 +- 5.3e-5  FID: 104.08 +- 0.56\n        WGAN GP     IW: 0.89 +- 0.0086  LS: 0.0005 +- 0.00037 FID:   2.56 +- 0.25\n        LSGAN       IW: 0.68 +- 0.0086  LS: 0.0208 +- 0.00290 FID:   0.62 +- 0.13\n        BEGAN       IW: 0.90 +- 0.0159  LS: 0.0016 \t  0.00047 FID:   1.51 +- 0.16\n        DRAGAN      IW: 0.66 +- 0.0108  LS: 0.0219 +- 0.00232 FID:   0.97 +- 0.14\n          \n                              \nWe will make sure to add these descriptions and experiment results in the paper for the next revision cycle.\n\nThank you!!", "title": "Thank you for directing us to FID paper."}, "SkCF1x4kG": {"type": "rebuttal", "replyto": "r1GaH_NAW", "comment": "Thank you for directing us to C2ST.\n\nThere is a relationship between the methods proposed and classifier two-sample tests (C2ST). C2ST proposes to train a classifier that can distinguish samples drawn from two distribution P and Q and accept/reject the null hypothesis.\n\nOne of the commonalities that shared between our proposed test methods and C2ST is that both require optimizing a function (training a neural network) at test time. C2ST trains neural network to maximize the classification between data and samples, whereas our method is training neural network to maximize the distance between the data and samples.\n\nIn our paper, we considered four distance metrics that belong to two class of metrics, $\\phi$-divergence and IPMs. Sriperumbudur et al. have shown that the optimal risk function is associated with a binary classifier with P and Q distribution conditioned on class when the discriminant function is restricted to certain F (Theorem 17 from Sriperumbudur et al).\n\nLet the optimal risk function be\n\n   R(L,F) = inf_{f \\in F} \\int L(y, f(x)) dp(x,y)\n\nwhere F is the set of discriminant functions (classifier), y \\in {-1,1}, and L is the loss function.\n\nBy following derivation, we can see that the optimal risk function becomes IPM:\n\nR(L,F) = inf_{f \\in F} \\int L(y, f(x)) du(x,y)\n           = inf_{f \\in F} [  eps \\int L(1, f(x)) dp(x) + (1 - eps)  \\int L(0, f(x)) dq(x)\n           = inf_{f \\in F} f dp(x) + \\inf_{f \\in F} f dq(x) \n           = - IPM\n\nwhere L(1, f(x)) = 1/eps and L(0,f(x)) = -1 / (1-eps). \n\nThe second equality is derived by separating the loss for class 1 and class 0. \nThird equality is from the way how we chose L(1,f(x)) and L(0,f(x)).\nThe last equality follow by the fact that F is symmetric around zero (f \\in F => -f \\in F). Hence, this shows that with appropriately choosing L, MMD, Wasserstein distance can be understood as the optimal L-risk associated with binary classifier with specific set of F functions. For example, Wasserstein distance and MMD distances are equivalent to optimal risk function with 1-Lipschitz classifiers and RKHS classifier with an unit length.\n\nSimilarly, since every binary classifier has a corresponding distance metric from IPM, C2ST binary classifier must have a distance function associated with it with specific set of F. For example, if the binary classifier is KNN, then we are considering IPM metric with the topology induced by KNN, as well if the classifier is neural networks then we are considering IPM metric with the topology induced by the neural network.\n\nThank you for asking relationship between our proposed testing methods and C2ST.\nWe will make sure to add these descriptions in the paper for the next revision cycle!!!\n\n\n[1] Sriperumbudur et al.  On Integral Probability Metrics, $\\phi$-divergence and binary classification.", "title": "Thank you for directing us to C2ST"}}}