{"paper": {"title": "DEEP GRAPH SPECTRAL EVOLUTION NETWORKS FOR GRAPH TOPOLOGICAL TRANSFORMATION", "authors": ["Liang Zhao", "Qingzhe Li", "Negar Etemadyrad", "Xiaojie Guo"], "authorids": ["lzhao9@gmu.edu"], "summary": "", "abstract": "Characterizing the underlying mechanism of graph topological evolution from a source graph to a target graph has attracted fast increasing attention in the deep graph learning domain. However, there lacks expressive and efficient that can handle global and local evolution patterns between source and target graphs. On the other hand, graph topological evolution has been investigated in the graph signal processing domain historically, but it involves intensive labors to manually determine suitable prescribed spectral models and prohibitive difficulty to fit their potential combinations and compositions. To address these challenges, this paper proposes the deep Graph Spectral Evolution Network (GSEN) for modeling the graph topology evolution problem by the composition of newly-developed generalized graph kernels. GSEN can effectively fit a wide range of existing graph kernels and their combinations and compositions with the theoretical guarantee and experimental verification. GSEN has outstanding efficiency in terms of time complexity ($O(n)$) and parameter complexity ($O(1)$), where $n$ is the number of nodes of the graph. Extensive experiments on multiple synthetic and real-world datasets have demonstrated outstanding performance.", "keywords": ["deep graph learning", "graph transformation", "brain network"]}, "meta": {"decision": "Reject", "comment": "The reviewers kept their scores after the author response period, pointing to continued concerns with methodology, needing increased exposition in parts, and not being able to verify theoretical results. As such, my recommendation is to improve the clarity around the methodological and theoretical contributions in a revision."}, "review": {"rkete7wIjr": {"type": "rebuttal", "replyto": "rJxus2Q-jH", "comment": "Dear Reviewer #2, following Item 4 in your comments, we have added more analysis on the performance evaluation of our method and comparison methods on all four real-world datasets. This new evaluation is based on the R2 score, which is a widely-used metric for prediction performance evaluation. Please see them in Table 5 in the Appendix, along with the discussions in Appendix A3.\n\nBriefly speaking, as shown in Table 5, our method GSEN achieves the best performance in 3 out of 4 datasets and is still highly competitive in the remaining one dataset. Our GSEN also achieves the best performance on average with a large margin when compared with other methods.", "title": "Addition of more analysis with additional evaluation metric as suggested by Reviewer #2"}, "rJxus2Q-jH": {"type": "rebuttal", "replyto": "r1g_0Jxl5S", "comment": "We appreciate the comments from the reviewer very much.\n\n1. For the first concern. Actually our paper is neither targeted at nor has claimed to handle all types of graph evolution process. Instead, we only aim at those situations whose eigenvectors do not change or do not change much. And it can be seen in our Lemma 3.1, our method has good expressiveness for such situations, with significant contributions in the following aspects: \n\ni. Such situations, where eigenvectors do not change much during graph evolution, widely exist in the real world. First, there are hundreds of commonly-used graph kernels (e.g., diffusion kernels) who are aiming to explain those phenomena whose eigenvectors keep unchanged during evolution, as exemplified in Table 1. Beyond that, our method is an end-to-end framework that achieves higher expressiveness over all of them. Second, in many (and ever-increasing) domains, people observed that the eigenvectors do not change much. For example, in the \u201cbefriending process\u201d in a social network and in structural to functional connectivity transformation in the neuroscience domain. Moreover, our experimental results in 15 datasets with different types of graph process demonstrates the effectiveness of the proposed method for various types of graph evolution process.\n\nii. Our method enjoys huge efficiency advantages for the situations when eigenvectors do not change. Namely, the complexity is reduced to linear to the graph size, compared with the (at least) quadratic complexity of traditional methods.\n\niii. It is convenient to verify if a specific application is suitable for our method because it is easy to see whether a graph evolution has the eigenvectors unchanged or not. More importantly, our method can help to identify if an evolution process has an unchanged eigenvector or not. Specifically, when our methods achieve high prediction performance, then the process tends to be spectral evolution process with no (or little) change in eigenvectors. This is because as shown in Lemma 3.1, if our model, which enforces low error in Equation (1), also leads to low error in $\\|F(U\\Lambda U^T) \u2013 L\u2019\\|^2$, then U^T L\u2019 U is close to a diagonal matrix.\n\n2. For the second concern, we believe our contribution is significant. This paper is much more than proposing merely a new kernel, but instead propose an end-to-end framework that can fit any existing and (unknown) kernels as well as their compositions and summations. This is radically different because the previous graph kernels are still prescribed models that require human labors to tailor a predefined kernel to targeted data by prior knowledge or heuristics. But our model is the first which can purely rely on the data and demonstrate our dominating expressiveness over traditional kernels by extensive experiments on 15 datasets and by theoretical discussions in Lemma 3.2. Also, we believe that being as the first work for deep spectral graph translation, it opens a new window for the deep graph learning community.\n\n3. For the third concern, we want to clarify that our method is generic to whichever it is $L$ or $A$ (this is why we mentioned \u201cwithout loss of generalizability\u201d in Lemma 3.1, but we will explicitly mention this in the revision. This means we can also use $A$ instead of $L$. The reason why we prefer $L$ a little bit in this paper is because in deep graph learning domain, graph Laplacian $L$ seems more commonly used and its eigen-decomposition has higher popularity.\n\n4. For the last concern of the reviewer, we will have no problem to add more analysis using more metrics, such as RMSE. We are working on that and try our best to give the updates by the end of the rebuttal session. The reason why we did not use link prediction is due to the nature of our real-world applications. Specifically, for our brain network application, researchers in the domain all achieve the prediction of functional connectivity all at once. Similarly, the other application on the authentication networks also focuses on whole graph generation. Following the domains\u2019 inherent setting will not only ensure that our experiment is practically meaningful but also ensure we are able to compare with the state-of-the-art methods in their domain.  Correlation is commonly used for graph evolution papers, such as:\n\nAbdelnour, Farras, et al. \"Functional brain connectivity is predictable from anatomic network's Laplacian eigen-structure.\"\u00a0Neuroimage\u00a0172 (2018): 728-739.\n\nHoney, C. J., et al. \"Predicting human resting-state functional connectivity from structural connectivity.\"\u00a0Proceedings of the National Academy of Sciences (PNAS)\u00a0106.6 (2009): 2035-2040.\n\nXiaojie Guo, Liang Zhao, Cameron Nowzari, Setareh Rafatirad, Houman Homayoun, and Sai Dinakarrao. Deep Multi-attributed Graph Translation with Node-Edge Co-evolution. The 19th International Conference on Data Mining (ICDM 2019), Beijing, China.", "title": "Authors' Response to Reviewer #2"}, "r1gYsF--iB": {"type": "rebuttal", "replyto": "SygNO0FTtS", "comment": "Thanks a lot for the reviewer's comments. We devoted extensive efforts to the experiments and hence deeply believe that our experiments on 15 datasets (4 real-world datasets + 11 synthetic datasets) against 7 comparison methods are sufficient to demonstrate the effectiveness and efficiency of our methods. \n\nMore details are as follows:\n\nFirst, the experiments on four real-world datasets demonstrate that the proposed model outperforms all the comparison methods in accuracy and efficiency significantly. Specifically, Table 3 shows that our method achieved the best performance on 3 out of 4 real-world datasets when being compared with 7 other state-of-the-art methods, and achieved the second-best in the remaining one dataset. Our method also obtained the best overall performance. Moreover, our method is 40 to 1000 times faster than the most competitive comparison methods as shown in Table 4. As also mentioned by the reviewer \"the biggest advantage of our method is scalability in terms of time and complexity\", we believe our above real-world experiments are sufficient to support it.\n\nSecond, the synthetic dataset also showed that our method achieved the highest performance in all the datasets. As explained in the caption of Table 2 and in Section 5.2.1, the results marked as \"GS\" are the performance achieved by the \"gold standard\" (i.e., the prediction model is the data generator itself). So the purpose of the experiment is to validate how close our method's performance is to the gold standard's. And it can be seen that our method achieved the best performance (i.e., closest to gold standard's) in 8 out of 11 synthetic datasets. And our overall performance is 50% higher than the best performer (i.e., C-DGT) among the comparison methods. This strongly demonstrates that our end-to-end methods are effective in various datasets even with different types of transformations.", "title": "Response to the comments on our experiments "}, "SygNO0FTtS": {"type": "review", "replyto": "rkxgHerKvH", "review": "This paper proposes a spectral graph neural network based on a graph kernel to predict graph evolution. The overall idea is interesting and the biggest advantage is scalability of the framework to large graphs in terms of time and parameter complexity. \nThe major drawback of the paper is the lack of experimentation with real datasets. Based on the results from four datasets they used, the efficacy of their proposed method is unclear. The synthetic datasets are hard to admit in this case.\nNote: I could not verify the theory in detail yet. ", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 2}, "r1g_0Jxl5S": {"type": "review", "replyto": "rkxgHerKvH", "review": "The paper proposed a method to model the graph topological evolution from the spectral domain by developing a new generalized graph kernel. The new graph kernels cover many existing graph kernels as well as their combination and composition as special cases. The idea of spectral graph translation and its integration with deep learning is interesting, especially considering that most previous spectral graph neural networks only transform the graph signal instead of graph structures. However, I do have some concerns of papers.\n\n1.\tMy major concern is the soundness of keeping eigenvectors unchanged in the evolution. Although the authors claim that in previous studies eigenvectors are found stable in evolution, it is very counter-intuitive, and I am not sure it is the case for all types of graphs. Let us look at the proof of Lemma 3.1, obviously $L^\\prime$ does not necessarily have the same eigenvectors, and $U^TL^\\prime U$ is not a diagonal matrix, so this loss is actually very large in many cases. That is to say, the evolution model does not have enough expressive power to recover the $L^\\prime$.\n2.\tSpectral graph translation looks interesting, but the main idea comes from Kunegis et al. (2010). Despite of a new designed graph kernel and adding nonlinear activations, the contributions seem not so significant.\n3.\tIn Kunegis et al. (2010), they consider the evolution of adjacency matrix $A$, but in this paper the authors use the Laplacian matrix $L$. If there any reason to make this choice? Also, I think some of the conclusions (e.g. stable eigenvectors) in Kunegis et al. (2010) may not work since $L$ is used instesd of $A$.\n4.    The correlation metric is acceptable, but it will be much better if the authors can do more analysis. For example, why not add the link prediction task as in Kunegis et al. (2010)? BTW, is correlation analysis used before in previous graph evolution papers? (if so, please add a reference)  \n", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 3}}}