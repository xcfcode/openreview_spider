{"paper": {"title": "Stochastic Canonical Correlation Analysis: A Riemannian Approach", "authors": ["Zihang Meng", "Rudrasis Chakraborty", "Vikas Singh"], "authorids": ["~Zihang_Meng1", "~Rudrasis_Chakraborty1", "~Vikas_Singh1"], "summary": "We present an efficient stochastic algorithm (RSG+) for canonical correlation analysis (CCA) derived via a differential geometric perspective of the underlying optimization task.", "abstract": " We present an efficient stochastic algorithm (RSG+) for canonical correlation analysis (CCA) derived via a differential geometric perspective of the underlying optimization task. We show that exploiting the Riemannian structure of the problem reveals natural strategies for modified forms of manifold stochastic gradient descent schemes that have been variously used in the literature for numerical optimization on manifolds. Our developments complement existing methods for this problem which either require $O(d^3)$ time complexity per iteration with $O(\\frac{1}{\\sqrt{t}})$ convergence rate (where $d$ is the dimensionality) or only extract the top $1$ component with $O(\\frac{1}{t})$ convergence rate. In contrast, our algorithm achieves $O(d^2k)$ runtime complexity per iteration for extracting top $k$ canonical components with $O(\\frac{1}{t})$ convergence rate. We present our theoretical analysis as well as experiments describing the empirical behavior of our algorithm, including a potential application of this idea for training fair models where the label of protected attribute is missing or otherwise unavailable.", "keywords": ["CCA", "streaming", "differential geometry", "DeepCCA", "fairness"]}, "meta": {"decision": "Reject", "comment": "This paper gives a new algorithm for the CCA problem. The main idea of the new algorithm is to reformulate the matrices in the CCA problem as a product of three matrices: one orthonormal matrix, one rotation and one upper-diagonal matrix. The algorithm then performs remannian gradient descent to these components. The per-iteration complexity of the algorithm is O(d^2k) while the (local) convergence rate is O(1/t). Overall the reformulation is interesting and the algorithm seems effective in practice. On the other hand the convergence rate proof relies on local strong convexity and it's not clear why the algorithm converges globally (or even what is the radius of convergence locally)."}, "review": {"0_9Ym4E4Hx6": {"type": "rebuttal", "replyto": "1mDMWe9VUve", "comment": "1. by `\"asymptotically converge\": does it mean when the number of samples goes to infinity?\n\nAns: Here \u201casymptotic\u201d is with respect to the number of steps of the Riemannian gradient  descent procedure (Bonnabel, 2013 at https://arxiv.org/pdf/1111.5280.pdf), see (4) and Theorem 1. This analysis style has also been used in (http://proceedings.mlr.press/v97/kasai19a/kasai19a.pdf; ICML 19, see Theorem. 4.4 and Corollary 4.5)\n(https://www.di.ens.fr/~fbach/colt_2018_tripurareni_flammarion_bach_jordan.pdf; JMLR 18, see Theorem 1) and others. \nIn our case, as the number of steps goes to infinity, $\\widetilde{U}$ converges to the principal directions, and hence $C(\\widetilde{X}_k)$ converges to $C(X_k)$. Note that as the number of samples is fixed, the number of steps corresponds to the number of iterations over the dataset. In our setup, we used one pass over the dataset as the setting is stochastic. \n\n\n2. By \"ensuring\" the diagonal of S to be nonzero, what exactly does it mean?. A constraint such as $S_{ii} \\neq 0$ may not be nontrivial. How is this enforced without affecting the convergence proof? By optimizing over the log, you also need some non-negativity constraint on $S_ii$, I suppose---otherwise log is not defined. In addition, your algorithm descriptions in the main text and in the supplemental material seem not to reflect this point. The reviewer feels that this explanation seems not to be very convincing.\n\nAns: Observe that, without loss of generality we can assume diagonal of $S$, i.e., $S_{ii} > 0$ as if for any $j$, $S_{jj} <0$, we can flip the sign of the $j^{th}$ column of $Q$, where, $Q$ is an orthogonal matrix. Moreover, sign flip does not affect the converge analysis. \nHence, $S \\in \\mathbf{R}^{k\\times k}$ can be modeled as the manifold $\\mathbf{R}^{k(k-1)/2)}\\times \\mathbf{R}_+^k$ ($\\mathbf{R}_+$ is the space of positive reals). Now, parameterizing $\\mathbf{R}_+$ can be accomplished using Riemannian log map (which is the natural logarithm in this case) (see Cheng et al., AISTATS 2011).  Observe that the log space is unrestricted, as $x > 0 \\implies log(x) \\in \\mathbf{R}$, thus the optimization on the log space is unconstrained.\nFurther, Proposition 3 is used while optimizing for $S$, hence the convergence analysis remains valid. \n", "title": "Clarification of two points"}, "r9GK317rkDD": {"type": "rebuttal", "replyto": "9cxwZ5Ouedz", "comment": "1) Clarity\nAns: The reviewer\u2019s suggestions have helped us slow down the presentation and provide more details in many places throughout the paper with major reorganization. Several steps and concepts which we assumed that a reader will already be familiar with, are now more explicitly described. \n\nReadability\n\n2.1) give the exact definitions in the appendix\nAns: We significantly expanded the appendix describing the material to introduce the reader to every manifold we have used. We also provided additional references for the interested reader. \n\n2.2) page 3 not clear\nAns: Pages 3-4 have been significantly expanded as a response to this comment. \n\n2.3) why structural constraint is good\nAns: In this revised version, we devoted effort and space into describing most aspects of this adjustment and its ramifications regarding the feasibility set and computational efficiency. \n\n2.4) Theorem 1 says E goes to zero when Eq 3b is satisfied. \nAns: The additional discussion related to the initialization followed by the RGD scheme (which is used specifically to maintain feasibility) on pages 2, 3, and 4, we believe will clarify these doubts. \n\n2.5) If Eq 3b is not satisfied, U, V may not be desired solutions. \nAns: We can clarify this doubt. Equation (3b) can be easily satisfied by choosing $\\widetilde{U}$ as the principal vectors, $S_u$ to be the top-k eigenvalues of $X^TX$ and $Q_u$ to be any special orthogonal matrix. This ensures that the initialization is feasible. Notice that the solution space of (3) is a subset of the solution space of (2) under the distributional assumption, so any feasible solution to (3) satisfies (2). \n\n2.6) claims were not clear\nAns: Based on the reviewer\u2019s suggestion, we rewrote and restructured most parts of page 3 including the part preceding and following the \u201cIntuition\u201d subsection, together with other improvements on pages 2 and 4/5. \n\u2028\u2028\n 3) Equivalence of (1), (3). \nAns: We added a sub-section specifically to describe the feasible solution set for (3). We also devoted significantly more text to describe the conditions under which our proposed CC estimator as a solution for (3) is consistent. \n\n4) Thm. presented in abrupt way.\u2028\nAns: Based on these suggestions, we removed Proposition 1 and used Proposition 2 (Proposition 1 in the revised pdf) as a bridge between CC estimator and Theorem 1. We added significantly more text and explanation so that a reader will follow the reasoning much more easily. \n\n5) Prop 5. how is this used\nAns: We have added text after Prop 5 to clarify this point. \n\u2028\n 6) algorithm isn't in main text.\nAns: We included more details of the algorithm in the text and more explanation of each of the main parts of the algorithm which will simplify understanding. \n\n7) major reorganization.\u2028\nAns: This concern is now addressed in this revision. As per reviewer\u2019s suggestion, we much improved the level of detail/explanation provided for each step and expanded the proof of Thm. 1 with additional description and annotation. \n\n8) Thm. 1: sub-Gaussian data; how far from I_k\nAns: For data that severely violates the assumption, our algorithm can be applied but evaluation will be empirical. But the consistency proof may not hold or will need to be reworked or additional assumptions or algorithmic adjustments will be needed on a case-by-case basis based on the distribution at hand to make the current steps go through. While removing such assumptions is desirable, such an assumption is not uncommon for such analyses for CCA as well as many other generic models. For example, Vershynin et al. (The Mathematics of Data, 2017) notes \u201cSub-gaussian distributions form a sufficiently wide class of distributions. Many results in probability and data science are proved nowadays for sub-gaussian random variables\u2026\u201d. \nThm. 1 states that if the data is sub-Gaussian, our proposed CC estimator (soln. of (3)) is consistent, i.e., asymptotically converges to the soln. of (2).\nObserve that, $C(X_k) = I_k$ using the ``whitening constraint\u2019\u2019 as X_k is the projection of X on U, thus  $C(\\tilde{X}_k) \\rightarrow I_k$ implies $C(\\tilde{X}_k) \\rightarrow C(X_k)$. We clarify this in the proof of Theorem 1. Observe that, with $\\widetilde{U}$ as the top-k principal directions, $S_u$ as inverse of the square root of top-k eigenvalues of $X^TX$ and $Q_u$ as any SO matrix, we can satisfy $C(\\tilde{X}_k)  = I_k$. But in practice, because of streaming PCA,  $C(\\tilde{X}_k)  \\rightarrow I_k$ asymptotically. Moreover, as $C(\\tilde{X}_k)$ converges to $I_k$, the error between $F$ (objective function (1)) and $\\widetilde{F}$ (objective function (3)) converges to $0$. \n\n9) Even if the upper triangular structure is preserved, S can be rank deficient\nAns: We ensure the full rank of the upper triangular matrix by ensuring that the diagonal to be non-zero. During optimization, we ensure the non-zero diagonal entries by optimizing over the log of the diagonal entries. We have added additional text which describe these details.   ", "title": "Clarifications for Reviewer 1"}, "3TB5BjWMuR_": {"type": "rebuttal", "replyto": "qyrddLnfRvx", "comment": "As per the reviewer's suggestion, we have added a comparison with Yager, 2012 in Table 1.", "title": "Comparison with Yger+2012"}, "qyrddLnfRvx": {"type": "rebuttal", "replyto": "wfYblg_7hRr", "comment": "1. Claim that their proposed method captures more correlation than MSG, two of the three datasets in which their method is superior (MNIST and CIFAR) are not realistic settings (i.e., correlation between left/right half of images).\n\nAns: We chose these datasets mainly because they are commonly used test-beds in other papers which study the CCA model (such as Ge et al. 2016 and Andrew et al. 2013). At a minimum,  they help in evaluating whether a model works satisfactorily in settings where one would certainly expect sufficient correlation. The use of CCA for the fairness experiment was designed to leverage the model\u2019s ability to work in a high dimensional setting, which will otherwise present a key bottleneck. Other use cases which could benefit from a CCA type objective during the training process, appear to be feasible.   \n\n2. Is it possible to make a (numerical) comparison with Yger+2012, which reformulates CCA as an optimization on the generalized Stiefel manifold?\n\nAns: We thank the reviewer for the suggestion. Yes, we will shortly update the paper with these experiments. \n", "title": "Experimental clarification+addtional comparison"}, "igA9vszSRsJ": {"type": "rebuttal", "replyto": "01DOtEv948y", "comment": "Dear Reviewer 1, \n\nIf these clarifications are useful, please let us know and we will update the paper. We also appreciate any other suggestions. \n\nThanks again for your time. ", "title": "Paper will be updated shortly"}, "01DOtEv948y": {"type": "rebuttal", "replyto": "yMX_CbHRtO7", "comment": "  1. *Paper is packed*\n\n  We hope that the reviewer agrees that most papers that draw upon a few different areas to derive the algorithm or to analyze its properties may appear to be packed. Indeed, some of the concepts can only be reviewed briefly given the page limits. This is not necessarily due to sloppy presentation. \n\n  2. *Improve readability: contributions, formulations, etc*\n\n  We request the reviewer to briefly look at page 3 again, if possible. Putting the analysis components aside temporarily, we are happy to clarify anything specific on page 3 regarding the contribution/formulation that is unclear. \n\n  Page 3 describes what the overall model is (shown in 2), what a minor adjustment to it yields (in 3a--3b) and also provides an intuition about why this may potentially help. The subsequent sections actualize this intuition and show that this is reasonable by describing the analysis and the mechanics of the numerical scheme.\n\n  3. *Equivalence of (1) and (3) was not clearly shown.*\n\n  We should note that (1) and (2) are both standard for CCA. So we can focus on (3). The reviewer can check that the adjustment from (2) -- (3) is actually minor: everything from (2) stays the same in (3) except that we further tease out the structure in $U$ as $U = \\tilde{U}A$ where $A = QS$, similarly for $V$. Q10 below will help easily clarify this doubt completely.\n\n  4. *Theorems presented in a bit abrupt way.*\n\n  We will much appreciate any guidance on a segue to Theorem 1 that will help address this concern. \n\n  5. *Even the algorithm does not appear in the main text.*\n\n  Alg. 1 in the main paper is indeed the full pseudocode. The appendix simply writes out the low-level details of the gradient calculations, needed only if someone wants to cross-reference with our code. We thought that an explicit description of these calculations (tedious but not difficult) will only add marginal value and impact readability. \n\n  6. *Color code is confusing.*\n\n  The colors were used to easily reference the description in Section 2.2 with the algorithm blocks in Algorithm 1. We are happy to modify it.\n\n  7. *Main Contribution*\n\n  We hope that the clarifications here and the other reviews will help resolve this concern fully. \n\n  8. *Unclear why the reformulation in (3) is a good approximation for CCA.*\n\n  We can clarify this doubt in two parts. \n  First notice that (3) rewrites the standard CCA in (2) using the additional structure on U and V (see Q10 below). Now, the second part deals with whether this is a sensible approximation. This is where we can check Theorem 1: we show that under some assumptions, solution of (3) is a consistent estimator of the CC directions. For the non-Gaussian distribution, one can resort to a case by case analysis but will involve additional assumptions as well as  adjustments to the algorithm making a concise presentation of the ideas difficult.\n\n  9. *\u201cSO(n)\u201d: what is  \u201cspecial\u201d here?*\n\n  $SO(n)$ is the group of $n\\times n$ orthogonal matrices with determinant 1. Referring to it as the special orthogonal group is common practice (https://mathworld.wolfram.com/SpecialOrthogonalGroup.html).  \n\n  10. *Why is there a Q in the middle?*\n\n  Here $\\tilde{U}$ is the matrix of principal vectors and $U$ is the canonical  directions (similarly for $V$). \n  A key observation is that $U$ lies in the column span of $\\tilde{U}$, i.e., $U = \\tilde{U}A$ where we have a full rank matrix $A$. But optimization on the space of full rank matrices is more challenging since we need to satisfy the rank constraint. So, we may decompose $A=QS$ where $Q$ is orthogonal and $S$ is an upper triangular matrix. Substituting $QS$ by $A$, we get $U = \\tilde{U} QS$. We hope this addresses the doubt regarding (3a)-(3b). \n\n  This adjustment makes the optimization convenient. Constraints for orthogonal and upper triangular matrices can be implicitly preserved throughout the Riemannian gradient descent optimization without any explicit regularization.\n\n  11. *How high-level description connects CCA with PCA using this reformulation*\n\n  The key premise (see Reviews 2, 4) is that by treating the PCA and maximizing canonical correlation objective separately, we can achieve potential benefits. The PCA part enforces the whitening constraint. The CC directions lie in the principal subspaces, hence with an efficient scheme to compute principal directions we need to find the appropriate coefficients, which is enabled by (3).\n\n  12. *Hard to see how the complexity of algorithm is calculated.*\n\n  Our algorithm involves structured matrices of size $d\\times k$ and $k\\times k$, so any matrix operation should not exceed a cost of $O(\\max(d^2k, k^3))$, since in general $d \\gg k$, which directly gives $O(d^2k)$. Specifically, as mentioned in the paper, the most expensive calculation is SVD of matrices of size $d\\times k$, which is $O(d^2k)$ (Cline and Dhilon, Handbook of Linear Algebra, 2006). All other calculations are dominated by this term.", "title": "Clarifications for Reviewer 1"}, "0zW7Tn_sbP": {"type": "rebuttal", "replyto": "D_YudmtLrJr", "comment": "We thank the reviewer for the review and appreciating our work. We clarify the main questions below, \n\nQ: The theoretical results are obtained under a strong assumption that X and Y both have Gaussian distribution.\n\nAns: \nWe answer in two parts, \n(1) Our setting closely followed the formulation described in a well known result on probabilistic CCA (A probabilistic interpretation of canonical correlation analysis,(Bach and Jordan, 2005 Tech report). Our convergence theorem (Theorem 1) holds under identical assumptions on $X$ and $Y$ as described in probabilistic CCA. The reviewer will likely agree that this assumption is common, even in the batch setting of CCA, to derive convergence or consistency guarantees in the finite sample regime. If we inspect the analysis described in Appendix A.2 that accompanies the result in Prop 2, we see that the steps will carry through as long as $\\Delta$ is bounded. On a case by case basis, depending on the distributional assumption, some other assumptions will need to be imposed to ensure that this condition holds so that the analysis leads to the desired guarantees. \n\n(2) The key computational advantages that the algorithm offers, as noted by the reviewer, emerge from breaking down the CCA objective into PCA (which satisfies the whitening constraint) and the module which maximizes correlation. This modification leads to the computational complexity benefits. On the other hand, the consistency of our estimator to compute canonical directions requires consistency of the PC estimator as well. It is for this reason that the Gaussian assumption seemed sensible.\n\n\nQ: Most propositions are from other papers.\n\nAns: The main theorem (Theorem 1) is original to this paper while we do utilize or restate results from other articles, as needed, at various places in the analysis. \n", "title": "Gaussian assumption"}, "p3VPG-r0j1B": {"type": "rebuttal", "replyto": "bUrYiJNWEff", "comment": "We thank the reviewer for appreciating our work. We are happy to answer any additional questions or address any outstanding concerns. Yes, we included (Andrew, 2013) in our deepCCA experiments shown in Table 1.", "title": "Appreciate for the positive comments"}, "bUrYiJNWEff": {"type": "review", "replyto": "pAq1h9sQhqd", "review": "The paper presents an approach to find canonical directions in a streaming fashion, i.e. without direct calculation of covariance matrices (which becomes hard when the number of examples is large). This solution to that task is not obvious, because the objective function of CCA, together with whitening constraints, does not allow simple additive decomposition.\n\nFirst, the optimization task is reformulated as a task over certain Riemannian manifolds, and a natural initialization is suggested. It is shown that under a certain assumption, this initialization is already a solution of good quality. Then, a natural minimization algorithm is presented, which is based on stochastic gradient descent on a Riemannian manifold. The key aspect of the algorithm is a combination of 2 types of gradient, the gradient for top-k principal vectors and standard gradient.\n\nThe experimental part shows that the algorithm successfully solves CCA in a streaming fashion. Also, it can be effectively combined with deep feature learning (Andrew, 2013) to find common features for multi-view representation learning tasks. Experiments look convincing.", "title": "A successful approach to streaming CCA", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "wfYblg_7hRr": {"type": "review", "replyto": "pAq1h9sQhqd", "review": "This paper aims to reduce the computational complexity of canonical correlation analysis.By decomposing the CCA projection matrices into a product of several structured matrices, a stochastic gradient-based optimization on a Riemannian manifold is provided reducing the computational complexity from $d^3$ to $d^2k$.\n\n\nStrength:\nCCA is a classic and still important method, especially in combination with deep neural networks (e.g., multi-view learnings). \nThe proposed method enables the applications of CCA to high-dimensional vectors with small memory.\nThis makes it easier to use CCA as an objective function of deep neural networks, which is trained on GPUs.\nExperiments show the benefits of their proposed method, in particular, the computational speed is 5-10 times faster than the existing method (MSG).\n\nWeakness:\nAlthough the authors claim that their proposed method captures more correlation than MSG, two of the three datasets in which their method is superior (MNIST and CIFAR) are not realistic setting (i.e., correlation between left/right half of images).\n\nQuestion:\nIs it possible to make a (numerical) comparison with Yger+2012, which reformulates CCA as an optimization on the generalized Stiefel manifold?", "title": "Official Blind Review #3", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "yMX_CbHRtO7": {"type": "review", "replyto": "pAq1h9sQhqd", "review": "main contribution\n\nThis work offers a stochastic CCA algorithm based on Riemannian optimization approach\n\nStrength\n\n- The paper offers a theory-backed algorithm for CCA under the assumption that the two views are sub-Gaussian. The complexity-accuracy tradeoff of the algorithm seems to be appealing according to the experiments.\n\nWeakness\n\nOverall, the biggest concern is readability. The paper is very packed and many treatments seem to be unbalanced. Important details are missing, and proofs seem to be hastily. The paper may need some re-packaging and re-organization before its core technical contents could be easily followed.\n\n- readability. The biggest concern of the work is that it is very hard to read. This creates a lot of barriers in understanding key aspects of the paper, e.g., contributions, formulations, novelty of the proof, just to name a few. The key formulation and the definitions of the manifolds were not clearly defined. The equivalence of (1) and (3) was not clearly shown. The theorems were presented in a bit abrupt way. Even the algorithm does not appear in the maintext but the appendix (which is also hard to read). The color code is used in a way that is a bit confusing (does ICLR allow writing in different colors?).\n\n- Clarify about the contribution. It is hard to clearly see how much is the contribution. The proofs seem to be short and most of the proofs are presented using \u201cpropositions\u201d cited from existing papers. If the authors think the contributions lie in reformulating the CCA problem as a PCA problem, then the reformulating part is perhaps the contribution. But from the current writing it is hard to follow how the reformulation comes through and how the reformulation enables using these existing propositions to prove convergence of the proposed algorithm.\n\n- It is unclear why the reformulation in (3) is a good approximation for CCA. Some attempts for justifying this were offered in Theorem 1, but it was based on the assumption X and Y are sub-Gaussian, which is at best a special case, even if the proof is correct (which, due to the current organization of the paper, is hard to read and fully understand).\n\n- \u201cSO(n): group/manifold of nxn special orthogonal matrices.\u201d what is the definition of \u201cspecial\u201d here? are they the commonly understood orthogonal matrices?\n\n- The paper has this upper triangular structure of the S matrices but this point seems to have no detailed explanation. If one understands U = \\tilde{U}S as the QR decomposition, then indeed S is upper triangular, but why is there a Q in the middle? Why is this Q useful?\n\n- the \u201chigh-level\u201d description of the algorithm says the idea is to connect CCA with PCA using this reformulation, but this point was not clearly explained.\n\n- Complexity. From the current writing, it is very hard to see how the complexity of the algorithm is calculated. The gradients needed are tabulated in the supplementary materials, but it is very hard for a reader to directly see why the algorithm saves computational complexity.\n\n- It is also unclear how the convergence and convergence rate analyses come together. These parts may need to be elaborated.\n", "title": "The major concern is clarity", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "D_YudmtLrJr": {"type": "review", "replyto": "pAq1h9sQhqd", "review": "1. Paper summary:\n\nThis paper proposes a method for solving linear CCA on high dimensional data. Linear CCA has a closed form solution. The solution requires a whitening step that costs O(d^3). This makes it not applicable to data in high dimensional spaces, e.g. representations learnt by deep networks. \n\nTo resolve the issue, the authors propose a reformulation of linear CCA which decomposes the transformation matrix U (V) into a product of three matrices. Those three matrices have the following properties:\n\n- Their initial values can be obtained by efficient streaming PCA on original view matrix X (Y). Streaming PCA costs O(d^2 * k) only where k is the top k eigenvectors.\n\n- They allow for Riemannian stochastic gradient descent which ensures their updated values lie on the same manifold.\n\n2. Strong points of the paper:\n\nThe new linear CCA formulation justifies the rationale of batch CCA training.\n\nUnder Gaussian distribution assumption:\n- The absolute difference between correlation found by original linear CCA and stochastic one is bounded.\n- The convergence of the training process is proven.\n\nThe experiments are performed on different aspects:\n- Recovering groundtruth transformations on MNIST, CIFAR and Mediamill data sets.\n- Learning deep features on MNIST data set.\n- Improving fairness in deep learning by adding CCA term to the loss function.\n\n3. Weak points of the paper:\n\nThe theoretical results are obtained under a strong assumption that X and Y both have Gaussian distribution.\n\nMost propositions are from other papers.", "title": "A stochastic linear CCA method for high dimensional data", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}