{"paper": {"title": "Unsupervised Learning of State Representations for Multiple Tasks", "authors": ["Antonin Raffin", "Sebastian H\u00f6fer", "Rico Jonschkowski", "Oliver Brock", "Freek Stulp"], "authorids": ["antonin.raffin@ensta-paristech.fr", "sebastian.hoefer@tu-berlin.de", "rico.jonschkowski@tu-berlin.de", "oliver.brock@tu-berlin.de", "freek.stulp@dlr.de"], "summary": "Learning method for automatic detection of multiple reinforcement tasks and extraction of state representations from raw observations", "abstract": "We present an approach for learning state representations in multi-task reinforcement learning. Our method learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed, nor of the number of tasks involved.\nThe method is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. In simulated experiments, we show that our method is able to learn better state representations for reinforcement learning, and we analyze why and when it manages to do so.", "keywords": ["Reinforcement Learning", "Unsupervised Learning"]}, "meta": {"decision": "Reject", "comment": "The authors propose to explore an important problem -- learning state representations for reinforcement learning. However, the experimental evaluation raised a number of concerns among the reviewers. The method is tested on only a single relatively easy domain, with some arbitrarily choices justified in unconvincing ways (e.g. computational limitations as a reason to not fix aliasing). The evaluation also compares to extremely weak baselines. In the end, there is insufficient evidence to convincingly show that the method actually works, and since the contribution is entirely empirical (as noted by the reviewers in regard to some arbitrary parameter choices), the unconvincing evaluation makes this method unsuitable for publication at this time. The authors would be encouraged to evaluate the method rigorously on a wide range of realistic tasks."}, "review": {"H1T0JpU8x": {"type": "rebuttal", "replyto": "r1aGWUqgg", "comment": "We would like to thank all reviewers for their thorough and helpful comments!\n\n1) Before we turn to the individual questions raised the reviewers, we would like to address the main issue that all reviewers raised, namely the relationship of our method to multi-task learning:\n\n\u201cThe authors state that the proposed method is orthogonal to multi-task learning though the end goal of learning to solve multiple tasks is the same.\u201d (AnonReviewer1)\n\u201cThe argument did not support the lack of comparison to multi-task joint-learning.\u201d (AnonReviewer2)\n\u201cLimiting the multi-task learning to be different to individual tasks rather than sharing and transferring knowledge between tasks\u201d (AnonReviewer 3)\n\nThe intro has been rewritten to clarify our motivation and how our work compares to multi-task learning. We completely agree that successful RL will require multi-task learning to share knowledge that generalizes over multiple tasks. \n\nBut there are sets of tasks that require multiple dedicated skills without sharing knowledge. For instance, in a video game, an agent have to achieve several subgoals (fight an enemy, avoid obstacles,...), each of these can be seen as individual task. Learning multiple, (sub-)policies dedicated to *different* tasks is a problem of its own right, as it faces significant theoretical issues, such as \u201ccatastrophic forgetting\u201d. We have elaborated on this argument in the introduction.\n\nSince there is few work approaching this problem in RL, our paper studies the question of how to learn fully independent policies for different tasks. We fully agree that future work will need to combine learning shared and separate representations but we regard our work on the  independent-policy multi-task RL problem as a contribution in itself. \n\nWe now reply to the individual comments raised by the reviewers.\n\n----\n\nAnonReviewer1\n\n1) \u201cReferences to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR \u201916), may be appropriate as well.\u201d\n\nThank you for the pointers, we have integrated the two suggested papers in the related work of the paper.\n\n2) \u201cThe approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task\u201d\n\nThis is the very idea of the method proposed, we updated the introduction to clarify the reasons we focused on this approach.\n\n3) \u201cIn the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.\u201d\n\nWe agree and as mentioned before this was a preliminary and incomplete experiment, and we decided to remove it from the paper. \n\n4) \u201cLastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups [...]\u201d\n\nWe agree that it is beneficial to apply a method to a wider range of tasks. Yet, we chose to invest into rigorously evaluating the performance of the method on the chosen task, and provide a thorough argument why and how the method works. We believe that it will scale to a wider range of tasks, but we will have to address this in future work.\n\n5) \u201cCould this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.\u201d\n\nYes, in principle it would be possible to use other state representation learning objectives.  Note, however, that in the slot car racing scenario a PCA/auto-encoder loss will not perform as well as LRP, as it has will try to explain all variations in the observation, in particular the second slot car. This has been shown in our previous work (Jonschkowski & Brock 2015) and is also reflected in the performance of PCA in the slot-car experiment.\n\n6) \u201dOne additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the \u201cknown car position\u201d baseline (which is also useful in its own right).\u201d\n\nThank you for this suggestion; in our experiment, however, the performance of the car in the single-task setting is identical to the performance we see in the multi-task setting. The reason is that the task detector module has a very high accuracy (greater than 99%) for the slot car tasks, and in consequence, a separate policy for each slot car is learned.\n\n7) \u201cDoes the \u201cobservations\u201d baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.\u201d\n\nOur experiments and previous work (Jonschkowski & Brock 2015) suggest that it will eventually reach the same performance with enough data, but for now, even in our largest experiments, we did not see it happening.\n\n8) \u201cIf there are aliasing issues with the images, why not just use higher resolution images?\u201d\n\nMainly computational reasons: we wanted to evaluate a wide variety of parameter settings and study their influence on our algorithm, yet we did not have the computational power to do this exhaustively on higher resolutions.\n\n---\n\nAnonReviewer2\n\n1) \u201cThe author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.\u201d\n\nWe are sorry that our answer in the pre-review phase did not address your question. We were trying to explain that our method is technically a soft gating, but effectively learns to perform hard gating. We are not sure how whether and how using a hard would influence the conclusion of the paper, and we are not aware of a way to implement a differentiable hard gate (if it is not differentiable, we cannot train it using backpropagation).\n\n---\n\nAnonReviewer3\n\n2) \u201cParameters choice is arbitrary (w parameters)\u201d\nThe weights w for the different methods are chosen as described in Jonschkowski & Brock 2015, by monitoring the gradient on a small part of the training set. The goal is to have gradients of the same magnitude for the different terms in the loss, so only relative weighting matters. Small changes to the parameters do not affect the method and there is no need for careful tuning.\n\n3) \u201cThe experiments could have been conducted using a standardized simulation tool such as OpenAI Gym to make it easy to compare.\u201d\nWe agree that evaluating experiments on a standardized tool such as OpenAI gym is a great idea. We want to point out, though, that the slot car racing task considered in the paper is a well-known task that has been evaluated in previous work, too, e.g. (Lange et al., 2012). Moreover, it is the simplest task that has the properties we are interested in this paper (non-overlapping tasks).\nBut we agree that that open simulation tools such as OpenAI gym are great and we will apply our method to these tasks in future work.", "title": "Reply to Reviewer Comments"}, "Bknjdl87g": {"type": "rebuttal", "replyto": "BkshJF17l", "comment": "Thank you for your questions!\n\n\n1) Thank you for this interesting question, and we think it is very relevant to discuss it. \nWe would like to rephrase your question as follows: should we try to learn 1) a single, multi-task policy (e.g. neural network) that is able to learn all tasks a robot ever encounters (for example walking, grasping, drilling, throwing\u2026), or should we try to learn 2) separate policies for each of these tasks? We believe that we need both: 1) multi-task policies that generalize over similar tasks, e.g. grasping objects of similar shape - but already for these \u201cnarrow\u201d tasks, generalization is very difficult (see, for example, Sergey Levine\u2019s work). Another drawback is \u201ccatastrophic forgetting\u201d of tasks that have not been seen for a long time. \nTherefore, we argue that we should investigate 2) the capability to learn task-specific policies, and extract them automatically. This allows to conserve task solutions and specialize them to the task more easily.\nWe focus on the latter problem of learning multiple task-specific policies, because it has gained far less attention so far than multi-task learning, although clearly both approaches are needed.\n\n2) The navigation task is a preliminary experiment which we have added to the paper to show how the method scales to non-linear function approximators. The fact that the method has identified most tasks can be seen in the cluster-like structure of the figure, and the fact that each cluster coarsely reflects the quadratic structure of the maze with the reward area located in a corner. We agree that future work will have to provide a more thorough analysis of this task and also different ones. But the main contribution of this paper is to present an architecture for learning to detect and solve multiple tasks; we believe that scaling its components up to more complex tasks is a straightforward extension due to the following reasons: 1) pre-trained ConvNets are freely available and can plugged into our architecture to be fine-tuned to the task. 2) The preliminary results of the MNIST task show that we can substitute the linear functions by a ConvNet. 3) (Jonschkowski & Brock, 2015) present a real world robot experiment operating on real images, using the LRP loss. \n\n3) Indeed, our architecture can do both: since we employ softmax units as output of the task detector, the network can perform soft or hard gating. In our case, the softmax outputs return close to binary outputs, thus effectively performing hard gating; this is encouraged by the cross-entropy task consistency loss we are using. However, (Droniou et al. 2015), who use a similar network architecture, but no loss on the output of the softmax units, report that in some cases the softmax units perform soft gating; in our case, this would mean that the network \u201cmixes\u201d different state representations. In preliminary experiments where we used linear instead of softmax units, we saw such a behavior, too; however, it did not allow us to converge to state representations that were useful for policy learning, which is why we chose to use softmax units and the suggested task-consistency loss.\n\nAlong a similar line of the argument outlined in our response to your first question, we believe that in many cases \u201chard gating\u201d is more effective for isolating and specializing on specific tasks. But multi-task learning with soft gating could be complementary by allowing \u201cmixing\u201d solutions to multiple tasks in order to solve more complex tasks.", "title": "Reply to AnonReviewer2"}, "Sk4rdxUmg": {"type": "rebuttal", "replyto": "B1gEwXyme", "comment": "Thank you very much for your questions!\n\n1) Overall, the method\u2019s parameters are not sensitive to small changes. The relative weighting  of the individual terms plays a role, but can be easily chosen as discussed by Jonschkowski & Brock 2015 - by monitoring the gradient on a small part of the training set. For the task-coherence terms, we made similar experiences, and for the tasks we studied, we achieved the best results when setting them roughly 10 times higher than the LRP losses, but there was no need for careful tuning.\n\n2) Thank you for bringing this up, we have not consider this relationship yet, but we do see one. \nOn the abstract level, our architecture clearly bears resemblance to attention mechanisms, as the task detector can be viewed as the \u201cwhat\u201d pathway and the state extraction as the \u201cwhere\u201d pathway of a cognition pipeline. On a technical level, we see differences, though, as the attention of the state extraction is not guided to a particular part of the image, but rather \u201cmodulated\u201d by the multiplicative gating weights. Although soft attention can be implemented by such a modulation, during learning the network is free choose how exactly the task detector modulates the state extractor. \n\n3) The key contribution of the paper is to identify how to structure and train a network to simultaneously detect tasks and extract state representations. Scaling to real images \u201conly\u201d involves substituting subparts of this network. Additional arguments can be made to support the view that our approach will scale: 1) Nowadays, pre-trained ConvNets are freely available and can be fine-tuned to the task at hand. 2) The preliminary results of the MNIST task show that we can substitute the linear functions by a ConvNet. 3) (Jonschkowski & Brock, 2015) present a real world robot experiment operating on real images, using the LRP loss. \n\nTherefore, we see no reason why the approach should not scale.\n\n4) That is an excellent question. The implementation in the paper operates only on the current observation -- therefore, it is unlikely that it detects the task without any visual cue.\n\nBut we would like to turn this question around and ask: which situation occurs more frequently in real-world scenarios, ones with an available visual cue or ones without? For example, a robot that has to learn use different tools, such as a screwdriver and a hammer, can easily distinguish them visually from single observation. \n\nMoreover, we see straightforward extensions to support identifying the task through motion, for example by augmenting the input with a time-delayed signal, and possibly also the previous action. Another, more complex, solution would be to use a recurrent network for task detection rather than a feedforward architecture. But the proposed architecture, which is the main contribution of the paper, easily supports such extensions.\n\n5) The navigation task is a preliminary experiment which we have added to the paper to show how the method scales to non-linear function approximators. We did not learn a policy in this experiment, but concluded from visual inspection of the representation and by comparing it to the results in (Jonschkowski & Brock 2015), it will be sufficient for learning a policy. We compared to linear baselines which do not learn the structure, and to the LRP method which learns a similar mapping as MT-LRP, but without identifying the individual tasks. As argued before, we consider the overall architecture and training loss as the main contribution of the paper.\n\n6) Yes, thank you!", "title": "Reply to AnonReviewer1"}, "BkshJF17l": {"type": "review", "replyto": "r1aGWUqgg", "review": "1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general multiple task setting, in particular over the multi-task learning?\n2. Following up to the previous question, please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem).\n3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting. In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks, the method aims to identify individual task and solve them separately. To this end, the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence, i.e., a task only changes representation between training episodes. The method has been evaluated on two tasks, multi-task slot-car racing and mobile navigation to prove its efficacy.\n\nthere were several unclear issues:\n\n1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general mutiple task setting, in particular over the multi-task learning?\nThe reply of the authors was not fully satisfactory. The argument did not support the lack of comparison to multi-task joint-learning. It seems they don't plan to include any comparison neither. I think it's important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage.\n\n2.Following up to the previous question, please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem).\nThe explanation of the authors did provide more details and more explicit information. \n\n3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.\nThe author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.\n\nIn summary, while there are remaining concerns about lacking comparisons, the is a weak tendency towards accepting the submission.", "title": "pre-review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rJIjX1MEx": {"type": "review", "replyto": "r1aGWUqgg", "review": "1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general multiple task setting, in particular over the multi-task learning?\n2. Following up to the previous question, please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem).\n3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.The paper presents a method to learn a low-dimensional state representations from raw obervation for multi-task setting. In contrast to classic multi-task learning setting where a joint representation is usually learned by exploring the transferable information among different tasks, the method aims to identify individual task and solve them separately. To this end, the authors extend the learning with robotic priors approach by extending the loss function with additional term for task coherence, i.e., a task only changes representation between training episodes. The method has been evaluated on two tasks, multi-task slot-car racing and mobile navigation to prove its efficacy.\n\nthere were several unclear issues:\n\n1. The first question is that if the method is only appealing on the scenario like the slot-car racing, otherwise it should be benchmarked with mutli-task learning. While the author made the argument in the related work, the proposed method is orthogonal to multi-task learning they did admit both explore shared knowledge between tasks. What's the advantage and disadvantage for the proposed method for general mutiple task setting, in particular over the multi-task learning?\nThe reply of the authors was not fully satisfactory. The argument did not support the lack of comparison to multi-task joint-learning. It seems they don't plan to include any comparison neither. I think it's important for the fundamental motivation for the work, without such comparison, the method seems to be purely an alternative to multi-task joint-learning without any(or much) practical advantage.\n\n2.Following up to the previous question, please clarify the results on the mobile navigation scenario. It's not clear how the plot on the right indicates MT-LRP identifies all tasks as the author claimed and and seems very weak to support the method, in particular compared to the multi-slot car-racing driving experiment, there is too little results to make sound argument (almost no comparison to alternative methods, i.e. no baseline method, is that true for the problem).\nThe explanation of the authors did provide more details and more explicit information. \n\n3. The proposed gated neural network architecture seems to be a soft gated structure(correct me if I am wrong), a possible baseline would be a hard gated unit, how would this affect the conclusion. This is particularly interesting as the authors reflect on the constraint that the representation should stay consistent during the training.\nThe author simply stated again what they did for the modeling without counter the comparison to hard-gating, but it's probably less an issue compared to Question 1.\n\nIn summary, while there are remaining concerns about lacking comparisons, the is a weak tendency towards accepting the submission.", "title": "pre-review questions", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B1gEwXyme": {"type": "review", "replyto": "r1aGWUqgg", "review": "Here are my pre-review questions:\n1. There seem to be a number of hyperparameters that weigh each of the loss terms on the representation objective (5 in total). How do you choose these hyperparameters, and how sensitive are the results to different values?\n\n2. The gating mechanism also seems related to soft attention mechanisms. How does it compare/differ?\n\n3. Could this approach scale to real images?\n\n4. How well would this approach work if there was no explicit cue for the task? For example, if the controlled car could only be identified by its motion, or if the background was not changed for different navigation environments?\n\n5. For the navigation task, the state representation is visualized. How do the policies perform for this task? Is the state representation sufficient for learning a good policy? Can you show the state representations learned by other methods?\n\n6. On page 8, the text references Figure 5.3 -- should this be a reference to Figure 7?This paper builds upon the method of Jonschkowski & Brock to learn state representations for multiple tasks, rather than a single task. The research direction of learning representations for multiple tasks is an interesting one, and largely unexplored. The approach in the paper is to learn a different representation for each task, and a different policy for each task, where the task is detected automatically and built into the neural network.\n\nThe authors state that the proposed method is orthogonal to multi-task learning, though the end goal of learning to solve multiple tasks is the same. It would be interesting and helpful to see more discussion on this point in the paper, as discussed in the pre-review question phase. References to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR \u201916), may be appropriate as well.\n\nThe method proposes to jointly learn a task classifier with a state representation learner, by using a differentiable gating mechanism to control the flow of information. The paper proposes a task coherence prior for this gating mechanism to ensure that the learned task classifier is temporally coherent. Introducing this structure is what enables the method to improve performance over the standard, non-multitask approach.\n\nThe evaluation involves two toy experimental scenarios. The first involves controlling one of two cars to drive around a track. In this task, detecting the \u201ctask\u201d is very easy, and the learned state representation is linear in the observation. The paper evaluates the performance of the policies learned with the proposed approach, and shows sufficient comparisons to demonstrate the usefulness of the approach over a standard non-multitask set-up.\n\nIn the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.\n\nLastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups, to demonstrate the generality of the approach and show that the method applies to more complex tasks. While in theory, the method should scale, the experiments do not demonstrate that it can handle more realistic scenarios, such as scaling beyond MNIST-level images, to 3D or real images, or higher-dimensional control tasks. Evaluating the method in this more complex scenario is important, because unexpected issues can come up when trying to scale. If scaling-up is straight-forward, then running this experiment (and including it in the paper) should be straight-forward.\n\nIn summary, here are the pros and cons of this paper:\nCons\n- The approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task\n- Only one experimental set-up that evaluates learned policy with multi-task state representation\n- No experiments on more realistic scenarios, such 3D environments or high-dimensional control problems\nPros: \n- This approach enables using the same network for multiple tasks, which is often not true for transfer and multi-task learning approaches\n- Novel way to learn a single policy for multiple tasks, including a task coherence prior which ensures that the task classification is meaningful\n- Experimentally validated on two toy tasks. One task shows improvement over baseline approaches\n\nThus, my rating would be higher if the paper included an evaluation of the control policy for navigation and included another more challenging and compelling scenario.\n\n\nLastly, here are some minor comments/questions on how I think the paper could be improved, but are not as important as the above:\n\nApproach:\nCould this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.\n\nExperiments:\nOne additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the \u201cknown car position\u201d baseline (which is also useful in its own right).\n\nDoes the \u201cobservations\u201d baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.\n\nIf there are aliasing issues with the images, why not just use higher resolution images?", "title": "pre-review questions", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJkwiyG4e": {"type": "review", "replyto": "r1aGWUqgg", "review": "Here are my pre-review questions:\n1. There seem to be a number of hyperparameters that weigh each of the loss terms on the representation objective (5 in total). How do you choose these hyperparameters, and how sensitive are the results to different values?\n\n2. The gating mechanism also seems related to soft attention mechanisms. How does it compare/differ?\n\n3. Could this approach scale to real images?\n\n4. How well would this approach work if there was no explicit cue for the task? For example, if the controlled car could only be identified by its motion, or if the background was not changed for different navigation environments?\n\n5. For the navigation task, the state representation is visualized. How do the policies perform for this task? Is the state representation sufficient for learning a good policy? Can you show the state representations learned by other methods?\n\n6. On page 8, the text references Figure 5.3 -- should this be a reference to Figure 7?This paper builds upon the method of Jonschkowski & Brock to learn state representations for multiple tasks, rather than a single task. The research direction of learning representations for multiple tasks is an interesting one, and largely unexplored. The approach in the paper is to learn a different representation for each task, and a different policy for each task, where the task is detected automatically and built into the neural network.\n\nThe authors state that the proposed method is orthogonal to multi-task learning, though the end goal of learning to solve multiple tasks is the same. It would be interesting and helpful to see more discussion on this point in the paper, as discussed in the pre-review question phase. References to other multi-task learning works, e.g. policy distillation and actor-mimic (both ICLR \u201916), may be appropriate as well.\n\nThe method proposes to jointly learn a task classifier with a state representation learner, by using a differentiable gating mechanism to control the flow of information. The paper proposes a task coherence prior for this gating mechanism to ensure that the learned task classifier is temporally coherent. Introducing this structure is what enables the method to improve performance over the standard, non-multitask approach.\n\nThe evaluation involves two toy experimental scenarios. The first involves controlling one of two cars to drive around a track. In this task, detecting the \u201ctask\u201d is very easy, and the learned state representation is linear in the observation. The paper evaluates the performance of the policies learned with the proposed approach, and shows sufficient comparisons to demonstrate the usefulness of the approach over a standard non-multitask set-up.\n\nIn the second navigation scenario, only the state representation is qualitatively shown, not the resulting control policy nor any other learned state representations for comparison. Since the multi-task state representation learning approach is only useful if you can also learn control better, the paper should also evaluate on control, with the same comparisons as in the first experiment. Without this evaluation, the experiment is incomplete.\n\nLastly, to be on par with publications at a venue like ICLR, the method should be evaluated more thoroughly, on a wider range of set-ups, to demonstrate the generality of the approach and show that the method applies to more complex tasks. While in theory, the method should scale, the experiments do not demonstrate that it can handle more realistic scenarios, such as scaling beyond MNIST-level images, to 3D or real images, or higher-dimensional control tasks. Evaluating the method in this more complex scenario is important, because unexpected issues can come up when trying to scale. If scaling-up is straight-forward, then running this experiment (and including it in the paper) should be straight-forward.\n\nIn summary, here are the pros and cons of this paper:\nCons\n- The approach does not necessarily share information across tasks for better learning, and requires learning a different policy for each task\n- Only one experimental set-up that evaluates learned policy with multi-task state representation\n- No experiments on more realistic scenarios, such 3D environments or high-dimensional control problems\nPros: \n- This approach enables using the same network for multiple tasks, which is often not true for transfer and multi-task learning approaches\n- Novel way to learn a single policy for multiple tasks, including a task coherence prior which ensures that the task classification is meaningful\n- Experimentally validated on two toy tasks. One task shows improvement over baseline approaches\n\nThus, my rating would be higher if the paper included an evaluation of the control policy for navigation and included another more challenging and compelling scenario.\n\n\nLastly, here are some minor comments/questions on how I think the paper could be improved, but are not as important as the above:\n\nApproach:\nCould this approach be combined with other state representation learning approaches? e.g. approaches that use an autoencoder.\n\nExperiments:\nOne additional useful comparison would be to evaluate performance in the single-task setting (e.g. only controlling the red car), as an upper bound on how well the policy should be able to perform. Does the learned multi-task policy reach the same level of performance? This upper bound will be tighter than the \u201cknown car position\u201d baseline (which is also useful in its own right).\n\nDoes the \u201cobservations\u201d baseline eventually reach the performance of the LRP approach? It would be useful to know if this approach simply speeds up learning (significantly) or if it enables better performance.\n\nIf there are aliasing issues with the images, why not just use higher resolution images?", "title": "pre-review questions", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}