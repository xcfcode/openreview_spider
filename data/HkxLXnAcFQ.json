{"paper": {"title": "A Closer Look at Few-shot Classification", "authors": ["Wei-Yu Chen", "Yen-Cheng Liu", "Zsolt Kira", "Yu-Chiang Frank Wang", "Jia-Bin Huang"], "authorids": ["weiyuc@andrew.cmu.edu", "ycliu@gatech.edu", "zkira@gatech.edu", "ycwang@ntu.edu.tw", "jbhuang@vt.edu"], "summary": " A detailed empirical study in few-shot classification that revealing challenges in standard evaluation setting and showing a new direction.", "abstract": "Few-shot classi\ufb01cation aims to learn a classi\ufb01er to recognize unseen classes during training with limited labeled examples. While signi\ufb01cant progress has been made, the growing complexity of network designs, meta-learning algorithms, and differences in implementation details make a fair comparison dif\ufb01cult. In this paper, we present 1) a consistent comparative analysis of several representative few-shot classi\ufb01cation algorithms, with results showing that deeper backbones signi\ufb01cantly reduce the gap across methods including the baseline, 2) a slightly modi\ufb01ed baseline method that surprisingly achieves competitive performance when compared with the state-of-the-art on both the mini-ImageNet and the CUB datasets, and 3) a new experimental setting for evaluating the cross-domain generalization ability for few-shot classi\ufb01cation algorithms. Our results reveal that reducing intra-class variation is an important factor when the feature backbone is shallow, but not as critical when using deeper backbones. In a realistic, cross-domain evaluation setting, we show that a baseline method with a standard \ufb01ne-tuning practice compares favorably against other state-of-the-art few-shot learning algorithms.", "keywords": ["few shot classification", "meta-learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper provides a number of interesting experiments for few-shot learning using the CUB and miniImagenet datasets. One of the especially intriguing experiments is the analysis of backbone depth in the architecture, as it relates to few-shot performance. The strong performance of the baseline and baseline++ are quite surprising. Overall the reviewers agree that this paper raises a number of questions about current few-shot learning approaches, especially how they relate to architecture and dataset characteristics.\n\nA few minor comments:\n- In table 1, matching nets are mistakenly attributed to Ravi and Larochelle. Should be Vinyals et al.\n- The notation for cosine similarity in section 3.2 is odd. It looks like you\u2019re computing some cosine function of two vectors which doesn\u2019t make sense. Please clarify this.\n- There are a few results that were promised after the revision deadline, please be sure to include these in the final draft.\n"}, "review": {"Bklu8z2wGB": {"type": "rebuttal", "replyto": "ByeHZRrvfr", "comment": "Thanks for your comment. However, both meta-learning methods and baseline/baseline++ methods see all available data points in training. Despite meta-learning methods only use n shots to train in each episode, the n shot batches they use are different in each episode.", "title": "Response"}, "BJxhOmgVVE": {"type": "rebuttal", "replyto": "Bye5ng5xEV", "comment": "Thank you for your interests. We have made our code publicly available: https://github.com/wyharveychen/CloserLookFewShot\n\nPlease free to drop us an email at weiyuc@andrew.cmu.edu if you have any questions.", "title": "Code publicly available"}, "HkloHv_hxV": {"type": "rebuttal", "replyto": "SJx2ptEhgV", "comment": "Thanks for your questions.\n\nFor hyper-parameters, we use the standard parameters of the ResNet architecture. We use the standard image size of 224 x 224 as input to the ResNet. We believe that the major difference between our re-implementation and the publicly available code for meta-learning methods (including ProtoNet, MatchingNet, MAML, and relation networks) lies in the data augmentation. \n\nWe expect to release our code by mid-Jan. Before that, we would be happy to chat more if you have specific questions. Feel free to drop us an email at weiyuc@andrew.cmu.edu\n", "title": "Response"}, "B1gwuwZLeV": {"type": "rebuttal", "replyto": "S1lH_j94gN", "comment": "Hi, thanks for your questions! We reply to the three questions below.\n\n1. Did the authors run your learning for matching networks, prototypical networks, maml, and relation networks with episodic training (sampled from N-classes and K-Shot every episode) from plain networks(conv and resnet)? Or did you train from baseline networks(pre-trained)? \n\nYes, we train all the networks (including matching networks, prototypical networks, MAML, and relation networks) with episodic training from the plain networks. All the networks were randomly initialized with He initialization, the standard initialization used in ResNet.\n\n2. What is the number of iteration here? is it the number of episode? or the number you learn the feature extractor (baseline)?\n \nYes, the number of iterations refers to the number of the episode. Thanks for pointing this out. We will use the number of episode in the revised manuscript for clarity.\n\n3. In MAML paper, they stated that using 64 filters may cause overfitting, do the authors suffer the same thing as you change the backbone of MAML?\n\nWe do not see the overfitting effect from observing the validation loss. We believe that it is due to the data augmentation used in all our experiments.\n", "title": "Response to the three questions"}, "S1eOn4vQeV": {"type": "rebuttal", "replyto": "B1lauBoZlE", "comment": "Thank you, we will include them in the appendix of the revised manuscript.", "title": "Thank you, we will"}, "Byl1iLIhRX": {"type": "rebuttal", "replyto": "HylrSpd9RQ", "comment": "Thanks R1 for the reply. Our goal of showing the cross-domain adaptation is to highlight the limitations of existing few-shot classification algorithms problem in handling domain shift. Our results in this setting show that 1) the baseline algorithm surprisingly outperforms all other few-shot classification methods and 2) the performance of few-shot classification algorithms can greatly benefit from further adaptation to the target domain even with a limited amount of data. We believe that our unified experimental setup will facilitate future efforts along this direction.\n\nIn the following, we also provide a taxonomy of existing work in related topics based on the availability of labeled/unlabeled data in the target domain, we would add the table to the appendix of the camera ready version to provide a more complete picture for the readers.\n\nDomain adaptation (DA): Evaluated on the *same* classes\t\t\t\t\t\n\t\t\t\t\t                \t                    Source domain\t\t        Target domain\t\n\t\t\t\t                 Domain shift\tLabeled \tUnlabeled\tLabeled (few) Unlabeled\nSupervised DA\t\t\n[Saenko et al., ECCV 2010]\t         V\t\t              V\t      -\t\t                   V \t\t        -\n[Motiian et al., NIPS 2017]\t\t\t\t\t\n\n\nSemi-supervised DA\t\t        V\t\t              V\t      -\t\t                   V\t\t        V\n\n\nUnsupervised DA \t\t        V\t\t              V\t      -\t\t                    -\t\t        V\n\n\n\n\nFew-shot classification: Evaluated on the *novel* classes\n\t\t\t\t\t\t                                          Base class\t\t           Novel class\t\n\t\t\t\t                       Domain shift\tLabeled \t Unlabeled\tLabeled (few) Unlabeled\nFew-shot\t\t\t                         -\t\t     V\t                 -\t\t           V\t\t          -\n\n\nCross-domain few-shot\t\t\n[Ours (third setting);\t\t                V\t\t     V\t                 -\t\t           V\t\t          -\nDong et al. ECML-KDD 2018]\t\t\t\t\t\n\n\nSemi-supervised few-shot \t\t\t\t\n[Ren et al. ICLR 18]\t\t               -\t\t     V\t                 V\t\t            V\t\t          V", "title": "Reply about the third setting "}, "S1ehsqeSAX": {"type": "rebuttal", "replyto": "HyeAj7bFnX", "comment": "Thanks for your comments! Our responses are as follow:\nQ1: If a relatively simple modification could improve the baselines, are there simple modifications available to other meta-learning algorithms being investigated? \n\nA1: The simple modification we made for the baseline approach is to replace the softmax layer with a distance-based classifier. However, among other meta-learning algorithms, only the MAML method is applicable to this modification. Both ProtoNet and MatchingNet already use distance-based classifier in their algorithm. RelationNet has its own relation module so is not applicable for this modification. While MAML could adopt this strategy, we did not include it into our experiment since our primary goal is not to improve one specific method.  \n\nQ2:  If the other algorithms are not as good as they claimed, can you give any insights on why and what to improve?\n\nA2: \nMeta-learning for few-shot classification algorithms are not as good as they claimed because of the following two aspects:\n\nFirst, in the CUB setting, the gap among each algorithm diminished when using a deeper backbone. That is, with a deeper feature backbone, the improvement from different meta-learning algorithm become less significant. Our results suggest that both deeper backbones and meta-learning algorithms both aim to reduce intra-class variation for improving few-classification accuracy. Consequently, when intra-class variation has been dramatically reduced using a deeper backbone, the contribution from meta-learning becomes less significant.\n\nSecond, in the CUB -> mini-ImageNet setting where a larger domain shift exists, the Baseline method outperforms all meta-learning algorithms. That is, existing meta-learning algorithms are not robust to larger domain shift. As discussed in section 4.4, while meta-learning methods learn to learn from the support set during the meta-training stage, all of the base support sets are still within the same dataset. Thus, these algorithms did not learn how to learn from a support set with large domain shift.\n\nWith our results, we encourage the community to tackle the challenge of potential domain shifts in the context of few-shot learning. We will release the source code and evaluation setting that will facilitate future research directions.\n", "title": "Responses to AnonReviewer3"}, "BJlgV0er07": {"type": "rebuttal", "replyto": "r1xNrc0Ts7", "comment": "Q4: In the Matching Nets paper, there is a good baseline classifier based on k-NNs. Do you know how does that one compares to Baseline and Baseline++ models if used with the same architecture for the feature extractor?\n\nA4: Here we show our 1-shot and 5-shot accuracy of Baseline and Baseline++ with the softmax and 1-NN classifier on the mini-ImageNet dataset with a Conv4 backbone. We only include the result of k = 1 with cosine distance to match the setting of Matching Nets paper.\n\n1-shot\n \t\t      softmax\t\t        1-NN (cosine distance)\nBaseline\t      42.11% +- 0.71%\t44.18% +- 0.69%\nBaseline++  48.24% +- 0.75%\t49.57% +- 0.73%\n\n5-shot\n \t\t      softmax\t\t        1-NN (cosine distance)\nBaseline\t      62.53% +- 0.69%\t56.68% +- 0.67%\nBaseline++  66.43% +- 0.63%\t61.93% +- 0.65%\n\nAs shown above, using 1-NN classifier has better performance than that of using the softmax classifier in 1-shot setting, but softmax classifier is better in 5-shot setting instead. We note that that the number presented here are not directly comparable to the results reported in the Matching Nets paper because we use a different \u201cmini-ImageNet\u201d separation. In this paper, we follow the data split provided by [Ravi et al. ICLR 2017], which is used in most few-shot papers. We have included the result in the appendix of the revised paper.\n\nQ5: The conclusion from the network depth experiments is that \u201cgaps among different methods diminish as the backbone gets deeper\u201d. However, in a 5-shot mini-ImageNet case, this is not what the plot shows. Quite the opposite: the gap increased. Did I misunderstand something? Could you please comment on that?\n\nA5: Sorry for the confusion. As addressed in 4.3, gaps among different methods diminish as the backbone gets deeper *in the CUB dataset*. In the mini-ImageNet dataset, the results are more complicated due to the domain difference. We further discuss this phenomenon in Section 4.4 and 4.5. We have clarified related texts in the revised paper. \n", "title": "Responses to AnonReviewer2 -- part 2 "}, "rklgcRlSC7": {"type": "rebuttal", "replyto": "r1xNrc0Ts7", "comment": "Thanks for your opinions! Our responses are as follow:\nQ1:  Is there an overlap between CUB and mini-ImageNet? If so, then domain shift experiments might be too optimistic or even then it is not a big deal?\n\nA1:  There are only 3 out of 64 base classes that are *birds* in the mini-ImageNet dataset. Furthermore, these three categories (house_finch, robin, toucan) are different from the 200 bird categories in CUB. Thus, a large domain shift still exists between the mini-ImageNet and the CUB dataset.\n\nQ2: The paper includes much redundant information which could go to the appendix in order to not weary the reader. For instance, everything related to Table 1. There is also some overlap between Section 2 and 3.3, while MAML, for instance, is still not well explained. Also, tables with too many numbers are difficult to read, e.g. Table 4. \n\nA2: Thanks for the comments. \nFirst, our purpose for showing Table 1 is two-fold: 1) it validates our reimplementation by comparing results from the reported numbers and 2) it shows that the implementations of the Baseline method in prior works are underestimated.\n\nSecond, we have included a more detailed description of MAML in the revised paper. \n\nThird, thanks for the suggestion. To improve the readability, we have modified Table 4 in the original paper to a figure (see Figure 5 in the revised paper). We include the detailed numbers in the appendix for reference.\n\nQ3:  Many of the few-shot learning papers use Omniglot, so I think it would be a valuable addition to the appendix. Moreover, there exists a cross-domain scenario with Omniglot-> MNIST which I would also like to see in the appendix.\n\nA3:  Thanks for the suggestions. We did not include Omniglot because its performance has been saturated in most of the recent work (~99%). We will add the results to the appendix in the camera-ready version for completeness. We agree that the Omniglot-> MNIST experiment will be a good addition to the paper. We will also add the results to the appendix in the camera-ready version.\n", "title": "Responses to AnonReviewer2 -- part 1"}, "H1xDM6erA7": {"type": "rebuttal", "replyto": "HJlAtk3vhm", "comment": "Q3: Another concern is that the same number of novel classes is used in the training and the testing stage. A more practical application of the learned meta model is to use it to handle different testing scenarios.\n\nA3: Thanks for pointing this out. As suggested, we conduct the experiments of 5-way meta-training and N-way meta-testing (where we vary the number of N to be 5, 10, and 20) to examine the effect of handling testing scenarios that are different from training. We compare the methods Baseline, Baseline++, MatchingNet, ProtoNet, and RelationNet. Note that we are unable to apply the MAML method as MAML learns the initialization for the classifier and can thus only be updated to classify the same number of classes.\n\nWe show the experimental results on mini-ImageNet with 5-shot meta-training as follows.\n\nBackbone: Conv4\t\t\t\n\t                5-way test\t                10-way test\t                20-way test\nBaseline\t        62.53% +- 0.69%\t        46.44% +- 0.41%\t        32.27% +- 0.24%\nBaseline++\t66.43% +- 0.63%\t        *52.26% +- 0.40%*\t*38.03% +- 0.24%*\nMatchingNet\t63.48% +- 0.66%\t        47.61% +- 0.44%\t         33.97% +- 0.24%\nProtoNet\t64.24% +- 0.68%\t        48.77% +- 0.45%\t        34.58% +- 0.23%\nRelationNet\t*66.60% +- 0.69%*\t47.77% +- 0.43%\t        33.72% +- 0.22%\n\t\t\t\nBackbone: ResNet18\t\t\t\n\t                 5-way test\t                10-way test\t                 20-way test\nBaseline\t         74.27% +- 0.63%\t        55.00% +- 0.46%\t         42.03% +- 0.25%\nBaseline++\t *75.68% +- 0.63%*\t*63.40% +- 0.44%*\t *50.85% +- 0.25%*\nMatchingNet\t 68.88% +- 0.69%\t        52.27% +- 0.46%\t         36.78% +- 0.25%\nProtoNet\t 73.68% +- 0.65%\t        59.22% +- 0.44%\t         44.96% +- 0.26%\nRelationNet\t 69.83% +- 0.68%\t        53.88% +- 0.48%\t         39.17% +- 0.25%\n\nOur results show that for classification with a larger-way (e.g., 10 or 20-way) in the meta-testing stage, the proposed Baseline++ compares favorably against other methods in both shallow or deeper backbone settings.\n\nWe attribute the results to two reasons. \n1) To perform well in a larger N-way classification setting, one needs to further reduce the intra-class variation to avoid misclassification. Thus, in both shallow and deeper backbone settings, Baseline++ has better performance than Baseline.\n\n2)  As meta-learning algorithms were trained to perform 5-way classification in the meta-training stage, the performance of these algorithms may drop significantly when increasing the N-way in the meta-testing stage because the tasks of 10-way or 20-way classification are harder than that of 5-way classification. \n\nOne may address this issue by performing a larger N-way classification in the meta-training stage (as suggested in [Snell et al. NIPS 2017]). However, this may encounter the issue of memory constraint. For example, to perform a 20-way classification with 5 support images and 15 query images in each class, we need to fit a batch size of 400 (20 x (5 + 15)) that must fit in the GPUs. Without special hardware parallelization, the large batch size may prevent us from training models with deeper backbones such as ResNet. We have include the result in the appendix of the revised paper.\n\nQ4: It is misleading by the following: \u201cVery recently, Motiian et al. (2017) addresses the few-shot domain adaptation problem.\u201d...\n\nA4: Thanks for the correction. Indeed, both Saenko et al. Gong et al. address the supervised domain adaptation problem with only a few labeled instances prior to [Motiian et al., NIPS 2017]. \n\nOn the other hand, we would like to point out another research direction. Very recently, the method in [Dong et al. ECML-PKDD 2018] addresses the few-shot problem where both the domain *and* the categories change.  This work is more related to our setting, as we also consider novel category accuracy in few-shot classification under domain differences. We have corrected the statement in the revised paper.", "title": "Responses to AnonReviewer1  -- part 2"}, "B1l0Vper0X": {"type": "rebuttal", "replyto": "HJlAtk3vhm", "comment": "Thanks for your comments! Our responses are as follow:\nQ1: \u201cUsing validation set to determine the free parameters...\u201d\n\nA1: Thank you for the comment. In our paper, we did use the validation set to select the best number of training iterations for meta-learning methods. Specifically, the exact iterations for experiments on the mini-ImageNet in the 5-shot setting with a four-layer ConvNet are:\n\n- ProtoNet:        24,600 iterations\n- MatchingNet: 35,300 iterations\n- RelationNet:   37,100 iterations\n- MAML:             36,700 iterations\n\nWe have clarified this in the revised paper. \n\nOn the other hand, we were not able to use the validation set for the Baseline and Baseline++. Note that validation set for few-shot problem splits by class, and does not split data in one class. With these validation classes in meta-training stage, one can validate how well the model can predict novel classes in meta-testing stage. However, the Baseline and Baseline++ methods cannot predict validation classes, as they has a fixed softmax layer to predict base classes. On the other hand, for meta-learning methods, the class to predict is conditioned on the class in the support set. Thus, with the support set in validation class, meta-learning methods can predict the validation class. As an alternative for Baseline and Baseline++, we directly train 400 epoches. We observe convergence from the training curve in both the Baseline and Baseline++ methods.\n\nFor the learning rate and optimizer, we use Adam with an initial learning rate 0.001 for all of the methods because the ProtoNet, RelationNet, and MAML methods all use the same setting as described in the respective papers. However, we cannot find information about the learning rate for MatchingNet. The learning rate of 0.001 is also given as a default hyper-parameter for Tensorflow and PyTorch. The results in Table 1 of our paper ensure that the results reproduce the performance presented in the original papers.\n\nFor other hyper-parameters such as the network depth in the backbone architecture, we have a detailed comparison as shown in Section 4.3 of the paper.\n\nQ2: The results of RelationNet are missing in Table 4.\n\nA2: Adapting RelationNet using training data in the support set (from novel classes) at the meta-testing stage is non-trivial. As the relation module in RelationNet takes convolution maps as input, we are not able to not replace the relation module with a softmax layer as we do for the ProtoNet and MatchingNet. \n\nAs an alternative, at the meta-testing stage, we split the training data in the novel class into support and query data and use them to update the relation module. Specifically, we take the RelationNet with a ResNet-18 feature backbone. We randomly split the few training data in novel class into 3 support and 2 query data to finetune the relation module for 100 epochs. The results on CUB, mini-ImageNet and mini-ImageNet ->CUB are shown below.\n\n\t\t CUB\t\t\tmini-ImageNet\tmini-ImageNet -> CUB\noriginal\t 82.75% +- 0.58%\t69.83% +- 0.68%\t57.71% +- 0.73%\nadapted\t 83.17% +- 0.57%\t70.49% +- 0.68%\t58.54% +- 0.72%\n\nIn all three cases, adapting the relation module using the support data in the meta-testing stage improves the results. However, the improvement is somewhat marginal. We have included the additional results in the revised paper.\n", "title": "Responses to AnonReviewer1 -- part 1"}, "HyeAj7bFnX": {"type": "review", "replyto": "HkxLXnAcFQ", "review": "The paper tried to propose a systematic/consistent way for evaluating meta-learning algorithms. I believe this is a great direction of research as the meta-learning community is growing quickly. However, my question is if a relatively simple modification could improve the baselines, are there simple modifications available to other meta-learning algorithms being investigated? If the other algorithms are not as good as they claimed, can you give any insights on why and what to improve?", "title": "Conclusion is a bit confusing", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "HJlAtk3vhm": {"type": "review", "replyto": "HkxLXnAcFQ", "review": "This paper gives a nice overview of existing works on few-shot learning. It groups them into some intuitive categories and meanwhile distills a common framework (Figure 2) employed by the methods. Moreover, the authors selected four of them, along with two baselines, to experimentally compare their performances under a cleaned experiment protocol. \n\nThe experiments cover three few-shot learning scenarios respectively for generic object recognition, fine-grained classification, and cross-domain adaptation. While I do *not* think the third scenario is \u201cmore practical\u201d, it is certainly nice to have it included in the experiments. \n\nThe experiment setup is unfortunately questionable. Since there is a validation set, one should use it to determine the free parameters (e.g., the number of epochs, learning rates, etc.). However, it seems like the same set of free parameters are used for different methods, making the comparison unfair because this set may favor some methods and yet hurt the others.  \n\nThe results of RelationNet are missing in Table 4.\n\nAnother concern is that the same number of novel classes is used in the training and the testing stage. A more practical application of the learned meta model is to use it to handle different testing scenarios. There could be five novel classes in one scenario, 10 novel classes in another, and 100 in the third, etc. The number of labeled examples per class may also vary from one testing scenario to anther. \n\nIt is misleading by the following: \u201cVery recently, Motiian et al. (2017) addresses the few-shot domain adaptation problem.\u201d There are a few variations in domain adaptation (DA). The learner has access to the fully labeled source domain and a small set of labeled target examples in supervised DA, to the source domain, a couple of labeled target examples, and many unlabeled target examples in semi-supervised DA, and to the source domain and many unlabeled target data points in the unsupervised DA. These have been studied long before (Motiian et al., 2017), for instance the works of Saenko et al. (2010) and Gong et al. (2013). \n\n[ref] Saenko K, Kulis B, Fritz M, Darrell T. Adapting visual category models to new domains. InEuropean conference on computer vision 2010 Sep 5 (pp. 213-226). Springer, Berlin, Heidelberg.\n\n[ref] Gong B, Grauman K, Sha F. Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation. InInternational Conference on Machine Learning 2013 Feb 13 (pp. 222-230).\n\nOverall, the paper is well written and may serve as a nice survey of existing works on few-shot learning. The unified experiment setup can facilitate the future research for fair comparisons, along with the three testing scenarios. However, I have some concerns as above about the experiment setups and hence also the conclusions. ", "title": "A nice experimental survey; experiment design could be improved", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "r1xNrc0Ts7": {"type": "review", "replyto": "HkxLXnAcFQ", "review": "There are a few things I like about the paper. \n\nFirstly, it makes interesting observations about the evaluation of the few-shot learning approaches, e.g. the underestimated baselines, and compares multiple methods in the same conditions. In fact, one of the reasons for accepting this paper would be to get a unified and, hopefully, well-written implementation of those methods. \n\nSecondly, I like the domain shift experiments, but I have the following question. The description of the CUB  says that there is an overlap between CUB and ImageNet.  Is there an overlap between CUB and mini-ImageNet? If so, then domain shift experiments might be too optimistic or even then it is not a big deal?\n\nOne thing I don\u2019t like is that, in my opinion,  the paper includes much redundant information which could go to the appendix in order to not weary the reader. For instance, everything related to Table 1. There is also some overlap between Section 2 and 3.3, while MAML, for instance, is still not well explained. Also, tables with too many numbers are difficult to read, e.g. Table 4.  \n\n---- Other notes -----\n\nMany of the few-shot learning papers use Omniglot, so I think it would be a valuable addition to the appendix. Moreover, there exists a cross-domain scenario with Omniglot-> MNIST which I would also like to see in the appendix.    \n\nIn the Matching Nets paper, there is a good baseline classifier based on k-NNs. Do you know how does that one compares to Baseline and Baseline++ models if used with the same architecture for the feature extractor?\n\nThe conclusion from the network depth experiments is that \u201cgaps among different methods diminish as the backbone gets deeper\u201d. However, in a 5-shot mini-ImageNet case, this is not what the plot shows. Quite the opposite: the gap increased. Did I misunderstand something? Could you please comment on that?\n", "title": "thought-provoking observations and nice comparative experiments", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}