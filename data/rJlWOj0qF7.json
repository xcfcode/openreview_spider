{"paper": {"title": "Imposing Category Trees Onto Word-Embeddings Using A Geometric Construction", "authors": ["Tiansi Dong", "Chrisitan Bauckhage", "Hailong Jin", "Juanzi Li", "Olaf Cremers", "Daniel Speicher", "Armin B. Cremers", "Joerg Zimmermann"], "authorids": ["tian1shi2@gmail.com", "christian.bauckhage@iais.fraunhofer.de", "jinhl15@mails.tsinghua.edu.cn", "lijuanzi2008@gmail.com", "cremerso@iai.uni-bonn.de", "dsp@bit.uni-bonn.de", "abc@iai.uni-bonn.de", "jz@bit.uni-bonn.de"], "summary": "we show a geometric method to perfectly encode categroy tree information into pre-trained word-embeddings.", "abstract": "We present a novel method to precisely impose tree-structured category information onto word-embeddings, resulting in ball embeddings in higher dimensional spaces (N-balls for short). Inclusion relations among N-balls implicitly encode subordinate relations among categories. The similarity measurement in terms of the cosine function is enriched by category information. Using a geometric construction method instead of back-propagation, we create large N-ball embeddings that satisfy two conditions: (1) category trees are precisely imposed onto word embeddings at zero energy cost; (2) pre-trained word embeddings are well preserved. A new benchmark data set is created for validating the category of unknown words. Experiments show that N-ball embeddings, carrying category information, significantly outperform word embeddings in the test of nearest neighborhoods, and demonstrate surprisingly good performance in validating categories of unknown words. Source codes and data-sets are free for public access \\url{https://github.com/gnodisnait/nball4tree.git} and \\url{https://github.com/gnodisnait/bp94nball.git}. ", "keywords": ["category tree", "word-embeddings", "geometry"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors provide an interesting method to infuse hierarchical information into existing word vectors. This could help with a variety of tasks that require both knowledge base information and textual co-occurrence counts.\nDespite some of the shortcomings that the reviewers point out, I believe this could be one missing puzzle piece of connecting symbolic information/sets/logic/KBs with neural nets and hence I recommend acceptance of this paper."}, "review": {"BygIkYrVm4": {"type": "rebuttal", "replyto": "r1eKFZSEX4", "comment": "the point is that all reviewers neglect the main point of this paper : a novel geometric method to perfectly impose structures onto vector space -- using higher dimensional regions than vectors. ", "title": "it is also an issue raised some weeks ago"}, "rJxuuPBEm4": {"type": "rebuttal", "replyto": "rJg1Mw07QN", "comment": "here is the earlier reply:\n(1) this year's submission policy accepts papers from arXiv. \"papers that have appeared on non-peered reviewed websites (like arXiv) or that have been presented at workshops (i.e., venues that do not have a publication proceedings) do not violate the policy\". So, you can detect authors of submissions that exist in arXiv.  \n\n(2) the detective work of your kind may round up names with less or no claims to authorship, because they may be master students from our institute or partner universities who help data collection and implementation. \n\n------\naccording to the policy, it is possible for reviewers to identify authors. in this case, you raised the reverse technique, after the real names are published, instead of the reviewing process. That is, the anonymization technique actually works. ", "title": "this issued has been replied blow"}, "H1l5RsAxzE": {"type": "rebuttal", "replyto": "HJlPWIflzN", "comment": "if we understand your explanation correctly, the implication rule y_1 ==> y_2 will not be logically expressed in terms of s_1(x) and s_2(x), in both [1] and [2]. The correct way shall be one of the two equivalent assertions:  (1) s_1(x) >= \\delta ==> s_2(x) >= \\delta, (2)  s_1(x) < -\\delta \\or s_2(x) >= \\delta.  \n", "title": "exact the point, not a distraction"}, "BkgXhXRezE": {"type": "rebuttal", "replyto": "HkguZ1dlG4", "comment": "(1) this year's submission policy accepts papers from arXiv. \"papers that have appeared on non-peered reviewed websites (like arXiv) or that have been presented at workshops (i.e., venues that do not have a publication proceedings) do not violate the policy\". So, you can detect authors of submissions that exist in arXiv.  \n\n(2) the detective work of your kind may round up names with less or no claims to authorship, because they may be master students from our institute or partner universities who help data collection and implementation. \n\n", "title": "two points have been neglected by the reviewer"}, "SkgQtt5kGN": {"type": "rebuttal", "replyto": "r1giuzlyGN", "comment": "please use your own words to do the explanation (why  s_1(x) only needs to be greater than  minus \\delta (-\\delta))  \n\nthanks. \n\nPS: the theorems you pointed in [1] and [2] do not directly go to the formalism, something is still missing. ", "title": "unacceptable comment"}, "S1gHx1sOZN": {"type": "rebuttal", "replyto": "ryeu1bGD-E", "comment": "\nin [2] page 65, s_k(x) is defined as s(x, y_k = 1) - s(x, y_k = 0)\nin page 66, y_1 ==> y_2 is explained as whenever the label y_1 is set to 1 (true) then the label y_2 must also be set to 1 (true). \nhowever, the formalism of y_1 ==> y_2 is defined as  s_1(x) >=  -\\delta ==>  s_2(x) >=  \\delta\n\nplease explain why  s_1(x) only needs to be greater than  minus \\delta (-\\delta).", "title": "Re: Not really..."}, "rJeV9kIS-N": {"type": "rebuttal", "replyto": "HJgwomi6gN", "comment": "It is not correct that \u201ein [1, 2], each entity is represented with a high dimensional ball with parametrized centers and radii learned from data, as mentioned in the review.\u201c \n\nIn [2] page 3, it is clearly stated that \u201eLet the decision region for each tag be modeled with a Euclidean ball centered at the embedding point of the tag, so that any image is tagged as, say, \u201ccat\u201d if and only if it is embedded inside the Euclidean ball corresponding to the \u201ccat\u201d tag.\u201c In [2] page 63, the author continues that \u201eThe idea is to ensure that the decision regions for the labels are constrained to match a conceptual Venn diagram that expresses the desired logical constraints between the labels, in terms of their inclusion and exclusion relationships.\u201c\n\nIt is clear that in [1] and [2] entities are represented by points, instead of high dimensional balls. \n\nOur work does not stem from [1] and [2], and has a number of fundamental differences.\u00a0\n\n1. Different representation framework. (a) Our work is region-based embedding (n-ball). So 'animal' and 'flower' are represented as high-dimensional regions. 'dog is an animal' is represented as the region of dog is inside of the region of animal; [1] and [2] are vector-based embedding, in there work, 'animal' and 'flower' are represented as points. 'dog is an animal' is interpreted as the distance from dog to animal is less than a threshold. (b) our work impose tree-structures ONTO pre-trained vector embeddings; while [1] and [2] introduces decision regions WITHIN the vector space.\n\n2. Different methods. Our work introduces novel geometric construction into learning representation; [1] and [2] follow the standard method \u2013 minimizing some objective functions.\u00a0\n\n3. Different quality of imposition. With geometric construction, we achieve precise imposition (zero energy cost). The methods of [1] and [2] cannot. \n", "title": "The basic representation of entity in [1] and [2] has been misunderstood"}, "rJgajhp3gE": {"type": "rebuttal", "replyto": "rygdUcV3eV", "comment": "do not see the line of your argument. ", "title": "Re: not really..."}, "Hkl-Ujd9eN": {"type": "rebuttal", "replyto": "BJl_Bmw9gN", "comment": "Thanks for the comment.  Your feeling is understandable. We have reviewed Figure 5.1 (in page 62) and Figure 5.2 (in page 70) of [2]. As is described in [2], this idea of representation dates back to Venn digram in 1880, see the caption of Figure 5.1 in [2]. And the region relation as illustrated in Figure 5.2 (in page 70) of [2] dates back the region calculus of Alfred North Whitehead in 1929 in the book \u201eProcess and Reality\u201c. We will add this two references in the final version. If space available, we consider add one of the two references you mentions which appear in 2015 and 2017. By the way, neither of the [1] and [2] references John Venn's work in 1880 and Alfred Whitehead's work in 1929.", "title": "agree to add reference, original work has the priority"}, "HklO7Cdt37": {"type": "review", "replyto": "rJlWOj0qF7", "review": "This paper proposes N-ball embedding for taxonomic data. An N-ball is a pair of a centroid vector and the radius from the center, which represents a word.\n\nMajor comments:\n\n- The weakness of this paper is lack of experimental comparisons with other prominent studies. The Poincare embedding and the Lorentz model are recently proposed and show a good predictive performance in hypernymy embedding.\n- WordNet concepts are actually structed in DAG. Recent studies on structure embedding can hadle DAG data. It is not clear how to extend N-ball embedding for handling DAT structures. \n\n- Related work is not sufficiently described.\n\n- It is not clear why N-ball embedding is suitable for hierarchical structures.\n", "title": "Proposal of N-ball embedding for tree structures", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1lpBN786Q": {"type": "rebuttal", "replyto": "rJlWOj0qF7", "comment": "Thanks for the comments. Unfortunately, three reviewers misunderstood or neglected the main point of this work.  \n\nOur work is targeted at the possibility of strictly imposing structures into  deep-learning systems. Such imposing must achieve global minimum (zero energy cost).  We imposed external tree structures into word-vectors produced by neural networks, achieving zero energy cost (this has not been achieved in the literature of representational learning). Our embedding also leads to perfect experimental results \u2013 no deep-learning systems in the literature has achieved 100% precision, as their training processes are only targeted at local minimums.  We would not use models, which only achieving local minimum, to compare with our model. This is not our aim.\n\nOur aim is to perfectly impose external structures into vectors produced by deep-learning models, while keeps these vectors unchanged as much as possible, which can be reviewed as an optimization for representation learning (one of the topics of iclr-19).\n\n\n\n \n\n\n\n\n\n", "title": "the main point of the work is unfortunately neglected by all reviewers"}, "S1gYrFNQCQ": {"type": "rebuttal", "replyto": "rJxJVYNq2Q", "comment": "thanks for the detailed comments, though not acceptable. \n\nthe anonymization issue is carefully taken cared of.  the github account is temporarily created for this projects. readers/reviews cannot figure out who we are.\n\non the other hand, the iclr submission policy allows papers that have appeared on non-peered reviewed websites (like arXiv).  therefore, anonymization  is not an issue.\n\n", "title": "the anonymization is carefully taken cared of"}, "rJxJVYNq2Q": {"type": "review", "replyto": "rJlWOj0qF7", "review": "Attention!!! This submission contains Github and Google Drive links to author-related accounts (see e.g. the abstract). I do not think this is permitted or standard. I leave the decision regarding \"automatic rejection\" of the submission to meta-reviewers of the paper.\n------------------------------------------------\nThe paper presents a method for tweaking existing vector embeddings of categorical objects (such as words), to convert them to ball embeddings that follow hierarchies. Each category is represented as a Eucldiean norm ball in high dimensional space, with center and radii adaptable to data. Next, inclusion and exclusion constraints on each pair of  balls are imposed based on the hierarchical structure. These constraints are imposed via an algorithmic approach.  The empirical study includes investigating the consistency of the representation with the hierarchy and demonstrating nearest neighbors for a set of words.\n\nOn the positive side, the paper addresses an important problem. It is readable and well organized. The related work could be improved by adding a number of representative related works such as [3,4].  \n\nThe major concern about the paper is the originality of the method. Encoding hierarchies with high dimensional balls and encoding inclusion and exclusion as constraints on those balls is a neat and powerful idea from modeling perspective. However, it is not novel, since the approach is already established for example in [1, 2 Chapter 5]. \nThe next major concern is regarding the evaluation of the quality of embeddings.\nThe empirical evaluation does not sufficiently evaluate the quality of tweaked embeddings. In contrast, the quantitative evaluation is more concerned with if the embeddings being consistent with the given hierarchy. In particular, not enough quantitative evidence that the proposed embeddings are actually effective in capturing semantics or in prediction tasks is provided. It should be noted that, the first aspect, ie consistency of the feasible solutions with hierarchy, can be theoretically established (see e.g. [1]).  The first paragraph of 3.2 seems unclear or wrong. See for example [2] for a gradient based solution for the problem.\nFinally, using an algorithmic approach as opposed to learning method for constructing embeddings, makes the method not directly related to the topic of ICLR conference.\n\nOverall, due to the above reasons, I vote the paper to be rejected. (The poor anonymization makes it a strong case for a reject.)\n\n[1] Mirzazadeh, F., Ravanbakhsh S., Ding N., Schuurmans D.,  \"Embedding inference for structured multilabel prediction\", NIPS 2015.\n[2] Mirzazadeh, F.\"Solving Association Problems with Convex Co-embedding\", PhD thesis, 2017. (Chapter 5)\n[3] Vilnis, Luke, and Andrew McCallum. \"Word representations via gaussian embedding.\", ICLR 2015.\n[4] Vendrov, I., Kiros, R., Fidler, S., Urtasun, R. \"Order-embeddings of images and language.\" ICLR 2016.", "title": "Nice geometrical observations, but not novel and have insufficient empirical evaluation of the quality", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJeKs2E52Q": {"type": "review", "replyto": "rJlWOj0qF7", "review": "This paper focuses on adjusting the pretrained word embeddings so that they respect the hypernymy/hyponymy relationship by appropriate n-ball encapsulation. They propose to do so by augmenting the word embeddings with information from a resource like Wordnet and applying 3 kinds of geometric transformations to enforce the encapsulation.\n\nThe motivation of doing this is not very clear and experimental results are mainly qualitative (and subjective) showing that hypernymy relation can be predicted and preserved by their adjustment. Since, this work relies on Wordnet, the coverage of vocabulary is severely limited and as the authors discus in the results with the section titled \"Experiment 3: Method 2\", they had to remove many words in the standard semantic similarity datasets which casts shadow on the usefulness of the proposed approach. It is unclear what the main contribution of such an approach.\n\nApart from this, the paper is diffcult to read and some parts (especially those pertaining to Figure 3) encode a simple concept that has been expressed in a very complicated manner.\n\nOverall, I give a score of 4 because of the limited coverage of the approach because of reliance on Wordnet and inadequate empirical evidence of usefulness of this approach.", "title": "Interesting task but weak evaluation mainly dominated by qualitative analysis", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}