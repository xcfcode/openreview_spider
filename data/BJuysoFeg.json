{"paper": {"title": "Revisiting Batch Normalization For Practical Domain Adaptation", "authors": ["Yanghao Li", "Naiyan Wang", "Jianping Shi", "Jiaying Liu", "Xiaodi Hou"], "authorids": ["lyttonhao@pku.edu.cn", "winsty@gmail.com", "shijianping5000@gmail.com", "liujiaying@pku.edu.cn", "xiaodi.hou@gmail.com"], "summary": "We propose a simple yet effective approach for domain adaptation on batch normalized neural networks.", "abstract": "Deep neural networks (DNN) have shown unprecedented success in various computer vision applications such as image classification and object detection. However, it is still a common annoyance during the training phase, that one has to prepare at least thousands of labeled images to fine-tune a network to a specific domain. Recent study shows that a DNN has strong dependency towards the training dataset, and the learned features cannot be easily transferred to a different but relevant task without fine-tuning. In this paper, we propose a simple yet powerful remedy, called Adaptive Batch Normalization (AdaBN) to increase the generalization ability of a DNN. By modulating the statistics from the source domain to the target domain in all Batch Normalization layers across the network, our approach achieves deep adaptation effect for domain adaptation tasks. In contrary to other deep learning domain adaptation methods, our method does not require additional components, and is parameter-free. It archives state-of-the-art performance despite its surprising simplicity. Furthermore, we demonstrate that our method is complementary with other existing methods. Combining AdaBN with existing domain adaptation treatments may further improve model performance.", "keywords": []}, "meta": {"decision": "Reject", "comment": "The paper performs domain adaptation using a very simple trick inspired by BatchNorm. The paper received below margin scores. The reviewers both, liked the simplicity of the approach, and at the same time felt that the contribution was too thin. Given the high bar of ICLR, this paper falls short."}, "review": {"HJlDXc1ZrX": {"type": "rebuttal", "replyto": "BJuysoFeg", "comment": "Although this paper has been rejected, I feel I need to say sth that matters if authors want to further develop their work. \n\nThe work tries to address a very important use case of BN when domain adaption is needed. I believe it is an important problem. I have some additional suggestions regarding its use cases.\n\n1. Think about what would happen if no fine-tuning is used at all. Would this trick improve generalization performance on a different domain?\n2. Instead of using accuracy, try more stable metrics based on ranking (e.g. AUC). I also suggest to report the variance of evaluation metrics under different random initialization. So we can observe how much gain is considered significant. \n3. Think about what is the reason to use this trick from the simplest case: (a) if you are using BN for logistic regression, what will happen? (b) how about auto-encoder? ", "title": "This is an important research question needing more addresses"}, "ByDjAQCHl": {"type": "rebuttal", "replyto": "HyxKF3sre", "comment": "We find another related paper [a] to support the effectiveness of our AdaBN. In that paper, the authors also proposed a simple but effective method to learn multiple styles at the same time for neural style transfer. In our previous comment, we have shown that neural style transfer can be intrinsically considered as a domain adaptation problem, which shares the same core process: distribution alignment. In [a], authors share a similar idea with our AdaBN method: modulating BN layers for distribution alignment. Differently, [a] learns different slope and bias terms in BN layers for different 'domains' (if we consider a target style image is an individual domain). While our AdaBN uses the mean and variance statistics in BN layers for different domains. \n\nWe believe our AdaBN and [a] both exploit the functionality of BN layers for distribution alignment, which is then applied in domain adaptation and neural style transfer, respectively. At the same time, our paper \"Demystifying Neural Style Transfer\" also connects the neural style transfer and domain adaptation. We think these three papers together provide a more holistic and systematical perspective of these two fields and the connection with BN layers.\n\n\n[a] A Learned Representation For Artistic Style (https://openreview.net/forum?id=BJO-BuT1g)\n", "title": "Another related paper"}, "HyxKF3sre": {"type": "rebuttal", "replyto": "BJuysoFeg", "comment": "This work also inspired our another recent work \"Demystifying Neural Style Transfer\"(https://arxiv.org/abs/1701.01036), which explains why Gram matrix represents the artistic style in neural style transfer. In that work, we also exploit the idea in our AdaBN method: matching BN statistics to align distributions in different domains, for neural style transfer and achieves comparable results with more complicated methods such as MMD. We believe this demonstrates the effectiveness of our method in domain adaptation field.", "title": "Additional explanation of our AdaBN method"}, "rJvrRuyrg": {"type": "rebuttal", "replyto": "BJuysoFeg", "comment": "According to the reviewers\u2019 helpful suggestions, we have revised and updated our paper. The main modifications are as follows:\n(1) We have updated the writing of our abstract and revised section 3.3 to make it clearer and merge it to the section 3.2.\n(2) We have removed the original section 4.3.1, and revised section \u201csensitivity to target domain size\u201d by adding the experimental results of using smaller number of images.\n(3) We have added a new analysis section \u201cAdaptation Effect for Different BN Layers\u201d in section 4.3.2.\n", "title": "Paper Updated"}, "ByGeAO1Se": {"type": "rebuttal", "replyto": "BJuysoFeg", "comment": "We do not think simplicity is our drawback of our method. On the contrary, we believe this is a great advantage of our method. Being technically simple does not mean no novelty, and it should never be a reason to reject a paper. In fact, Batch Normalization is a very simple method, while dropout is even simpler. These techniques have had huge impacts to the field despite the simplicity. As the Reviewer2 indicates, there are also prior simple but important methods in domain adaptation. (e.g. \u201cFrustratingly Easy Domain Adaptation\u201d and \u201cReturn of Frustratingly Easy Domain Adaptation\u201d) Further, to our best knowledge, there are no prior works to exploit Batch Normalization for domain adaptation.\n\nThe main contributions in our paper is that we propose a simple yet effective AdaBN method for domain adaptation by modulating the statistics in all BN layers, which outperforms the states-of-the-art methods. Furthermore, we demonstrate that our method is complementary with other existing methods. Thus, we think it is valuable for the researchers in the field.\n", "title": "About simplicity of our method."}, "HyrJnuJrx": {"type": "rebuttal", "replyto": "rkpVV6H4l", "comment": "Thanks for your insightful comments and suggestions of our work.\n1. As mentioned in the original BN paper: \u201cthe means and variances are fixed during inference.\u201d ( Ioffe & Szegedy, 2015). In practice, many methods \u201cfreeze the batch normalization parameters to those estimated during ImageNet pre-training\u201d (\u201cSpeed/accuracy trade-offs for modern convolutional object detectors\u201d) for detection and segmentation tasks. Thus, we think modulating BN statistics in the inference is far from been explored, especially for the domain adaptation task.\n\n2. About section 3.3 and section 4.3.1\nThanks for your suggestions. We have updated our paper.\n(1) We have revised section 3.3 to make it clearer and merge it to the section 3.2.\n(2) We have removed the original section 4.3.1. At the same time, we also added a new analysis section \u201cAdaptation Effect for Different BN Layers\u201d.\n", "title": "Response to Reviewer2"}, "ryxusdJBe": {"type": "rebuttal", "replyto": "rJW8h4GEl", "comment": "Thanks for your insightful comments and suggestions of our work.\n1.  About writing. \nWe have updated the writing of abstract to make our idea clearer.\n\n2.  About experiments.\n(1) We agree with the reviewer that same base network should be used for a fair comparison. Thus, we tried our best to transfer other methods to the Inception-BN model. As shown in our answers to the pre-review questions, we have implemented SA, GFK, LSSA and CORAL to reproduce results based on Inception-BN. For other methods, we had not reproduced promising results. This may be due to some missing implementation details (some methods do not have public implementations) or the uniqueness of AlexNet (AlexNet has intrinsic different structure with Inception). Thus, we do not report these controversial results. \n\n(2) We respectfully disagree with the reviewer\u2019s suggestion of training a model from scratch. Currently, most computer vision tasks (including classification, detection and segmentation) use an pre-trained model, and it has significant impact on the performance. For our domain adaptation task (especially for Office dataset), the number of images is too small (e.g. 500 images for DSLR domain) compared to that of ImageNet dataset (1M images).  Training a DNN like AlexNet or Inception-BN with such amount images would suffer from severe over-fitting and significantly drop the performance. Thus, we think this comparison with a model trained from scratch is meaningless. It cannot manifest the difference of different methods. \n", "title": "Response to Reviewer1"}, "S1MZj_yHl": {"type": "rebuttal", "replyto": "B1atIp-Ve", "comment": "Thanks for your insightful comments and suggestions of our work.\n1. About section 3.1\nThanks for your suggestion and we have updated our writing. However, we think aligning the distribution of training data is not just a side effect. It is the key way to achieve the purpose of BN which is to avoid the problem of vanishing gradients and help optimization. In original BN paper, the authors\u2019 motivation is to address the problem of \u201cinternal covariate shift\u201d, which means \u201cthe change in the distributions of layers\u2019 inputs\u201d. Thus, BN is proposed to \u201creduce internal covariate shift\u201d and make \u201cthe distribution of nonlinearity inputs more stable\u201d.\n\n(2) We also directly visualize the intermediate features with Inception-BN network instead of our BN features in Sec. 3.1. The figure can be seen at this link (https://s30.postimg.org/fdamc2l1t/a2d_feature_tsne.png). Red circles are features of samples from training domain (Amazon) while blue ones are testing features (DSLR). It blends much more than that in Figure 2. This demonstrates the statistics of BN layer indeed contain the traits of the data domain. The features of individual image cannot be separated directly in terms of different domains.\n\n2. About section 3.3 and section 4.3.1\nThanks for your suggestion. We have revised section 3.3 to make it clearer and merged it to the section 3.2.\nAccording to your and Reviewer2\u2019s suggestion, we remove the previous section 4.3.1. \n\n3. About section 4.3.2\nThanks for your suggestion. We have updated additional experimental results in the revised paper. \n(1) We have experiments with smaller number of samples and found that the performance will drop more (e.g. 0.652 with 16 samples, 0.661 with 32 samples.) We have updated the results in the section \u201cSensitivity to target domain size\u201d (section 4.3.1 now).\n(2) In the section \u201cAdaptation Effect for Different BN Layers\u201d (section 4.3.2 now), we add the detailed analysis of adaptation effect for different BN layers of our AdaBN method.\n", "title": "Response to Reviewer3"}, "rJKOUbbQe": {"type": "rebuttal", "replyto": "SJjZpU1me", "comment": "Thanks a lot for your questions. Our answers are shown as follows.\n1. In section 3.3, we make further discussions about two questions of our AdaBN after explaining the algorithm in section 3.1 and section 3.2. This section is not to explain more implementation details of the algorithm, but to discuss about some complementary thoughts of our method.\n     a) We explain why the simple translation and scaling operations in our AdaBN could approximate complex non-linear domain transfer function. This discussion demonstrates that our method has potentially strong transfer power despite of the simplicity.\n     b) We discuss about why we choose to transform neural response independently instead of decorrelating and then re-correlating the covariance matrix.\n\n2. In our knowledge, there are two unsupervised protocols for the Office dataset. \n      a) Use all source examples with labels and all target examples without labels in DAN (Long et al., 2015), CORAL (Sun et al., 2016) and RevGrad (Ganin & Lempitsky, 2015)\n      b) Subsample source examples (8 samples per category for webcam/dslr and 20 for amazon) in Saenko et al., 2010.\n      We use the first protocol in Table 1. In DAN (Long et al., 2015), CORAL (Sun et al., 2016) and RevGrad (Ganin & Lempitsky, 2015), the reproduced results are inconsistent with the same method (such as RevGrad) although these papers use the same protocol.\n      In our paper, the results of AlexNet, DDC and DAN are taken from the Table 1 in DAD (Long et al., 2015). The results of Deep CORAL are taken directly from its original paper. The results of RevGrad were taken from CORAL (Sun et al., 2016) before. We have updated the results of RevGrad from its original paper now.\n\n3. Since our method has to use deep models with BN layers, AlexNet is not suitable and we choose to use the Inception-BN model. For a fair comparison, we tried our best to transfer other methods to the Inception-BN model. We have implemented SA, GFK, LSSA and CORAL to reproduce results based on Inception-BN. \nHowever, it is not easy to transform from AlexNet to Inception-BN for other methods. There are two main reasons. First, some methods (e.g. DDC, Deep CORAL) do not have publicly available implementations. Second, AlexNet has 3 fully-connected layers, which is different with Inception-BN. However, some strategies in these methods are based on the structure of AlexNet. For example, DAN uses MK-MMD among the 3 fully connected layers. We tried to implement some of them based on Inception-BN but did not get promising results even worse than baseline method. This may be due to some missing implementation technologies or the unsuitableness of these methods for different network structures. Thus, we do not report these results for fair comparisons. \n\n", "title": "Answers to the questions of AnonReviewer1"}, "SJjZpU1me": {"type": "review", "replyto": "BJuysoFeg", "review": "1. p. 5, 3.3, What is the purpose of the second paragraph? I'm not sure I understand the link between this one and the previous one.\n\n2. p. 6, Table 1, What is the evaluation protocol for the Office dataset? DAN results are for the transductive setting; RevGrad results differ from the ones in the ICML paper (also transductive setting)\n\n3. The authors are using Inception BN as a base model while the other competing methods are built on top of AlexNet. Could that happen that those methods would also benefit from having a stronger base model. Is it possible to conduct a fair comparison?Update: I thank the authors for their comments. I still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pre-trained ImageNet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch. I would love to see a fair comparison of the state-of-the-art methods on toy datasets (e.g. see (Bousmalis et al., 2016), (Ganin & Lempitsky, 2015)). In my opinion, it's a crucial point that doesn't allow me to increase the rating.\n\nThis paper proposes a simple trick turning batch normalization into a domain adaptation technique. The authors show that per-batch means and variances normally computed as a part of the BN procedure are sufficient to discriminate the domain. This observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source dataset by the corresponding statistics from the target dataset.\n\nOverall, I think the paper is more suitable for a workshop track rather than for the main conference track. My main concerns are the following:\n\n1. Although the main idea is very simple, it feels like the paper is composed in such a way to make the main contribution less obvious (e.g. the idea could have been described in the abstract but the authors avoided doing so). \n\n2. (This one is from the pre-review questions) The authors are using much stronger base CNN which may account for the bulk of the reported improvement. In order to prove the effectiveness of the trick, the authors would need to conduct a fair comparison against competing methods. It would be highly desirable to conduct this comparison also in the case of a model trained from scratch (as opposed to reusing ImageNet-trained networks).\n", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJW8h4GEl": {"type": "review", "replyto": "BJuysoFeg", "review": "1. p. 5, 3.3, What is the purpose of the second paragraph? I'm not sure I understand the link between this one and the previous one.\n\n2. p. 6, Table 1, What is the evaluation protocol for the Office dataset? DAN results are for the transductive setting; RevGrad results differ from the ones in the ICML paper (also transductive setting)\n\n3. The authors are using Inception BN as a base model while the other competing methods are built on top of AlexNet. Could that happen that those methods would also benefit from having a stronger base model. Is it possible to conduct a fair comparison?Update: I thank the authors for their comments. I still think that the method needs more experimental evaluation: for now, it's restricted to the settings in which one can use pre-trained ImageNet model, but it's also important to show the effectiveness in scenarios where one needs to train everything from scratch. I would love to see a fair comparison of the state-of-the-art methods on toy datasets (e.g. see (Bousmalis et al., 2016), (Ganin & Lempitsky, 2015)). In my opinion, it's a crucial point that doesn't allow me to increase the rating.\n\nThis paper proposes a simple trick turning batch normalization into a domain adaptation technique. The authors show that per-batch means and variances normally computed as a part of the BN procedure are sufficient to discriminate the domain. This observation leads to an idea that adaptation for the target domain can be performed by replacing population statistics computed on the source dataset by the corresponding statistics from the target dataset.\n\nOverall, I think the paper is more suitable for a workshop track rather than for the main conference track. My main concerns are the following:\n\n1. Although the main idea is very simple, it feels like the paper is composed in such a way to make the main contribution less obvious (e.g. the idea could have been described in the abstract but the authors avoided doing so). \n\n2. (This one is from the pre-review questions) The authors are using much stronger base CNN which may account for the bulk of the reported improvement. In order to prove the effectiveness of the trick, the authors would need to conduct a fair comparison against competing methods. It would be highly desirable to conduct this comparison also in the case of a model trained from scratch (as opposed to reusing ImageNet-trained networks).\n", "title": "Questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}