{"paper": {"title": "Guided Exploration with Proximal Policy Optimization using a Single Demonstration", "authors": ["Gabriele Libardi", "Gianni De Fabritiis"], "authorids": ["~Gabriele_Libardi1", "~Gianni_De_Fabritiis1"], "summary": "An algorithm based on importance sampling for effienciently use a single human demonstration to solve hard-exploration problems in reinforcement learning ", "abstract": " Solving sparse reward tasks through exploration is one of the major challenges in deep reinforcement learning, especially in three-dimensional, partially-observable environments. Critically, the algorithm proposed in this article uses a single human demonstration to solve hard-exploration problems. We train an agent on a combination of demonstrations and own experience to solve  problems with variable initial conditions. We adapt this idea and integrate it with the proximal policy optimization (PPO). The agent is able to increase its performance and to tackle harder problems by replaying its own past trajectories prioritizing them based on the obtained reward and the maximum value of the trajectory.\nWe compare variations of this algorithm to different imitation learning algorithms on a set of hard-exploration tasks in the Animal-AI Olympics environment.\nTo the best of our knowledge, learning a task in a three-dimensional environment with comparable difficulty has never been considered before using only one human demonstration.", "keywords": ["PPO", "sparse rewards", "single demonstration", "3D environment"]}, "meta": {"decision": "Reject", "comment": "There was quite a bit of internal discussion on this paper. To summarize:\n- The idea is very neat and interesting and likely to work\n- The paper is likely to inspire future work\n- There are still serious doubts  about the experimental evaluation that is not entirely up to par with current standards\n  - The reviewers were not convinced 100% by the arguments about the 'custom' environments\n  - The reviewers were not convinced 100% that the baselines were given their best shot\n\nWhile the paper has potential to provide valuable input for the community, it needs a bit more work before being presentable at a highly competitive venue like ICLR.\n"}, "review": {"DpW_sqMMTJL": {"type": "rebuttal", "replyto": "xZCqmyNjsX5", "comment": "Thanks for the changes, clarifications, and comments.\n\n>Thank you\n\nI'll not yet update my rating because you didn't address my comment about MuJoCo environments. The ones that are built into OpenAI Gym fulfill your criteria (random init and partial observability are given; sparse reward is trivial to get from the dense reward).\n\n> We did initially try the Halfcheetah environment even if this environment is not partially observable. We could not get PPO+D to work on it and we did not feel confident enough if it was due to a bug in the adaptation of PPO+D for this environment, the fact that the continuous action space might need some adjustments to the algorithm or something else. For instance the fact that mujoco terminates the episode when the agent falls close to the ground affects the proportion of environment experience/demo replay since the way we regulate this in the algorithm relies on the episodes and not on the frames directly. This meant that most of the frames were repeating the demonstration for PPO+D, as the normal policy would terminate the episode almost immediately.\nThe experiments we tried with halfcheetah were with a sparsified reward. The fact that such a task is solvable when the reward is given only once the agent covers a certain distance, is not clear at all: the original reward function punishes large movements besides rewarding the distance and it is hard to imagine that such a reward would have the same effect when not given at each timestep. The fact we have so many doubts on this task made us omit this result, because there is really nothing we can say with confidence. \nWe switched to Atari because due to the discrete action space they are more similar to the Animal-AI environment and we discussed the results on Atari, where we could make PPO+D work, even though due to the dense rewards, it is not ideally suited for it. .\n\nAnd how long are your experiments that you don't have resources for 5 seeds? Usually, when I run PPO for 1mil steps with a single CPU and a single GPU, it takes 1h-2.5h, depending on the environment.\n\n> Doing the extra 2 seeds would have meant 100*2 + 60*2 +40*2 millions for the first plot and 40*2 + 40*2 + 100*2 for the second plot. Since the other reviewer requested more baselines, that would have meant for these we world have needed: GAIL: 40*5 + 120*5 PPO+BC: 5*40 and 6*5*4 for the ablation study.\nThese sum up to 1880 million frames to run on the 6x8 cores CPUs and 12 GPUs that we had available, it would have been quite difficult to make it in time. At the frame rate of AnimalAI, which is a slow environment due to the 3D rendering engine, we could perform around 300 frames per second on 16 cores+1GPU, meaning more than 20 days to finish. We thus made a choice to focus on porting PPO+D to work on Atari/Mujoco and adding the baselines requested by the other reviewers, rather than adding seeds as the difference between PPO+D and the other baselines is substantial and statistical significant already with 3 seeds (we use min/max in the plots not +-std). \n", "title": "Further explanation"}, "avpfKUtwXrU": {"type": "rebuttal", "replyto": "HFTVcaE4vCG", "comment": ">Thanks for your positive  feedback.\n\nWeaknesses\ncould have compared against an implementation of some facsimile of R2D3\n\n>We initially attempted to have a baseline with R2D3, but the  initial paper did not provide any code and the environment was also not public. The only public implementation we found is incomplete as it has missing proprietary code which was not publicly released and it was necessary for running parallel environments.\n\nlack of generalization to wider initial conditions means that the results may not be as interesting as they first seem.\n\n>We decided to frame the results around the quite remarkable fact that the agent is able to solve these very hard tasks given a *single* demonstration, in an environment with variable initial conditions. If the agent was given more demonstrations with very different initial conditions it is reasonable to believe (although should be investigated) that it would learn to solve any box configuration, when given the trajectory of a few demonstrations. With a single demonstration it is surprising the agent generalizes to different starting positions. It does not generalize to different boxes configuration. Nevertheless we added the following to section 5.1,on our guess to why PPO+D generalizes to some initial conditions and not others: \n\u201cThis could be because there is a smooth transition between some of the initial conditions. Due to this fact, if the agent is able to generalize even only between initial conditions that are close it will be able to progressively learn to perform well for all initial conditions starting from one demonstration. In other words the agent is automatically using a curriculum learning strategy, starting from the initial conditions that are closer to the demonstration. This approach fails when there is an abrupt transition between initial conditions, such as for different boxes configurations.\u201d\n\nbehavior cloning is, unsurprisingly, a poor baseline for a problem with variable initial conditions and with partial observability.\n\n>The baseline algorithm is given a hundred times more data than the PPO+D algorithm, each one with a different initial condition, whereas PPO+D only repeats the same trajectory with the same initial condition. Nevertheless, we have now also added more baselines such as GAIL and the PPO+BC obtaining similar results as before, i.e. PPO+D provides the best performance.\n\nthe main result is really: \"sometimes it works!\". That is actually of interest, although really a \"proof by example\" result.\n\n>Our results are strictly empirical, apart from how  to insert in a mathematically correct way demonstrations into PPO. \n\nonly tested on limited tasks, although similar to R2D3. Thus it is unclear how generalizable the method and results will be; it may overfit in some respects to the task at hand.\n\n> This is not a general RL algorithm nor it claims to be, and we do not expect it to perform particularly well in normal dense reward tasks. On such tasks PPO+D is likely to perform rather poorly as replaying the same trajectories is likely to be more of an hindrance than  booster of performance in this case. However, we do believe the algorithm can generalize to problems that have similar formulations, in terms of the sparse reward assignment, but are very different in the nature of the task.\n\nwhy could behavior cloning realistically be expected to work as a baseline, given the variable initial states?\n\n>Because BC is given a hundred times more data than the PPO+D algorithm, each one with a different initial condition, whereas PPO+D only repeats the same trajectory with the same initial condition. We also added more baselines such as GAIL and the PPO+BC suggested by reviewer 2 with similar outcomes.\n\nAdditional Feedback\nDR and DV: it's unclear what the 'R' stands for in DR. It's always helpful to define the origins of the chosen nomenclature for cases where it is not obvious.\n\n>We have added a clarification of the origin of the terms as we introduce them see \u201c... are the trajectories collected prioritizing the value estimate (V stands for value), and D_{R} (R stands for reward) contains the initial human...\u201d\n\nplease tell the reader where the value estimates come from for the value buffer for unsuccessful trajectories. Why aren't these all simply zero?\n\n>It is because the critic generalizes to yet unseen images. For instance if the agent sees something really similar to an image in the demonstration trajectory it is expected to assign it a non zero value. \nIn section 3.1 we added \u201cCrucially for the unsuccessful trajectories it is possible for the maximum value estimate not to be zero when the observations are similar to the ones seen in the demonstration.\u201d\n\nFigure 1 is really helpful; place it earlier in section 3?\n\n>Review N.1 likes it, so we have decided to keep it, but unfortunately had to reduce its size to make room for some of the changes\n\n", "title": "Response"}, "4z9D1Oku5fi": {"type": "rebuttal", "replyto": "BuarVHb8YRb", "comment": "From the text it seems like BC baseline doesn't have access to rewards. If that is the case, the baselines are very weak - they either have access to demonstrations only or sparse rewards only.\n\n>BC does not have access to rewards, but there are no rewards apart from successfully completing the problem. Furthermore, BC is given a hundred times more human demonstrations than PPO+D, each one with a different initial condition. PPO+D only has  one trajectory with a single initial condition.  To improve the benchmarking, we also added more baselines such as GAIL and the PPO+BC as  suggested.\n\nThe most basic fair baseline would be doing BC gradient updates to policy network from time to time during PPO training - should be a reasonable amount of work to implement. Ideally, comparison to relevant prior work should be at least attempted: Paine et al (R2D3) or some other whichever is easier to implement (see below for related work comments).\n\n>We have included the algorithm suggested, i.e. doing BC gradient updates to policy network from time to time during PPO training) in the same conditions as PPO+D with a single trajectory on the easiest task \u201cOne box easy\u201d. The results indicated that this too performs worse than PPO+D. We have added a baseline using GAIL which also performs worse than PPO+D .  As mentioned in the paper we would have been happy to compare with R2D3 but no multi-threaded implementation of the algorithm was made available  and the environment on which they conducted the experiments was also not publicly available.\n\n\nPros:\nNice research direction: combining demonstrations with sparse rewards.\n\n>Thanks for your positive  feedback.\n\nCons:\nMore baselines are needed to evaluate the value of the proposed method. Prior work does exist (e.g., Paine et al.) and warrants more extensive comparisons.\n\n>As indicated above, we have included the algorithm BC+PPO and GAIL to our baselines, still we can maintain that PPO+D performs better in the current settings.\n\nRelated work needs to mention some relevant papers:\n\"One-Shot Imitation Learning\" https://arxiv.org/pdf/1703.07326.pdf\n\"Watch, try, learn: meta-learning from demonstrations and rewards\" https://arxiv.org/pdf/1906.03352.pdf\n\n>We agree the two papers you mentioned are very relevant to the work presented here and have included the following in section 2: \u201cOther approaches, such as Duan et al. (2017); Zhou et al. (2019) pursue a meta-learning strategy where the agent learns to learn from demonstrations, such approaches are perhaps the most promising, but they require at least a demonstration for each task for a subset of all tasks.\u201d\n\nThere are some established benchmarks for sparse-reward tasks - it might be more productive to attack those as they already have some baselines. Introducing new tasks requires some discussion on why the established tasks are unsuitable for the goals of the paper. For example:\nVizdoom navigation by Pathak et al https://arxiv.org/pdf/1705.05363.pdf\nhttps://openai.com/blog/ingredients-for-robotics-research/\nhttps://aihabitat.org/\n\n>Like us Paine et al (R2D3) deem existing environments unsuitable as they do not exhibit all or some of the following characteristics: sparse rewards. partial observability and variable initial conditions. We mention this at the beginning of section 4.1, 1 and  2  as we discuss that some of the best performing algorithms on these baselines rely on the absence of  the characteristics  we list above.  We would have tested it on the environment proposed in R2D3 as we agree it fulfills these requirements, but it was not made available. As we mention in the paper we draw inspiration from that environment and try to create tasks of comparable complexity. \n", "title": "Response"}, "OVlGkQUGPBA": {"type": "rebuttal", "replyto": "n3GpgOTxq1", "comment": "\n>Thanks for your positive  feedback.\n\nWeaknesses\nThe main problem I have with this is actually the lack of further experiments. For such an easy extension of PPO, I would've expected you to have no problem running this on an Atari environment and at least one MuJoCo environment too (where it's also easy to gather a human demonstration, like controlling the reacher via inverse-kinematics or the tricky Pusher). Compared to vanilla PPO we should see improvements across the board, no?\n\n>Like us, Paine et al. (R2D3) deem existing environments unsuitable as they do not exhibit all or some of the following characteristics: sparse rewards, partial observability and variable initial conditions. We expect PPO+D to perform well in environments that have these characteristics.  For instance the fact that we replay certain trajectories to counter the effect of catastrophic forgetting, if used in tasks where catastrophic forgetting is not a problem to begin with, will slow down the training. We tried running PPO+D on the BreakoutNoFrameskip-v4 of the Atari environment and we concluded PPO+D leads to worse performance than vanilla PPO, although it also learns to complete the task, it is considerably slower.\nWe have added to the manuscript: \u201cWe emphasise that PPO+D is designed to perform well on hard-exploration problems with stochastic environment and variable different conditions, we tested it on the  of the Atari environment \u201cBreakoutNoFrameskip-v4\u201d and conclude that it does not lead to better performance than vanilla PPO when the reward is dense. PPO+D also learns to complete the task although more slowly than PPO\u201d at the end of section 5.1.\n \nSimilarly, you just arrived at the hyperparameters \ufffd=0.1,\ufffd=0.3 without explanation or ablation. Do you maybe want to justify how these parameters came to be and what happens if either parameter is higher or lower?\n\n>We chose the value of these hyperparameters by trial and error. We added now an ablation study in the Appendix.\n\nhow to make me raise my score: Include at least one Atari and one Mujoco/Robot environment (since Ilya Kostrikov's implementation that you use supports these out of the box) and either add an ablation study on a single env over p/phi or explain the importance of these values.\n\n>We tried running PPO+D on the BreakoutNoFrameskip-v4 of the Atari environment and we concluded PPO+D leads to worse performance than vanilla PPO, although it also learns to complete the task, it is considerably slower.\nWe have added to the manuscript: \u201cWe emphasise that PPO+D is designed to perform well on hard-exploration problems with stochastic environment and variable different conditions, we tested it on the  of the Atari environment \u201cBreakoutNoFrameskip-v4\u201d and conclude that it does not lead to better performance than vanilla PPO when the reward is dense. PPO+D also learns to complete the task although more slowly than PPO\u201d at the end of section 5.1.\nWe added now an ablation study in the Appendix.\n\nImpact & Recommendation\n... If the present method was really an all-around improvement over PPO, why did the authors not show it on a tried-and-true OpenAI Gym task but only in their own made-up setting?\n\n>See answer right above\n\nMinor Nitpicks\nI'd report a few more seeds - I think 5 seeds is a good starting point.\n\n>We tried but finally we could not run five seeds for all of the experiments due to lack of resources. We  hope  3 seeds are acceptable.\n\nYour plotting of runs is uncommon - usually, you either plot the mean and standard deviation or the mean and min/max.\n\n>We have now plotted mean and min/max as suggested.\n\nIn Fig. 3 and 4, the fonts in the legend need to be bigger\n\n>Fixed.\n\nOn page 6 you're twice in a row weirdly enthusiastic for Unity-ML / the \"flexibility allowed by this environment\". These are odd things to say in a research paper unless you're an employee of Unity\n\n>We have removed the sentence about Unity.  Although in this sentence: \u201cWe take advantage of the great flexibility allowed by this environment to design hard-exploration problems for our experiments\u201d we refer to the Animal AI environment and not ML-Agents.\n\nAlgorithm 1 is a bit verbose but great. Makes it very clear. On the flip side of that, Figure 1 is a bit redundant. These two things communicate the same idea and I like Algo 1 better.\n\n>We have kept Figure 1 as other reviewers specifically find it helpful, we have reduced its size to make space for some of the changes.\n\nThere are a few missing commas, like \"In our approach,\", bottom of the first page.\n>We have revised the text for this type of error. \n\n", "title": "Response"}, "HFTVcaE4vCG": {"type": "review", "replyto": "88_MfcJoJlS", "review": "## Summary\nThis paper looks at problems that have sparse rewards, are partially observable, and have\nvariable initial conditions.  Previous work [R2D3, Paine et al 2019] tackles this problem using \noff-policy recurrent Q-learning. Instead, this work proposes to use a PPO+D, an on-policy method (PPO)\nfor a policy with memory (GRUs). This relies on storing successful trajectories (demonstration(s) \nand discovered), and replaying these as experiences using importance sampling. Other details\nsuch as a value-prioritized buffer, annealed use of that buffer, an entropy term, and more\nare given.  The method is tested on four environment variations, as constructed in the Animal-AI Olympics challenge env.\nExact comparisons with R2D3 are not possible due to missing initial condition ranges. \nIt is compared to a behavior cloning baseline, vanilla PPO, and a value-buffer ablation,\nIt is shown that it can succeed with just one demonstration, although it fails to generalize\nto all many of the variations for the problems.\n\n## Strengths\n- interesting problem setting (although this is not novel, e.g., R2D3)\n- interesting exploration beyond R2D3, i.e., to on-policy methods\n- use of value buffer as a means to allow for progress on harder problems\n\n## Weaknesses\n- could have compared against an implementation of some facsimile of R2D3\n- lack of generalization to wider initial conditions means that the results may not be as interesting as they first seem\n- behavior cloning is, unsurprisingly, a poor baseline for a problem with variable initial conditions and with partial observability\n- the main result is really: \"sometimes it works!\".  That is actually of interest, although really a \"proof by example\" result.\n- only tested on limited tasks, although similar to R2D3.  Thus it is unclear how generalizable the method and results will be;  it may overfit in some respects to the task at hand.\n\n## Recommendation\nOverall, while the results come with some real caveats, the paper may inspire further work in the use of demonstrations in sparse-reward settings, in conjunction with on-policy methods such as PPO. The use of the value-prioritized buffer is also a feature\nthat could see further adoption. \n\n## Questions\n- why could behavior cloning realistically be expected to work as a baseline, given the variable initial states?\n\n## Additional Feedback\n- DR and DV: it's unclear what the 'R' stands for in DR.  It's always helpful to define the origins of the chosen nomenclature for cases where it is not obvious.\n- please tell the reader where the value estimates come from for the value buffer for unsuccessful trajectories.  Why aren't these all simply zero?\n- Figure 1 is really helpful;  place it earlier in section 3?", "title": "Progress on a difficult class of RL problem", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BuarVHb8YRb": {"type": "review", "replyto": "88_MfcJoJlS", "review": "##########################################################################\n\nSummary:\n\nThe paper proposes a modification of the PPO algorithm which can accommodate a single task demonstration with the goal of faster learning in sparse-reward tasks. There are 4 tasks proposed by the paper, inspired by Animal-AI Olympics. The baselines are Behavioural Cloning (BC) and vanilla PPO. The proposed method outperforms those baselines.\n\n\n##########################################################################\n\nReasons for score:\n\nFrom the text it seems like BC baseline doesn't have access to rewards. If that is the case, the baselines are very weak - they either have access to demonstrations only or sparse rewards only.\n\nThe most basic fair baseline would be doing BC gradient updates to policy network from time to time during PPO training - should be a reasonable amount of work to implement. Ideally, comparison to relevant prior work should be at least attempted: Paine et al (R2D3) or some other whichever is easier to implement (see below for related work comments).\n\n\n##########################################################################\n\nPros:\n\n1. Nice research direction: combining demonstrations with sparse rewards.\n\n##########################################################################\n\nCons:\n\n1. More baselines are needed to evaluate the value of the proposed method. Prior work does exist (e.g., Paine et al.) and warrants more extensive comparisons.\n2. Related work needs to mention some relevant papers:\n- \"One-Shot Imitation Learning\" https://arxiv.org/pdf/1703.07326.pdf\n- \"Watch, try, learn: meta-learning from demonstrations and rewards\" https://arxiv.org/pdf/1906.03352.pdf\n3. There are some established benchmarks for sparse-reward tasks - it might be more productive to attack those as they already have some baselines. Introducing new tasks requires some discussion on why the established tasks are unsuitable for the goals of the paper. For example:\n- Vizdoom navigation by Pathak et al https://arxiv.org/pdf/1705.05363.pdf\n- https://openai.com/blog/ingredients-for-robotics-research/\n- https://aihabitat.org/\n\n##########################################################################\n\nQuestions during rebuttal period: \n\nPlease address and clarify the cons above.", "title": "Baselines are very weak", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "n3GpgOTxq1": {"type": "review", "replyto": "88_MfcJoJlS", "review": "## Summary\n\nThe authors introduce a simple extension of PPO that uses a single expert demonstration and a modified sampling algorithm to show better performance than vanilla PPO in a difficult 3D setting.\n\n## Strengths & Weaknesses\n\n#### Strengths\n\n- The paper is well-written and the method is simple, yet powerful. Learning something from a single demonstration is no easy feat.\n- The videos shown in the submission look impressive in how difficult the setup is and how the agent manages to learn complex strategies like.\n- Overall, the paper is very \"short & sweet\" in that it's not a groundbreaking new technique, but a small change to PPO but it's well explained, and the results that _are_ in the paper are good.\n  \n#### Weaknesses\n\n- The main problem I have with this is actually the lack of further experiments. For such an easy extension of PPO, I would've expected you to have no problem running this on an Atari environment and at least one MuJoCo environment too (where it's also easy to gather a human demonstration, like controlling the reacher via inverse-kinematics or the tricky Pusher). Compared to vanilla PPO we should see improvements across the board, no?\n- Similarly, you just arrived at the hyperparameters $p = 0.1, \\phi=0.3$ without explanation or ablation. Do you maybe want to justify how these parameters came to be and what happens if either parameter is higher or lower?\n\n**TL;DR how to make me raise my score:** Include at least one Atari and one Mujoco/Robot environment (since Ilya Kostrikov's implementation that you use supports these out of the box) and either add an ablation study on a single env over p/phi or explain the importance of these values.\n\n## Impact & Recommendation\n\nThis is fundamentally good material. It's not groundbreakingly new but I think it could make for an easy-to-use imitation learning baseline that would help in a lot of scenarios get the method off the ground. However, the current paper doesn't show the rigour and depth of analysis that I would expect from an ICLR paper. I hope the authors can make up for that in the rebuttal week and then I'm happy to up my score. Currently, my recommendation is borderline. If the present method was really an all-around improvement over PPO, why did the authors not show it on a tried-and-true OpenAI Gym task but only in their own made-up setting? \n\n## Minor Nitpicks\n\n- I'd report a few more seeds - I think 5 seeds is a good starting point.\n- Your plotting of runs is uncommon - usually, you either plot the mean and standard deviation or the mean and min/max. \n- In Fig. 3 and 4, the fonts in the legend need to be bigger \n- On page 6 you're twice in a row weirdly enthusiastic for Unity-ML / the \"flexibility allowed by this environment\". These are odd things to say in a research paper unless you're an employee of Unity\n- Algorithm 1 is a bit verbose but great. Makes it very clear. On the flip side of that, Figure 1 is a bit redundant. These two things communicate the same idea and I like Algo 1 better.\n- There are a few missing commas, like \"In our approach,\", bottom of the first page.\n\n", "title": "Great work, but missing experiments or more depth in analysis", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}