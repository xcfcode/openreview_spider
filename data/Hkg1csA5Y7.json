{"paper": {"title": "A fast quasi-Newton-type method for large-scale stochastic optimisation", "authors": ["Adrian Wills", "Thomas B. Sch\u00f6n", "Carl Jidling"], "authorids": ["adrian.wills@newcastle.edu.au", "thomas.schon@it.uu.se", "carl.jidling@it.uu.se"], "summary": "", "abstract": "During recent years there has been an increased interest in stochastic adaptations of limited memory quasi-Newton methods, which compared to pure gradient-based routines can improve the convergence by incorporating second order information. In this work we propose a direct least-squares approach conceptually similar to the limited memory quasi-Newton methods, but that computes the search direction in a slightly different way. This is achieved in a fast and numerically robust manner by maintaining a Cholesky factor of low dimension. This is combined with a stochastic line search relying upon fulfilment of the Wolfe condition in a backtracking manner, where the step length is adaptively modified with respect to the optimisation progress. We support our new algorithm by providing several theoretical results guaranteeing its performance. The performance is demonstrated on real-world benchmark problems which shows improved results in comparison with already established methods.", "keywords": ["optimisation", "large-scale", "stochastic"]}, "meta": {"decision": "Reject", "comment": "The paper investigates a novel formulation of a stochastic, quasi-Newton optimization strategy based on the natural idea of relaxing the secant conditions.  This is an interesting and promising idea, but unfortunately none of the reviewers recommended acceptance.  The reviewers unanimously fixated on weaknesses in the paper's technical presentation.  In particular, the reviewers expressed some dissatisfaction with many aspects, including:\n\n- Key details of the experimental evaluation were omitted (particularly concerning configuration of the baseline competitors), which is an essential aspect of reproducibility.  One consequence is that the reviewers were not confident in the veracity of the experimental comparison.\n\n- The reviewers struggled with a lack of clarity and accurate rendering of some key technical details.  An example is dissatisfaction with the non-symmetry of the inverse Hessian approximation, which was not fully alleviated by the author responses.\n\n- The proposed approach does not appear to possess any intrinsic advantage over standard methods from a computational complexity perspective.\n\nI think this is promising work, but a careful revision that strengthened the underlying technical claims appears necessary to make this a solid contribution."}, "review": {"rklAzwpVhm": {"type": "review", "replyto": "Hkg1csA5Y7", "review": "This paper presents a new quasi-Newton method for stochastic optimization that solves a regularized least-squares problem to approximate curvature information that relaxes both the symmetry and secant conditions typically ensured in quasi-Newton methods. In addition to this, the authors propose a stochastic Armijo backtracking line search to determine the steplength that utilizes an initial steplength of 1 but switches to a diminishing steplength in later iterations. In order to make this approach computationally tractable, the authors propose updating and maintaining a Cholesky decomposition of a crucial matrix in the Hessian approximation. Although it is a good attempt at developing a new method, the paper ultimately lacks a convincing explanation (both theoretical and empirical) supporting their ideas, as I will critique below.\n\n1. Stochastic Line Search\n\nDetermining a steplength in the stochastic setting is a difficult problem, and I appreciate the authors\u2019 attempt to attack this problem by looking at stochastic line searches. However, the paper lacks much detail and rigorous reasoning in the description and proof for the stochastic line search.\n\nFirst, the theory gives conditions that the Armijo condition holds in expectation. Proving anything about stochastic line searches is particularly difficult, so I\u2019m on board with proving a result in expectation and doing something different in practice. However, much of the detail on how this is implemented in practice is lacking. \n\nHow are the samples chosen for the line search? If we go along with the proposed theory, then when the function is reevaluated in the line search, a new sample is used. If this is the case, can one guarantee that the practical Armijo condition will hold? How often does the line search fail? How does the choice of the samples affect the cost of evaluating the line search?\n\nThe theory also suggests that the particular choice of c is dependent on each iteration, particularly the inner product between the true search direction and the true gradient at iteration k. Does this allow for a fixed c to be used? How is c chosen? Is it fixed or adaptive? What happens as the true gradient approaches 0?\n\nThe algorithm also places a limit on the number of backtracks permitted that decreases as the iteration count increases. What does the algorithm do when the line search fails? Does one simply take the step although the Armijo condition does not hold?\n\nIn deterministic optimization, BFGS typically needs a smaller steplength in the beginning as the algorithm learns the scale of the problem, then eventually accepts the unit steplength to obtain fast local convergence. The line search proposed here uses an initial steplength of $\\min(1, \\xi/k)$ so that in early iterations, a steplength of 1 is used and in later iterations the algorithm uses a $\\xi/k$ steplength. When this is combined with the diminishing maximum number of backtracking iterations, this will eventually yield an algorithm with a steplength of $\\xi/k$. Why is this preferred? Are the other algorithms in the numerical experiments tuned similarly?\n\nThe theory also asks for a descent direction to be ensured in expectation. However, it is not the case that $E[\\hat{p}_k^T \\hat{g}_k] = E[\\hat{p}_k]^T g_k$, so it is not correct to claim that a descent direction is ensured in expectation. Rather, the condition is requiring the angle between the negative stochastic gradient direction and search direction to be acute in expectation.\n\nAll the proofs also depend on a linear Taylor approximation that is not well-explained, and I\u2019m wary of proofs that utilize approximations in this way. Indeed, the precise statement is that $\\hat{f}_{z\u2019} (x + \\alpha \\hat{p}_z) = \\hat{f}_{z\u2019} + \\alpha \\hat{p}_z\u2019 \\hat{g}_z(x + \\bar{\\alpha} \\hat{p}_z)$, where $\\bar{\\alpha} \\in [0, \\alpha]$. How does this affect the proof?\n\nLastly, I would propose for the authors to change the name of their condition to the \u201cArmijo condition\u201d rather than using the term \u201c1st Wolfe condition\u201d since the Wolfe condition is typically associated with the curvature condition (p_k\u2019 g_new >= c_2 p_k\u2019 g_k), hence referring to a very different line search. \n\n2. Design of the Quasi-Newton Matrix\n\nThe authors develop an approach for designing the quasi-Newton matrix that does not strictly impose symmetry or the secant condition. The authors claim that this done because \u201cit is not obvious that enforced symmetry necessarily produces a better search direction\u201d and \u201ctreating the [secant] condition less strictly might be helpful when [the Hessian] approximation is poor\u201d. This explanation seems insufficient to me to explain why relaxing these conditions via a regularized least-squares approach would yield a better algorithm, particularly in the noisy or stochastic setting. The lack of symmetry seems particularly strange; one would expect the true Hessian in the stochastic setting to still be symmetric, and one would still expect the secant condition to hold if the \u201ctrue\u201d gradients were accessible. It is also unclear how this approach takes advantage of the stochastic structure that exists within the problem.\n\nAdditionally, the quasi-Newton matrix is defined based on the solution of a regularized least squares problem with a regularization parameter lambda. It seems to me that the key to the approximation is the balance between the two terms in the objective. How is lambda chosen? What is the effect of lambda as a tuned parameter, and how does it affect the quality of the Hessian approximation? It is unclear to me how this could be chosen in a more systematic way.\n\nThe matrix also does not ensure positive definiteness, hence requiring a multiple of the gradient direction to be added to the search direction. In this case, the key parameter beta must be chosen carefully. What is a typical value of beta that is used for each of these problems? One would hope that beta is small, but if it is large, it may suggest that the search direction is primarily dominated by the stochastic gradient direction and hence the quasi-Newton matrix is not useful. The interplay of these different parameters needs to be investigated carefully.\n\nLastly, since (L-)BFGS use a weighted Frobenius norm, I am curious why the authors decided to use a non-weighted Frobenius norm to define the matrix. How does changing the norm affect the Hessian approximation?\n\nAll of these questions place the onus on the numerical experiments to see if these relaxations will ultimately yield a better algorithm.\n\n3. Numerical Experiments\n\nAs written, although the range of problems is broad and the numerical experiments show much promise, I do not believe that I could replicate the experiments conducted in the paper. In particular, how is SVRG and L-BFGS tuned? How is the steplength chosen? What (initial) batch sizes are used? Is the progressive batching mechanism used? (If the progressive batching mechanism is not used, then the authors should refer to the original multi-batch paper by Berahas, et al. [1] which do not increase the batch size and use a constant steplength.)\n\nIn addition, a more fair comparison would include the stochastic quasi-Newton method in [2] that also utilize diminishing steplengths, which use Hessian-vector products in place of gradient differences. Multi-batch L-BFGS will only converge if the batch size is increased or the steplength diminished, and it\u2019s not clear if either of these are done in the paper.\n\nTypos/Grammatical Errors:\n- Pg. 1: Commas are needed in some sentences, i.e. \u201cFirstly, for large scale problems, it is\u2026\u201d; \u201c\u2026compute the cost function and its gradients, the result is\u2026\u201d\n- Pg. 2: \u201cInterestingly, most SG algorithms\u2026\u201d\n- Pg 3: Remove \u201cat least a\u201d in second line\n- Pg. 3: suboptimal, not sup-optimal\n- Pg. 3: \u201cSuch a solution\u201d, not \u201cSuch at solution\u201d\n- Pg. 3: Capitalize Lemma\n- Pg. 4: fulfillment, not fulfilment\n- Pg. 7: Capitalize Lemma\n- Pg. 11: Before (42), Cov \\hat{g} = \\sigma_g^2 I\n- Pg. 11: Capitalize Lemma\n\nSummary:\n\nIn summary, although the ideas appear to provide better numerical performance, it is difficult to evaluate if the ideas proposed in this paper actually yield a better algorithm. Many algorithmic details are left unanswered, and the paper lacks mathematical or empirical evidence to support their claims. More experimental and theoretical work is needed before the manuscript can be considered for publication.\n\nReferences:\n[1] Berahas, Albert S., Jorge Nocedal, and Martin Tak\u00e1c. \"A multi-batch l-bfgs method for machine learning.\"\u00a0Advances in Neural Information Processing Systems. 2016.\n[2] Byrd, Richard H., et al. \"A stochastic quasi-Newton method for large-scale optimization.\"\u00a0SIAM Journal on Optimization\u00a026.2 (2016): 1008-1031.\n[3] Schraudolph, Nicol N., Jin Yu, and Simon G\u00fcnter. \"A stochastic quasi-Newton method for online convex optimization.\"\u00a0Artificial Intelligence and Statistics. 2007.", "title": "A good attempt, but lacks sufficient explanation and reasoning", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Syla5N-qAm": {"type": "rebuttal", "replyto": "HyeoUMuUAm", "comment": "(1) Response: We are indeed using different samples without any consistency enforcement, so any pair of function evaluations can be considered independent. It is correct that the line search may fail in the sense you have in view, and frequent failure would certainly slow down the progress of the algorithm. However, we didn\u2019t find this to be an issue in practice, and hence we have not addressed it further. Importantly, when the steplength is small, then we are effectively comparing two random numbers (the function evaluations) that are independently chosen and that have very similar mean values (since the function arguments are almost identical), which implies - with high likelihood - that we will satisfy the Armijo-type condition.\n\nRegarding the second point, it is true that the optimisation becomes harder when the noise level is large in relation to the function values, which is a natural challenge in the stochastic setting. Although demonstrated on Deep Learning problems, our algorithm is not specifically designed for that field but rather for stochastic functions in general. Therefore we have not considered any customised sampling techniques. \n\n(2) Response: Thank you for pointing this out, we should include this clarification in the paper. You are absolutely right here: in the setting we present, $\\bar{c}$ equals 0 whenever $\\|g_k\\|$ does, e.g. at local minimas. We have not investigated any guarantees of the kind you suggest. Instead we note that the choice of c was not found to cause problems in practice.\n\n(3-4) Response: What the theory does guarantee is that there is a non-zero step length such that a decrease is ensured in expectation. This is indeed not a strong result. However, it is not obvious how to strengthen the result without adding unrealistic assumptions. At the same time it is correct that the line search is designed to diminish the step length if the Armijo condition keeps failing; at most, the reduction continues down to machine epsilon precision. At the same time, when the steplength is small, then we are effectively comparing two random numbers that are independently chosen and that have very similar mean values, which implies - with high likelihood - that we will satisfy the Armijo-type condition before the steplength reaches machine epsilon.\n\n(11-12) Response: Thank you for this fair point, we are happy to make this clarification. We are using the mechanisms of Bollapragada that concerns the computation of the search direction, while for the remaining parts the proposed method is being employed. That is, when comparing the LMLS approach to L-BFGS, we are only employing the search direction part of Bollapragada while the line search procedure is common to both (including the ultimate O(1/k) steplength choice). Importantly, the L-BFGS approach in their paper requires that there is consistency between consecutive samples when forming the search direction and we have ensured that this is satisfied for the L-BFGS approach. In fact, it was discovered that this is essential for the method in Bollapragada to work in practice, while it is not required for the proposed LMLS approach. \n\nIn terms of SVRG, we chose the snapshot length to equal the batch size for LMLS and L-BFGS approaches. We also chose the steplength according to the following Matlab code (essentially this is equal to 4.0 / (max_i || X(:,i) ||^2) )\n\nOPT.stepSize     = sum(p.X(:,1).^2);\nfor i=2:size(p.X,2)\n    tmp = sum(p.X(:,i).^2);\n    if tmp > OPT.stepSize\n        OPT.stepSize = tmp;\n    end\nend\nOPT.stepSize = 1/(0.25*OPT.stepSize);\n\nThese choices provided the most consistent performance across all considered problems in the paper.\n ", "title": "Response to clarifying questions"}, "HJli9j4HRQ": {"type": "rebuttal", "replyto": "rklAzwpVhm", "comment": "\nStochastic Line Search\n(1)\nIt is correct that the function is repeatedly reevaluated in the line search, which is important for the method to work. This does not conflict with our claim that the Armijo condition holds in expectation and the associated proof. It is not entirely clear what \u2018failure\u2019 refers to, but we suppose it is in the sense of reaching the backtracking limit without having satisfied the Armijo condition; although we haven\u2019t collected any statistics on how many times this situation occurred, it was clearly nothing that caused any problems. Regarding the cost, it is linearly proportional to the number of function evaluations as well as to the size of the batch. \n\n(2)\nThe important insight is that there exists a value of c that, if chosen sufficiently small, ensures the Armijo condition to hold in expectation. In practise, we fixed c to 10^-6. This was found to work well and we did not see any need of modifying this value to improve the algorithm\u2019s performance.\n\n(3)\nYes, if the Armijo condition is not satisfied when the limit is reached, the step is still taken (this is akin to either a fixed or reducing learning rate in standard SGD methods). We motivate this by arguing that the backtracking loop does not contribute as much when the step length is small, and hence it is reasonable to a provide a limit on its reduction. The intention of the last paragraph in Section 3 is to clarify these details.\n\n(4)\nSee comment (4) in the response to Reviewer 2.\n\n(5)\nWe believe that there has been an misconception by the reviewer here. We do not make the claim $E[\\hat{p}_k^T \\hat{g}_k] = E[\\hat{p}_k]^T g_k$ anywhere in the paper, and it is not a property that is being utilised. The claim we make is that a descent direction in expectation can be ensured by adding a multiple of the negative gradient estimate to the search direction estimate. That ensures that the scalar product between the stochastic gradient and the search direction is negative in expectation; this is equivalent to the angle between the negative stochastic gradient direction and search direction to be acute in expectation.\n\n(6)\nOnce again, the reviewer is referring to a statement that is not in the paper, and therefore we can not respond to the comment. \n\n(7)\nThank you for this suggestion, we agree that this would remove the risk of confusion. \n\nDesign of Quasi-Newton Matrix\n(8)\nRegarding symmetry, see comment (1) in the response to Reviewer 2.\n\nThe secant condition is an approximation that treats the Hessian approximation as constant between two subsequent iterations. In contrast to the claim, we can not expect this condition to hold even in the deterministic case, and hence it enforces a property that is certainly not valid for the true Hessian. \n\n(9)\nSee comment (2) in the response to Reviewer 2.\n\n(10)\nIn the implementation we used a pragmatic approach mimicking the deterministic scenario, and added 2 * p^T*g / g^T*g whenever p^T*g was non-negative. It should be mentioned that this situation occurred very infrequently (less than 10 times in total across all Monte Carlo experiments) and hence we saw no need to further investigate this particular parameter choice. \n\nNumerical Experiments\n(11)\nIt is our clear intention to make the source code freely available to provide transparency and facilitate replication of the experiments. We have made a sincere effort in maximising the performance of the compared methods. Along with other parameter values, the batch sizes chosen are tabled in the Appendix. Regarding progressive batching, we used the method from Bollapragada 2018, but with a fixed batch size. The reason for this is that many other BFGS algorithms regularly failed to perform well, and in the interests of providing as fair a comparison as possible, we settled on the L-BFGS algorithm in Bollapragada 2018 with a fixed batch size. \n\n(12)\nWe find it interesting that the machine learning community does not appear to have agreed on a benchmark quasi-Newton method that is effective across a wide range of problems. Indeed, the method in [2] and in many others make comparisons against a subset of alternative methods, but not across all possible quasi-Newton methods. Further, there is a sincere lack of software for second-order methods within the community. Therefore, in light of the rather strong comments from the reviewer in this regard, we are uncertain if any comparison will ever satisfy all reviewers. Our solution to this in the current manuscript was to implement what we considered to be some state-of-the-art methods against the proposed method. ", "title": "Response to Reviewer 1"}, "H1gArsVH0Q": {"type": "rebuttal", "replyto": "B1lbltnL3X", "comment": "\n(1)\n--Reviewer Comment: \"The resulting Hessian inverse approximation in (14) is no longer symmetric, or guaranteed to be positive definite. While the underling true Hessian might not be positive definite because of the nonconvexity, it is always symmetric. Is it possible to impose symmetricity as a constraint in (12)?\"\n\nResponse: In terms of symmetry, while we understand that the true Hessian is symmetric by definition, it is not clear what benefit (if any) is gained by enforcing this constraint on a Hessian approximation. Indeed, the search direction is obtained by computing p = -Hg, and it is not obvious that symmetry is required to ensure that p is \"better\". At the same time, we also appreciate the long and successful history that quasi-Newton methods enjoy, and therefore we were reticent to make strong statements about symmetry.\n\n(2)\n--Reviewer Comment: \"What is the correct way to choose regularization parameter \u03bb in (14)?\"\n\nResponse: This is a great question that is not addressed in the paper. In fact, in the paper we have treated this as a \"tuning\" parameter and left choosing its optimal value as future work. How this can be done in a principled manner is not obvious, and similar tuning parameters are present in most optimisation algorithms. On a higher ground, this relates to the question of how to design an (appropriate) prior distribution in Bayesian methods, to which no precise answer exists.\n\n(3)\n--Reviewer Comment: \"The assumption that the covariance of gradient estimator is a constant multiple of identity is a strong and unrealistic assumption, which is never satisfied in machine learning.\"\n\nResponse: We completely agree with this comment. In fact, the assumption was made without further consideration, and the proofs would not suffer from a generalisation. This will certainly be updated in a revised version of the paper.\n\n(4)\n--Reviewer Comment: \"The algorithm performs a backtracking linesearch, with the initial trial stepsize decreaing as O(1/k), which means that the stepsize used is always decreasing at least as fast as O(1/k). This is in general in stark contrast with the intuition that a O(1) stepsize should be used for a quasi-Newton method. \"\n\nResponse: The algorithm only starts to reduce the step-length after $\\xi$ iterations (so the maximum step-length is actually chosen as min(1, \\xi / k) ). This means that in the initial phases the step-length could be O(1), but may also be quite small if required to satisfy the line-search condition. On the other hand, after $\\xi$ iterations, the step-size begins to reduce at O(1/k), which is primarily to ensure an eventual point-estimate.\n\n(5)\n--Reviewer Comment: \"Satisfying the Armijo condition in expectation does not lead to any useful convergence guarantee\"\n\nResponse: We agree that this is not a strong result. At the same time, it is not immediately obvious how to strengthen it without adding unrealistic assumptions.\n\n(6)\n--Reviewer Comment: \"The paper also presented some numerical experiments. While the numerical results look promising, I would appreciate some clarification about what method they are really comparing against. For example, for LBFGS the authors cite R. Bollapragada et al. \u201cA progressive batching L-BFGS method for machine learning\u201d. Is the paper comparing against progressive batching L-BFGS? The results of LBFGS here seem to be very different from the paper cited.\"\n\nResponse: This is a very good observation. In fact, we are employing the L-BFGS method from R. Bollapragada et al. , but with fixed batch size (the same size used for the proposed method) so that the timing comparison is fair. Out of the half-dozen or so BFGS algorithms that were coded and trialled, this gave the most consistent performance. All other BFGS methods failed on certain problems.\n\n(7)\n--Reviewer Comment: \"Finally, the paper could certainly benefit by making some mathematical statement more rigorous. For example, Lemma 1 and 2 are stated in expectation; however, since the algorithm is a stochastic algorithm, the whole sequence {x_k} generated is a stochastic process, and the expectation in the lemmas are conditional expectations. It is important to clarify w.r.t. what the conditional expectation is taken.\"\n\nResponse: Thank you for this comment. We agree that some statements could be made clearer in the manuscript. However, we have taken inspiration from Bottou et al [1] where convergence is proven in expectation and the Markov property (which also applies here) is relied on. \n\nReference\n[1] L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM Review, 60(2):223-311, 2018. ", "title": "Response to Reviewer 2"}, "S1l41oVBRQ": {"type": "rebuttal", "replyto": "BkgBV1acnm", "comment": "\n(1)\n--Reviewer Comment: \"The resulting algorithm is interesting, but it is not clear from the paper what the claimed advantage of doing this is. The LBFGS and SR1 unrolled update rules for H (Hessian inverse approximation) is O(m^2 d) (Sec 7.2 NW 2006), \u00a0and this seems to be the same for the authors' method, where the main matrix R_k that forms H has the same order. (BTW, did you mean 'd' in place of 'n' the computational order discussion preceding Sec 4.2?)\"\n\nResponse: Yes, we meant \"d\" and thank you for spotting this typo. You are quite correct that the computational load is similar between these two approaches.\n\n(2)\n--Reviewer Comment: \"The experiments show that this method's performance is impressive compared to an LBFGS implementation provided by Bollapragada 2018 , but as I recall that paper presented a variable/increasing batch method, while the authors' method uses fixed batches (as far as I can tell) so it is not clear that comparison on time alone is sufficient. The advantage over LBFGS and SGD seen in MNIST seems to go away by the CIFAR example, so it is unclear what might happen in larger problems like ImageNet.\"\n\nResponse: Thank you for the nice appraisal. In fact, we used the \u00a0method from Bollapragada 2018, but with a fixed batch size. The reason for this is that many other BFGS algorithms regularly failed to perform well, and in the interests of providing as fair a comparison as possible, we settled on the L-BFGS algorithm in Bollapragada 2018 with a fixed batch size. \n\n(3)\n--Reviewer Comment: \"I am also not able to see the difference between the 'stochastic' line search presented here and the standard backtracking method as applied to mini-batch evaluated estimates. What is different, and new that brings in consideration for the noise? I recall that bollapragada 2018 had an additional variance based rule to check. Some more conservative values are chosen for the step length, but I do not see the justification presented in the appendix, esp Eq 47 : p is not independent from g here, being calculated as p=Hg, \u00a0so E[p^t g] is not equal to the product of the individual expectations.\"\n\nResponse: There is a typo present in Eq 44-45, where p should be replaced by the estimate \\hat{p}_z. However, we still believe the conclusion in Eq 47 is correct, by making the same note as after Eq 37: the randomness in \\hat{p}_z and \\hat{g}_z\u2019 stem from different sources since \\hat{p}_z must be computed before the Taylor expansion is made. Hence, the gradient estimate used to compute \\hat{p}_z is not the same as the one that is used in the Taylor expansion. \u00a0\n\n(4)\n--Reviewer Comment: \"Some key points were left out in the discussion of the experiments. This is a common slip up when writing conf papers these days, but please do consider discussing the settings of parameters like mini-batches sizes , value of \\lambda in the H derivation, how one calculates the \\sigma^2_g within the algorithm presented in the Appendix. The last must include\nan extra computational cost, or are you using Adam style online variance estimator?\"\n\nResponse: We agree with this comment and can easily accommodate and appendix section to elaborate on these parameter value and calculations.\n\n(5)\n--Reviewer Comment: \"The MSE error alone seems insufficient in the results. Please publish the test mis-classification results too. Also, why is the MSE loss used with the softmax in CIFAR? Shouldn't cross-entropy, better justified theoretically, be better justified?\"\n\nResponse: We again agree with this comment and we will report mis-classification results also in the revised manuscript. MSE was a typo in the CIFAR case, it should be cross-entropy.", "title": "Response to Reviewer 3"}, "HyeT9q4BCX": {"type": "rebuttal", "replyto": "Hkg1csA5Y7", "comment": "We would like to thank all reviewers for their time and careful consideration of the paper. \n\nTwo reviewers largely agree that the manuscript proposes an interesting solution to an important problem. In contrast, a third reviewer has been very negative in their comments in areas where the the others have been more positive.\n\nBelow we address critical reviewer concerns and treat all remaining comments as positive.", "title": "Response introduction"}, "BkgBV1acnm": {"type": "review", "replyto": "Hkg1csA5Y7", "review": "The authors present an interesting variation of the standard QN methods. Their main point of departure from LBFGS/SR1 is in constructing a simpler Hessian inverse approximation. Recall that SR1 and LBFGS updates all satisfy the secant equation for each of the `m` previous gradient differences stored in memory. The authors choose to get \"close\" to satisfying the equations by solving an l_2 penalization of the secant equations. \n\nThe resulting algorithm is interesting, but it is not clear from the paper what the claimed advantage of doing this is. The LBFGS and SR1 unrolled update rules for H (Hessian inverse approximation) is O(m^2 d) (Sec 7.2 NW 2006),  and this seems to be the same for the authors' method, where the main matrix R_k that forms H has the same order. (BTW, did you mean 'd' in place of 'n'  the computational order discussion preceding Sec  4.2?)\n\nThe experiments show that this method's performance is impressive compared to an LBFGS implementation provided by Bollapragada 2018 , but as I recall that paper presented a variable/increasing batch method, while the authors' method uses fixed batches (as far as I can tell) so it is not clear that comparison on time alone is sufficient. The advantage over LBFGS and SGD seen in MNIST seems to go away by the CIFAR example, so it is unclear what might happen in larger problems like ImageNet. \n\nI am also not able to see the difference between the 'stochastic' line search presented here and the standard backtracking method as applied to mini-batch evaluated estimates. What is different, and new that brings in consideration for the noise? I recall that bollapragada 2018 had an additional variance based rule to check. Some more conservative values are chosen for the step length, but I do not see the justification presented in the appendix, esp Eq 47 : p is not independent from g here, being calucated as p=Hg,  so E[p^t g] is not equal to the product of  the individual expectations.\n\nSome key points were left out in the discussion of the experiments. This is a common slip up when writing conf papers these days, but please do consider discussing the settings of parameters like mini-batches sizes , value of \\lambda in the H derivation, how one calculates the \\sigma^2_g within the algorithm presented in the Appendix. The last must include\nan extra computational cost, or are you using Adam style online variance estimator?\n\nThe MSE error alone seems insufficient in the results. Please publish the test mis-classification results too. Also, why is the MSE loss used with the softmax in CIFAR? Shouldn't cross-entropy, better justified theoretically, be better justified?\n", "title": "interesting reformulation of Quasi Newton direction", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "B1lbltnL3X": {"type": "review", "replyto": "Hkg1csA5Y7", "review": "This paper presents a new quasi-Newton type method for stochastic optimization problems. The primary contributions of the paper include a new stochastic linesearch method as well as a novel way to incorporate second order information which is different from existing approaches such as BFGS or L-BFGS. \n\nIn terms of the clarity, I think this is a very well-written paper with nice organization. The paper does have some typos, though. \n\nIn terms of significance, how to incorporate second-order information in stochastic optimization has long been an important research topic. Most existing stochastic quasi-Newton methods use L-BFGS method to incorporate second order information and choose a fixed, small stepsize, with the only differences being how to compute the curvature pair (s_k, y_k). Therefore, this paper is addressing a very important question and has made respectable attempt to use mechanisms other than L-BFGS method and to incorporate a linesearch scheme. \n\nSpecifically, the paper relaxes the secant equation, which is natural for the stochastic settings because the difference in gradients y_k is computed from stochastic gradients, and the true Hessian only satisfies the secant equation in expectation. I believe replacing the secant equation is an important and promising direction.\n\nHowever, there are concerns about the new approach proposed in this paper:\n\n1.\tThe resulting Hessian inverse approximation in (14) is no longer symmetric, or guaranteed to be positive definite. While the underling true Hessian might not be positive definite because of the nonconvexity, it is always symmetric. Is it possible to impose symmetricity as a constraint in (12)?\n\n2.\tWhat is the correct way to choose regularization parameter \u03bb in (14)?\n\nThe paper also proposes a stochastic linesearch algorithm. For this part, there are several concerns as well:\n\n1.\tThe assumption that the covariance of gradient estimator is a constant multiple of identity is a strong and unrealistic assumption, which is never satisfied in machine learning. \n\n2.\tThe algorithm performs a backtracking linesearch, with the initial trial stepsize decreaing as O(1/k), which means that the stepsize used is always decreasing at least as fast as O(1/k). This is in general in stark contrast with the intuition that a O(1) stepsize should be used for a quasi-Newton method. \n\n3.\tSatisfying the Armijo condition in expectation does not lead to any useful convergence guarantee. \n\nThe paper also presented some numerical experiments. While the numerical results look promising, I would appreciate some clarification about what method they are really comparing against. For example, for LBFGS the authors cite R. Bollapragada et al. \u201cA progressive batching L-BFGS method for machine learning\u201d. Is the paper comparing against progressive batching L-BFGS? The results of LBFGS here seem to be very different from the paper cited. \n\nFinally, the paper could certainly benefit by making some mathematical statement more rigorous. For example, Lemma 1 and 2 are stated in expectation; however, since the algorithm is a stochastic algorithm, the whole sequence {x_k} generated is a stochastic process, and the expectation in the lemmas are conditional expectations. It is important to clarify w.r.t. what the conditional expectation is taken.\n\nIn summary, I believe that this paper has made a novel contribution. However, the author should address the concerns above.", "title": "Novel idea, but not without issues to be addressed", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}