{"paper": {"title": "Forecasting Deep Learning Dynamics with Applications to Hyperparameter Tuning", "authors": ["Piotr Kozakowski", "\u0141ukasz Kaiser", "Afroz Mohiuddin"], "authorids": ["p.kozakowski@mimuw.edu.pl", "lukaszkaiser@google.com", "afrozm@google.com"], "summary": "", "abstract": "Well-performing deep learning models have enormous impact, but getting them\nto perform well is complicated, as the model architecture must be chosen and a\nnumber of hyperparameters tuned. This requires experimentation, which is timeconsuming and costly. We propose to address the problem of hyperparameter\ntuning by learning to forecast the training behaviour of deep learning architectures.\nConcretely, we introduce a forecasting model that, given a hyperparameter schedule\n(e.g., learning rate, weight decay) and a history of training observations (such as\nloss and accuracy), predicts how the training will continue. Naturally, forecasting\nis much faster and less expensive than running actual deep learning experiments.\nThe main question we study is whether the forecasting model is good enough to be\nof use - can it indeed replace real experiments? We answer this affirmatively in two\nways. For one, we show that the forecasted curves are close to real ones. On the\npractical side, we apply our forecaster to learn hyperparameter tuning policies. We\nexperiment on a version of ResNet on CIFAR10 and on Transformer in a language\nmodeling task. The policies learned using our forecaster match or exceed the ones\nlearned in real experiments and in one case even the default schedules discovered\nby researchers. We study the learning rate schedules created using the forecaster\nare find that they are not only effective, but also lead to interesting insights.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper trains a transformer to extrapolate learning curves, and uses this in a model-based RL framework to automatically tune hyperparameters. This might be a good approach, but it's hard to know because the experiments don't include direct comparisons against existing hyperparameter optimization/adaptation techniques (either the ones based on extrapolating training curves, or standard ones like BayesOpt or PBT). The presentation is also fairly informal, and it's not clear if a reader would be able to reproduce the results. Overall, I think there's significant cleanup and additional experiments needed before publication in ICLR.\n"}, "review": {"rJla3R_jjr": {"type": "rebuttal", "replyto": "HkgABKrXir", "comment": "Thank you for mentioning this connection. We have added it to the updated version of our work.", "title": "Response to the comment"}, "rkgeDR_ojB": {"type": "rebuttal", "replyto": "SyeR04XiuS", "comment": "Thank you for the insightful review.\n\nWe updated the paper with better results and more tasks. We show that our method outperforms the human baseline in terms of training speed and either matches or outperforms the human in terms of final accuracy on all tasks. While it is true that the human baseline does not require any additional computational resources for training, it does require domain expertise acquired through years of learning, which is arguably even more costly. Notably, in all 4 problems where we compare to the human baseline, we believe that human researchers used a similar or higher number of runs as our tuner to design the baseline schedules that we compare against.\n\nWe also updated the paper with more details regarding Transformer and Proximal Policy Optimization.\n\nThank you for mentioning the existing learning curve modeling methods. We added an explanation of differences of our method with those works. [1] learn a probabilistic model of one training curve using a handcrafted basis of nonlinear functions of shapes similar to the training curves being modelled. Our method does not make any assumptions about the shape of the modelled curves and is able to jointly model many training curves - in our experiments, training and validation loss and accuracy. [2] learn a deterministic model of a learning curve, while our method also models stochasticity, hence providing diverse experience for training a reinforcement learning agent. Also in contrast to [1] and [2], our method allows the hyperparameters to change over the course of training and models the influence of those changes on the training metrics.\n\n[1] Baker, Bowen, et al. \"Accelerating neural architecture search using performance prediction.\" arXiv preprint arXiv:1705.10823 (2017).\n\n[2] Domhan, Tobias, Jost Tobias Springenberg, and Frank Hutter. \"Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves.\" Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015.", "title": "Response to the reviewer's comments"}, "HJlRC3djiB": {"type": "rebuttal", "replyto": "Hyxs5tB0FS", "comment": "We thank the reviewer for their comprehensive review.\n\nWe updated the paper with better results over more tasks, either matching or outperforming the human baseline in terms of final accuracy, and outperforming the model-free baseline in all cases. We also included results over multiple runs of all experiments, showing the minimum, maximum and mean accuracy.\n\n1. While it is true that the manually-tuned baseline we provided is simple, it is a standard practice in the field to adjust the learning rate during training and keep the rest of the hyperparameters constant. Adjusting all of them requires significantly more effort and is infeasible in many cases.\n\n2. Due to time constraints, we have not benchmarked our method against more hyperparameter-tuning baselines yet. We agree that it would be a very valuable comparison and leave that for future work. Nevertheless, please note that the human baselines we use for Transformer have been tuned by researchers using auto-tuners among other tools.\n\n3. [1] successfully use PPO with an LSTM policy on a challenging, partially-observable environment. It is equally principled to use a Transformer policy, since both would operate on the same sequence of observations. The SimPLe algorithm runs PPO on an MDP approximated by a powerful model that handles stochasticity well, which is also a valid approach.\n\n4. We updated the paper with a justification of our action discretization scheme. Such a discretization has a number of benefits, including multi-modality, which cannot be achieved using a parameterized Gaussian policy. [2] show that discretization of the action space improves the average performance, stability and robustness to hyperparameters of reinforcement learning agents on a range of continuous control tasks.\n\n5. While we have not included such transfer experiments in our current work, we do believe that a model trained on enough architectures and tasks will generalize to new ones. For instance, in the updated version of the paper, we show that the learned policy employs similar learning rate and weight decay rate adjustment schemes across very different tasks. Substantiating this claim in the general case will likely require a large-scale study, which we plan to perform in the future.\n\n[1] OpenAI et al. \u201cLearning Dexterous In-Hand Manipulation\u201d, arXiv preprint arXiv:1808.00177 (2018)\n\n[2] Tang et al. \u201cDiscretizing Continuous Action Space for On-Policy Optimization\u201d, arXiv preprint 1901.10500 (2019)", "title": "Response to the reviewer's comments"}, "ByghS3dssB": {"type": "rebuttal", "replyto": "rye5LcLAYH", "comment": "We thank the reviewer for the effort, however we believe there is a mis-understanding.\n\nAs for the synthetic curves experiment, we updated the paper with a justification. This task, while simple, showcases the ability of Transformer to model a distribution over curves of similar shape to real training curves with varying speeds of convergence. It has been designed so it is easy to quantify the diversity of generated curves and the fit between the distribution generated by the model and the real one. Furthermore, we included two additional tasks, attesting to the ability of Transformer to model a wide range of distributions over training curves. We also updated the citation of the paper you mentioned with an arxiv URL.\n\nWe still believe that while focusing on the synthetic task the reviewer might have missed the main point of the paper, namely that time-series forecasting with Transformer works really well, at least in the context of modeling deep learning dynamics. The general problem has been studied in the community for many decades and we believe that we made significant progress, so we kindly encourage the reviewer to reconsider their assessment of our contributions.", "title": "Response to the reviewer's comments"}, "SyeR04XiuS": {"type": "review", "replyto": "ByxHJeBYDB", "review": "This work focuses on learning a good policy for hyperparameters schedulers, for example learning rate or weight decay, using reinforcement learning. The main contributions include 1) a discretization on the learning curves such that transformer can be applied to predict the them; 2) an empirical evaluations using the predicted learning curves to train the policy. \n\nThe main novelties are two folds. On the methodology side, using predicted learning curves instead of real ones can speed up training significantly. On the technical side, the author presented a discretization step to use transformer for learning curve predictions. The results are mixed, we see slightly advantage over human baseline on one task but worse in the other. Human baseline does not need any training! On the writing part, it would be nice to provide more context for both transformer, Proximal Policy Optimization and Simulated Policy Learning to make the paper more self-complete.\n\nI like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. There are already some works, not using deep learning method, for example the following:\n\n* Baker, Bowen, et al. \"Accelerating neural architecture search using performance prediction.\" arXiv preprint arXiv:1705.10823 (2017).\n* Domhan, Tobias, Jost Tobias Springenberg, and Frank Hutter. \"Speeding up automatic hyperparameter optimization of deep neural networks by extrapolation of learning curves.\" Twenty-Fourth International Joint Conference on Artificial Intelligence. 2015.\n\nWhy these methods are not considered in the beginning? In my opinion, transformer is good for modeling long term dependency and concurrent predictions which is not necessarily the case for learning curves. How does the transformer based method comparing to others? ", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 2}, "Hyxs5tB0FS": {"type": "review", "replyto": "ByxHJeBYDB", "review": "The paper investigates the possibility of learning a model to predict the training behaviour of deep learning architectures from hyperparameter information and a history of training observations. The model can then be used by researchers or a reinforcement learning agent to make better hyperparameter choices. The paper first adapts the Transformer model to be suitable to this prediction task by introducing a discretization scheme that prevents the transformer decoder's predictions from collapsing to a single curve. Next, the problem is formalized as a partially-observable MDP with a discrete action set, and PPO and SimPLe are introduced. The proposed model-based method is compared against a human and a model-free baseline training a Wide ResNet on CIFAR-10. The model-based method achieves better validation error than the other baselines that use actual data. Next, the method is compared against a human and a model-free baseline training Transformer models on the Penn Treebank dataset. While the human achieves the best performance at the end of the run, the proposed method appears to learn more quickly than the others and finishes with performance comparable to the model-free baseline.\n\nCurrently I lean towards accepting this paper for publication, despite a few issues. It asks an interesting question: can we learn a model of the training dynamics to avoid actually having to do the training? This could potentially prevent a lot of unnecessary computation and also lead to better-performing models. It then shows some experimental evidence suggesting that this is possible.\n\nMost importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it's impossible to assess whether the results are likely to be significant or not. Other questions:\n1. In the PTB experiment, it looks like the human only adapts the learning rate and leaves the rest of the hyperparameters alone. Why was this policy used as the baseline? It seems extremely basic and unlikely to truly lead to optimal performance.\n2. Why were more baselines from the related work not included? I understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do.\n3. How do PPO and SimPLe handle partial observability? Is it principled to apply them to partially-observable environments?\n4. Why not use continuous actions with a parameterized policy (e.g. Gaussian)?\n5. Is it reasonable to assume that the learning dynamics of all deep learning architectures are similar enough that a model trained on one set of deep learning architectures and problems will generalize to new architectures and problems?", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 1}, "rye5LcLAYH": {"type": "review", "replyto": "ByxHJeBYDB", "review": "This paper proposed to train a network with training curves and corresponding parameters, and use policy search to find optimal parameter to replace hundreds or thousands of training in real case scenario, and it is clearly much faster using the trained network to infer parameters, instead of tuning the network manually.\n\nThe first point would be: what's the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? Most practical problems would not have the same distribution as the previously gathered public dataset, thus the data is not representative, and synthetic training curves just does not make sence.\n\nThe cited paper 'Learning an adaptive learning rate schedule' does not appear online.\n\n", "title": "Official Blind Review #2", "rating": "1: Reject", "confidence": 1}}}