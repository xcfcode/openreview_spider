{"paper": {"title": "On the Turing Completeness of Modern Neural Network Architectures", "authors": ["Jorge P\u00e9rez", "Javier Marinkovi\u0107", "Pablo Barcel\u00f3"], "authorids": ["jperez@dcc.uchile.cl", "javier.marinkovic95@gmail.com", "pbarcelo@dcc.uchile.cl"], "summary": "We show that the Transformer architecture and the Neural GPU are Turing complete.", "abstract": "Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser & Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.", "keywords": ["Transformer", "NeuralGPU", "Turing completeness"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper provides a theoretical analysis of the Turing completeness of popular neural network architectures, specifically Neural Transformers and the Neural GPU. The reviewers agreed that this paper provides a meaningful theoretical contribution and should be accepted to the conference. Work of a theoretical nature is, amongst other types of work, called for by the ICLR CFP, but is not a very popular category for submissions, nor is it an easy one. As such, I am happy to follow the reviewers' recommendation and support this paper."}, "review": {"HkxVcSSeg4": {"type": "rebuttal", "replyto": "H1gfQ37Cy4", "comment": "Thanks for your comment. \n\nWe believe that your doubt has been already clarified by the authors of the paper mentioned in your comment (\"Universal Transformers\"), and we thank the authors for their response. We just want to emphasize that our results only hold when unbounded precision is admitted, which is a standard assumption in the theoretical analysis of the computational power of neural networks (see, e.g., the Universal  Approximation Theorem, or Turing Completeness for RNNs). As mentioned in the response provided by the authors of the Universal Transformers paper, when only bounded precision is allowed, then the model is no longer Turing complete. In fact, we formally prove in our paper that the latter holds even if one sees the Transformer as a seq-to-seq network that produces an arbitrary long output.  We will include some further comments about this in the final version of our paper.\n", "title": "On your comment"}, "BJl39ddR37": {"type": "review", "replyto": "HyGBdo0qFm", "review": "The paper shows Turing completeness of two modern neural architectures, the Transformer and the Neural GPU. The paper is technically very heavy and gives very little insight and intuition behind the results. Right after surveying the previous work the paper starts stacking definitions and theorems without much explanations.\n\nWhile technical results are potentially quite strong I believe a major revision to the paper might be necessary in order to clarify the ideas. I would even suggest to split the paper into two, one about each architecture as in the current form it is quite long and difficult to follow. \n\nResults are claimed to hold without access to external memory, relying just on the network itself to represent the intermediate results of the computation. I am a bit confused by this statement -- what if the problem at hand is, say EXPSPACE-complete? Then the network would have to be of exponential size (or more generally of arbitrary size which is independent of the input). In this case the claim about not using external memory seems to be kind of vacuous as the network itself has unbounded size. The whole point of Turing-completeness is that the program size is independent of the input size so there seems to be some confusion here.\n", "title": "Potentially interesting results, very dense and confusing writing", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "SkxNcv55nm": {"type": "review", "replyto": "HyGBdo0qFm", "review": "This paper presents interesting theoretical results on Turing completeness of the Transformer and Neural GPU architectures, as modern architectures based on attention and convolutions, under particular assumptions. The basis of proofs in the paper relies on Turing completeness of the seq2seq architecture, which is Turing complete since it contains Turing complete RNNs. Turing completeness of the Transformer and the Neural GPU is proven by showing they can simulate seq2seq architecture.\n\nThe Transformer, using additive hard attention and residual connections, is Turing complete in the case when positional encoding is used. Otherwise, if no positional encoding is used, the model is order-invariant which makes it not Turing complete.\n\nA version of the Neural GPU, dubbed Uniform Neural GPU is proven to be Turing complete. Moreover, the presented theoretical results are backed by a recent publication by Karlis and Liepins. Interestingly, Neural GPUs using circular convolutions are not Turing complete, while the ones using zero padding are.\n\nThe repercussion of the paper for similar architectures is the not just in the theoretical section but also in a set of discoveries of practical importance, like the importance of the use of residual connections, positional coding in Transformers, and zero padding in Neural GPUs.\n\nAlbeit the paper presents an original and significant theoretical progress and is well written, it is not fit for ICLR, primarily as the paper is impossible to review and verify without a thorough perusal and analysis of the appendix. Although the results and the proof sketches fit the body of the paper, the necessity of verifying proofs makes this paper 23 pages long and makes it a better fit for a journal and not a conference.", "title": "Better fit for a journal", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "SJedZxpFAX": {"type": "rebuttal", "replyto": "SyemDwc_AX", "comment": "We appreciate the comment. We empathize with your concern about the difficulty of checking long technical proofs in appendices, and in fact we often have to struggle with this ourselves as reviewers. Still, we decided to present our proofs in full detail so that they could be verified exhaustively by reviewers if needed. The only way we could do this was by presenting them as supplementary material. While we believe that in general the writing of the proof in the body of the paper and in the appendix is good and can be more or less easily followed, we will do our best to improve it further if the paper gets accepted.", "title": "We empathize with your concern"}, "SJgnoh3dp7": {"type": "rebuttal", "replyto": "Syx12inOam", "comment": "Responses to AnonReviewer1:\n\n\n** [comment] \u201cResults are claimed to hold without access to external memory [...] what if the problem at hand is, say EXPSPACE-complete? Then the network would have to be of exponential size [...] The whole point of Turing-completeness is that the program size is independent of the input size so there seems to be some confusion here.\u201d\n\n[response] As stated in the paper, Turing completeness for Transformer and Neural GPU is obtained by taking advantage of the internal representations used by both architectures. We prove that the Transformer and the Neural GPU can use the values in their internal activations to carry out the computations while having a network with a fixed number of neurons and connections. For the case of Neural GPUs we even restrict the architecture to ensure a fixed number of parameters (Uniform Neural GPUs). Thus our proof actually uses a \u201cprogram size which is independent of the input size\u201d as mentioned by the reviewer. The confusion might arise because of our assumption that internal representations are rational numbers with arbitrary precision; we are trading external memory by internal precision. This is a classical assumption in the study of the computational power of neural networks (e.g. Universal Approximation Theorem for FFNs and Turing Completeness for RNNs). We mention this property in the Introduction, in the Conclusions, and also when formally proving the results, but we will make it more explicit in the next version of the paper.\n\n** [comment] \u201cThe paper is technically very heavy [...] I believe a major revision to the paper might be necessary in order to clarify the ideas.\u201d\n\n[response] It is true that the paper is a bit dense, but we prove a technically involved result. To be precise in our claims we needed to include all the definitions in the paper. Moreover, our formal definitions can be used in the future to prove more properties for these and similar architectures with theoretical and practical implications. Though technical, the two other reviewers explicitly mention that the paper is well written. \n\n** [comment] \u201cThe paper [...] gives very little insight and intuition behind the results.\u201d\n\n[response] The main intuition in our results is that both architectures can effectively simulate an (Elman)RNN-seq2seq computation, which by Siegelmann and Sontag\u2019s classical result [1] are Turing complete when internal representations are rational numbers of arbitrary precision. We mentioned this in the Introduction and in each proof sketch, but we will make it more explicit in the next version of the paper.\n\n**[comment] \u201cI would even suggest to split the paper into two, one about each architecture\u201d.\n\n[response] We wanted to have both architectures in the paper as they are two of the most popular architectures in use today, yet based on different paradigms; namely, self-attention mechanisms and convolution. We wanted to understand to what extent the use of these features could be exploited in order to show Turing completeness for the models. Moreover, the computational power of Transformers has been compared with that of Neural GPUs in the current literature, but both are only informally used. We wanted to provide a formal way of approaching this comparison.\n\n[1] Siegelmann and Sontag. On the computational power of neural nets. JCSS-95\n", "title": "Responses to AnonReviewer1"}, "Hkg1123_pX": {"type": "rebuttal", "replyto": "Syx12inOam", "comment": "Responses to AnonReviewer3:\n\n** [comment] \u201cAlbeit the paper presents an original and significant theoretical progress and is well written, it is not fit for ICLR, primarily as the paper is impossible to review and verify without a thorough perusal and analysis of the appendix. Although the results and the proof sketches fit the body of the paper, the necessity of verifying proofs makes this paper 23 pages long and makes it a better fit for a journal and not a conference.\u201d\n\n[response] We included the appendices to allow the interested reader to see the techniques used in our theoretical proof and potentially extend it or apply it to other architectures, to understand the full implications of the results, and to validate the results for themselves. We see the proofs in our appendix more as a \u201ccompanion code to backup our findings\u201d as one usually do for an experimental paper, and we include it mostly for reproducibility purposes. As we stated in the general comments, although submitting to a journal is an option, we do want  to discuss the theoretical implications of our work face-to-face with people of the interested community without waiting for a long journal review process.\n", "title": "Responses to AnonReviewer3"}, "S1xAQn3dpX": {"type": "rebuttal", "replyto": "Syx12inOam", "comment": "Responses to AnonReviewer2:\n\n** [comment] \u201cof the simplifications and approximations used for the proof, how much does that take the model away from what is used in practice?\u201d \n\n[response] Most of our changes are actually simplifications, which means that models as used in practice can have even more space to simulate computations. Take for example the relationship between Uniform Neural GPUs that we use, and (regular) Neural GPUs that are used in practice. Uniform Neural GPUs have a number of parameters that cannot depend on the input size, while (regular) Neural GPUs have a number of parameters that depend linearly on the input size. Transformer on the other hand can use multiple heads per layer but we only use one head. For the case of the Transformer one difference is that we use additive attention in our proof while multiplicative attention is used in practice most of the time. A detailed comparison between both uses in terms of computational power is a good topic for future research.\n\n** [comment] \u201cFor example, the assumption of the piecewise linear sigmoid seems like a quite big change, as there are large regions of the space which now have zero gradients. If you run a real implementation of these models, with the normal sigmoid replaced by this one, does training still work? If not, what are the implications for the proof?\u201d\n\n[response] This is a really interesting question. For the case of the Neural GPU, as we mention in the paper, there is a recent work by Freivalds and Liepins [2] showing that piece-wise linear activations dramatically increase the training performance. These activations (along other changes) allowed the learning of decimal multiplication from examples which was impossible with the original Neural GPU [2]. Thus  having piecewise linear activations actually helps in practice. For the case of the Transformer more experimentation is needed to have a conclusive response. We will add some comments on this in the next version of the paper.\n\n** [comment] \u201c[...] all floating points on a computer represent rationals, but it would be interesting to get a better understanding on how the lack of infinite precision rationals on real hardware affects the main results.\u201d\n\n[response] This is similar to a comparison between a computer with bounded vs unbounded memory. With bounded memory a computer is, theoretically, just a finite state machine. Similarly, with rationals of bounded precision, a Transformer is computationally very weak. Actually, your question made us realize that from our results it follows that bounded precision Transformers cannot even simulate finite automaton (this is a corollary of Proposition 3.1 in our submission). We will add a discussion on this result since it will definitely improve the paper. Thank you for the comment.\n\n** [comment] \u201cDoes the proof rely on the input and output dimensionality being the same? Eg in the preliminaries, x_i and y_i are both d-dimensional - could this be changed?\u201d\n\n[response] The short answer is \u201cyes\u201d it can be changed, as one can always pad the shorter with zeroes as a trick to make them of the same dimension. But having both of the same dimension is more of a practical concern of the architectures we use. For the case of the Transformer, the fact that the decoder puts attention over the output of the encoder, plus the use of residual connections in every layer, forces dimensions to coincide. For the case of the Neural GPU, input vectors are transformed without changing their dimensions, thus input and output vectors have naturally the same size.\n\n** [comment] \u201cCircular convolution definition only appears to define the values directly adjacent to the border, would it be more appropriate to define S_{h+n, :, :} = S{n, :, :}?\u201d\n\n[response] Yes, you are right. We will include this change in the next version, thank you.\n\n\n[2] Freivalds and Liepins. Improving the Neural GPU Architecture for Algorithm Learning. NAMPI-18 (workshop at ICML-18)", "title": "Responses to AnonReviewer2"}, "Syx12inOam": {"type": "rebuttal", "replyto": "HyGBdo0qFm", "comment": "We thank the reviewers for their comments. We first make some general comments and then answer directly to each reviewer. \n\nAll reviewers appear to agree that our technical results on the Turing completeness of the Transformer and the Neural GPU are potentially interesting/important. In two reviews, there is however a general question about the fit of our results for ICLR. One reviewer advised to go directly to a journal. We did consider submitting to a journal or to a theoretical conference, but we felt it important to discuss the computational properties of the Transformer and the Neural GPU directly with the community involved in their design, implementation, and practical use. We felt that submitting to ICLR would generate more impact noting that the Neural GPU was initially proposed at ICLR2016, and the Transformer architecture (proposed at NIPS2017) is used in several ICLR2018 papers (and also now ICLR2019 submissions). \n\nWe also observe that there is a need for more theoretical foundations with regards to the computational power of modern NN architectures at ICLR, and in particular about the two architectures that we study. Consider for example the following ICLR2019 submission: \u201cUniversal Transformers\u201d (https://openreview.net/forum?id=HyzdRiR9Y7&noteId=HyzdRiR9Y7). Universal Transformers are networks that combine the parallelizability and ease of train of recently proposed feed-forward mechanisms based on self-attention, such as the Transformer, with the learning abilities of recurrent NNs. This is a strong paper, in our opinion, with a thorough experimental part and the potential for significant practical impact. Though it received three positive reviews, two reviewers would like to see a more thorough theoretical analysis of the proposed architecture (which is, admittedly, beyond the scope of the paper). One of the reviewers states \u201cI miss a proof that the Universal Transformer is computationally equivalent to a Turing machine.\u201d while the other states \u201cI am having trouble understanding the universal aspect of the transformer\u201d. Our paper brings light into this, by showing what are some of the minimal sets of features that make self-attention networks, in particular, the Transformer, Turing-complete. Moreover, in that paper, Neural GPUs are used as a yardstick to compare the computational power of the Transformer. Thus our paper presents a formal theoretical basis to address problems that are currently being discussed at ICLR. (We emphasize that we are not involved in any way with the \u201cUniversal Transformer\u201d paper, and that we are not reviewers of it.) \n\nBelow we provide detailed responses to each one of the individual reviews. \n", "title": "Rebuttal"}, "r1ldhR0Knm": {"type": "review", "replyto": "HyGBdo0qFm", "review": "This paper seeks to answer the question of whether models which process sequences, but are not strictly classical RNNs, are Turing complete.\n\nThe authors present proofs that both the Transformer and Neural GPU are turing complete, under certain conditions. I do not consider myself qualified to properly verify the proof but it seems to be presented clearly. The authors note that the conditions involved are not how these models are used in the real world. Given the complex construction required for this more theoretically based proof, it seems reasonable that this should be published now, rather than waiting until the further work discussed in the final section is completed.\n\nI have a number of questions where if a brief answer is possible, this would enhance the manuscript. The main question is, of the simplifications and approximations used for the proof, how much does that take the model away from what is used in practice? For example, the assumption of the piecewise linear sigmoid seems like a quite big change, as there are large regions of the space which now have zero gradients. If you run a real implementation of these models, with the normal sigmoid replaced by this one, does training still work? If not, what are the implications for the proof?\n\nThe rational numbers assumption is interesting - again I wonder how this would affect the model in reality, obviously all floating points on a computer represent rationals, but it would be interesting to get a better understanding on how the lack of infinite precision rationals on real hardware affects the main results.\n\nDoes the proof rely on the input and output dimensionality being the same? Eg in the preliminaries, x_i and y_i are both d-dimensional - could this be changed?\n\nOverall this paper is novel and interesting, I have to give a slightly low confidence score because I'm unfamiliar with a lot of the background here (eg the Siegelamnn & Sontag work). The paper does seem concise and well written.\n\ntypos and minor points:\n\nCircular convolution definition only appears to define the values directly adjacent to the border, would it be more appropriate to define S_{h+n, :, :} = S{n, :, :}?\n\nparagraph above equation 5, 'vectores' -> 'vectors'", "title": "Clear and well written, some questions about how these results map to training the real models.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "Syla8U8ZcQ": {"type": "rebuttal", "replyto": "rJgJkiz-9X", "comment": "Our proofs are based on having unbounded precision for internal representations (neuron values). For weights one can prove that fixed precision (actually very small) is enough.\n\nOur results say nothing about the computational power when fixed precision (like float32) is assumed for internal representations. We actually state the fixed-precision case as an interesting topic for future research.", "title": "We need unbounded precision for internal representations"}}}