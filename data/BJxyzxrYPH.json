{"paper": {"title": "Deep geometric matrix completion:  Are we doing it right?", "authors": ["Amit Boyarski", "Sanketh Vedula", "Alex Bronstein"], "authorids": ["amitboy@cs.technion.ac.il", "sanketh@cs.technion.ac.il", "bron@cs.technion.ac.il"], "summary": "A simple spectral geometric approach for matrix completion, based on the framework of functional maps.", "abstract": "We address the problem of reconstructing a matrix from a subset of its entries. Current methods, branded as geometric matrix completion, augment classical rank regularization techniques by incorporating geometric information into the solution. This information is usually provided as graphs encoding relations between rows/columns.\nIn this work we propose a simple spectral approach for solving the matrix completion problem, via the framework of functional maps. We introduce the zoomout loss, a multiresolution spectral geometric loss inspired by recent advances in shape correspondence, whose minimization leads to state-of-the-art results on various recommender systems datasets. Surprisingly, for some datasets we were able to achieve comparable results even without incorporating geometric information. This puts into question both the quality of such information and current methods' ability to use it in a meaningful and efficient way.", "keywords": ["Geometric Matrix Completion", "Spectral Graph Theory", "Functional Maps", "Deep Linear Networks"]}, "meta": {"decision": "Reject", "comment": "This paper proposes a multiresolution spectral geometric loss called the zoomout loss to help with matrix completion, and show state-of-the-art results on several recommendation benchmarks, although experiments also show that the result improvements are not always dependent upon the geometric loss itself.\nReviewers find the idea interesting and the results promising but also have important concerns about the experiments not establishing how the approach truly works. Authors have clarified their explanations in the revisions and provided requested experiments (e.g., on the importance of the initialization size), however important reservations re. why the approach works are still not sufficiently addressed, and would require more iterations to fulfill the potential of this paper.\nTherefore, we recommend rejection."}, "review": {"HJgg1pKRtB": {"type": "review", "replyto": "BJxyzxrYPH", "review": "This paper proposes a new method for geometric matrix completion based on functional maps. The proposed algorithm is a simple shallow and fully linear network. Experimental results demonstrate the effectiveness of the proposed method. \n\nThe proposed method is new and has been shown good empirical results. The paper also points out a new way to interpret matrix completion. On the other hand, the proposed method seems ad hoc and there is no clear evidence why it is better than other baselines except the empirical results. The paper also has some clearance issues, making it hard to understand. I vote for a weak reject of the paper at the current pace and would like to increase my score if the following questions can be clearly answered.\n\n1.\tWhy do we need to propose the algorithm? Is it because we have the functional maps technique motivated from shape correspondence, and we can see some connection of such technique with matric completion? If it is true, we surely can have a new algorithm based on such a new technique. But I can still not understand why the method work, at least, in an intuitive way.\n2.\tWhat is the sample complexity of the proposed matrix completion algorithm? \nThe introduction of the paper is poorly written. The first paragraph and the third one both contain some introduction to matric completion, which results in a lot of redundant information. The second paragraph and the fourth one are redundant in the same way since they both focus on geometric matrix completion. I think besides introducing what is matrix completion and what is geometric completion, the introduction part should focus more on the motivation to propose the algorithm. However, I can only see from the end of the second paragraph (some simple models need to be proposed) and the fifth paragraph (\u201cThe inspiration of our paper\u201d) some motivation information. The introduction part needs to be re-organized to provide more useful information about the paper rather than a literature review.\n\nThere is some unclear/inaccurate/subjective statement in the introduction part. For example, \u201cSelf-supervised learning\u201d needs a reference. Why geometric matrix completion generalizes the standard deep learning approaches is not clear. What does it mean by \u201ctheir design is \u2026 cumbersome and non-intuitive\u201d? The shape correspondence is never explained until very later in the paper.  Also, there are some unclear issues besides the Introduction part. For example, what does it mean by \u201cthe product graph\u201d? All these issues need to be clarified before the paper can be accepted. \n\n---------------------------------------------------\nThank you for the detailed rebuttal. For Q1, it clearly explains how does the method work. However, it is still not clear why does the method work. I also have another concern after reading the rebuttal, if the shape correspondence is not that important, why make it an important motivation in the paper? For Q2, it is interesting to see some theoretical results on the sample complexity, rather than an experimental one. The paper would also be much better if the clearance issues can be addressed. Even if I would not vote for an accept this time, I am looking forward to a revised version in the future.\n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}, "S1e0phdhoH": {"type": "rebuttal", "replyto": "BJxyzxrYPH", "comment": "Dear reviewers & ACs,\n\nWe have uploaded a revised version of our paper according to the discussions below.\n\nThe following updates have been made:\n\n(1) We improved the introduction, eliminated redundancies and added some motivation.\n(2) We updated the paper\u2019s main contributions.\n(3) We added the missing definitions, i.e., product graph.\n(4) We included a study on the effect of the scale of the initialization.\n(5) We filled in the missing results for the FM method and updated the results obtained with DMF to account for the initialization.\n(6) We included a study on the effect of the rank and the number of samples on the reconstruction error for both DMF and SGMC.\n(7) We added a discussion summarizing the results of these studies. In particular, conjecturing that the superb performance of SGMC is related to the fact that we work in an extremely data-poor regime and therefore the implicit regularization of DMF is not enough.\n(8) We included a link to a simplified version of the code in the form of a jupyter notebook.\n\nWe thank you for your time reviewing our paper and for the useful comments. It helped us improve our paper and reconfirm our reportings.\n\nRegards,\nThe authors.", "title": "Revised paper"}, "Byg11OH_sH": {"type": "rebuttal", "replyto": "HJlk4jujYB", "comment": "Experimentation:\n\n(1)  The number of trainable parameters for our method is the number of elements in $\\mathbf{P},\\mathbf{C},\\mathbf{Q}$. This is chosen according to $p_{\\mathrm{max}},q_{\\mathrm{max}}$ - a hyperparameter in our setting reported in Table 5. Overparameterization alone is not enough to produce the empirical improvements we reported. Without explicit regularization it would overfit the training data and perform poorly on the test data, specifically in the data poor regime. Also see comment (3) of reviewer #3 and the answer we provided.\n\nRegarding the other methods: we omitted those details as it is not clear how to compare the number of parameters between methods of ostensibly different nature, and it wasn\u2019t the focus of our paper.\n\n(2) To generate the Synthetic ML-100K dataset we did the following:\n\n - We computed the first $k=50$ eigenvectors $\\mathbf{\\Phi}_k,\\mathbf{\\Psi}_k$ of $\\mathbf{L}_\\mathrm{r}$ and $\\mathbf{L}_\\mathrm{c}$, the Laplacians of the row and column graphs for the ML-100K dataset\n\n- We projected $\\mathbf{M}$ on this subspace, i.e., \n\\[\n\\mathbf{M}_{\\mathrm{proj}} =\\mathbf{\\Phi}_k\\mathbf{\\Phi}_k^\\top\\mathbf{M}\\mathbf{\\Psi}_k\\mathbf{\\Psi}_k^\\top;\n\\]\n- We performed histogram matching between $\\mathbf{M}_\\mathrm{proj}$ and $\\mathbf{M}$ such that the histogram of entries in $\\mathbf{M}_\\mathrm{proj}$ is the same as in $\\mathbf{M}$. This nonlinear operation increases the rank of the matrix, so it is no longer k, but the perturbation to the singular values is small (verified empirically).\n\n(3) We do not have the training times for the other methods as we took the results from the corresponding papers. Our method (SGMC) runs in just a few minutes, depending on the datasets. For example, on ML-100K with the parameters reported in table 5 it takes about a minute, including the eigendecomposition and excluding the time taken to build the computational graph in Tensorflow.\n\n(4) The results of the FM method are poor for the other datasets so we did not include them. We will include them in the revised version.\n\nMinor comments:\n\n(2) Regarding equation (15):  $\\odot S$ should appear twice, following from the computation of the gradient. If $S$ is binary (as in our case) then $S\\odot S = S$ and one of the $S$ disappears.\n\nWe again thank you for the suggestions and will incorporate your comments into the revised version of the paper.\n\nYours sincerely, \nThe authors.", "title": "Reply to reviewer #2 (2/2)"}, "SkgW4BB_oB": {"type": "rebuttal", "replyto": "SyxRPVxfqB", "comment": "Dear reviewer #3,\n\nThank you for your comments! In what follows, we will try to address in detail the issues you raised:\n\n(1) We believe this is a misunderstanding of the hyperparams involved. While we stated in the paper the full scope of possible hyperparams in this general framework, we limited ourselves to only two settings:\n\t\n(a) SGMC - In this setting we set the weights $w_{ij}$ to 0 at all resolutions except the last one (full resolution). \n\n(b) SGMC-Z - In this setting we chose a-priori some spectral skip parameters (p_skip, q_skip) and we set $w_{ij}=1$ for (i=1+k*p_skip, j=1+k*q_skip), i.e., we sample the parameter space (p,q) on a grid with spacing (p_skip, q_skip), and set $w_{ij}=1$ only on the diagonal of this grid.  We did not try to explore any other setting for $w_{ij}$. \n\nOverall, the number of hyperparams involved is between 8 to 10 (2 for each energy involved and the p_max/q_max, p_skip/q_skip params). Moreover, we usually define the same hyperparams for the rows and columns energies, so it is about half that number. In a future work we will also make some of these parameters such as p_skip/q_skip learnable.\n\nAlso note that, following our ablation study, the dependence on the hyperparams is quite small (on some even negligible), and it is rather easy to tune them using a validation set.\n\n(2) As we noted, the data term of the SGMC is a special form of DMF from Arora et. al. But we also introduced two important additional terms: \n\nA Dirichlet energy term - promoting smoothness on the (inferred) graphs.\n\nA diagonalization term - promoting the new (inferred) bases to be Laplacian eigenbases.\n\nThese three terms together provide the geometric interpretation: If we treat the two factors $P,Q$ as corrections to some harmonic bases, and approximately enforce those new bases to also be harmonic bases (i.e., approximately diagonalizing the corresponding graph Laplacians), then we can model our matrix as some approximately bandlimited signal on a new product graph, whose functional space can be spanned by the new bases.\n\nFollowing your remark, we acknowledge there is some lack of clarity in the way we presented our approach: We do not provide a geometric interpretation to DMF but rather embed it within a bigger geometric framework. Once the aforementioned two terms are included, the geometric interpretation emerges.\n  \n(3) We thank you for raising this point. Our intention in including a comparison to DMF was to show how a simple method such as DMF can produce results on par with state-of-the-art geometric methods. This is one of the main messages of our paper - to show how badly the underlying geometry is being used (or how bad is the geometry being used) in geometric matrix completion methods.\nFollowing your remark, we performed the experiments with DMF again, using a \u201csmall\u201d initialization, and indeed we got a large improvement on the synthetic datasets! On the real datasets, however, we did not observe any improvement. The experiments we report in the following link measure the reconstruction error achieved by each method when the initialization is scaled by $10^{-\\alpha}$, $\\alpha>0$, as suggested in Li et al.\n\nhttps://drive.google.com/open?id=1pduAXS_NHwC1DhornDD9A78YehwCAjzR\n\n(4) Our regularization is explicit. It follows from both the Dirichlet energy and the multiresolution loss which weighs more heavily the lower frequency part of the functional map, as explained in the text. In order to better illustrate why the approach works, we came up with a toy example that can be found in the following link:\n\nhttps://colab.research.google.com/drive/1OkNEiTHok14gcVf3NxFIbAFutDN6-Tx6\n\nThis regularization does not necessitate depth, as in DMF, but still allows to enjoy the implicit regularization inherent to DMF with gradient descent methods. \n\nWe have a compelling explanation for the better performance of our method compared to DMF: As Arora et al reports (see Figure 2 in their paper), above a certain number of samples, DMF converges to the minimum norm solution. Below that number it induces a better regularization on the rank of the matrix, which still allows to recover low rank matrices. However, in the real datasets we tested on, the number of available samples is way below that threshold, as the rank of those matrices is not that low (See Table 4 in our paper). In this extremely data poor regime, DMF performs poorly, and the extra information present in the graphs is crucial. This is consistent with our experimentation with the toy problem we shared in the link above, and you can test yourself by changing the number of training samples. For a rank-10 matrix, using more than 30% of the entries allows for a very low reconstruction error with DMF, which outperforms our method (by a small margin). However, when going below 20%, our method demonstrates a clear advantage. We will add a discussion along these lines with relevant plots to the revised version.\n\nYours sincerely, \nThe authors.\n", "title": "Reply to Reviewer #3"}, "HkeYUUBdjr": {"type": "rebuttal", "replyto": "HJgg1pKRtB", "comment": "Dear reviewer #1,\n\nThank you for your comments! In what follows, we will try to address in detail the issues you raised:\n\nOur method comes from geometric considerations rather than an ad-hoc construction. We advocate that focusing on the geometric interpretation can sometimes lead to simplified architectures, therefore the title of our paper. In our case, it results in a fully linear network which, in our humble opinion, is a simpler architecture compared to some other competing geometric matrix completion methods. This motivates our subjective claims regarding \u201ccumbersome and non-intuitive designs\u201d. The message we were trying to convey was that architectural designs that originated in Euclidean deep learning such as convolutional layers followed by pointwise non-linearities, might not be the best candidates for other domains. For example, Wu et al. 2019 showed that it is possible to simplify  graph neural network architecture with a minor compromise to the end task. We will try to clarify these claims and address your major concerns below:\n\n(1) Our inspiration for the method came from problems in shape correspondence. A correspondence problem is a matrix completion problem with some constraints on the matrix. Although it served as an inspiration, it is unnecessary to understand the correspondence problem on shapes in order to understand our method. An intuitive explanation can be given along the following lines:\n\nWe are given a rating matrix, where each entry in the matrix is the rating given by a user i to an item j. Our model assumes that similar users should rate items similarly, and similar items should be rated similarly by different users. Similarity between users/items is encoded by some external graphs (constructed, respectively, on the row/column spaces of the matrix). \nOn a graph, one can define a function, i.e., a vector $\\mathbf{x}$ whose entries are values on the nodes of the graph. With some abuse of proper mathematical terminology, we call the function \u201csmooth\u201d if its values on adjacent nodes are close. This kind of smooth behaviour is encoded by the projection of the function on the first eigenvectors of the graph Laplacian $\\mathbf{L}$, in the same way that a \u201csmooth\u201d function in Euclidean space is composed only of harmonic functions with small frequencies. The eigenbasis of a graph Laplacian is the graph analogue of the Euclidean Fourier basis.\nSince we have two graphs, we have two such Fourier bases, $\\mathbf{\\Phi},\\mathbf{\\Psi}$, and we can treat the rating matrix as an outer product of two functions: one defined on the users graph and one defined on the items graph. Each one of these functions is smooth in its own right.  We therefore write our matrix as $\\mathbf{X} = \\mathbf{\\Phi}\\mathbf{C}\\mathbf{\\Psi}^\\top$.\n\nThe Dirichlet energy,\n\\begin{equation}\n    \\mathbf{x}^\\top \\mathbf{L}\\mathbf{x} = \\sum_{(a,b)\\in E}\\omega_{a,b}\\left(x(a)-x(b)\\right)^2,\n\\end{equation}\npenalizes the difference between the function values on adjacent nodes, and therefore minimizing it promotes such smooth functions. So, in principle, one can find smooth functions on a graph by minimizing some data term (e.g., the L2 norm) and regularize it with some smoothness term such as the quadratic Dirichlet energy. This gives rise to a simple convex problem which can give great results if the graphs are accurate,\n\n(equation 1)\n\\begin{equation}\n\\min_{\\mathbf{C}} \\|\\left(\\mathbf{\\Phi}\\mathbf{C}\\mathbf{\\Psi}^\\top-\\mathbf{M}\\right)\\odot \\mathbf{S}\\|_F^2 + \\mu_rE_{Dirichlet}^r(\\mathbf{C})+\\mu_cE_{Dirichlet}^c(\\mathbf{C}).\n\\end{equation}\nUnfortunately, our graphs are inaccurate since it is hard to model the relationship between users and items. Nevertheless, we would like to enforce our matrix to follow the model described above due to its simplicity, despite the inaccuracies in the graphs. To do that, we assume that the graphs can be \u201ccorrected\u201d, and on the \u201ccorrected\u201d graphs the matrix will still be smooth. Since correcting the graphs seems like a hard task, and anyway we are only interested in the representation of the function in the eigenbases of the \u201ccorrected\u201d graph Laplacians, we try to directly get these bases $\\mathbf{\\Phi}_{new},\\mathbf{\\Psi}_{new}$ by applying a linear transformation to the old bases: $\\mathbf{\\Phi}\\mathbf{P},\\mathbf{\\Psi}\\mathbf{Q}$.\nWe only need to make sure that the new bases are indeed Laplacian eigenbases, and this can be done by requiring them to diagonalize the new Laplacians. Since we don\u2019t have the new Laplacians, we will use the old ones as proxies.\n\nIf all is working according to plan, we will end up with new graphs (which remain latent), on which our unknown matrix  $\\mathbf{X}=\\mathbf{\\Phi}\\mathbf{P}\\mathbf{C}\\mathbf{Q}^\\top\\mathbf{\\Psi}^\\top$\nis smooth, and therefore has a low Dirichlet energy. This entire story is captured by equation (10) in our paper, with some additional minor details.", "title": "Reply to reviewer #1 (1/2)"}, "HJxIzuSOjB": {"type": "rebuttal", "replyto": "HJlk4jujYB", "comment": "Dear reviewer #2,\n\nThank you for your comments! In what follows, we will try to address in detail the issues you raised:\n\n(0) Regarding scalability of the spectral decomposition: This is an issue we acknowledged in the paper. Despite this shortcoming, our method is useful for a variety of mid-size problems, and we believe that the scalability issue does not take away from its theoretical and practical merits. \n\nRegarding the quality of the available geometric model: \n\nFirst, one of the main purposes of our paper is to reflect on the quality of geometric matrix completion (GMC) methods. We raise the question whether the models used by GMC methods are truly useful, or their results are due to some underlying phenomena (e.g., an implicit regularization inherent in gradient descent)? Indeed, our experimentation shows that results on par with state-of-the-art GMC can be obtained even with simple baselines such as deep matrix factorization (Arora et. al). We believe that these observations following from our extensive experimentation provide an important contribution to the community.\n\nSecond, it is not always difficult to obtain accurate geometric models. As we acknowledge in the paper, our intuition and inspiration comes from shape analysis. In this field the geometric models are usually quite accurate. Focusing on such cases allowed us to develop the geometric intuition behind our method and attribute its success (or failure) to the quality of the available geometric model.\n\nThird, there is some merit in starting with a crude geometric model and refining it with our optimization. This crude geometric model can be obtained via pre-processing, e.g., a supervised learning approach, and refined on the actual data. In the following link we provide a toy example that allows to investigate what happens when there is a mismatch between the graphs used to generate the data, and the graphs used to estimate it. There is a clear evidence that learning the graphs (i.e., by \u201crotating\u201d the Laplacian bases) improves the estimation, (compared to using the graphs as is)\n\nhttps://colab.research.google.com/drive/1OkNEiTHok14gcVf3NxFIbAFutDN6-Tx6\n\n(1) Our model tacitly assumes a particular form of an (approximate) low rank matrix: \nA matrix that is composed of a linear combination of the first harmonic vectors of some product graph (i.e., those corresponding to low frequencies). We further simplified this assumption and assumed this matrix is smoothed separately on the rows graph and the columns graph.\nWhy is this a good model? We believe that information about a particular user can be shared across other similar users (as captured by the graph edges) via a process of smoothing (or diffusion). Following this process, similar users should give similar ratings. In the same way, similar items should have similar ratings. This similarity across neighbouring nodes is captured by the Dirichlet energies for the rows and column graphs - small Dirichlet energy corresponds to a smooth function. Please also refer to the explanation we provided to reviewer #1.\nUsing this model is particularly helpful (empirically) in the data poor regime which most real data sets lie in. In this regime it is not enough to assume just that the matrix is low rank, despite methods like DMF (Arora et. al) exploiting complex rank regularizations. An additional assumption on the structure of this matrix turns out to be very helpful, provided that a geometric model (even a mildly crude one) is available. See our cold start analysis (FIgure 1) that explores this regime and our reply to reviewer #3. You are also welcome to play with the number of samples in the aforementioned link and witness the effectiveness of our method in the data poor regime.\n\n(2) The fact that our model only provides a marginal improvement in the case of a poor geometric model definitely makes sense to us. When the geometric model is poor, there is a small difference (if at all) between using the graphs or not using them altogether. In these cases there is no clear evidence as to what is the contributing factor to the rank regularization - be it an implicit regularization due to the gradient descent or some other factor. For the Douban dataset for example (table 1) we see that the DMF method (Arora et. al) is competitive with the other more complicated methods. In our opinion, this is due to poor geometry. We further explored this phenomena (i.e., by perturbing the graphs with increasing noise) in the toy example in the aforementioned link.\n", "title": "Reply to reviewer #2 (1/2)"}, "S1gT7LHdoH": {"type": "rebuttal", "replyto": "HJgg1pKRtB", "comment": "Another interesting observation we made is that our method is essentially an overparameterized deep matrix factorization (DMF) method with some additional structure. DMF has been proven recently (see Arora et al. 2019 and the discussion with reviewer #3) to promote a low rank via implicit regularization of gradient descent. This is a contributing factor to the success of our method.\n\nWe made up a tutorial to allow experimenting with our method in the link below, and we hope you can find it useful to understand the method better:\n\n\thttps://colab.research.google.com/drive/1OkNEiTHok14gcVf3NxFIbAFutDN6-Tx6\n\n(2) The number of available ratings for each dataset is provided in Table 4. If by \u201csample complexity\u201d you mean how the test error changes with the size of the training set, we believe that our cold start analysis (Figure 1) provides an answer: we show that the SGMC-Z version of our method is particularly more effective in the data-poor regime than the SGMC and other competing algorithms. Even after retaining only 5 ratings for more than half the users, we still get competitive results compared, for example, to RGCNN (compare Figure 1 and Table 1). \n\nRegarding the presentation issues: we thank you for the suggestions and will reformulate some parts of the paper according to your recommendations.\n\nYours sincerely, \nThe authors.\n\n", "title": "Reply to Reviewer #1 (2/2)"}, "HJlk4jujYB": {"type": "review", "replyto": "BJxyzxrYPH", "review": "This paper proposes a novel approach for the loss function of matrix completion when geometric information is available. The proposed method consists of two ideas: (1) spectral regularization (i.e., Dirichlet energy) with a re-parameterizing basis and (2) multiresolution of spectral loss (i.e., zoomout loss). In addition, the zoomout loss is motivated by the approach for shape correspondence and can be a generalization of the recent matrix completion method (deep matrix factorization). Empirical results show the best performance compared to other recent methods under small-scale datasets. Moreover, the proposed method outperforms when the geometric model is accurate (verified on the synthetic setting) and this can reflect that the proposed method is a good choice when the graph structures are given.\n\nThis work can be a significant contribution as it is a simple linear model but practically performs better than other deep nonlinear networks (e.g., RGCNN). Additionally, the proposed loss functions utilize only the spectral information of graph structure with novel approaches. However, there are some drawbacks to this work. First, it requires a good quality of geometric model which is hard to obtain in practical datasets. Second, the proposed method has a scalability issue since it requires eigendecompositions of graph Laplacians (as discussed in the paper). This can be a problem for real and large-scale datasets.\n\nOverall, this paper presents a novel approach utilizing graph spectral information with empirical improvements. But, I vote for weak acceptance due to its drawbacks as mentioned above.\n\nMain concerns:\n\n1. It is not clear why minimizing Dirichlet energy can improve the performance of matrix completion. In the paper, the authors mention that it promotes smooth functions on the graph nodes, but not fully clear why smooth functions are good. And how much does the accuracy increase (or decrease) when using the Dirichlet regularization? \n\n2. Authors argue that the re-parameterizing of the basis (emerging P and Q) can find a better geometric model (section 2). So, it is expected that the proposed method shows a better result when the given geometric model is not accurate. However, the empirical results are reported poor improvements for inaccurate geometric models. Does this make sense?\n\nFor experiments:\n\n1. What is the number of trainable parameters for each method? Since the proposed method is overparameterized, it is not clear that the empirical improvements come from the overparameterizing or the proposed loss function. It would be great to report the number of parameters of all other methods by setting similar numbers.\n\n2. It is not clear how to generate the synthetic dataset, i.e., projecting a random matrix on te the first few eigenvectors of L_r and L_c. It would be better to give more details.\n\n3. What are the training times of the proposed method and other competitors?\n\n4. Why results of FM are not reported under other datasets?\n\nMinor comments:\n\n1. In page 4, please edit \u201cWe explore The\u201d -> \u201cWe explore the\u201d.\n\n2. In equation (15), writing \u201c\\odot S\u201d twice seems to be unnecessary.\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 3}, "SyxRPVxfqB": {"type": "review", "replyto": "BJxyzxrYPH", "review": "This paper aims to solve the matrix completion problem by incorporating geometric information. The proposed approach involves using graphs encoding relations between rows (and columns), applying spectral decomposition to these graphs, and using a multi-resolution spectral geometric loss to reconstruct the functional map which could then be used to directly recover the underlying matrix. The paper evaluates the proposed network on both synthetic and real datasets and shows improvements over the existing geometric methods and convex relaxations.\n\nWhile the geometric approach looks interesting and the experimental results seem promising, it is unclear why the proposed approach works, and the comparison with [Arora et al. (2019)] is not fair. Below are the specific comments.\n\n(1) The proposed approach (formulation (10)) involves too many parameters (including the weights w in (9)) that need to be tuned. The authors should discuss how to select the parameters after (10). This also raises the question of how practical the proposed approach is.\n\n(2) The authors claim the first contribution is to provide the geometric interpretation of deep matrix factorization via the functional maps framework. However, I didn't see clearly the interpretation. If it refers to the parametrization of X by \\Phi P C Q^T \\Psi^T, then it is just a special case of deep matrix factorization since both \\Phi and \\Psi are fixed, and P and Q are optimized to be approximately orthonormal.\n\n(3) Due to over-parameterization, in general deep matrix factorization would suffer from overfitting. That being said [Gunasekar et al. (2017), Arora et al. (2019)] prove that gradient descent induces implicit regularization if the algorithm is initialized with factors that are very \"small\". However, in the experiments, both P and Q are initialized as the identity, which is not close to zero. Indeed, it was proved in the following paper that the generalization gap will be proportional to the energy of the initialization, even for matrix factorization.\n\nYuanzhi Li, Tengyu Ma, and Hongyang Zhang, Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations.\n\n(4) As a followup question, without such implicit regularization, it is unclear why the proposed approach does not suffer from overfitting. A discussion along this line is required. Though the authors include the connection between [Arora et al. (2019)], this is not convincing enough since as explained above, the implicit regularization there depends on the smallness of the initialization.", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 4}, "H1l2FMPGOB": {"type": "rebuttal", "replyto": "Hylfk4HzOB", "comment": "Thank you for your interest in our paper!\n\na). We ablated all but the coefficients for the orthogonalization terms.\u00a0 Figure 2 shows the ablation study for Dirichlet (left) and Diagonalizaition (right) terms for SGMC,\u00a0\nand Figure 3 shows the ablation study\u00a0for Dirichlet (left) and Diagonalizaition (right) terms for SGMC-Z.\u00a0\n\nWe can summarize the importance of the different terms as follows:\n\nDirichlet - seems to be rather important.\nDiagonalization - seems to be moderately important.\nOrthogonalization - hardly important (that's why we did not include it in our ablation study).\n\nIt should be emphasized that the Dirichlet energy is with respect to the new basis and not the old basis (please refer to equation\u00a07).\u00a0 \nYou can also look at the hyperparameters table (table 5) and and see that the Dirichlet energy was needed.\u00a0 \u00a0\n\nb). The diagonalization energy proposed\u00a0in Coupled quasi harmonic basis (Kovnatsky et al.) is essentially the same as ours. The differences are due to notation:\nWe define off(A) as the off-diagonal entries of A, whereas Kovnatsky et al. define off(A) as the sum of squares of the off-diagonal entries of A.\nIn our notation, that would be ||off(A)||^2 = sum(i~=j) a^2_{ij}.\nAlso,\u00a0 Kovnatsky et al. enforces orthogonality (w.r.t to the manifold inner product), whereas\u00a0we only promote approximate orthogonality via a penalty function.\nIt should be noted that we are not the first to propose functional map estimation via joint diagonlization. These ideas have been spinning around for some time in the shape analysis community, and we borrowed them for the problem of matrix completion.\u00a0We acknowledge Litany et. al's\u00a0 \"Fully Spectral Functional Maps\", the most up to date shape correspondence method based on joint diagonalization, as our source of inspiration.\n\nc). We shall provide a link to the source code soon. Stay tuned!\n\nThe authors", "title": "re: Ablation study and source code"}}}