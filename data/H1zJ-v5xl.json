{"paper": {"title": "Quasi-Recurrent Neural Networks", "authors": ["James Bradbury", "Stephen Merity", "Caiming Xiong", "Richard Socher"], "authorids": ["james.bradbury@salesforce.com", "smerity@salesforce.com", "cxiong@salesforce.com", "rsocher@salesforce.com"], "summary": "QRNNs, composed of convolutions and a recurrent pooling function, outperform LSTMs on a variety of sequence tasks and are up to 16 times faster.", "abstract": "Recurrent neural networks are a powerful tool for modeling sequential data, but the dependence of each timestep\u2019s computation on the previous timestep\u2019s output limits parallelism and makes RNNs unwieldy for very long sequences. We introduce quasi-recurrent neural networks (QRNNs), an approach to neural sequence modeling that alternates convolutional layers, which apply in parallel across timesteps, and a minimalist recurrent pooling function that applies in parallel across channels. Despite lacking trainable recurrent layers, stacked QRNNs have better predictive accuracy than stacked LSTMs of the same hidden size. Due to their increased parallelism, they are up to 16 times faster at train and test time. Experiments on language modeling, sentiment classification, and character-level neural machine translation demonstrate these advantages and underline the viability of QRNNs as a basic building block for a variety of sequence tasks.", "keywords": ["Natural language processing", "Deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper is well written and easy to follow. It has strong connections to other convolutional models such as pixel cnn and bytenet that use convolutional only models with little or no recurrence. The method is shown to be significantly faster than using RNNs, while not losing out on the accuracy.\n \n Pros:\n - Fast model\n - Good results\n \n Cons:\n - Because of its strong relationship to other models, the novelty is incremental."}, "review": {"BJ7iqx_vl": {"type": "rebuttal", "replyto": "Bke2fQDDg", "comment": "Thanks for letting me know. After reading the gated/conditional PixelCNN paper more carefully, the similarities--especially to the encoder-decoder model--are clear and definitely worth mentioning, so we've added a detailed discussion to Related Work. We also added the ByteNet IWSLT result to the machine translation comparison table.", "title": "Made requested changes"}, "Bke2fQDDg": {"type": "rebuttal", "replyto": "HkxmzLgwl", "comment": "I would agree with the anonymous reviewer, and with private discussions with reviewers that the following are required for acceptance of the paper:\n1. Addition of PixelCNN reference to the paper because of its strong relationship to the paper. Sequential tasks with CNNs were first reported with pixel cnn and it makes sense to me that the proper attribution should be made here.\n2. Addition of the Bytenet results on the IWSLT dataset that were reported above. \nPlease make those changes and upload to help us make the final decision.\nThanks!", "title": "Please add reference to pixelcnn + comparison with bytenet on iwslt to paper."}, "HkxmzLgwl": {"type": "rebuttal", "replyto": "Hk5V6jOLl", "comment": "Changes are:\n\n-- Added results on copy and addition tasks to appendix\n-- Added discussion and citation of query-reduction networks to related work\n-- Added explanation of why small batch sizes/long sequences show the best speed-up\n-- Added justification and citation of the specific attention mechanism used in the MT experiment", "title": "Updated manuscript"}, "Sy7nJp1wl": {"type": "rebuttal", "replyto": "HJyFl3dIl", "comment": "Although the authors replied to all but one (12.) of my comments, I could not find any changes in the manuscript. In particular, the authors still do not compare their model to the related and very recent ByteNet model. The link in the discussion in insufficient. Without this comparison and addressing the remaining minor comments, I am against acceptance of the manuscript.\n\nMajor comment\n=============\nQRNNs are closely related to PixelCNNs, which leverage masked dilated convolutional layers to speed-up computations. However, the authors cite ByteNet, which builds upon PixelCNN, only at the end of their manuscript and do not include it in the evaluation. The authors should cite PixelCNN already when introducing QRNN in the methods sections, and include it in the evaluation. At the very least, QRNN should be compared with ByteNet for language translation. How well does a fully convolutional model without intermediate pooling layers perform, i.e. what is the effect to the introduced pooling layers? Are their performance difference between f, fo, and ifo pooling? Did the authors investigate dilated convolutional layers?\n\n> You can find a ByteNet comparison, using results Nal Kalchbrenner provided during NIPS, above in our response to the official reviews. A fully convolutional model without pooling would not be able to solve the sentiment and translation tasks because it would have a short, fixed context length (i.e., it would be an n-gram model). We did not investigate dilated convolutions, likely the most important single component of the ByteNet model, but they are definitely a good area for future QRNN research.\n\n>> QRNNs should be compared in the manuscript to ByteNet. I disagree that a convolutional model is unsuited for sentiment classification of translation: by stacking multiple masked (dilated) convolutional layers, it will have a large receptive field and hence be able to efficiently take larger sequence context into account. This is the very idea of the ByteNet model.\n\n\nMinor comments\n==============\n1. How does a model without dense connections perform, i.e. what is the effect of dense connections? To illustrate dense connections, the authors might draw them in figure 1 and refer to it in section 2.1. \n\n> The main use for dense connections is to enable training of deeper QRNN models. Performance with and without dense connections was similar for three-layer networks, but a four-layer network without dense connections was significantly worse than a three-layer one while a four-layer network with dense connections was better. A three-layer QRNN has results comparable to a two-layer LSTM, while a two-layer QRNN is slightly worse. But these differences are on the order of small fractions of a percentage point, and are difficult to distinguish from random initialization effects, while the four-layer densely-connected QRNN was consistently better than any LSTM-based model we tried.\n\n>> I suggest to insert a table for figure in the appendix, which shows to which extent performance gains are accounted by either the new pooling layers, dense connections, or zone-out regularization. Otherwise it is unclear which components are actually relevant and if the new pooling layers help.\n\n\n2. The run-time results shown in figure 4 are very helpful, but as far as I understood, the breakdown shown on the left side was measured for language modeling (referred in 3.2), whereas the dependency on batch- and sequence size shown on the right side for sentiment classification (referred in 3.1). I suggest to consistently show the results for either sentiment classification or language modeling, or both. At the very least, the figure caption should describe the task explicitly. Labeling the left and right figure by a) and b) would further improve readability. \n\n> The left-hand part of the speed figure displays results for the forward and backward pass of all parts of a QRNN language model. The right-hand part reports speed ratios for the forward pass of the QRNN layer alone, compared to an equal-sized LSTM layer. This will be clarified in the text.\n\n>> Since each figure should stand on its own, this should be clarified in the figure caption. I also cannot see any changes in the text.\n\n\n3. Section 3.1 describes a high speed-up for long sequences and small batch sizes. I suggest motivating why this is the case. While computations can be parallelized along the sequence length, it is less obvious why smaller batch sizes speed-up computations.\n\n> It is not so much that smaller batch sizes speed up QRNN computations as that they slow down LSTM ones, even the fairly well optimized NVIDIA CUDNN LSTM kernels, largely because the recurrent weight matrix must be reloaded from memory between timesteps. (Baidu has released kernels for simple (non-LSTM) RNNs that work around this problem for small hidden sizes and specific models of GPU by caching the weight matrix on individual compute cores). This reloading wastes compute cycles, and the wasted cycles represent a larger fraction of total cycles when the batch size is relatively low or the sequence length is relatively high.\n\n>> This should be described in the text\n\n\n4. The proposed encoder-decoder attention is different from traditional attention in that attention vectors are not computed and used as input to the decoder sequentially, but on top of decoder output states in parallel. This should be described and motivated in the text.\n\n> There are a wide variety of attentional architectures in use for machine translation, with no one architecture holding a clear advantage over others. The architecture we chose for the QRNN MT experiment is most similar to Minh-Thang Luong's late-fusion dot product attention without \"input-feeding,\" as described in Luong et al. (2015). The motivation is to allow the QRNN layers to run in a fully timestep-parallel way during training, even if they can't during inference; any kind of input feeding would make this impossible, although it would likely result in slightly better performance.\n\n>> This should be motivated in the text and Luong et al. (2015) be cited.\n\n\nSentiment classification\n------------------------\n\n5. What was the size of the hold-out development set and how was it created? The text describes that data were split equally into training and test set, without describing the hold-out set.\n\n> We used a randomly selected 20% of the training set as a held-out development set to select hyperparameters, then set those hyperparameters in stone and retrained using the entire training set to compute test accuracy.\n\n>> Okay.\n\n\n6. What was the convolutional filter size?\n\n> We report results for filters of size two and size four, and didn't find any advantage to larger filters for word-level tasks.\n\n>> The filter size should be stated in the text.\n\n\n7. What is the speed-up for the best hyper-parameters (batch size 24, sequence length 231)?\n\n> The speedup for sentiment was about a factor of three, and this differs from the factor of about five shown in figure 4 because the dense connections between layers provide slightly more built-in parallelism (compared to the situation without dense connections, a larger fraction of the overall LSTM computation depends only on the input to each layer and can be run in parallel).\n\n>> The expected factor of ~3x should be mentioned in the figure caption and the text. I also suggest surrounding it in Figure 4.\n\n\n8. Figure 3 would be easier to interpret by actually showing the text on the y-axis. For the sake of space, one might use a smaller text passage, plot it along the x-axis, and show the activations of fewer neurons along the y-axis. Showing more examples in the appendix would make the authors\u2019 claim that neurons are interpretable even more convincing.\n\n> We tried a few different ways of presenting Figure 3 with corresponding text in the paper without much success, but an interactive version of the figure with text that shows up on hover is available on our blog at http://metamind.io/research/new-neural-network-building-block-allows-faster-and-more-accurate-text-understanding/.\n\n>> The linked interactive visualized does not show the text either and only a single example. More examples along with the text on the x-axis are needed to show that neurons learn interpretable patterns.\n\n\nLanguage modeling\n-----------------\n9. What was the size of the training, test, and validation set?\n\n> The training set of the Penn Treebank consists of 887521 words of running text, the validation set of 70390, and the test set of 78669.\n\n>> Okay.\n\n\n10. What was the convolutional filter size, denoted as \u2018k\u2019?\n\n> The convolutional filter size was two, and we didn't find any advantage to larger filters for word-level tasks.\n\n>> The filter size should be stated in the text, e.g. via \u2018k=2\u2019.\n\n\n11. Is it correct that a very high learning rate of 1 was used for six epochs at the beginning?\n\n> Yes, that is fairly standard in the language modeling literature and seems to work better than smaller learning rates.\n\n>> Okay.\n\n\n12. The authors should show learning curves for a models with and without zone-out.\n\n>> The authors did not address this question.\n \n\nTranslation\n-----------\n\n13. What was the size of the training, test, and validation set?\n\n> 209772 sentence pairs of training data, 1110 of validation, and 1415 of test.\n\n>> Okay.\n\n\n14. How does translation performance depend on k?\n\n> As a character-level task, our machine translation benchmark is more dependent on k than the two word-level tasks, for which k=2 is sufficient. For translation, performance with k=2 is not significantly better than the LSTM model.\n\n>> Okay.\n\n \n \n", "title": "Did not address comments in manuscript"}, "HJyFl3dIl": {"type": "rebuttal", "replyto": "Sk3LHqb4e", "comment": ">>>At the very least, QRNN should be compared with ByteNet for language translation. How well does a fully convolutional model without intermediate pooling layers perform, i.e. what is the effect to the introduced pooling layers? Are their performance difference between f, fo, and ifo pooling? Did the authors investigate dilated convolutional layers?\nYou can find a ByteNet comparison, using results Nal Kalchbrenner provided during NIPS, above in our response to the official reviews.\nA fully convolutional model without pooling would not be able to solve the sentiment and translation tasks because it would have a short, fixed context length (i.e., it would be an n-gram model). We did not investigate dilated convolutions, likely the most important single component of the ByteNet model, but they are definitely a good area for future QRNN research.\n\n>>> 1. How does a model without dense connections perform?\nThe main use for dense connections is to enable training of deeper QRNN models. Performance with and without dense connections was similar for three-layer networks, but a four-layer network without dense connections was significantly worse than a three-layer one while a four-layer network with dense connections was better. A three-layer QRNN has results comparable to a two-layer LSTM, while a two-layer QRNN is slightly worse. But these differences are on the order of small fractions of a percentage point, and are difficult to distinguish from random initialization effects, while the four-layer densely-connected QRNN was consistently better than any LSTM-based model we tried.\n\n>>> 2. At the very least, the (speed) figure caption should describe the task explicitly. Labeling the left and right figure by a) and b) would further improve readability. \nThe left-hand part of the speed figure displays results for the forward and backward pass of all parts of a QRNN language model. The right-hand part reports speed ratios for the forward pass of the QRNN layer alone, compared to an equal-sized LSTM layer. This will be clarified in the text.\n\n>>> 3. Section 3.1 describes a high speed-up for long sequences and small batch sizes. I suggest motivating why this is the case. While computations can be parallelized along the sequence length, it is less obvious why smaller batch sizes speed-up computations.\nIt is not so much that smaller batch sizes speed up QRNN computations as that they slow down LSTM ones, even the fairly well optimized NVIDIA CUDNN LSTM kernels, largely because the recurrent weight matrix must be reloaded from memory between timesteps. (Baidu has released kernels for simple (non-LSTM) RNNs that work around this problem for small hidden sizes and specific models of GPU by caching the weight matrix on individual compute cores). This reloading wastes compute cycles, and the wasted cycles represent a larger fraction of total cycles when the batch size is relatively low or the sequence length is relatively high.\n\n>>> 4. The proposed encoder-decoder attention is different from traditional attention in that attention vectors are not computed and used as input to the decoder sequentially, but on top of decoder output states in parallel. This should be described and motivated in the text.\nThere are a wide variety of attentional architectures in use for machine translation, with no one architecture holding a clear advantage over others. The architecture we chose for the QRNN MT experiment is most similar to Minh-Thang Luong's late-fusion dot product attention without \"input-feeding,\" as described in Luong et al. (2015). The motivation is to allow the QRNN layers to run in a fully timestep-parallel way during training, even if they can't during inference; any kind of input feeding would make this impossible, although it would likely result in slightly better performance.\n\n>>> 5. What was the size of the hold-out development set for sentiment and how was it created? The text describes that data were split equally into training and test set, without describing the hold-out set.\nWe used a randomly selected 20% of the training set as a held-out development set to select hyperparameters, then set those hyperparameters in stone and retrained using the entire training set to compute test accuracy.\n\n>>> 6. What was the convolutional filter size?\nWe report results for filters of size two and size four, and didn't find any advantage to larger filters for word-level tasks.\n\n>>> 7. What is the speed-up for the best hyper-parameters (batch size 24, sequence length 231)?\nThe speedup for sentiment was about a factor of three, and this differs from the factor of about five shown in figure 4 because the dense connections between layers provide slightly more built-in parallelism (compared to the situation without dense connections, a larger fraction of the overall LSTM computation depends only on the input to each layer and can be run in parallel).\n\n>>> 8. Figure 3 would be easier to interpret by actually showing the text on the y-axis. For the sake of space, one might use a smaller text passage, plot it along the x-axis, and show the activations of fewer neurons along the y-axis. Showing more examples in the appendix would make the authors\u2019 claim that neurons are interpretable even more convincing.\nWe tried a few different ways of presenting Figure 3 with corresponding text in the paper without much success, but an interactive version of the figure with text that shows up on hover is available on our blog at http://metamind.io/research/new-neural-network-building-block-allows-faster-and-more-accurate-text-understanding/.\n\n>>>9. What was the size of the training, test, and validation set?\nThe training set of the Penn Treebank consists of 887521 words of running text, the validation set of 70390, and the test set of 78669.\n\n>>>10. What was the convolutional filter size, denoted as \u2018k\u2019?\nThe convolutional filter size was two, and we didn't find any advantage to larger filters for word-level tasks.\n\n>>>11. Is it correct that a very high learning rate of 1 was used for six epochs at the beginning?\nYes, that is fairly standard in the language modeling literature and seems to work better than smaller learning rates.\n\nTranslation\n-----------\n>>>13. What was the size of the training, test, and validation set?\n209772 sentence pairs of training data, 1110 of validation, and 1415 of test.\n\n>>>14. How does translation performance depend on k?\nAs a character-level task, our machine translation benchmark is more dependent on k than the two word-level tasks, for which k=2 is sufficient. For translation, performance with k=2 is not significantly better than the LSTM model.", "title": "Many thanks for your thoughtful comments!"}, "Hk5V6jOLl": {"type": "rebuttal", "replyto": "H1zJ-v5xl", "comment": "Reviewer 1 asks, \"Can you offer any results comparing the performance of the proposed Quasi-Recurrent Neural Network (QRNN) to that of the recently published ByteNet? How does it compare from a computational perspective?\"\n\nYes. Nal Kalchbrenner released results for the ByteNet on the same IWSLT dataset we used in this paper in his slides for NIPS https://drive.google.com/file/d/0B7jhGCaUwDJeZWZWUXJ4cktxVU0/view. He obtained BLEU of 24.7, or about five points higher than the QRNN, using the full size architecture used on larger datasets in the original paper. We don't know how much of this difference can be attributed to the overall ByteNet model architecture, as opposed to separate contributions of that paper like residual multiplicative blocks or sub-batch normalization. Computationally, the QRNN shares many of the computational advantages of the ByteNet architecture, including the use of operations that run in parallel along the sequence length (one of their desiderata) and the relatively short paths traversed by gradients from the target to the source sentence. While the QRNN does not have linear-time attention, the (quadratic-complexity) computation of attention weights is a small fraction of overall training time. As implemented for the MT experiment in our paper, the QRNN encoder-decoder architecture is likely several times faster for training and inference than ByteNet, but that is not so much an inherent fact about the two architectures as it is the result of different layer counts and the complexity of the ByteNet residual block/residual multiplicative block.\n\nReviewers 1 and 2 had similar questions about QRNN performance on common toy sequence tasks. Reviewer 1 asks, \"What kind of task will the QRNN struggle with? More specifically, of the standard RNN/LSTM tasks, are there any for which the QRNN is ill-suited? For example, would the depth need to grow linearly for the COPY task?\"\nThese are questions we've been very curious about too. While we used the copy and addition tasks to help with early debugging of the QRNN code and the sequence to sequence architecture, we hadn't yet run a more systematic investigation of QRNN performance on those tasks. On the addition task, the QRNN does in fact generalize to long sequences less well than LSTMs; on the copy task, it generalizes as well or better. We will add a brief appendix to the paper describing the following experimental results:\nWe take the addition task to mean sequence-to-sequence decimal addition of unaligned, variable-length numbers (the hardest of several versions). An example of this task for maximum length 5 digits is \"73952+9462\"-->\"83414\". We train on n_train randomly generated examples for up to 100 epochs with early stopping, and report the smallest model setup that achieves >99% character-level validation accuracy, or the best validation accuracy achieved by any model setup if none achieve 99%.\nFor n_digits=5 and n_train=100000, the QRNN converges with models larger than 3 layers of 256 units, while in our experiments LSTMs require only 2 layers of 128 units. For n_digits=10 and n_train=100000, an LSTM reaches 98.5% with 3 layers of 1024 units, while the best QRNN model (4 layers of 512 units) reaches only 95.0%.\nThe copy task is similarly implemented as sequence-to-sequence reconstruction of variable-length decimal numbers. For 5 digits, an example is \"23532\"-->\"23532\". We train on 10000 randomly generated examples for up to 100 epochs. For n_digits=5, the QRNN converges with models larger than 2 layers of 32 units or 1 layer of 256 units, while the LSTM requires only 1 layer of 32 units. For n_digits=10, the QRNN requires a model larger than 2 layers of 128 units while the LSTM requires a model of at least 2 layers of 64 units or 1 layer of 256 units. For n_digits=40, a QRNN with 5 layers of 512 units reaches 98.0% while the best LSTM model, with 3 layers of 512 units, only reaches 95.5%. A deeper LSTM would likely need some kind of residual or highway connections to converge on this task, while the deep QRNN converges relatively well despite our earlier experiences with 4-layer QRNNs without dense connections not converging successfully on the sentiment task.\n\nReviewer 2 asks, \"Are densely connected layers necessary to obtain good results on the IMDB task? How does a simple 2-layer QRNN compare with 2-layer LSTM?\"\nThe main use for dense connections is to enable training of deeper QRNN models. Performance with and without dense connections was similar for three-layer networks, but a four-layer network without dense connections was significantly worse than a three-layer one while a four-layer network with dense connections was better. A three-layer QRNN has results comparable to a two-layer LSTM, while a two-layer QRNN is slightly worse. But these differences are on the order of small fractions of a percentage point, and are difficult to distinguish from random initialization effects, while the four-layer densely-connected QRNN was consistently better than any LSTM-based model we tried.\n\nReviewer 2 asks, \"How does the f-fo-ifo pooling perform comparatively?\"\nOur early experiments with language modeling and toy sequence tasks, along with the results from Balduzzi and Ghifary (2016) suggested to us that fo-pooling consistently outperforms f-pooling and ifo-pooling. We wanted to demonstrate that the various different elementwise building blocks found in traditional recurrent networks can all be adapted to the quasi-recurrent context. The QRNN can use any kind of elementwise gating function, but we haven't found one that performs better than fo-pooling.\n\nReviewer 3 asks to clarify the speed comparison, perhaps invoking big-O descriptions. We agree with the reviewer's summation that the speed improvements of the QRNN exist because \"you're moving compute from a serial stream to a parallel stream and also making the serial stream more parallel.\" There is no asymptotic difference in computational complexity of a QRNN versus a traditional recurrent model, but the dependency structure of the computation (what needs to be computed before each operation can take place) makes it significantly better suited to the kinds of parallel computing architectures that are widely used today. It's likely that this advantage can be formalized (for instance, in terms of longest computational dependency path or average dependency branching factor) but not in terms of big-O notation.\n\nReviewer 3 also asks about replication. We're excited about the interest we've seen in replicating the QRNN model; one team that we've heard from has successfully replicated the language modeling results from the paper; another has in-progress work with QRNN results on a non-sentiment classification task that significantly outperform that task's published LSTM numbers.", "title": "Response to official reviews, plus results on copy and addition tasks"}, "By-yYazNe": {"type": "review", "replyto": "H1zJ-v5xl", "review": "Can you offer any results comparing the performance of the proposed Quasi-Recurrent Neural Network (QRNN) to that of the recently published ByteNet? How does it compare from a computational perspective?\n\nWhat kind of task will the QRNN struggle with? More specifically, of the standard RNN/LSTM tasks, are there any for which the QRNN is ill-suited? For example, would the depth need to grow linearly for the COPY task?\nThis paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of the temporal transitions in\nsequence data. Briefly (and slightly inaccurately) model starts with the LSTM structure but removes all but the diagonal elements to the transition\nmatrices. It also generalizes the connections from lower layers to upper layers to general convolutions in time (the standard LSTM can be though of as a convolution with a receptive field of 1 time-step). \n\nAs discussed by the authors, the model is related to a number of other recent modifications of RNNs, in particular ByteNet and strongly-typed RNNs (T-RNN). In light of these existing models, the novelty of the QRNN is somewhat diminished, however in my opinion their is still sufficient novelty to justify publication.\n\nThe authors present a reasonably solid set of empirical results that support the claims of the paper. It does indeed seem that this particular modification of the LSTM warrants attention from others. \n\nWhile I feel that the contribution is somewhat incremental, I recommend acceptance. \n", "title": "Questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJcItTz4e": {"type": "review", "replyto": "H1zJ-v5xl", "review": "Can you offer any results comparing the performance of the proposed Quasi-Recurrent Neural Network (QRNN) to that of the recently published ByteNet? How does it compare from a computational perspective?\n\nWhat kind of task will the QRNN struggle with? More specifically, of the standard RNN/LSTM tasks, are there any for which the QRNN is ill-suited? For example, would the depth need to grow linearly for the COPY task?\nThis paper introduces the Quasi-Recurrent Neural Network (QRNN) that dramatically limits the computational burden of the temporal transitions in\nsequence data. Briefly (and slightly inaccurately) model starts with the LSTM structure but removes all but the diagonal elements to the transition\nmatrices. It also generalizes the connections from lower layers to upper layers to general convolutions in time (the standard LSTM can be though of as a convolution with a receptive field of 1 time-step). \n\nAs discussed by the authors, the model is related to a number of other recent modifications of RNNs, in particular ByteNet and strongly-typed RNNs (T-RNN). In light of these existing models, the novelty of the QRNN is somewhat diminished, however in my opinion their is still sufficient novelty to justify publication.\n\nThe authors present a reasonably solid set of empirical results that support the claims of the paper. It does indeed seem that this particular modification of the LSTM warrants attention from others. \n\nWhile I feel that the contribution is somewhat incremental, I recommend acceptance. \n", "title": "Questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkRqYCsmg": {"type": "rebuttal", "replyto": "S180bwJXx", "comment": "1. We haven't run such a model ourselves on the tasks from the paper, but the literature contains many examples of traditional convolution + pooling applied to NLP problems. This is a strong architecture for sequence classification tasks like IMDb sentiment (e.g. Johnson and Zhang 2014) and performs well when used as an encoder for machine translation (e.g. Cho 2014). On the other hand, applying pooling with a fixed temporal window to language modeling or as an MT decoder would reduce the problem to n-gram language modeling and prevent learning of long-term dependencies.\n\n2. Larger convolutional filter size appeared not to help for the two word-level tasks we used, while it provided a fairly significant improvement on character-level MT (BLEU scores were around 16-17 for a few different experiments with k=2, although those runs also differed slightly in other ways).", "title": "Great questions"}, "r1jJ1AsXx": {"type": "rebuttal", "replyto": "rJMFbhRzg", "comment": "The following are all equally accurate ways to describe the QRNN; we chose to focus on the first one in order to emphasize that QRNNs bridge the spaces of recurrent and convolutional architectures:\n\n1. QRNNs are neural network layers composed of a one-dimensional convolution followed by an LSTM-inspired gating function that performs dynamic average pooling across time.\n\n2. QRNNs are gated recurrent networks similar to LSTMs or GRUs, except that their gates and state updates do not include a dependence on the previous timestep's state, instead depending only on recent values of the input. Since the dependence of each state update on the previous state value is what makes an RNN \"recurrent,\" and our model lacks that, we call it \"quasi-recurrent.\"\n\n3. QRNNs are a way of generalizing strongly-typed recurrent neural networks (Balduzzi and Ghifary, 2016) such that they can take into account more input timesteps at once, and except that they use a different set of nonlinearities and are implemented with an emphasis on performance.\n\n4. QRNNs with only a forget gate are query-reduction networks (Seo et al., 2016), except that they lack the query component and take into account more input timesteps at once.", "title": "Yes. There are many equally fair descriptions of the QRNN; that's one of them."}, "S180bwJXx": {"type": "review", "replyto": "H1zJ-v5xl", "review": "1) How do the f-fo-ifo poolings compare to a simple average/max pooling over fix temporal windows? Did you try to evaluate such model on a given task?\n\n2) How important is the temporal size of the input filters ?\n\n\n\n\n\nThis paper introduces a novel RNN architecture named QRNN.\n\nQNNs are similar to gated RNN , however their gate and state update  functions depend only on the recent input values, it does not depend on the previous hidden state. The gate and state update functions are computed through a temporal convolution applied on the input.\nConsequently, QRNN allows for more parallel computation since they have less  operations in their hidden-to-hidden transition depending on the previous hidden state compared to a GRU or LSTM. However, they possibly loose in expressiveness relatively to those models. For instance, it is not clear how such a model deals with long-term dependencies without having to stack up several QRNN layers.\n\nVarious extensions of QRNN, leveraging Zoneout, Densely-connected or seq2seq with attention, are also proposed.\n\nAuthors evaluate their approach on various tasks and datasets (sentiment classification, world-level language modelling and character level machine translation). \n\nOverall the paper is an enjoyable read and the proposed approach is interesting,\nPros:\n- Address an important problem\n- Nice empirical evaluation showing the benefit of their approach\n- Demonstrate up to 16x speed-up relatively to a LSTM\nCons:\n- Somewhat incremental novelty compared to (Balduzizi et al., 2016)\n\nFew specific questions:\n- Is densely layer necessary to obtain good result on the IMDB task. How does a simple 2-layer QRNN compare with 2-layer LSTM?  \n- How does the i-fo-ifo pooling perform comparatively? \n- How does QRNN deal with long-term time depency? Did you try on it on simple toy task such as the copy or the adding task? ", "title": "Pre-review questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1pm8p-Ve": {"type": "review", "replyto": "H1zJ-v5xl", "review": "1) How do the f-fo-ifo poolings compare to a simple average/max pooling over fix temporal windows? Did you try to evaluate such model on a given task?\n\n2) How important is the temporal size of the input filters ?\n\n\n\n\n\nThis paper introduces a novel RNN architecture named QRNN.\n\nQNNs are similar to gated RNN , however their gate and state update  functions depend only on the recent input values, it does not depend on the previous hidden state. The gate and state update functions are computed through a temporal convolution applied on the input.\nConsequently, QRNN allows for more parallel computation since they have less  operations in their hidden-to-hidden transition depending on the previous hidden state compared to a GRU or LSTM. However, they possibly loose in expressiveness relatively to those models. For instance, it is not clear how such a model deals with long-term dependencies without having to stack up several QRNN layers.\n\nVarious extensions of QRNN, leveraging Zoneout, Densely-connected or seq2seq with attention, are also proposed.\n\nAuthors evaluate their approach on various tasks and datasets (sentiment classification, world-level language modelling and character level machine translation). \n\nOverall the paper is an enjoyable read and the proposed approach is interesting,\nPros:\n- Address an important problem\n- Nice empirical evaluation showing the benefit of their approach\n- Demonstrate up to 16x speed-up relatively to a LSTM\nCons:\n- Somewhat incremental novelty compared to (Balduzizi et al., 2016)\n\nFew specific questions:\n- Is densely layer necessary to obtain good result on the IMDB task. How does a simple 2-layer QRNN compare with 2-layer LSTM?  \n- How does the i-fo-ifo pooling perform comparatively? \n- How does QRNN deal with long-term time depency? Did you try on it on simple toy task such as the copy or the adding task? ", "title": "Pre-review questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJMFbhRzg": {"type": "review", "replyto": "H1zJ-v5xl", "review": "To clarify, is it fair to say that the QRNN is exactly identical to dynamic average pooling or GRU / LSTM, except the gates are only computed as a function of x and do not include a dependence on h? And since a single x_t might be too little information you include a few of the last x's.\nThis paper points out that you can take an LSTM and make the gates only a function of the last few inputs  - h_t = f(x_t, x_{t-1}, ...x_{t-T}) - instead of the standard - h_t = f(x_t, h_{t-1}) -, and that if you do so the networks can run faster and work better. You're moving compute from a serial stream to a parallel stream and also making the serial stream more parallel. Unfortunately, this simple, effective and interesting concept is somewhat obscured by confusing language.\n\n- I would encourage the authors to improve the explanation of the model. \n- Another improvement might be to explicitly go over some of the big Oh calculations, or give an example of exactly where the speed improvements are coming from. \n- Otherwise the experiments seem adequate and I enjoyed this paper.\n\nThis could be a high value contribution and become a standard neural network component if it can be replicated and if it turns out to work reliably in multiple settings.\n", "title": "a better TLDR", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Bk3_qAxNg": {"type": "review", "replyto": "H1zJ-v5xl", "review": "To clarify, is it fair to say that the QRNN is exactly identical to dynamic average pooling or GRU / LSTM, except the gates are only computed as a function of x and do not include a dependence on h? And since a single x_t might be too little information you include a few of the last x's.\nThis paper points out that you can take an LSTM and make the gates only a function of the last few inputs  - h_t = f(x_t, x_{t-1}, ...x_{t-T}) - instead of the standard - h_t = f(x_t, h_{t-1}) -, and that if you do so the networks can run faster and work better. You're moving compute from a serial stream to a parallel stream and also making the serial stream more parallel. Unfortunately, this simple, effective and interesting concept is somewhat obscured by confusing language.\n\n- I would encourage the authors to improve the explanation of the model. \n- Another improvement might be to explicitly go over some of the big Oh calculations, or give an example of exactly where the speed improvements are coming from. \n- Otherwise the experiments seem adequate and I enjoyed this paper.\n\nThis could be a high value contribution and become a standard neural network component if it can be replicated and if it turns out to work reliably in multiple settings.\n", "title": "a better TLDR", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}