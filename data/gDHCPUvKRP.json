{"paper": {"title": "Sparse Linear Networks with a Fixed Butterfly Structure: Theory and Practice", "authors": ["Nir Ailon", "Omer Leibovitch", "Vineet Sreedharan Nair"], "authorids": ["~Nir_Ailon1", "~Omer_Leibovitch1", "~Vineet_Sreedharan_Nair1"], "summary": "Replacing a dense linear layer in any neural network with an architecture based on butterfly network.", "abstract": "A butterfly network consists of logarithmically many layers, each with a linear number of non-zero weights (pre-specified). The fast Johnson-Lindenstrauss transform (FJLT) can be represented as a butterfly network followed by a random projection to a subset of the coordinates. Moreover, a random matrix based on FJLT with high probability approximates the action of any matrix on a vector. Motivated by these facts, we propose to replace a dense linear layer in any neural network by an architecture based on the butterfly network. The proposed architecture significantly improves upon the quadratic number of weights required in a standard dense layer to nearly linear with little compromise in expressibility of the resulting operator. In a collection of wide variety of experiments, including supervised prediction on both the NLP and vision data, we show that this not only produces results that match and often outperform existing well-known architectures, but it also offers faster training and prediction in deployment. To understand the optimization problems posed by neural networks with a butterfly network, we study the optimization landscape of the encoder-decoder network, where the encoder is replaced by a butterfly network followed by a dense linear layer in smaller dimension. Theoretical result presented in the paper explain why the training speed and outcome are not compromised by our proposed approach. Empirically we demonstrate that the network performs as well as the encoder-decoder network.", "keywords": ["Butterfly Network", "Matrix approximation", "Encoder-Decoder Network", "Optimization Landscape"]}, "meta": {"decision": "Reject", "comment": "This paper shows that linear layers can be replaced by butterfly networks. Put simple, the paper follows the idea of sketching to design new architectures that can reduce the number of trainable parameters and also gives the theoretical and empirical analysis to validate this claim. In this regard, the paper would be  appealing.  But the theoretical results given in this paper are incremental."}, "review": {"-hP6wXmzUoX": {"type": "rebuttal", "replyto": "b4kjyvzuT0e", "comment": "We thank the reviewer for the additional feedback. \n\nWith respect to Fig1a, we have added a line in the new version stating that the black vertical lines denote the error bars corresponding to standard deviation, and the values above the rectangles denote the average accuracy.\n\nWith respect to the first point, we are sorry for not responding clearly to your question.  We feel that we may have misunderstood your concern, and appreciate your time reading our rather detailed response herein.\n\nOn the properties of natural data that make the butterfly structure work well: We do not know what it is about 'natural data' that makes the butterfly structure work well.  From the experimental side, we see that our approach seems promising on 'natural data'.  From the theoretical side, we know that even if we randomly permute the coordinates of the input data, thus breaking any natural temporal/spatial structure, the results would hold, because the temporal/spatial structure is not used in the analysis in any way.  In that sense our architecture is different from (say) convolutional layers, which are also sparse, and strongly rely on the temporal/spatial structure.  On the other hand, we do not claim that our method can be used as a replacement to CNN's, because obviously we do want to take advantage of the temporal/spatial structure of the signals when available.  In fact, our method is much more suitable for replacing dense layers in places where, incidentally, one would not think of putting a CNN.  Also incidentally, these dense layers can be bottleneck (in terms of computational resources) in training and testing. \n\nRegarding literature on the extensive known theory of approximating matrices with the same structure:  The references in the literature are indeed mentioned in the following papers, which are readily cited in our work. Ailon+Chazelle 2009, Ailon+Liberty 2009, Clarkson+Woodruff 2009, Krahmer+Ward 2011, Sarlos 2006. Some classic theory on which these works are based on is Johnson+Lindenstrauss 1984 (this is before the discovery of the fast Johnson-Lindenstrauss trasform, which gives rise to the butterfly structure).\n\nWe also cite some related theory on fast, low rank matrix approximation: Indyk+Vakilian+Yuan 2019, Pillai+Smith 2020 (although they do not use the butterfly structure, they use something else, and our experiments use their data).\n\nRegarding our remark in the first rebuttal: \"We note that the butterfly structure may work well not only on natural data, but also on synthetic data\" - By this we meant, as stated above here, that there is nothing in the theory that uses any property of \"nature\", and therefore we expect this scheme to work on any data, including synthetic.  In fact we even expect the method to \"work\" on contrived, deliberately crafted data in the sense that our approach would be, in terms of accuracy, on par with an architecture that uses dense layers naively, except that it would probably require fewer resources.  As a note: We would probably have to assume that the contrived data does not \"know\" about the initialization of the network, because with that knowledge it is possible to generate data that falls in the kernel of the dimension reducing linear transformation (the \"truncated butterfly\") , causing complete collapse of information.  But this is exponentially unlikely if the contrived data generation is unaware of the initialization.", "title": "Responses to the Queries"}, "2pXWN0DtXj": {"type": "review", "replyto": "gDHCPUvKRP", "review": "This paper proposes to impose a particular sparsity structure (butterfly network) to replace dense connected layers in deep neural networks. It is motivated by the theoretical results involving the Fast Johnson Lindenstrauss Transform (FJLT). The work is well motivated and experimental results validate the theoretical findings. However, it is not clear the advantage for the case of image datasets as CIFAR10 and CIFAR100. I have the following comments and questions that should be clarified to evaluate the relevance of the results:\n-\tWhen comparing with other architectures on image classification tasks (CIFAR10 and CIFAR 100), what is the dense layer that is replaced by the butterfly structure? Most very well-known architectures are based on large concatenation of convolutional layers with a dense layer at the end. Is this last layer the one that is replaced? What is the compression attained by this replacement? \n-\tIn this architecture, the sparsity pattern is fixed, and it seems to work very well for the dataset used in the paper. It would be interesting to provide some insights on why this particularly structure works well on natural data. Is there any property on datasets that makes butterfly pattern optimal?\n-\tFor natural images, it is well known that sparsity structure imposed by convolutional layers is optimal because of local structure of natural images. I think a comparison replacing a convolutional layer by a butterfly layer could bring some useful insights.\n-\tTheorem 1: What is Omega in the exponent? I couldn\u2019t find its definition.\n-\tFigure 1(a): The small difference between normal and BF models seems not to be statistically significant. It would be good to show some variability of results (error bars).\n-\t For the results of Figure 1(b), it would be also useful to report the training time reduction and model compression attained by the normal and BF models.\n-\tFigure 1(b): The starting point for the four methods should be the same. Please include in the plot the Test Accuracy at the beginning (epoch 0).\n-\tFigure 1(b): Please extend the plots beyond epoch 20 to see how the values are stabilized for the four methods.\n-\tQuality of Figures, for example Fig 3, must be improved (too small fonts, blurred, etc.)", "title": "Further details on experimental setting are required", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "_ZgaPzZMOEM": {"type": "rebuttal", "replyto": "u3JSxT0Oq8R", "comment": "We appreciate the significant effort of all the reviewers to deliver such detailed and constructive suggestions. We also appreciate that most reviewers identify the novelty of our work. We have prepared a new version of the document by incorporating all the suggestions. Specific queries raised by Reviewer 1 are addressed below.\n\n1) Theorems 1 and 2 are mostly direct consequences of existing theorems in sketching literature.\n\nResponse: We note that our main theoretical contribution is Theorem 3 (renamed as Theorem 1 in the new version), which is on the optimization landscape of the encoder-decoder network where the encoder is replaced as mentioned in Section 5.  Also, as pointed out by Reviewer 2 we feel Theorem 3 (renamed as Theorem 1 in the new version) in our paper raises important questions about the optimization landscape of butterfly networks. As adequately noted by Reviewer 1, Theorem 1 (renamed as Proposition 1 in the new version to avoid confusion) can be proved easily using the existing theorems. We prove it for completeness in paper. Theorem 2 (renamed as Proposition 2 in the new version to avoid confusion) is from Sarlos[06]. We state and use it (without proving it) for motivating our proposed replacement of the encoder layer.\n\n2) The paper strongly lacks motivation, and requires better motivation and further exploration of the utility of the ideas presented.\n\nResponse: The motivations of this work are two-fold: \na) practical aspect of deep learning: to reduce the number of weights from quadratic to (near) linear by replacing a dense linear layer in any neural network by the proposed butterfly architecture. This offers faster training and prediction in deployment while producing results that match and often outperform existing known architectures.\n\nIn the new version of the paper we have added the training times (see Figures 10 and 11 in Appendix D.1) required in the experiments in Section 6.1 (as suggested by Reviewers 3 and 4). This aptly quantifies the improvements in training times in our experiments.\n\nb) theoretical aspect of deep learning: Since the proposed replacement adds logarithmic depth to the architecture, it might pose convergence related issues. Hence to study theoretically the problems posed by such a replacement. We take a small step towards this by studying the optimization landscape of an encoder-decoder network where the encoder is replaced as mentioned in Section 5. Also see response to point 1.\n\nTo further demonstrate the utility of truncated butterfly networks, we consider a supervised learning approach as done by [IndykVY19], where we learn how to derive low rank approximations of a distribution of matrices by multiplying a pre-processing linear operator represented as a butterfly network, with weights trained using a sample of the distribution.\n\nWe have used a part of the additional page to add these points in the \u2018Discussion and Future Works\u2019 section of the paper to better convey the motivation of our work. We have also added the point about resilience to over-fitting as a future direction.\n\n3. It is unclear what the expected impact is.\n\nResponse: We expect this paper will help both `theory and practice of deep learning\u2019. As shown by the experiments of our paper the proposed replacement of a dense linear layer achieves the same loss and better training times compared to the original model. \nWith respect to theory, the convergence question related to the proposed replacement cannot be handled by the current tools. We expect this would spurn more techniques to address convergence related questions in deep networks with butterfly networks. We have added a few lines stating the potential impact of our work in 'Our contribution' (renamed 'Our contribution and Potential Impact' in the new version)\n", "title": " Responses to the Queries Raised by the Reviewer"}, "g3Bu2BD1gVU": {"type": "rebuttal", "replyto": "2pXWN0DtXj", "comment": "We appreciate the significant effort of all the reviewers to deliver such detailed and constructive suggestions. We also appreciate that most reviewers identify the novelty of our work. We have prepared a new version of the document by incorporating all the suggestions. Specific queries raised by Reviewer 4 are addressed below.\n\n1.  When comparing with other architectures on image classification tasks (CIFAR10 and CIFAR 100), what is the dense layer that is replaced by the butterfly structure? Most very well-known architectures are based on large concatenation of convolutional layers with a dense layer at the end. Is this last layer the one that is replaced? What is the compression attained by this replacement? \n\nResponse: Yes, in the experiments in Section 6.1 the last layer in all the considered architectures is replaced. We state this in footnote 5, page 6 (Also see response to point 3 of reviewer 3). We have added the exact parameter counts in Figures 7 and 8 in Appendix D.1 of our new version.\n\n2.  In this architecture, the sparsity pattern is fixed, and it seems to work very well for the dataset used in the paper. It would be interesting to provide some insights on why this particular structure works well on natural data. Is there any property on datasets that makes butterfly pattern optimal?\n\nResponse: We note that the butterfly structure may work well not only on natural data, but also on synthetic data. This is supported by the extensive known theory of approximating matrices with the exact same structure.\n\n3. For natural images, it is well known that sparsity structure imposed by convolutional layers is optimal because of local structure of natural images. I think a comparison replacing a convolutional layer by a butterfly layer could bring some useful insights.\n\nResponse: We include it as a possible future direction. We note that a convolutional layer is suitable in data which is represented in a basis that is induced by temporal or spatial structure, while our architecture works with any basis.\n\n4. Theorem 1: What is Omega in the exponent? I couldn\u2019t find its definition.\n\nResponse: We have renamed Theorem 1 as Proposition 1. In this \\Omega is the standard \u2018big Omega\u2019 notation. A note about this notation can be found here https://www.freecodecamp.org/news/big-omega-notation/\n\n5. Figure 1(a): The small difference between normal and BF models seems not to be statistically significant. It would be good to show some variability of results (error bars). For the results of Figure 1(b), it would be also useful to report the training time reduction and model compression attained by the normal and BF models.\n\nResponse: We have made the changes to Figure 1a, and added the training times and the number of parameters in the new version of our document. See Figures 7, 8, 10, 11 in Appendix D.1\n\n6.  Figure 1(b): The starting point for the four methods should be the same. Please include in the plot the Test Accuracy at the beginning (epoch 0). Figure 1(b): Please extend the plots beyond epoch 20 to see how the values are stabilized for the four methods. Quality of Figures, for example Fig 3, must be improved (too small fonts, blurred, etc.)\n\nResponse: We have incorporated the changes in the new version of our document. The plot for 20 epochs has been added in Figure 12, Appendix D.1.\n", "title": "Responses to the Queries Raised by the Reviewer"}, "G0jrL1LgYvv": {"type": "rebuttal", "replyto": "BQRr-tMzMQl", "comment": "We appreciate the significant effort of all the reviewers to deliver such detailed and constructive suggestions. We also appreciate that most reviewers identify the novelty of our work. We have prepared a new version of the document by incorporating all the suggestions. Specific queries raised by Reviewer 3 are addressed below.\n\n1. The paper says that the proposed method would speed up the training. But there is no quantitative argument. Also the paper does not report the computing time or memory used in all the experiments.\n\nResponse: We have added the training times and the parameter count to the new version.  See Figures 7, 8, 10, 11 in Appendix D.1.\n\n2. In Definition 4.1,  n is required to be a power of 2. And in the experiments, it seems that all the n are powers of 2 (except Tech). What do one do if  nis not a power of 2? \n\nResponse: Suppose n is not a power of 2 and let n\u2019 be the closest number to n that is greater than n and is a power of 2. Note that n\u2019<2n. Now take the truncated butterfly network of \\ell \\times n\u2019 and work with only the first n columns of this truncated butterfly network. We have added this as a footnote (page 4) in the new version of the document. \n\n3. In all the experiments, the final hidden layer is replaced by the proposed architecture. Is there any reason for doing this? How should one choose which layer(s) to be replaced?\n\nResponse: We experimented with the final layer. But we believe that multiple layers can be replaced simultaneously, and this is part of our ongoing research.\n\n4. It is argued that by replacing a dense layer with a truncated butterfly network and a dense layer, the number of parameters is reduced from kn to kl+O(nlog\u2061 l). But since the Big O can hide potentially large constants, I wonder how many parameters are used exactly. I expect the paper would report the parameter numbers in the experiments.\n\nResponse: The effective number of parameters in a \\ell x n truncated butterfly network is at most (2n\\log \\ell +6n). In the new version of our paper, we state this in the paragraph after Definition 4.1 (page 4), and include a proof of this fact in Appendix G for completeness.  We also state the exact parameters counts in Figures 7 and 8 in Appendix D.1. \n\n5. Theorem 1 shows that replacing a dense layer by two truncated butterfly networks and a dense layer will not be too different from the original network. But this holds only when J_1 and J_2 are sampled from the FJLT distribution. Since the weights J_1 and J_2 are updated throughout the training process, the theorem will only hold at initialization. Then what is the point of Theorem 1?\n\nResponse: Theorem 1, which is now proposition 1 (in the new version), shows that the new network has very little loss in representation compared to the old network with more parameters. In particular, it tells us that any solution in the original dense network is approximated w.h.p. in the modified sparse network, but as the reviewer adequately points out, it doesn't say what happens along the optimization path. Our experiments do indicate that learning is better and faster in the modified sparse network, and we leave the question of theoretically explaining that for future work.\n", "title": "Responses to the Queries Raised by the Reviewer "}, "to_WU1S7UJp": {"type": "rebuttal", "replyto": "2MXhCez_-Sb", "comment": "We appreciate the significant effort of all the reviewers to deliver such detailed and constructive suggestions. We also appreciate that most reviewers identify the novelty of our work. We have prepared a new version of the document by incorporating all the suggestions. Specific queries raised by Reviewer 2 are addressed below.\n\n1. The beginning of section 7 needs to be expanded with more details on the original Indyk et al 2019 paper -- the authors mention that they use the same setting as the 2019 paper but the details on how Indyk et al train their network are lacking without looking up the original paper.\n\nResponse: Due to the limit of eight pages we tried to convey only the main idea and presented the results. We have used the additional page to add the necessary details to improve the readability of this section. We have included a line on how  [IndyVY19] train their network. \n\n2. I think that low matrix approximation experimentation (Sec 7) can be more thorough. \n\na) Why are only three datasets (in Table 3) used? \n\nResponse a) The two datasets (HS-SOD and Tech) are used in [IndykVY19]. They have three more data sets generated from three videos on youtube respectively. Also, we feel the message and the utility of a truncated butterfly network is conveyed with the results established on the three datasets considered by us. \n\nb)Additionally and more importantly, Table 4 shows the approximation results for low values of k (max of 30) -- what happens when k=(min(n,d))? \n\nResponse:b) Firstly, in low rank matrix recovery the interesting regime is k<< n, and we show results for this interesting regime. That is the approximation error is only going to improve for large k. In particular for k=(min(n,d)) the error would be zero. Secondly, the results in [IndykVY19] are also established for k at most 30. \n\nc) Also, the error is measured with respect to the best rank k approximation of X (the eq. following eq. 20) While this way of measuring the error is often used in the low rank matrix approximation literature it is not very informative of the actual approximation quality of \\tilde X. It could well be the case that \\tilde X = X_k but the error ||X-\\tilde X||_F/||X||_F might be poor hence this way of measuring the approximation performance is a better indicator of how well X is actually approximated in practice.\n\nResponse c): As remarked this is often used in low-rank matrix approximation, and also used in [IndyVY19]. But as noted, the actual error  ||X-\\overline{X}||_F might be poor. This happens if ||X-X_k|| is large to begin with. But the || X-X_k|| is theoretically the best possible error that can be achieved (PCA), and in particular X_k is theoretically the best possible low-rank matrix recovery. Hence APP_{Te} is the best average expected error that one can hope for. This motivates why the error is measured with respect to the best rank k approximation of X.\n", "title": "Responses to the Queries Raised by the Reviewer"}, "BQRr-tMzMQl": {"type": "review", "replyto": "gDHCPUvKRP", "review": "I list the strong and weak points in the following:\n\n### Strong points\n\n- It is a novel idea (only to my knowledge) to combine the butterfly network and FJLT to optimize the neural network architectures. Specifically, the proposed method replaces a dense linear layer by a composition of three layers, which are smaller and can be computed faster by FJLT. \n\n\n### Weak points\n\n- The paper says that the proposed method would speed up the training. But there is no quantitative argument. Also the paper does not report the computing time or memory used in all the experiments.\n\n- In Definition 4.1, $n$ is required to be a power of 2. And in the experiments, it seems that all the $n$ are powers of 2 (except Tech). What do one do if $n$ is not a power of 2? \n\n- In all the experiments, the final hidden layer is replaced by the proposed architecture. Is there any reason for doing this? How should one choose which layer(s) to be replaced?\n\n- It is argued that by replacing a dense layer with a truncated butterfly network and a dense layer, the number of parameters is reduced from $kn$ to $k\\ell+O(n\\log\\ell)$. But since the Big O can hide potentially large constants, I wonder how many parameters are used exactly. I expect the paper would report the parameter numbers in the experiments. \n\n- Theorem 1 shows that replacing a dense layer by two truncated butterfly networks and a dense layer will not be too different from the original network. But this holds only when $J_1$ and $J_2$ are sampled from the FJLT distribution. Since the weights $J_1$ and $J_2$ are updated through the training process, the theorem will only hold at initialization. Then what is the point of Theorem 1?", "title": "Motivated by Johnson-Lindenstrauss-type results, this paper proposes to replace a dense linear layer in any neural network by a composition of two truncated butterfly networks and a smaller dense layer in between. Theoretical result is given for the two-layer encoder-decoder network where the encoder is replaced by one truncated butterfly network and a dense layer. Empirical results are given for several synthetic  and real datasets.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "2MXhCez_-Sb": {"type": "review", "replyto": "gDHCPUvKRP", "review": "\nThe paper provides an interesting and novel use of butterfly factoziations in encoder-decoder networks. Specifically, the paper proposes replacing the \nencoder with a truncated butterfly network followed by a dense linear layer. The parameters are chosen so as to keep the number of weights in the (replaced) encoder near linear in the input dimension. The authors provide a theoretical result related to auto-encoder optimization.\n\n######\n\nI vote for accepting the paper. The main reason is for my vote is that the main idea introduced is novel and the algorithmic contribution is substantial. \n\n######\n\npros \n+ The proposed truncated butterfly network is novel. Aside from the algorithmic contribution, the theorem in the paper raise important questions about\nthe optimization landscape of butterfly networks. \n+ The paper is clearly written and well justified \n+ Exhaustive literature survey and background on relevant work in both the matrix factorization and neural networks front\n\n######\n\ncons \n- The beginning of section 7 needs to be expanded with more details on the original Indyk et al 2019 paper -- the authors mention that they use the same setting as \nthe 2019 paper but the details on how Indyk et al train their network are lacking without looking up the original paper. \n\n- I think that low matrix approximation experimentation (Sec 7) can be more thorough. Why are only three datasets (in Table 3) used? Additionally and more importantly, Table 4 shows the approximation results for low values of k (max of 30) -- what happens when k=(min(n,d))? Also, the error is measured with respect to the best rank k approximation of X (the eq. following eq. 20) While this way of measuring the error is often used in the low rank matrix approximation literature it is not very informative of the actual approximation quality of \\tilde X. It could well be the case that \\tilde X = X_k but the error ||X-\\tilde X||_F/||X||_F might  be poor hence this way of measuring the approximation performance is a better indicator of how well X is actually approximated in practice.   \n\n######\n\nquestions: see the above section ", "title": "Interesting paper", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "u3JSxT0Oq8R": {"type": "review", "replyto": "gDHCPUvKRP", "review": "The paper studies \u201cbutterfly networks\u201d, where, a logarithmic number of linear layers with sparse connections resembling the butterfly structure of the FFT algorithm, along with linear layers in smaller dimensions are used to approximate linear layers in larger dimensions. In general, the paper follows the idea of sketching to design new architectures that can reduce the number of trainable parameters.  In that regard, the paper is very appealing, as it shows that replacing linear layers with the butterfly networks does not result in any loss in performance. \n\nThe paper\u2019s aim is to establish that linear layers can be replaced by butterfly networks and uses three different experiments to show this. The experiments do lend evidence to the claim that butterfly networks do not lead to a loss in performance. \n\nThe paper also present some theoretical results to show that the using butterfly networks sampled from the FJLT family preserves certain metric. However, my opinion is that Theorems 1 and 2 are mostly direct consequences of existing theorems in the sketching literature.\n\nFinally, although I appreciate the experiments in the paper, I feel strongly that the paper lacks motivation. The idea of sketching and using FJLT transforms have had a great impact on numerical linear algebra and generally in reducing computational complexity of algorithms. However, it is unclear from this paper what the expected impact is. Probably the authors could focus more on this. Possible directions are to show improved training times, or maybe even showing that such networks are more resilient towards overfitting. \n\nAs interesting as the experiments are, the paper needs a better motivation and further exploration of the utility of the ideas presented. For example, can using butterfly networks improve theory or practice of deep learning/machine learning? I do like the paper, but the paper can be much stronger and much more appealing if the motivation is better justified. \n\n", "title": "The paper studies the idea of butterfly networks, drawing inspiration from the sketching literature.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}