{"paper": {"title": "Poincar\u00e9 Wasserstein Autoencoder", "authors": ["Ivan Ovinnikov"], "authorids": ["ivan.ovinnikov@inf.ethz.ch"], "summary": "Wasserstein Autoencoder with hyperbolic latent space", "abstract": "This work presents the Poincar\u00e9 Wasserstein Autoencoder, a reformulation of\nthe recently proposed Wasserstein autoencoder framework on a non-Euclidean\nmanifold, the Poincar\u00e9 ball model of the hyperbolic space H n . By assuming the\nlatent space to be hyperbolic, we can use its intrinsic hierarchy to impose structure\non the learned latent space representations. We show that for datasets with latent\nhierarchies, we can recover the structure in a low-dimensional latent space. We\nalso demonstrate the model in the visual domain to analyze some of its properties\nand show competitive results on a graph link prediction task.", "keywords": ["Variational inference", "hyperbolic geometry", "hierarchical latent space", "representation learning"]}, "meta": {"decision": "Reject", "comment": "The paper received 3, 3, 6. All reviewers agree that the method is technically interesting. The main concern shared by the reviewers are the experiments which are somewhat underwhelming. The AC believes that this is a solid technical paper that needs a little bit more work. The authors are encouraged to strengthen their evaluation and resubmit to a future conference."}, "review": {"S1gDWKE3sB": {"type": "rebuttal", "replyto": "r1x-hoU6KH", "comment": "We thank the reviewer for the feedback. In the following, we will attempt to address some of the concerns raised: \n\n(1) It is unclear what is the major contribution of this paper over the existing work, e.g., [10,22,23]. The authors may want to provide more discussion regarding this in the literature review. \n\n-> The biggest difference to existing work is the use of the Wasserstein autoencoder formulation and corresponding Maximum Mean Discrepancy metric to match \nthe prior and posterior latent variable distributions. Although we do not explore the implications in our paper as the WAE formulation was \na design choice in order to circumvent the large gradient variance associated with MC ELBO approximation, the benefits of adopting the MMD metric\nas regularizer include the better handling of outliers depending on kernel selection, which results in an improved \noverlap between the prior and posterior.\nFurthermore, our work uses a different reparametrization scheme which allows for more complex dispersion representations of the posterior. \nAs stated in Mathieu et al., their work was developed concurrently to ours, which has previously been published as a workshop paper.\n\n(2 & 3) It is unclear where is the impact of the proposed PWA model in real applications. Also, it is unclear why the data like MNIST, CORA, CITESEER, PUBMED exhibit hierarchical data structure. \nThe experiments are questionable. In table 2, why  CORA, CITESEER, PUBMED are adopted as the evaluation datasets? why only VGAE is considered as the baseline methods in link predict? \n\n-> We agree that more experimental results on real-world datasets such as text hierarchies would be beneficial to showcase the PWA model's capabilities.\nWe explicitly state that we do not assume the MNIST to exhibit latent hierarchical structure. The motivation for the use of this dataset was mainly for expositional \npurposes to qualitatively demonstrate some of the properties of the hyperbolic latent space. \nThe citation network datasets may partially exhibit hierarchical structure in their graphs. Similar to [5], where the \npartial cyclical structure of these datasets is explored, it is a reasonable benchmark to show whether the model can capture the \npartial hierarchical structure of the network graphs in the latent space codes which would show improvement upon the Euclidean VGAE\nbaseline.\n\n[5] T. R. Davidson, L. Falorsi, N. De Cao, T. Kipf, and J. M. Tomczak. Hyperspherical variational\nauto-encoders.\n\n", "title": "Authors' response "}, "S1gJ8TVhsr": {"type": "rebuttal", "replyto": "Hyx-F2-otr", "comment": "We thank the reviewer for the detailed feedback and would like to address the concerns below:\n\nQ: Section 3.2, please explain the significance of \\lambda_x^c\n\n-> \\lambda_x refers to the conformal factor between the hyperbolic and the Euclidean metric tensors $g_H$ and $g_E$ \nrespectively. There is actually a typo in the third equation: it should be $g_H = \\lambda_x^2 g_E$\nThe constant c describes the radius of the Poincar\u00e9 ball and determines the degree of curvature of the \nhyperbolic space.\n\nQ: Please explain in the related work and/or introduce in more detail than you already have the differences and \nsimilarities between this work and your nearest neighbour \u201cHierarchical representations with poincar\u00e9 variational \nauto-encoders\u201d by Mathieu et al. (2019). I would like to know the advantages/disadvantages of using a Wasserstein formulation instead of a sampled ELBO.\n\n-> Typically the closed form version of the ELBO has significantly lower variance as opposed to the one-sample Monte\nCarlo ELBO gradient estimation. Based on this intuition, the choice was made to adopt the Wasserstein autoencoder\napproach (Section 4.4) since the Maximum Mean Discrepancy metric can be estimated using an unbiased minimum\nvariance U-statistic estimator (Eq. (7)). The Maximum Mean Discrepancy metric can be estimated using a number\nof samples of the magnitude of the dimensionality of the latent space. While the MMD does not require parametric\nassumptions about the form of the distributions, allowing for more flexible encoders and decoders, it is known to\nperform well when matching high dimensional Gaussian distributions.\nSimilar to the arguments presented in the original WAE paper, the use of kernel mean embedding via the Maximum Mean Discrepancy metric can be \nbeneficial for the matching of the prior and the posterior due to the kernel properties. On the other hand,\nthe selection of the kernel introduces further hyperparameters (the choice of the kernel and its bandwidth),\nwhich can be seen as a disadvantage. We will make sure to highlight these differences in the final revision of the paper.\n\nQ: Please explain in the related work and/or introduce in more detail than you already have the differences and \nIn your section on \u201cDispersion representation\u201d you state: \u201cSince the maximum mean discrepancy can be estimated via \nsamples, we do not require a closed form definition of the posterior density as is the case with training using the evidence lower bound. \nThis allows the model to learn richer latent space representations.\u201d Why exactly does not using a closed form posterior density \nlead to a richer latent space representation? Please back up this statement.\n\n-> The closed form of the Gaussian posterior in hyperbolic space allows for a scalar dispersion. Our model allows \nthe use of vector dispersions (analogous to diagonal covariance in Euclidean space) as well as dense matrix dispersions \nsince we perform the dispersion scaling in the tangent space as opposed to implicit reparametrization in Mathieu et al. \nWe will make sure to present a more clear formulation of the statement in the final version of the paper.\n\nQ: Experimentally it would have been nice to have a direct comparison between this method and Mathieu et al. (2019), \nthat paper being very similar to this one. At least a shared experiment would have been useful.\n\n-> Both Mathieu et al. and our work uses the MNIST dataset to demonstrate the latent space code distribution. \nWe have also performed a number of more quantitative experiments on MNIST which expand on the reconstruction scores\nincluded in the appendix and will gladly include them in the revised version of the paper.\n\nQ: Novelty of the method\n\n-> As stated in Mathieu et al., their work has been developed concurrently with multiple authors, one of \nwhich is this work which has previously been published as a workshop paper.\n\nQ: Experimental results\n\n-> In our link prediction experiments, we show that the PWA model outperforms the Euclidean baseline \non two of the datasets. In general, it is not trivial to determine which datasets could exhibit a latent space, hence an initial \nexploratory exposition is presented. Further work directed towards methods of identifying datasets with \nlatent hierarchies could be a great avenue to explore.\n\nQ:\n- In section 3.3, please point readers forward to the Appendix for a list of gyrovector operations. \n- Please use equation numbers on all equations. \n- In the presentation of the ELBO, there is a typo after the second equality where p(z) is listed twice.\n- This may just be personal preference on my behalf, but I think a short treatment on VAEs and optimal transport would \nbe useful in the related work and background sections.\n\n-> We will be happy to address these issues in the final revision. \n\n", "title": "Authors' response"}, "SkedF54hsS": {"type": "rebuttal", "replyto": "H1eOuBThKr", "comment": "We thank the reviewer for the feedback and will attempt to address the concerns in the following:\n\nQ: Overall, I think this work is interesting. My main concern is about the experiments. \nThe results on MNIST is not sufficient to demonstrate the usefulness of the proposed method. \nAs the authors mentioned, a potential reason is that the MNIST is not a typical hierarchical dataset. \nI suggest adding experiments on real-world hierarchical datasets, e.g., representing/generating sentences (which can be viewed as trees of words) or \narticles (which can be viewed as trees of sections/subsections). \n\n-> Of the three experiments we have provided in the paper, the intention behind the MNIST experiment was to shed light on some of the \nproperties of the hyperbolic latent space in a qualitative way. We have also performed some quantitative experiments on MNIST which expand \nupon the reconstruction scores recorded in the appendix figures. We will make sure to include these in the final revision.\nThe citation network datasets that are used in the link prediction exhibit partial hierarchical structure. \nThe motivation for using the link prediction task as a benchmark was due to the fact that the model could capture\nsome hierarchical properties of graphs in the latent space representations \nand leverage this capability in order to improve upon the Euclidean baseline scores.\nWe agree with the suggestion that more experiments involving real-world hierarchical datasets could provide useful\nto illustrate the benefits of using the hierarchy inducing properties of the hyperbolic latent space. \n", "title": "Authors' response"}, "Hyx-F2-otr": {"type": "review", "replyto": "BJgLpaEtDS", "review": "*Paper summary*\n\nThe authors endow a variational auto-encoder with a hyperbolic space for the latent space. This enables them to model hierarchically ranked relationships in the data in a natural way. They use a Wasserstein formulation instead of the standard ELBO, which they claim reduces the variance of the gradients during training.\n\n*Paper decision*\n\nThanks for an interesting paper, I enjoyed reading it. While there are parts of this paper I like very much and I think it is technically sound, I feel the experiments section is lacking somewhat (see comments below). I therefore am recommending a weak reject; although, upon a favourable rebuttal would happily upgrade this position.\n\n*Supporting arguments*\n\n- In the introduction is well written and the motivation for the introduction of a hyperbolic later space is well laid out.\n- The section on Hyperbolic geometry is very well written indeed and presents this non-trivial material in a very simple to understand manner. Maybe what it lacks a bit is motivation for a machine learning audience, for why these mathematical tools from differential geometry would be useful or needed by the community to tackle presenting machine learning problems.\n- The structure of the method is well laid out and every hyperbolic version of an auto-encoder operation is explained. I would prefer to see methods from cited works, which are deployed in this paper, written in the paper instead of just referred to, so that I don\u2019t have to search through the cited works to find out how a key operation works.\n- I think the method itself is technically sound. \n- I am not sure about the novelty of the method, seeing that there is a very similar paper from earlier this year (Mathieu et al. 2019).\n- The experiments are lacking a little. I do not feel that they show a clear reason why a hyperbolic latent space should be used and compared to standard methods, they do not perform any. better. It would have been nice to have a more structured discussion on the merits of the method.\n\n*Questions/notes for the authors*\n\n- Section 3.2, please explain the significance of \\lambda_x^c\n- Please explain in the related work and/or introduce in more detail than you already have the differences and similarities between this work and your nearest neighbour \u201cHierarchical representations with poincar\u00e9 variational auto-encoders\u201d by Mathieu et al. (2019). I would like to know the advantages/disadvantages of using a Wasserstein formulation instead of a sampled ELBO.\n- In section 3.3, please point readers forward to the Appendix for a list of gyrovector operations.\n- Please use equation numbers on all equations. \n- In your section on \u201cDispersion representation\u201d you state: \u201cSince the maximum mean discrepancy can be estimated via samples, we do not require a closed form definition of the posterior density as is the case with training using the evidence lower bound. This allows the model to learn richer latent space representations.\u201d Why exactly does not using a closed form posterior density lead to a richer latent space representation? Please back up this statement.\n- In the presentation of the ELBO, there is a typo after the second equality where p(z) is listed twice.\n- This may just be personal preference on my behalf, but I think a short treatment on VAEs and optimal transport would be useful in the related work and background sections.\n- Experimentally it would have been nice to have a direct comparison between this method and Mathieu et al. (2019), that paper being very similar to this one. At least a shared experiment would have been useful.\n\n\n\n\n\n", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 2}, "H1eOuBThKr": {"type": "review", "replyto": "BJgLpaEtDS", "review": "In this paper, the authors proposed a Poincare Wasserstein autoencoder for representing and generating data with latent hierarchical structures. \nThe proposed model extends the Wasserstein autoencoder to the hyperbolic space. It is a new and powerful member of the family of VAEs. \nA hyperbolic Gaussian reparametrization method is designed and a Wasserstein loss with MMD regularizer is applied as an objective function.\nThe paper is well-organized and easy to read. The notations are clear. \n\nOverall, I think this work is interesting. My main concern is about the experiments. \nThe results on MNIST is not sufficient to demonstrate the usefulness of the proposed method. \nAs the authors mentioned, a potential reason is that the MNIST is not a typical hierarchical dataset. \nI suggest adding experiments on real-world hierarchical datasets, e.g., representing/generating sentences (which can be viewed as trees of words) or articles (which can be viewed as trees of sections/subsections).", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 2}, "r1x-hoU6KH": {"type": "review", "replyto": "BJgLpaEtDS", "review": "The work extend the variational autoencoder model to the hyperbolic space for exploring the hierarchical data structures. Experimental results on the synthetic and real data sets show the performance of the proposed model. In general, the paper is well organized.  The proposed algorithm is promising. My main concern is the motivation of the paper and the experiments. In particular, \n \n(1) It is unclear what is the major contribution of this paper over the existing work, e.g., [10,22,23]. The authors may want to provide more discussion regarding this in the literature review. \n(2) It is unclear where is the impact of the proposed PWA model in real applications. Also, it is unclear why the data like MNIST, CORA, CITESEER, PUBMED exhibit hierarchical data structure. \n(3) The experiments are questionable. In table 2, why  CORA, CITESEER, PUBMED are adopted as the evaluation datasets? why only VGAE is considered as the baseline methods in link predict? ", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 3}}}