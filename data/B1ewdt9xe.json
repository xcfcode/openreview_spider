{"paper": {"title": "Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning", "authors": ["William Lotter", "Gabriel Kreiman", "David Cox"], "authorids": ["lotter@fas.harvard.edu", "gabriel.kreiman@tch.harvard.edu", "davidcox@fas.harvard.edu"], "summary": "", "abstract": "While great strides have been made in using deep learning algorithms to solve supervised learning tasks, the problem of unsupervised learning - leveraging unlabeled examples to learn about the structure of a domain - remains a difficult unsolved challenge. Here, we explore prediction of future frames in a video sequence as an unsupervised learning rule for learning about the structure of the visual world. We describe a predictive neural network (\"PredNet\") architecture that is inspired by the concept of \"predictive coding\" from the neuroscience literature. These networks learn to predict future frames in a video sequence, with each layer in the network making local predictions and only forwarding deviations from those predictions to subsequent network layers. We show that these networks are able to robustly learn to predict the movement of synthetic (rendered) objects, and that in doing so, the networks learn  internal representations that are useful for decoding latent object parameters (e.g. pose) that support object recognition with fewer training views. We also show that these networks can scale to complex natural image streams (car-mounted camera videos), capturing key aspects of both egocentric movement and the movement of objects in the visual scene, and the representation learned in this setting is useful for estimating the steering angle. These results suggest that prediction represents a powerful framework for unsupervised learning, allowing for implicit learning of object and scene structure.", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes an interesting architecture for predicting future frames of videos using end-to-end trained deep predictive coding.\n  The architecture is well presented and the paper is clearly written. The experiments are extensive and convincing, include ablation analyses, and show that this architecture performs well compared to other current methods.\n Overall, this is an interesting, solid contribution."}, "review": {"ByWUtKT7g": {"type": "rebuttal", "replyto": "r1AFGcEfx", "comment": "# Motivation\nWhile predictive coding can be formulated in a few different ways, the general idea as it pertains to neuroscience is that the brain is constantly making predictions of incoming stimuli. These predictions take a hierarchical form where feedback (and perhaps lateral) connections convey the predictions and feedforward signals convey the error between predictions and actual observations (e.g. Rao & Ballard 1999).  Predictive coding theories also postulate that there are distinct neural populations that represent the state of the world and make predictions and those which communicate the errors (ex. Kanai et al. 2015). We used all of these principles in designing the PredNet. The model makes hierarchical predictions, based off of top-down and lateral informational flow from \u201crepresentation\u201d neurons, which are recurrent to be designed to represent the world state. The predictions are compared against layer-specific targets, from which \u201cerror\u201d signals are generated, which are then propagated to inform an update of the representation neurons.\n\n# The PredNet Model\nA fundamental insight that inspired this work is the idea that accurately predicting the world requires an (at least) implicit model of the structure of the world.  We evaluated this by testing how the model represented the underlying latent factors, representing the \u201cstructure of the world\u201d.  For the face stimuli, we show that the model, which was solely trained to predict future video frames, learns a representation suitable for decoding pose and identity parameters of the face, which supports classification. We assess this through the use of simple linear readouts on the learned representation. This is a common way to ascertain representation quality, which has roots in neuroscience (ex. Hung, Kreiman, Poggio, DiCarlo, (2005); DiCarlo & Cox (2007)) and machine learning (ex. Radford, Metz, Chintala, ICLR (2016)).  For the car-cam datasets, we show that the model again learns a representation that is useful for decoding a key latent parameter, namely the steering angle.\n\n# Experiment 3.1\nEach sequence contained a unique face, so there were 16,000 different faces in training. The faces were randomly sampled from FaceGen, which models components such as age, gender, race, facial features, etc., designed to model the natural variabilities in the human population.\n\nThe idea is that the representation is learned in an unsupervised way and then only a simple linear classifier is trained to decode the variable of interest. This is a common way to assess an \u201cunsupervised\u201d method; for a couple examples, this is used in Radford et al. ICLR 2016 and Wang & Gupta ICCV 2015.\n\nSee note above on the comparisons we have now added. The original reasons that we didn\u2019t prioritize comparisons in prediction evaluation (note we did for comparing on the supervised tasks) are as follows:  1) We felt it was more instructive to have a tighter control for the more novel parts of our architecture by comparing against the CNN-LSTM Encoder-Decoder. 2) The evaluation of generative models is still very much an open problem (see Theis et al. (2016)). 3) As next frame prediction is still a relatively newly explored problem, a standard dataset has yet to emerge, which leads to some difficulty in comparing models. For instance, we were concerned that comparing competing models on our dataset was unfair, since those models are reported with hyperparameters optimized for different datasets. However, we do agree that adding comparisons is useful, so we have retrained our PredNet model on two datasets from recent papers to compare performance, and we have added these comparisons to the paper.  \n\n# Experiment 3.2\nBy linear readout we mean a linear classifier that is used to decode a variable of interest from a representation. \nThe input to the fully supervised models is the image frame for which the steering angle is to be estimated. Please see note posted above regarding the steering angle analysis.\nWe have not noticed any obvious differences between prediction accuracy depending on moving objects versus ego motion, but a more detailed analysis could reveal more subtle differences.\n\n# Discussion\nWe have shown a few additional latent parameters that can be decoded in Fig. 3, including parameters related to object identity and motion.  We believe that more is possible, and we are excited to explore other possible scene parameters that could potentially be decoded from PredNet representations.  We are working on developing some of these methods currently, though we believe that such efforts are beyond the scope of the current paper.\n\nThis is a very interesting question. We believe that we have provided both qualitative and quantitative evidence that the PredNet learns an implicit model of the objects and scenes on which it is trained. For the face stimuli, we show that our model learns a representation that is suitable for decoding underlying latent parameters. In examining the prediction plots, it is also clear that the model has at least implicitly learned some properties of faces. For instance, in the top row of Figure 3, the model predicts the appearance of the right ear when it comes into view, even though it hadn\u2019t seen the ear in previous frames. For the car-cam datasets, it is also evident that the model learns at least some properties of natural scenes. In the top plot in Figure 1 (for an enlarged view, see https://coxlab.github.io/prednet/), the model predicts that the oncoming car will appear bigger as it approaches. It also fills in the road and tree texture behind the car. While it is open for debate what it means to fully learn a \u201cmodel\u201d of objects and scenes, it is clear that the PredNet learns a non-trivial and useful representation of the stimuli on which it is trained.\nWe would argue that the difference between learning an implicit model and making it easier for supervised learning to extract model parameters is at least partly a philosophical distinction \u2013 interrogating an implicit model requires a definition of what you want to extract of it, and this requires at least some labels. In any event, even if one does not accept easy supervised readout as evidence of an implicit \u201cmodel\u201d, we nonetheless argue that the development of unsupervised methods for improving accuracy or reducing the number of labeled training examples needed in subsequent supervised training is intrinsically useful and is an important direction in deep learning research.\n\n", "title": "Response to Questions"}, "HyBXIKT7e": {"type": "rebuttal", "replyto": "ryJJRKCzg", "comment": "1. Yes, when we began testing these models, we tried different combinations of layer losses, but when evaluated in terms of next frame prediction, having losses at upper layers empirically didn\u2019t help, and a systematic hyperparameter search excluded architectures that include loss on the upper layers.  This makes sense, since if the models are evaluated on the basis of predictions at the lowest layer, loss should be focused at this layer.  When we continued experimenting, given the large number of other hyperparameters, we decided to simply fix the loss to be only at the lowest layer.  Besides simplifying the hyperparameter search, this also allows us to frame the problem as \u201cwhat do we get for free?\u201d when solely predicting pixels, which is partly the motivation for the work.  It could be the case that loss on upper layers makes it a harder optimization problem because the ground-truth is indeed only \u201cgrounded\u201d at this layer.  Perhaps adding different constraints or regularizers on the objective could help this problem, which we plan to more thoroughly explore in future work.  We note that, for the Ladder Network, which also potentially have losses at each layer, the published model had weights on upper layers that were 100 to 1000 times smaller than the loss on the lowest layer, so it is likely those authors ran into similar issues.\n\n2. This is a good question and intuition, and indeed, performance does increase by ~10% if the classification is done at later timesteps.  We chose to report the results after just 1 timestep because we wanted to emphasize that this is a \u201cstatic\u201d face classification task, quite different from prediction, and that the results would be most comparable to the other networks if we used the first timestep.  \n\n3. We apologize that our description was unclear.  We have since moved much of the analysis to the appendix, where we can properly explain all the details and keep only the essential analysis in the main text.  The autoencoder was trained to reconstruct single frames and then the representation from a single frame was used to estimate the steering angle.  This mainly serves as a simple baseline approach.  The CNN models were also trained using single frames in an end-to-end supervised fashion with a MSE loss.  As detailed in Bojarski et al. (2016), the thought is that such models can learn cues from, for instance, the lines in the road, to estimate the steering angle.  If estimation of the current steering angle was the sole purpose, these models would almost certainly benefit from the inclusion of dynamics.  This is outside of the scope of the current paper, so we have de-emphasized the comparisons to these models.\n\n", "title": "Response to Questions"}, "H1OurtpXe": {"type": "rebuttal", "replyto": "B1MDvDyQg", "comment": "1.  We have updated our manuscript to clarify the differences between the models.  While both our model and the model of Chalasani & Principe (2013) have a hierarchical structure motivated by ideas of predictive coding, the actual implementations of each model are quite different. The building blocks of the Chalasani & Principe model are linear dynamical systems and sparse coding and their model is trained in a greedy layer-wise fashion. Our model is built using CNN and LSTM components and is trained directly using gradient descent. The specific formulation of our model, with its four distinct modules per layer, and the state update algorithm is also quite different.  Altogether, we are able to train on more complex sequences and also explicitly demonstrate that the representations learned are useful for auxiliary tasks.\n\n2.  While standard future-frame evaluation benchmarks have arguably not yet emerged, we agree that this comparison is useful, and we have added direct comparison to state-of-the-art models.\n\n3. The MSE improvement is by roughly a factor of two; in carefully visually examining the prediction vs. actual sequences, it is clear that the model offers substantial improvements over just copying the previous frame.  It is easier to see this in the video examples at https://coxlab.github.io/prednet/. \n", "title": "Response to Questions"}, "BJiyrKTQx": {"type": "rebuttal", "replyto": "HJZHiB8Xe", "comment": "Thank you for the suggestion. We have posted a video to help illustrate the information flow in the network here: https://coxlab.github.io/prednet/prednet_animation.html. We originally omitted the boundary conditions in Eq. 1-4 for conciseness, but the full algorithm, including the boundary conditions and update rules can now be found in Algorithm 1.\n\nRegarding Figure 6: This is indeed a test sequence.  The fine-tuning was still done on the KITTI dataset, as in the original training, and testing was done on the Caltech dataset. We have updated the text to clarify this point.\n", "title": "Response to questions"}, "ByfN4FTmg": {"type": "rebuttal", "replyto": "B1ewdt9xe", "comment": "In response to the helpful comments and questions, we have made several changes to the manuscript:\n\n1.  In our original manuscript, we primarily compared the PredNet to a CNN-LSTM Encoder-Decoder, which we chose because it serves as a tight control for the more novel elements of our architecture. However, we agree that it is useful to compare against other published architectures.  One reason that this isn\u2019t a trivial task is because a standard benchmark for next frame prediction arguably has yet to be established.  Another issue is that published models are often optimized for performance on particular datasets, so evaluating competing models on KITTI/CalTech isn\u2019t necessarily fair to those models.  Searching the very recent literature, we found that the most relevant comparison to make is probably against the DFN model by Brabandere et al. (2016), which was recently presented at NIPS and was developed concurrently with our work.  One of their experiments was on a 64x64 pixel, grayscale car-cam dataset.  Training our KITTI model on this dataset, we outperform their results by 29%.  To compare against another concurrently developed model, also published at NIPS 2016, we have additionally evaluated on the Human3.6M dataset (Ionescu et al., 2014).  Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).  We have added all of these comparisons to the appendix.\n\n2.  To make the main text more clear and concise, and to properly explain all of the necessary details, we have moved portions of the steering angle analysis to the appendix.  Our main point has been to demonstrate that our model learns a representation of important underlying factors, using other models as points of reference, so we have emphasized this.\n\nAt the reviewer\u2019s suggestion, we have added a video clip to help illustrate the flow of information in the network: https://coxlab.github.io/prednet/prednet_animation.html.\n", "title": "Update for all reviewers and commenters"}, "HJZHiB8Xe": {"type": "review", "replyto": "B1ewdt9xe", "review": "The results that progressively over time adapt and improve is very interesting to see.\nUnfortunately, from the Figures and description I cannot say I exactly understand how information flows, e.g., boundary conditions are missing from Eq. 1-4 , which makes then hard to follow.\nIf you could help us follow better the information flow in the model, (maybe with a gif? ) that would be great, since the model architecture is very different than what most people have seen so far.\n\nOne question is: In your Figure 6, the finetuned model, has it seen this particular video sequence during finetuning, or it is a test sequence you are showing?\nAn interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence.\n\nClarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper.\n\n \"Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).\"\n\n It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.\n\n", "title": "Model architecture - finetuning", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rkGabqHNg": {"type": "review", "replyto": "B1ewdt9xe", "review": "The results that progressively over time adapt and improve is very interesting to see.\nUnfortunately, from the Figures and description I cannot say I exactly understand how information flows, e.g., boundary conditions are missing from Eq. 1-4 , which makes then hard to follow.\nIf you could help us follow better the information flow in the model, (maybe with a gif? ) that would be great, since the model architecture is very different than what most people have seen so far.\n\nOne question is: In your Figure 6, the finetuned model, has it seen this particular video sequence during finetuning, or it is a test sequence you are showing?\nAn interesting architecture that accumulates and continuously corrects mistakes as you see more and more of a video sequence.\n\nClarity: The video you generated seems very helpful towards understanding the information flow in your network, it would be nice to link to it from the paper.\n\n \"Our model with hyperparameters optimized for KITTI underperforms the model of Finn et al. (2016), but outperforms the previous state-of-the-art model by Mathieu et al. (2016).\"\n\n It is not clear how different are the train and test sequences at the moment, since standard benchmarks do not really exist for video prediction and each author picks his/her favorite. Underperforming Finn et al 206 at the H3.6m Walking videos is a bit disappointing.\n\n", "title": "Model architecture - finetuning", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B1MDvDyQg": {"type": "rebuttal", "replyto": "B1ewdt9xe", "comment": "\nWhat's the difference between this work and \"Deep Predictive Coding Networks\" (ref [4]).\nPlease explain it clearly in the text.\n\nWhy not comparing the performance of prednet with the previous work of authors(ref[25])?\n\nThe improvement in MSE of prednet over previous frame prediction is 0.005. How significant is this?\nMaybe you could provide images showing the difference between images in t+1 and t, and between t+1 and your prediction.\nThis could reveal in which regions prednet does better than previous frame prediction. ", "title": "Questions"}, "ryJJRKCzg": {"type": "review", "replyto": "B1ewdt9xe", "review": "Most of the questions I had on reading the paper were answered by the ablative analysis in the appendix. Thank you for having that!\n\n(1) For the experiments on natural image sequences (KITTI dataset), it is mentioned that the models were trained with loss only on the activity of the lowest error layer. Did having the loss at higher layers as well not help ? Since the loss is \"grounded\" in the lowest layer only and all other layers are measuring losses in learned representations, I would suspect it would make this a hard optimization problem. Is that the only reason or is something else at play ?\n\n(2) For the classification experiments, the representations were read out after one step of running the model. Does it make sense/help to run the model for longer, feeding in the same image at each step ? In other words, since one would expect the prediction after 1 step to not be very sharp, are there any benefits to letting the model recurse on the same image till it can make sharper predictions ?\n\n(3) For the steering angle experiments, steering angle is a function of an image sequence, not a single image. It is not immediately clear how the Autoencoder Model (Fig 5, right) uses sequence information in predicting the steering angle. Also for the comma.ai model and ResNet50 could you please clarify how these were trained ? A brief description of what the objective function was, and where sequence information was used would help put these models in context.Paper Summary\nThis paper proposes an unsupervised learning model in which the network\npredicts what its state would look like at the next time step (at input layer\nand potentially other layers).  When these states are observed, an error signal\nis computed by comparing the predictions and the observations. This error\nsignal is fed back into the model. The authors show that this model is able to\nmake good predictions on a toy dataset of rotating 3D faces as well as on\nnatural videos. They also show that these features help perform supervised\ntasks.\n\nStrengths\n- The model is an interesting embodiment of the idea of predictive coding\n  implemented using a end-to-end backpropable recurrent neural network architecture.\n- The idea of feeding forward an error signal is perhaps not used as widely as it could\n  be, and this work shows a compelling example of using it. \n- Strong empirical results and relevant comparisons show that the model works well.\n- The authors present a detailed ablative analysis of the proposed model.\n\nWeaknesses\n- The model (esp. in Fig 1) is presented as a generalized predictive model\n  where next step predictions are made at each layer. However, as discovered by\nrunning the experiments, only the predictions at the input layer are the ones\nthat actually matter and the optimal choice seems to be to turn off the error\nsignal from the higher layers. While the authors intend to address this in future\nwork, I think this point merits some more discussion in the current work, given\nthe way this model is presented.\n- The network currently lacks stochasticity and does not model the future as a\n  multimodal distribution (However, this is mentioned as potential future work).\n\nQuality\nThe experiments are well-designed and a detailed analysis is provided\nin the appendix.\n\nClarity\nThe paper is well-written and easy to follow.\n\nOriginality\nSome deep models have previously been proposed that use predictive coding.\nHowever, the proposed model is most probably novel in the way it feds back the\nerror signal and implements the entire model as a single differentiable\nnetwork.\n\nSignificance\nThis paper will be of wide interest to the growing set of researchers working\nin unsupervised learning of time series. This helps draw attention to\npredictive coding as an important learning paradigm.\n\nOverall\nGood paper with detailed and well-designed experiments. The idea of feeding\nforward the error signal is not being used as much as it could be in our\ncommunity. This work helps to draw the community's attention to this idea.", "title": "Why loss at input layer only and other questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "H1_DwlWEl": {"type": "review", "replyto": "B1ewdt9xe", "review": "Most of the questions I had on reading the paper were answered by the ablative analysis in the appendix. Thank you for having that!\n\n(1) For the experiments on natural image sequences (KITTI dataset), it is mentioned that the models were trained with loss only on the activity of the lowest error layer. Did having the loss at higher layers as well not help ? Since the loss is \"grounded\" in the lowest layer only and all other layers are measuring losses in learned representations, I would suspect it would make this a hard optimization problem. Is that the only reason or is something else at play ?\n\n(2) For the classification experiments, the representations were read out after one step of running the model. Does it make sense/help to run the model for longer, feeding in the same image at each step ? In other words, since one would expect the prediction after 1 step to not be very sharp, are there any benefits to letting the model recurse on the same image till it can make sharper predictions ?\n\n(3) For the steering angle experiments, steering angle is a function of an image sequence, not a single image. It is not immediately clear how the Autoencoder Model (Fig 5, right) uses sequence information in predicting the steering angle. Also for the comma.ai model and ResNet50 could you please clarify how these were trained ? A brief description of what the objective function was, and where sequence information was used would help put these models in context.Paper Summary\nThis paper proposes an unsupervised learning model in which the network\npredicts what its state would look like at the next time step (at input layer\nand potentially other layers).  When these states are observed, an error signal\nis computed by comparing the predictions and the observations. This error\nsignal is fed back into the model. The authors show that this model is able to\nmake good predictions on a toy dataset of rotating 3D faces as well as on\nnatural videos. They also show that these features help perform supervised\ntasks.\n\nStrengths\n- The model is an interesting embodiment of the idea of predictive coding\n  implemented using a end-to-end backpropable recurrent neural network architecture.\n- The idea of feeding forward an error signal is perhaps not used as widely as it could\n  be, and this work shows a compelling example of using it. \n- Strong empirical results and relevant comparisons show that the model works well.\n- The authors present a detailed ablative analysis of the proposed model.\n\nWeaknesses\n- The model (esp. in Fig 1) is presented as a generalized predictive model\n  where next step predictions are made at each layer. However, as discovered by\nrunning the experiments, only the predictions at the input layer are the ones\nthat actually matter and the optimal choice seems to be to turn off the error\nsignal from the higher layers. While the authors intend to address this in future\nwork, I think this point merits some more discussion in the current work, given\nthe way this model is presented.\n- The network currently lacks stochasticity and does not model the future as a\n  multimodal distribution (However, this is mentioned as potential future work).\n\nQuality\nThe experiments are well-designed and a detailed analysis is provided\nin the appendix.\n\nClarity\nThe paper is well-written and easy to follow.\n\nOriginality\nSome deep models have previously been proposed that use predictive coding.\nHowever, the proposed model is most probably novel in the way it feds back the\nerror signal and implements the entire model as a single differentiable\nnetwork.\n\nSignificance\nThis paper will be of wide interest to the growing set of researchers working\nin unsupervised learning of time series. This helps draw attention to\npredictive coding as an important learning paradigm.\n\nOverall\nGood paper with detailed and well-designed experiments. The idea of feeding\nforward the error signal is not being used as much as it could be in our\ncommunity. This work helps to draw the community's attention to this idea.", "title": "Why loss at input layer only and other questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "r1AFGcEfx": {"type": "review", "replyto": "B1ewdt9xe", "review": "# Motivation\nThe leap/link from `predictive coding' to the `PredNet' architecture is not very clear from the text.\nCan you clarify this?\n\n# The PredNet Model\nHow did you decide that the `PredNet' architecture was good for deducing the structure of the world by predicting future video frames? This is unclear from the text. Was it arrived at through trial and error? Is there prior work that has already shown the architecture to be promising for this application?\n\n# Experiment 3.1\nHow many different faces are there? What kinds of variations are present across the different faces?\n\nDeducing the latent orientations/categories of the faces from the representation of the network via ridge-regression/linear-svm respectively is a supervised learning process, is that right?\nDoesn't that go against the philosophy of deducing the object/s structure and properties from video in an unsupervised manner?\n\nWho proposed the CNN-LSTM decoder? \nWhy not compare your results to other methods that predict future image frames (in your paper you cite 9 papers predicting future frames)?\n\n# Experiment 3.2\nWhat is a linear readout?\nWhat is the input to the fully supervised methods?\nDo you notice any differences in prediction accuracy between sequences in which there are moving objects vs sequences in which only ego-motion is present?\n\n# Discussion\nWhat other structures (apart from single object pose, and steering angle) do you think could be deduced using this method?\nHow did you address the original goal of deducing an object's structure in natural scenes?\n\n\"predicting future frames requires at least an implicit model of the objects that make up the scene\", do you have any experimental evidence that you have captured the properties of objects in a scene?\nRather, the main message of the paper seems to be that learning representations in an unsupervised manner is useful for supervised training methods to get better prediction accuracy. Is that a fair comment?\nLearning about the physical structure and semantics of the world from video (without supervision) is a very hot area in computer vision and machine learning.\nIn this paper, the authors investigate how the prediction of future image frames (inherently unsupervised) can help to deduce object/s structure and it's properties (in this case single object pose, category, and steering angle, (after a supervised linear readout step))\n\nI enjoyed reading this paper, it is clear, interesting and proposes an original network architecture (PredNet) for video frame prediction that has produced promising results on both synthetic and natural images.\nMoreover, the extensive experimental evaluation and analysis the authors provide puts it on solid ground to which others can compare.\n\nThe weaknesses:\n- the link to predictive coding should be better explained in the paper if it is to be used as a motivation for the prednet model.\n- any idea that the proposed method is learning an implicit `model' of the `objects' that make up the `scene' is vague and far fetched, but it sounds great.\n\nMinor comment:\nNext to the number of labeled training examples (Fig.5), it would be interesting to see how much unsupervised training data was used to train your representations.", "title": "Several questions!", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1RNR8-Vg": {"type": "review", "replyto": "B1ewdt9xe", "review": "# Motivation\nThe leap/link from `predictive coding' to the `PredNet' architecture is not very clear from the text.\nCan you clarify this?\n\n# The PredNet Model\nHow did you decide that the `PredNet' architecture was good for deducing the structure of the world by predicting future video frames? This is unclear from the text. Was it arrived at through trial and error? Is there prior work that has already shown the architecture to be promising for this application?\n\n# Experiment 3.1\nHow many different faces are there? What kinds of variations are present across the different faces?\n\nDeducing the latent orientations/categories of the faces from the representation of the network via ridge-regression/linear-svm respectively is a supervised learning process, is that right?\nDoesn't that go against the philosophy of deducing the object/s structure and properties from video in an unsupervised manner?\n\nWho proposed the CNN-LSTM decoder? \nWhy not compare your results to other methods that predict future image frames (in your paper you cite 9 papers predicting future frames)?\n\n# Experiment 3.2\nWhat is a linear readout?\nWhat is the input to the fully supervised methods?\nDo you notice any differences in prediction accuracy between sequences in which there are moving objects vs sequences in which only ego-motion is present?\n\n# Discussion\nWhat other structures (apart from single object pose, and steering angle) do you think could be deduced using this method?\nHow did you address the original goal of deducing an object's structure in natural scenes?\n\n\"predicting future frames requires at least an implicit model of the objects that make up the scene\", do you have any experimental evidence that you have captured the properties of objects in a scene?\nRather, the main message of the paper seems to be that learning representations in an unsupervised manner is useful for supervised training methods to get better prediction accuracy. Is that a fair comment?\nLearning about the physical structure and semantics of the world from video (without supervision) is a very hot area in computer vision and machine learning.\nIn this paper, the authors investigate how the prediction of future image frames (inherently unsupervised) can help to deduce object/s structure and it's properties (in this case single object pose, category, and steering angle, (after a supervised linear readout step))\n\nI enjoyed reading this paper, it is clear, interesting and proposes an original network architecture (PredNet) for video frame prediction that has produced promising results on both synthetic and natural images.\nMoreover, the extensive experimental evaluation and analysis the authors provide puts it on solid ground to which others can compare.\n\nThe weaknesses:\n- the link to predictive coding should be better explained in the paper if it is to be used as a motivation for the prednet model.\n- any idea that the proposed method is learning an implicit `model' of the `objects' that make up the `scene' is vague and far fetched, but it sounds great.\n\nMinor comment:\nNext to the number of labeled training examples (Fig.5), it would be interesting to see how much unsupervised training data was used to train your representations.", "title": "Several questions!", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}