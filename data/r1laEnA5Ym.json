{"paper": {"title": "A Variational Inequality Perspective on Generative Adversarial Networks", "authors": ["Gauthier Gidel", "Hugo Berard", "Ga\u00ebtan Vignoud", "Pascal Vincent", "Simon Lacoste-Julien"], "authorids": ["gauthier.gidel@umontreal.ca", "hugo.berard@gmail.com", "gaetan.vignoud@gmail.com", "vincentp@iro.umontreal.ca", "slacoste@iro.umontreal.ca"], "summary": "We cast GANs in the variational inequality framework and import techniques from this literature to optimize GANs better; we give algorithmic extensions and empirically test their performance for training GANs.", "abstract": "Generative adversarial networks (GANs) form a generative modeling approach known for producing appealing samples, but they are notably difficult to train. One common way to tackle this issue has been to propose new formulations of the GAN objective. Yet, surprisingly few studies have looked at optimization methods designed for this adversarial training. In this work, we cast GAN optimization problems in the general variational inequality framework. Tapping into the mathematical programming literature, we counter some common misconceptions about the difficulties of saddle point optimization and propose to extend methods designed for variational inequalities to the training of GANs. We apply averaging, extrapolation and a computationally cheaper variant that we call extrapolation from the past to the stochastic gradient method (SGD) and Adam.", "keywords": ["optimization", "variational inequality", "games", "saddle point", "extrapolation", "averaging", "extragradient", "generative modeling", "generative adversarial network"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper presents a variational inequality perspective on the optimization problem arising in GANs. Convergence of stochastic gradient descent methods (averaging and extragradient variants) is given under monotonicity (or convex) assumptions. In particular, binlinear saddle point problem is carefully studied with batch and stochastic algorithms. Experiments on CIFAR10 with WGAN etc. show that the proposed averaging and extrapolation techniques improve the GAN training in such a nonconvex optimization practices.\n\nGeneral convergence results in the context of general non-monotone VIPs is still an open problem for future exploration. The questions raised by the reviewers are well answered. The reviewers unanimously accept the paper for ICLR publication."}, "review": {"ryT6uWAa7": {"type": "rebuttal", "replyto": "HkekxnDu3m", "comment": "The authors first would like to thank AnonReviewer2 for his thorough evaluation and interesting remarks. In the following, we try to address as clearly as possible the points risen by AnonReviewer2.\n\n\u201cI'm a bit skeptical about the experiments on GANs.:\u201d\nAs suggested, we tried to train a WGAN-GP with a ResNet architecture. The new results have been included in the updated version of the paper (see the experimental section 7.2, table 1 and figure 4 for the new results). After few days of experiments, we were able to obtain state-of-the art results on this architecture using ExtraAdam with averaging. As developed in our new experimental section (see the revised paper), we are not claiming that our principled methods are the solution to the many challenges of practical GAN optimization ( it is possible that a very fine-grained hyperparameter tuning on a standard method may give similar results) but after spending a similar limited time budget on optimizing the hyperparameters of each algorithm it looks clear to us that for this task the ExtraAdam method is much more robust to hyperparameter tuning, i.e, it yields reasonably good results for a large range of step-sizes.\nThe code in pytorch containing all the algorithms presented in the paper as well as the exact experimental setup is ready and will be released after the anonymity period due to the reviewing process.\n\n\u201cProposition 2 is a bit misleading.\u201d\nOur goal was not to weaken the value of implicit methods. When a closed form for the implicit updates is known, this method is very effective, but unfortunately for neural network optimization we are not aware of any practical way to implement the implicit steps. More precisely, an implicit step is equivalent to computing a minimization step of the original objective with a l2 regularization (see [1] for more details on implicit SGD and its applications), this subproblem is supposed to be simpler because of the strong convexity of the l2 regularization. Unfortunately, for neural networks, the optimization problem remains non-convex for any small step-size (which are the step-sizes of interest). Hence, we considered that implicit steps were prohibitively expensive for our applications of interest.\n\n\u201cThe theory is presented for variational inequalities with monotone operators.\u201d\nAs we mentioned it in the second paragraph of Sec. 2.2 \u201cStandard GAN objectives are non-convex (i.e. each cost function is non-convex),\u201d, meaning that they are non-monotone since as we explain it right after the definition of monotonicity, \u201cIf F can be written as (6), it implies that the cost functions are convex.\u201c. We added a clarification in the paper right after the definition of monotonicity (page 7) stating that \u201cGANs parametrized with neural networks lead to non-monotone VIPs\u201d to clarify this. For further discussion about the extension of the VI to non-monotone operators we refer the reviewer to App. C.3.\n\n\u201cA provably convergent algorithm for that setting is still an open problem, no?*\u201d\nTo our knowledge, general convergence results in the context of general non-monotone VIPs is still an open question. The only partial results we are aware of are mentioned in our related work section: \n-\u201cfor a new notion of regret minimization, by Hazan et al. (2017) and in the context of GANs by Grnarova et al. (2018)\u201c\n-\u201c Mertikopoulos et al. (2018) also independently explored extrapolation providing asymptotic convergence results (i.e. without any rate of convergence) in the context of coherent saddle point. The coherence assumption is slightly weaker than monotonicity\u201d.\n\n\n\n[1] TOULIS, Panagiotis, AIROLDI, Edoardo, et RENNIE, Jason. Statistical analysis of stochastic gradient methods for generalized linear models. In : International Conference on Machine Learning. 2014.", "title": "Many thanks for the constructive comments. **New experimental results**"}, "HJe-3rW0pQ": {"type": "rebuttal", "replyto": "Bye1MVvq2m", "comment": "The authors first would like to thank AnonReviewer1 for his meticulous analysis and his insightful comments.\n\n\u201cI think this behaviour is limited to only linear discriminator/generator and might not extend beyond the linear case.\u201d\nThis behavior is a local behavior (and then can be true for a globally non-monotone operator), i.e, if the objective is bilinear in a neighborhood of an (local) equilibrium this behavior is true in that neighborhood. It means that the iterates of the simultaneous method will be expelled from this neighborhood geometrically, the ones of the alternated method will stay in the neighborhood but will not converge to the equilibrium. On the contrary the averaged iterates  and the iterate of the extragradient method will converge to the equilibrium. We also think that these results could be generalized to any game which is locally Hamiltonian (see [1]) around the (local) equilibrium. \n\n\u201cExperiments are shown on the DCGAN architecture. \u201c\nAs suggested by AnonReviewer2, we tried our methods to train a ResNet architecture with the WGAN-GP objective (see the experimental section 7.2, table 1 and figure 4 for the new results). After few days of experiments, we were able to match current the state-of-the art results of ~8.2 on this architecture by using ExtraAdam with averaging (and without using spectral normalization). Contrary to the previous experiments of the paper, the hyperparameter search was less exhaustive (due to time reason) but a similar time budget was spent for fine tuning each algorithm. We observed that with quite few hyperparameter tuning it was possible to match the state of the art with ExtraAdam. We also observed that ExtraAdam is less sensitive to the choice of learning rate, making the hyperparameter tuning easier and enabling the use of higher learning rate.\n\n[1] Balduzzi, D., Racaniere, S., Martens, J., Foerster, J., Tuyls, K., & Graepel, T. The Mechanics of n-Player Differentiable Games. ICML 2018", "title": "Thank you for the knowledgeable review "}, "SJeigrZCaX": {"type": "rebuttal", "replyto": "SJlRNFEa3Q", "comment": "The authors first would like to thank AnonReviewer3 for his careful evaluation and his detailed comments.\nWe would like to point out that we addressed the points raised by AnonReviewer2  in our updated version, particularly, we tried our methods to train a ResNet architecture with the WGAN-GP objective (see the experimental section 7.2, table 1 and figure 4 for the new results).  After few days of experiments, we were able to match the current state-of-the art results of 8.2 on this architecture by using ExtraAdam with averaging (and without using spectral normalization). A similar time budget was spent for fine tuning each algorithm (SimAdam, AltAdam1, AltAdam5, ExtraAdam). We observed that with quite few hyperparameter tuning it was possible to match the state of the art with ExtraAdam. We also observed that ExtraAdam is less sensitive to the choice of learning rate, making the hyperparameter tuning easier and enabling the use of higher learning rate.", "title": "Thank you for the positive comments"}, "BkgcGClCa7": {"type": "rebuttal", "replyto": "rygmGXa_p7", "comment": "Hello, \nfirst of all we would like to thank this anonymous reader for his interest on the paper. We agree that [Chiang et al. 2012] is a relevant and we will consider to incorporate it in the revision.\n\nHowever, note that we already mentioned in our paper a more general or a more seminal related work:\n\n1.\nWe are aware of existing convergence proof for strongly monotone VI: We actually mention in Section 3 a seminal work on strongly monotone VIPs: \u201cThese iterates are known to converge linearly under an additional assumption on the operator\\footnote{ Strong monotonicity, a generalization of strong convexity. See \u00a7A.} (Chen and Rockafellar, 1997)\u201d.\n\nAs you pointed out, Nesterov and Scrimali (2011) consider another algorithm. The Forward-Backward algorithm presented in (Chen and Rockafellar, 1997)  is another denomination (a bit more general though) for what we called \u201cgradient method\u201d (in Section 3). The Forward-Backward algorithm is more related to our work than Nesterov and Scrimali\u2019s method is. Moreover, the proof of linear convergence of what we called \u201cextrapolation from the past\u201d algorithm (Theorem 1) is non trivial and, to our knowledge, does not directly extend from any existing work.\n\n2.\nWe mention right after (21), \u201c This update scheme can be related to the optimistic mirror descent (Rakhlin and Sridharan, 2013)\u201d. Rakhlin and Sridharan (2013) explain in the beginning of Section 2 that \u201c[they] exhibit a Mirror Descent type method which can be seen as a generalization of the recent algorithm of [9]\u201d [9] being (Chiang et al. 2012).  \n\nAs developed in our paper right after the definition of \u201cextrapolation from the past\u201d (and pointed out by the anonymous reviewers) we are bringing a new perspective on this method: \u201cHowever our technique comes from a different perspective, it was motivated by VIP and inspired from the extragradient method\u201d and \u201c Using the VIP point of view we are able to prove a linear convergence rate for a projected version of the extrapolation from the past (see details and proof of Theorem 1 in \u00a7B.3). We also extend these results to the stochastic operator setting in \u00a74\u201d.\n", "title": "Response to \"an interesting perspective and missing important references\""}, "SJlRNFEa3Q": {"type": "review", "replyto": "r1laEnA5Ym", "review": "Summary:\nThe authors take a variational inequality perspective to the study of the saddle point problem that defines a GAN. By doing so, they are able to profit from the corresponding literature and propose a few methods that are variants of SGD. The authors show in a simple example (a bilinear function) these exhibit better performance than Adam and a basic gradient method. After showing theoretical guarantees of these methods (linear convergence) the authors propose to combine them with existing techniques, and show in fact this leads to better results.\n\nEvaluation\nThis is a very good paper and I cannot but recommend its acceptance:\nIt is clear and well written. \nIt has the right level of balance between theory and experiments. \nTheoretical results are far from trivial. \nI haven't seen something similar.\nThe authors's do not make overstatements: they do not claim to have solved the GAN problem, but they do report improvements which are due to a thorough analysis (see above points). These results are much appreciated.\n", "title": "Principled optimization for GANS", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Bye1MVvq2m": {"type": "review", "replyto": "r1laEnA5Ym", "review": "This paper looks at solving optimization problems that arise in GANs, via a variational inequality perspective (VIP). VIP entails solving an optimization problem that is related to the first order condition of the optimization problem that we wish to solve. VIP have been very successful in solving min-max style problems. Given that, GAN formulations tend to be min-max style problems (though not necessarily 0 sum) the VIP perspective is very natural, though under-explored in machine learning. Two techniques that have been widely used to solve VIP problems are averaging and extragradient methods. The authors look at a simple GAN setup where both the generator and the discriminator are linear models. In this case two kinds of gradient updates can be derived. First are simultaneous updates, and the other is alternated updates. The authors show that simultaneous updates are not even bounded and diverge to infinity, whereas alternated updates are more stable and stay bounded, but need not necessarily converge. However, I think this behaviour is limited to only linear discriminator/generator and might not extend beyond the linear case. The second key idea is the use of extra-gradient updates. Extra-gradient updates perform an \"extra\" or fake gradient step to get to a new point, and then kind of retracks back and perform a gradient step using the gradient step obtained from the \"extra step\".  This extra-gradient method is a close approximation to Euler's method, though far more computationally efficient.  However, the extragradient step requires one to calculate gradient twice, which can be expensive in large models. For this reason, the authors suggest using gradients from past as the \"extragradient\" in the extragradient method. \n\nFor strongly-monotone operators (a generalization of strongly-convex functions) extrapolation updates are shown to have linear convergence.  Furthermore, the authors show that using extrapolation and averaging under the assumption that the operator is monotonic, and using constant step size SGD the rates of convergence are better than the rates obtained using plain SGD with averaging but without extrapolation. Authors also show how one can use these ideas using other first order methods such as ADAM instead of SGD. Experiments are shown on the DCGAN architecture. \n\nOn the whole this is a really nice paper, that shows how standard ideas from VIP can be useful for training GANs. I recommend acceptance", "title": "A new perspective on optimization problems arising in GANs which helps provide insights into why averaging helps, why certain type of updates are bad, and how extrapolation can be used to obtain even better solvers.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HkekxnDu3m": {"type": "review", "replyto": "r1laEnA5Ym", "review": "Overall, the paper is well-written and of high quality, therefore I recommend acceptance. \n\nPros:\n+ The work gives an accessible but still rigorous introduction to the literature on VIs which I find highly valuable, as it creates a bridge between the classical mathematical programming literature and applications in AI. \n\n+ The theory for optimization of VIs with stochastic gradients (though only in monotone setting) was very interesting to me and contains some novel results (Theorem 2, Theorem 4)\n\nCons:\n- I'm a bit skeptical about the experiments on GANs. They indicate that for the specific choice of architectures and hyper-parameters \"ExtraAdam\" works better, but the chosen architectures are not state-of-the art. What would convince me if the algorithm can be used to improve a current best inception score of 8.2 reached with SNGANs. Also with WGAN-GP, scores of ~7.8 are reported which are much higher than the 6.4 reported in the paper. But I understand that producing state-of-the-art inception scores is not the focus of the paper, therefore I would suggest that the authors release an implementation of the proposed new optimizers (ExtraAdam) for a popular DL framework (e.g. pytorch) such that practitioners working with GANs can quickly try them out in a \"plug-and-play\" fashion.\n\n- Proposition 2 is a bit misleading. While for \\eta \\in (0, 1) implicit and extrapolation are similar, adding the remark that implicit method is stable for any \\eta > 0 (and therefore can lead to an arbitrary fast convergence) would give a more balanced view. Right now, only the advantages of extrapolation method and disadvantages of implicit method are mentioned which I find unfair for the implicit method.\n\n- The theory is presented for variational inequalities with monotone operators. For clarity it should be mentioned that GANs parametrized with neural nets lead to non-monotone VIs. A provably convergent algorithm for that setting is still an open problem, no?\n", "title": "Good review on algorithms for VIs in the context of GANs", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}