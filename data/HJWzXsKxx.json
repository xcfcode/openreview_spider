{"paper": {"title": "Training Long Short-Term Memory With Sparsified Stochastic Gradient Descent", "authors": ["Maohua Zhu", "Minsoo Rhu", "Jason Clemons", "Stephen W. Keckler", "Yuan Xie"], "authorids": ["maohuazhu@ece.ucsb.edu", "mrhu@nvidia.com", "jclemons@nvidia.com", "skeckler@nvidia.com", "yuanxie@ece.ucsb.edu"], "summary": "A simple yet effective technique to induce considerable amount of sparsity in LSTM training", "abstract": "Prior work has demonstrated that exploiting the sparsity can dramatically improve the energy efficiency and shrink the memory footprint of Convolutional Neural Networks (CNNs).\nHowever, these sparsity-centric optimization techniques might be less effective for Long Short-Term Memory (LSTM) based Recurrent Neural Networks (RNNs), especially for the training phase, because of the significant structural difference between the neurons. To investigate if there is possible sparsity-centric optimization for training LSTM-based RNNs, we studied several applications and observed that there is potential sparsity in the gradients generated in the backward propagation. In this paper, we illustrate why the sparsity exists and propose a simple yet effective thresholding technique to induce further more sparsity during training an LSTM-based RNN training. Experiment results show that the proposed technique can increase the sparsity of linear gate gradients to higher than 80\\% without loss of performance, which makes more than 50\\% multiply-accumulate (MAC) operations redundant in an entire LSTM training process. These redudant MAC operations can be eliminated by hardware techniques to improve the energy efficiency and training speed of LSTM-based RNNs.", "keywords": ["Optimization", "Deep learning"]}, "meta": {"decision": "Reject", "comment": "The main point of the paper was that sparsifying gradients does not hurt performance; however, this in itself is not enough for a publication in this venue. As noted by R1 and R2, showing how this can help in more energy efficient training would make for a good paper; without that aspect the paper only presents an observation that is not too surprising to the practitioners in this area.\n \n Further, while the main point of the paper was relatively clear, the scientific presentation was not rigorous enough. All the reviewers point out that several details were missing (including test set performance, reporting of results on the different sets, etc). \n \n Paper would be strengthened by a better exploration of the problem."}, "review": {"ry0hlixXe": {"type": "rebuttal", "replyto": "SkwEAIkXg", "comment": "Thanks for the questions!\n\n1) In this paper we only present the sparsifying method, which induces a considerable amount of sparsity in the LSTM training process. High sparsity has been demonstrated crucial to improve the training speed and energy efficiency of hardware accelerators, as said in the paper. We are now developing a hardware design to exploit the sparsity and will deliver it in our future work.\n\n2) We have tested the final models trained by our proposed sparsified SGD and they behave almost the same as the baseline. For example, we fed the character-based language model with input strings \"though\", \"peac\", \"stron\", \"tha\" and both the sparsified model and the baseline model output \"thought\", \"peace\", \"strong\" and \"that\". The sparsified model is trained with a threshold of 1e-7, which exhibits more than 70% sparsity during the training. The final validation loss for the sparsified model is 1.47013, which is almost the same as the baseline (1.47021). ", "title": "Speedup expected to show in future work"}, "HJzntqgQx": {"type": "rebuttal", "replyto": "SJSoFXJml", "comment": "Thanks for your question!\n\nThe experimental results for sensitivity tests and other applications show the same conclusion as the data in the paper, as we say in the paper.\nFor example, language models with 2, 3, 6, and 9 LSTM layers (128 LSTM cells each) exhibit 9.92%, 10.62%, 27.96% and 16.09% sparsity with standard SGD algorithm. By applying a threshold of 1e-6, we can achieve 82.84%, 84.62%, 83.09% and 99.92% sparsity respectively, without performance loss. If we increase the threshold further to 1e-5, the performance loss will be unacceptable. \nSimilarly, a 3-layer RNN language model with 128, 256 and 512 cells each layer exhibit 84.62%, 91.19% and 96.76% sparsity given a threshold of 1e-6 without performance loss. \nOur sparsifying method is also insensitive to the sequence length of training data. \n\nFor other applications, for example, neuraltalk2 (https://github.com/karpathy/neuraltalk2), we can achieve 71.5% sparsity by a threshold of 1e-6. And by increasing the threshold to 5e-6, the sparsity will be more than 80%. However, when we apply 1e-5 as threshold, the validation loss will be significantly higher than the baseline. \nIn seq2seq (https://www.tensorflow.org/versions/r0.12/tutorials/seq2seq/index.html), the sparsity in this encoder-decoder model is considerably high even in the baseline (~40%). We can achieve a sparsity more than 80% by applying a threshold of 1e-5 without performance loss. \n\nAll these data points are supportive to our sparsified SGD method. Due to the recommended 8-page limit, we saved them for more space for discussion.\nIf the data points are interesting to you, please feel free to contact me (maohuazhu@ece.ucsb.edu) for more details about the experiments :)\n", "title": "Data for other experiments"}, "SkwEAIkXg": {"type": "review", "replyto": "HJWzXsKxx", "review": "1) Any results to show the sparsified SGD does speedup the training? \n\n2) How does the final model perform on testing sets, not just the losses? The findings of applying sparsity in the backward gradients for training LSTMs is interesting. \n\nBut the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique. \n\nAlso actual justification of the gains in terms of speed and efficiency would make the paper much stronger.\n", "title": "training speed comparisons and final model performance", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Sy_P2-fNx": {"type": "review", "replyto": "HJWzXsKxx", "review": "1) Any results to show the sparsified SGD does speedup the training? \n\n2) How does the final model perform on testing sets, not just the losses? The findings of applying sparsity in the backward gradients for training LSTMs is interesting. \n\nBut the paper seems incomplete without the proper experimental justification. Only the validation loss is reported which is definitely insufficient. Proper testing results and commonly reported evaluation criterion needs to be included to support the claim of no degradation when applying the proposed sparsity technique. \n\nAlso actual justification of the gains in terms of speed and efficiency would make the paper much stronger.\n", "title": "training speed comparisons and final model performance", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJSoFXJml": {"type": "review", "replyto": "HJWzXsKxx", "review": "Why are detailed results from the sensitivity, captioning and translation experiments not provided?This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations.\n\nMinor note:\nThe LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models.\n\nThe paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported.\n\nWhile the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and benchmarking of the promised speedups on multiple tasks. For example, how would the gains be affected by various architecture choices?\n\nAt present this is an interesting technical report and I would like to see more detailed results in the future.", "title": "Results of Section 4.2", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJjgObfNe": {"type": "review", "replyto": "HJWzXsKxx", "review": "Why are detailed results from the sensitivity, captioning and translation experiments not provided?This paper presents the observation that it is possible to utilize sparse operations in the training of LSTM networks without loss of accuracy. This observation is novel (although not too surprising) to my knowledge, but I must state that I am not very familiar with research on fast RNN implmentations.\n\nMinor note:\nThe LSTM language model does not use a 'word2vec' layer. It is simply a linear embedding layer. Word2vec is the name of a specific model which is not directly to character level language models.\n\nThe paper presents the central observation clearly. However, it will be much more convincing if a well known dataset and experiment set up are used, such as Graves (2013) or Sutskever et al (2014), and actual training, validation and test performances are reported.\n\nWhile the main observation is certainly interesting, I think it is not sufficient to be the subject of a full conference paper without implementation (or simulation) and benchmarking of the promised speedups on multiple tasks. For example, how would the gains be affected by various architecture choices?\n\nAt present this is an interesting technical report and I would like to see more detailed results in the future.", "title": "Results of Section 4.2", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}