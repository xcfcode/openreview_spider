{"paper": {"title": "A Trainable Optimal Transport Embedding for Feature Aggregation and its Relationship to Attention", "authors": ["Gr\u00e9goire Mialon", "Dexiong Chen", "Alexandre d'Aspremont", "Julien Mairal"], "authorids": ["~Gr\u00e9goire_Mialon1", "~Dexiong_Chen1", "~Alexandre_d'Aspremont1", "~Julien_Mairal1"], "summary": "We propose a new, trainable embedding for large sets of features such as biological sequences, and demonstrate its effectiveness.", "abstract": "We address the problem of learning on sets of features, motivated by the need of performing pooling operations in long biological sequences of varying sizes, with long-range dependencies, and possibly few labeled data. To address this challenging task, we introduce a parametrized representation of fixed size, which  embeds and then aggregates elements from a given input set according to the optimal transport plan between the set and a trainable reference. Our approach scales to large datasets and allows end-to-end training of the reference, while also providing a simple unsupervised learning mechanism with small computational cost. Our aggregation technique admits two useful interpretations: it may be seen as a mechanism related to attention layers in neural networks, or it may be seen as a scalable surrogate of a classical optimal transport-based kernel. We experimentally demonstrate the effectiveness of our approach on biological sequences, achieving state-of-the-art results for protein fold recognition and detection of chromatin profiles tasks, and, as a proof of concept, we show promising results for processing natural language sequences. We provide an open-source implementation of our embedding that can be used alone or as a module in larger learning models at https://github.com/claying/OTK.", "keywords": ["bioinformatics", "optimal transport", "kernel methods", "attention", "transformers"]}, "meta": {"decision": "Accept (Poster)", "comment": "All reviewers agreed that the paper proposes some interesting and novel ideas on the use of OT for pooling. It also provides some nice insights and strong experimental results. As suggested by one of the reviewer, a discussion about the impact of the number of references may be of interest though.\n\n"}, "review": {"d7rq4cqrlo": {"type": "review", "replyto": "ZK6vTvb84s", "review": "This paper proposes a kernel embedding for a set/sequence of features, based on the optimal transport distance to a set of references, leading to a fixed-dimensional embedding of variable length sequences.\nThe set of references can be obtained as cluster centers over the full dataset (unsupervised), or learned based on a downstream objective (supervised).\nThe method is somewhat related to a single layer of self-attention with an optimal transport map instead of dot product attention.\nExperiments on classifying variable-length sequences are presented between protein, chromatin and NLP sentiment classification.\n\nI recommend weak accept because (+) a well-motivated novel embedding/pooling architecture with (-) somewhat weak, initial proof-of-concept style results and (-) somewhat strange positioning against self-supervised methods and self-attention.\n\nStrenghts:\n* Well-motivated novel layer for embedding a variable-size set or sequence to a *fixed dimensional* embedding space (a point that's not directly apparent though, consider emphasizing this point)\n* Relatively elegant formulation grounded in OT and kernel methods.\n* The same framework allows an unsupervised and supervised variant.\n\nWeaknesses:\n* Experimental results are not very strong. For protein fold classification, I expect a comparison to transformer-based methods on proteins (eg ESM-1, https://github.com/facebookresearch/esm), simply average-pooled over the full sequence. For sentiment analysis, it is cool to see the unsupervised/linear probe on fixed features, beating the frozen BERT - however not competitive to BERT+finetuning which is how it is usually done.\n* Given weak results, the impact of this method is not very clear. It is only shown on shallow single-layer OTKE, without clear discussion how this could be extended to deeper architectures.\n* Given the shallow architecture, the paper is somewhat mispositioned, given the (a) introduction calling out limitations of self-supervised transformers and (b) comparison to self-attention in Sec 4: this suggests inserting the OTKE as a drop-in replacement for the dot product in a transformer model, trained with a self-supervised objective like MLM. Given the framing it seems such a logical step, it surprises me to see at least a discussion of it missing. This would be akin to recent work [1,2] of approximating self-attention by mapping to a fixed-dimensional space.\nAlternatively, it seems like the method could be applied on top of strong/SotA MLM-transformer embeddings to enable better pooling than just [CLS] token or mean pooling (both protein fold prediction and sentiment classification).\n\nMinor comments / questions:\n* The labeling \"unsupervised\" in Table 2 and 4 is somewhat confusing/misleading, rather I'd name it \"linear probe\" or \"supervised finetuning\". Maybe re-clarify in intro of 5.1 \"supervised == learning the z also\"?\n* What are the input features for protein fold classification? it is just mentioned they are 45-dimensional, not what is their nature (amino acid identity + pssm?)\n* On end-to-end learning by back-propping through unrolled sinkhorn (Sec 3.4): this seems to be the method introduced in [3]? I think this paper should be cited in this context.\n* For clarity: In both abstract, end of introduction, start of Sec 3.1 and Sec3.3, it is simply not clear to the reader what is the input/output of your method: a phrase like this would be much needed: \"an embedding+pooling layer which aggregates a variable-size set or sequence of features to a fixed-size embedding\".\n\n\n- [1] arXiv:2006.03555 Choromanski et al. Masked Language Modeling for Proteins via Linearly Scalable Long-Context Transformers\n- [2] arXiv:2006.04768 Wang et al. Linformer: Self-Attention with Linear Complexity\n- [3] Genevay, A., Peyr\u00b4e, G., Cuturi, M., et al. (2018). Learning Generative Models with Sinkhorn Divergences. In AISTATS.\n\n\n\n----\n### EDIT: reply to authors' response\nI thank the authors for their in-depth response and revision. The additional results comparing to pre-trained features definitely adds perspective, and it shows OTKE giving a very modest boost indeed, mostly over the weaker input features. The revision of the paper also improves the clarity, but my comment around Sec4 remains.\nI stand by my original score for the 3 reasons I had originally listed.", "title": "A well-motivated novel embedding/pooling architecture with somewhat weak results", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "tlLgYH504Wf": {"type": "rebuttal", "replyto": "ZK6vTvb84s", "comment": "We have uploaded a revision of the paper, which takes into account the comments below. We would be happy to make further modifications, if the reviewers want to provide additional feedback.", "title": "revision is uploaded"}, "5bIbdP_46Gw": {"type": "rebuttal", "replyto": "5vM8iwoHn5D", "comment": "We thank the reviewer for his/her detailed feedback.\n\n\n> There have been several recent linear memory variants to it (attention), any comments?\n\nWe agree with the reviewer that a discussion is missing. We have addressed this point in the general comment section above.\n\n> I am slightly worried that right now there are just too many \"knobs\" whose correct configuration will have to be worked out (anchor points, reference measures, nystrom, etc). However, I think this can be left to future work.\n    \nWe believe that our approach and code are very easy to use (after all, the number of parameters of the OTKE layer is similar to that of a two-layer neural network), but that indeed a summary of the impact of these parameters is missing in the paper. We have addressed this point in the general comments.\n    \n> Empirical analysis: in the experiments for Table 2, 3, 4, do you use the same number of heads for other baselines as the number of references in your case?\n    \n- Table 2;4: We tuned the number of heads and hidden dimensions for other baselines following the grids used in the original papers (which are relatively consistent with our grids for OT). The search grids for these hyper-parameters are detailed in Table 10 and 18.\n- Table 3: we used one reference in our model. The baseline (and state-of-the-art) architecture for this task is a CNN hence does not have attention heads.\n\n> Empirical analysis: it seems quite funny that, for their unsupervised results, the optimal number of supports p equal 1, which would basically imply that optimal transport wasn\u2019t much of use and the features of x_i were just pooled based on an average. Do you have any comments on this and why this is happening?\n    \nThe answer to this question is very simple: **we made a typo!** The legend should be \"q references x p supports\" instead of \"p supports x q references\". We thank you for noting this very confusing point.\n\n> Empirical analysis: The NLP experiments are relatively weak and could still have been strengthened by testing on more tasks from the GLUE benchmark. Another surprising thing is that even though scores of your method unsupervised are higher than mean pooling of BERT features, the latter significantly outperforms in the supervised case. Do you have any explanation for this?\n    \nIn the supervised case, all parameters of BERT are fine-tuned on the SST-2 data set while the OTKE is trained end-to-end on the frozen BERT features without fine-tuning. Moreover, we believe many NLP tasks to have different characteristics from problems for which our embedding was initially designed (see below and also response to R4). \n    \n> Besides, why do you write in the text that NLP is \u201can a priori less favourable setting\u201d?\n    \nOur work was mainly motivated by long sequences with few labeled data, where its competitive advantage lies for now. In most GLUE tasks, the sequences are shorter (e.g, <66 words for SST-2) and/or there are more labeled data per class than in our bioinformatics tasks. We therefore expected the gain with our method to be less substantial than other baselines designed and benchmarked on NLP GLUE tasks, e.g, fully-trained BERT. \n    \n> Can you give some precise estimates of how the runtime of this OTKE compares to dot product attention? Also, how expensive is the Nystrom procedure?\n\nThe runtime of OTKE on supervised protein fold classification is about 4h with 5 references and 10 supports for 100 epochs, while runtime of the dot product attention is about 3h for 100 epochs as well, on the same gpu. More estimates of the runtime for other baseline methods can be found in Table 13. \n\nOn the Nystr\u00f6m procedure:\n- In the unsupervised setting, we used a K-means algorithm on subsampled features from the entire dataset to learn the anchor points. K-means is GPU-friendly and very fast in practice. The computation of \nthe $k \\times k$ projection matrix is $O(k^3)$, only computed once at train time.\n- In the supervised setting, the anchor points w are initialized with the K-means algorithm and then optimized with back-propagation as other parameters. The embedding given by Nystr\u00f6m is similar to a fully connected layer with the computation of the projection matrix for every minibatch, which is not a bottleneck as $k$ is small in this case. More discussion on Nystr\u00f6m procedure can be found in [Chen et al 2019a,b].\n\n> Misc: This is a bit of nitpicking, but the authors are a bit lazy while citing, and in several places just cite the textbooks for OT and Kernels (which is not a problem). However, it is useful for the reader to additionally have the accurate references as well, e.g. for Sinkhorn\u2019s algorithm perhaps also cite Sinkhorn & Knopp (1967) and Cuturi (2013); for OT theory, Villani 2008 etc.\n    \nWe agree with this point and will make the references more accurate.\n\n", "title": "individual response to R1"}, "V0GRWo7dJl": {"type": "rebuttal", "replyto": "ZK6vTvb84s", "comment": "First, we would like to thank all reviewers for their detailed feedback. We answer general comments here and provide individual answers to each reviewer. We are working on a revision of the paper taking into account these comments, which we will upload as soon as possible.\n\n**Positioning with respect to self-supervised learning and self-attention (R4, R1)**\n\nWe agree with R4 that our introduction is misleading regarding the positioning of our paper. Our approach should be simply seen as a _feature aggregation method_, with few parameters, providing a fixed-sized embedding for sets of possibly varying sizes. We illustrate this method on sequence data, even though other modalities may be used in principle (e.g., point clouds, aggregation of local features in graphs). Therefore,\n- _on self-supervision_:  our approach should indeed not be positioned against self-supervision models. In fact, it may be plugged in sota self-supervised models trained on large unannotated corpus, when it is possible, as suggested by R4 (see additional experiment in response to R4). \n- _on self-attention_: as shown in Sec. 4, there is a relation between our work and self attention, but our goal (providing fixed-size embeddings for sets) is different from approximating classical self-attention matrices. Nevertheless, we agree that references [1,2] from R4 and mentioned by R1 are relevant, as they use low-rank approximations of a (kernel) attention matrix, whereas our embedding provides a factorization of the approximate transport plan $P_z$.\n\n**On multi-layer OTKE (R3,R4)**\n\nThis is indeed a very natural question, which can addressed from two points of views: \n* plugging OTKE as the last aggregation layer of a deep architecture is feasible and seems to work in practice (see experiment in response to R4). See also the DeepSea experiment, where we replace the last convolutional layers of the DeepSea CNN by OTKE.\n* building a multi-layer OTKE is an interesting research direction, but it is not straightforward and our early attempts have been inconclusive. One possible reason is that our method performs well when aggregating features from large sets of varying sizes (which is well adapted to the last layer of a deep architecture) and it is not clear how to find a right definition of intermediate feature aggregation in a multi-layer OTKE model. Note that for DeepSEA, our model with single-layer OTKE already outperforms a multi-layer CNN.\n\n**On architecture choices for OTKE (R1,R3)**\n\nWe agree that a summary of the impact of hyper-parameter choices is currently missing in the paper. In short,\n- _Number of references $q$_: for biological sequences, a single reference was found to be enough in the unsupervised case (see Table 11). In the supervised setting, Table 13 suggests that using $q=5$ provides slightly better results but $q=1$ remains a good baseline. Table A (added below for this rebuttal) is more exhaustive than Table 13 and suggests that the sensitivity to number of references is moderate.\n- _Number of supports p in a reference_: Table 11 and Table A suggest that the sensitivity of the model to the number of supports is also moderate, but we agree that finding automatically the optimal number of supports in a principled way is an important open problem.\n- _Nystr\u00f6m anchors_: an anchor point can be seen as a neuron in a feed-forward neural network (see last line of page 4). For unsupervised settings, the more anchors, the better the approximation of the kernel matrix, and the performance saturates when this number is large enough, as illustrated below in Table B, added for this rebuttal. In the supervised setting, the optimal number of anchors points is much smaller than unsupervised, as also observed in [Chen et al. 2019a], Figure 6. \n- _bandwidth $\\sigma$ in gaussian kernel_:  $\\sigma$ for the features was chosen as in [Chen et al. 2019a] and we did not try to optimize it for the submission, as it seemed to already provide good results. Nevertheless, slightly better results can be obtained when tuning this parameter. For instance, $\\sigma=0.6$ yields better validation and test accuracy on SCOP (91.24$\\pm$0.28/96.77$\\pm$0.17/97.78$\\pm$0.19 vs  88.7/95.9/97.3 in Table 2)\n\n**Table A (same as Table 11 on SCOP, but for supervised OTKE)**\n\n| $q \\times p$ | 10 | 50 |100| 200 |\n|-------------------| ----| ----|-----|------|\n| 1  | 88.3/95.5/97.0 | 88.4/95.8/97.2 | 87.1/94.9/96.7 | 87.7/94.9/96.3 |\n| 2  | 87.8/95.8/97.0 | 89.6/96.2/97.5 | 86.5/94.9/96.6 | 87.6/94.9/96.3 |\n| 5  | 87.0/95.1/96.7 | 88.8/96.0/97.2 | 87.4/95.4/97.0 | 87.4/94.7/96.2 |\n| 10| 84.5/93.6/95.6 | 89.8/96.0/97.2 | 88.0/95.7/97.0\t| 85.6/94.4/96.1 |\n\n**Table B (additional results for unsupervised OTKE on SCOP when varying the number of anchor points)**\n\n| Method | Accuracies |\n| ----------- | ------ |\n| OTKE-1024\t| 85.8/95.3/96.8 |\n| OTKE-2048\t| 86.6/95.9/97.2 |\n| OTKE-3072\t| 87.8/96.1/97.4 |\n\n", "title": "Rebuttal: general comments"}, "1MPUyQrLNX": {"type": "rebuttal", "replyto": "0z-mF1x74-k", "comment": "We thank the reviewer for his/her feedback and for his/her encouraging comments. We are currently working on a revision of the paper, which will take into account the reported typos. In particular, the entropic penalty is indeed  - \\sum P ( log P - 1 ) and not - \\sum P log (P - 1). ", "title": "individual response to R2"}, "hXvqmKu8dgG": {"type": "rebuttal", "replyto": "38vfg9_DABA", "comment": "> Major concern: settings of the number of references and supports, which would have a large number of possible settings. Discussing the selection of the number of references and supports in more detail would have been informative. In my understanding, these are most important hyper-parameters. Currently, no practical recipes and analysis of sensitivity to these settings are provided. How were they chosen in the experiments? The number of supports can be different for each one of references. Therefore, there exist a huge number of possible settings even when only considering moderate sizes.\n    \n This is indeed an important aspect that we will clarify in the paper. In particular, each reference has the same number of supports in our implementation for the sake of simplicity and similarity to the transformer, but it would be interesting to investigate the possibility of varying the number of supports. Regarding the choice of parameters, we have addressed this point in the general comments. Note that each configuration choice was made by optimizing the validation error.\n\n> The optimized references seemingly have some interpretation about discovery of an important set of vectors. I'm not fully sure, but it seems to provide a different view point from self-attention, and thus, it is perhaps worth mentioning if possible. \n    \nWe agree with the reviewer and propose to emphasize this point in Section 4. Visualization and interpretation of the learned references would be of interest for biological sequences, which we plan to investigate for future work. We thank the reviewer for this suggestion.\n    \n> A naive extension could be to make multiple transportation layers (transport z to another references). Is it meaningless? (I guess, at least, some activation function is required) If it has some meaning, providing comments may be informative.\n\nThis is an interesting question and wether it is meaningless or not is not clear to us yet. We discuss this point in the general comments.\n\n> In eq.(5), 'R^k x p' should be 'R^p x k'?\n    \nThis was indeed a typo. Thank you for noting this.", "title": "individual response to R3"}, "1sPmTCsRkR": {"type": "rebuttal", "replyto": "d7rq4cqrlo", "comment": "We thank the reviewer for his/her detailed feedback.\n\n> For protein fold classification, I expect a comparison to transformer-based methods on proteins (eg ESM-1, [...]\n    \nWe thank the reviewer for pointing out this work. We were a bit surprised by the request to compare to ESM as the code of ESM was released less than a month before the ICLR deadline. Nevertheless, we admit that this is a very good suggestion. We have therefore added several baselines for the SCOP experiments:\n* using ESM-1, pretrained on 250 millions sequences, with mean pooling, followed by a linear classifier. As we do not have the computational ressources to fine-tune ESM-1_t34, we only train a linear layer on top of the extracted features. We tried finetuning ESM-1_t6 but did not get any improvement.\n* using the same ESM-1 model, when replacing mean pooling by our OTKE layer. To be fair, we consider only unsupervised OTKE here, and also only train the linear layer.\n\nThe results are presented in Table C below. In contrast, our best OTKE model (mentioned in general comments) achieves 91.24/96.77/97.78, which yields the following conclusions\n* training huge self-supervised learning models on large datasets is effective, as the performance of ESM-1_t34 is impressive. Note however that the context is different, as ESM-1_t34 admits more than 2500 times more parameters (trained with self-supervision on large-scale external data) than our single-layer OTKE model (260k parameters vs 670M for ESM-1_t34).\n* our single-layer OTKE outperforms ESM-1_t6 (43M parameters).\n* plugging unsupervised OTKE on top on ESM-1 brings some benefits, mostly for ESM-1_t6.\n\nThis confirms that competitive advantages of ESM and OTKE are different and complementary (see also general comments on positioning), and that pre-training with self-supervision a large model including OTKE on large data sets would be interesting for fair comparison.\n\n**Table C**\n\n| model\t| #params | mean pooling | with unsupervised OTKE |\n| -- | -- | -- | -- |\n| esm1_t6_43M_UR50S | 43M | 84.01/93.17/95.07 | 85.91/93.72/95.30 |\n| esm1_t34_670M_UR50S | 670M | 94.95/97.32/97.91 |95.22/97.32/98.03 |\n\n\n> For sentiment analysis, it is cool to see the unsupervised/linear probe on fixed features, beating the frozen BERT - however not competitive to BERT+finetuning which is how it is usually done.\n\nFor NLP, and given its positioning (see general comments), we have chosen to compare our work to other models for sets, which we outperform in this setting. We believe our method to be of interest in a constrained ressource context, where it is not possible to fine-tune a whole transformer architecture. We indeed do not claim that the method will be interested for all NLP tasks (we rather indeed claim that the results on NLP should be seen as a proof of concept). For instance, many NLP tasks (e.g, tasks of GLUE) have a different setting from the one for which our method was initially built: sequences are shorter and/or there are more data. We choose SST-2 because it is a sentence classification task with relatively long sequences with a moderate amount of data.\n\n> It is only shown on shallow single-layer OTKE, without clear discussion how this could be extended to deeper architectures.\n\nsee general comments.\n\n>  Given the shallow architecture, the paper is somewhat mispositioned, [...] , it seems like the method could be applied on top of strong/SotA MLM-transformer embeddings ... \n\nWe agree with many of these remarks, see general comments.\n\n> The labeling \"unsupervised\" in Table 2 and 4 is somewhat confusing/misleading, ... \n    \nHere, \"unsupervised\" refers to \"representation learning without supervision\" (z, but also Nystr\u00f6m anchors w). In this setting, only the classifier is learned with supervision. The same terminology is used in previous works on which we rely [Mairal 2016, Chen et al. 2019 a;b]. We propose to keep those terms and clarify their meaning in the paper.\n    \n> What are the input features for protein fold classification? it is just mentioned they are 45-dimensional, not what is their nature (amino acid identity + pssm?)\n    \nThe input features consist of a 20-dimensional one-hot encoding of the sequence, a 20-dimensional position-specific scoring matrix (PSSM) representing the profile of amino acids computed by PSI-BLAST against the nr90 database, a 3-class secondary structure represented by a one-hot vector and a 2-class solvent accessibility. More details are provided in Appendix C.3.\n\n> On end-to-end learning by back-propping through unrolled sinkhorn (Sec 3.4): this seems to be the method introduced in [3]?\n    \n[3] also uses such an approach. We thank the reviewer for this comment and will add the reference.\n    \n> For clarity: In both abstract, end of introduction, start of Sec 3.1 and Sec 3.3, it is simply not clear to the reader what is the input/output of your method: a phrase like this ....\n    \nWe agree with the reviewer and will add a sentence to clarify this.", "title": "individual response to R4"}, "38vfg9_DABA": {"type": "review", "replyto": "ZK6vTvb84s", "review": "The paper proposes a transport-based feature representation for the input of a set of vectors. The feature is defined through the transport to reference sets. Vectors in reference sets can be learned by supervised or unsupervised approaches. The relation with self-attention is also discussed. Experiments empirically show a good performance of the proposed method.\n\nTransporting a set of inputs to trainable references is seemingly a more sophisticated extension of the classical basis function regression with trainable basis (basis corresponds to references), and thus, I think the basic idea would be reasonable and easy to have an intuition. My major concern is on about the settings of the number of references and supports, which would have a large number of possible settings. Except for that, I currently only have minor comments.\n\nDiscussing the selection of the number of references and supports in more detail would have been informative. In my understanding, these are most important hyper-parameters. Currently, no practical recipes and analysis of sensitivity to these settings are provided. How were they chosen in the experiments?\n\nThe number of supports can be different for each one of references. Therefore, there exist a huge number of possible settings even when only considering moderate sizes.\n\nThe optimized references seemingly have some interpretation about discovery of an important set of vectors. I'm not fully sure, but it seems to provide a different view point from self-attention, and thus, it is perhaps worth mentioning if possible. \n\nA naive extension could be to make multiple transportation layers (transport z to another references). Is it meaningless? (I guess, at least, some activation function is required) If it has some meaning, providing comments may be informative.\n\nIn eq.(5), 'R^k x p' should be 'R^p x k'?", "title": "Simple, but reasonable approach ", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "5vM8iwoHn5D": {"type": "review", "replyto": "ZK6vTvb84s", "review": "**Summary**:\nThe authors propose a new way to aggregate the embeddings of elements in a set (or sequence) by comparing it with respect to (trainable) reference set(s) via Optimal Transport (OT). The motivation to build such a pooling operation is derived from self-attention and the authors suggest an OT spin to that (e.g., the different reference sets/measures can be thought of as different heads in attention). This is, however, done in a principled way with the help of kernel embeddings and not just ad-hoc using the transport plan as the attention matrix.\n \n**Pros**:\n-They properly bridge the gap of OT with attention via their non-local pooling perspective. This allows them to have a similar feature aggregator, but which requires linear memory as compared to quadratic for the vanilla self-attention (although there have been several recent linear memory variants to it, any comments?). \n-The method results in an improvement over other baselines for several biology-based sequencing applications (also they demonstrate early results for NLP). \n-The paper is well written and clear.\n\n**Cons**: \n-The empirical results are okay, but could have been better or tested more extensively. Also, there are some peculiarities about their empirical analysis. \n-This kind of technique has promise for use in NLP, but I am slightly worried that right now there are just too many \"knobs\" whose correct configuration will have to be worked out (anchor points, reference measures, nystrom, etc). However, I think this can be left to future work. \n\n*-> It would be great if the authors can answer the questions below:* \n\nEmpirical analysis:\n-In the experiments for Table 2, 3, 4, do you use the same number of heads for other baselines as the number of references in your case?\n\n-It seems quite funny that, for their unsupervised results, the optimal number of supports p equal 1, which would basically imply that optimal transport wasn\u2019t much of use and the features of x_i were just pooled based on an average. Do you have any comments on this and why this is happening?\n\n-The NLP experiments are relatively weak and could still have been strengthened by testing on more tasks from the GLUE benchmark. Another surprising thing is that even though scores of your method unsupervised are higher than mean pooling of BERT features, the latter significantly outperforms in the supervised case. Do you have any explanation for this?\n\n-Besides, why do you write in the text that NLP is \u201can a priori less favourable setting\u201d?\n\n-Can you give some precise estimates of how the runtime of this OTKE compares to dot product attention? Also, how expensive is the Nystrom procedure?\n\nMiscellaneous: \n-This is a bit of nitpicking, but the authors are a bit lazy while citing, and in several places just cite the textbooks for OT and Kernels (which is not a problem). However, it is useful for the reader to additionally have the accurate references as well, e.g. for Sinkhorn\u2019s algorithm perhaps also cite Sinkhorn & Knopp (1967) and Cuturi (2013); for OT theory, Villani 2008 etc.  \n-Typos: \"a set x and a reference z in X and a reference z\u201d\n-ISAB and PMA (unexplained acronyms)\n\n**Conclusion**:  Overall, I think the idea is quite interesting, and carries the potential for OT-based building blocks in NLP, or even analysing the benefit provided by attention. Hence, I am in favour of accepting this paper.  \n\n(PS: I might be slightly biased because I like OT and have also been thinking about ideas on similar lines connecting attention and OT.)", "title": "Nice and well-executed idea for feature aggregation based on OT (with connections to attention)", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "0z-mF1x74-k": {"type": "review", "replyto": "ZK6vTvb84s", "review": "##########################################################################\n\nSummary:\n\nThe paper proposes a parametrized embedding based on optimal transport in the kernel space. The method is scalable and the parameters of the embedding can be learned in either unsupervised or supervised fashion. The paper demonstrates the effectiveness of the purposed approach using real data in bioinformatics and natural language processing. \n\nOverall, the proposed embedding seems to be novel and well-motivated. The empirical results are impressive. Also, the paper is well written and technically sound. I don't have major concerns about the paper. \n\n\n##########################################################################\n\nMinor comments: \n\n- Contributions paragraph: \"in either unsupervised and supervised fashion\" --> \"in either unsupervised or supervised fashion\"\n- The line below Eq. (1): the entropy should be - \\sum P ( log P - 1 ) ? \n- Section 3.2 \"Infinite-dimensional embedding in RKHS\" line 1: \"a reference z in X\" repeated twice.\n- Third line below Eq. (4): \\Phi_z(x) instead of \\Phi_z(z)\n- First line in Section 5.2: \"either unsupervised or supervised settings\" --> \"both unsupervised and supervised settings\"", "title": "Interesting idea", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}}