{"paper": {"title": "THE EFFICACY OF L1 REGULARIZATION IN NEURAL NETWORKS", "authors": ["Gen Li", "Yuantao Gu", "Jie Ding"], "authorids": ["g-li16@mails.tsinghua.edu.cn", "~Yuantao_Gu1", "~Jie_Ding2"], "summary": "We develop novel theoretical results on the efficacy of L1 regularization for shallow neural networks. ", "abstract": "A crucial problem in neural networks is to select the most appropriate number of hidden neurons and obtain tight statistical risk bounds. In this work, we present a new perspective towards the bias-variance tradeoff in neural networks. As an alternative to selecting the number of neurons, we theoretically show that $L_1$ regularization can control the generalization error and sparsify the input dimension. In particular, with an appropriate $L_1$ regularization on the output layer, the network can produce a statistical risk that is near minimax optimal. Moreover, an appropriate $L_1$ regularization on the input layer leads to a risk bound that does not involve the input data dimension. Our analysis is based on a new amalgamation of dimension-based and norm-based complexity analysis to bound the generalization error. A consequent observation from our results is that an excessively large number of neurons do not necessarily inflate generalization errors under a suitable regularization.\n", "keywords": ["Model selection", "Neural Network", "Regularization"]}, "meta": {"decision": "Reject", "comment": "The paper presents a generalization bounds for l1 regularized networks.  The reviewers thought the results were clear and sound, but on the other hand rely on rather standard technical tools, and their impact is limited.\nOne question is why this particular regularization is related to practical learning of neural nets where explicit regularization is not used. The authors may want to relate their results to e.g., Theorem 1 in https://arxiv.org/pdf/1412.6614.pdf.\n "}, "review": {"IJTtZlUCE-a": {"type": "rebuttal", "replyto": "B7ciORjgaLi", "comment": "Thank you for your comments. We address each concern below.\n\nOn contribution:\nWe agree that the theoretical tool is not beyond the existing statistical learning theory. However, the discovery of the tight statistical risk bound (at the rate of n^{1/2} under L1 loss) is new. The existing tight bound for neural networks is n^{1/4} under L2 loss (equivalently, the square-root of MSE).\n\nOn eq(10):\nThere was a typo under the sup in (10). The sup should be over the w whose l1 norm is bounded by eta_n (which we introduced in Assumption 3).\n\nOn constraint of w: \nThe constraint on w in Thm 3.4 will not affect the approximation error established in Thm 2.1, according to the definition of (5) in Theorem 2.1.\n\nOn Eq (17):\nWe double checked that (17) is correct. The function class at the left-hand side does not show on the right side, because we applied the contraction lemma (Lemma A.1) in conjunction with norm inequalites.\n\nOn Eq (14):\nWe will fix this typo.\n", "title": "Thank you for your comments. We address each concern below."}, "jy1IsPL-fXt": {"type": "rebuttal", "replyto": "Wuap3M5xMd", "comment": "Thank you for your comments. We address each comment below.\n\nNotational point:\nWe will change the notation as you suggest.\n\nOn related work:\nWe will add a discussion on the related work that used norm-based analysis, including the mentioned work.\n\nOn contraction lemma:\nWe will cite earlier work as you suggest.\n", "title": "Thank you for your comments. We address each comment below."}, "B0DCVSqsjBV": {"type": "rebuttal", "replyto": "QkKj7eV3fY", "comment": "Thank you for your comments. We address each question below.\n\nOn Assumption 2:\nIn practice, for the purpose of model selection, we do need to search V within the regularized training. Theoretically, our contribution is to show tight statistical risk bounds (as dependent on the sample size n) for any fixed V. It is a different theoretical perspective compared with the counterpart analysis based on r.\n\nOn Bernoulli random variables:\nIt is standard to use the Bernoulli random variables to define the Rademacher complexity and Rademacher process (originally from the symmetrization method).\n\nOn (5):\nThe quantity in equation (5) was introduced for technical derivations. It does not imply most of the errors occur around zero.\n\nOn eta in Assumption 3:\nThere is no additional sup over w. The sup is already over j, which indexes the w. \nWe do not require log eta to be at the same order of log n.\n\nOn the randomness inside P:\nThe randomness is from the random x (the input).\n\nThe quantifiers in Thm 3.2:\nIt is a minimax notaton, meant for all possible \\hat{f}_n (based on data). We will clarify this in the revision.\n\nOn Assumption 5:\nWhat you proposed is a very interesting idea. We conjecture that it is possible to extend the results to a sparse latent representation (not from the input layer). We will remark it as future work. \n\nOn \\hat{f} -> f_0:\nWe will fix this typo\n\nOn the second term (with y^2):\nWould you please let us know where it is? We could not locate the term with y^2 after (15).\n", "title": "Thank you for your comments. We address each question below."}, "B7ciORjgaLi": {"type": "review", "replyto": "6MaBrlQ5JM", "review": "This paper studies the statistical risk bounds for two-layer neural networks with $L_1$-regularization. The authors consider two types of $L_1$-regularization: the $L_1$-regularization on output layer and the $L_1$-regularization on the input layer. For the $L_1$-regularization on output layer, the authors develop nearly minimax statistical risk bounds. For the $L_1$-regularization on input layers, the authors develop bounds with no-dependency on the input dimension. The paper is clearly written and easy to follow.\n\nComments\n\n1. This is a theory paper. However, as far as I can see, theoretical analysis is a bit standard. It seems that the authors do not introduce new techniques and idea in statistical learning theory (SLT). Furthermore, the results are a bit standard. For example, the statistical risk bound of the order $\\sqrt{d/n}$ in Thm 3.1 is standard in SLT for general machine learning models. It is not surprising that this holds for two-layer neural networks.\n\n2. In my opinion, eq (10) is not correct. Since there is no constraints on $w$, the left-hand side can be infinity by taking $w$ of infinite magnitude. Therefore, I think one should also imposes a constraint on $w$ in the definition of $F_v$ in eq (13)\n\n3. In the definition of $F_{v,\\eta}$ in Thm 3.4, there is a constraint on $w$. I think this constraint may affect the approximation error established in Thm 2.1. Then this can further affect the statistical risk in Thm 3.4.\n\n4. Eq (17) should be not correct. Indeed, the left-hand side depends on a function class, while the right-hand side depends on $w_j$ and $b_j$. This is not meaningful.\n\n5. In eq (14), $\\hat{f}$ should be $f_0$", "title": "Analysis seems standard and nor rigorous", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Wuap3M5xMd": {"type": "review", "replyto": "6MaBrlQ5JM", "review": "This paper studies generalization bounds for neural networks with the following kind of setup:\n(0) 1 hidden layer and sigmoid-like activations. the weights in the input layer are bounded in either a general norm, or sometimes specifically the $\\ell_1$-norm.\n(1) the loss function is the L1 loss |y - \\hat{y}|, as opposed to the more common square loss.\n(2) the weights of the last layer of the network are bounded in the vector $\\ell_1$-norm. They also consider the case where the input layer is bounded in Sec 3.2.\n\nA small notational point: the authors use L1 to denote both cases, i.e. write L1 instead of $\\ell_1$ to denote the vector $\\ell_1$-norm\nof the weights (i.e. the sum of absolute values). Sometimes I do see the vector norm written L1 anyway, as in this paper. However, the vector norm is more commonly written as $\\ell_1$, and I think this would be a good idea to minimize confusion between (1) and (2) above, which are very different things. (E.g. In the linear regression world, the $\\ell_1$-norm is associated with sparsity and $L_1$ loss is associated with robustness to outliers.)\n\nThe main result of this paper is a generalization bound for this class of neural networks, which comes down to technical Lemma 2.3 bounding the Rademacher complexity of the class. This result follows in a relatively straightforward way, because L1 loss is lipschitz (so we can use contraction lemma) and because all of the weights are bounded in $\\ell_1$ norm; a small twist is they use that the input dimension is small to get a better bound on the term coming from the first layer of the network. They also state a result given by combining this generalization bound with Barron's approximation theorem.\n\nIt would be useful for the authors to compare their results further with previous work in this area. In particular, the idea of looking at the $\\ell_1$ norm of the output layer for generalization bounds has been considered as part of the paper \"The Sample Complexity of Pattern Classification with Neural Networks: The Size of the Weights is More Important than the Size of the Network\", Bartlett '98. See e.g. Theorem 17.\n\nOverall, this paper doesn't seem to contribute many fresh ideas to the study of generalization bounds for neural networks and so I would tend towards rejection.\n\nminor notes:\n- contraction lemma & rademacher complexity are basic results in this area, so preferably they shouldn't be cited from neyshabur et al (2015). For example, you could cite a textbook in the area, like Shalev-Shwartz + Ben-David book.\n", "title": "Minor contribution", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "QkKj7eV3fY": {"type": "review", "replyto": "6MaBrlQ5JM", "review": "1. Summarize what the paper claims to contribute.\n\nThe paper bounds the excess L1 risk of two-layer neural networks when L1 regularization is applied to the second layer. The rates obtained are better than the rates with L2 risk and nearly optimal. Crucially, the L1 regularization removes the size of the hidden layer from the estimation error. \n\n2. List strong and weak points of the paper.\n\nPros:\n* The lower and upper bounds in the paper are almost matching. \n* The discussion of the assumptions and results are clear and informative. I'm quite happy about this.\n* The paper is overall clearly written and correctly executed (modulo minor issues that can be fixed).\n\nCons:\n* The contributions have limited impact.\n\n3. Clearly state your recommendation (accept or reject) with one or two key reasons for this choice.\n\nUnfortunately I am recommending rejection. The paper is well written and well executed, but I am concerned that the contributions are few and that their impact is limited.\n\n4. Provide supporting arguments for your recommendation.\n\nConcerns:\n* Limiting the setting to two-layer neural networks and excluding common activations like ReLU limits the impact of the results.\n* The paper applies existing results/analyses to a new setting (combining neural networks and L1 regularization). This setting does not pose any particular technical challenge. The findings are interesting but few, in my opinion. It is unclear whether they are enough for acceptance.\n\n5. Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.\n\nImportant questions/requests:\n* Please clarify the significance of the contributions, and their potential impact.\n* If the papers address a new technical challenge, please clarify.\n* Are they widely applicable as they are? \n* Has the paper opened up a path for future research to use the knowledge contributed, and to make the results more broadly applicable?\n\nTangential questions:\n* Assumption 2 seems to include the assumption \"our choice of V is appropriate\". So one does not need to do model selection on V. The results remove r and introduce V into the estimation error, but is V easier to choose than r? \n\n6. Provide additional feedback with the aim to improve the paper.\n\nThe paper uses Bernoulli random variables for the Rademacher complexity, but they should be Rademacher random variables (see Appendix A in Neyshabur et al., 2015).\n\n(5) makes me wonder whether most of the error happens around zero, and whether easy cases are significantly easier\n\n7. Typos/Minor questions\n\nFor eta in Assumption 3, should there be a sup over w?\nIn the definition of eta, maybe write \\log \\eta \\asymp \\log n ?\n\nWhat's the randomness inside the P in the alternative definition of Assumption 3? I'm assuming it's X, based on the explanation that follows the equation.\n\nThe quantifiers in Thm 3.2 look strange, but I think it's because in the original result the \\hat{\\theta} (mapped to \\hat{f}_n here) refers to the estimator and not the actual empirical minimizer. Could you please clarify the notation here?\n\nFor Assumption 5, I was wondering if you wouldn't want a sparse latent representation instead? I think the case where the inputs are actually sparse is not usual, but the case where you can learn a sparse representation at least from an empirical perspective seems more realistic.\n\np.8 typo in definition of a_0\n\nSigma's Lipschitz constant is missing from the derivation (first step to second) at the bottom of p.8. The derivations following that are missing some constants. Also, please cite sources for Bernstein's Inequality and the covering number of W_eps.\n\n(14): \\hat{f} -> f_0\n\nIn the equations following (15) for that derivation it would be useful to add how the second term (with y^2) came to be.", "title": "Clearly written paper with interesting findings, but I'm concerned the contributions are few and limited in scope.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}