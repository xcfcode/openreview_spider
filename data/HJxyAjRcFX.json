{"paper": {"title": "Harmonizing Maximum Likelihood with GANs for Multimodal Conditional Generation", "authors": ["Soochan Lee", "Junsoo Ha", "Gunhee Kim"], "authorids": ["soochan.lee@vision.snu.ac.kr", "kuc2477@gmail.com", "gunhee@snu.ac.kr"], "summary": "We prove that the mode collapse in conditional GANs is largely attributed to a mismatch between reconstruction loss and GAN loss and introduce a set of novel loss functions as alternatives for reconstruction loss.", "abstract": "Recent advances in conditional image generation tasks, such as image-to-image translation and image inpainting, are largely accounted to the success of conditional GAN models, which are often optimized by the joint use of the GAN loss with the reconstruction loss. However, we reveal that this training recipe shared by almost all existing methods causes one critical side effect: lack of diversity in output samples. In order to accomplish both training stability and multimodal output generation, we propose novel training schemes with a new set of losses named moment reconstruction losses that simply replace the reconstruction loss. We show that our approach is applicable to any conditional generation tasks by performing thorough experiments on image-to-image translation, super-resolution and image inpainting using Cityscapes and CelebA dataset. Quantitative evaluations also confirm that our methods achieve a great diversity in outputs while retaining or even improving the visual fidelity of generated samples.", "keywords": ["conditional GANs", "conditional image generation", "multimodal generation", "reconstruction loss", "maximum likelihood estimation", "moment matching"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper presents new loss functions (which replace the reconstruction part) for the training of conditional GANs. Theoretical considerations and an empirical analysis show that the proposed loss can better handle multimodality of the target distribution than reconstruction based losses while being competitive in terms of image quality."}, "review": {"HkgbB4TxeV": {"type": "rebuttal", "replyto": "r1gdodFyx4", "comment": "We are deeply grateful to reviewer3 for a quick reply that reveals the detailed ground for the decision. Now we can understand the review much better to offer more focused answers to the concerns raised by reviewer3.\n\n1. Novelty\n===================================\nAccording to reviewer3\u2019s clarification, per-pixel mean and variance prediction is the core of our methods, and thus our methods don\u2019t have enough novelty compared to the cited papers.\n\nAlthough some of our methods involve mean and variance prediction, the key idea of our methods is matching the moments of the sample distribution to the maximum likelihood estimates of the real moments. As such, MLMM_1 and MCMLE_1, for example, do not use the variance prediction but achieve great diversity and quality. \n\nNote that our methods suggest two simple modifications to existing conditional GANs as final recipes; thus it would not be surprising that some previous work used similar techniques in other applications. However, we would like to emphasize that our methods are novel in the context of conditional GANs and mode collapse of GANs. \n\n\n2. Theoretical results\n===================================\nWe would like to clarify that the proof that reviewer3 looks for is in section 4.4 not in section 3.2. During the rebuttal period, we reorganized section 3.2 and section 4.4 to reflect reviewer3\u2019s comments and to streamline the logic. In the current draft, section 3.2 contains the proof about the conflict between the reconstruction loss and the GAN loss, while section 4.4 proves that our approach does not suffer from the same problem.\n", "title": "Re: Re: Re-clarification to Reviewer3\u2019s Updated Review"}, "Skg_k4wklV": {"type": "rebuttal", "replyto": "rJxtHml92m", "comment": "We thank again the reviewer for the comments. \nHowever, we have the impression that some critics are unfair, imprecise and unhelpful; thus, hardly acceptable for us. Please see below why. \n\n\n1. Novelty\n===================================\nReviewer3 raised again the concern about novelty in the updated review. \n\n\nIn our rebuttal, we clarified that our work is the first to analyze why the use of reconstruction loss leads to the mode collapse (lose of multimodality) in conditional GANs. Our work is also the first to propose alternatives to the reconstruction loss which greatly improve the multimodality of conditional GANs without losing the visual fidelity of the output samples.\n\nReviewer3 did not leave any comment on this clarification and failed to mention any specific works that undermine our novelty and how closely they are related to our work. In the initial review, reviewer3 referred several papers about variance prediction; however, these papers have no relation with conditional GANs or mode collapse. \n\nWe sincerely ask reviewer3 to be specific and detailed on the claim that our work lacks novelty with proper ground.\n\n\n2. Theoretical results\n===================================\n\u201cProving that the proposed method is actually effective in what is designed to do\u201d\nAccording to the modified review, reviewer3 seems to take the lack of proof that our model prevents mode collapse as a serious flaw in our work.\nHowever, we think reviewer3 largely misunderstood the key to our paper. Our methods have no multimodality-enhancing mechanism; instead, GANs are responsible for multimodality. Our methods are designed not to interrupt the GAN optimization and we proved it. The methods simply offer training stability without interference. Thus, the multimodality observed in our methods is inherent from GANs, and we pointed out that it is suppressed by the reconstruction loss in existing conditional GANs.\n\nCompared with a parallel submission to ICLR 2019 below, it becomes more obvious that we provide necessary proofs.\nDiversity-Sensitive Conditional Generative Adversarial Networks (https://openreview.net/forum?id=rJliMh09F7)\n\nBoth papers share the same goal: multimodal generation in conditional GANs. However, the approaches are vastly different. Unlike our work, they add a regularization term to the loss while keeping the reconstruction loss. Their regularization term directly forces the model to generate diverse outputs. In this case, the proof that it facilitates diversity is necessary, so they present it. In contrast, we point out that the reconstruction loss conflicts with GANs in a way that reduces the output variance and proposes alternatives without such problem. Thus, we prove the problem of reconstruction loss and that our methods do not conflict with the GAN objective.\n\n\n3. Suggestion for better reviewing process \n===================================\nWe carefully prepared for the rebuttal to answer to the initial review critics. However, we feel that our rebuttal is completely ignored because we cannot find in the updated review which specific question is not answered by our rebuttal and why our clarification cannot be the answer to the original review questions. We strongly believe the communications between authors and reviewers should be precise, specific and helpful to one another. \n", "title": "Re-clarification to Reviewer3\u2019s Updated Review"}, "rJxtHml92m": {"type": "review", "replyto": "HJxyAjRcFX", "review": "The paper proposes a modification to the traditional conditional GAN objective (which minimizes GAN loss as well as either L1 or L2 pixel-wise reconstruction losses) in order to promote diverse, multimodal generation of images. The modification involves replacing the L1/L2 reconstruction loss -- which predicts the first moment of a pixel-wise gaussian/laplace (respectively) likelihood model assuming a constant spherical covariance matrix -- with a new objective that matches the first and second moments of a pixel-wise gaussian/laplace likelihood model with diagonal covariance matrix. Two models are proposed for matching the first and second moments - the first one involves using a separate network to predict the moments from data which are then used to match the generator\u2019s empirical estimates of the moments (using K samples of generated images). The second involves directly matching the empirical moment estimates using monte carlo.\n\nThe paper makes use of a well-established idea - modeling pixel-wise image likelihood with a diagonal covariance matrix i.e. heteroscedastic variance (which, as explained in [1], is a way to learn data-dependent aleatoric uncertainty). Following [1], the usage of first and second moment prediction is also prevalent in recent deep generative models (for example, [2]) i.e. image likelihood models predict the per-pixel mean and variance in the L2 likelihood case, for optimizing Equation 4 from the paper. Recent work has also attempted to go beyond the assumption of a diagonal covariance matrix (for example, in [3] a band-diagonal covariance matrix is estimated). Hence, the only novel idea in the paper seems to be the method for matching the empirical estimates of the first and second moments over K samples. The motivation for doing this makes intuitive sense since diversity in generation is desired, which is also demonstrated in the results.\n\nSection specific comments:\n- The loss of modality of reconstruction loss (section 3.2) seems like something which doesn\u2019t require the extent of mathematical and empirical detail presented in the paper. Several of the cited works already mention the pitfalls of using reconstruction loss.\n\n- The analyses in section 4.4 are sound in derivation but not so much in the conclusions drawn. It is not clear that the lack of existence of a generator that is an optimal solution to the GAN and L2 loss (individually) implies that any learnt generator using GAN + L2 loss is suboptimal. More explanation on this part would help.\n\nThe paper is well written, presents a simple idea, complete with experiments for comparing diversity with competing methods. Some theoretical analyses do no directly support the proposition - e.g. sections 3.2 and 4.4 in my specific comments above. Hence, the claim that the proposed method prevents mode collapse (training stability) and gives diverse multi-modal predictions is supported by experiments and intuition for the method, but not so much theoretically. However, the major weakness of the paper is the lack of novelty of the core idea.\n\n=== Update after rebuttal:\nHaving read through the other reviews and the author's rebuttal, I am unsatisfied with the rebuttal and I do not recommend accepting the paper. My rating has decreased accordingly.\n\nThe reasons for my recommendation, after discussion with other reviews, are -- (1) lack of novelty and (2) weak theoretical results (some justification of which was stated in my initial review above). Elaborating more on the second point, I would like to mention some points which came up during the discussion with other reviewers: The theoretical result which states that not using reconstruction loss given that multi-modal outputs are desired is a weaker result than proving that the proposed method is actually effective in what it is designed to do. There are empirical results to back that claim, but I strongly believe that the theoretical results fall short and feel out of place in the overall justification for the proposed method. This, along with my earlier point of lack of novelty are the basis for my decision.\n\n\nReferences:\n[1] Kendall, Alex, and Yarin Gal. \"What uncertainties do we need in bayesian deep learning for computer vision?.\" Advances in neural information processing systems. 2017.\n[2] Bloesch, M., Czarnowski, J., Clark, R., Leutenegger, S., & Davison, A. J. (2018). CodeSLAM-Learning a Compact, Optimisable Representation for Dense Visual SLAM. CVPR 2018.\n[3] Dorta, G., Vicente, S., Agapito, L., Campbell, N. D., & Simpson, I. (2018, February). Structured Uncertainty Prediction Networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.", "title": "Lack of novelty and weak theoretical results", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Hyxkbojj2Q": {"type": "review", "replyto": "HJxyAjRcFX", "review": "The paper describes an alternative to L1/L2 errors (wrt output and one ground-truth example) that are used to augment adversarial losses when training conditional GANs. While these augmented losses are often needed to stabilize and guide GAN training, the authors argue that they also bias the optimization of the generator towards mode collapse. To address this, the method proposes two kinds of alternate losses--both of which essentially generate multiple sample outputs from the same input, fit these with a Gaussian distribution by computing the generating sample mean and variance, and try to maximize the likelihood of the true training output under this distribution. The paper provides theoretical and empirical analysis to show that the proposed approach leads to generators that produce samples that are both diverse and high-quality.\n\nI think this is a good paper and solves an important problem---where one usually had to sacrifice diversity to obtain stable training by adding a reconstruction loss. I recommend acceptance.\n\nAn interesting ablation experiment might be to see what happens when one no longer includes the GAN loss and trains only with the MLMM or MCMLE losses, and compare this to training with only the L1/L2 losses. The other thing I'd like the authors to comment on are the potential shortcomings of using a simple un-correlated Gaussian to model the sample distributions. It seems that such a distribution may not capture the fact that multiple dimensions of the output (i.e., multiple pixel intensities) are not independent conditioned on the input. Perhaps, it may be worth exploring whether Gaussians with general co-variance matrices, or independent in some de-correlated space (learned from say simply the set of outputs) may increase the efficacy of these losses.\n\n====Post-rebuttal\n\nI've read the other reviews and retain my positive impression of the paper. I also appreciate that the authors have conducted additional experiments based on my (non-binding) suggestions---and the results are indeed interesting. I am upgrading my score accordingly.", "title": "Accept", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1gG_obrAQ": {"type": "rebuttal", "replyto": "B1e85wUE67", "comment": "\n1. The scope of our proof\n===================================\nThat\u2019s a great point. We have to make it clear in the draft. Our proof is confined to conditional GAN models with no explicit latent variable. Since the explicit latent variables provide the model with a vehicle that can represent variability and multimodality, our argument in section 4.4 may not be applicable to the models that explicitly encode latent variables. We add this discussion to the end of section 4.4.\n\n2. BicycleGAN\n===================================\nBicycleGAN has been applied to image-to-image translation, but not to image inpainting and super-resolution. Thus, we cannot find any standard implementation (or learned parameters) of BicycleGAN for the two tasks, which was the main reason why we did not report its results on the two tasks - image inpainting and super-resolution. \n", "title": "Thank you for your interest in our paper."}, "B1l6yj-H0m": {"type": "rebuttal", "replyto": "rJxtHml92m", "comment": "We are sincerely grateful for Reviewer 3\u2019s thoughtful review. Please see blue fonts in the newly uploaded draft to check how our paper is updated.\n\n1.  Novelty\n===================================\nWe believe that our work has significant novelties as follows:\n\n(1) To the best of our knowledge, our work is the first to formally criticize the use of reconstruction loss in conditional GANs. We also connect this problem to mode collapse (lose of multimodality). Of the prior works in conditional generation tasks, several papers empirically mention the loss of stochasticity in conditional GANs. However, they fail to analyze why this happens or propose what solutions can solve this problem. On the other hand, we reveal that the GAN loss and the reconstruction loss cannot coexist in harmony, and propose a solution to overcome this problem.\n\n(2) We propose alternatives to the reconstruction loss to greatly improve the multimodality of conditional GANs. As  Reviewer 3 pointed out, the components of our methods, MLE and moment matching, are well-established ideas. However, it is novel to combine them as a solution to the loss of multimodality in conditional generation. Furthermore, we think the simplicity of our methods is not a weakness but a strength, which makes our methods easily applicable to a wide range of conditional generation tasks.\n\n2.  Specific comments on organization and drawn conclusions\n===================================\nWe reorganize section 3.2 and 4.4 to reflect Reviewer 3\u2019s suggestion. Specifically, we simplify section 3.2 and move some content about reconstruction loss from 4.4 to 3.2. \n\nWe agree with Reviewer 3 that the conclusion of section 4.4 may be rather over-stated. Our proof says that any generator cannot be optimal to both GAN and L2 loss simultaneously. It does not prove the generator is underperforming or suboptimal. Therefore, we remove the term \u2018suboptimal\u2019 and tone down the overall argument.\n\nWe also cite the papers that Reviewer 3 suggested.\n", "title": "Answers to Reviewer 3"}, "H1ljacZB0X": {"type": "rebuttal", "replyto": "S1gPNubqn7", "comment": "We thank Reviewer 2 for positive and constructive reviews. Please see blue fonts in the newly uploaded draft to check how our paper is updated.\n\n1.  Convergence speed\n===================================\nWe observe that our methods need more training steps (about 1.5x) to generate high-quality images compared to that with the reconstruction loss. It might be obvious because our methods train the model to generate a much wider range of outputs. We add some comments to Appendix B.1 regarding the convergence speed.\n\n2.  Training stability\n===================================\nMLMM is similar to the reconstruction loss in terms of training stability. Encouragingly, our methods stably work with a large range of hyperparameter \\lambda. For example, the loss coefficient of MLMM is settable across several orders of magnitude (from tens to thousands) with similar results. However, as noted in the paper, MCMLE is unstable compared to MLMM.\n\n3.  Why only MLMM_1 is not compared\n===================================\nDue to many combinations between our methods and tasks, we had to choose only a few of our methods for human evaluation. Although MLMM_1 and MLMM_{1/2} attained similar performance for all three tasks, we chose MLMM_{1/2} as the \u2018default\u2019 method because it better implements our idea - matching more statistics (i.e. not only means but also variances). \n", "title": "Answers to Reviewer 2"}, "Byer85ZHAQ": {"type": "rebuttal", "replyto": "Hyxkbojj2Q", "comment": "We thank Reviewer 1 for your encouraging and constructive comments. Please see blue fonts in the newly uploaded draft to check how our paper is updated.\n\n1.  Ablation experiments\n===================================\nWe carry out the ablation experiments and present the results in appendix G (page 22). The results are indeed interesting. When trained with MLMM_1 or MCMLE_1 only, the outputs are indistinguishable from those with the reconstruction loss only, since there is no variation-inducing term to generate diverse output. In the case of MLMM_{1/2} and MCMLE_{1/2}, the model shows high variation in the output. However, the patterns of the variations differ greatly. Specifically, MLMM_{1/2} shows variations in low-frequency while MCMLE_{1/2} shows those in high-frequency.\n\nWe also add experiments of using GAN loss, MLMM_{1/2} loss, and reconstruction loss altogether. Whiling fixing the coefficient of GAN loss and MLMM loss to 1 and 10 respectively, we gradually increase the coefficient of reconstruction loss from 0 to 100. We find that the output variation decreases as the reconstruction loss increases. Interestingly, the sample quality is high when the reconstruction loss is absolutely zero or dominated by the MLMM loss. In contrast, the samples show poor quality when the reconstruction coefficient is 1 or 10. It seems that either method can assist the GAN loss to find visually appealing local optima but the joint use of them leads to a troublesome behavior.\n\n2.  Shortcomings of un-correlated Gaussian\n===================================\nThis is a very interesting and profound question that may need to be further investigated in the future work. In summary, we believe that incorporating more statistics is not guaranteed to improve the performance, and un-correlated Gaussian may not be a bad choice.\n\nAn ideal GAN loss can match with any kind of statistics since it minimizes the JS divergence between sample distribution and real distribution. In this sense, additional loss term should be regarded as a \u2018guidance\u2019 signal, while the key player is still the GAN loss. However, it is unclear whether a tighter guidance necessarily yields better outputs.\n\nRegarding the tightness of guidance, the loss terms can be ordered as follows:\nMLMM_1 = MCMLE_1 < MLMM_{1/2} = MCMLE_{1/2} < general covariance Gaussian.\n\nInterestingly, our qualitative evaluations show that MLMM_1 and MCMLE_1 generate comparable or even better outputs compared to MLMM_{1/2} and MCMLE_{1/2}. That is, matching means could be enough to guide GAN training in many cases. Adding more statistics may be helpful in some cases, but generally may not improve the performance. Moreover, we should consider the errors arising from the statistics prediction because a wrong estimation of statistics can even misguide the GAN training. \n\nPlease see blue fonts in section 5.2 of the newly uploaded draft to check how our paper is updated.\n", "title": "Answers to Reviewer 1"}, "S1gPNubqn7": {"type": "review", "replyto": "HJxyAjRcFX", "review": "This paper analyzes the model collapse problems on training conditional GANs and attribute it to the mismatch between GAN loss and reconstruction loss. This paper also proposes new types of reconstruction loss by measuring higher statistics for better multimodal conditional generation.\n\nPros:\n1.\tThe analysis in Sec 4.4 is insightful, which partially explains the success of MLMM and MCMLE over previous method in generating diverse conditional outputs.\n2.\tThe paper is well written and easy to follow.\n\nCons:\nAnalysis on the experiments is a little insufficient, as shown below.\n\nI have some questions (and suggestions) about experiments. \n1.\tHow does the training process affected by changing the reconstruction loss (e.g., how the training curve changes?)? Do MLMM and MCMLE converge slower or faster than the original ones? What about training stability? \n2.\tWhy only MLMM_1 is not compared with other methods on SRGAN-celebA and GLCIC-A? From pix2pix cases it seems that Gaussian MLMM_1 performs much better than MLMM_{1/2}.\n", "title": "An interesting paper in analyzing and improving model collapse problems in conditional GANs", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}