{"paper": {"title": "Deep Information Propagation", "authors": ["Samuel S. Schoenholz", "Justin Gilmer", "Surya Ganguli", "Jascha Sohl-Dickstein"], "authorids": ["schsam@google.com", "gilmer@google.com", "sganguli@stanford.edu", "jaschasd@google.com"], "summary": "We predict whether randomly initialized neural networks can be trained by studying whether or not information can travel through them.", "abstract": "We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively.", "keywords": ["Theory", "Deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This is one of the two top papers in my stack. In total the reviews are a little bit on the light side in terms of level of detail and there are some concerns regarding how useful the results are from a practical point of view. However, I am confident that the paper should be accepted. "}, "review": {"B1e8CWvUl": {"type": "rebuttal", "replyto": "rJGLovrHx", "comment": "Thanks for your comment. During evaluation of both the test / training accuracy dropout is indeed turned off.", "title": "Response"}, "BJcyCWwLl": {"type": "rebuttal", "replyto": "H1gjgmjEx", "comment": "Thank you for your thoughtful comments! They have prompted us to clarify several points in the paper. \n\n>> Minor point on presentation: Speaking of the \"evolution\" of x_{i;a} as it travels through the network could give some readers helpful intuition, but for me it was confusing because x_{*;a} is the immutable input vector, and it's the just-introduced z and y variables that represent its so-called evolution, no?\n\nThis is an interesting point. You are correct that in some sense the x_{i;a} is more of an initial condition for the dynamics which evolve through the z and y variables. Nonetheless, we do feel that describing the \"evolution\" of x_{i;a} with slightly looser language might be useful to some. We have added a small clarifying remark in the paper to this point.\n\n>> In interpreting this analysis - A network may be trainable if information does not pass through it, if the training steps, by whatever reason, perturb the weights so that information starts to pass through it (without subsequently perturbing the weights to stop information from passing through it.) Perhaps this could be clarified by a definition of \u201ctraining algorithm\u201d?\n\nIt is true that the weights of a network could be perturbed from an initialization in which no information could flow through it to one close enough to criticality such that information could once again travel through the network. Such a procedure would by construction be dataset / label independent. We therefore feel that this would amount to pre-training, as opposed to training, the neural network.\n\nHaving said this, we feel that exploring pre-training schemes on the basis of information flow and criticality would be very interesting to pursue and we have added a short discussion to that effect to our manuscript.\n\n>>Comments on central claims:\n>>Previous work on initializing neural networks to promote information flow (e.g. Glorot & Bengio, http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf) concluded (1) that the number of units in the next layer and the previous layer should both figure into the variance of the elements of the weight matrices, and (2) that they should be drawn from a Uniform distribution rather than a Gaussian. Could the authors comment on the merit of that initialization strategy in light of this analysis?\n\nThank you for pointing out this interesting paper by Glorot & Bengio. We have added a short discussion of their work to our paper. To respond to your specific points: \n\n(1) They find that the number of units in the next layer and previous layer are relevant for gradient backpropagation (as opposed to signal forward propagation.) This is true for us as well (see eq. 15). Indeed when we discuss vanishing/exploding gradients we restrict our attention to networks of constant width for this reason. A more thorough discussion of non-constant width networks would be interesting. \n\n(2) The crux of the mean field approximation rests with the use of the central limit theorem to approximate the pre-activations as a gaussian. So long as the weights are i.i.d. the specific distribution of the weights will not affect our results (note that this is not the case for the bias distribution.) Thus, we expect our results to hold for uniformly distributed weights.  \n\nOne caveat is that our results hold only for nonzero bias variance (which is not what is done in Glorot & Bengio). The phase diagram when the bias variance is zero is slightly different as is the accompanying analysis.\n\n>>Comments on evaluation:\n>>Why does the dashed line in the result figures correspond to twice the depth scale instead of just the depth scale? What is the significance of 14000 steps of SGD on MNIST? Does it represent convergence of SGD? Why are all the best SGD models well above the depth scale? Why is there a little dark area precisely under the peak in Figure 5(a) and (c)? That\u2019s interesting - initializations that propagate error best seem untrainable at the depths traditionally used - but only with SGD not RMSProp?\n\nThe bright red region above the depth scale and the dark area below the peak were a result of plotting inconsistencies. First, the small dark area referenced in fig 5 (a) and (c) was a missing datapoint. Second, the red region was due to a poor choice of binning and is not statistically significant. Thank you for being so diligent. We have updated the figure to include this missing point and round our training accuracy.\n\nRelated to this point, one might notice that at large depths right at criticality our previous results made it seem as though networks were untrainable. This was due to a confluence of two effects: first, the grid spacing we used in generating fig 5 was too large when the depth of the networks became large and second, the learning rate must be lowered as the depth increases (as was noted in the paper by Saxe et al. (2014)). We therefore reran the networks for figure 5 using a smaller grid spacing and lower learning rate near criticality. We have updated the corresponding figures.\n\nYour question on our choices for the experiments is a good one. When $L = n\\xi_c$ for some integer n, our results show that the signal passing through the network is attenuated by a factor of $e^n$. Therefore, it is likely (and obviously true) that networks can be trained when they are deeper than $\\xi_c$. The exact pre-factor of $\\xi_c$ is not so important as the overall form of $\\xi_c$. We have included a short discussion of this fact. We have also added to fig. 5(a) different curves corresponding to different $n\\xi_c$. We find, approximately, that networks can be trained when $L<6\\xi_c$. We have therefore updated all of the figures in the paper to use the line $6\\xi_c$ instead of $2\\xi_c$.\n\nThere is no real significance of 14k steps of SGD on MNIST except that it is long enough that the vast majority of the SGD runs have converged.\n\n>>The accuracy of the trained models on CIFAR-10 and MNIST are not reported - it seems important to the overall argument of the paper that the sorts of networks underlying Figures 5 and 6 are the same as the ones that people would consider state-of-art within the model class (fully connected, sigmoidal nonlinearities, etc.).\n\nIt is important to note that our theoretical results are only valid in the pre-training regime and bound whether or not a specific architecture can be trained. As yet, they do not have any obvious connection with the final accuracy. Having said this, once our models are sufficiently large they all overfit both CIFAR-10 and MNIST (and so have perfect training accuracy.) The test accuracy of our models is therefore somewhat low (~98% in the case of MNIST and ~55% in the case of CIFAR10), but simultaneously is not a quantity that we were particularly interested in maximizing. We agree that we should have reported these numbers and have added them to the text.\n\n", "title": "Response"}, "rJGLovrHx": {"type": "rebuttal", "replyto": "H1W1UN9gg", "comment": "Just making sure, your experiments with dropout follows the convention that during evaluation, it is turned off?", "title": "dropout"}, "S1K7RfsVx": {"type": "review", "replyto": "H1W1UN9gg", "review": "-I'm not familiar enough with mean-field techniques to judge the soundness of Eq 2, but I'm willing to roll with it.\n\nMinor point on presentation: Speaking of the \"evolution\" of x_{i;a} as it travels through the network could give some readers helpful intuition, but for me it was confusing because x_{*;a} is the immutable input vector, and it's the just-introduced z and y variables that represent its so-called evolution, no?\n\nIn interpreting this analysis - A network may be trainable if information does not pass through it, if the training steps, by whatever reason, perturb the weights so that information starts to pass through it (without subsequently perturbing the weights to stop information from passing through it.) Perhaps this could be clarified by a definition of \u201ctraining algorithm\u201d?\n\nComments on central claims:\nPrevious work on initializing neural networks to promote information flow (e.g. Glorot & Bengio, http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf) concluded (1) that the number of units in the next layer and the previous layer should both figure into the variance of the elements of the weight matrices, and (2) that they should be drawn from a Uniform distribution rather than a Gaussian. Could the authors comment on the merit of that initialization strategy in light of this analysis?\n\nComments on evaluation:\nWhy does the dashed line in the result figures correspond to twice the depth scale instead of just the depth scale? What is the significance of 14000 steps of SGD on MNIST? Does it represent convergence of SGD? Why are all the best SGD models well above the depth scale? Why is there a little dark area precisely under the peak in Figure 5(a) and (c)? That\u2019s interesting - initializations that propagate error best seem untrainable at the depths traditionally used - but only with SGD not RMSProp?\n\nThe accuracy of the trained models on CIFAR-10 and MNIST are not reported - it seems important to the overall argument of the paper that the sorts of networks underlying Figures 5 and 6 are the same as the ones that people would consider state-of-art within the model class (fully connected, sigmoidal nonlinearities, etc.).\n", "title": "-", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "H1gjgmjEx": {"type": "review", "replyto": "H1W1UN9gg", "review": "-I'm not familiar enough with mean-field techniques to judge the soundness of Eq 2, but I'm willing to roll with it.\n\nMinor point on presentation: Speaking of the \"evolution\" of x_{i;a} as it travels through the network could give some readers helpful intuition, but for me it was confusing because x_{*;a} is the immutable input vector, and it's the just-introduced z and y variables that represent its so-called evolution, no?\n\nIn interpreting this analysis - A network may be trainable if information does not pass through it, if the training steps, by whatever reason, perturb the weights so that information starts to pass through it (without subsequently perturbing the weights to stop information from passing through it.) Perhaps this could be clarified by a definition of \u201ctraining algorithm\u201d?\n\nComments on central claims:\nPrevious work on initializing neural networks to promote information flow (e.g. Glorot & Bengio, http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf) concluded (1) that the number of units in the next layer and the previous layer should both figure into the variance of the elements of the weight matrices, and (2) that they should be drawn from a Uniform distribution rather than a Gaussian. Could the authors comment on the merit of that initialization strategy in light of this analysis?\n\nComments on evaluation:\nWhy does the dashed line in the result figures correspond to twice the depth scale instead of just the depth scale? What is the significance of 14000 steps of SGD on MNIST? Does it represent convergence of SGD? Why are all the best SGD models well above the depth scale? Why is there a little dark area precisely under the peak in Figure 5(a) and (c)? That\u2019s interesting - initializations that propagate error best seem untrainable at the depths traditionally used - but only with SGD not RMSProp?\n\nThe accuracy of the trained models on CIFAR-10 and MNIST are not reported - it seems important to the overall argument of the paper that the sorts of networks underlying Figures 5 and 6 are the same as the ones that people would consider state-of-art within the model class (fully connected, sigmoidal nonlinearities, etc.).\n", "title": "-", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}}