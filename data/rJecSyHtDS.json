{"paper": {"title": "Learning to Recognize the Unseen Visual Predicates", "authors": ["Defa Zhu", "Si Liu", "Wentao Jiang", "Guanbin Li", "Tianyi Wu", "Guodong Guo"], "authorids": ["zhudefa@iie.ac.cn", "liusi@buaa.edu.cn", "jiangwentao@buaa.edu.cn", "liguanbin@mail.sysu.edu.cn", "wutianyi01@baidu.com", "guoguodong01@baidu.com"], "summary": "We propose and address a new problem named predicate zero-shot learning in visual relationship recognition. ", "abstract": "Visual relationship recognition models are limited in the ability to generalize from finite seen predicates to unseen ones. We propose a new problem setting named predicate zero-shot learning (PZSL): learning to recognize the predicates without training data. It is unlike the previous zero-shot learning problem on visual relationship recognition which learns to recognize the unseen relationship triplets (<subject, predicate, object>) but requires all components (subject, predicate, and object) to be seen in the training set. For the PZSL problem, however, the models are expected to recognize the diverse even unseen predicates, which is meaningful for many downstream high-level tasks, like visual question answering, to handle complex scenes and open questions. The PZSL is a very challenging task since the predicates are very abstract and follow an extreme long-tail distribution. To address the PZSL problem, we present a model that performs compatibility learning leveraging the linguistic priors from the corpus and knowledge base. An unbalanced sampled-softmax is further developed to tackle the extreme long-tail distribution of predicates. Finally, the experiments are conducted to analyze the problem and verify the effectiveness of our methods. The dataset and source code will be released for further study. ", "keywords": ["Visual Relationship Detection", "Scene Graph Generation", "Knowledge", "Zero-shot Learning"]}, "meta": {"decision": "Reject", "comment": "The paper proposes a new problem setting of predicate zero-shot learning for visual relation recognition for the setting when some of the predicates are missing, and a model that is able to address it.\n\nAll reviewers agreed that the problem setting is interesting and important, but had reservations about the proposed model. In particular, the reviewers were concerned that it is too simple of a step from existing methods. One reviewer also pointed towards potential comparisons with other zero-shot methods.\n\nFollowing that discussion, I recommend rejection at this time but highly encourage the authors to take the feedback into account and resubmit to another venue."}, "review": {"ryeMPURijr": {"type": "rebuttal", "replyto": "B1eKItmTFr", "comment": "We thank the reviewer for tending towards accepting our work and mentioning that our work has \u201ca few strong contributions\u201d. The \u201cminor edits\u201d are done in the revised version. \n \n*** Response to the things that could be strengthened or addressed further ***\n \nQ 1. There could be more meaningful comparison to other zero-shot learning algorithms. \nA 1. Thanks for the great suggestion. Due to the time limit, we will add such results after the rebuttal period. \n \n \nQ 2. Why was the unbalanced sampled-softmax was being used for only predicate prediction and not entity prediction? \nA 2. (Paragraph 2, Sec 5, P6). The recognition of the entity is a full-supervised task. Since entity recognition is not the focus of this work, we have controlled the number of object categories by clustering with hypernym relationships. After that, the distribution of entities is relatively balanced. In this setting, the softmax has a good recognition effect.\n \n \nQ 3. It wasn\u2019t totally clear to the reviewer whether all of the verbs/entities were in WordNet and/or Glove.\nA 3. \na. All of the verbs/entities are in WordNet. \nIn this problem setting, we let all the predicate categories to exist in WordNet.\nAn original annotation in Visual Genome is shown in the follows: \n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\"predicate\": \"has\", \n\u2026\n\"relationship_canon\": \n[ { \"synset_name\": \"have.v.01\", \"synset_definition\": \"have or possess, either in a concrete or an abstract sense\" } ] \n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\nThe relationship instances are not only labeled with the \u201cpredicate\u201d (in natural language), but also with the synset (node of WordNet) corresponding to the \u201cpredicate\u201d. We filter out all relationship instances without labeling synset and used the synset to replace the original predicate, e.g., \u201chave.v.01\u201d replace \u201chas\u201d. Thus, all the predicate labels are in WordNet. \n \nb. Are all of the verbs/entities in Glove?\nTake the above sample as an example, the definition of \u201chave.v.01\u201d is \u201chave or possess, either in a concrete or an abstract sense\u201d. We take all the words of the definition sentence as tokens to obtain the word embeddings and average all the word embeddings. We ignore the word not in GloVe. \n \nQ 4. Can human performance on this task be measured?\nA 4. Thanks for the great suggestion. The experiment is ongoing. \nWe are calling on undergraduates to perform the experiment and the process of the user study is proposed as: \n1. A test image with two boxes is shown to the user. \n2. The user types the keywords into the interface. \n3. The system searches the predicates with the input keywords. \n4. The user selects several output predicates as the answers. \nIn the final version paper, we will report the human experimental results and details. ", "title": "Response to Reviewer #1"}, "BJg4MLRsjS": {"type": "rebuttal", "replyto": "BJeeZWtaFr", "comment": "We thank the reviewer for the appreciation of this paper and have followed the valuable advice to revise the paper. \n \n*** Response to the main questions ***\nQ 1. The special properties and challenges of predicate zero-shot learning. \nA 1. \nProperties of PZSL are summarized as follows: \na) PZSL is very necessary since the difficulty of labeling. To avoid exhaustively traversing all pairs ($O(N^2)$) of entities ($O(N)$), the labels of predicates (VG) are just extracted based on image descriptions. In this process, the predicates are seriously missing. It reflects the necessity of PZSL. \n\nb) PZSL is significant for downstream tasks. VRD is often used to generate a scene graph (SG, edge: predicate, node: entity) for visual reasoning (like VQA). As long as the SG is restricted to a closed vocabulary, the reasoning engine [1,2] is inevitably difficult to cope with the open questions with very diverse unseen predicates or entities. For visual reasoning, PZSL may make the state transition (the edge of SG) more generalized. (Paragraph 2, Sec 1)\n\nChallenges of PZSL are summarized as follows:\nc) Recognizing predicates is very difficult. 1). It is difficult to represent predicate. On the one hand, predicates are often abstract not as specific as objects. On the other hand, the representation of predicates also depends on that of subjects and objects. How to effectively represent the abstract predicates is still a hard problem. 2) Analogizing the seen abstract predicates to the unseen ones further brings in new difficulty. Furthermore, unlike many object ZSL methods, they adopt the pre-defined attributes of objects to recognize the unseen object. However, it is difficult to define the attributes of predicates. (Paragraph 3, Sec 1)\n \nd) The long-tail distribution of predicates is significantly more severe than that of objects. As suggested by the reviewer, the extreme long-tail distribution of predicates is presented in Fig. 4 of Appendix A. In the VG-Zero dataset, the instances' number of the top $5$ predicates count for about $80\\%$ of the total predicate instances. Under this distribution, the model tends to collapse to output few frequent predicates. As an initial solution to PZSL, we have to give priority to solving the extreme long-tail distribution (unbalanced sampled-softmax). (Paragraph 3, Sec 1)\n\n[1] Hudson D A, Manning C D. Learning by abstraction: The neural state machine.\n[2] Mao J, Gan C, Kohli P, et al. The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision.\n\n \nQ 2. The proposed method looks correct but is a rather direct application of existing methods. \nA 2. \na. We are the first to propose the unbalanced sampled-softmax to handle the extreme long-tail distribution. The most similar method Sampled Softmax, proposed in NLP for the large-scale vocabulary, fails under the extreme long-tail distribution. \n \nb. The graph sampling is borrowed from the recommender system domain after sufficient problem modeling and is very suitable to deal with large-scale KG for VRD problem. \n\n*** Response to the additional questions ***\nQ 3. How well would a human do on this task? \nA 3. Thanks for the great suggestion; the experiment is ongoing. \nWe are calling on undergraduates to perform the experiment, where the process of the user study is proposed as: \n1. A test image with two boxes is shown to the user. \n2. The user types the keywords into the interface. \n3. The system searches the predicates with the input keywords. \n4. The user selects several output predicates as the answers. \nIn the final version paper, we will report the human experimental results and details. \n \nQ 4. How much would Hits@k be if the test label is seen during training (not zero-shot)?\nA 4. We test the model in Row 7, Table 2. The results of the generalized setting are as follows: \n \nTest set                     | Hit@5 Hit@10 Hit@20\n====================================\nUnseen predicates |   4.1         7.4       11.8\nSeen predicates      |  67.5        75.7     81.6\nMixed predicates    |  62.0        69.7     75.4\n \n\u201cSeen predicates\u201d means we only test on the region pairs that labeled with the seen predicates. \n\u201cMixed predicates\u201d means we test on all the region pairs from the test images. ", "title": "Response to Reviewer #2 "}, "BJg-INCsjr": {"type": "rebuttal", "replyto": "rkxZmVqCYS", "comment": "Q 5. Do you plan to provide the proposed dataset splits so others can work on this setting? \nA 5. Yes, we will release the dataset as well as the splits setting. Source code will also be released to facilitate further researches. \n \n \nQ 6 (Extra). \u201cI have a question regarding the ablation studies with GloVe, Normal and InferSent initialization. The question is whether this initialization is necessary?\u201d \nA 6. The experimental results in Row 1 (only with GloVe)\uff0c14 (only with WordNet) and 3 (with both) in Table 2 are analyzed as follows. \n \nThe generalized setting of PZSL is a very difficult test setting. Note that we only constrain the visual features matching the embeddings of the seen predicates during training. Thus, the visual features are more likely to match the embeddings of seen predicates. As the GT labels in the test set only containing the unseen predicates, all the seen predicates are wrong answers during testing, which makes the task difficult (both versions \u201conly with GloVe\u201d and \u201conly with WordNet\u201d get 0.0 accuracy). \n \nObserving on the traditional setting (only search on the unseen predicates for inference), the version \u201conly with GloVe\u201d (37.9 Hit@20) significantly outperforms the version \u201conly with WordNet\u201d (19.6 Hit@20). Furthermore, combining these two versions (\u201cwith both\u201d) will achieve significant improvement (48.7 Hit@20). \n \nThe reason why the version \u201conly with WordNet\u201d get a poor performance can be analyzed as follows: \nThe GCN is just trained to align the visual features and semantic embeddings of the seen predicates. In this process, we have NO explicit constraints to build a good embedding space, such as putting together the semantically similar predicates. Due to the randomness of the initial embedding, the initial embeddings of two semantically similar predicates may be quite different. In this setting, the GCN is not explicitly supervised to pull closer the two predicates in the embedding space. Therefore, there is no guarantee that the randomly initialized embedding is able to achieve good results with WordNet. \n \nQ 7. \u201cThe represented graphs show Object to Object and Predicate to Predicate connections which I\u2019m not sure if it is correct?\u201d \nA 7. Object to-Object and Predicate-to-Predicate connections do exist in WordNet. \nActually, the synsets (nodes in WordNet) are used to replace the original predicate categories. As shown in the following example, we use \u201chave.v.01\u201d to replace \u201chas\u201d as the predicate category. \n \nAn original annotation in Visual Genome is shown in the follows (Please focus on the \u201csynset_name\u201d): \n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\"predicate\": \"has\", \n\u2026\n\"relationship_canon\": \n[ { \"synset_name\": \"have.v.01\", \"synset_definition\": \"have or possess, either in a concrete or an abstract sense\" } ] \n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n \nWordNet contains the linguistic relationships of the synsets. For example, \u201chave.v.01\u201d is the hypernym of \u201cstock.v.01\u201d (the definition of \u201cstock.v.01\u201d is \u201chave on hand\u201d). Thus, there is an edge between \u201chave.v.01\u201d and \u201cstock.v.01\u201d. This explains why Predicate-to-Predicate connections exist. Similarly, Object-to-Object connections also exist. \n \nAs mentioned in Paragraph \u201cKnowledge graph\u201d, Sec 5, we use the following linguistic relationships (provided in WordNet) to build edges: \nhypernym, \nhyponym, \npart meronym, \npart holonym, \nsubstance meronym, \nsubstance holonym, \nentailment, \nsubstance holonym \nand sharing lemmas. \n \nAs long as there is one linguistic relationship between two nodes, there will be one edge. \n", "title": "Response to Reviewer #3 (Response to comments and feedback, 2/2)"}, "HJgsbSRisH": {"type": "rebuttal", "replyto": "rkxZmVqCYS", "comment": "We thank the reviewer for the valuable comments and constructive suggestions on our paper. The minor errors have been revised and the related references have also been added. \n \n*** Response to the main discussion ***\nQ 1. Comparing \u201cfast GCN\u201d and \u201cPinSage\u201d. Why the authors have changed the name of PinSage and just mentioned that \u201ctheir\u201d \u201cFast Graph Convolution Network\u201d is \u201cinspired\u201d from PinSage? \nA 1. Thanks for the valuable comments. \u201cFast GCN\u201d is a simplified version of \u201cPinSage\u201d. The name \u201cFast GCN\u201d tends to cause confusion so that we have changed the name to \u201cSimplified PinSage\u201d. \n \nThe components in PinSage that we don't use include: \n1. Producer-consumer minibatch construction\n2. Efficient MapReduce inference\n3. Importance pooling\n4. Curriculum training\n \nThe components in PinSage that we use include: \n1. The architecture of GCN. \n2. The node sampling strategy in minibatch, which we call \u201con-demand sampling\u201d. \n3. The neighbor sampling strategy is a little different: \nPinSage: adopt the \u201cimportance pooling\u201d to sample neighborhoods.\nOurs: We randomly sample neighborhoods. \n \n \nQ 2. \u201cgeneralized\u201d and \u201ctraditional\u201d settings lack clarity.\nA 2. The revised definition is shown as follows: \nThe seen predicates are the predicates that appear in the training set, while the unseen predicates are those not in the training set. For the generalized setting, the methods search on the seen and unseen predicates vocabulary $\\mathcal{V}_{tr} \\cup \\mathcal{V}_{te}$ for inference. For the traditional setting, the methods only search on the unseen predicates vocabulary $\\mathcal{V}_{te}$. Note that the generalized setting is more challenging since the search space of the generalized setting is significantly larger than that of the traditional one. \n \n \nQ 3. Please define the metrics clearly (Hits@k). \nA 3. The definition of the metrics is revised as: \nWe evaluate methods with Hit@k on generalized and traditional settings (Table 1 and 2): Given any test region pair, we check whether the GT label falls within the predicted categories with top k score. If yes, the sample is counted as a $Hit$. The Hit@k is $\\frac{\\#Hit}{\\#Test\\ region\\ pair}$.\n \n \nQ 4. The descriptions for Table 1 and Table 2 fail to provide enough details to help understand the difference between the results in these two tables. \nA 4. \nIn Table 1 (Accuracy of unseen predicate recognition), the methods are only required to \u201chit\u201d the unseen predicates ignoring the subjects and objects. \n \nIn Table 2 (Accuracy of recognition of triplets with unseen predicates), the methods are required to \u201chit\u201d the full relationship triplets (subject, predicate, object). Only when all the elements of the triplet are \u201chit\u201d simultaneously can it be counted as a \u201chit\u201d. ", "title": "Response to Reviewer #3 (Main discussion, 1/2) "}, "H1lPSWAooH": {"type": "rebuttal", "replyto": "rJecSyHtDS", "comment": "We thank reviewers for the very constructive comments and appreciation. We have revised the paper based on most of the comments. Due to the time limit, some experiments are ongoing and will be added in the final version. Please refer to the updated paper and answers to each reviewer.", "title": "To all reviewers"}, "B1eKItmTFr": {"type": "review", "replyto": "rJecSyHtDS", "review": "This paper creates a new task for zero-shot learning of predicates (specifically in cases where the individual predicate components have never been seen in the training, rather than the more traditional setting where the full s-v-o relationship is unseen but each component is).  They create a new subset of the Visual Genome dataset specifically targeted towards predicting unseen predicates.  They also provide results using a knowledge graph (in this case WordNet), to integrate linguistic and visual features for prediction.  Interestingly, their pipeline introduces a new softmax variant, the unbalanced sampled softmax, which addresses the problem of over-predicting common predicates.\n\nI generally tend towards accepting this paper.  The reasons being that this paper has a few strong contributions: (1) they design a new task set-up with data they selected and cleaned from VG, (2) new modelling pipeline with empirical analysis backing the modeling choices, and (3) new softmax variant.  \n\nA few comments about things that could be strengthened or addressed further: \n- There could be more meaningful comparison to other zero-shot learning algorithms.  Even if they are not fully comparable because they were originally meant for a slightly different zsl set-up, it would be nice to have more baselines from external work.\n- Why was the unbalanced sampled softmax was being used for only predicate prediction and not entity prediction? \n- It wasn\u2019t totally clear to me whether all of the verbs/entities were in WordNet and/or Glove.  If not, can the authors clarify what the overlap was and how this might be affecting performance?\n- As noted by the authors, there are some cases in Figure 3 where humans would consider the answer to be confusing or might actually prefer the machine response.  Can human performance on this task could be measured?  Perhaps humans could possibly evaluate a subset of the machine vs. gold answers?\n\nMinor Edits:\n- Related work: WordNet is misspelled in the last line\n- Figure 3: please consider using colors other than red/green, this is not readable for color-blind readers\n- Section 5.2: \u201cThe case c is confusing that even\u201d \u2192  \u201cThe case c is so confusing that even\u201d\n- Section 5.2: \u201cand output a more appropriate\u201d --> \u201cand outputs a more appropriate\u201d\n- In the references: the Devise paper by [Frome et al 2013] is listed twice\n- In the references: the first author of ConceptNet5 should be Robyn Speer", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 2}, "BJeeZWtaFr": {"type": "review", "replyto": "rJecSyHtDS", "review": "The paper considers the task of predicting visual predicates (e.g., eat, bite, take) between pairs of entities. In particular, the paper focuses on the zero-shot setting where the test predicates are unseen during training. The model uses linguistic prior from a knowledge graph (WordNet):\nwith graph embedding (fast GCN), unseen predicates are embedded based on the information propagated from seen predicates. The model is trained so that the visual feature vector and the correct predicate embedding are nearby in the joint embedding space. The method was evaluated on a zero-shot split of the Visual Genome dataset.\n\nOverall, as a task and dataset paper, the paper should have sold the task more by highlighting its special properties. While the task of predicting unseen predicates is interesting, the setting and the technique are similar to previous work on predicting other types of unseen labels (e.g., unseen objects, as referenced in the paper). The new task could still be interesting if it presents different challenges (e.g., maybe predicates are more ambiguous than objects, or visual predicates are harder to embed). But from the description in the paper, most of the challenges seem to also exist in other zero-shot settings (long-tail distribution; large space of labels). The paper could benefit from providing examples or statistics that demonstrate the challenges of the task.\n\nThe proposed method looks correct but is a rather direct application of existing methods. The experiment setup looks OK. Based on the error analysis, the labels look very noisy and subjective, but this seems to be a common problem in the visual predicate prediction task (hence the recall-based evaluation).\n\nAdditional questions:\n\n- The provided examples in the error analysis look pretty tricky; e.g., \"swing\" and \"slug\" are judged as different. How well would a human do on this task?\n\n- How much would Hit@k be if the test label is seen during training (not zero-shot)?\n", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 2}, "rkxZmVqCYS": {"type": "review", "replyto": "rJecSyHtDS", "review": "Title: Good work, requires some edits.\n\n1. Summarize:\n\nThis paper proposes a new problem setting in visual relation detection which is called \u201cPredicate Zero-shot Learning (PZSL)\u201d. They provide a clear motivation and description of this setting. They propose a solution to this problem which leverages linguistic priors and knowledge bases. Furthermore they propose an unbalanced sampled-softmax to tackle the long tail distribution of predicates.\n\n2. Clearly state your decision. One or two key reasons for this choice.\n\nI will go for a weak accept for the paper at this stage. (+) I think the proposed problem setting is well-motivated and useful. Also, (+) the proposed initial solution to this problem is interesting. However, (-) they propose a \u201cfast graph convolution network\u201d which seems to be precisely equivalent to a PinSage.  Also, (-) the paper requires to be polished as it lacks clarity.\n\n3. Main discussion\n\nMy first argument is: I\u2019m not sure why the authors have changed the name of PinSage and just mentioned that \u201ctheir\u201d \u201cFast Graph Convolution Network\u201d is \u201cinspired\u201d from PinSage. To me it looks exactly the same. If there are any differences, it should be stated clearly. In fact, I would not be against using PinSage as a part of their approach. However, trying to rename it without clear reasons is not a good idea.\n\nMy second argument is that the paper lacks clarity in writing (for detailed suggestions please refer to comments and feedbacks). Specially the evaluation section lacks details and clarity: a) In the beginning of this section (page 7), the authors talk about \u201cgeneralized\u201d and \u201ctraditional\u201d settings without properly defining them. b) The descriptions for Table 1 and Table 2 fail to provide enough details to help understand the difference between the results in these two tables (one of them states \u201cAccuracy of unseen predicate recognition\u201d and the other one \u201cAccuracy of recognition of triplets with unseen predicates\u201d). \n\n4. Comments and feedback.\n\nIntroduction: \n\nParagraph one in the: \n1. The relationship recognition methods are mainly supervised \u201cthat\u201d \u2192 \u201cto\u201d.\n2. last line: \u2026. and do not study \u201con generalizing\u201d \u2192 \u201cthe generalization of\u201d.\n\nParagraph two:\n1. no manual annotations or \u201creal samples\u201d \u2192 \u201cimage samples\u201d. (a real sample is ill-defined)\n2. For example, no instance of chew \u2192 For example \u201cgiven\u201d no instance of chew.\n\nParagraph three:\n1. \u2026 is difficult since predicates are often abstract not as specific \u2192   is difficult since predicates are often abstract \u201cand\u201d not as specific.\n2. Furthermore, unlike many object ZSL methods \u2026 \u2192 This line to the end is very complicated and hard to understand.\n\nRelated Works:\n\n1. Visual Relationships: I would cite \u201cGraph R-CNN for scene graph generation\u201d since it is the most relevant work regarding the similarity of pipeline (using GCNs).\n2. External Knowledge bases  (KB): I would cite \u201cImproving Visual Relationship Detection using\nSemantic Modeling of Scene Descriptions\u201d since it is one of the most relevant works using knowledge graph modelings to improve visual relation detection.\n\nProblem Setup:\n\nDo you plan to provide the proposed dataset splits so others can work on this setting? I consider this very important given your paper\u2019s contribution. Maybe it is better if it is also mentioned in the paper.\n\nPipeline:\n\n1. Paragraph 2: \u2026 the output of which is fused with \u2026: Given Figure 2, it does not seem like $V_p$ is being created by fusing $V_s$ and $V_o$. It looks more like it is extracted directly from the image (union of bounding boxes).\n\n2. In Figure 2: In the representation of Pipeline (A), the graph is colored by dark blue for objects and light blue for predicates. The represented graphs show Object to Object and Predicate to Predicate connections which I\u2019m not sure if it is correct. Shouldn\u2019t we always have a light blue between every pair of dark blue connections?\n\nEvaluation:\n\n1. Please consider the mentioned points in the Main Discussions.\n2. In Table 1, I suggest re-naming \u201cembedding\u201d to \u201cinitial embedding\u201d.\n3. In Table 1, Hit@k should be Hits@k.\n4. Please define the metrics clearly (Hits@k).\n\nExtra: I have a question regarding the ablation studies with GloVe, Normal and InferSent initialization. The question is whether this initialization is necessary? It seems like in the setting \u201cW/O KG\u201d, even though the embeddings are initialized with GloVe, there is no gain (all of the Hits@k values are 0.0). So GloVe embedding without KG bring no external semantic knowledge? Then why use them? Regarding that, I can see that a GCN, initialized with normally distributed embeddings (row 14 in Table 1) has given 0.0 accuracies, but I find this very counter intuitive, as graph convolution layers already have trainable weights capable of compensating for the lack of \u2018proper\u2019 initialized embedding and getting 0.0 does not make sense to me.\n\nConclusions and Future Work:\n\n1. two lines before the last: please use \u201c\\citep\u201d.\n\n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 3}}}