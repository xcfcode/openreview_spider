{"paper": {"title": "Generalisation Guarantees For Continual Learning With Orthogonal Gradient Descent", "authors": ["Mehdi Abbana Bennani", "Thang Doan", "Masashi Sugiyama"], "authorids": ["~Mehdi_Abbana_Bennani1", "~Thang_Doan1", "~Masashi_Sugiyama1"], "summary": "An NTK framework for Continual Learning, with robustness and generalisation guarantees for Orthogonal Gradient Descent.", "abstract": "In Continual Learning settings, deep neural networks are prone to Catastrophic Forgetting. Orthogonal Gradient Descent (Farajtabar et al., 2019) was proposed to tackle the challenge. However, no theoretical guarantees have been proven yet. We present a theoretical framework to study Continual Learning algorithms in the NTK regime. This framework comprises closed form expression of the model through tasks and proxies for transfer learning, generalisation and tasks similarity. In this framework, we prove that OGD is robust to Catastrophic Forgetting then derive the first generalisation bound for SGD and OGD for Continual Learning. Finally, we study the limits of this framework in practice for OGD and highlight the importance of the NTK variation for Continual Learning.", "keywords": ["Continual Learning", "Neural Tangent Kernel", "Optimisation"]}, "meta": {"decision": "Reject", "comment": "The reviewers were excited by the paper's theoretical contribution to continual learning, since that aspect of continual learning is underdeveloped.  However, all reviewers (including the most positive reviewer during discussions) expressed that the paper would benefit from revisions to improve the clarity and the thoroughness of comparisons in the paper.  The paper's focus on OGD is not necessarily an issue for it to be of use to the community, as mentioned as a negative point in one review that other reviewers disagreed with. The authors are encouraged to revise this paper incorporating the reviewers' suggestions."}, "review": {"OfZApqJSdCq": {"type": "rebuttal", "replyto": "hecuSLbL_vC", "comment": "Thanks again for your insightful suggestions and comments.\n\nWe have uploaded a new revision, which comprises all the results.\n\nAdditionally, for clarity, we wanted to present a brief overview of the changes we have applied to the manuscript overall :\n- **Experiments** : \n   - Added the benchmark against the baselines EWC, MAS, SI and Stable SGD [1] and the dataset CUB200 [2] (Sec. 7, Table 2)\n   - Reported the metrics Average Forgetting, Average Accuracy, Forward Transfer and Backward Transfer for the benchmark  (Tables 1 and 2 (p. 9), and Tables 9 to 12 (p. 36 to 37))\n   - Reported the overparameterization ratio of the benchmarks (Table 1)\n   - Rewrote the experiments section (Sec. 7 (p. 7 to 9) ) and the experiments appendix (App. F (p. 30 to 37) ) following the updates above and the other feedback\n\n- **Clarity** : \n   - Updated the introduction in order to introduce the concepts more clearly\n   - Fixed and clarified the preliminaries for more clarity\n   - Rewrote the experiments section highlighting more clearly the ablations and the goal behind the experiments\n   - Fixed some small mistakes and typos\n\nPlease don't hesitate if you have additional concerns, comments or suggestions.\n\nThanks again.\n\n**References**\n- [1] Mirzadeh et al., Understanding the Role of Training Regimes in Continual Learning (https://arxiv.org/abs/2006.06958)\n- [2] Jung et al., Continual Learning with Node-Importance based Adaptive Group Sparse Regularization (https://arxiv.org/abs/2003.13726)", "title": "We have completed the experiments - Overview of the updates"}, "AGT5YYnjugw": {"type": "rebuttal", "replyto": "SaKwuqpMnq", "comment": ">\u201cWhat is the time complexity of OGD+?\u201d\n\nThe time complexity of OGD+ is the same as OGD, the only difference is updating the Jacobians memory, which implies a backpropagation on all samples in the memory at the end of each task.\nDenoting : \n- T : the number of tasks \n- M : the memory size\n- N : the number of samples per task\n\nConsidering that the backprop cost is constant, the total complexity of the memory update operation is O(T^2 M^2)\n\nThe time complexity of OGD is : O(NT) + O(M^2T)\n\nTherefore the complexity of OGD+ is O(NT) + O(M^2T) + O(T^2 M^2)\n", "title": "Thank you for your comments and suggestions - Continued"}, "zAawUOoBaAj": {"type": "rebuttal", "replyto": "9VOAPhPl9H7", "comment": ">\u201cWhat about comparing with more enlarged and various benchmark datasets beyond MNIST and CIFAR-100, like CUB200 or Omniglot as shown in https://arxiv.org/pdf/2003.13726.pdf ? How does OGD or OGD+ compare with other baselines like EWC or MAS?\u201d\n\n>\u201c The experimental results are also very limited and weak since it only compares with SGD, an obvious weak scheme that suffers from catastrophic forgetting, and does not compare with any other continual learning baselines. \u201c\n\nThanks for your suggestions and for the reference.\n\n**Comparing against more benchmarks and methods**\nWe have added new results for the baselines SGD+Dropout, EWC, SI and MAS. (Sec. 7.3 and App. F.4, pages 35 to 38)\nWe have also added results for the CUB200 dataset [1] on these baselines, except SGD+Dropout for which we are still running experiments for CUB200. \n\n**Ablation study : OGD and OGD+**\nAlso, we wanted to clarify the main reason we did not include the other Continual Learning baselines in our experiments, and restricted the comparison between OGD and OGD+. \n\nOGD+ was intended as an ablation of OGD, in order to highlight the limits of the theoretical framework in non-overparameterized settings.\n\nThe goal behind the OGD+ experiment is to study, in non-overparameterized settings in practice, the importance of accounting for the Jacobian\u2019s variation for OGD. The motivation behind this experiment is that Theorem 2, which states the robustness of OGD to CF, relies on the assumption that the Jacobian is constant (overparameterization), which may not hold in practice. \n- Table 1 shows that accounting for the Jacobian\u2019s variation implies a significant improvement on non-overparameterized benchmarks, indicating that the assumption may be too strong for the MNIST settings in practice, and a more refined analysis may be needed in order to explain more precisely the properties of OGD in non-NTK regime settings.\n- Regarding the performance of OGD+ in comparison with other baselines, Table 2 shows that OGD+ is competitive with the Continual Learning baselines on non-overparameterized benchmarks, for which it draws a significant improvement over OGD, by taking into account the variation of the Jacobian which is a property of these benchmarks.\n\n**Clarity**\nWe have written over the whole experiments section (Sec. 7) for clarity. We also highlight more clearly the ablation study, its motivation and implications.\n\n>\u201cEven though the paper aims for a theoretical contribution, it is very limited only for OGD based scheme, which is not strong in practice. So, I am not sure about the significance of the contribution of the paper.\u201d\n\nWhile we agree that the analysis is limited to the OGD based scheme, to our knowledge, it is one of the first theoretical works on convergence and optimisation for Continual Learning in the literature, which may provide keys for further investigation.\nAlso, our work is motivated by the following : \n- understanding the theoretical properties of the OGD algorithm may lead to insights to improve it or to design more robust algorithms.\n- providing mathematical tools and proof techniques to analyse other Continual Learning algorithms. \n\nIn Appendix A, we present early theoretical results extending the presented framework in order to draw a theoretical connection between the OGD and the A-GEM algorithms. This early result indicates that the theoretical framework may lead to new viewpoints on existing Continual Learning algorithms beyond OGD.  \n\n**References**\n[1] Jung et al., Continual Learning with Node-Importance based Adaptive Group Sparse Regularization (https://arxiv.org/abs/2003.13726)", "title": "Thank you for your suggestions and comments - Continued"}, "SaKwuqpMnq": {"type": "rebuttal", "replyto": "gUZ45jsK7A", "comment": "Thank you for your appreciation and comments  Please find our responses below.\n\n>\u201ca) For instance, NTK is referred without explaining it well first. In page 2, authors use \"CL\" for referring to continual learning but it has not been defined. b) There are many typos and capital letters have been used inappropriately. c) f_t has not been defined. \u201c\n\nWe apologise for the confusion incurred by the lack of clarity. \nWe took due note of your suggestions and we have revised and updated the flow of the paper to ensure the notions are introduced clearly.\n\n>\u201c2- Although the proposed method provides several experiments, there are still many other methods and datasets that have been ignored. There has been recent studies and frameworks that have outperformed OGD, it woud be great if you include them in your baselines. For instance, SGD+Dropout in https://arxiv.org/pdf/2004.11545.pdf beats OGD.\u201d\n\nThanks for your suggestions and for the reference. \n\n**Comparison with other methods and datasets**\n\nIn order to provide a broader picture of the performance of OGD+, we have added new results for the baselines SGD+Dropout, EWC, SI and MAS. (Sec. 7.3 and App. F.4, pages 35 to 38)\nWe have also added results for the CUB200 dataset [1] on these baselines, except SGD+Dropout for which we are still running experiments forCUB200. \n\n**Ablation study : OGD and OGD+**\n\nAlso, we wanted to clarify the main reason we didn\u2019t include the other Continual Learning baselines in our experiments, and restricted the comparison to OGD and OGD+. \nThe introduction of OGD+ was intended for an ablation study in order to verify the applicability of the theoretical framework in practice.\n\nOur analysis relies on the overparameterization assumption, which implies that the Jacobian is constant through tasks. Theorem 2 follows from this property. However, in practice, this assumption may not hold and forgetting is observed for OGD. We study the impact of the variation of the Jacobian in practice on OGD.\nIn order to measure this impact, we compare the performance of OGD, which doesn\u2019t take into account the variation of the Jacobian to OGD+ which does. As opposed to OGD, OGD+ takes into account the Jacobian\u2019s variation by updating all the stored Jacobians at the end of each task.\n\nTable 1 (Sec 7.2) shows that on the least overparameterized benchmarks, OGD+ presents more robustness, while there is no significant difference on the most overparameterized benchmarks.\n\nThis results highlights the scope of applicability of our analysis, and suggests that an important next step is extending the theoretical framework to the non-overparameterized setting, in order to capture the training dynamics of the MNIST benchmarks for instance. \n\nThis result also shows that OGD is expected to perform better in overparameterised settings, and that accounting for the Jacobian's variation is a well motivated improvement in non-overparameterised settings.\n\n**Clarity**\n\nWe apologise for the lack of clarity, we have written over the whole experiments section for more clarity.\n\n\n> 3- There are many metrics to evaluate continual learning frameworks like backward transfer(BWT) or average accuracy over tasks. I would suggest the authors to look at the defined metrics in GEM (gradient episodic memory), https://arxiv.org/abs/1706.08840, and compute those values.\n\nThanks for your suggestion.\nIn our new experiments, we have integrated the metrics Average Accuracy (AAC), Forward Transfer (FWT), Backward Transfer (BWT) and Average Forgetting Measure (AFM) as suggested. \nWe report concise results in the main paper (Sec. 7.2 and 7.3). \nWe also report the full results on all metrics in App. F.5.1 (pages 35 to 36).\n\n**References**\n[1] Jung et al., Continual Learning with Node-Importance based Adaptive Group Sparse Regularization (https://arxiv.org/abs/2003.13726)\n", "title": "Thank you for your comments and suggestions "}, "9VOAPhPl9H7": {"type": "rebuttal", "replyto": "x0qkc0FQgL0", "comment": "Thank you for your comments and suggestions. We address your comments below.\n\n>\u201cAlso, the result is for regression, as shown in the loss function in Sec 3.2, but the experiments are on classification, so it's not clear with the connection with the theory and the experiments.\u201d\n\n**The analysis is for regression only :**\n\nWe agree on the importance of a theoretical framework for the classification setting, however this setting brings additional challenges due to the non linearity of the classification loss function, which makes the analysis intractable. \nEven though our analysis is for the regression setting only, it provides multiple insights on the foundations of Continual Learning, such as Theorem 1, which describes the transfer of knowledge across tasks, or Theorem 2, which states the robustness of OGD to Catastrophic Forgetting.\nAlso, our experiments in Sec. 7.1 and Sec. 7.2 concur with our theoretical analysis, even though it is limited to the regression setting.\n \n**Connection between the theory and the experiments :**\n\nClassification experiments highlight the similarity to the regression analysis in Continual Learning as follows : \n- Experiment 1 (7.1) checks the validity of Theorem 2 in the classification setting. Figure 1 shows that Catastrophic Forgetting decreases with overparameterization which concurs with Theorem 2 states that there is no Catastrophic Forgetting in the overparameterized regression case.\n- Experiment 2 (7.2) checks the applicability of the theoretical framework in practice for non overparameterized settings. \nTable 1 shows that accounting for the Jacobian\u2019s variation in non overparameterized settings is in part responsible for the failure of OGD to Catastrophic Forgetting in practice. This results enforces Theorem 2 and  indicates that the constant Jacobian assumption is critical for a further theoretical investigation of the OGD algorithm in non-overparameterized settings.\n\n\n>\u201cThe results also seem to be somewhat simple derivations from the known papers, like Jacot et al., (2018) and Liu et al, (2019).\u201d\n\nWe respectfully disagree with the reviewer on this point, the proof techniques and mathematical tools we use are different from the mentioned works as follows :\n- Jacot et al., (2018) used extensively the Kernel Tangent mathematical object, while in our case we tackled the problem without relying on this object. Instead, we build the analysis on a linear approximation which makes the analysis more tractable in our setting. We found that the mathematical tools used by Jacot et al., (2018), even though they were powerful, were more challenging to use to build the framework for Continual Learning. \n- Liu et al, (2019) focused on the two-layer RELU network setting and relied on Gaussian approximations in order to make the analysis tractable. In our analysis, we did not manipulate the distributions of the neural network weights.\n- However, we used multiple proof techniques and mathematical tools presented by Hu et al. (2020) (https://arxiv.org/abs/1905.11368). In this work, they studied noisy supervision in the NTK framework.\n\nAlso, the analysis for the OGD Continual Learning setting brings additional challenges which are specific to Continual Learning and OGD such as : \n- All the proofs are for the OGD optimisation algorithm, while to our knowledge most papers on the Neural Tangent Kernel study the SGD optimisation algorithm\n- The proof of Theorem 1 (Continual Learning as a recursive Kernel Regression)\n- The proof of Lemma 1 by extending the standard bound to the Continual Learning setting (Rademacher complexity for Continual Learning)\n- The proof of Theorem 2 (Non-forgetting property of OGD)\n\n", "title": "Thank you for your suggestions and comments "}, "kE4yJjt5h28": {"type": "rebuttal", "replyto": "KP68nw6ma4d", "comment": "Thanks for your appreciation, we address your comment below.\n\n> \u201cThe primary drawback of the paper is that the authors do not compare the OGD+ algorithm to other continual learning algorithms (synaptic intelligence, elastic weight consolidation, etc.). As a result it is difficult to know how OGD+ compares to alternatives. It is not clear to the reviewer why improving OGD to OGD+ is itself a contribution. Given the expense occurred by OGD-type methods in storing ever increasing numbers of directions, it would be important to know the comparison of this method with others.\u201d\n\nThanks for your feedback.\n\n**Comparing OGD+ to other methods :**\n\nWe have added new results for the baselines SGD+Dropout, EWC, SI and MAS. (Sec. 7.3 and App. F.4, pages 35 to 38)\nWe have also added results for the CUB200 dataset [1] on these baselines, except SGD+Dropout for which we are still running experiments forCUB200. \n\n**Is OGD+ a contribution ?**\n\nWe mainly leverage OGD+ as an ablation of OGD, in order to highlight the limits of the theoretical framework in non-overparameterized settings.\nThe goal behind the OGD+ experiment is to study, in non-overparameterized settings in practice, the importance of accounting for the Jacobian\u2019s variation for OGD. The motivation behind this experiment is that Theorem 2, which states the robustness of OGD to CF, relies on the assumption that the Jacobian is constant (overparameterization), which may not hold in practice. \n\n- Table 1 shows that accounting for the Jacobian\u2019s variation implies a significant improvement on non-overparameterized benchmarks, indicating that the assumption may be too strong for the MNIST settings, and a more refined analysis may be needed in order to explain more precisely the properties of OGD in these settings, which are not under the NTK regime.\nThis result also shows that OGD is expected to work better on overparameterized benchmarks.\n- Regarding the performance of OGD+ in comparison with other baselines, Table 2 shows that OGD+ is competitive with the Continual Learning baselines on non-overparameterized benchmarks, for which it draws a significant improvement over OGD, by taking into account the variation of the Jacobian which is a property of these benchmarks.\n\n**Clarity**\n\nWe have written over the whole experiments section (Sec. 7) for clarity. We also highlight more clearly the ablation study, its motivation and implications.\n\n>\u201cMinor comments\n(1) Section 3.2: f^* is not defined as of this point in the paper. (2) Theorem 1: The theorem needs a quantifier of lambda (3) Line above Remark 1 k_\\tau -> \\kappa_\\tau (4) Theorem 2: The paper should define what \"is in the memory\" means when introducing OGD s (5) Theorem 3: Definition of R_T has incorrect dummy index in the summation\u201d\n\nThank you for the additional suggestions. We took due note of them and applied the corrections.\n\n**References**\n\n[1] Jung et al., Continual Learning with Node-Importance based Adaptive Group Sparse Regularization (https://arxiv.org/abs/2003.13726)\n", "title": "Thanks for your appreciation and comments "}, "gUZ45jsK7A": {"type": "review", "replyto": "hecuSLbL_vC", "review": "##########################################################################\n\nSummary:\n\n \nThis paper studies the theoretical aspect of a continual learning method called orthogonal gradient descent (OGD).\nIn this study, authors leverage Neural Tangent Kernel and over parameterized neural networks to prove the generalization of OGD. \n\n##########################################################################\n\nReasons for score: \n\n \nOverall, I vote for rejection. I like the idea of the paper to analyze an exist method from different aspect and even improving it. However, my major concern is about the clarity of the paper (see cons below).\n\n \n##########################################################################Pros: \n\n \n1. The paper investigate an important problem in continual learning framework which is the generalization.\n\n \n2. This paper provides some experiments to show the effectiveness of the proposed framework. \n\n \n##########################################################################\n\nCons: \n\n1. Unfortunately, the paper is not clear and very difficult to follow.\na) For instance, NTK is referred without explaining it well first. \nIn page 2, authors use \"CL\" for referring to continual learning but it has not been defined.\nb) There are many typos and capital letters have been used inappropriately.\nc) f_t has not been defined.\n2- Although the proposed method provides several experiments, there are still many other methods and datasets that have been ignored. There has been recent studies and frameworks that have outperformed OGD, it woud be great if you include them in your baselines. For instance, SGD+Droput in https://arxiv.org/pdf/2004.11545.pdf beats OGD.\n\n3-  There are many metrics to evaluate continual learning frameworks like backward transfer(BWT) or average accuracy over tasks.\nI would suggest the authors to look at the defined metrics in GEM (gradient episodic memory), https://arxiv.org/abs/1706.08840, and compute those values. \n\n \n##########################################################################\n\nQuestions during rebuttal period: \n\n \n- Please address and clarify the cons above .\n- Would you please elaborate more what could be the superior performance of OGD+ on Rotated Mnist dataset w.r.t OGD?\n- What is the time complexity of OGD+?\n\n \n\n\n\n", "title": "Generalisation Guarantees For Continual Learning With Orthogonal Gradient Descent ", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "KP68nw6ma4d": {"type": "review", "replyto": "hecuSLbL_vC", "review": "The authors use a Neural Tangent Kernel (NTK) approximation of wide neural nets to establish generalization bounds for continual learning (CL) using stochastic gradient descent (SGD) and orthogonal gradient descent (OGD).  In this regime, the authors prove that OGD does not suffer from catastrophic forgetting of training data.  The authors additionally introduce a modification to OGD which causes significant performance improvements in the Rotated MNIST and Permuted MNIST problems.  OGD involves storing feature maps from data points from previous tasks.  The modified OGD method (OGD+) additionally stores feature maps from the current task.  \n\nThe primary contribution of this paper is the theoretical analysis of continual learning.  Given that the CL problem does not have an extensive theoretical foundation, the generalization bound in this paper is a notable advance. The theory presented also provides a justification for the empirical observations observed by the authors that as overparameterization increases, the effect of catastrophic forgetting decreases in a variety of CL task setups.  The primary drawback of the paper is that the authors do not compare the OGD+ algorithm to other continual learning algorithms (synaptic intelligence, elastic weight consolidation, etc.).  As a result it is difficult to know how OGD+ compares to alternatives.  It is not clear to the reviewer why improving OGD to OGD+ is itself a contribution.  Given the expense occurred by OGD-type methods in storing ever increasing numbers of directions, it would be important to know the comparison of this method with others.  \n\nMinor comments:\n\n(1) Section 3.2: f^* is not defined as of this point in the paper.\n(2) Theorem 1: The theorem needs a quantifier of lambda\n(3) Line above Remark 1 k_\\tau -> \\kappa_\\tau\n(4) Theorem 2: The paper should define what \"is in the memory\" means when introducing OGD\ns\n(5) Theorem 3: Definition of R_T has incorrect dummy index in the summation\n", "title": "Novel Theory for Continual Learning in the context of Orthogonal Gradient Descent.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "x0qkc0FQgL0": {"type": "review", "replyto": "hecuSLbL_vC", "review": "The paper provides a theoretical analysis on the OGD based continual learning method. The method is in fact proposed by a previous paper (Farajtabar et al. 2019) and the current paper shows a generalization bound for the regression case. The result (Thm 3) compares the generalization bounds between SGD and OGD and shows OGD leads to a tighter bound. The theorem is also based on the bound on the Rademacher Complexity (Lemma 1).  The paper also suggests OGD+, which stores some data points from past tasks. They also present some experimental results on small benchmark datasets, and show OGD+ outperforms SGD and OGD.\n\nWhile the paper makes an interesting attempt on theoretical analyses of OGD based continual learning method, I feel the result is quite limited only to the OGD scheme. Also, the result is for regression, as shown in the loss function in Sec 3.2, but the experiments are on classification, so it's not clear with the connection with the theory and the experiments. The results also seem to be somewhat simple derivations from the known papers, like Jacot et al., (2018) and Liu et al, (2019). \n\nThe experimental results are also very limited  and weak since it only compares with SGD, an obvious weak scheme that suffers from catastrophic forgetting, and does not compare with any other continual learning baselines. For example, the state-of-the-art on CIFAR-100 is around 65%, and the performance of OGD is very weak. Even though the paper aims for a theoretical contribution, it is very limited only for OGD based scheme, which is not strong in practice. So, I am not sure about the significance of the contribution of the paper. But, I haven't fully read the entire proof of the paper, and I may have missed some details regarding the proof. I would like to see other reviewers' opinion as well. \n\nWhat about comparing with more enlarged and various benchmark datasets beyond MNIST and CIFAR-100, like CUB200 or Omniglot as shown in https://arxiv.org/pdf/2003.13726.pdf ? How does OGD or OGD+ compares with other baselines like EWC or MAS?", "title": "Theoretical analysis for OGD for continual learning", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}