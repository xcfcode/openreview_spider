{"paper": {"title": "Polar Transformer Networks", "authors": ["Carlos Esteves", "Christine Allen-Blanchette", "Xiaowei Zhou", "Kostas Daniilidis"], "authorids": ["machc@seas.upenn.edu", "allec@seas.upenn.edu", "xiaowz@seas.upenn.edu", "kostas@seas.upenn.edu"], "summary": "We learn feature maps invariant to translation, and equivariant to rotation and scale.", "abstract": "Convolutional neural networks (CNNs) are inherently equivariant to translation. Efforts to embed other forms of equivariance have concentrated solely on rotation. We expand the notion of equivariance in CNNs through the Polar Transformer Network (PTN). PTN combines ideas from the Spatial Transformer Network (STN) and canonical coordinate representations. The result is a network invariant to translation and equivariant to both rotation and scale. PTN is trained end-to-end and composed of three distinct stages: a polar origin predictor, the newly introduced polar transformer module and a classifier. PTN achieves state-of-the-art on rotated MNIST and the newly introduced SIM2MNIST dataset, an MNIST variation obtained by adding clutter and perturbing digits with translation, rotation and scaling. The ideas of PTN are extensible to 3D which we demonstrate through the Cylindrical Transformer Network.", "keywords": ["equivariance", "invariance", "canonical coordinates"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes a new deep architecture based on polar transformation for improving rotational invariance. The proposed method is interesting and the experimental results strong classification performance on small/medium-scale datasets (e.g., rotated MNIST and its variants with added translations and clutters, ModelNet40, etc.). It will be more impressive and impactful if the proposed method can bring performance improvement on large-scale, real datasets with potentially cluttered scenes (e.g., Imagenet, Pascal VOC, MS-COCO, etc.)."}, "review": {"SyyrPZf_z": {"type": "rebuttal", "replyto": "rJMOFFWOM", "comment": "Thanks for your interest. Could you please be a bit more specific? I revisited the ModelNet leaderboard and it seems that only two volumetric methods currently outperforms ours:\n1) LightNet, which we were not aware of (it is listed as accepted on 10/23, 4 days before ICLR deadline),\n2) VRN, which is an ensemble, hence an unfair comparison.\n\nPlease let me know if I'm missing something. Note that there is usually a lot of confusion when comparing ModelNet results, since many papers make different assumptions (i.e., accuracy per instance or per class, or taking subsets of the dataset).", "title": "Please be more specific"}, "rkMG9c_gf": {"type": "review", "replyto": "HktRlUlAZ", "review": "This paper presents a new convolutional network architecture that is invariant to global translations and equivariant to rotations and scaling. The method is combination of a spatial transformer module that predicts a focal point, around which a log-polar transform is performed. The resulting log-polar image is analyzed by a conventional CNN.\n\nI find the basic idea quite compelling. Although this is not mentioned in the article, the proposed approach is quite similar to human vision in that people choose where to focus their eyes, and have an approximately log-polar sampling grid in the retina. Furthermore, dealing well with variations in scale is a long-standing and difficult problem in computer vision, and using a log-spaced sampling grid seems like a sensible approach to deal with it.\n\nOne fundamental limitation of the proposed approach is that although it is invariant to global translations, it does not have the built-in equivariance to local translations that a ConvNet has. Although we do not have data on this, I would guess that for more complex datasets like imagenet / ms coco, where a lot of variation can be reasonably well modelled by diffeomorphisms, this will result in degraded performance.\n\nThe use of the heatmap centroid as the prediction for the focal point is potentially problematic as well. It would not work if the heatmap is multimodal, e.g. when there are multiple instances in the same image or when there is a lot of clutter.\n\nThere is a minor conceptual confusion on page 4, where it is written that \"Group-convolution requires integrability over a group and identification of the appropriate measure dg. We ignore this detail as implementation requires application of the sum instead of integral.\"\nWhen approximating an integral by a sum, one should generally use quadrature weights that depend on the measure, so the measure cannot be ignored. Fortunately, in the chosen parameterization, the Haar measure is equal to the standard Lebesque measure, and so when using equally-spaced sampling points in this parameterization, the quadrature weights should be one. (Please double-check this - I'm only expressing my mathematical intuition but have not actually proven this).\n\nIt does not make sense to say that \"The above convolution requires computation of the orbit which is feasible with respect to the finite rotation group, but not for general rotation-dilations\", and then proceed to do exactly that (in canonical coordinates). Since the rotation-dilation group is 2D, just like the 2D translation group used in ConvNets, this is entirely feasible. The use of canonical coordinates is certainly a sensible choice (for the reason given above), but it does not make an infeasible computation feasible.\n\nThe authors may want to consider citing\n- Warped Convolutions: Efficient Invariance to Spatial Transformations, Henriques & Vedaldi.\nThis paper also uses a log-polar transform, but lacks the focal point prediction / STN.\nLikewise, although the paper makes a good effort to rewiev the literature on equivariance / steerability, it missed several recent works in this area:\n- Steerable CNNs, Cohen & Welling\n- Dynamic Steerable Blocks in Deep Residual Networks, Jacobsen et al.\n- Learning Steerable Filters for Rotation Equivariant CNNs, Weiler et al.\nThe last paper reports 0.71% error on MNIST-rot, which is slightly better than the PTN-CNN-B++ reported on in this paper.\n\nThe experimental results presented in this paper are quite good, but both MNIST and ModelNet40 seem like simple / toyish datasets. For reasons outlined above, I am not convinced that this approach in its current form would work very well on more complicated problems. If the authors can show that it does (either in its current form or after improving it, e.g. with multiple saccades, or other improvements) I would recommend this paper for publication.\n\n\nMinor issues & typos\n- Section 3.1, psi_gh = psi_g psi_h. I suppose you use psi for L and L', but this is not very clear.\n- L_h f = f(h^{-1}), p. 4\n- \"coordiantes\", p. 5", "title": "Good idea but not there yet", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1XT6wdeG": {"type": "review", "replyto": "HktRlUlAZ", "review": "This paper proposes a method to learn networks invariant to translation and equivariant to rotation and scale of arbitrary precision. The idea is to jointly train\n- a network predicting a polar origin,\n- a module transforming the image into a log-polar representation according to the predicted origin,\n- a final classifier performing the desired classification task.\nA (not too large) translation of the input image therefore does not change the log-polar representation.\nRotation and scale from the polar origin result in translation of the log-polar representation. As convolutions are translation equivariant, the final classifier becomes rotation and scale equivariant in terms of the input image. Rotation and scale can have arbitrary precision, which is novel to the best of my knowledge.\n\n(+) In my opinion, this is a simple, attractive approach to rotation and scale equivariant CNNs.\n\n(-) The evaluation, however, is quite limited. The approach is evaluated on:\n 1) several variants of MNIST. The authors introduce a new variant (SIM2MNIST), which is created by applying random similitudes to the images from MNIST. This variant is of course very well suited to the proposed method, and a bit artificial.\n 2) 3d voxel occupancy grids with a small resolution. The objects can be rotated around the z-axis, and the method is used to be equivariant to this rotation.\n\n(-) Since the method starts by predicting the polar origin, wouldn't it be possible to also predict rotation and scale? Then the input image could be rectified to a canonical orientation and scale, without needing equivariance. My intuition is that this simpler approach would work better. It should at least be evaluated.\n\nDespite these weaknesses, I think this paper should be interesting for researchers looking into equivariant CNNs.\n", "title": "Interesting approach to equivariant CNNs", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1aLPb5eM": {"type": "review", "replyto": "HktRlUlAZ", "review": "The authors introduce the Polar Transformer, a special case of the Spatial Transformer (Jaderberg et al. 2015) that achieves rotation and scale equivariance by using a log-polar sampling grid. The paper is very well written, easy to follow and substantiates its claims convincingly on variants of MNIST. A weakness of the paper is that it does not attempt to solve a real-world problem. However, I think because it is a conceptually novel and potentially very influential idea, it is a valuable contribution as it stands.\n\nIssues:\n\n- The clutter in SIM2MNIST is so small that predicting the polar origin is essentially trivially solved by a low-pass filter. Although this criticism also applies to most previous work using \u2018cluttered\u2019 variants of MNIST, I still think it needs to be considered. What happens if predicting the polar origin is not trivial and prone to errors? These presumably lead to catastrophic failure of the post-transformer network, which is likely to be a problem in any real-world scenario.\n\n- I\u2019m not sure if Section 5.5 strengthens the paper. Unlike the rest of the paper, it feels very \u2018quick & dirty\u2019 and not very principled. It doesn\u2019t live up to the promise of rotation and scale equivariance in 3D. If I understand it correctly, it\u2019s simply a polar transformer in (x,y) with z maintained as a linear axis and assumed to be parallel to the axis of rotation. This means that the promise of rotation and scale equivariance holds up only along (x,y). I guess it\u2019s not possible to build full 3D rotation/scale equivariance with the authors\u2019 approach (spherical coordinates probably don\u2019t do the job), but at least the scale equivariance could presumably have been achieved by using log-spaced samples along z and predicting the origin in 3D. So instead of showing a quick \u2018hack\u2019, I would have preferred an honest discussion of the limitations and maybe a sketch of a path forward even if no implemented solution is provided.\n", "title": "Very nice and potentially influential paper whose practical usefulness remains to be shown", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJxiU46XG": {"type": "rebuttal", "replyto": "HktRlUlAZ", "comment": "- Included Appendix C, with experiments on the Street View House Numbers dataset (SVHN),\n- fixed/clarified math issues raised by AnonReviewer3,\n- included citations suggested by AnonReviewer3,\n- included clarification in section 5.5, to address issues raised by AnonReviewer1, and\n- rearranged paragraphs and removed redundant sentences to maintain number of pages.\n\n", "title": "List of changes in revised version"}, "S15H7uNMz": {"type": "rebuttal", "replyto": "r1XT6wdeG", "comment": "Thank you for the review.\n\n> (-) The evaluation, however, is quite limited.\n\nPlease check Appendix C for the newly included results on the Street\nView House Numbers dataset (SVHN), which shows that our method is also\napplicable to real-world RGB images.  We show superior performance\nthan the baselines when perturbations are present.\n\n> (-) Since the method starts by predicting the polar\n> origin, wouldn't it be possible to also predict rotation\n> and scale? Then the input image could be rectified to a\n> canonical orientation and scale, without needing\n> equivariance. My intuition is that this simpler approach\n> would work better. It should at least be evaluated.\n\nYour suggestion seems to be what is done in the Spatial Transformer\nNetworks (STN) (Jaderberg et al).  Our experiments show that\nregressing scale and rotation angle is a hard problem, requiring more\ndata and larger networks; on the other hand, learning the coordinates\nof a single point as a heatmap centroid is easier.  We show direct\ncomparison of our method and the STN on tables 1 and 2. The advantage\nof our method is significant, specially with small number of samples\n(rotated MNIST, SIM2MNIST) and large perturbations (SIM2MNIST).\n", "title": "Response to AnonReviewer2"}, "BJzqfu4fM": {"type": "rebuttal", "replyto": "rkMG9c_gf", "comment": "Thank you for the review and insightful comments.\n\n> One fundamental limitation of the proposed approach is\n> that although it is invariant to global translations, it\n> does not have the built-in equivariance to local\n> translations that a ConvNet has. Although we do not have\n> data on this, I would guess that for more complex datasets\n> like imagenet / ms coco, where a lot of variation can be\n> reasonably well modelled by diffeomorphisms, this will\n> result in degraded performance.\n\nWe trade-off local translation equivariance for global roto-dilation\nequivariance.  It is likely that for imagenet/coco local translation\nequivariance is more important, but that may not be the case when\nglobal rotations and large scale variance is present.  We show that\nthe trade-off favors roto-dilation equivariance for the rotated MNIST,\nModelNet40, and rotated SVHN (included in the appendix during the\nreview period); a more interesting real life example where\nroto-dilation equivariance could be beneficial is object recognition\nin satellite images.\n\n> The use of the heatmap centroid as the prediction for the\n> focal point is potentially problematic as well. It would\n> not work if the heatmap is multimodal, e.g. when there are\n> multiple instances in the same image or when there is a\n> lot of clutter.\n\nOur method assumes that there is a single correct instance per image,\nso the multiple instance case could indeed be a problem.  Note that in\nthe newly included SVHN results there are often multiple instances\npresent (see figure 6); however, the correct label is the one of the\ncentral digit and our origin predictor learns to use that.  If we\nassume that multiple instances may be present in different positions,\nwith no central prior, we need to treat the problem as multiple object\ndetection.  Since the origin predictor is fully convolutional, we\nbelieve we could train our model on single objects and test it with\nmultiple objects, with some sort of test-time non-maximum-suppression\non the final heatmap.  It is likely that a soft argmax (perhaps with a\nloss term enforcing concentration) should be used instead of computing\nthe centroid.  We could also pre-train the origin predictor with\nobject center supervision, and then fine-tune it end-to-end (since we\nhave shown that the object center is not necessarily the best origin).\nNote that we have not tried this approach yet, though we did\nexperiment with the soft argmax and concentration loss in the\nsingle-instance setting, and found no performance difference.\n\n> There is a minor conceptual confusion on page 4, where it\n> is written that \"Group-convolution requires integrability\n> over a group and identification of the appropriate measure\n> dg. We ignore this detail as implementation requires\n> application of the sum instead of integral.\" When\n> approximating an integral by a sum, one should generally\n> use quadrature weights that depend on the measure, so the\n> measure cannot be ignored.\n\nThis sentence is incorrect and was removed: \"We ignore this detail as\nimplementation requires application of the sum instead of integral.\",\nthanks for bringing it to our attention.  Equation (8) is determined\nto be true by applying the definition of the Haar measure for the\ndilated-rotation group and a change of coordinates to log-polar,\nshowing that the quadrature weights are indeed one.  We have updated\nthe text accordingly.\n\n\n> It does not make sense to say that \"The above convolution\n> requires computation of the orbit which is feasible with\n> respect to the finite rotation group, but not for general\n> rotation-dilations\", and then proceed to do exactly that\n> (in canonical coordinates). Since the rotation-dilation\n> group is 2D, just like the 2D translation group used in\n> ConvNets, this is entirely feasible. The use of canonical\n> coordinates is certainly a sensible choice (for the reason\n> given above), but it does not make an infeasible\n> computation feasible.\n\nTrue, implying that the computation is 'infeasible' is wrong. We have\nupdated the text.\n\n> The authors may want to consider citing (..)\n\nThanks for pointing these out, we have updated our citations.  Note\nthat Weiler et al. only appeared on 11/21, almost a month after the\nsubmission deadline, and that we did cite Henriques & Vedaldi,\nalthough mistakenly not in \"Related Work\" (already fixed).\n\n> For reasons outlined above, I am not convinced that this\n> approach in its current form would work very well on more\n> complicated problems. If the authors can show that it does\n> (either in its current form or after improving it,\n> e.g. with multiple saccades, or other improvements) I\n> would recommend this paper for publication.\n\nPlease check Appendix C for the newly included results on the Street\nView House Numbers dataset (SVHN), which shows that our method is also\napplicable to real-world RGB images.  We show superior performance\nthan the baselines when perturbations are present.", "title": "Response to AnonReviewer3"}, "H11Me_VGM": {"type": "rebuttal", "replyto": "B1aLPb5eM", "comment": "Thank you for the review.\n\n> A weakness of the paper is that it does not attempt to\n> solve a real-world problem.\n\nPlease check Appendix C for the newly included results on the Street\nView House Numbers dataset (SVHN), which shows that our method is also\napplicable to real-world RGB images.  We show superior performance\nthan the baselines when perturbations are present.\n\n> The clutter in SIM2MNIST is so small that predicting the\n> polar origin is essentially trivially solved by a low-pass\n> filter. Although this criticism also applies to most\n> previous work using \u2018cluttered\u2019 variants of MNIST, I still\n> think it needs to be considered. What happens if\n> predicting the polar origin is not trivial and prone to\n> errors? These presumably lead to catastrophic failure of\n> the post-transformer network, which is likely to be a\n> problem in any real-world scenario.\n\nWhile we agree that the amount of clutter could be overcome by\nhand-designed methods, we argue that learning the origin in an\nend-to-end fashion is advantageous since, in this case, the origin is\nlearned precisely for classification of the log-polar representation.\nThis is quantified in Table 1, which compares our method (PTN), with\nthe Polar CNN (PCNN), which fixes the origin at the image center.  The\nresults show that even though the digits on the rotated MNIST are\ncentered, the learned origin results in significant improvements.\n\nIn more challenging scenarios, it is likely that a deeper origin\npredictor network or more sophisticated object detection model would\nbe necessary.  For example, we could pre-train the origin predictor\nwith object center supervision, and then fine-tune it end-to-end.  For\nthe newly included SVHN experiments we used a deeper residual origin\npredictor, but no pre-training was necessary.\n\n> I\u2019m not sure if Section 5.5 strengthens the paper. Unlike\n> the rest of the paper, it feels very \u2018quick & dirty\u2019 and\n> not very principled. It doesn\u2019t live up to the promise of\n> rotation and scale equivariance in 3D. If I understand it\n> correctly, it\u2019s simply a polar transformer in (x,y) with z\n> maintained as a linear axis and assumed to be parallel to\n> the axis of rotation. This means that the promise of\n> rotation and scale equivariance holds up only along\n> (x,y). I guess it\u2019s not possible to build full 3D\n> rotation/scale equivariance with the authors\u2019 approach\n> (spherical coordinates probably don\u2019t do the job), but at\n> least the scale equivariance could presumably have been\n> achieved by using log-spaced samples along z and\n> predicting the origin in 3D. So instead of showing a quick\n> \u2018hack\u2019, I would have preferred an honest discussion of the\n> limitations and maybe a sketch of a path forward even if\n> no implemented solution is provided.\n\nYour understanding is correct.  The purpose of section 5.5 is to show\nthat our method is applicable to a more challenging problem, from a\ncompletely different domain.  Even though our implementation may be\nconsidered a hack (applying channel-wise polar transforms), the\nconcept of using cylindrical coordinates for azimuthal rotation\nequivariance is solid, and so is learning axis of the transform.  This\nis the direct extension of the PTN to 3D.\n\nAs you mentioned, it is not possible to achieve full SO(3)\nequivariance using cylindrical or spherical coordinates.  Hence, we\naim for equivariance to rotations around axes parallel to z.  We\nconsider this a reasonable assumption, which is equivalent to assuming\nsensors parallel to the ground, or known gravity direction in robotics\napplications, for example.  Moreover, the vast majority of results on\nModelNet40 are with azimuthal rotation only.\n\nIt is indeed the case that scale equivariance could be achieved by\nlog-spaced samples along z.  It could also be achieved by using\nspherical coordinates.  We experimented with those but neither improve\nperformance on ModelNet40, since the scale variability is negligible\non it.  We included these considerations in the text of section 5.5.", "title": "Response to AnonReviewer1"}}}