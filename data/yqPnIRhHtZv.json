{"paper": {"title": "Learning Hyperbolic Representations of Topological Features", "authors": ["Panagiotis Kyriakis", "Iordanis Fostiropoulos", "Paul Bogdan"], "authorids": ["~Panagiotis_Kyriakis1", "~Iordanis_Fostiropoulos1", "~Paul_Bogdan1"], "summary": "We develop a method to learn representations of topological features (i.e., persistence diagrams) on hyperbolic spaces.", "abstract": "Learning task-specific representations of persistence diagrams is an important problem in topological data analysis and machine learning. However, current state of the art methods are restricted in terms of their expressivity as they are focused on Euclidean representations. Persistence diagrams often contain features of infinite persistence (i.e., essential features) and Euclidean spaces shrink their importance relative to non-essential features because they cannot assign infinite distance to finite points. To deal with this issue, we propose a method to learn representations of persistence diagrams on hyperbolic spaces, more specifically on the Poincare ball. By representing features of infinite persistence infinitesimally close to the boundary of the ball, their distance to non-essential features approaches infinity, thereby their relative importance is preserved. This is achieved without utilizing extremely high values for the learnable parameters, thus the representation can be fed into downstream optimization methods and trained efficiently in an end-to-end fashion. We present experimental results on graph and image classification tasks and show that the performance of our method is on par with or exceeds the performance of other state of the art methods.\n", "keywords": ["representation learning", "hyperbolic deep learning", "persistent homology", "persistence diagrams"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes a novel method for representing persistence diagrams by embedding them to a Poincare ball.  The representation is learnable, and unifies essential and non-essential features.   The experimental comparisons with existing representation methods show significant improvements in performance.   \nThe flexible data-driven embedding to a suitable geometric space is a novel idea, which will certainly advance the usefulness of TDA.  The  experimental resutls demonstrate well the advantage of the proposed repesentation.  The authors have also addressed the review comments appropriately, with some extra experiments.  This is also a good addition.   "}, "review": {"NTpqIcJOSoT": {"type": "review", "replyto": "yqPnIRhHtZv", "review": "In this paper, the authors proposed a new representation of persistence diagrams that can include `''essential features''. Essential features correspond to the intrinsic topology of the underlying space that will not die during the filtration. To include the fact that the essential features are infinitely far from other normal features in the diagram, the authors proposed to use a Poincare ball representation, which maps the diagram into a disk whose boundary is infinitely far from inside. \n\nThe authors further proposed a classifier that learns the parameterization of the embedding of a diagram in the Poincare ball. The presentation learning procedure seems to be similar to (Hofer et al. 17), except for using a Poincare ball representation instead of the Euclidean square/triangle representation. Experiments are carried on graph classification tasks and image classification task.\n\nOn the positive side, I think the proposed representation well unified essential and non-essential topological structures. It is elegant and well-thought. A stability theorem is proven (in a similar manner as other known representations). The references are also reasonably complete.\n\nHowever, I am having doubts on the practical motivation of the representation in the learning context. To me, essential features can be considered by simply adding the histogram of their birth times as additional features (to the neural networks at an appropriate layer). I think this approach is a natural and necessary baseline to be compared with.\n\nGenerally, the empirical results are not particularly strong. On graph datasets, the proposed method is only winning 2 out of 5 times. Many important baselines are also missing. For graph classification methods, state-of-the-art classifiers such as GIN and GraphSAGE should be at least compared with. For topological classifiers, many kernel methods could be compared with: PWGK, PI (both of which were used in the other experiment), also Sliced-Wasserstein Kernel. I honestly think an ensemble of these methods and the histogram of birth times of essential diagrams can be easy to tune and perform better. \n\nOther questions/comments:\n\nThe references are not standard and need to be fixed. \n\nFor graph with Rips filtration, why are there 1-dim essential homology? Wouldn't all 1D homology features be killed eventually?\n\nDetails of the classifier (how the representation is learned) is still not clear even after reading the section in the supplemental material. \n\nExperimental details are missing. I understand this is 80% training 20% testing. But how many folds were used to evaluate? Some baseline numbers are very similar to the numbers in the GIN paper. However, the GIN paper was using a 10-fold cross-validation. So there is some discrepancy in the experiments. I would appreciate if the authors could kindly elaborate.\n\nOverall I think this is a nice mathematical formulation to incorporate essential features into the representation of persistent homology. But the practical usage in learning is not very convincing. Fundamentally, the essential features are completely different from non-essential ones. There is nothing in between them. Thus the benefit of a unified representation does not bring much more information than simply treating them separately.\n\n\n** After rebuttal: \nI am increasing the score to 6. I appreciate the authors' response to the reviews. They did the additional experiments I asked for. \n\nAs I stated in the original review, I really liked the unified approach. It is elegant and is nicely presented.\n\nAfter reading the authors' response to R4, it is clarified that the 1D essential homology is because the computation over all threshold is too expensive. I think there might be an opportunity to better justify this paper: we often have to stop the filtration early due to computational concern. This unified representation could potentially be a good solution for this: without computing the actually death time, the unified solution can still 'learn' the real death time of the 1D essential classes. The authors might want to discuss or ideally empirically verify this in the final version. For example, can you show that using the new approach, and stopping earlier during the filtration, the unified classifier can be as good as when we run the whole filtration and compute the real death  time for all 1D classes. Moreover, it will be ideal if the authors can manage to show that the unified approach can actually learn the real death time for these fake essential classes (I do not know how). This way the paper can potentially have a bigger impact. \n\n\n", "title": "Insufficient Experiments", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "LNuETgQ_0qh": {"type": "review", "replyto": "yqPnIRhHtZv", "review": "The authors propose to learn representations of topological persistence diagrams in hyperbolic spaces (Poincare balls). They provide a step-by-step methodology, and an algorithm for creating the representation. They also compare their approach with various graph classification and other ways of representing persistence diagrams.\n\nStrengths:\n=========\n1. This is the first work that learns hyperbolic representations of persistence diagrams.\n2. The methodology is sufficiently clear and the paper is reasonably well-written.\n3. Applications to graph data as well as image data show some promise.\n\nThings to improve:\n=================\n1. My biggest question in this paper is around the assumptions the authors have made and the motivation. They have mentioned that essential features have infinite persistence and truncating the persistence is not the best thing to do. I agree, however, there is really only one essential feature - the 0th order persistent homology group that becomes a single connected component at a scale that is sufficiently large. Since this \"essential\" group is one and the same for all point clouds that we compare,  they always match to one another, and it is safe to say that this truly essential group can be ignored in ideal cases. All other \"essential\" groups are only artifacts of our construction, which happen because we do not use a large enough scale for computational or some other reason.\n\n2. Even in an ideal setting (with large enough scales), it would be good to discuss why hyperbolic coordinates make sense. Why do we expect the distance between the persistence homology module to have some hyperbolic trend (see fig. 2 in https://dawn.cs.stanford.edu/2018/03/19/hyperbolics/). Some thought and discussion around this would be helpful to understand the motivation of the approach.\n\n3. Based on fig. 2 I was initially under the impression that individual points will be mappable between persistence diagrams and hyperbolic representations, but it took some more reading to understand that there is one single representation for the entire persistence diagram in the hyperbolic space. Perhaps the authors can make it clear somewhere (even in Fig. 2).\n\n4. How is x0 chosen in (11). Why is summing in tangent space meaningful? Why not some  other operation?\n\n5. What is the absolute state-of-the-art  for Table 1 and Table 2? I am not holding this as a negative, but it may be good for the readers to know where TDA methods stand in these applications.\n\n6. Along similar lines, are there any applications that are especially suited to TDA that the authors can demonstrate? \n\n7. The authors seem to have used multiple runs for results in Table 1, but the text in Sec. 4.1 does not indicate how it is done. Please elaborate.\n\n8. Why are some values missing for some methods in Table 1.\n\n9. Why does P-Eucl make sense as a baseline? Is this like a so-called ablation study for this procedure? How are the representations created for P-Eucl? Please feel free to add more details on this in supplement.\n\n10. In Sec. 4.2, do the  compared methods also use the same simplicial construction (cubical complexes constructed in a  specific way)? If so, would the compared baselines work better with some other construction?\n\nIn summary, this is an interesting direction, but clarity is needed regarding the motivation,  and presenting extra details along the way will also be appreciated.  Finally, it could make sense to think about  an appropriate application.\n\nUpdate post-rebuttal\n==================\n- I am happy with the thorough engagement by the authors and their clarifications. So  I am bumping the up the score a notch.", "title": "Interesting direction but needs to be motivated and discussed more", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "bO61maWLZ3": {"type": "rebuttal", "replyto": "B3J7GqNd9Of", "comment": "We are glad that you are satisfied with our revision and the added baselines!\n\nWe have updated the reference for the GFL paper. ", "title": "Thank you!"}, "iuWx8HlKICW": {"type": "rebuttal", "replyto": "NTpqIcJOSoT", "comment": "We would like to thank the reviewer for this feedback and accept his criticism regarding the motivation and the lack of sufficient experiments to demonstrate our motivation. We have revised the paper following your suggestions and hope that the updated version addresses your concerns.\n\nWe have added the baseline that you recommended. We created a paragraph in the experimental section and consider a histogram-based feature separation baseline. While including essential features via the histogram of their birth times is natural and simple to implement, unfortunately, it does not yield the best results. We agree that there may be a combination of methods (e.g., PWGK + histogram of birth times for essential features) that performs better than our method. An important benefit of our approach is, as you noticed, the unified representation. Therefore, we do not need to hand-tune \u201ccomponents\u201d of other methods to obtain good performance. Also, we added an ablation study which demonstrates that the performance of our method is driven by the hyperbolic embedding. \n\nWe have also added more state of the art results in the graph case study. Admittedly, our method does not achieve the best performance on all datasets. Our comparisons with other methods act as a proof-of-concept rather than an attempt to find the \u201cuniversal best\u201d. Please note that despite comparing several state of the art methods, it is hard to pick one that performs \u201cbest\u201d on all benchmarks. We hope that you will agree that the facts that \n\n1) our representation is unified.\n2) the benefit of hyperbolic representation has been demonstrated via the ablation study,\n3) the simplified feature separation baseline performs poorly irrespectively of the inclusion of essential features,\n4) our method performs better or on par with several state of the art methods \n\nsuffice to lend merit to our approach. \n\nThe number of folds is 10 and we added that to the experimental section. The representation is learned using standard gradient methods. We highlighted that at the beginning of the experimental section and the full details are given at the end of the appendix. \n\nRegarding your comment on the essential features of the Rips filtration, please look at the follow-up response to Reviewer 4.", "title": "Response to Reviewer Feedback"}, "KEvM0qWVXP": {"type": "rebuttal", "replyto": "ddq_OUNzykQ", "comment": "Thank you for your second response. \n\nWe now understand better what you meant by your initial comment.  We kindly ask you to have a quick look at this: (https://ripser.scikit-tda.org/notebooks/Basic%20Usage.html) practical computation of PDs under the VR filtration for a simple point cloud and focus on cells 9-12. As you can see, by varying the maximum radius of the VR filtration we obtain 0-th and (multiple) 1-th dimensional essential features. While we may be able to increase the scale sufficiently high and end up having only the trivial essential feature that you mention, the resulting persistence diagrams may not be the ones that best summarize the data in terms of performance on the underlying learning task. We have added this comment in the second paragraph on page 2. \n\nThe process of obtaining the PD is essentially a feature extraction process and the scale can be considered as a hyper-parameter (among others).  In our experimental setup, we set all parameters related to PD extraction to pre-defined constants. Extending this to account for learnable parameters in the PD extraction process (such as parameters that appear in the filtration function or the scale itself) is an interesting direction for future work. We mention the paper Graph Filtration Learning by Hofer et al. (also mentioned by Reviewer 2) which is along these lines. \n\nExtended persistence is indeed a different way to construct persistence diagrams and our novelty is in the interpretation of ordinary persistence diagrams.  Other than our intuitive motivation in our previous response (2 in the list), unfortunately, we do not have a formal justification why we should treat points in a PD this way. Whether or not there exists some hyperbolic trend in the distance between persistence homology modules, as you commented in your previous response, is a very interesting question and a good direction for future search. \n\nWe have added these two directions for future work in the conclusion. \n\nRegarding the modification on Fig.2, we had added a small label to indicate that the representation of the persistence diagram belongs on the Poincare ball (blue color, above the arrow). We newly added a sentence in the caption to emphasize that.\n\nRegarding your comment on a niche application, we have thoroughly thought of that and tried to identify a case where our method would be particularly good. We observed that the hyperbolic embedding outperforms all other baseline methods for the IMDB datasets, which are known to be smaller graphs compared to the REDDIT ones. Therefore, we hypothesize based on this that it could be better suited for smaller networks. We have added this at the end of Sec. 4.1.  ", "title": "Follow-up on reviewer's comments"}, "zCqrv0z_kLH": {"type": "review", "replyto": "yqPnIRhHtZv", "review": "The authors propose to learn a representation for the persistence diagram (PD) in the hyperbolic space to incorporate the essential features (i.e., infinite persistence). The authors show that the hyperbolic representation has stability. Empirically, the authors illustrate that the hyperbolic representation for PD compares favorably with other baselines on graph and image classification.\n\nThe motivation to use hyperbolic representation to include essential features for PD is interesting. The authors give some details about the background (e.g., persistence diagrams, Poincare ball). However, the main part of the framework and how to learn the parameters of the embedding are missing.\n\nAs I understand, the authors propose: \n\n(1) to use some auxiliary transformation to lift points of $R^2$ into $R^m$ (however, there are no properties or information about this auxiliary transformation, and how one can do it)\n\n(2) projects points in $R^m$ into the Poincare ball, parameterized by $\\theta$. Why ones need this parameterization, and why this parameterization is important in applications? (It seems the authors combine projection with some transformation here?). It is better in case the authors give the projection (w.r.t. what distance?) and then transform the projected points. \n\n+ There is no description of the space of $\\Theta$ for the parameterization?\n\n(3) combine the representations of each point in PD. It seems that the authors use the sum of all points w.r.t. hyperbolic manifold to represent PD.\n\n+ I also concern about the usage of exp and log map at point $x$ for this combination. Typically, the tangent space is just simply a \"flattened\" space of the hyperbolic space at point $x$, it can only preserve the geometry for some close neighbor points of $x$ (this \"flat\" approach has a large distortion for those points which are far to $x$. It is better in case the authors give more discussion about how to choose $x$, and how it affects the geometry of the hyperbolic?\n\nThe main framework to learn those parameters is not presented in the main manuscript, and it is also unclear how one uses the learned hyperbolic representation for the downstream task?\n+ It seems that the authors incorporate the procedure to learn those parameters inside the networks for classification and learn end-to-end?\n\nSomehow, it seems that the authors propose to use a hyperbolic embedding within a neural network as a classifier for PDs. (in my opinion, the novelty may be incremental in case the authors simply replace the Gaussian-like embedding of Hofer et al. into a hyperbolic embedding inside a neural network. However, the hyperbolic representation for PD may be still interesting by itself. )\n\nOverall, I think this work seems interesting and has good potential. The authors may need to describe more details about the neural framework use to learn the parameters of the hyperbolic representation.\n", "title": "Learning Hyperbolic Representations of Topological Features", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S04Rl_bKU6": {"type": "rebuttal", "replyto": "q4Pm8b_u4DP", "comment": "We would like to thank the reviewer for his/her comprehensive feedback and we are glad that s/he liked our paper. We found your suggestions very insightful and they helped us bring the paper in much better shape. \n\n# Summary of the review - Response\n\n1) That is correct, we can use any of the representations developed by Hofer. In fact, there is not one single choice for $\\rho$. To guarantee stability, there are two conditions that $\\rho$ needs to satisfy and we have re-written them more clearly in Theorem 1. To ensure that the representation is not only stable but also expressive, we require injectivity (please also see response to your comment about an \u201cunderpowered\u201d $\\rho$). We have added a relevant discussion below Theorem 1.  The exact formula for $\\rho$ is of little interest and we have added it in the implementation details in the appendix. \n\n2) Thank you for providing the references. Indeed the GFL and the WKPI methods achieve better performance on some datasets that the state of the art we had included and we added them. The P-WL focuses on labeled graphs, which our method cannot currently handle, so direct comparison is difficult. Additionally, we adopted your suggestion about the ablation study and added a paragraph in the experimental section. The P-Eucl and P-Poinc variants are essentially an ablation study and we added a hybrid method (H-Hybrid). Please have a look at that part and the newly added experimental results. We believe that you will find them interesting. \n\n3) We moved that part from the appendix to the experimental section of the paper and elaborated on how we select the manifold dimension and the number ($K$) of the so-called projection bases (i.e., the replicas which we concatenate). Both of these are treated as hyper-parameters and the empirical rule for picking them is as follows: We initially attempt small values for the manifold dimension (e.g, $m=3$) and pick the number of projection bases depending on the type of data (i.e, in the range 5-10 for images and 200-500 for graphs). As we increase m, we decrease K to maintain a similar model capacity and avoid overfitting. We cannot say that a certain manifold dimension (e.g., $m=32$) would suffice for all practical cases, as it could differ from one dataset to another. In practice, we choose the optimal combination via the validation dataset. There is a discussion about how increasing m affects the validation performance at the end of Sec. 4.2.  \n\n# Detailed comments - Response\n\n>In the abstract, I would say that 'Existing methods are restricted in terms of their expressivity'. I would use 'bottleneck' only to refer to the eponymous distance, in order to prevent confusion.\n\nThank you for this suggestion. We like  the term \u201cexpressivity\u201d and we adopted it throughout the paper whenever appropriate.  \n\n> The definition of 'filtration' in the introduction is incorrect; a filtration is a sequence of subspaces.\n\nWe corrected this. \n\n> I would not refer to diagrams being injective. I understand what is mean here, viz. the fact that under some assumptions (!), one can reconstruct the input space from a diagram (this is known as solving the inverse problem), but I think this might be slightly confusion here. Why not focus on the expressivity properties of diagrams and the fact that one can approximate their inputs under some conditions?\n\nYou are right, \u201cinjective\u201d is misleading. We are referring to the inverse problem, as you noted. We re-phrased to avoid confusion. \n\n>The comment on 'extended persistence' on p. 2 is slightly imprecise; 'extended persistence' is well studied now, but PersLay is indeed the first 'deep learning' method incorporating it. This should be clarified.\n\nWe re-phrased this to emphasize that PersLay is the first deep learning method to utilize extending persistence. \n\n> When discussing filtrations and homology groups, I would stick with \u2286 instead of \u2282. The former is more generic and would make the write-up more consistent, as currently both forms are being mixed.\n\nWe corrected this. \n\n>As outlined above, I would suggest to be more verbose when it comes to the description of the method in terms of the individual functions, i.e. \u03c8 and \u03c1.\n\nWe tried to be more verbose on what $\\rho$ is and the conditions that it needs to satisfy. Regarding $\\psi$, we chose not to add any details in the main text as they are of little interest for our method and we prefer to limit the number of equations that the reader is exposed to. The analytical expression for it is still given in the appendix.  \n\n(continued)", "title": "Response to Reviewer Feedback"}, "1kcA4mxrlzQ": {"type": "rebuttal", "replyto": "q4Pm8b_u4DP", "comment": "(continued)\n\n>Am I mistaken or could \u03c1  also be any universal approximator, such as a set function or, more generically, a deep neural network? Would it be possible to use the method by Hofer et al. (2017) to obtain this map and then subsequently train a better hyperbolic embedding?\n\nIn practice, this auxiliary $\\rho$ could be any generic neural network approximator or the representation method by Hofer et al. (2017) or the PersLay by Carriere et al. (2020). Note that such a $\\rho$ would not satisfy, in general, the conditions of stability given by Theorem 1. Nonetheless, in practice, even unstable representations seemed to perform equally good in the classification tasks. In fact, we even tried picking $\\rho$ to be a simple zero-padding embedding, which does not satisfy the second condition of Theorem 1, i.e, it is not zero on the diagonal. The classification performance was equally good. This is explainable because the condition requiring $\\rho$ to be zero on the diagonal exists due to theoretical artifacts. Please see the discussion following Theorem 1. \n\n>It is my understanding that the Euclidean method P-Eucl is only driven by \u03c1. Is this correct? How critical is this choice in practice? Moreover, how was it chosen for this paper? I am asking because it is clear that the hyperbolic embedding helps improve predictive performance, but I wonder what would happen if one uses an 'underpowered' \u03c1 function.\n\nIn all cases (P-Eucl, the newly added P-Hybrid and the P-Poinc), \\rho is the same (as described in the newly added paragraph in the Appendix C). To obtain the P-Hybrid, we need to replace the Poincare ball with the Euclidean space. This implies that the exponential and logarithmic maps reduce to identities. Additionally, we obtain P-Eucl  from P-Hybrid by replacing the learnable parameterization given by Eq. 8 by simple addition of the learnable parameters. Please see the ablation study paragraph in the beginning of the experimental section. \n\nYour comment about an \u201cunderpowered\u201d $\\rho$ function highlights an important subtlety: The stability theorem (Theorem 1) does not prohibit us from choosing a degenerate $\\rho$. For example, $\\rho=0$ satisfies the conditions of Theorem 1 and therefore leads to a stable representation. However, such representation is obviously not useful for learning tasks. An implicit requirement for $\\rho$ is that it is injective (everywhere except the diagonal), which, given that $\\rho$ is a higher dimensional embedding this is a mild condition, fairly easy to satisfy. Please see the discussion following Theorem 1. \n\n# Style and clarity - Response\nWe correct typos and/or adopted your suggestions. \n", "title": "Response to Reviewer Feedback "}, "n5gJ-aRgIZe": {"type": "rebuttal", "replyto": "LNuETgQ_0qh", "comment": "We would like to thank the reviewer for the feedback. We give our responses to your comments bellow:\n\n1) The scale is indeed very important for determining when features appear and when/if they disappear. But the filtration plays a pivotal role as well. Consider for example a filtration over graphs where a certain loop appears, unaltered, at the very beginning and the very end of the filtration. The persistence of this loop (i.e., 1-dim homology group) is infinite as it never \u201cdies\u201d in the filtration. We kindly ask you to look at the section \u201cExtended Persistence Diagrams\u201d of the paper by Carriere et al. (ref. 19 in our paper) where that same example is mentioned and used for motivating the introduction of extended persistence in the deep learning context. \n2) The hyperbolic representation is meaningful as it allows us to treat essential and non-essential features via a unified representation. The Euclidean representation would shrink the relative importance of essential features and the Euclidean metric cannot assign infinite distance to finite points.  \n3) Yes you are right, that figure might give that impression. We added a label to emphasize that the PD is mapped on a single point in the Poincare ball.\n4) The choice of $x_0$ is highlighted in Theorem 1. We choose $x_0=0$ to guarantee the stability of the representation. \n5) It is hard to pick an absolute state of the art as different methods perform differently across different datasets. We have included more state of the art results in Table 1 to show the comparison with an even wider range of methods. \n6) The method is not necessarily best suited for any special application. It is a generic method for learning representations of persistence diagrams extracted from any type of data to which TDA can be applied (e.g., graphs, images, time-series, high-dimensional point clouds etc). \n7) The multiple runs are the different cross-validation folds. We have highlighted that. \n8) Some values are not available for the corresponding dataset in the respective papers.\n9) In fact, the P-Eucl does act as an ablation study for this procedure. We have added a paragraph in the experimental section elaborating on P-Eucl and a  newly added P-Hybrid method. \n10) For the baselines, we used the same PDs as the ones we used for our method. We obtained the baseline results using the publicly available code from the respective paper. \n", "title": "Response to Reviewer Feedback "}, "NwhSGlxMnvD": {"type": "rebuttal", "replyto": "zCqrv0z_kLH", "comment": "We would like to thank the reviewer for the feedback and we are glad that s/he found hyperbolic representations of persistence diagrams an interesting research topic. Based on the feedback, it seems that the reviewer has fully understood our method. We provide clarifications on details that may have been unclear in our paper. \n\n1) Regarding the properties of the auxiliary transformation, we kindly point the reviewer to Theorem 1. We have re-written the theorem to highlight the properties. The auxiliary transformation \\rho is assumed to be Lipschitz continuous and zero on the diagonal, i.e., \\rho(x) = 0 for all x \\in \\mathbb{R}_\\Delta. Both assumptions are needed to ensure stability as per Theorem 1. There is a short explanation of why we need \\rho(x) = 0 on the diagonal and the full proof is provided in Appendix B. Please also note that we have added (following comments from Reviewer 2) a paragraph after Theorem 1 where we explain another property of \\rho. \n\n2) The \u201cprojects\u201d word is a misleading choice. We use it interchangeably with \u201ctransform\u201d. What we are actually doing is transforming the points from \\mathbb{R}^m to the Poincare ball via the learnable parameterization given by Eq. 8. We rephrase the text replacing the word \u201cproject\u201d with \u201ctransform\u201d to avoid any confusion. The parameter space \\Theta is essentially the R^m space, we corrected that. The parameters \\theta are a pivotal part of our method because they allow us to learn the representation of each point in the PD (and consequently, to learn the representation of the PD itself) on the Poincare ball. Our method is somehow similar to the one by Hofer et al. in the sense that both learn the representations of PD and feed them to neural networks for classification tasks. However, we believe that this is not an incremental contribution; as the reviewer points out, the hyperbolic representation of PDs is the main novelty of our work. \n\n3) We kindly point the reviewer to Theorem 1 where we state the choice of x (or x_0 as denoted in that theorem). We choose x_0=0 which guarantees that the representation is stable. As before, this is discussed in the paragraph following Theorem 1. The choice of x_0 does not have any major impact on our method. Please note that the desired property of the Poincare space (i.e., the ability to assign infinite distances to finite points) holds irrespectively of the chosen x_0. The exponential and logarithmic maps are utilized so that we can combine the representations of individual points of the PD into a single point in the Poincare space. Regarding your comment about the tangent space. The tangent space is a vector space and if we equip it with the Euclidean metric it does indeed cause distortion on geometric quantities. However, we typically equip the tangent space with the induced metric.  The transformations given by these maps are norm-preserving, i.e., for example, the geodesic distance from $x$ to the transformed point $\\exp_{x}(v)$ coincides with the metric norm $||v||_g$ induced by the metric tensor $g^\\mathcal{B}_x$.\n\n>It seems that the authors incorporate the procedure to learn those parameters inside the networks for classification and learn end-to-end?\n\nThis is in fact exactly what we are doing. The learnable representation acts as an input layer in a DNN and the parameters are learned end-to-end. The reviewer is right, we are not explicitly mentioning that in the main text, we only implicitly mention it in Appendix C. We added a short clarification in the beginning of the experiments section (Sec. 4). For reason for mentioning this in the experiment section rather that Sec. 3 is the following: Even though our method is validated on a neural network classifier using the representation as an input layer, the hyperbolic representation itself may be of independent interest, as the reviewer points out, not necessarily tied to the fact that is used as input to a neural network or to the classification task. \n", "title": "Response to Reviewer Feedback"}, "q4Pm8b_u4DP": {"type": "review", "replyto": "yqPnIRhHtZv", "review": "# Synopsis of the paper\n\nThis paper proposes a novel embedding or representation algorithm for\npersistence diagrams, i.e. topological descriptors. Existing methods are\nrestricted because they do not feature *learnable* or *trainable*\nparameters for their representations (with the exception of methods such\nas the one by Hofer et al. or PersLay). This method, by contrast,\npresents a trainable embedding to the Poincar\u00e9 ball, thus representing\npersistence diagrams in a hyperbolic space.\n\nThis has the advantage of being more appropriate for representing\n*essential features*, i.e. features of infinite persistence. Thus,\nit is possible to perform end-to-end training of neural networks\nthat use persistence diagrams as their input.\n\nExperiments with graph classification and image classification tasks\ndemonstrate the utility of the proposed method.\n\n# Summary of the review\n\nThis is a very well-written paper, with a strong contribution to\ntopological data analysis and machine learning. The paper is technically\nsound, apart from some minor inconsistencies, which I shall discuss\nbelow. It is exciting for me to see how to obtain fully-trainable\nembeddings here, and I am happy to endorse this paper.\n\nThere are a few issue that I would like to see rectified in a revision\nof the paper, though:\n\n1. Some details on the method are missing. This concerns primarily the\n   learnable auxiliary transformation from Eq. 7. While the paper\n   subsequently discusses potential choices for this function, this is\n   not specified. In fact, the method tying everything together even\n   cites this equation again, without providing a definition for it.\n\n   This needs to be rectified---I am assuming that one could, for\n   example, use a transformation such as the one provided by Hofer et\n   al.; is this correct?\n\n2. The experiments on graph classification are lacking some comparison\n   partners. Specifically when discussing topology-based approaches, it\n   is useful to compare to other topology-based approaches. Here are\n   some suggestions:\n\n      - Hofer et al.: *Graph Filtration Learning*\n      - Rieck et al.: *A Persistent Weisfeiler\u2013Lehman Procedure for Graph Classification* \n      - Zhao & Wang: *Learning metrics for persistence-based summaries and applications for graph classification*\n\n    There is an overlap of the data sets assessed using these methods,\n    so the appropriate results could be cited and used. I wanted to\n    raise this concern primarily because I am aware of other methods\n    obtaining somewhat better classification performance in some cases.\n    \n    It would strengthen the paper immensely if it could perform some\n    form of 'ablation study', i.e. highlighting to what extent the\n    improved results are driven by the embedding in hyperbolic space, or\n    the choice of filtration, etc.\n\n3. Adding to this, some details of the method need to be described\n   better; only the supplements briefly mention how $m$, the embedding\n   parameter, is chosen in the end, but I would prefer a more in-depth\n   analysis of the choice of this parameter. Is it sufficient to pick,\n   say, $m = 32$ for all practical purposes? Or does it make sense to\n   concatenate the representations afterwards, as discussed in the\n   supplements? Adding more details will aid in understanding the paper\n   and, ultimately, promote further adoption of the method.\n\n# Detailed comments\n\n- In the abstract, I would say that 'Existing methods are restricted in\n  terms of their expressivity'. I would use 'bottleneck' only to refer\n  to the eponymous distance, in order to prevent confusion.\n\n- The definition of 'filtration' in the introduction is incorrect;\n  a filtration is a *sequence* of subspaces. \n\n- I would not refer to diagrams being *injective*. I understand what is\n  mean here, viz. the fact that under some assumptions (!), one can\n  reconstruct the input space from a diagram (this is known as solving\n  the inverse problem), but I think this might be slightly confusion\n  here. Why not focus on the expressivity properties of diagrams and the\n  fact that one can *approximate* their inputs under some conditions?\n\n- The comment on 'extended persistence' on p. 2 is slightly imprecise;\n  'extended persistence' is well studied now, but PersLay is indeed the\n  first 'deep learning' method incorporating it. This should be\n  clarified.\n\n- When discussing filtrations and homology groups, I would stick with\n  $\\subseteq$ instead of $\\subset$. The former is more generic and would\n  make the write-up more consistent, as currently both forms are being\n  mixed.\n\n- As outlined above, I would suggest to be more verbose when it comes to\n  the description of the method in terms of the individual functions,\n  i.e. $\\psi$ and $\\rho$.\n\n- Am I mistaken or could $\\rho$ also be *any* universal approximator,\n  such as a set function or, more generically, a deep neural network?\n  Would it be possible to use the method by Hofer et al. (2017) to\n  obtain this map and *then* subsequently train a better hyperbolic\n  embedding?\n\n- When running the experiments, is $m$ fixed or is the best $m$ selected?\n  I see that $m \\in \\{2, \\dots, 12\\}$, but I do not understand how this\n  is actually used in the network. Figure 4 is somewhat helpful here,\n  but it would be interesting to see what happens for a range of\n  parameters.\n\n- It is my understanding that the Euclidean method P-Eucl is only driven\n  by $\\rho$. Is this correct? How critical is this choice in practice?\n  Moreover, how was it chosen for this paper? I am asking because it is\n  clear that the hyperbolic embedding helps improve predictive\n  performance, but I wonder what would happen if one uses an\n  'underpowered' $\\rho$ function.\n\nAll in all, I feel that this could make a very strong addition to the\ntopological machine learning literature!\n\n# Style & clarity\n\nThe paper is very well-written, the authors are to be commended for\nthat. I have a few minor suggestions, some of which are more personal\npet peeves, but which might help make this paper shine even more!\n\n- 'root of a complex polynomial' --> 'roots of a complex polynomial' (?)\n\n- I would prefer not to use citations as nouns, i.e. I would prefer\n  writing 'Kusano et al. (5)' instead of 'In (5), Kusano et al.'; the\n  former strikes me as more readable and also translates well to\n  different citation styles.\n\n- The image of $f_n^{i,j}$ is *the* $n$th persistent homology group\n\n- Whenever possible, I would use $\\operatorname{text}$ for operators or\n  functions, instead of 'raw' $text$. This concerns for example the rank\n  function but also the degree function.\n\n- I would write 'Poincar\u00e9' everywhere\n\n- 'persistent diagram' --> 'persistence diagram'\n\n- 'persistent image' --> 'persistence image'\n\n- 'Vietories' --> 'Vietoris'\n\n- 'gray-scale' --> 'grey-scale' (or vice versa, if American English is\n  the preferred spelling)", "title": "A novel embedding method for persistence diagrams", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}