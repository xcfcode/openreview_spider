{"paper": {"title": "Learning Robust State Abstractions for Hidden-Parameter Block MDPs", "authors": ["Amy Zhang", "Shagun Sodhani", "Khimya Khetarpal", "Joelle Pineau"], "authorids": ["~Amy_Zhang1", "~Shagun_Sodhani1", "~Khimya_Khetarpal1", "~Joelle_Pineau1"], "summary": "We propose a new framework for tackling environments with different dynamics in rich observation settings and learning abstractions with this framework for improved generalization performance.", "abstract": "Many control tasks exhibit similar dynamics that can be modeled as having common latent structure. Hidden-Parameter Markov Decision Processes (HiP-MDPs) explicitly model this structure to improve sample efficiency in multi-task settings.\nHowever, this setting makes strong assumptions on the observability of the state that limit its application in real-world scenarios with rich observation spaces.  In this work, we leverage ideas of common structure from the HiP-MDP setting, and extend it to enable robust state abstractions inspired by Block MDPs. We  derive instantiations of this new framework for  both multi-task reinforcement learning (MTRL) and  meta-reinforcement learning (Meta-RL) settings. Further, we provide transfer and generalization bounds based on task and state similarity, along with sample complexity bounds that depend on the aggregate number of samples across tasks, rather than the number of tasks, a significant improvement over prior work. To further demonstrate efficacy of the proposed method, we empirically compare and show improvement over multi-task and meta-reinforcement learning baselines.", "keywords": ["multi-task reinforcement learning", "bisimulation", "hidden-parameter mdp", "block mdp"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper addresses the problem of learning and exploiting common (latent) task structure in multi-task reinforcement learning settings. The authors introduce a new formalism for capturing this type of structure and derive a gradient-based learning algorithm. They provide novel theoretical insights and strong empirical results.\n\nReviewers initially raised several concerns, regarding assumptions and especially accessibility of the paper (and in particular theoretical discussions). The majority of these concerns have been addressed in the detailed rebuttal. The resulting consensus is to accept the paper. Authors are encouraged to continue to improve accessibility of the paper for the camera ready submission."}, "review": {"yCsUtB6tqac": {"type": "rebuttal", "replyto": "83FzptQ-LTz", "comment": "Thank you for reading our rebuttal and changing your score. And thank you for the reference to a meta-RL paper from pixels! We will try HiP-BMDP in this setting for a comparison to strengthen the paper.", "title": "Thanks for the response!"}, "NIQ82AGSfpI": {"type": "review", "replyto": "fmOOI2a3tQP", "review": "### Summary\n\nThe authors combine hidden-parameter MDPs and state abstractions to model multi-task problems with dynamics parameterized by some latent variables and where the agent receives high-dimensional observations whose corresponding low-dimensional states, on which dynamics are defined, are unobserved (as in block MDPs). They provide both a theoretical analysis and an empirical evaluation of the proposed method, showing that it performs better than competitive baselines on complex continuous control domains.\n\n### Pros\n\n- The idea of combining HiP-MDPs with state abstractions seems interesting and relevant. One of the limitations of HiP-MDPs is indeed that they do not easily handle high-dimensional observations and the proposed approach overcomes this limitation. \n- The method seems also quite general, in the sense that it can be applied in multi-task/meta-RL settings and combined with existing algorithms.\n- Experiments are conducted on complex tasks and show convincing results.\n\n### Cons\n\n- I found the paper quite hard to read in its core parts (e.g., Sec. 3). This is in part due to a complex/confused notation, which overall made it hard for me to go through the theoretical part (and proofs). See detailed comments below.\n- The theoretical results focus on assessing the value-function errors for fixed abstractions/dynamics-structure rather than on the errors in learning the abstractions/structure themselves (and learning these components seems to be one of the primary concerns here).\n- The requirement that task IDs are known could be a potential limitation for applying this method\n\n### Detailed comments\n\n1. Are the environment labels/ids used only for communicating to the learner that the task changed or could they provide more information? For instance, if we face the same task twice in two non-consecutive episodes, are the corresponding labels equal?\n\n2. What makes these state abstractions \"robust\" (as in the title)? Is there any particular theoretical or empirical evidence that justifies this term?\n\n3. Existing analyses of multi-task settings, including the one of Brunskill and Li [1] and others [2,3,4], provide guarantees for task-structure (e.g., dynamics models) learned from data, while here the focus seems to be on guarantees for arbitrary abstractions/structures as a function of their errors, without considering how they are learned. This makes it difficult to interpret the results in the context of the proposed approach\n\n4. Regarding the statement: \"sample complexity bounds that depend on the aggregate number of samples across tasks, rather than the number of tasks, a significant improvement over prior work\", the bounds derived in [4] depend only on a number of abstract \"transition templates\" that they define, while those derived in [3] depend on the minimum between the number of tasks and the number of states.\n\n5. To be clear, the notation \\| x - y \\|_1 is used for the Wasserstein distance and not l1-norm, right? In such a case, the sentence \"We omit d but use d(x, y) = \\|x \u2212 y\\|_1 in our setting\" is quite confusing.\n\n6. The definitions of the terms \\epsilon_R, \\epsilon_T, \\epsilon_\\theta could be moved to the theoretical analysis since they are introduced early and never used before Sec 3.3\n\n7. Sec. 3.2: the definition of \\phi maps S to Z. Isn't it X to Z?\n\n8. In Sec. 3.2, the 2-Wasserstein distance is used, while the 1-Wasserstein distance was introduced earlier. Which one is used?\n\n9. In Eq. 3, it was not clear to me why the \"model learning error\" term uses the MSE for two different environment labels (isn't one enough)?\n\n10. What is exactly the \"evaluation of the optimal Q function of \\bar{M} on M\" in Theorem 2? Do you mean that we take the optimal policy of \\bar{M} and test it on M?\n\n11. [if the above comment is correct] Theorem 2 was slightly weird to me: it takes the optimal policy of \\bar{M}, evaluates it on M_{\\theta_i}, and check how much the corresponding Q function differs from the optimal Q function of M_{\\theta_j}. If we wanted to figure out the transfer error of the optimal policy of \\bar{M} on M_{\\theta_j}, shouldn't we compare the optimal Q function of M_{\\theta_j} with the Q function of the policy on the same MDP (rather than on M_{\\theta_i})?\n\n12. How exactly is the \"empirical measurement of Q^*_{\\bar{M}} over D\" computed?\n\n13. In Th. 4, the last term bounds the concentration of expectations of Q functions (which are bounded by rmax/(1-\\gamma). Shouldn't there be a multiplicative rmax in front of the sqrt?\n\n14. Page 6: \"We compare the our method\" -> \"We compare our method\"\n\n15. In the multi-task experiments, it was not clear what method was used in combination with HiP-BMDPs to learn policies (though it is in appendix).\n\n16. How many adaptation steps for \"few-shot generalization\" were used in Figure 5?\n\n17. Below Figure 5: \"and use a behavior policy to collect transitions.\" What is the behavior policy exactly and how was it chosen?\n\nSome minor comments/questions:\n\n- Page 1: \"This additional structure gives us better sample efficiency, both theoretically and empirically.\" Better than what algorithm?\n- Page 2: \"don't\" -> \"do not\"\n- Page 2: \"which naturally leads to a gradient-based representation learning algorithm.\" Why is it natural to use a gradient-based algorithm in this setting?\n- Background: you could mention that the finite-MDP assumption is only for deriving the theoretical results, while the method can be applied in continuous domains (right?)\n- Page 2: \"a unobservable \"-> \"an unobservable\"\n- Page 3: \"where l2 distance corresponds to d\" Why? d isn't the l2 distance, right?\n- Sec. 3.1: \\phi in \\epsilon was never defined.\n\n### Overall comment\n\nOverall I believe that the paper is interesting and the results significant. However, at the present time, I have too many doubts to vote for acceptance. I will be happy to increase my score after the authors have clarified them.\n\n### Update\n\nI have increased my score to 7 after reading the authors' response and the updated paper.\n\n### References\n\n[1] Brunskill, Emma, and Lihong Li. \"Sample complexity of multi-task reinforcement learning.\" UAI (2013).\n[2] Liu, Yao, Zhaohan Guo, and Emma Brunskill. \"Pac continuous state online multitask reinforcement learning with identification.\" AAMAS (2016).\n[3] Tirinzoni, Andrea, Riccardo Poiani, and Marcello Restelli. \"Sequential transfer in reinforcement learning with a generative model.\" ICML (2020).\n[4] Sun, Yanchao, Xiangyu Yin, and Furong Huang. \"TempLe: Learning Template of Transitions for Sample Efficient Multi-task RL.\" arXiv (2020).", "title": "Interesting paper which requires some improvements/clarifications", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "k6JJrFGW0Gu": {"type": "review", "replyto": "fmOOI2a3tQP", "review": "Summary:\nThis paper combines representation learning via approximate bisimulation with the HiP-MDP framework for representing multiple tasks. The result is multi-task and meta-RL algorithms that operate from images and in the meta-RL case adapt to changes in the dynamics of the environment.\n\nPros:\nCombines HiP-MDP and block MDP formalisms\nEvaluates resulting method on a set of multi-task and meta-RL problems operating from image observations\nCons:\nNot clear (to me) if any insight is gained from the theoretical analysis (the derivation of value and sample complexity bounds for approximate bisimulation was performed in Gelada et al. 2019)\nEmpirical gains are modest and it\u2019s not clear if they are due to the image representation learning component of the loss or other aspects of the method.\n\nDetailed Comments:\nI think this paper would be relevant to cite: Learning an Embedding Space for Transferable Robot Skills (Hausman et al. 2018).\n\nOne thing that stood out to me was the very small number of training tasks used - only 4! I wonder if using more training tasks improves the generalization of single-task methods like DeepMDP that might be able to simply generalize over the dynamics changes given more examples. How much overfitting to the training tasks do you observe for these baseline methods versus HiP-BMDP?\n\nIt seems like there might be a baseline missing - generic multi-task algorithm that conditions on the environment ID but does use the bisimulation loss to help process the image observations. From my understanding, all baselines have no image representation learning component except DeepMDP which is not a multi-task or meta-RL algorithm.  I am a bit concerned that the gains we are seeing here have more to do with image representation learning than the structural assumptions employed by the algorithm. \n\nI think that for the meta-RL setting, \u201cenvironment ID\u201d is replaced everywhere by experience seen so far in the task - is that correct? It would be good to make that clear, or if environment IDs are being used in meta-RL, how they are being used.\n\nI\u2019m a bit confused why there\u2019s a gap between your method in the multi-task experiments but not in the meta-RL ones. Do the meta-RL experiments use image observations? If so, how is the PEARL baseline adapted to handle images? If not, why not?\n\nWhy does HiP-BMDP handle the sticky observation setting better than the baselines? Isn\u2019t it impossible for it to persist state information over time steps?\n\nRecommendation:\nBorderline\nI think the strong point of this paper would be the theoretical contribution of connecting bisimulation to HiP-MDPs. As I\u2019m not very familiar with bisimulation theory, I cannot comment with confidence on the value of this contribution. It is not clear to me what new insights are gained from this extension.\nEmpirically, I am not convinced that the structural assumptions on the MDP used by the proposed algorithm yield performance improvement. For example, one assumption is that the reward function won\u2019t change across tasks, but the method doesn\u2019t really outperform a baseline that doesn\u2019t make that assumption. I am concerned that the gains that are observed come from image representation learning via approximate bisimulation, and that this combined with any multi-task or meta-RL algorithm might achieve the same results.\n\n---**Update**---\nIncreased score 5 -> 7 thanks to clarifications from authors.\n", "title": "Nicely connects two formalisms, but insight isn't clear and empirical results not convincing", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "eGmCqeBShWG": {"type": "rebuttal", "replyto": "RQmm50Xo4a5", "comment": "We thank the reviewer for their kind words and address concerns below and in the updated version of the paper. \n\n1. Authors have to be more upfront on why this assumption is needed, it's hard now to exactly find why do we need Block MDP assumption, what would break if we were to relax this?  \nA: A more extensive discussion of the Block MDP assumption has been added to the Background (Section 2). We also copy a similar version here for convenience: \n\n\u201cThe Block MDP assumption gives the Markov property in observation space, a key difference from partially observable MDPs (POMDPs), which have no guarantee of determining the generating state from the history of observations. This assumption allows us to compute reasonable bounds for our algorithm in k-order MDPs (which many real world problems can be described as, such as navigation, locomotion, and manipulation tasks from first person point of view) and avoids the intractability of true POMDPs, which have no guarantees on providing enough information to sufficiently predict future rewards. A relaxation of this assumption entails providing less information in the observation for predicting future reward, which will degrade performance.  We  show empirically that our method is still more robust to a relaxation of this assumption compared to other MTRL methods.\u201d \n\n2. Authors have an experiment for this, but I would like to see how much p- probability of sticky observation would actually affect the performance, for example what would happen if we increase p, and what point the algorithm will break down? (Basically, I'm asking how sensitive the algorithm is to Block MDP assumption).  \nWeakening the Block MDP assumption breaks performance for all algorithms, because there is not enough information in the observation to predict information for any method. However, our experiments show that our method, HiP-BMDP, still outperforms other MTRL methods in this setting, which means it is better at extracting the limited information in the observation compared to other methods. We have added an additional plot to figure 7 showing the performance of HiP-BMDP under different values of p: 0.01, 0.02, 0.05, 0.1, 0.2, 0.5. Performance starts to decrease at p=0.1, and is completely unable to learn at p=0.5. \n\n3. It seems like some notational clarity can help the paper, for example what is \\Phi in the definition of \\epsilon_T? Or what is Z in definition of \\phi ?\u201d   \nA: Apologies, this definition was accidentally moved to appendix. It is now explained in the definition of \\epsilon_T as: $\\Phi T$ denotes the \\textit{lifted} version of $T$, where we take the next-step transition distribution from observation space $\\mathcal{X}$ and lift it to latent space $\\mathcal{S}$. The $\\mathbb{Z}$ in the definition of $\\psi$ (I believe this is what you\u2019re referring to, please correct me if I\u2019m wrong) is the notation for the set of integers. We have clarified this to be the set of positive integers in the text.\n", "title": "Thank you for your assessment!"}, "o0ilfeZEh7": {"type": "rebuttal", "replyto": "FCdJFgI2o9d", "comment": "7. Sec. 3.2: the definition of \\phi maps S to Z. Isn't it X to Z?  \nA: Yes, thank you for the catch. Fixed!\n\n8. In Sec. 3.2, the 2-Wasserstein distance is used, while the 1-Wasserstein distance was introduced earlier. Which one is used?  \nA: We use the 2-Wasserstein distance because the W_2 metric has a convenient closed form. We have corrected the theta objective equation to use W_2 to be consistent.\n\n9. In Eq. 3, it was not clear to me why the \"model learning error\" term uses the MSE for two different environment labels (isn't one enough)?  \nA: Yes. The two terms here just reflect the two transitions used to compute the bisimulation distance for training the task representation.\n\n10. What is exactly the \"evaluation of the optimal Q function of \\bar{M} on M\" in Theorem 2? Do you mean that we take the optimal policy of \\bar{M} and test it on M?  \nA: Yes.\n\n11. [if the above comment is correct] Theorem 2 was slightly weird to me: it takes the optimal policy of \\bar{M}, evaluates it on M_{\\theta_i}, and check how much the corresponding Q function differs from the optimal Q function of M_{\\theta_j}. If we wanted to figure out the transfer error of the optimal policy of \\bar{M} on M_{\\theta_j}, shouldn't we compare the optimal Q function of M_{\\theta_j} with the Q function of the policy on the same MDP (rather than on M_{\\theta_i})?  \nA: Apologies for the confusion -- this was a typo in the theorem! Thank you for the catch. You\u2019re correct, this is the optimal policy of \\bar{M} applied to M_{\\theta_j}. We have fixed it in the PDF.\n\n12. How exactly is the \"empirical measurement of Q^*_{\\bar{M}} over D\" computed?  \nA: This is the optimal value function of the MDP constructed from our learned state abstraction as computed from data D. This is computed with tabular value iteration with data in D.\n\n13. In Th. 4, the last term bounds the concentration of expectations of Q functions (which are bounded by rmax/(1-\\gamma). Shouldn't there be a multiplicative rmax in front of the sqrt?  \nA: You are correct. Fixed.\n\n14. Page 6: \"We compare the our method\" -> \"We compare our method\"   \nA: fixed\n\n15. In the multi-task experiments, it was not clear what method was used in combination with HiP-BMDPs to learn policies (though it is in appendix).  \nA: We have added a paragraph to the Background section to detail that we are using Soft Actor-Critic for downstream evaluation.\n\n16. How many adaptation steps for \"few-shot generalization\" were used in Figure 5?  \nA: 100 steps, as used in PEARL.\n\n17. Below Figure 5: \"and use a behavior policy to collect transitions.\" What is the behavior policy exactly and how was it chosen?  \nA: The behavior policy is an expert policy trained on this task, because we are interested in how well the model generalizes on a data distribution collected by an optimal policy, which is a very different prediction problem (and likely more difficult) than from data generated by a random policy.\n\nSome minor comments/questions: \na. Page 1: \"This additional structure gives us better sample efficiency, both theoretically and empirically.\" Better than what algorithm?  \nA: Better than baselines and theoretical results of other works presented in the related work, such as Brunskill et al. 2013. Added this detail to this sentence in the updated draft.\n\nb. Page 2: \"don't\" -> \"do not\" fixed\n\nc. Page 2: \"which naturally leads to a gradient-based representation learning algorithm.\" Why is it natural to use a gradient-based algorithm in this setting?  \nA: Our wording was meant to imply that this representation learning objective can be optimized with a gradient-based algorithm.\n\nc. Background: you could mention that the finite-MDP assumption is only for deriving the theoretical results, while the method can be applied in continuous domains (right?)   \nA: correct, added a footnote to Background to note this.\n\nd. Page 2: \"a unobservable \"-> \"an unobservable\" fixed\n\ne. Page 3: \"where l2 distance corresponds to d\" Why? d isn't the l2 distance, right?   \nA: This was meant to give intuition that if you have a state space (or learned representation) where d is the distance metric, you have a Lipschitz MDP. We have simplified this to say V* is Lipschitz with respect to d.\n\nf. Sec. 3.1: \\phi in \\epsilon was never defined.  \nA: Apologies -- the definition was accidentally moved to the appendix. Adding back.\n", "title": "Response Cont."}, "FCdJFgI2o9d": {"type": "rebuttal", "replyto": "NIQ82AGSfpI", "comment": "Thank you for your thoughtful response and detailed review. To address your main concerns: we did not focus on the errors in learning the abstractions and structure themselves because these are supervised learning problems with well understood bounds. Assuming that we can get low error with these, and measuring what that error is, we want to understand how this affects the downstream control problem. The requirement that task IDs are known is truly a potential limitation, as we also bring up in our discussion (Section 6). As we note there, we can incorporate a system identification procedure at training time to replace this assumption. Further, we note that most MTRL literature including the baselines we compare against, similarly assume this task id knowledge. Finally, we have improved the readability of the paper based on your comments (and fixed some typos and errors!), and also individually address each point below.\n\n1. Are the environment labels/ids used only for communicating to the learner that the task changed or could they provide more information? For instance, if we face the same task twice in two non-consecutive episodes, are the corresponding labels equal?   \nA: The environment labels are tied to the environment throughout training. Therefore, yes, if we face the same task twice in non-consecutive episodes, the corresponding labels are equal.\n\n2. What makes these state abstractions \"robust\" (as in the title)? Is there any particular theoretical or empirical evidence that justifies this term?   \nA: We chose to call these state abstractions \u201crobust\u201d due to two reasons: 1) Our theoretical analysis of value bounds (Sec 3.3) quantifies the error in transferring a policy learned on task i to a task j is primarily governed by the distance in the task-space defined by the hidden parameter \\theta. This translates to the robustness of a policy learned in task i when evaluated in a task j, 2) Our empirical analysis of learning state abstractions for HiP-MDPs further corroborate our claims on how \u201crobust\u201d the learned representations are across a family of tasks (Sec. 4).\n\n3. Existing analyses of multi-task settings, including the one of Brunskill and Li [1] and others [2,3,4], provide guarantees for task-structure (e.g., dynamics models) learned from data, while here the focus seems to be on guarantees for arbitrary abstractions/structures as a function of their errors, without considering how they are learned. This makes it difficult to interpret the results in the context of the proposed approach. \nA: Yes, that is correct. One drawback of existing guarantees is that they are tied to a specific algorithm and do not handle deep learning family of approaches. However, the guarantees presented in the proposed approach are learning approach agnostic for any abstractions which define an approximate bisimulation on a HiP-BMDP family. \n\n4. Regarding the statement: \"sample complexity bounds that depend on the aggregate number of samples across tasks, rather than the number of tasks, a significant improvement over prior work\", the bounds derived in [4] depend only on a number of abstract \"transition templates\" that they define, while those derived in [3] depend on the minimum between the number of tasks and the number of states.  \nA: Thank you for these references, we will add them to related work. We will rephrase our claim in the abstract. However, we argue that our method is superior to those in [3, 4] --  [4] only works with discrete state spaces, and does not scale well as the abstraction they learn for each state-action pair is of dimension |S|+1. [3] relies on a generative model to query any possible state-action pair -- an unreasonable assumption in most environments.  Further, since [3] depends on the minimum between number of tasks and number of states, and in our setting states always vastly outnumber the number of available tasks, we argue that [3] still depends on number of tasks. \n\n5. To be clear, the notation | x - y |_1 is used for the Wasserstein distance and not l1-norm, right? In such a case, the sentence \"We omit d but use d(x, y) = |x \u2212 y|_1 in our setting\" is quite confusing.  \nA: This notation refers to l1-norm in the constructed task embedding space, which corresponds to Wasserstein distance between probability distributions across tasks.\n\n6. The definitions of the terms \\epsilon_R, \\epsilon_T, \\epsilon_\\theta could be moved to the theoretical analysis since they are introduced early and never used before Sec 3.3. \nA: Moved!\n\n", "title": "Thank you for your very helpful review"}, "E5H5PhnOHJa": {"type": "rebuttal", "replyto": "k6JJrFGW0Gu", "comment": "Thank you for your detailed comments and thoughtful review! To address your main concerns -- 1) insights: the bisimulation theory was used to design an objective for learning the hidden parameter in HiP-MDPs, which is our task bisimulation loss. This allows for our theoretical results -- the transfer and generalization bounds which extend beyond the results in the Gelada paper to handle different dynamics and the multi-task setting. The DeepMDP paper further does not have sample complexity bounds.  2) empirical results: We are not sure what baseline you are referring to that handles other reward functions but is not outperformed, as our method outperforms all other MTRL baselines in Fig 4. Could you clarify this concern further? Finally, the additional requested baseline is actually already given (specified in more detail below). We hope you will reconsider your score, given that we have clarified that the gains are not due to the image representation alone.\n\nQ: One thing that stood out to me was the very small number of training tasks used - only 4! I wonder if using more training tasks improves the generalization of single-task methods like DeepMDP that might be able to simply generalize over the dynamics changes given more examples. How much overfitting to the training tasks do you observe for these baseline methods versus HiP-BMDP?   \nA: One of the goals of our work is to examine generalization capabilities in few environment settings, and how additional structural assumptions like HiP-MDP can improve few-shot generalization. Figure 9 in the appendix shows evaluation performance on the training environments, where we similarly see poorer performance by baselines, and therefore that overfitting is not yet happening for baselines.\n\n\nQ: It seems like there might be a baseline missing - generic multi-task algorithm that conditions on the environment ID but does use the bisimulation loss to help process the image observations. From my understanding, all baselines have no image representation learning component except DeepMDP which is not a multi-task or meta-RL algorithm. I am a bit concerned that the gains we are seeing here have more to do with image representation learning than the structural assumptions employed by the algorithm.  \nA: We have clarified this in the paper, but the HiP-BMDP-nobisim baseline is this exact experiment! It is DeepMDP (what you are calling the bisimulation loss, aka a latent model loss and reward loss) without the \u201cbisimulation loss\u201d which we use to refer to our task embedding loss which using a form of bisimulation metric for the task embedding. Apologies for the confusion, we will rename our \u201cbisimulation loss\u201d to \u201ctask bisimulation metric loss\u201d to better clarify the difference in objectives. This baseline shows that using the image representation learning component of DeepMDP with a task id is not sufficient to achieve the good performance exhibited by HiP-BMDP, and that our task bisimulation metric loss which leverages the structural assumptions of HiP-MDP is necessary. \n\n\nQ: I think that for the meta-RL setting, \u201cenvironment ID\u201d is replaced everywhere by experience seen so far in the task - is that correct? It would be good to make that clear, or if environment IDs are being used in meta-RL, how they are being used.   \nA: Correct. The environment ID is not used in the meta-RL setting, and is replaced by the context constructed from the history. We use the same setup as PEARL.\n\n\nQ: I\u2019m a bit confused why there\u2019s a gap between your method in the multi-task experiments but not in the meta-RL ones. Do the meta-RL experiments use image observations? If so, how is the PEARL baseline adapted to handle images? If not, why not?  \nA: The Meta-RL experiments do not use image observations, but proprioceptive state since all baseline algorithms are in that setting. Meta-RL techniques are too time intensive to train on pixel observations directly. We have updated the paper to make that more clear.\n\n\nQ: Why does HiP-BMDP handle the sticky observation setting better than the baselines? Isn\u2019t it impossible for it to persist state information over time steps?  \nA: Yes, what we are evaluating is the ability of each method to extract as much information from the current observation as possible and be robust to missing observations -- a relaxation of the Block MDP assumption. HiP-BMDP handles this setting better than other methods because is capturing all information available to predict future rewards in a principled manner through the bisimulation losses, unlike the other baselines.\n\n", "title": "Clarification of empirical results and insights"}, "y8ngIgT-X15": {"type": "rebuttal", "replyto": "Q5ZyXOYR507", "comment": "Thank you for your review and kind words. Please do let us know if there is anything we can address for you to increase your score. ", "title": "Anything to address?"}, "Q5ZyXOYR507": {"type": "review", "replyto": "fmOOI2a3tQP", "review": "This paper studies a family of Markov Decision Process (MDP) models with a low-dimensional unobserved state, called the block MDP. The authors assume that system dynamics of the underlying MDP is sufficiently summarized by a parameter $\\theta$. This learning setting could be seen as a combination of Block MDP and the Hidden Parameter MDP; hence the name HiP-BMDP.\n\nThe authors then propose algorithms to learn parameters $\\theta$ of HiP-BMDP from observed trajectories of the system. The authors derive the error bounds over between the Q-value parametrized by the learned $\\theta$ and that of the actual parameter $\\theta^*$. The authors then apply these results to the transfer learning settings where one applies the learned parameter $\\theta$ to a different but somewhat similar environment parametrized by $\\theta_i$. The error bound over Q-values in such a transfer learning settings is also provided. These results, including Theorems 2 and 3, seem sensible, while I haven't checked the details of the proof. Finally, the authors validate the proposed method through extensive simulations.\n\nOverall, I believe this paper is well written and well organized. The assumptions of HiP-BMDP seem sensible and the derived error bounds look promising. These results are applicable in multi-taks learning settings. They explicate a set of parametric conditions under which one could accelerate the learning process of a future task from prior knowledge derived from previous tasks.", "title": "Hidden Parameter Block MDPs", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "RQmm50Xo4a5": {"type": "review", "replyto": "fmOOI2a3tQP", "review": "Hi,\n\nFirst I want to thank authors for putting this together, I enjoyed reading it. \n\n*Summary* Authors propose mixing Block MDP and Hidden Parameter MDP, and offer and algorithm (loss function) to learn this model that they claim to be useful for sample complexity and transferability among environments (that share state and action space). The proposed method showed promising results in experiments. \n\nIn general I enjoyed reading the paper, and I believe the idea is novel, a good step toward multi-task learning and well-written. My main concern is the validity of Block MDP assumption in real world. \n1. Authors have to be more upfront on why this assumption is needed, it's hard now to exactly find why do we need Block MDP assumption, what would break if we were to relax this?\n2. Authors have an experiment for this, but I would like to see how much p- probability of sticky observation would actually affect the performance, for example what would happen if we increase p, and what point the algorithm will break down? (Basically, I'm asking how sensitive the algorithm is to Block MDP assumption)\n\nIn general I enjoyed the paper, I'd like the theory part. \nIt seems like some notational clarity can help the paper, for example what is \\Phi in the definition of \\epsilon_T? Or what is Z in definition of \\phi ? \n", "title": "Good Paper", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}