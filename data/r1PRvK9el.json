{"paper": {"title": "Implicit ReasoNet: Modeling Large-Scale Structured Relationships with Shared Memory", "authors": ["Yelong Shen*", "Po-Sen Huang*", "Ming-Wei Chang", "Jianfeng Gao"], "authorids": ["yeshen@microsoft.com", "pshuang@microsoft.com", "minchang@microsoft.com", "jfgao@microsoft.com"], "summary": "", "abstract": "Recent studies on knowledge base completion, the task of recovering missing relationships based on recorded relations, demonstrate the importance of learning embeddings from multi-step relations. However, due to the size of knowledge bases, learning multi-step relations directly on top of observed instances could be costly. In this paper, we propose Implicit ReasoNets (IRNs), which is designed to perform large-scale inference implicitly through a search controller and shared memory. Unlike previous work, IRNs use training data to learn to perform multi-step inference through the shared memory, which is also jointly updated during training. While the inference procedure is not operating on top of observed instances for IRNs, our proposed model outperforms all previous approaches on the popular FB15k benchmark by more than 5.7%.", "keywords": ["Deep learning", "Reinforcement Learning"]}, "meta": {"decision": "Reject", "comment": "This paper develops a new shared memory based model for doing inference in knowledge bases. The work shows strong empirical results, and potentially could be impactful. However the reviewers felt that the work was not completely convincing without more analysis into the mechanisms of the system itself. \n \n Pros:\n - Quality: The reviewers like the experimental results of this work, praising them as \"strikingly good\", but giving the caveat the dataset used is now a bit old for this task. \n \n Mixed:\n - Clarity: Some reviewers found the work to be fairly well-written although there was mixed opinions about the exposition. Details of the model could be better explained, as could the development of the model\n \n Cons:\n - Quality: The main criticism is not feeling that the methodology is motivated for this task. Multiple reviewers claim there is \"little analysis about how it works\". Or that was \"hard to see\" how this would help. All reviewers are in agreement, that the paper should explore more deeply what shared memory is adding to this task, and introduce the approach better in this regard."}, "review": {"SkRbj6LUl": {"type": "rebuttal", "replyto": "BJF_A9INl", "comment": "Thanks for your insightful review. To address your comment, we added Table 2 and a corresponding paragraph to analyze the performance with different termination steps (T_max = 1, 2, 5, 8) and memory sizes (|M| = 32, 64, 128, 256, 4096).\nAs for the introduction section, we will update the introduction to incorporate the motivation and the comparison against existing methods from Section 2.\n", "title": "Reply to Reviewer3"}, "Hk4lipIUx": {"type": "rebuttal", "replyto": "SyHbpXIVl", "comment": "Thanks for your insightful review. To address your comments, we added Table 2 and a corresponding paragraph to analyze the performance with different termination steps (T_max = 1, 2, 5, 8) and memory sizes (|M| = 32, 64, 128, 256, 4096). In the T_max = 1 cases, it is the case where IRNs do not use the shared memory. We found the number of times IRNs access the shared memory is critical for the performance, so IRNs cannot achieve the same level of performance without using shared memory.\nRegarding the attention over memory cells, we have found some interesting behaviors of the shared memory by counting the most active relations of each memory cell. For example, in one particular memory cell, we observe its most active relations from a cluster around \u201cfamily/spouse based\u201d relations. We will update some findings in the discussion and we will test the current model on more challenge tasks, i.e., knowledge base QA, machine reading, and conversation bot in the future.\n", "title": "Reply to Reviewer1"}, "Skya5aIIl": {"type": "rebuttal", "replyto": "B1g02CM4g", "comment": "Thanks for your insightful review. To address your comments, we added Table 2 and a corresponding paragraph to analyze the performance with different termination steps (T_max = 1, 2, 5, 8) and memory sizes (|M| = 32, 64, 128, 256, 4096). Regarding the shortest path experiments on KB and models with dynamic memory size, we will investigate these directions in the future.", "title": "Reply to Reviewer2 "}, "S1Blkk5Ng": {"type": "rebuttal", "replyto": "r1PRvK9el", "comment": "Thanks all reviewers for your great feedback and comments. We have updated the paper to address the major comments for adding analysis in the KB experiments. We add Table 2 and a corresponding paragraph to analyze the performance with different termination steps and memory sizes. (Note that for T_max =1, it is the case where IRNs do not use the shared memory.)  We found the number of times IRNs access the shared memory is critical for the performance, so IRNs cannot achieve the same level of performance without using shared memory. \nIn response to reviewer 2, regarding the shortest path experiments on KB and models with dynamic memory size, we will investigate these directions for future work. \nWe will update our paper to improve the introduction section and figure 1, and add more analysis on the termination steps.\n", "title": "Report the performance of IRNs with different memory sizes and inference steps on FB15K"}, "rJKomhHml": {"type": "rebuttal", "replyto": "ryU05ayXx", "comment": "Thanks for providing feed-backs about the paper. Here is our reply to your questions. \n\n- Is there any way to know what is encoded in the shared memory? It seems a bit hard to understand what is stored there.\nReply :\nDeveloping a full-scale visualization method for the neural network weights is a research topic by itself, and so far it is not clear to us what is the best way to visualize the shared memory. We are open and happy to discuss with you about how to perform visualization experiment on shared memories.\n\nHowever, we tried a few visualization experiments, and did find some interesting behaviors of the shared memory by sampling some shared memory cells and their most active relations (according to the sum of the attention weights).  For example, in one particular memory cell, we observe its most active relations from a cluster around  \u201cfamily/spouse based\u201d relations such as \u201c/celebrity/lived_with\u201d,\u201c/celebrity/breakup\u201d and \u201c/people/person/spouse\u201d. Other clusters can be observed in other cells as well.  \nWe also plan to apply the shared memory structure to other applications to fully understand its potential power in the future.\n\n- In section 3 is written \"We sample a set of incorrect entity embeddings\". This is just for training, right? For test, all entities have to be considered as candidate.\nReply : Yes, sampling negative entities is just for training phase. For test, all entities will be considered as candidates.\n\n- Is the same model used to predict head and tail entities?\nReply : Yes, we use the same model to predict head and tail entities.\n", "title": "Reply to AnonReviewer1"}, "r1Yb-SeQg": {"type": "rebuttal", "replyto": "SyyWn3w-x", "comment": "Thank you for clarifying this!", "title": "Comment on shared memory"}, "ryU05ayXx": {"type": "review", "replyto": "r1PRvK9el", "review": "- Is there any way to know what is encoded in the shared memory? It seems a bit hard to understand what is stored there.\n- In section 3 is written \"We sample a set of incorrect entity embeddings\". This is just for training, right? For test, all entities have to be considered as candidate.\n- Is the same model used to predict head and tail entities?This paper proposes a method for link prediction on Knowledge Bases. The method contains 2 main innovations: (1) an iterative inference process that allows the model to refine its predictions and (2) a shared memory component. Thanks to these 2 elements, the model introduced in the paper achieved remarkable results on two benchmarks.\n\n\nThe paper is fairly written. The model is interesting and the experimental results are strikingly good. Still, I only rate for a weak accept for the following reasons.\n\n* The main problem with this paper is that there is little explanation of how and why the two new elements aforementioned are leading to such better results. For instance:\n  - What are the performance without the shared memory? And when its size is grown? \n  - How does the performance is impacted when one varies Tmax from 1 to 5 (which the chosen value for the experiments I assume)? This gives an indications of how often the termination gate works.\n  - It would also be interesting to give the proportion of examples for which the inference is terminated before hitting Tmax.\n  - What is the proportion of examples for which the prediction changed along several inference iterations?\n\n* A value of \\lambda set to 10 (Section 2) seems to indicate a low temperature for the softmax. Is the attention finally attending mostly at a single cell? How do the softmax activations change with the type of relationships? the entity type?\n\n* FB15k and WN18 are quite old overused benchmarks now. It would be interesting to test on larger conditions.", "title": "Questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyHbpXIVl": {"type": "review", "replyto": "r1PRvK9el", "review": "- Is there any way to know what is encoded in the shared memory? It seems a bit hard to understand what is stored there.\n- In section 3 is written \"We sample a set of incorrect entity embeddings\". This is just for training, right? For test, all entities have to be considered as candidate.\n- Is the same model used to predict head and tail entities?This paper proposes a method for link prediction on Knowledge Bases. The method contains 2 main innovations: (1) an iterative inference process that allows the model to refine its predictions and (2) a shared memory component. Thanks to these 2 elements, the model introduced in the paper achieved remarkable results on two benchmarks.\n\n\nThe paper is fairly written. The model is interesting and the experimental results are strikingly good. Still, I only rate for a weak accept for the following reasons.\n\n* The main problem with this paper is that there is little explanation of how and why the two new elements aforementioned are leading to such better results. For instance:\n  - What are the performance without the shared memory? And when its size is grown? \n  - How does the performance is impacted when one varies Tmax from 1 to 5 (which the chosen value for the experiments I assume)? This gives an indications of how often the termination gate works.\n  - It would also be interesting to give the proportion of examples for which the inference is terminated before hitting Tmax.\n  - What is the proportion of examples for which the prediction changed along several inference iterations?\n\n* A value of \\lambda set to 10 (Section 2) seems to indicate a low temperature for the softmax. Is the attention finally attending mostly at a single cell? How do the softmax activations change with the type of relationships? the entity type?\n\n* FB15k and WN18 are quite old overused benchmarks now. It would be interesting to test on larger conditions.", "title": "Questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkaKICgfl": {"type": "rebuttal", "replyto": "ryKrzdCbg", "comment": "Thank you for your interests in our paper. It seems that there are some misunderstandings in the comments, and please see our responses below.\n\nThe point of our model is that IRNS do *not* need human to manually construct the content of the shared memory. In the previous the neural KB paper such as [1], the authors construct the memory by storing the whole KB information explicitly at the initialization step. Unlike these models, IRNs initialize the memory randomly. At each step, a training triplet is processed through the model by Algorithm 1 (hence, no explicit path information is given). Instead of storing any triplet information explicitly, the shared memory in IRNs is designed to store the information implicitly. Therefore, the size of memory can be much smaller than the size of the whole knowledge base.\n\nWe did list the comparisons of our work and ReasonNet in Section 2. We found that the usage of the shared memory is necessary for KB completion, and this critical issue is not addressed in our previous paper. In original ReasoNets, the memory vectors are explicitly reinitialized to store the given information associated with each instance (the size of the memory is same as the length of the corresponding information source). In contrast, IRNs incorporate the shared memory component, which is used by all instances.\n\nThanks again for your comments. (We add this clarification in Section 3.)\n\n[1] Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, Jason Weston, Key-Value Memory Networks for Directly Reading Documents, In EMNLP, 2016\n", "title": "comment on the shared memory"}, "ryKrzdCbg": {"type": "rebuttal", "replyto": "r1PRvK9el", "comment": "Can you explain in detail how you construct the shared memory for each training sample <t, r, h>? It's completely unclear to me and never mentioned in the paper. If you use many triples (those triples in which any of the t, r, h appears) for each sample as shared memory, then you use the path information and you should mentioned this in the table. \n", "title": "comment on the shared memory"}, "SyyWn3w-x": {"type": "rebuttal", "replyto": "BJ5pYRxZl", "comment": "We have added some clarifications in the latest draft.  The idea of exploiting shared memory is proposed by [1] independently.  Despite of using the same term, the goal and the operations used by IRNs are different from the one used in [1], as IRNs allow the model to perform multi-step for each instance dynamically.\nIn [1], the input instance encoding and the memory  access is synchronized, as the number of memory access is equal to the input length. In our work, the number of memory access is decided by the complexity of instance, which is modeled by the search controller. The paper [2] is in fact more related to our earlier previous work [3] (in terms of model and the task), which proposes the dynamic termination for machine comprehension tasks. \n\n[1]. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Semantic Encoders.\" arXiv preprint arXiv:1607.04315 (2016), July 20, 2016. \n[2]. Munkhdalai, Tsendsuren, and Hong Yu. \"Reasoning with Memory Augmented Neural Networks for Language Comprehension.\" arXiv preprint arXiv:1610.06454 (2016), Oct 20 2016.\n[3] Shen et al., ReasoNet: Learning to Stop Reading in Machine Comprehension, https://arxiv.org/abs/1609.05284, Sep 17, 2016\n", "title": "Comment on shared memory"}, "BJ5pYRxZl": {"type": "rebuttal", "replyto": "r1PRvK9el", "comment": "The idea of shared memory in context of memory augmented neural networks is not novel. Neural Semantic Encoders previously introduced shared and multiple memory accesses [1, 2]. Please discuss the connection between Implicit ReasoNet and Neural Semantic Encoders in your manuscript.\n\nThanks,\n\n\nRef:\n\n1. Munkhdalai, Tsendsuren, and Hong Yu. \"Neural Semantic Encoders.\" arXiv preprint arXiv:1607.04315 (2016).\n2. Munkhdalai, Tsendsuren, and Hong Yu. \"Reasoning with Memory Augmented Neural Networks for Language Comprehension.\" arXiv preprint arXiv:1610.06454 (2016).", "title": "Comment on shared memory"}}}