{"paper": {"title": "Domain Adaptation via Low-Rank Basis Approximation", "authors": ["Christoph Raab", "Frank-Michael Schleif"], "authorids": ["christoph.raab@fhws.de", "frank-michael.schleif@fhws.de"], "summary": "The paper describes a low-rank basis transfer algorithm using only a subset from the domains with outstanding results.", "abstract": "Domain adaptation focuses on the reuse of supervised learning models in a new context. Prominent applications can be found in robotics, image processing or web mining. In these areas, learning scenarios change by nature, but often remain related and motivate the reuse of existing supervised models.\nWhile the majority of symmetric and asymmetric domain adaptation algorithms utilize all available source and target domain data, we show that efficient domain adaptation requires only a substantially smaller subset from both domains. This makes it more suitable for real-world scenarios where target domain data is rare. The presented approach finds a target subspace representation for source and target data to address domain differences by orthogonal basis transfer. By employing a low-rank approximation, the approach remains low in computational time. \nThe presented idea is evaluated in typical domain adaptation tasks with standard benchmark data.", "keywords": ["Domain Adaptation", "Basis Transfer", "Transfer Learning", "Low Rank Approximation", "Nystr\u00f6m Approximation"]}, "meta": {"decision": "Reject", "comment": "Three reviewers have scored this paper  as 1/1/3 and they have not increased their rating after the rebuttal and the paper revision. The main criticism revolves around the choice of datasets, missing comparisons with the existing methods, complexity and practical demonstration of speed. Other concerns touch upon a loose bound and a weak motivation regarding the low-rank mechanism in connection to DA. On balance, the authors resolved some issues in the revised manuscripts but reviewers remain unconvinced about plenty other aspects, thus this paper cannot be accepted to ICLR2020."}, "review": {"SygYXty2jB": {"type": "rebuttal", "replyto": "SJxZbsC9oB", "comment": "> Authors claim that the proposed method is the fastest domain adaptation algorithm in terms of computational complexity.  It is necessary to demonstrates this statement experimentally, especially in large-scale datasets.\nWe proposed a domain adaptation algorithm. Hence, we have tested it on domain adaptation datasets. Notably, the Newsgroup is the highest dimensionally, which is crucial for evaluating computational time. But also the dataset has a high number of samples compared to other DA datasets.\nFurther, we claimed not to be the fastest, but the fastest in comparison to the tested algorithms. \n\n> For fairness,  authors can not compare with an algorithm that has no definite or correct results.\nWe removed CGCA out of the experiments to be aligned with the review. \n\n> \"Out BT approach has no free parameter, and for NBT, the subspace dimension is set  to 500 for text and 210 for images.\"  (on page 14, the last sentence in the subsection of A.4). \nThis is not an \"Out\", but a \"The\" and has no spelling issue. ", "title": "Re"}, "rJlLQXevoH": {"type": "rebuttal", "replyto": "Bklsscm6Yr", "comment": "Thanks for the review. Please find our statements subsequently to the various points mentioned in your review. We do not expect that this comment will substantially\nchange your opinion but believe that some clarifications are required which should\nbe in your interest and also in the interest of the area chair or respectively the\nhonored organizers of this ICLR 2020 conference.\n\n1. \"This manuscript is mainly based on the previous work BT (Raab and Schleif, 2018). The novelty seems to be too limited.\"\n>This sentence contains no useful feedback. The only thing we have in common with BT is the principle. We discuss this in various points throughout the paper. \n\n2. The Datasets used in the experiments are not representative. \n>The proposed datasets are commonly used in the area of domain adaptation and recently used. \nFurther, we showed in Landau notation that we have lower complexity. Therefore it is representative.\n\n\n3. CGCA is not provided\n>For CGCA a comparison is complicated due to numerical errors in the - original - implementation. We have provided the obtained results in the experiments.\nThe deep adaptation techniques are missing, mainly because we proposed a non-deep learning approach. As you can easily check, we have evaluated our approach on the variety of typically\nused benchmark data. There are additional very good reasons not to focus on deep learning approaches alone. Deep learning requires a huge amount of reliable (labeled) training data which is not applicable in many real life problems (e.g. manufacturing domains, for small to medium scale companies a.s.o). In those settings, we may have rather a few 100 to thousands of data but not at the scale of deep learning. This is a particular motivation why non-deep learning methods are also extended to transfer learning. Otherwise, we could completely abandon everything aside of deep learning. This paper was not around deep learning - on purpose - and due to the aforementioned reasons. \n\n\n4. \" (1) 'Out BT approach has no free parameter\u2026' on page 14.\"\n>Sorry, but this sentence does not appear on page 14. Not even in the whole paper. \n\n4.2 Thank you, we corrected it. \n\nThank you for reading. \n", "title": "Comment to Review 3"}, "rkl5S7xPsH": {"type": "rebuttal", "replyto": "SJeQ8PccYr", "comment": "Thanks for the review. Please find our statements subsequently to the various points mentioned in your review. We do not expect that this comment will substantially\nchange your opinion but believe that some clarifications are required which should\nbe in your interest and also in the interest of the area chair or respectively the\nhonored organizers of this ICLR 2020 conference.\n \n\n\"This paper lacks research motivation and solid experimental validation.\"\n> As mentioned in the paper, the motivation is discussed in section 1. And the benefits and differences to related work are further considered in section 2, section 4, and section 5.\n> For the reasoning about experimental design, see the comments to review 1 and 2.\n\n'The authors claim \"it is the fastest domain adaptation algorithm in terms of computational complexity\" which is not very convinced.'\n>Honestly, we do not know how to respond to this. We showed in landau notation and experimental results that the method is efficient.\n\n\"I cannot observe new knowledge in domain adaptation, except the Nystrom approximation technique used. \"\n>As mentioned in the paper, the override of the basis of source in the target subspace leads to effective domain adaptation. Note the proposed approach is detailed at the beginning of eq. 11.\n\n\"The equation (9) is similar to the representational based transfer model, where the T matrix is just the reconstruction matrix because X and Z have a different number of samples.\"\n>As mentioned in the paper, the NBT does -- not -- need a reconstruction matrix and also eq. 9 is -- not -- the NBT formulation but a recap of related work. \n-- Please consider the correct approach beginning at eq. 11. --\n\nThis paper written is a little poor, and the novelty of NBP is not clearly claimed.\n>We claimed it four times in section1, section 2, section 4, and section 5. We clearly want to make sure the reader gets it.\n>We cannot extract useful feedback given \"a little poor\" please provide constructive feedback. \n\n\"In deep learning era, could you discuss what is the value in deep transfer learning or deep domain adaptation?\"\n> This paper was not at all-around deep learning - on purpose - and due to the aforementioned reasons. Please see the comment to reviewer 1. \n\n \nThank you for reading.\n", "title": "Comment to Review 2"}, "SJx8DGevir": {"type": "rebuttal", "replyto": "HkgkZ4H6tH", "comment": "Thanks for the review. Please find our statements subsequently to the various points mentioned in your review. We do not expect that this comment will substantially\nchange your opinion but believe that some clarifications are required which should\nbe in your interest and also in the interest of the area chair or respectively the\nhonored organizers of this ICLR 2020 conference.\n\n>>  This paper should be rejected because [...]\n\n(1) the paper lacks important latest references on domain adaptation, \n>In your opinion, we miss just two papers: MEDA and JGSA. The rest of the related work is partly very recent.  \nWe define related work as related to our work, which in this case are subspace approaches\nfor transfer learning in traditional learning models, where only a moderate amount of data may be available. Deep learning approaches for transfer learning are competitive approaches\nto the considered methods but are not in particular focus of our approach nor the other\nvery recent high ranked work we compared within the paper. \nThe methods MEDA and JGSA have been published -- very -- recently. \n\n(2) the paper misuses the notations that makes the paper is not easy to follow,\n>We are using a standard domain adaptation notation. But for two domain matrices with singular value decompositions with their low-rank versions, we got 14 different matrices. We are sorry that it requires a variety of notations. \nYou claim that \"In line 2 of page 5, the authors claim that BT assumes S_Z ~ S_X, which is not true.\"\n> This is mentioned in the paper, and we invalidated this by ourselves for some cases. Take a look at the pseudo-code and figure 2. How can we get a reject for something we claim by ourselves and provide a possible solution for it?\nFurther, the critiques on wrong equations or claims (3,4,6,8) are very detailed before or after the equations, respectively. Finally, we call it low-rank approach because we are using approximated low-rank matrices via Nystr\u00f6m. \n\n\n(3.1) the algorithm is not well justified either by theory\n>The theory of this approach is based on the singular value decomposition, eigenvalue decomposition, and the orthogonal Procrustes problem. These techniques and problems are well known and studied for decades. We do not change the mechanics of the approaches but interpreted the results for domain adaptation techniques, and therefore, the theory is also valid. Your current complains about the theory in the paper are too vague to help us in improving the paper. You mentioned that the bound is not very meaningful but actually shows that matrices are very similar after NBT resulting in similar distributions and is perfectly fine for our case. \n\n(3.2) or experiments\n>The deep adaptation techniques are missing, mainly because we proposed a non-deep learning approach. As you can easily check, we have evaluated our approach on the variety of typically\nused benchmark data. There are additional very good reasons not to focus on deep learning approaches alone. Deep learning requires a huge amount of reliable (labeled) training data which is not applicable in many real life problems (e.g. manufacturing domains, for small to medium scale companies a.s.o). In those settings, we may have rather a few 100 to thousands of data but not at the scale of deep learning. This is a particular motivation why non-deep learning methods are also extended to transfer learning. Otherwise, we could completely abandon everything aside of deep learning. This paper was not around deep learning - on purpose - and due to the aforementioned reasons. \nWe will integrate MEDA into the revision of the paper, but this will not change the overall picture. We did not do any cherry-picking. \n\n>The reviewer misses the essential part of this paper. We do not learn or find a new projections matrix. We interpret the solutions of the orthogonal Procrustes problem to give a subspace transformation. This paper shows that for effective domain adaptation, the learning of complicated functions is not necessarily required. \n\n\n>Note, we subsequently will provide the experimental results for MEDA, which seems to be an important requirement for acceptance. Parts are already included right now. \n\n(4) Thank you for the feedback. We fixed the issues mentioned in point 9. \n \n\nThank you for reading. ", "title": "Comment to Review 1"}, "SJeQ8PccYr": {"type": "review", "replyto": "BJeUs3VFPH", "review": "This paper is mainly an improvment of the basis transfer model, by introducing the Nystrom approximation and a NBT model is formulated. This paper lacks of research motivation and solid experimental validation.\nThe authors claim \"it is the fastest domain adaptation algorithm in terms of computational complexity\", which is not very convinced.\nI cannot observe new knowledge in domain adaptation, except the Nystrom approximation technique used.\nThe equation (9) is similar to the representational based transfer model, where the T matrix is just the reconstruction matrix, because X and Z have different number of samples.\nThis paper written is a little poor, and the novelty of NBP is not clearly claimed.\nThe experimental effectiveness is weak and have no improvement compared with BT 2018 in image dataset.\nIn deep learning era, could you discuss what is the value in deep transfer learning or deep domain adapatation?", "title": "Official Blind Review #2", "rating": "1: Reject", "confidence": 4}, "Bklsscm6Yr": {"type": "review", "replyto": "BJeUs3VFPH", "review": "1.\tThis manuscript is mainly based on the previous work BT (Raab and Schleif, 2018). The novelty seems to be too limited.\n2.\tThe proposed method NBT improves the computional efficiency of BT. The Datasets used in the experiments are not representative. More experiments should be conducted to demonstrate its efficiency on large-scale Datasets, such as VisDA and Offic-Home Dataset.\n3.\tThe precise results of CGCA are not provided, which is unfair. Besides, there is no comparsion between the proposed methods and the state-of-the-art deep learning-based methods.The experimental results seems unconvincing.\n4.\tTypos:\n(1)\t\u2018Out BT approach has no free parameter\u2026\u2019 in page 14. Here \u2018Out\u2019 means \u2018Our\u2019?\n(2)\t\u2018\u2026NBT is fastest at Newsgroup und Image data set. In page 14.\u2019\n", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 2}, "HkgkZ4H6tH": {"type": "review", "replyto": "BJeUs3VFPH", "review": "In this work, the authors improve the work (Raab & Schleif, 2018) by (1) reducing the computational complexity, (2) neglecting the sample size requirement, and (3) achieving a low-rank projection through Nystrom approximation. More specifically, the feature dimensionality is reduced by using only s biggest eigenvalues and eigenvectors, and the sample size is coordinated through Nystrom approximation. Class-wise sampling is used for the source, and uniform sampling is used for the target. Experimental studies on three datasets have been done.\n\nThis paper should be rejected because (1) the paper lacks important latest references on domain adaptation, (2) the paper misuses the notations that makes the paper is not easy to follow, (3) the algorithm is not well justified either by theory or experiments, and (4) the presentation should be further polished. Here are some detailed comments:\n\n(1)\tA lot of recent deep domain adaptation methods are missing. These deep works achieve the state-of-the-art results on many transfer benchmark tasks. Without comparison with them (the paper does not mention any deep works), it is very unconvincing to conclude the paper makes new contributions to the transfer community. \n(2)\tIn line 2 of page 5, the authors claim that BT assumes S_Z ~ S_X, which is not true. The key idea of BT is to construct new source data using target basis and source eigenvalues. Similarly, the authors make the same claim in section 4.1 for NBT, which is also not valid. \n(3)\tThe notations in eq. (12) and (13) are very misleading. Eq. (12) follows the notations of the first line in page 5, but in the first line below eq. (12), why R_X \\in R^{d \\times s}, S_X^2 \\in R^{s \\times s}? I understand that X_s is the low dimensional X, then X_s = XR_s with R_s is the dimensionality reduction matrix whose size is d \\times s (using biggest s eigenvectors is fine). With this, eq. (13) is incorrect as X should be L_X*S_X*R_X^T, but not L_s*S_s*R_s^T. Moreover, it is also unclear why X is decomposed into product of two matrices in this work, is there any benefit of doing so for transfer purpose? \n(4)\tIn section 4.1, A_Z and A_X have exactly same form with X and Z, i.e., L_Z*S_Z*R_Z^T and L_X*S_X*R_X^T, please clarify. How eq. (14) X_s = \\tide{L_X}*S_X come from? Is it the same as eq. (13)? \n(5)\tThe title highlights low-rank, but it is not very clear how low-rank matters in the proposed method. I do not find contents stating the low-rank property of the proposed algorithm in the main technical sections.  \n(6)\tRegarding section 4.2, what is the benefit of using class-wise sampling for the source? Have you tried to use uniform sampling for both the source and target domains?\n(7)\tThe bound in eq.(16) is not very meaningful as s << n, m, d. Moreover, I am also not convinced by the claim this reduces the distribution differences, please give more theoretical justifications.\n(8)\tRegarding the experimental studies, why not use accuracy as many existing works do? The baselines are all subspace-based papers, and are out-of-the-date. The latest subspace papers, e.g., JGSA and MEDA, should be included. Moreover, deep methods are completely missing, which makes the empirical evaluation much less convincing. The improvements of NBT to BT are very marginal, 0.6. \n(9)\tSome typos and unclear points (please further polish the paper): \n(a)\tThe last sentence of para 2, X and Z should be data, not features/\n(b)\tPara 3, it is unclear what are the implicit alignment and explicit alignment of domain distributions.\n(c)\tThe first sentence in section 2, it should be homogeneous. \n(d)\tPage 5 above eq. (15), it should be S_Z ~ S_X.\n(e)\tSection 4.2 the second line, it should be \u201cin the data matrix\u201d\n(f)\tThe second last line of page 6, it should be inequality (16).\n(g)\tSome references miss page information.\n\n", "title": "Official Blind Review #1", "rating": "1: Reject", "confidence": 4}}}