{"paper": {"title": "Asynchronous Modeling: A Dual-phase Perspective for Long-Tailed Recognition", "authors": ["Hu Zhang", "Linchao Zhu", "Yi Yang"], "authorids": ["~Hu_Zhang1", "~Linchao_Zhu1", "~Yi_Yang4"], "summary": "", "abstract": "This work explores deep learning based classification model on real-world datasets with a long-tailed distribution. Most of previous works deal with the long-tailed classification problem by re-balancing the overall distribution within the whole dataset or directly transferring knowledge from data-rich classes to data-poor ones. In this work, we consider the gradient distortion in long-tailed classification when the gradient on data-rich classes and data-poor ones are incorporated simultaneously, i.e., shifted gradient direction towards data-rich classes as well as the enlarged variance by the gradient fluctuation on data-poor classes. Motivated by such phenomenon, we propose to disentangle the distinctive effects of data-rich and data-poor gradient and asynchronously train a model via a dual-phase learning process. The first phase only concerns the data-rich classes. In the second phase, besides the standard classification upon data-poor classes, we propose an exemplar memory bank to reserve representative examples and a memory-retentive loss via graph matching to retain the relation between two phases. The extensive experimental results on four commonly used long-tailed benchmarks including CIFAR100-LT, Places-LT, ImageNet-LT and iNaturalist 2018 highlight the excellent performance of our proposed method.", "keywords": ["long-tailed classification", "gradient distortion", "asynchronous modeling"]}, "meta": {"decision": "Reject", "comment": "The paper received mixed reviews. Reviewers were concerned about the clarity of the presentation, including both the analyses of gradients and the approach. Some reviewers also suggested improvements to the experiments. The two reviewers who gave a rating of 6 liked the gradient perspective to the long-tailed recognition, but their concerns seemed to overshadow their excitement. AC suggested authors improving the paper, especially its clarity and empirical part, following Reviewers' comments and submitting the paper elsewhere. "}, "review": {"A9lAyKvzMlt": {"type": "review", "replyto": "u846Bqhry_", "review": "**Overview:**\n\nThe paper considers the issue of gradient distortion in long-tailed recognition including shifted gradient direction towards data-rich classes and the enlarged variance introduced by data-poor classes. It proposes to disentangle the data-rich and data-poor classes and train a model via a dual-phase learning process. The experimental results proved the effectiveness of the method.\n \n**Strengths:**\n\nThe observation is interesting and the method is reasonable. The memory retentive loss via graph matching in the second phase makes sense, and the method is well evaluated in four commonly used datasets.\n \n \n**Weaknesses, questions, and suggestions:**\n\n1. One of my main concerns is that the separation of the data-rich and data-poor classes is unnatural because the long-tailed distribution is continuous. The main idea of the method is kind of similar to that of Gidaris et al.[1] as the authors also cited, but in the few-shot setting in their paper, the separation of base and novel classes is more natural and reasonable.\nThe issue of gradient distortion is addressed and is the motivation of the method, however, the variety in gradient does not necessarily lead to worse performance, and even when the data-rich classes are extracted for training in Phase I, the imbalance and shifting to the relatively rich classes still exists while being less informative because of fewer classes and less data. The trade-off is not easy to balance and could be sensitive to the disentanglement points as in Figure 3, where 294 and 864 are carefully selected for Places-LT and ImageNer-LT This introduces limitation to the generality of the method.\n \n2. Are the formations of equation (2) and (5) presented correctly? Maybe the authors intend to present \\sum_i{exp{...}} as the denominator?\n \n3. The presentation and clarity need to be improved. For example,\n\n- The caption for Figure 1 is too long and could be more concise, e.g., to use 'titles' to distinguish CIFAR100 and CIFAR100-LT; to use grad_rich / grad_poor or something as the legend and save any extra explanation. Some of the details can be embedded in the main paper, and I\u2019m not sure how are the gradient statistics calculated, layer-by-layer, or directly use the average of the whole network. It would be better to see it clarified in the paper or appendix, and sorry if I missed it.\n\n- There are double y-axis in Figure 2 and 3, but there is no legend or caption showing each plot is assigned to which axis.\n\n- What does 'extended parameters' mean in the experiment section?\n \n4. The performance improvement seems not significant compared with previous works, e.g. ImageNet-LT on ResNet-10 and iNaturalist.\n \n5. I\u2019m not sure I\u2019ve correctly comprehended the construction of the exemplar memory bank and the reason why use s(c_j + \\Delta, X_1) to search for the new entry. More explanation for Equation (4) and a clearer presentation of the algorithm (e.g. use an Algorithm module) is preferred.\n\n[1] Gidaris et al., Dynamic few-shot visual learning without forgetting, in CVPR 2018\n\n**Post-Rebuttal**\n\nAfter reading the rebuttal and other reviewers' comments, I am actually on the fence for this submission. On the one hand, it provides several interesting observations about the phase transition in long-tailed recognition, which would be valuable to the community. On the other hand, its experimental evaluation needs to be strengthened. The authors are encouraged to include more many-shot/medium-shot/few-shot analysis across the dual phases.\n\nTherefore, I upgrade my score to 6 (marginally above acceptance threshold).\n\n", "title": "Official Blind Review #1", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SYbxLlfihF0": {"type": "review", "replyto": "u846Bqhry_", "review": "# Summary\nThis paper works on long-tailed classification. The authors conducted an analysis and claimed that the difference of gradients computed on the head and tail classes plays an important role in the performance drop. The authors then proposed a two-stage approach to first train on the head classes and then train on the tail classes in an incremental learning fashion. The proposed algorithm achieved better performance than existing methods on benchmark datasets.\n\n# Strengths\n\n- The analysis of the gradients between the head and tail classes seems to be novel.\n\n- The proposed methods achieve good performance on benchmark datasets.  \n\n\n# Weaknesses\n\n1. The paper lacks a detailed description of how the gradient analysis is conducted. For example, is the gradient computed at some layers or all the layers? How is the variance computed? Is it possible that the relatively large variance is due to a smaller sample size of tail classes? Why does the gradient computed on both types of classes have a larger norm? I would suggest that the authors provided some equations. Moreover, given just Figure 1 and the difference to balanced training, it is still unclear if such a gradient difference really leads to poor long-tailed performance. An analysis of the relationship between gradients and the classification performance will make the paper stronger. \n\n2. The idea of learning the classifier with two-phase has been proposed (Kaidi Cao et al., 2019; Kang et al., 2020). The most similar ones to the paper are [A, B, C], in which the first stage only considers data-reach classes. The authors, however, failed to identify and discuss them.\n\n[A] Wang et al., Frustratingly Simple Few-Shot Object Detection, ICML 2020\n\n[B] Zhang et al., A Study on Action Detection in the Wild, arxiv 2019\n\n[C] Cui et al., Large Scale Fine-Grained Categorization and Domain-Specific Transfer Learning, CVPR 2018\n\nMemories are also used in (Liu et al., 2019) but the authors did not discuss it.\n\n\n3. Details and motivations of the proposed methods are not clear, making it very hard to understand the proposed algorithm.\n\n- It is unclear why the memory bank is needed. Many existing works, including [A, B, C], have proposed to simply train the second stage with all classes, with the subsampled class-balanced data, and shows promising results. Some of them even \"freeze\" the features but only train the classifier in the second stage.\n\n- The way the authors tackle the second-stage training is similar to incremental learning, but the authors failed to discuss those works. One example is [D].\n\n[D] Rebuffi et al., iCaRL: Incremental Classifier and Representation Learning, CVPR 2017.\n\n- The design of Eq. (2), (3), (4) are not well-described and justified. Why do we need a new state? Why do we need Delta? The computation of Delta seems wrong: c_j + c_j -z_{k+1} = 2*c_j \u2013 z_{k+1}? Moreover, what does the state mean here?\n\n- It is unclear why we need graphs. It is unclear what \u201cz\u201d stands for in Eq. (5), (6), (7). It is unclear how a_{ij} is computed.\n\n- The motivations and description of the intra-class loss in Eq. (8) is unclear.\n\n- I would suggest that the authors provide a figure for their algorithm architecture and pipelines.\n\n\n4. Experiments and analysis:\n\n- Back to my comment in 1., there is no analysis to further justify that gradient distortion is really the cause of the poor longtailed performance, and there is no analysis if the proposed algorithm resolves it. There is no analysis if graphs and memory banks are really needed. \n\n- It is unclear what Ours(Plain) refers to. What is the model without asynchronous modeling?\n\n# Minor\nI would suggest that the authors replace \"asynchronous\" with other terms. The proposed algorithm is just a two-stage algorithm. There is no component of distributed learning and communications that require synchronization or not.\n\n# Justification\nWhile the proposed algorithm achieves promising results, the algorithm is not written, well-described, and motivated. It is also unclear if the gradient distortion is really the cause of poor long-tailed performance. I thus give a score of 3.\n\n-------------------------- Post-rebuttal ----------------------------\n\nI read the authors' rebuttal and I appreciate their efforts. I would suggest that the authors incorporate those clarifications into their manuscript. I would also suggest that the authors re-motivate their paper and modify their approach section.\n\nIn terms of the discussions to related work, I do think [A, B, C] is about long-tailed recognition/detection, not few-shot learning. For instance, [A] works on LVIS, a long-tailed object detection dataset; [C] works on iNaturalist, which is clearly long-tailed. [B]'s Fig 1 clearly shows that the problem is long-tailed. I'm surprised that the authors simply said that [A, B, C,] works on different problems but did not intend to discuss the similarity in methodologies.\n\nThere are some very important questions not addressed yet, specifically, my comments 3 and 4: There is no analysis if the proposed algorithm resolves gradient distortion. There is no analysis if graphs and memory banks are really needed. \n\nI also read other reviewers' comments and I agree with R4 that the current version still lacks critical insights and some justifications are questionable.\n\nGiven these, I would keep my initial score unchanged.", "title": "This paper works on long-tailed classification. The authors conducted an analysis and claimed that the difference of gradients computed on the head and tail classes plays an important role in the performance drop. The authors then proposed a two-stage approach to first train on the head classes and then train on the tail classes in an incremental learning fashion. The proposed methods, however, are not well-motivated and described. More details and clarifications are needed.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "G3U4WXeDBM1": {"type": "review", "replyto": "u846Bqhry_", "review": "Summary:\n\n- This paper proposes a two-stage learning algorithm to address imbalanced datasets. Only data-rich classes is used in the first representation learning stage. In the second stage,  an exemplar memory bank with graph matching is used together with standard classification. Experimental results on benchmarks highlight the effectiveness of the proposed method.\n \nPros:\n- The total experiments conducted are thorough and satisfactory.\n\nCons:\n- The presentation of this paper can be improved. For example, the notation in Figure 1 is very hard to parse. The writing could be improved as well. There's a chance that I didn't understand the paper so I just list all my concerns in questions.\n- In the implementation detail section, there's only implementation details shared with the baseline. There's no detail about the proposed method, e.g., what's the choice of $\\lambda$, s, how does the authors actually do the two-stage training. what is fixed and what is learned in the second stage? \n\nAdditional Questions:\n- For Figure 1 (c) and (d), why is the L2 norm of the overall gradient larger than both grad1 and grad2?\n- What's the rationale behind the design of memory bank? To be more specific, what's the rationale behind the design of Equation (3), (4)?\n- \"In contrast to the aforementioned strategies, we approach the long-tailed recognition problem by\nanalyzing gradient distortion in long-tailed data\"  How does the proposed method get connected with this statement and differ from other two-stage training algorithms?\n- For equation (6), what's the intuition to use $a_{ji}$ to reweight each norm?\n- For $L_{intra}$, why do the authors choose hinge loss rather than cross entropy? To the best of my knowledge, Hinge loss does not work well in deep learning, esperically with large amoung of classes as the gradient can be vary sparse.\n\n----\npost-rebuttal update\n\nI appreciate the authors for the responses. Some of my concerns have been addressed, so I increased my score. However, I this the current version still lacks critical insights and some justifications are questionable.\n", "title": "Unsatisfactory writing with important missing details", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "oLBxZES0N06": {"type": "rebuttal", "replyto": "rXH7NIYDtm", "comment": "8. It is unclear why we need graphs. It is unclear what \u201cz\u201d stands for in Eq. (5), (6), (7). It is unclear how a_{ij} is computed.\n\n    **Answer:** When we transit from stage I to stage II, we expect that the model reserves the knowledge in stage I. For each input in stage II, we can obtain a feature map on the old model learned in stage I and the new feature map in stage II. Rather than solely considering the difference of these two features of each input, we also consider the connection among a set of input samples. For all input samples, a graph is constructed based on the old features and another graph is constructed by  the new features. We thus consider the relations of the two graphs, which is a strong constraint to keep the model retentive. $z$ is the feature map in two graphs and a_{ij} is the scalar computed based on the distance of one sample in the old graph and one node in new graph, and it is used to reweight the difference between nodes in the old graph and nodes in the new graph.\n\n9. The motivations and description of the intra-class loss in Eq. (8) is unclear.\n\n    **Answer:** The intra-class loss is to avoid that the new model in stage II classifies data in old classes into new classes. \n\n10. It is unclear what Ours(Plain) refers to. What is the model without asynchronous modeling?\n\n    **Answer:** Yes. Ours (Plain)  means the classes are considered together without distinguishing head and tail, that is we consider the dataset together in one stage, rather than in a two-stage process.\n", "title": "Answer (Part II)"}, "rXH7NIYDtm": {"type": "rebuttal", "replyto": "SYbxLlfihF0", "comment": "Thank you for the detailed comment. We answer your questions point-by-point as below:\n\t\n1. Is the gradient computed at some layers or all the layers?\n\n    **Answer:** The gradient is computed at some layers. In Figure 1, we show the gradient computed in the last FC layer.\n\n2. How is the variance computed? \n\n    **Answer:** Take the norm of gradient for example, each iteration will return one value. We compute the mean and variance of the norms in every n iterations. \n\n3. Is it possible that the relatively large variance is due to a smaller sample size of tail classes? \n\t\n    **Answer:** Yes, it is likely to lead to larger variance due to the limited samples in tail classes. \n\t\n4. Why does the gradient computed on both types of classes have a larger norm? I would suggest that the authors provided some equations. \n\n    **Answer:** Suppose we denote $x_1$ as the gradient computed on head classes, and $x_2$ is the gradient computed on tail classes. The gradient computed on all classes is denoted as $x$, where $x = x_1 + x_2$. The norm of $x$ is computed as $||x||^2 = ||x_1||^2 + ||x_2||^2 + 2\\langle x_1,x_2\\rangle$. The overall gradient norm is possibly larger than $||x_1||^2$ or $||x_2||^2$ when  $||x_2||^2 + 2\\langle x_1,x_2\\rangle > 0$ or $||x_1||^2 + 2\\langle x_1,x_2\\rangle > 0$.\n\n5. Moreover, given just Figure 1 and the difference to balanced training, it is still unclear if such a gradient difference really leads to poor long-tailed performance. An analysis of the relationship between gradients and the classification performance will make the paper stronger.\n\n    **Answer:** Compared to the balanced case, the gradients in long-tailed data indeed exhibit different properties. Moreover, the results in our method outperform prior methods on a range of datasets, suggesting that separate consideration of gradients of head and tail is beneficial for long-tailed recognition.\n\t\t\n6. The idea of learning the classifier with two-phase has been proposed. Discuss difference with A,B,C, D. Memories are also used in (Liu et al., 2019) but the authors did not discuss it. \n\n    **Answer:** Discussion with (Kang et al., 2020): (Kang et al., 2020) divides the feature learning and the classifier learning into two stages. Instead, we propose to separate the data into data-rich classes and data-poor classes.\n\n    Discussion with  A, B, C: A & C focus on few-shot classification or transfer learning, where the goal is to generalize well on a target dataset given a training dataset. We target long-tailed classification to improve the overall classification performance on both head and tail classes. Zhang et al. [B] proposes to transfer information from head to tail classes on action detection and trains the second stage simply by jointly fine-tuning. We introduce a memory bank and memory-retentive loss to realize the seamless connection between two data splits.\n\n     Discussion with D: D tackles the incremental learning setting, which first considers part of the data and then involves the rest data. In our case, we first consider samples in head classes then consider data in the rest. However, different from D that directly considers the incremental setting, our goal in long tail is trying to boost the performance in long tail. \n\n     Discussion with (Liu et al., 2019): For the memory, we are different from (Liu et al., 2019). (Liu et al., 2019) tries to compute the average feature in each class and strengthen the feature of new samples.\nIn our paper, we reserve a few samples in head classes and the memory is used to keep the model retentive when it meets samples in new classes. \n\n7. The design of Eq. (2), (3), (4) are not well-described and justified. Why do we need a new state? Why do we need Delta? The computation of Delta seems wrong: $c_j + c_j -z_{k+1} = 2*c_j \u2013 z_{k+1}$? Moreover, what does the state mean here?\n\n    **Answer:** Initially, $c_j$ is the average of all feature maps in class $j$, which is just an estimation of the center. While selecting new samples to the memory bank, we also use the selected samples to adjust $c_j$. State $z_{k+1}$ is the weighted average of selected samples. $\\Delta$ is the difference between $c_j$ and $z_{k+1}$, it can be reformulated as: $\\Delta = \\sum_{i}p_i(c_j - m_i)$, which means the weighted deviation of selected sample from $c_j$. To fix the deviation, we have $c_j + \\Delta$. \n", "title": "Answer (Part I)"}, "K-J4VidWm4": {"type": "rebuttal", "replyto": "G3U4WXeDBM1", "comment": "We appreciate your review and answer the questions point-by-point.\n1. In the implementation detail section, there's only implementation details shared with the baseline. There's no detail about the proposed method, e.g., what's the choice of \\lambda. how does the authors actually do the two-stage training. What is fixed and what is learned in the second stage?\n\n    **Answer:** We empirically set \\lambda to be  $\\sqrt{\\frac{num\\ old}{num\\ new}}$, where \u201cnum old\u201d indicates the number of classes in the first stage and \u201cnum new\u201d is the number of new classes in the second stage.\nAs discussed in the ablation, take Places-LT for example, we train the model on first 294 classes in stage one and then extend the classifier to involve the samples in rest 71 classes. While we learn new classes embeddings in the classifier in the second stage, the old class embeddings learned in the first stage also need to be tuned. \n\n2. For Figure 1 (c) and (d), why is the L2 norm of the overall gradient larger than both grad1 and grad2?\n\n    **Answer:** Please see our response to AnonReviewer3 Q4.\n\n3. What's the rationale behind the design of memory bank? To be more specific, what's the rationale behind the design of Equation (3), (4)?\n\n    **Answer:** When applying the memory bank, we want to keep the model retentive by reserving a few samples in old classes. For example, we only reserve 10 samples in each old class. Ideally, we want to select the 10 samples that are closest to the center of each class. However, the real center is unknown and we want to upgrade the estimated center through selected samples in previous steps. This part has been updated that Equation (3) becomes Equation (4)  and Equation (4) becomes Equation (5). Equation (4) denotes a variable computed through selected samples in previous steps and Equation (5) returns the new selected sample.\n\n4. \"In contrast to the aforementioned strategies, we approach the long-tailed recognition problem by analyzing gradient distortion in long-tailed data\" How does the proposed method get connected with this statement and differ from other two-stage training algorithms?\n\n    **Answer:** Previous two-stage methods first train the model on the whole datasets then fine-tune the model on a balanced subset or balanced sampler. In our paper, we first show that the difference of gradients computed on head classes and tail classes, then split the dataset into head classes and tail classes. We first train the model on head classes then involve the rest. When we learn the rest in the second stage, we also finetune the classifier embeddings based on the constructed exemplar bank which selects a few samples in classes in the first stage.\n\t\n5. For equation (6), what's the intuition to use to reweight each norm?\n\n    **Answer:** For one input, we not only consider the difference of its old feature map and new feature map but consider its difference between the neighbor points. Intuitively if the input is far away from its one neighbor, we will assign a smaller weight for the difference as penalty. That is $a_{ji}$ is used for.\n\n6. For $L_{intra}$, why do the authors choose hinge loss rather than cross entropy? To the best of my knowledge, Hinge loss does not work well in deep learning, especially with large amount of classes as the gradient can be vary sparse.\n\n    **Answer:** Loss $L_{intra}$ is not used for classification. It only works to avoid the case that the model classifies the samples in old classes to new classes when learning in the second stage. $L_{cls}$ is the one used for classification, which is a cross-entropy loss.\n", "title": "Answer"}, "bX4PXipXvi7": {"type": "rebuttal", "replyto": "kkH2FVkvqPk", "comment": "Thank you for the positive feedback. We address your concerns as below.\n\t\n1. Why the grad (a combination of grad1 and grad2 in my opinion) becomes larger?\n\n    **Answer:** Please see our response to AnonReviewer3 Q4.\n\t\n2. Discussion with paper \"Overcoming Classifier Imbalance for Long-tail Object Detection with Balanced Group Softmax\".\n\n    **Answer:** In their paper, they focus on long-tailed detection and try to utilize group softmax to deal with head and tail data. They still address all classes in the dataset simutinuously. Our method, however, tries to disentangle the effect between head and tail by two-stage learning and connects them through memory bank and memory-retentive loss.\n\n3. The authors conduct experiments on almost all common long-tailed classification datasets, which is great. But the improvements seem to be very limited on Places-LT, ImageNet-LT, iNaturalist18 comparing with recent works. It seems that it only works well on small datasets like CIFAR, which is somehow weak.\n\n    **Answer:** Take ImageNet-LT on ResNet-50 for example, we improve baseline 47.7 to ours 51.0. and Places-LT, we improve the baseline 37.6 to ours 40.4. Even for IEM (39.7) that introduces large extra parameters and memory data, we still outperform it.\n\n4. About the training schedule, in the Appendix.Implementation Details, it states that the number of training epochs is 200. It means that we train both 200 epochs for both phase1 and phase2 right? Though and are not overlapped, the exemplar bank will be trained twice. How much will it cost? How is the cost comparing with re-sampling approaches?\n\n    **Answer:** Yes, for each phase, it will be trained 200 epochs. For samples in the exemplar bank, we only reserve 10 samples in each old class to keep the model memory-retentive. The samples are actually used to avoid the dramatic change of model when it meets new samples. Compared to re-sampling approaches that the classifier is fine-tuned by re-sampling on the whole dataset, the introduced memory cost actually will not introduce too much overhead. \n\n", "title": "Answer"}, "nwu43feCy8": {"type": "rebuttal", "replyto": "A9lAyKvzMlt", "comment": "Thank you for recognizing that \u201cthe observation is interesting and the method is reasonable\u201d. We answer your questions below:\n\n1. One of my main concerns is that the separation of the data-rich and data-poor classes is unnatural because the long-tailed distribution is continuous. The main idea of the method is kind of similar to that of Gidaris et al.[1] as the authors also cited, but in the few-shot setting in their paper, the separation of base and novel classes is more natural and reasonable. The issue of gradient distortion is addressed and is the motivation of the method, however, the variety in gradient does not necessarily lead to worse performance, and even when the data-rich classes are extracted for training in Phase I, the imbalance and shifting to the relatively rich classes still exists while being less informative because of fewer classes and less data. The trade-off is not easy to balance and could be sensitive to the disentanglement points as in Figure 3, where 294 and 864 are carefully selected for Places-LT and ImageNer-LT This introduces limitation to the generality of the method.\n\t\n\t**Answer:** Gidaris et al.[1] considers the few-shot setting, which aims to quickly adapt to novel classes. Our separation of data-rich classes and data-poor classes is motivated by the gradient distortion phenomenon. For long-tailed classification, we not only consider the representation learning but the classification. Actually for head data, the tail data actually helps little in the representation learning since the limited samples. The classification of tail classes, however, will distract the classification of head classes. This is reflected by the gradient distortion between them. As we show in Figure 3, though the final performance is affected by the specific threshold selected, but the change is minor. The final performance actually keeps rather stable in a wide range of threshold.\n\n2. Are the formations of equation (2) and (5) presented correctly? Maybe the authors intend to present \\sum_i{exp{...}} as the denominator?\n\n\t**Answer:** Sorry for the mistake, we will update it. \n\t\n3. The presentation and clarity need to be improved. For example, the caption for Figure 1 is too long and could be more concise, e.g., to use 'titles' to distinguish CIFAR100 and CIFAR100-LT; to use grad_rich / grad_poor or something as the legend and save any extra explanation. Some of the details can be embedded in the main paper, and I\u2019m not sure how are the gradient statistics calculated, layer-by-layer, or directly use the average of the whole network. It would be better to see it clarified in the paper or appendix, and sorry if I missed it.\n\n\t**Answer:** Figure 1 will be updated. Here we simply show the gradient computed in the  FC layer.\n\n4. There are double y-axis in Figure 2 and 3, but there is no legend or caption showing each plot is assigned to which axis. \n\n\t**Answer:** We will update the figure. In the paper, we have described \u201cSimilarly, the axis for describing different shots is in the left. The change of overall result is depicted in the right of the figure, which is an independent axis\u201d. \n\n5. What does 'extended parameters' mean in the experiment section? \n\n\t**Answer:** The extended parameters here means that they introduce extra modules and incorporate more parameters. \n\n6. The performance improvement seems not significant compared with previous works, e.g. ImageNet-LT on ResNet-10 and iNaturalist.\n\n\t**Answer:** You are correct. For ImageNet-LT on ResNet-10, the model is limited in its parameters, so the improvement is not significant. For iNaturalist, considering its scale (400K image), further improvement can be very difficult.\n\n7. I\u2019m not sure I\u2019ve correctly comprehended the construction of the exemplar memory bank and the reason why use $s(c_j + \\Delta, X_1)$ to search for the new entry. More explanation for Equation (4) and a clearer presentation of the algorithm (e.g. use an Algorithm module) is preferred.\n\n\t**Answer:** When $c_j + \\Delta$ is used to update our estimated center since the real center of each class is unknown,  we use selected samples in previous steps to modify it. We then compute the sample which is closest to the new center $c_j + \\Delta$ and return the sample.\n", "title": "Answer to address the concern"}, "kkH2FVkvqPk": {"type": "review", "replyto": "u846Bqhry_", "review": "This paper proposes an interesting view to analyze the long-tailed problem. It states that the gradients are dominated by the head classes so that the tail classes perform poorly. From this observation, the authors propose a dual-phase approach that first train $W_r, W_c^1$ with only head-class data, and extend to train $W_r, W_c$ with tail-class data and the constructed exemplar memory bank for head classes with a newly proposed memory retentive loss.\n\n#### Pros:\n1. This paper is well organized and written.\n2. Analyzing through the gradient seems to be an interesting view, which could bring insights into the long-tailed problem.\n\n#### Some questions and concerns:\n1. In Fig.1 (c)(d), despite the norm variance, I don't understand why the \"norm of grad\" is larger than \"the norm of grad1\" and \"the norm of grad2\". They are all norms, right? So why the grad (a combination of grad1 and grad2 in my opinion) becomes larger?\n2. This dual-phase approach somehow seems similar to [this paper](https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Overcoming_Classifier_Imbalance_for_Long-Tail_Object_Detection_With_Balanced_Group_CVPR_2020_paper.pdf), which actually groups categories that own a similar number of training instances and train the classifier separately. And this paper divides all categories into 2 groups (head and tail), but with more complicated operations.\n3. The authors conduct experiments on almost all common long-tailed classification datasets, which is great. But the improvements seem to be very limited on Places-LT, ImageNet-LT, iNaturalist18 comparing with recent works. It seems that it only works well on small datasets like CIFAR, which is somehow weak.\n4. About the training schedule, in the Appendix.Implementation Details, it states that the number of training epochs is 200. It means that we train both 200 epochs for both phase1 and phase2 right? Though $X_1$ and $X_2$ are not overlapped, the exemplar bank will be trained twice. How much will it cost? How is the cost comparing with re-sampling approaches?", "title": "Good view, but limited improvement.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}