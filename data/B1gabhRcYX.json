{"paper": {"title": "BA-Net: Dense Bundle Adjustment Networks", "authors": ["Chengzhou Tang", "Ping Tan"], "authorids": ["cta73@sfu.ca", "pingtan@sfu.ca"], "summary": "This paper introduces a network architecture to solve the structure-from-motion (SfM) problem via feature bundle adjustment (BA)", "abstract": "This paper introduces a network architecture to solve the structure-from-motion (SfM) problem via feature-metric bundle adjustment (BA), which explicitly enforces multi-view geometry constraints in the form of feature-metric error. The whole pipeline is differentiable, so that the network can learn suitable features that make the BA problem more tractable. Furthermore, this work introduces a novel depth parameterization to recover dense per-pixel depth. The network first generates several basis depth maps according to the input image, and optimizes the final depth as a linear combination of these basis depth maps via feature-metric BA. The basis depth maps generator is also learned via end-to-end training. The whole system nicely combines domain knowledge (i.e. hard-coded multi-view geometry constraints) and deep learning (i.e. feature learning and basis depth maps learning) to address the challenging dense SfM problem. Experiments on large scale real data prove the success of the proposed method.", "keywords": ["Structure-from-Motion", "Bundle Adjustment", "Dense Depth Estimation"]}, "meta": {"decision": "Accept (Oral)", "comment": "The first reviewer summarizes the contribution well: This paper combines [a CNN that computes both a multi-scale feature pyramid and a depth prediction, which is expressed as a linear combination of \"depth bases\"]. This is used to [define a dense re-projection error over the images, akin to that of dense or semi-dense methods]. [Then, this error is optimized with respect to the camera parameters and depth linear combination coefficients using Levenberg-Marquardt (LM). By unrolling 5 iterations of LM and expressing the dampening parameter lambda as the output of a MLP, the optimization process is made differentiable, allowing back-propagation and thus learning of the networks' parameters.] \n\nStrengths:\nWhile combining deep learning methods with bundle adjustment is not new, reviewers generally agree that the particular way in which that is achieved in this paper is novel and interesting. The authors accounted for reviewer feedback during the review cycle and improved the manuscript leading to an increased rating. \n\nWeaknesses:\nWeaknesses were addressed during the rebuttal including better evaluation of their predicted lambda and comparison with CodeSLAM.\n\nContention:\nThis paper was not particularly contentious, there was a score upgrade due to the efforts of the authors during the rebuttal period.\n\nConsensus:\nThis paper addresses an interesting  area of research at the intersection of geometric computer vision and deep learning and should be of considerable interest to many within the ICLR community. The discussion of the paper highlighted some important nuances of terminology regarding the characterization of different methods. This paper was also rated the highest in my batch. As such, I recommend this paper for an oral presentation. "}, "review": {"r1x8O_Sw3X": {"type": "review", "replyto": "B1gabhRcYX", "review": "edit: the authors added several experiments (better evaluation of the predicted lambda, comparison with CodeSLAM), which address my concerns. I think the paper is much more convincing now. I am happy to increase my rating to clear accept.\n\nI also agree with the introduction of the Chi vector, and with the use of the term of \"photometric BA\", since it was used before, even if it is unfortunate in my opinion. I thank the authors to replace reprojection by alignment, which is much clearer.\n\n---------------\n\n\nThis paper presents a method for dense Structure-from-Motion using Deep Learning:\nThe input is a set of images; the output is the camera poses and the depth maps for all the images.\nThe approach is inspired by Levenberg-Marquardt optimization (LM): A pipeline extracting image features computes the Jacobian of an error function. This Jacobian is used to update an estimate of the camera poses. As in LM optimization, this update is done based on a factor lambda, weighting a gradient descent step and a Gauss-Newton step. In LM optimization, this lambda evolves with the improvement of the estimate. Here lambda is also predicted using a network based on the feature difference.\n\nIf I understand correctly, what is learned is how to compute image features that provide good updates, how to predict the depth maps from the features, and how to predict lambda.\n\nThe method is compared against DeMoN and other baselines with good results.\n\nI like the fact that the method is based on LM optimization, which is the standard method in 'geometric bundle adjustment', while related works consider Gauss-Newton-like optimization steps. The key was to include a network to predict lambda as well.\n\nHowever, I have several concerns:\n\n* the ablation study designed to compare with a Gauss-Newton-like approach does not seem correct. The image features learned with the proposed method are re-used in an approach using a fixed lambda. If I understand correctly, there are 2 things wrong with that:\n- for GN optimization, lambda should be set to 0 - not a constant value. Several constant values should also have been tried.\n- the image features should be re-trained for the GN framework:  Since the features are learned for the LM iteration, they are adapted to the use of the predicted lambda, but they are not necessarily suitable to GN optimization.\nThus, the advantage of using a LM optimization scheme is not very convincing.\n\nSince the LM-like approach is the main contribution, and the reported experiments do not show an advantage over GN-like approaches (already taken by previous work), this is my main reason for proposing rejection.\n\n* CodeSLAM (best paper at CVPR'18) is referenced but there is no comparison with it, while a comparison on the EuRoC dataset should be possible.\n\nLess critical concerns that still should be taken into account if the paper is accepted:\n\n- the state vector Chi is not defined for the proposed method, only for the standard bundle adjustment approach. If I understand correctly is made of the camera poses.\n\n- the name 'Bundle Adjustment' is actually not adapted to the proposed method.  'Bundle Adjustment' in 'geometric computer vision' comes from the optimization of several rays to intersect at the same 3D point, which is done by minimizing the reprojection errors. Here the objective function is based on image feature differences. I thus find the name misleading. The end of Section 3 also encourages the reader to think that the proposed method is based on the reprojection error. The proposed method is more about dense alignment for multiple images.\n\n\nMore minor points:\n\n1st paragraph:  Marquet -> Marquardt\ntitle of Section 3: revisitED\n1st paragraph of Section 3: audience -> reader\ncaption of Fig 1: extractS\nEq (2) cannot have Delta Chi on the two sides. Typically, the left side should be \\hat{\\Delta \\Chi}\nbefore Eq (3): the 'photometric ..' -> a 'photometric ..'\n1st paragraph of Section 4.3: difficulties -> reason\ntypo in absolute in caption of Fig 4\nEq (6): Is B the same for all scenes?  It would be interesting to visualize it.\nSection 4.5: applies -> apply\n", "title": "dense SfM with Deep Learning", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SylPHRPDnQ": {"type": "review", "replyto": "B1gabhRcYX", "review": "I believe that the authors have a solid contribution that can be interesting for the ICLR community.\nTherefore, I recommend to accept the paper but after revision because the presentation and explanation of the ideas contain multiple typos and lacking some details (see bellow). \n\nSummary:\nThe authors propose a new method called BA-Net to solve the SfM problem by explicitly incorporating geometry priors into a machine learning task. The authors focus on the Bundle Adjustment process. \n\nGiven several successive frames of a video sequence (2 frames but can be extended up to 5), BA-Net jointly estimates the depth of the first frame and the relative camera motion (between the first frame and the next one).\nThe method is based on a convolutional neural network which extracts the features of the different pyramid levels of the two images and in parallel computes the depth map of the first frame. The proposed network is based on the DRN-54 (Yu et al., 2017) as a feature extractor. \n\nThis is complemented by the linear combination of depth bases obtained from the first image.\nThe features and the initial depth then passed to the optimization layer called BA-layer where the feature re-projection error is minimized by the modified LM algorithm. \n\nThe authors adapt the standard multi-view geometry constraints by a new concept of feature re-projection error in the BA framework (BA-layer) which they made differentiable. \nDifferentiable optimization of camera motion and image depth via LM algorithm is now possible and can be used in various other DL architectures (ex. MVS-Net can probably benefit from BA-layer).\n\nThe authors also propose a novel depth parametrization in the form of linear combination of depth bases which reduces the number of parameters for the learning task, \nenables integration into the same backbone net as used or feature pyramids and makes it possible to jointly train the depth generator and the BA-layer. \n\nOriginally the proposed approach depicts the network operating in the two-view settings. The extensibility to more views is also possible and, as shown by authors, proved to improve performance. It is, however, limited by the GPU capacity. \n\nOverall, the authors came up with an interesting approach to the standard BA problem. They have managed to inject the multi-view geometry priors and BA into the DL architecture. \n\nMajor comments regarding the paper:\n\nIt would be interesting to know the evaluation times for the BA-net and more importantly to have some implementation details to ensure reproducibility.\n\nMinor comments regarding the paper:\n\n-\tThe spacing between sections is not consistent. \n-\tFigures 1 is way too abstract given the complicated set-up of the proposed architecture. It would be nice to see more details on the subnet for depth estimator and output of the net. \nOverall it would be helpful for reproducibility if authors can visualize all the layers of all the different parts of the network as it is commonly done in the DL papers.\n-\tTalking about proposed formulation of BA use either of the following and be consistent across the paper:\nFeaturemetric BA / Feature-metric BA / Featuremetric BA / \u2018Feature-metric BA\u2019\n-\tTalking about depth parametrization use \u2018basis\u2019 or \u2018bases\u2019 not both and clearly defined the meaning of this important notion.\n-\tAttention should be given to the notation in formulas (3) and (4). The projection function there is no longer accepts a 3D point parametrized by 3 variables. Instead only depth is provided. \nIn addition, the subindex \u20181\u2019 of the point \u2018q\u2019 is not explained. \n-\tMore attention should be given to the evaluation section. Specifically to the tables (1 and 2) with quantitative results showing the comparison to other methods. \nIt is not clear how the depth error is measured and it would be nicer to have the other errors explained exactly as they referred in the tables (e.g. ATE?).\n-\tHow the first camera pose is initialized?\n-\tIn Figure 2.b I\u2019m surprised by the difference obtained in the feature maps for images which seems very similar (only the lighting seems to be different). Is it three consecutive frames?\n-\tAttention should be given to the grammar, formatting in particular the bibliography. \n\n\n\n", "title": "An interesting work but lacking some details concerning the implementation and experimentations", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1ljP2vqAQ": {"type": "rebuttal", "replyto": "SJx-VMJcnm", "comment": "\nWe thank the reviewer for the comments and appreciation, and would like to answer the reviewer\u2019s questions as follows:\n\nQ1. The use of the word \u201cguarantees\u201d is imprecise:\nThanks for pointing out this. We have adjusted the word. A theoretical analysis will be an interesting future work.\n\nQ2. Whole sequence reconstruction results:\nOur current implementation only allows up to 5 images in a single 2015 TITANX GPU with 12GB memories. This is because we implemented the whole pipeline using tensorflow in python, which is memory inefficient, especially during training. Each image takes about 2.3GB memory on average, and most of the memory is consumed by the CNN features and matrix operation. But it is straightforward to concatenate multiple 5-frame segments to reconstruct a complete sequence, which is demonstrated in the comparison with CodeSLAM in Figure 7 of the revised version.  It is also straightforward to implement our BA-Layer in CUDA directly to reduce the memory consumption of matrix operation and push the number of frames.\n", "title": "Response to Reviewer 1"}, "r1xqEgFcCX": {"type": "rebuttal", "replyto": "r1x8O_Sw3X", "comment": "We thank the reviewer for raising the score.\n\nWe submitted the response and the revision until the last minute because a lot of extra works have been done for the revision, and we want to ensure the correctness and completeness.\n\nBut we will have a better-planned schedule for the next ICLR to fit the purpose of openreview. \n\n\n\n", "title": "We thank the reviewer for raising the score."}, "H1gAMXd90X": {"type": "rebuttal", "replyto": "r1x8O_Sw3X", "comment": "We thank the reviewer for the comments and appreciate that the reviewer likes our idea of including optimization in the network. But our contribution is beyond adopting Levenberg-Marquardt instead of Gauss-Newton. We would like to clarify several things to address the reviewer's concerns:\n\nQ1. The advantages of Levenberg-Marquardt over Gauss-Newton is unclear (the main reason for rejection):\n\nFirstly, we want to clarify that our contribution is beyond improving the Gauss-Newton optimization to Levenberg-Marquardt. More importantly, our contribution is the combination of conventional multi-view geometry (i.e. joint optimization of depth and camera poses) and end-to-end deep learning (I.e. depth basis generator learning and feature learning). This contribution is achieved by our differentiable LM optimization that allows end-to-end training. \n\nSecondly, we agree with the reviewer that comparing with the Gauss-Newton algorithm will be interesting and have updated such a comparison in Appendix B in the revised version according to the reviewer\u2019s suggestions: \n\n    1. We retrained the whole pipeline with Gauss-Newton, to make sure the features are learned specifically for Gauss-Newton.\n\n    2. We compared with various constant lambda values to see how the performance varies along with lambda. Note that we also fine-tune the network to make sure the features fit different lambda.   \n\nIn Table 4 of the revised version (Appendix B), our method outperforms the Gauss-Newton algorithm in the last column. This is because the objective function to be optimized is non-convex, and the vanilla Gauss-Newton method might get stuck at saddle point or local minimum. This is why the Levenberg-Marquardt algorithm is the standard choice for conventional bundle adjustment.\n\nIn Figure 6 of the revised version (Appendix B), our method also consistently performs better than different constant lambda values. This is because the value of lambda should be adapted to different data and optimization iterations. There is no \u2018optimal\u2019 constant lambda for all data and iterations.\n\n\nQ2.  Comparison with CodeSLAM:\nWe have included that in Figure 7 of the revised version (Appendix E). Since there is no public code for CodeSLAM, we cite its results directly from the CodeSLAM paper.\n\nQ3. The state vector Chi is not defined for the proposed method.\nThe Chi is defined in Section 3 as the vector containing all camera poses and point depths. Since our method also solves for these unknowns as in classic methods, we did not redefine the Chi. But in the revised version we have recapped the definition of Chi when introducing our method at the beginning of Section 4.\n\nQ4. Should the paper be called Bundle Adjustment?:\nThe term \u2018Bundle Adjustment\u2019 is originally used to refer to the joint optimization of 3D scene points and camera poses by minimizing the reprojection error. The keyword Bundle comes from the fact that a bundle of camera view rays pass through each of the 3D scene points. Multiple recent works, e.g. [Engel et al., 2017,Delaunoy and Pollefeys, 2014], have generalized it to \u201cphotometric BA\u201d where scene points and camera poses are optimized together by minimizing the photometric error. Our method is along this line. But we further improve the photometric error to featuremetric error. Each 3D scene point is still constrained by a bundle of camera view rays, though the error function has been changed. So we believe it is justified to call this method feature-metric BA. \n\nBut we agree with the reviewer that the word \u2018reprojection\u2019 is misleading when we introduce our feature-metric BA and the photometric BA. So we use the word \u2018align\u2019 as the reviewer suggested and use \u2018reprojection\u2019 only for the geometric BA.\n\nQ5. Is B the same for all scenes?:\nIn the revised version, We added Figure 8 to visualize of the term B in Equation 7 (Page 6) for different scenes. We can clearly see that it is scene dependent. \n\nQ6.Typos:\nWe have fixed all the typos as suggested in the revised version.\n", "title": "Response to Reviewer 3"}, "rkxhFe_qAm": {"type": "rebuttal", "replyto": "SylPHRPDnQ", "comment": "\nWe thank the reviewer for the comments. We have revised the paper according to the suggestions and would like to clarify several things:\n\nQ1. Evaluation Time: \nWe have added the detailed running time for each component in Table 3 in Appendix A of the revised version.\n\nQ2. Implementation Details: \nWe will share all the source code to make sure it is reproducible. Meanwhile, we have included more details as suggested in Appendix A, including a visualization of all layers of the different parts of the network. If 1-2 extra pages are allowed, we can include those details to the paper.\n\nQ3. Figure 1 is too abstract:\nWe have updated the figure to make it more intuitive and contains more details.\n\nQ4. The top row of Figure 2b is confusing:\nWe apologize for the confusion caused. Shown at the top row of Figure 2b are not three consecutive frames. They are the R, G, B channels of a single frame. To avoid confusing, we use different colors for them and explained that in the figure.\n\nQ5. How the first camera pose is initialized?:\nAll the camera pose including the first camera are initialized with identity rotation and zero translation, which are aligned with the coordinate system of the first camera. We clarified this at the end of Section 4.3 in the revised version.\n\nQ6. Evaluation metrics are not clear:\nTo facilitate comparisons with other methods, we use the evaluation metrics in previous works in Table 1 and 2, so that we can cite the results of previous methods. As we described in the paper, the depth metric are the same as Eigen and Fergus (2015). The translation metrics(ATE) are the same as [Wang et al. 2018, Zhou et al. 2017]. In the revised version, we briefly introduce the definition of these metrics at the beginning of each paragraph in Section 5.2.\n\nQ7. Attention should be given to the notation in formulas (3) and (4):\nWe changed the parameters from \u2018d\u2019 to \u2018d \\cdot p\u2019 which is a 3D point. We also removed the redundant subindex \u20181\u2019, because all points \u2018q\u2019 are on the first frame.\n      \nQ8. Terminology consistency through the paper:\nThanks for the suggestion. We consistently use the term \u201cfeature-metric BA\u201d and \u201cbasis depth maps\u201d through the paper now.\n\nQ9. Typos, Grammar, Format, and Bibliography:\nThanks for pointing them out. We have revised the paper to fix these problems.", "title": "Response to Reviewer 2"}, "HkeWLII90Q": {"type": "rebuttal", "replyto": "B1gabhRcYX", "comment": "We thank all the reviewers for their insightful comments. We have revised the paper as suggested by the reviewers, and summarize the major changes as follows:\n\n* Network architecture details and evaluation time required by Reviewer2 are added as Appendix A.\n\n* The Figure 1. is updated  to include more details as required by Reviewer2.\n\n*Ablation studies comparisons with Gauss-Newton and different constant lambda value required by Reviewer3  are updated in Appendix B.\n\n*Comparison with CodeSLAM on EuroC required by Reviewer3 are updated in Appendix E.\n\nWe also would like to ask for the reviewers\u2019 suggestions if it is allowed to have one more extra page to include more details and comparisons, and make the paper more informative to ensure reproducibility.  We targeted at 8 pages in the initial submission, but according to the reviewers\u2019 comments, it will be helpful to have more details in the main text \n\nThe other concerns raised by the reviewers have also been addressed individually.", "title": "Response to all reviewers"}, "SJx-VMJcnm": {"type": "review", "replyto": "B1gabhRcYX", "review": "This paper presents a novel approach to bundle adjustment, where traditional geometric optimization is paired with deep learning.\nSpecifically, a CNN computes both a multi-scale feature pyramid and a depth prediction, expressed as a linear combination of \"depth bases\".\nThese values are used to define a dense re-projection error over the images, akin to that of dense or semi-dense methods.\nThen, this error is optimized with respect to the camera parameters and depth linear combination coefficients using Levenberg-Marquardt (LM).\nBy unrolling 5 iterations of LM and expressing the dampening parameter lambda as the output of a MLP, the optimization process is made differentiable, allowing back-propagation and thus learning of the networks' parameters.\n\nThe paper is clear, well organized, well written and easy to follow.\nEven if the idea of joining BA / SfM and deep learning is not new, the authors propose an interesting novel formulation.\nIn particular, being able to train the CNN with a supervision signal coming directly from the same geometric optimization process that will be used at test time allows it to produce features that  will make the optimization smoother and the convergence easier.\nThe experiments are quite convincing and seem to clearly support the efficacy of the proposed method.\n\nI don't really have any major criticism, but I would like to hear the authors' opinions on the following two points:\n\n1) In page 5, the authors write \"learns to predict a better damping factor lambda, which gaurantees that the optimziation will converged to a better solution within limited iterations\".\nI don't really understand how learning lambda would _guarantee_ that the optimization will converge to a better solution.\nThe word \"guarantee\" usually implies that the effect can be somehow mathematically proved, which is not done in the paper.\n\n2) As far as I can understand, once the networks are learned, possibly on pairs of images due to GPU memory limitations, the proposed approach can be easily applied to sets of images of any size, as the features and depth predictions can be pre-computed and stored in main system memory.\nGiven this, I wonder why all experiments are conducted on sets of two to five images, even for Kitti where standard evaluation protocols would demand predicting entire sequences.", "title": "Very well written paper on an important subject, with clear technical contribution and convincing results", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}