{"paper": {"title": "Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data", "authors": ["Puyudi Yang", "Jianbo Chen", "Cho-Jui Hsieh", "Jane-Ling Wang", "Michael I. Jordan"], "authorids": ["pydyang@ucdavis.edu", "jianbochen@berkeley.edu", "chohsieh@ucdavis.edu", "janelwang@ucdavis.edu", "jordan@cs.berkeley.edu"], "summary": "We develop two methods for generating adversarial examples on discrete data under a probabilistic framework.", "abstract": "We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.", "keywords": ["Adversarial Examples"]}, "meta": {"decision": "Reject", "comment": "I appreciate the willingness of the authors to engage in vigorous discussion about their paper. Although several reviewers support accepting this submission, I do not find their arguments for acceptance convincing. The paper considers automated methods for finding errors in text classification models. I believe it is valuable to study the errors our models make in order to understand when they work well and how to improve them. Crucially, in the later case, we should demonstrate how to use the errors we find to close the loop and create better models.\n\nA paper about techniques to find errors for text models should make a sufficiently large contribution to be accepted. I view the following hypothetical contributions as the most salient in this specific case thus my decision reduces to determining if any of these conditions have been met. A paper need not achieve all of these things, any one of them would suffice:\n\n1. Show that the errors found can be used to meaningfully improve the models. \n\nThis requires building a better model than the one probed by the method and convincingly demonstrating that it is superior in an important way that is relevant to the original goals of the application. Ideally it would also consider alternative, simpler ways to improve the models (e.g. making them larger).\n\n2. Show that errors are difficult to find, but that the proposed method is nonetheless capable of finding errors and that the method is non-obvious to a researcher in the field.\n\nThis is not applicable here because errors are extremely easy to find on the test set and from labeling more data. If we demand an automated method, then the greedy algorithm does not qualify as sufficiently non-obvious and it seems to work fine, making the Gumbel method unnecessary.\n\n3. Show that the particular specific errors found are qualitatively different from other errors in their implications and that they provide a unique and important insight.\n\nI do not believe this submission attempts to show this type of contribution. One example of this type of paper would be a paper that does a comparative study of the errors that different models make and finds something interesting (potentially yielding a path to improved models).\n\n4. Generate a new, more difficult/interesting, dataset by finding errors of one or more trained models\n\nGiven that the authors use human labelers to validate examples this is potentially another path. Here is an example of a paper using adversarial techniques in this way: https://arxiv.org/abs/1808.05326\nHowever, I believe the paper would need to be rethought and rewritten to make this sort of contribution.\n\n\nUltimately, the authors and reviews supporting acceptance must explain the contribution succinctly and convincingly. The reviewers most strongly advocating for accepting this submission seem to be saying that there is a valuable new method and probabilistic framework proposed here for finding model errors. I believe researchers in the field could have easily come up with the greedy algorithm (a standard approach to discrete optimization problems) proposed here without needing to read the paper. Furthermore, I believe the other more complicated Gumbel algorithm proposed is not necessary given the similarly effective and simpler greedy algorithm. If the authors believe that the Gumbel algorithm provides application-relevant advantages over the greedy algorithm, then they should specify how these errors will be used and rewrite the paper to make the greedy algorithm a baseline. However, I do not believe the experimental results support this idea.\n"}, "review": {"rJlr2RR-xV": {"type": "rebuttal", "replyto": "ByghKiC5YX", "comment": "4. In terms of security, finding the error is a way to attack the model.\n\nFrom the security perspective, the ability of finding nearby errors (adversarial examples) leads to many security threats. Several important applications including attacking self-driving cars (slightly perturbed traffic sign can fool self-driving cars), and malicious ads (small change to ads pictures can pass the ML-based blocking or ranking models, see a nice new paper at https://arxiv.org/abs/1811.03194). \n\nWe believe this motivation is still valid for text data. For instance, if an attacker has a spam email to send out but the original email cannot bypass an ML-based spam filter, he or she can slightly perturb the text to bypass spam filter. The same thing can be done in many other online applications.  \n\nOr, for instance, A sends B a message or a document, and an attacker between A, B can change a small number of bits in the message to make the ML model in B mis-classify. Number of bits could correspond to the edit distance in text data, which is one motivation for using edit distance. \n\n5. Is \u201cedit distance\u201d the best distance measurement for text adversarial example? \n\nWe think edit distance is a natural way as a distance measurement for text, but of course it might not be perfect. In general, even in image adversarial examples, there\u2019s debate on which norm should be used.  FGSM ( https://arxiv.org/pdf/1412.6572.pdf) aims to control the L_\\infty norm perturbation; C&W works for different L_p norm (https://arxiv.org/abs/1608.04644); recent papers also proposed more complicated norms such as L1-L2 norm (https://arxiv.org/abs/1709.04114), one pixel change (https://arxiv.org/abs/1710.08864), rotate/shift (https://arxiv.org/pdf/1712.02779.pdf) and semantic similarity (https://arxiv.org/pdf/1804.00499.pdf). We agree that exploiting different similarity measurement will be important, but in this paper we just focus on the most intuitive distance measurement in text and develop algorithms to find minimal adversarial perturbation. Also, we conduct human evaluation to show that minimizing edit distance works to some extent to achieve the goal that \u201cthe semantic meaning of the sentence is not changed\u201d. ", "title": "Why is the topic (finding nearby errors) important? (Part 2) "}, "HJgu9RRWxE": {"type": "rebuttal", "replyto": "ByghKiC5YX", "comment": "1. Finding the error is the first step toward fixing it.\n\nIn general, given a sample x with correct prediction, the model is robust if it outputs the same correct label for all the nearby points *within a small distance*. The \u201cdefense\u201d algorithms are proposed to improve the robustness of models, but before doing defense, it\u2019s necessary to know *how to evaluate the robustness of a model*. \n\n**There is no way to improve robustness if you don\u2019t know how to measure it.**\n\nTherefore, to evaluate the robustness of models, we need to *find nearby errors* for a given x. The AC might think doing random perturbation will work, but based on the experiments, simple random perturbation does not work for text (see our explanations in the \u201cMotivation\u201d threads) and also doesn\u2019t work for image applications. Therefore, finding nearby errors is the first step before fixing it, so this has become an important task in our community. See the list of attacks for text data in our \u201cMotivation\u201d thread, and there\u2019s much longer list of work on computer vision applications. \n\nMoreover, recently researchers also found that *the error with small perturbation can be used to improve robustness*. The strategy is called \u201cadversarial training\u201d: When training the model, we can keep finding adversarial examples and adding them into training data to train the model. Random perturbation again doesn\u2019t work well in this case, so the state-of-the-art method now works like 1) finds nearby adversarial samples based on current batch 2) run SGD on adversarial samples. See a seminal work (https://arxiv.org/abs/1412.6572) and one of the state-of-the-art methods (https://arxiv.org/abs/1706.06083). This is another reason that we want to find adversarial examples (nearby errors). \n\n2. It\u2019s not surprising that such nearby error exists. The question is how to find it. \n\nWe totally agree that it\u2019s not surprising that such nearby error exists, and it\u2019s also not our focus to show such nearby error exists. What we are doing in this paper is to propose an efficient way to find such error, which can be used to measure the robustness and identify the blind spots of the model (see our point 1).\n\n3. Attacks can often be reduced to an optimization problem. Does this mean attacks are trivial? \n\nAs the AC pointed out and we also agreed, finding adversarial example (based on edit distance) for text classification can be formulated as a discrete optimization problem. Actually this is the case for most attacks. To attack a machine learning model, we want to \n\nfind x\u2019 \\in Ball(x, \\epsilon) to *maximize* Loss(f(x\u2019), y)\n\nAnd this is naturally a constrained optimization problem. For image classification, x\u2019 is in the continuous and bounded space. The seminal paper (https://arxiv.org/pdf/1312.6199.pdf) proposed to solve this by LBFGS. State-of-the-art  C&W (https://arxiv.org/abs/1608.04644) attack is based on the similar formulation with different loss functions. Given this, there are still tons of papers proposing different attacks in ICLR, in both black-box and white-box settings. \n\nThe AC pointed out that \u201cSince a standard greedy algorithm works, there can't be anything special about this particular optimization problem that standard methods can't handle.\u201d So does this imply there\u2019s no contribution to apply an existing optimization algorithm to solve an ML problem? We disagree. \n\nIn general, we think \u201cshowing that an optimization algorithm works well for an ML problem\u201d itself is an important contribution. There has been many works trying to apply existing optimization algorithms to ML models, such as SVM optimization, graphical models, sparse recovery, low-rank recovery, and these work have led to faster training/inference and many easy-to-use ML packages that were really beneficial to the community. Furthermore, when people apply some existing algorithm to some ML problem, they usually need to slightly change the algorithm to exploit the structure of problem, which is very important in practice. \n\nWe can see the same trend in the research of adversarial attacks, in both white-box setting (LBFGS is used initially, and then gradient descent, and then Adam), and black-box setting (coordinate descent is used initially, and then NES, genetic algorithm, etc). In our case, the algorithm is based on greedy optimization but has some treatments to make it adapt to text attack, see the discussion thread \u201cEfficiency of Gumbel Attack; Difference and Connections in Discrete Optimization and Adversarial Attack.\u201d Furthermore we show how to attack efficiently using the Gumbel trick, which is also an interesting finding. \n\n", "title": "Why is the topic (finding nearby errors) important? (Part 1)"}, "r1gzkf3Zx4": {"type": "rebuttal", "replyto": "Ske46vsZeE", "comment": "Wow! Thanks for proposing these interesting experiments! I personally agree with you that they are worth to investigate. Actually your thoughts point to some directions we are thinking about. I'd like to share with you some more of my personal thoughts, not necessarily related to the paper. \n\nYou mentioned \"papers which identify the minimum perturbation without much other contribution\" may not be interesting. I agree with you. Experiments also need to be carried out to see whether humans can make the right decision after this, how to do this efficiently in the setting of adversarial examples, etc. \n\nAs AC said: \" this is not surprising in the discrete case because the models considered certainly have a non-trivial amount of test error in the data distribution.\" Yeah, that's truth! But does it suffice to know the test error exists? No, it is not. We need to investigate what the error is, or 'what is the inherent bias' as you mentioned, more importantly, 'how to characterize that bias?' Finally, 'how to fix that?'\n\nFundamentally, the first thing we care about is: \"How do we define the bias of a model?\" I think maybe we can summarize in an inaccurate language: if most humans think the label of an instance is A, but the label is B, then the model has a bias. After that, we can proceed to find out \"what is a precise way to summarize the bias of existing models\". Maybe we can use math language, maybe we can summarize with other domain-specific abstraction (e.g.: texture). In the end, equipped with the knowledge, we proceed to fix the models. Perhaps one can use adversarial training? Or add some simple rules? So many potentially interesting directions there waiting for us!\n\nOne more thing, although you pointed me to a nice paper, I feel sad about the cat in that paper who was enforced to wear the elephant skin:)\n\n\n", "title": "Your proposed experiments are interesting!"}, "HkxPdeCh14": {"type": "rebuttal", "replyto": "r1lwJY3tyE", "comment": "We express our sincere thanks to Reviewer 3 for the support. We think adversarial examples on texts are interesting to study, as is explained by Reviewer 3, as well as for the reasons we explained in previous posts.\n\nOn the other hand, we also understand it is natural for different people to be excited about different areas, and to feel certain pieces of work less interesting. We still appreciate Reviewer 4 for reading our rebuttal.", "title": "We agree to disagree."}, "Syg18fj11E": {"type": "rebuttal", "replyto": "HyeQu_ns07", "comment": "We thank the reviewer for reading our paper and giving detailed comments on our paper. However, we observe the review is posted after the deadline of paper modification has passed, and hope to address a few points of this review. \n\nIn short,\n1. We think the comparison between adversarial attack on images and on texts is unfair.\n2. Some of the questions in the review are highly correlated with AC\u2019s questions and we have answered in our previous rebuttal. \n\nWe now address them in details. \n\nFirst, we agree with the reviewer that adversarial attack on texts is at a relatively new stage compared to the counterpart on images. However, to evaluate our work, we think it makes more sense to compare our methods with the best methods in this area, instead of with methods on a different data set. As an example, it does not make sense to reject a paper because their model achieves a lower accuracy on ImageNet than the accuracy of a very simple model on MNIST. \nWe have shown that our method outperforms previous text adversarial attack algorithms in Figure 3, and we have even compared all methods under human evaluation in Appendix B, which indicates that humans are least sensitive to adversarial examples generated by our algorithm. So we believe our method can advance state-of-the-art in attacks on texts. \n\n\u201cThe attacks on character-based models are closer to adversarial examples from this perspective.\u201d\nWe aim to propose a general mathematical framework to generate adversarial examples for models with discrete input. Thus, the same algorithm works for both character-based and word-based models, and could be potentially useful for other NLP models such as word-piece models. We are happy to see that the reviewer consider character-based adversarial examples more interesting.\n\n\u201cThe Greedy attack is a straightforward application of greedy optimization on discrete data and is not very novel or interesting.\u201d\nSee our rebuttal to AC (point 2) in  \u201cEfficiency of Gumbel Attack; Difference and Connections in Discrete Optimization and Adversarial Attack.\u201d We will also elaborate this in our final version.\n\nThe reviewer also proposed to include an experiment on how our attacks perform on models trained with data augmentation techniques. We agree with the reviewer that the proposed experiment can be interesting. However, given the timeline, we are not able to update the paper now. We are willing to add it in our final version.", "title": "Response to Reviewer 4"}, "HyeQu_ns07": {"type": "review", "replyto": "ByghKiC5YX", "review": "This paper introduces two new methods for generating adversarial examples for text classification models. The paper is well written, the introduced algorithms and experiments are easy to understand. \n\nHowever, I do not believe that these two methods are sufficiently significant. First of all, I am not convinced that the attacks can be classified as \u201cadversarial examples\u201d, especially the ones on the word-based models. The community originally got interested in adversarial examples because while they can easily be classified correctly by humans, they seemed to fool machine learning models with high efficiency. For example, the PGD attack by Madry et al. can reduce the accuracy of a CIFAR-10 model to 0% by using distortions that are not at all noticeable to humans. In the case of the word-based task studied here, human accuracy drops by 8-11%. \n\nWhile the question of whether adversarial examples are actually a security threat is under debate, the attacks on the word-based models here do not even classify as adversarial examples. Of course, it is interesting that the ML models are much less robust to these distortions than humans are, however, this is a well known problem. This paper did not perform comprehensive experiments to investigate this phenomenon. For example, they could have evaluated a wide range of distortions (including random distortions), and then check if training with all of these distortions makes the network more robust \u2026 etc (for example, see [1]).  \n\nThe attacks on character-based models are closer to adversarial examples from this perspective. However, the performance of the Gumbel Attack is significantly worse on character-based models than an attack as simple as the Delete-1 attack. The Greedy attack is more successful than the Delete-1 attack, however it is a straight-forward application of greedy optimization on discrete data and is not very novel or interesting. \n\n[1] Generalisation in humans and deep neural networks, arXiv:1808.08750 ", "title": "Review", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkgBBnMApX": {"type": "rebuttal", "replyto": "H1e_TlAhTQ", "comment": "We thank the area chair for reading the reviews and our rebuttals carefully. We will answer the questions of Area Chair from the authors\u2019 perspective. \n\nThe area chair proposes two questions:\n1) Why do we need Gumbel as a new discrete optimization algorithm?\n2) Have we improved \u201cthe state-of-the-art\u201d in discrete optimization?\n\nThe short reply is\n1)  Gumbel attack is efficient. The efficiency can be a practical concern in the setting of adversarial attack.\n2) We propose better algorithms in terms of accuracy or efficiency in the regime of adversarial attack. This regime is not exactly the same as discrete optimization. \n\nWe address the details below. \n1)  \n1.1 Gumbel attack is efficient both in terms of the number of model evaluations and in terms of real time. First, no model evaluation is required during the attack stage. Also, Figure 4 in the manuscript provides a comparison of real-time efficiency, which shows Gumbel attack is orders-of-magnitude faster. (Gumbel attack is around 10^-2 seconds per sample while FGSM, Delete-1 Score and other methods are between 10^-1s and 1s per sample on Yahoo! Answers.)\n1.2 In practice, attackers may not be able to conduct many model evaluations to attack a real system.\n1.3 It may also help design more efficient adversarial training algorithms. \n\n2)\n2.1 We first address the difference between Greedy attack and standard greedy methods. \nThe most standard greedy methods choose the first perturbation by evaluating models d * V times, where d is the length of the sentence/paragraph and V is the size of dictionary, and choose the next perturbation with complexity (d-1) *V, etc. Greedy attack follows a two-stage procedure motivated from a probabilistic framework, and takes O(d + k*V) evaluations in total (k being the number of perturbations). Moreover, Greedy attack is easier to parallelize. Given the efficiency concern of adversarial attacks, it can be more practical. \n2.2 The area of adversarial attack is not exactly the same as discrete optimization. \nWe formulate the problem of adversarial attack as a constrained discrete optimization problem. The true constraint here is that \u201chumans will not change their decisions\u201d, which we approximate by constraining the number of perturbed words.  Experiments involving human subjects have been carried out to validate the effectiveness of approximation. \n2.3 We only show the superior performance of our algorithms to algorithms in adversarial attack ([1-4]), and we do not have the intention to claim it achieves the state-of-the-art in discrete optimization. \n\n[1] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial text sequences to evade deep learning classifiers. arXiv preprint arXiv:1801.04354, 2018.\n[2] Jiwei Li, Will Monroe, and Dan Jurafsky. Understanding neural networks through representation erasure. arXiv preprint arXiv:1612.08220, 2016.\n[3] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adversarial input sequences for recurrent neural networks. In Military Communications Conference, MILCOM 2016-2016 IEEE, pp. 49\u201354. IEEE, 2016.\n[4] Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. Deep text classification can be fooled. arXiv preprint arXiv:1704.08006, 2017.", "title": "Efficiency of Gumbel Attack; Difference and Connections in Discrete Optimization and Adversarial Attack."}, "BJxl0Dtjpm": {"type": "rebuttal", "replyto": "ByghKiC5YX", "comment": "We have elaborated the Greedy attack with a clearer presentation in Section 3.1. First, we have adopted the common notations and marked each expectation with a subscript to indicate the source of the expectation. Second, in the updated version, we have added a detailed explanation of the approximation in Equation (5, 7).  To summarize, when one assumes other features are perturbed adversarially, the Greedy Attack can be interpreted as maximizing a lower bound of the original objectives.\n\nWe have added another experiment to compare various algorithms with human evaluation on the IMDB movie review data set. On each instance, we increase the number of words to be perturbed until the prediction of the model changes. Then we ask humans to label original texts and perturbed texts. Greedy attack yields the best performance in the experiment. Please see Appendix B of the updated version for details.\n\nWe again express our sincere thanks to all the reviewers, who have provided very useful suggestions for helping build our manuscript into a better shape.", "title": "UPDATED: Elaborate Section 3.1 and add new human evaluation based on the reviewers\u2019 suggestions."}, "Hkx5iwFoam": {"type": "rebuttal", "replyto": "HJlmF5xq27", "comment": "We thank the reviewer for the detailed and encouraging comments! \n\nTo address the reviewer\u2019s concern on Equation 5, we have added a more rigorous and detailed explanation of the approximation. Roughly, when one assumes other features are perturbed adversarially, the Greedy Attack can be interpreted as maximizing a lower bound of the original objectives. Details can be found in Section 3.1 of the updated version. ", "title": "Response to Reviewer 2"}, "S1x8tPKiam": {"type": "rebuttal", "replyto": "S1gSsdb537", "comment": "We thank the reviewer for the encouraging comments and the help in addressing the importance of the task!\n\nWhat\u2019s the \u201crandom attack\u201d baseline in these tasks? In computer vision it\u2019s often sufficient to add a little bit of salt-and-pepper noise or Gaussian noise to change the model decision.\n\nWe define \u201crandom attack\u201d as randomly sample k positions in the sentence, and replace them with randomly sampled words. We run random perturbation on the test set of the IMDB movie review dataset used in our paper. The average consistency of the predictions of the model from the perturbed and the original instances is 99.9% after k = 10 words are changed, and 92%, 90.4% after k = 50, 100 words are changed respectively. See the following link for a plot of comparison with our algorithms (on the first five words): https://drive.google.com/file/d/1T6UJQPz4iDFqsK9XQZ0nYv-bBcYxWraP/view?usp=sharing. \nWe conclude that random perturbation does not work.  \n\n\u201cWhat the human evaluation scores would be on adversarials from other adversarial attacks?\u201d \n\nWe have added another experiment to compare various algorithms with human evaluation on the IMDB movie review data set. On each instance, we increase the number of words to be perturbed until the prediction of the model changes. Then we ask humans to label original texts and perturbed texts. Greedy attack yields the best performance in the experiment. Please see Appendix B of the updated version for details.\n\n\u201cAre you planning to release the code? Will it be part of CleverHans or Foolbox?\u201d\n\nYes, we plan to release the code. We will either release the code in a stand-alone github repository or merge it into CleverHans.", "title": "Response to Reviewer 3"}, "B1lXIwYsT7": {"type": "rebuttal", "replyto": "BJekkfSq3Q", "comment": "We thanks the reviewer for the comments, and the explanations on the motivation of the task. We have improved the clarity of Section 3.1 based on the reviewer\u2019s suggestions. Below we respond in detail to the reviewer\u2019s comments:\n\n\u201cthe Gumbel method performs poorly compared to other baselines\u201d\n\nWe agree that the performance of the Gumbel method is comparable to previous methods. However, its running time is significantly shorter than all the previous methods and our Greedy Attack method (See Figure 4). Thus, Gumbel Attack is the most efficient one across all methods even after taking into account the training stage. The efficiency of generating adversarial examples is an important factor for large-scale data.  \n\n\"what is causing their greedy approach to perform better\u201d than \u201csome gradient based adversarial attacks\u201d?\n\nWhile gradient-based methods have led to several successful algorithms in the continuous domain (e.g., natural images), they have been observed to be less effective compared to discrete methods (e.g., [1]). It is mainly because gradient based methods focus on the sensitivity of response to each feature in the infinitesimal space, while perturbation is carried out in discrete space. \n\n\u201cit is egregiously difficult to read in parts and is poorly written\u201d\n\nWe apologize for the difficulty of reading and have addressed the problem carefully. First, we have adopted the common notations and marked each expectation with a subscript to indicate the source of the expectation. Second, in the updated version, we have added a clearer and more detailed explanation of the approximation in Equation (5, 7).  To summarize, when one assumes other features are perturbed adversarially, the Greedy Attack can be interpreted as maximizing a lower bound of the original objectives.\n\n\u201cThe argument about approximation to the objective by considering the i positions independently is not convincing\u201d\n\nWe agree with the reviewer that this is an unnecessary assumption and have removed it from our framework (but still keep it in the design of Gumbel attack.) The independence assumption is used in Gumbel Attack for the sake of efficiency. This can be interpreted as a constraint on the search space so that decisions can be made in parallel. It can be a promising future direction to consider a framework where features are perturbed sequentially, with a termination gate [2] to control when to stop the perturbation. The latter enables the use of variable sizes of perturbation, instead of top-k perturbation.\n\n[1] Gao, Ji, et al. \"Black-box Generation of Adversarial Text Sequences to Evade Deep Learning Classifiers.\" arXiv preprint arXiv:1801.04354 (2018)\n[2] Shen, Yelong, et al. \"Reasonet: Learning to stop reading in machine comprehension.\" Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2017.", "title": "Response to Reviewer 1"}, "SkxNEow7Tm": {"type": "rebuttal", "replyto": "Hkxb5b3kpX", "comment": "Dear Area Chair and Anonymous Reader:\n\nThanks for your questions on the motivation of adversarial attack for discrete data. Below we briefly explain the motivation, followed by the evidence that simple random perturbation does not work. \n\nIn summary, the area chair and another reader posed the following questions\uff1a\n\n1. Why does one need to study the phenomenon of adversarial examples on discrete data?\n2. Why is this paper worth reading?\n3. Do simple methods like random perturbation work on text data?   \n\nIn short, our reply is\n\n1. Robustness is an important criterion for models on discrete data. The generation of adversarial examples can be used to evaluate robustness or even improve robustness.\n2. In this paper, our goal is to propose methods with better performance (Greedy attack) or with higher efficiency (Gumbel attack).\n3. We provide evidence that simple methods like random perturbation do not work. \n\nBelow are concrete details:\n\nRobustness is an important criterion for the application of machine learning models in critical areas such as medicine, financial markets, recommendation systems, and criminal justice. Adversarial examples have been used to evaluate the (adversarial) robustness of models (e.g., [1, 2, 5]) and have also been applied to train robust models (e.g., [3, 4]).\n\nThe phenomenon of adversarial examples was first found in state-of-the-art deep neural network models for classifying images (e.g., [5, 6, 2]), where small perturbations unobservable by human can easily fool neural networks. Similar to image data, the problem of adversarial perturbation on discrete data can be defined as altering the prediction of a model via minimal perturbation to an original sample (e.g., [7-14]).  \n\nWhile there have been many pioneered and interesting papers in this area (e.g., [7-14]), we proposed Greedy attack, a method to increase the misclassification rate of a model with a comparable scale of perturbation, and Gumbel attack, a method to improve the efficiency of generating adversarial examples, (It just happens to be fashionable :) ).\n\nIt is natural to ask how the simplest algorithm, random perturbation, works before one is persuaded to read our paper. We compare our methods with random perturbation on the test set of the IMDB movie review dataset used in our paper. For each instance, we randomly sample k positions in the sentence, and replace them with randomly sampled words. The average consistency of the predictions of the model from the perturbed and the original instances is 99.9% after k = 10 words are changed, and 92%, 90.4% after k = 50, 100 words are changed respectively. See the following link for a plot of comparison: https://drive.google.com/file/d/1T6UJQPz4iDFqsK9XQZ0nYv-bBcYxWraP/view?usp=sharing. \nWe conclude that random perturbation does not work. \n\n[1] Carlini, Nicholas, and David Wagner. \"Towards evaluating the robustness of neural networks.\" 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017.\n[2] Agarwal, Chirag, et al. \"An Explainable Adversarial Robustness Metric for Deep Learning Neural Networks.\" arXiv preprint arXiv:1806.01477 (2018).\n[3] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. ICLR (2018).\n[4] Alex Kurakin, Ian Goodfellow, Samy Bengio. Adversarial machine learning at scale. ICLR 2017. \n[5] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. ICLR, 2015.\n[6] Moosavi-Dezfooli, Seyed-Mohsen, Alhussein Fawzi, and Pascal Frossard. \"Deepfool: a simple and accurate method to fool deep neural networks.\" CVPR, 2016.\n[7] Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. Black-box generation of adversarial text sequences to evade deep learning classifiers. IEEE Security and Privacy Workshops (SPW), 2018.\n[8] Robin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pp. 2021\u20132031, 2017.\n[9] Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and Wenchang Shi. IJCAI, 2018. \n[10] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard Harang. Crafting adversarial input sequences for recurrent neural networks. In Military Communications Conference, MILCOM 2016-2016 IEEE, 2016.\n[11] Suranjana Samanta and Sameep Mehta. Towards crafting text adversarial samples. arXiv preprint arXiv:1707.02812, 2017.\n[12] Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, and Cho-Jui Hsieh. Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples. arXiv preprint arXiv:1803.01128, 2018.\n[13] Javid Ebrahimi, Anyi Rao, Daniel Lowd, Dejing Dou. Hotflip:White-box adversarial examples for text classification. ACL, 2018. \n[14] Jiwei Li, Will Monroe, Dan Jurafsky. Understanding neural networks through representation erasure.  arXiv preprint arXiv:1612.08220, 2016.", "title": "Motivation"}, "BJekkfSq3Q": {"type": "review", "replyto": "ByghKiC5YX", "review": "This paper addresses the problem of generating adversarial examples for discrete domains like text. They propose two simple techniques:\n1) Greedy: two stage process- first stage involves finding the k words in the sentence/paragraph to perturb and second step changes the word in the positions identified in step 1.\n2) Gumbel: first approach amortized over datasets where first and second steps are parametrized and learned over the dataset with the loss being the probability of flipping the decision.\nSpecifically, for the Gumbel approach, the authors use the non-differentiable top-k-argmax output to train the module in the second step which is not ideal and it would be better to train both first and second steps jointly in an end-to-end differentiable manner.\n\nThe results show that Greedy approach is able to significantly affect the accuracy of the systems compared to other adversarial baselines. Mturk evaluation shows that for tasks like sentiment analysis, humans weren't as confused as the systems were when the selected words were changed which is encouraging. However, the Gumbel method performs poorly compared to other baselines.\nMoreover, a thorough analysis of why Greedy is doing better than some gradient based adversarial attacks is needed in the paper because it is unclear what is causing their greedy approach to perform well; is it the two-stage nature of the process?\n\nMy major gripe with the paper is that it is egregiously difficult to read in parts and is poorly written. There are dangling conditional bars in many equations (5, 7, Greedy attack etc.), unclear \"expectation (E)\" signs and many other confusing notational choices which make the math difficult to parse. I am not even sure if those equations are correctly conveying the idea they are meant to convey. I found  the algorithms to be more clearly written and realize that the text in the models and equations is unnecessarily complicated. The argument about approximation to the objective by considering the i positions independently is not convincing and their is nothing in the paper to show if the assumption is reasonable.", "title": "Important task; very poorly written", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1gSsdb537": {"type": "review", "replyto": "ByghKiC5YX", "review": "In this work the authors introduce two new state-of-the-art adversarial attacks on discrete data based on a two-stage probabilistic process: the first step identifies key features which are then replaced in the second step through choices from a dictionary.\n\nOverall the manuscript is very well written and easy to follow. The evaluation is extensive and contains all previous attacks I am aware of. The greedy attack outperforms all prior work by a large margin while the Gumbel attack works on par with the previous state-of-the-art while being significantly faster. \n\nI only have a few questions and remarks:\n\n* What\u2019s the \u201crandom attack\u201d baseline in these tasks? In computer vision it\u2019s often sufficient to add a little bit of salt-and-pepper noise or Gaussian noise to change the model decision.\n\n* Another thing I am wondering is what the human evaluation scores would be on adversarials from other adversarial attacks? Adversarial attacks in general (e.g. in computer vision) can work in two ways: one being actually changing the semantic content (thus also \u201cfooling humans) while the other changes background features / add noise to which humans are pretty insensitive (unless you add too much of it). The greedy attack does seem to change some semantics as can be seen in the increased error rate of humans (which is pretty rare for computer vision adversarials). It might be that other attacks are rather changing words or characters which are not as semantically meaningful, as would be revealed by the accompanying human scores.\n\n* Are you planning to release the code? Will it be part of CleverHans or Foolbox?\n\nOverall, I find this work to be a really exciting advance on discrete adversarial attacks.", "title": "Exciting advance in discrete adversarial attacks", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "HJlmF5xq27": {"type": "review", "replyto": "ByghKiC5YX", "review": "The authors proposed a novel probabilistic framework to model adversarial attacks on deep networks with discrete inputs such as text. The proposed framework assumes a two step construction of an adversarial perturbation: 1) finding relevant features (or dimensions) to perturb (Eq. 3); 2) finding values to replace the features that are selected in step 1 (Eq. 4). The authors approximate some terms in these two equations to make the optimization easier. For example, it is *implicitly* assumed that given the i-th feature is removed from consideration, the probability of attack success does not change *on average* under probabilistic *adversarial* attack on other features (Eq. 5). It is not clear why that should hold and under what conditions that assumption would be reasonable (given that the attacks on other features are adversarial, although being probabilistic). \nThe proposed framework allows one to solve the computation vs. success rate trade-off by either estimating the best attack from the network (called greedy attack Eq. 6) or using a parametric estimation that does not require model evaluation (called Gumbel attack). Experimental results suggest that Gumbel attack has better or competitive attack rate on models developed for text classification while having the most computationally efficiency among other methods. It is also noticeable that the greedy attack achieves the best success rate with a large margin among all the tested methods. ", "title": "Novel probabilistic framework for making adversarial attacks on deep networks with discrete valued inputs; flexible framework that allows solving the trade-off between attack success rate and computation time", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}