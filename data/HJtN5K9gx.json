{"paper": {"title": "Learning Disentangled Representations in Deep Generative Models", "authors": ["N. Siddharth", "Brooks Paige", "Alban Desmaison", "Jan-Willem van de Meent", "Frank Wood", "Noah D. Goodman", "Pushmeet Kohli", "Philip H.S. Torr"], "authorids": ["nsid@robots.ox.ac.uk", "brooks@robots.ox.ac.uk", "alban@robots.ox.ac.uk", "j.vandemeent@northeastern.edu", "fwood@robots.ox.ac.uk", "ngoodman@stanford.edu", "pkohli@microsoft.com", "philip.torr@eng.ox.ac.uk"], "summary": "", "abstract": "Deep generative models provide a powerful and flexible means to learn complex distributions over data by incorporating neural networks into latent-variable models. Variational approaches to training such models introduce a probabilistic encoder that casts data, typically unsupervised, into an entangled and unstructured representation space. While unsupervised learning is often desirable, sometimes even necessary, when we lack prior knowledge about what to represent, being able to incorporate domain knowledge in characterising certain aspects of variation in the data can often help learn better disentangled representations. Here, we introduce a new formulation of semi-supervised learning in variational autoencoders that allows precisely this. It permits flexible specification of probabilistic encoders as directed graphical models via a stochastic computation graph, containing both continuous and discrete latent variables, with conditional distributions parametrised by neural networks. We demonstrate how the provision of structure, along with a few labelled examples indicating plausible values for some components of the latent space, can help quickly learn disentangled representations. We then evaluate its ability to do so, both qualitatively by exploring its generative capacity, and quantitatively by using the disentangled representation to perform classification, on a variety of models and datasets.", "keywords": ["Semi-Supervised Learning", "Deep learning", "Computer vision"]}, "meta": {"decision": "Reject", "comment": "The paper is a clearly presented application of deep generative models in the semi-supervised setting. After reviewing the discussion and responses, the reviewers felt that the paper while interesting, is limited in scope, and unfortunately not yet ready for inclusion in this year's proceeding."}, "review": {"H14GcVBIl": {"type": "rebuttal", "replyto": "HyW_r-fEl", "comment": "> [...] auxiliary variables could be avoided [...] modelling-wise one can get\n> the same model without these auxiliary variables and recover a minimal\n> extension of VAE where part of the generating space is actually observed\n\nWhile this is true in principle, in practise such an approach can often be much\nharder to handle. In particular, our formulation allows us to handle different\nparts of the model being observed for different data points.\n\nFor example, in the experiment with intrinsic faces (Section 4.2), some labelled\ndata points could each have only lighting as their observed variables, and some\nothers only identity.\n\nHaving to reformulate each such case to recover the minimal version is a tedious\nand potentially unnecessary process, when instead one can have this happen\nautomatically.\n\n> [...] experiments show a large deviation in these two methods' results\n> [...] give a possible explanation on the superiority of their method\n\nThe MNIST results are marginally lower that the comparison largely due to our\nuse of the plug-in estimator. The comparison marginalises out the discrete label\nvariable when unsupervised.\n\nThe SVHN results are better than the comparison due to our use of CNNs for the\nrecognition (convolutional) and generative (deconvolutional) networks in\ncontrast to using (multi-stage M1+M2) MLPs.\n\n> if the experimental setup is the same [...] for the results [...] not clear\n> if Kingma et al. do the same\n\nThe experiment setup is the same as the M2 model from Kingma et.al 2014 for the\nMNIST dataset. That is, we go directly from data to latents and back to data.\nFor the SVHN dataset, we compare against the M1+M2 model (since that is the only\nmodel reported for that dataset), but we don't do the preprocessing steps of PCA\nwhitening and learning the M1 model that they do with MLPs.\n\nInstead, we directly employ a CNN as a feature learner, and simultaneously learn\na classifier with it.\n\n> [...] comparison with Jampani et al. 2015 [...] is that model also using the\n> same rate of supervision for a fair comparison?\n\nThe comparison with Jampani et al. 2015 is not equal as they employ\n*fully-supervised* learning. We in contrast, only use a fraction of available\nlabels to perform semi-supervised learning.\n\n> Moreover, from the intro I expected to see a more general approximation\n> scheme for the variational posterior (similar to Ranganath et al. 2015 which\n> truly allows very flexible distributions), however this is not the case\n> here.\n\nUnless we misunderstand the reviewer\u2019s point, our formulation is indeed general\nenough to represent a wide variety of models, including control-flow structures\nlike if-then-else, folds, and loops. We can express the model indicated in\nRanganath et al. 2015 (Figure 1, right) using the constructs in our domain\nspecific language. Maybe the reviewer could clarify in what sense the model\ndescribed by Ranganath et al. could be seen as more flexible?\n\nThe exposition in the manuscript talks about a particular simple factorisation\nof the approximation q(z,y|x) as q(y|x) and q(z|y,x), in order to separate the\nobserved and unobserved variables. However, nothing in the formulation requires\nthe factorisation to be exactly this. In fact, z, y, and x can all be viewed as\nmultivariate, with dependencies between the dimensions.\n\nIndeed the experiments incorporate much richer factorisations.\n\n> Minor note: three of Kingma's papers are all cited in the main text as Kingma\n> et al. 2014, causing confusion. I suggest using Kingma et al. 2014a etc.\n\nThank you for catching this error! We have fixed it in the revised manuscript.\nThe three variants are now:\n- Kingma & Welling -- Auto-Encoding Variational Bayes\n- Kingma et.al. -- Semi-supervised learning with deep generative models\n- Kingma & Ba -- Adam: A Method for Stochastic Optimization", "title": "Individual Response"}, "Bk3y9ES8e": {"type": "rebuttal", "replyto": "B1FyYJmVe", "comment": "> From the experiment results it seems that these terms do not do much [...]\n> [...] benefit of the new formulation is likely to be just software\n> engineering flexibility and convenience.\n\nAs set out in our top-level response, and clarified in the updated manuscript,\nour formulation actually allows us to deal with more general models involving\ncontinuous random variables and automatically scales the classifier/regressor\nterm within the objective -- neither of which were possible before.\n\n> performance difference [...] not very significant (Figure 5).\n\nOur primary contribution in this work is not the development of better\nclassifiers or regressors, but to extend the state-of-the-art in semi-supervised\nlearning in deep generative models to incorporate a wider variety of models and\nsettings, in a unified framework.\n\nThe purpose of the experiments in Section 4.1 were to indicate that our general\nre-formulation of the problem matches the specific single-discrete-variable\nlatent model used in Kingma et.al 2014. It serves as a sanity check in the\nsimpler case.\n\n> [...] better to demonstrate a few situations where the proposed method can be\n> applied while for other previous methods it is non-trivial to do.\n\nThe experiments in Sections 4.2 and 4.3 are intended to do precisely this.\n\n - Intrinsic Faces\n\n   Here, we evaluate the ability to incorporate partial supervision on\n   *continuous* random variables, where the lighting is taken to be sampled from\n   a 3-dimensional Gaussian distribution.\n\n - Multi-MNIST\n\n   Here we evaluate the ability to incorporate recurrent structures, allowing\n   for *variable-dimension* latent spaces, where the number of digits that are\n   represented in the latent space are varied between 1 and 3.\n\nNeither of these experiments would be feasible under the standard formulation in\nKingma et.al 2014, especially with partial supervision.\n\n> [...] plug-in estimation [...] limitations.\n\nYes indeed!\nAs we note in the manuscript, the plug-in estimator holds only in some\nparticular cases. The assumption we make though is that h(x,y) is a function\nthat parameterises the variational approximation q(z|x,y). In the case of the\nVAE, this typically means that h(x,y) is a neural network and is thus continuous\nin y.\n\nWe do not always have to rely on the plug-in estimator though; as for the more\ngeneral cases, we can use any of the REINFORCE method, marginalisation (where\nappropriate), or even adopt the (very) recently developed\nConcrete/Gumbel-Softmax distribution instead.", "title": "Individial Response"}, "HkF6tEB8g": {"type": "rebuttal", "replyto": "rJUw6gmNx", "comment": "Thank you for your comments and suggestions.\n\n> [...] lacking in terms of methodological advances\n\nAs set out in our top-level response, and clarified in the updated manuscript,\nour formulation actually allows us to deal with more general models involving\ncontinuous random variables and automatically scales the classifier/regressor\nterm within the objective -- neither of which featured in their work.\n\n> [...] don't agree with the strong contrast made between deep generative\n> models and graphical models\n\nWe do not intend to particularly contrast graphical models and deep generative\nmodels; indeed as the reviewer points out, the latter are a particular variant\nof the former. However, we do intend to contrast the kinds of models assumed for\nthe recognition networks (typically simple mean-field) against more general, and\ncomplex, graphical models.\n\nOur particular interests herein are twofold:\n\n a. Incorporating a particular graphical model into the recognition network\n    naturally lends interpretability to the latents in terms of the variables\n    (and their semantics) of the graphical model.\n\n b. Adding domain knowledge in the form of the given graphical model,\n    particularly in perceptual domains such as vision, helps better model the\n    complex variation introduced by the implicit 'rendering' process in the\n    generative model.\n\n> [...] having multiple stochastic variables is not exclusive to graphical\n> models\n\nOur claim in this work is not simply that we can employ multiple stochastic\nvariables, but that we can encode arbitrary dependencies in the *recognition\nmodel*. This has the effect of disentangling the latent representation through\nthe provided dependency structure and supervision.\n\nThe Multi-MNIST experiment (Section 4.3) employs elements of both DRAW and rVAE,\nwith the recognition network structured as an RNN with disentangled state, and\nwith the generative model attending to each digit sequentially, in the spirit of\nDRAW. This is indicated in the models specified in the Appendix.\n\n> It is debatable whether the models themselves have structure.\n\nAs we note in our top-level response, we agree that the word \u201cstructure\u201d can be\ninterpreted in multiple ways. It is of course true that the prior on latent\nvariables has no (or minimal, in the case of multi-MNIST) structure. It is\nhowever not clear whether such structure in the prior is necessary, since the\nneural network that approximates the generative model can capture correlations\nand dependencies between variables. Our results show that dependency structures\nin the recognition networks (Figures 3, 6, and 8) help capture the complex\ndependencies which occur in the true model posterior. As noted above, we have\nedited the manuscript as well as changed the title to help clarify this\nperspective.", "title": "Individial Response"}, "Syqct4BLg": {"type": "rebuttal", "replyto": "HJtN5K9gx", "comment": "We thank the reviewers for their comments and suggestions and are encouraged by\nthe positive feedback regarding its value, interest, relevance, and clarity.\n\nIn this top-level response, we address the two central issues raised by the\nreviewers.\n\n- The title and the semantics of 'structure'\n\n  The reviewers found the title somewhat confusing and perhaps overly general in\n  addition to potential confusion due the many meanings of the term 'structure'.\n  To ameliorate this issue, we have done the following:\n\n  a. Title change\n\n     We changed the title to\n     \t``Learning Disentangled Representations in Deep Generative Models''\n     to better reflect the contributions of the submission.\n\n  b. Clarification for 'structure'\n\n     We realise that the term 'structure' can have multiple meanings, even\n     within the confines of graphical and generative models.\n\n     Our use of the term is intended to refer to the (arbitrary) dependencies\n     one would like to employ in the recognition model, particularly in regard\n     to there being consistent 'interpretable' semantics of what the variables\n     in the model represent.\n\n     We have updated the abstract and the introduction (Section 1, end) in the\n     manuscript making this clarification and removing extraneous instances.\n\n- Relation to Kingma.et.al 2014 [1]\n\n  We would like to note here that we do not simply reformulate the\n  semi-supervised work in [1]. Our formulation is not a trivial extension of\n  [1], rather, it allows us to extend semi-supervised learning to a broader\n  class of latent-variable models.\n  We list below the important distinctions:\n\n  a. Continuous-domain semi-supervision\n\n     We can handle partial labels for continuous random variables, not just\n     discrete ones. In this case, the factorisation in Eqn 4 corresponds to a\n     *regressor* instead of a classifier. The work in [1] requires\n     marginalization over the partially-observed variable\u2019s support in the\n     unsupervised case, which means that latent variables must in practice be\n     discrete.\n\n     Indeed we make use of continuous latent variables in the Faces experiment\n     (Section 4.2), where the lighting is a partially supervised 3-dimensional\n     Gaussian random variable.\n\n  b. Scaling the classifier term\n\n     The formulation in Eqn 4 naturally incorporates the classifier term, as\n     opposed to a separate fixed hyper-parameter (alpha, in Eqn 9 in [1]) that\n     controls the contribution of the classifier term in the objective. \n\n     This makes the formulation more flexible and general purpose for different\n     factorisations of the variational approximations used.\n\n  c. Framework for  implementation of models\n\n     As pointed out by the reviewers, our formulation allows for easy automated\n     implementation of a wide variety of models. This is in the same spirit as a\n     number of approaches such as Automatic Differentiation (AD) and\n     Probabilistic Program inference, where the choice of a particular means of\n     representation enables ease of automation for a great variety of different\n     cases.\n\n     The ease with which one can describe even minimally complex models, such as\n     that employed for the Multi-MNIST experiments, is shown in the Appendix.\n\n  d. Semi-supervised learning on varying subsets of variables\n\n     A particular benefit of the automation mentioned above, is that we derive\n     the ability to partially supervise any subset of variables, regardless of\n     type. Indeed we even have the ability to supervise *different* latent\n     variables for different data points by virtue of our formulation\n     automatically factorising into labelled and unlabelled terms, on a\n     per-data-point (or minibatch) basis. This is a particularly desirable\n     characteristic to have in dealing with missing-data issues often\n     encountered in large datasets.\n\nWe have updated the Framework and Formulation section (Section 3, after Eqn 4)\nin the manuscript with a description of these differences.\n\nWe address the remainder of the comments by the reviewers in the corresponding\nresponses to the reviews.\n\n[1] Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling.\n    Semi-supervised learning with deep generative models. In Advances in Neural\n    Information Processing Systems, pp. 3581\u20133589, 2014\n", "title": "Response to reviewers"}, "Bkxl_uDXx": {"type": "rebuttal", "replyto": "rkW4ZarQl", "comment": "Thank you for your comments!\n\nIn our framework, structural constraints on the latents refers to the combination of the type of stochastic variables used, their dimensionality, and the explicit dependencies introduced between the variables in the recognition model.\nThis is in direct contrast to the typically-used mean-field assumption in the recognition model in which each variable is conditionally independent given data, with the same dimensionality and type (typically multivariate Gaussian).\n\nThe provision of such constraints implicitly affects the distributions induced over the latent space by the recognition model.\nAll our experiments involve such structural constraints in the recognition and generative models employed as indicated in figures 3,6, and 8.\n", "title": "Response"}, "rkW4ZarQl": {"type": "review", "replyto": "HJtN5K9gx", "review": "Could the authors clarify why the described model can help introduce structural constraints on the latent variables in a VAE?  From the paper it seems the most significant thing the model does is to handle labeled and unlabeled data in a single formulation in Eq.4, but it is unclear to me how the claim on being able to specify structural constraints on the latents is supported in the paper.  The three experiments do not seem to have structural constraints on the latents, and the latents are not much more structural than previous work.This paper proposed a variant of the semi-supervised VAE model which leads to a unified objective for supervised and unsupervised VAE.  This variant gives software implementation of these VAE models more flexibility in specifying which variables are supervised and which are not.\n\nThis development introduces a few extra terms compared to the original semi-supervised VAE formulation proposed by Kingma et al., 2014.  From the experiment results it seems that these terms do not do much as the new formulation and the performance difference between the proposed method and Kingma et al. 2014 are not very significant (Figure 5).  Therefore the benefit of the new formulation is likely to be just software engineering flexibility and convenience.\n\nThis flexibility and convenience is nice to have, but it is better to demonstrate a few situations where the proposed method can be applied while for other previous methods it is non-trivial to do.\n\nThe paper's title and the way it is written make me expect a lot more than what is currently in the paper.  I was expecting to see, for example, structured hidden variable model for the posterior (page 4, top), or really \"structured interpretation\" of the generative model (title), but I didn't see any of these.  The main contribution of this paper (a variant of the semi-supervised VAE model) is quite far from these.\n\nAside from these, the plug-in estimation for discrete variables only works when the function h(x,y) is a continuous function of y.  If however, h(x, y) is not continuous in y, for example h takes one form when y=1 and another form when y=2, then the approach of using Expectation[y] to replace y will not work.  Therefore the \"plug-in\" estimation has its limitations.\n", "title": "structural constraints on the latents", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1FyYJmVe": {"type": "review", "replyto": "HJtN5K9gx", "review": "Could the authors clarify why the described model can help introduce structural constraints on the latent variables in a VAE?  From the paper it seems the most significant thing the model does is to handle labeled and unlabeled data in a single formulation in Eq.4, but it is unclear to me how the claim on being able to specify structural constraints on the latents is supported in the paper.  The three experiments do not seem to have structural constraints on the latents, and the latents are not much more structural than previous work.This paper proposed a variant of the semi-supervised VAE model which leads to a unified objective for supervised and unsupervised VAE.  This variant gives software implementation of these VAE models more flexibility in specifying which variables are supervised and which are not.\n\nThis development introduces a few extra terms compared to the original semi-supervised VAE formulation proposed by Kingma et al., 2014.  From the experiment results it seems that these terms do not do much as the new formulation and the performance difference between the proposed method and Kingma et al. 2014 are not very significant (Figure 5).  Therefore the benefit of the new formulation is likely to be just software engineering flexibility and convenience.\n\nThis flexibility and convenience is nice to have, but it is better to demonstrate a few situations where the proposed method can be applied while for other previous methods it is non-trivial to do.\n\nThe paper's title and the way it is written make me expect a lot more than what is currently in the paper.  I was expecting to see, for example, structured hidden variable model for the posterior (page 4, top), or really \"structured interpretation\" of the generative model (title), but I didn't see any of these.  The main contribution of this paper (a variant of the semi-supervised VAE model) is quite far from these.\n\nAside from these, the plug-in estimation for discrete variables only works when the function h(x,y) is a continuous function of y.  If however, h(x, y) is not continuous in y, for example h takes one form when y=1 and another form when y=2, then the approach of using Expectation[y] to replace y will not work.  Therefore the \"plug-in\" estimation has its limitations.\n", "title": "structural constraints on the latents", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Sk2K6OXXg": {"type": "rebuttal", "replyto": "HJ0AAiy7g", "comment": "Thank you for your comments!\nWe address the questions below and have also updated the manuscript with changes clarifying some of the questions raised.\n\na.\n\nThank you for raising this issue and apologies for the confusion!\nWe corrected a typo in Eq (3) with the prior in the formulation now denoted p(z|y_i) instead of p(z,y_i). \nThis resolves a consistency error between Eqs (3) and (4).\nWe have also reformulated the paragraphs around Eq (3) and (4) to clarify theexposition.\n\nEq (3) does not have q(y|x) because it is a conditional ELBO, conditioning on a given y_i, where the expectation is over just q(z|x,y). This is similar to the formulation in Sohn et.al. 2015.\n\nb.\n\nThe auxilliary-variable formulation enables a unified implementation that handles the different cases by separating out *where* the value for a label/interpretable latent comes from in the estimation.\n\nWhen supervised, the label is given and is scored against q(y|x), and when unsupervised, the label is sampled from q(y|x), along with it's associated score. However, for the rest of the latent variables, the conditional elbo is computed without regard to where the value y_i came from.\n\nIn our framework, we can handle this by having the stochastic computation graph simply mark variables as observed or not, and automatically factor the distribution over latents into its supervised and unsupervised factors.\n\nc.\n\nThe use of a \"supervision-rate\" is not necessarily something only our model allows.\nFor example, the setup in Kingma et.al. 2014 could be modified such that for a dataset of size D (with D_s elements supervised) instead of partitioning the supervised data over all the minibatches, effectively drawing a fixed proportion (D_s/D), *without replacement*, it could draw *with replacement* from D_s to fill any requisite proportion r of a minibatch.\nOur particular formulation is a consequence of the fact that our minibatches consist of wholly supervised or unsupervised data instead of having each minibatch be mixed.\n\nPractically, the rate controls how well one learns the generative and recognition-network parameters under interpretability constraints.\nFor a given (small) labelled set, full supervision, i.e always observing only the labelled data, can overfit even though it learns a disentangled representation. Conversely, no-supervision can avoid overfitting to generalise well, but loses out on interpretability.\nFor a fixed-size labelled set, the rate helps explore the spectrum between these two cases by adjusting how many times one observed a particular labelled data point.\n\nd.\n\nThanks for pointing it out and apologies for the omission!\nWe updated the manuscript to include a sentence on which estimator we use for our experiments.\n\nFor the particular experiments run for the manuscript, we used the plug-in estimator where applicable (discrete latent variables).\nHowever, our framework (the torch library) has the ability to use any of the three kinds (REINFORCE, marginalise, plug-in) of estimators through the use of appropriate modelling primitives.\n\nWe use the plug-in estimator because it's computationally cheaper than both REINFORCE and marginalisation, requiring just a single sample to estimate, and also, performs comparably to both the other methods qualitatively (quite a bit better than REINFORCE with a few samples).", "title": "Response"}, "HJ0AAiy7g": {"type": "review", "replyto": "HJtN5K9gx", "review": "I would appreciate if the authors could clarify:\n\na. Why is q(y|x) missing in equation (3)? It appears correctly in (4), but why not in (3)? I understand that we extend the z-layer with variables y in (2), and in (3) we assume additionally that q(z,y|x) = q(z|y,x) q(y|x). So why is q(y|x) missing? (Or maybe I'm just missing something?)\n\nb. I don't quite get what the auxiliary variable offers in (4), in terms of explanation or formulation. Of course if we insert and marginalize it out, we go back to (2). If we don't marginalize it out but at the same time use the delta, we basically set it to y. So what is the role of this representation?\n\nc. I was wondering if the discussion about \"Supervision rate\" is particular to the model presented. What are the practical implications from this observation?\n\nd. In the discussion about plug-in estimation for discrete variables, can you please clarify out of these approaches, which is the one *you* decided to use and why?\n\n\nThis paper introduces a variant of the semi-supervised variational auto-encoder (VAE) framework. The authors present a way of introducing structure (observed variables) inside the recognition network.\n\nI find that the presentation of the inference with auxiliary variables could be avoided, as it actually makes the presentation unnecessarily complicated. Specifically, the expressions with auxiliary variables are helpful for devising a unified implementation, but modeling-wise one can get the same model without these auxiliary variables and recover a minimal extension of VAE where part of the generating space is actually observed. The observed variables mean that the posterior needs to also condition on those, so as to incorporate the information they convey. The way this is done in this paper is actually not very different from Kingma et al. 2014, and I am surprised that the experiments show a large deviation in these two methods' results. Given the similarity of the models, it'd be useful if the authors could give a possible explanation on the superiority of their method compared to Kingma et al. 2014. By the way, I was wondering if the experimental setup is the same as in Kingma et al. 2014 for the results of Fig. 5 (bottom) - the authors mention that they use CNNs for feature extraction but from the paper it's not clear if Kingma et al. do the same. \n\nOn a related note, I was wondering the same for the comparison with Jampani et al. 2015. In particular, is that model also using the same rate of supervision for a fair comparison?\n\nThe experiment in section 4.3 is interesting and demonstrates a useful property of the approach.\n\nThe discussion of the supervision rate (and the pre-review answer) is helpful in giving some insight about what is a successful training protocol to use in semi-supervised learning.\n\nOverall, the paper is interesting but the title and introduction made me expect something more from it. From the title I expected a method for interpreting general deep generative models, instead the described approach was about a semi-supervised variant of VAE - naturally including labelled examples disentangles the latent space, but this is a general property of any semi-supervised probabilistic model and not unique to the approach described here. Moreover, from the intro I expected to see a more general approximation scheme for the variational posterior (similar to Ranganath et al. 2015  which trully allows very flexible distributions), however this is not the case here.\n\nGiven the above, the contributions of this paper are in defining a slight variant of the semi-supervised VAE, and (perhaps more importantly) formulating it in a way that is amendable to easier automation in terms of software. But methodologically there is not much contribution to the current literature. The authors mention that they plan to extend the framework in the probabilistic programming setting. It seems indeed that this would be a very promising and useful extension. \n\nMinor note: three of Kingma's papers are all cited in the main text as Kingma et al. 2014, causing confusion. I suggest using Kingma et al. 2014a etc.\n", "title": "Clarifications", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HyW_r-fEl": {"type": "review", "replyto": "HJtN5K9gx", "review": "I would appreciate if the authors could clarify:\n\na. Why is q(y|x) missing in equation (3)? It appears correctly in (4), but why not in (3)? I understand that we extend the z-layer with variables y in (2), and in (3) we assume additionally that q(z,y|x) = q(z|y,x) q(y|x). So why is q(y|x) missing? (Or maybe I'm just missing something?)\n\nb. I don't quite get what the auxiliary variable offers in (4), in terms of explanation or formulation. Of course if we insert and marginalize it out, we go back to (2). If we don't marginalize it out but at the same time use the delta, we basically set it to y. So what is the role of this representation?\n\nc. I was wondering if the discussion about \"Supervision rate\" is particular to the model presented. What are the practical implications from this observation?\n\nd. In the discussion about plug-in estimation for discrete variables, can you please clarify out of these approaches, which is the one *you* decided to use and why?\n\n\nThis paper introduces a variant of the semi-supervised variational auto-encoder (VAE) framework. The authors present a way of introducing structure (observed variables) inside the recognition network.\n\nI find that the presentation of the inference with auxiliary variables could be avoided, as it actually makes the presentation unnecessarily complicated. Specifically, the expressions with auxiliary variables are helpful for devising a unified implementation, but modeling-wise one can get the same model without these auxiliary variables and recover a minimal extension of VAE where part of the generating space is actually observed. The observed variables mean that the posterior needs to also condition on those, so as to incorporate the information they convey. The way this is done in this paper is actually not very different from Kingma et al. 2014, and I am surprised that the experiments show a large deviation in these two methods' results. Given the similarity of the models, it'd be useful if the authors could give a possible explanation on the superiority of their method compared to Kingma et al. 2014. By the way, I was wondering if the experimental setup is the same as in Kingma et al. 2014 for the results of Fig. 5 (bottom) - the authors mention that they use CNNs for feature extraction but from the paper it's not clear if Kingma et al. do the same. \n\nOn a related note, I was wondering the same for the comparison with Jampani et al. 2015. In particular, is that model also using the same rate of supervision for a fair comparison?\n\nThe experiment in section 4.3 is interesting and demonstrates a useful property of the approach.\n\nThe discussion of the supervision rate (and the pre-review answer) is helpful in giving some insight about what is a successful training protocol to use in semi-supervised learning.\n\nOverall, the paper is interesting but the title and introduction made me expect something more from it. From the title I expected a method for interpreting general deep generative models, instead the described approach was about a semi-supervised variant of VAE - naturally including labelled examples disentangles the latent space, but this is a general property of any semi-supervised probabilistic model and not unique to the approach described here. Moreover, from the intro I expected to see a more general approximation scheme for the variational posterior (similar to Ranganath et al. 2015  which trully allows very flexible distributions), however this is not the case here.\n\nGiven the above, the contributions of this paper are in defining a slight variant of the semi-supervised VAE, and (perhaps more importantly) formulating it in a way that is amendable to easier automation in terms of software. But methodologically there is not much contribution to the current literature. The authors mention that they plan to extend the framework in the probabilistic programming setting. It seems indeed that this would be a very promising and useful extension. \n\nMinor note: three of Kingma's papers are all cited in the main text as Kingma et al. 2014, causing confusion. I suggest using Kingma et al. 2014a etc.\n", "title": "Clarifications", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}