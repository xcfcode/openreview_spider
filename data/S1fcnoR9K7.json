{"paper": {"title": "Learning with Random Learning Rates.", "authors": ["L\u00e9onard Blier", "Pierre Wolinski", "Yann Ollivier"], "authorids": ["leonardb@fb.com", "pierre.wolinski@u-psud.fr", "yol@fb.com"], "summary": "We test stochastic gradient descent with random per-feature learning rates in neural networks, and find performance comparable to using SGD with the optimal learning rate, alleviating the need for learning rate tuning.", "abstract": "Hyperparameter tuning is a bothersome step in the training of deep learning mod- els. One of the most sensitive hyperparameters is the learning rate of the gradient descent. We present the All Learning Rates At Once (Alrao) optimization method for neural networks: each unit or feature in the network gets its own learning rate sampled from a random distribution spanning several orders of magnitude. This comes at practically no computational cost. Perhaps surprisingly, stochastic gra- dient descent (SGD) with Alrao performs close to SGD with an optimally tuned learning rate, for various architectures and problems. Alrao could save time when testing deep learning models: a range of models could be quickly assessed with Alrao, and the most promising models could then be trained more extensively. This text comes with a PyTorch implementation of the method, which can be plugged on an existing PyTorch model.", "keywords": ["step size", "stochastic gradient descent", "hyperparameter tuning"]}, "meta": {"decision": "Reject", "comment": "The paper proposes a new optimization approach for neural nets where, instead\nof a fixed learning rate (often hard to tune), there is one learning rate per\nunit, randomly sampled from a distribution. Reviewers think the idea is\nnovel, original and simple. Overall, reviewers found the experiments\nunconvincing enough in practice. I found the paper really borderline,\nand decided to side with the reviewers in rejecting the paper."}, "review": {"HyeBGxI52Q": {"type": "review", "replyto": "S1fcnoR9K7", "review": "POST REBUTTAL: I think the paper is decent, there are some significant downsides to the method but it could constitute a first step towards a more mature learning-rate-free method. However, in its current state the paper is left with some gaping holes in its experiment section. The authors tried to add experiments on Imagenet, but these experiments apparently didn't finish before the end of the rebuttal period. For that reason, the paper probably should not be accepted for publication (even if the authors manage to finish running these experiments, we would not have a chance to review these results). \n\n------\nOriginal review\n------\n\nIn this paper, the authors present a method for training deep networks with randomly sampled feature-wise learning rates, removing the need for fixed learning rates and their tuning. The method is shown to perform comparatively to SGD with a learning rate roughly optimized with regards to validation performance. The method applies to the most popular types of deep learning architectures, which includes fully connected layers, convolutional layers and recurrent cells. \n\nQuality: The paper is of a decent quality in general, I noticed no glaring omissions while reading the paper. However, I do worry that the method provides little gain for a lot of work. It is becoming more and more easy to tune the learning rate of deep learning models with strategies such as early stopping, and this method comes at a high cost for models with a big final layer. \n\nClarity: The paper is well written, but the reader is often (too often?) sent to the Appendix, which is itself ordered in a strange way (e.g., the first reference to the Appendix in the paper refers to Appendix F?). If some sections of the Appendix are not needed, I would remove them. \n\nOriginality: The work is original in the approach, i.e. randomization as a way to get rid of learning rates is a novel method. However, there was one work presented last year at NIPS which concerns itself with the same problem, which is getting rid of learning rates:\n\n\u201cTraining Deep Networks without Learning Rates Through Coin Betting\u201d by Francesco Orabona and Tatiana Tommasi, NIPS, 2017.\n\nThey don\u2019t compare on the same methods and the same datasets, but I think the authors should be aware of this work and perhaps compare themselves with it. The work takes a very different approach to solve the problem so I don\u2019t think it\u2019s an issue for this paper. \n\nSignificance: I think the work is important, in that it adds another tool to solve the learning rate problem. I would not say it is likely to have a very high impact, because it involves a lot of work, for little benefit. Furthermore, the cost of reproducing multiple times the last layer of the network will be prohibitive in many cases for NLP. \n\nThe method feels ad-hoc in many respects, and there are no guarantees that it would work any better than Adam does on pathological cases. Perhaps some mathematical analysis on simpler problems would help make the contributions stronger. \n\nThe authors state that the learning rate range has little impact on performance, yet it still has enough impact to justify tuning it for different models and datasets (on CIFAR it is 10^-5 to 10^1, on Pennbank it is 10^-3 to 10^2). I would tend to agree that the alrao method is more robust to the choice of learning rate than plain SGD, however the fact of the matter is that there are still parameters to tune. \n\nFigure 5. also seems to suggest that the range is important, although the models were not trained until the end, so it is not clear.\n\nSome additional comments:\n\nNitpicking: In Section 2, most sub-sections (or paragraphs titles?) have the name of the method in them. That\u2019s redundant. Instead of \u201cAlrao principle\u201d, \u201cAlrao update\u201d, etc., just write \u201cPrinciple.\u201d, \u201cUpdate.\u201d.\n\nIs there a justification for using the same learning rate for all weights in an LSTM unit? \n\nI believe there is a mistake in Equation 2. The denominator should be log(\\eta_{max}) - log(\\eta_{min})\n\n[second paragraph on page 4.] Once again nitpicking for the sake of clarity: \u201cFor each classifier C\u03b8 cl j, we set a learning rate log eta_j = \u2026\u201d this reads as if the learning rate would be set to log eta_j, but you probably mean you will set the learning rate to eta_j = exp(...).\n\nFigure 5b in the appendix does not specify which curve has which learning rate interval. \n", "title": "not convinced it's worth it", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hyly32Fqhm": {"type": "review", "replyto": "S1fcnoR9K7", "review": "\nThis work proposes an optimization method called All Learning Rate\nAt Once (Alrao) for hyper-parameter tuning in neural networks.\nInstead of using a fixed learning rate, Alrao assigns the learning\nrate for each neuron by randomly sampling from a log-uniform\ndistribution while training neural networks. The neurons with\nproper learning rate will be well trained, which makes the whole\nnetwork eventually converge. The proposed method achieves\nperformance close to the SGD with well-tuned learning rate on the\nexperiments of image classification and text prediction.\n\n\n#Pros:\n\n-- The use of randomly sampled learning rate for deep learning\nmodels is novel and easy to implement. It can become a good\napproximation of using SGD with the optimal learning rate.\n\n-- The paper is well-written and easy to follow. The proposed\nmethod is illustrated in a clear way.\n\n-- The experiments are solid, and the performance on three\ndifferent architectures are shown for comparison. According to the\nexperiments, the proposed method is not sensitive to the\nhyper-parameter \\eta_{min} and \\eta_{max}.\n\n#Cons:\n\n-- The authors have not given any theoretical convergence analysis\non the proposed method.\n\n-- Out of all four experiments, the proposed method only\noutperforms Adam once, which does not look like strong support.\n\n-- Alrao achieves good performance with SGD, but not with Adam.\nAlso, there are no experimental results on Alrao with other\noptimization methods.\n\n#Detailed comments:\n\n(1) I understand that Alrao will be more efficient compared to\napplying SGD with different learning rate, but will it be more\nefficient compared to Adam? No clear clarification or experimental\nresults have been shown in the paper.\n\n(2) The units with proper learning rate could learn well and\nconstruct good subnetworks. I am wondering if the units with \"bad\"\n(too small or too large) learning rate might give a bad influence\non the convergence or performance of the whole network.\n\n(3) The experimental setting is not clear, such as, how the input\nnormalized, how data augmentation is used in the training phase,\nand what are the depth, width and other settings for all three\narchitectures.\n\n(4) The explanation on the influence of using random learning rate\nin the final layer is not clear to me.\n\n(5) Several small comments regarding writing:\n    (a) Is the final classifier layer denoted as $C_{\\theta^c}$ or  $C_{\\theta^{cl}}$ in the third paragraph of \"Definitions and notations\"?\n    (b) In algorithm 1, what is the stop criteria for the do while? The \"Convergence ?\" in the while condition is confusing.\n    (c) Is the learning curve in Figure 2 from one run or is it the average of all runs? Are the results consistent for each run? How about the learning curves for VGG19 and LSTM, do they have similar learning curves with the two architectures in Figure 2?\n    (d) For Figure 3, it will be easier to compare the performance on the training and test set, if the color bars for the two figures share the same range.\n", "title": "Interesting idea, does not seem to work consistently, limited theoretical explanation", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1lrajxM07": {"type": "rebuttal", "replyto": "rkg4HYxL27", "comment": "Thank you for your comments and for insisting on more substantial\nexperimental validation.  This comment spurred us to perform a series of\nexperiments on ImageNet with several architectures, all confirming our\nprevious observations (see general comment above).\n\nWe hope these additional experiments may mitigate your opinion that \"related\nto the experimental evaluation of the method. I find the experimental\nevidence for the effectiveness of Alrao insufficient.\"\n\nIndeed the text offers no experimental use case related to architecture\nsearch. As a result, following your comment we have decided to\nde-emphasize architecture search as a motivation in the text. Instead, we\nprovide more experiments to insist on the robustness of the method and\nits ability to provide results in a single run.\n\nAbout Alrao versus setting per-weight random learning rates: our design\nis based on a theoretical argument. Indeed, with per-weight learning\nrates, *every* unit would have some incoming weights with large learning\nrate. So, every unit would be at risk of divergence or quick saturation\n(eg for sigmoid activations). With per-unit learning rates, units with\ntoo-large learning rates may saturate and \"die\", but units with suitable\nlearning rates will be able to propagate information to subsequent\nlayers. Hopefully a sub-network made of units with suitable learning\nrates will emerge.\n\nAbout additional time in practice: the per-iteration or per-epoch\ncomputational overhead of Alrao (a few seconds) is negligible compared to\nthe computation time of each epoch (approximately an hour) in our Imagenet experiments.\nThe number of epochs for convergence may be slightly worse than SGD\nbut varies depending on the setup, see Figs 2 and 5.\n", "title": "Specific answers to your comments"}, "SJgycsgMAQ": {"type": "rebuttal", "replyto": "HyeBGxI52Q", "comment": "Thank you for your compliments on the importance, originality and clarity\nof the paper, and for pointing out ways to improve the paper (eg\ntheoretical analysis).\n\nAbout possible gains from the method: It is true that \"It is becoming more\nand more easy to tune the learning rate of deep learning models\".\nHowever, the early stopping method still assumes we can launch several\ncopies of the model: this is not the case in an online setting, for\ninstance.  Future applications may require on-the-spot fine-tuning or\ntransfer, for instance to adapt to a specific end-user's characteristics,\nwithout off-line retraining.\n\nThe added ImageNet experiments (see general comment above) provide more\narguments for the benefits of the method, notably its robustness compared\nto Adam.\n\nThank you for pointing out Orabona-Tommasi 2017. This is a very\nintriguing paper, but it seems to us that doing away with gradients\naltogether is a bold step which requires extensive testing,\nwhile Alrao stays closer to the well-tested SGD principle.\nOrabona-Tommasi's method has still to be tested for standard\narchitectures (VGG, GoogleNet, etc.). We have added this reference in the\nrelated work.\n\nAbout Alrao needing \"a lot of work\": We have provided a generic Alrao\nimplementation, so the additional work to incorporate Alrao into an\nexisting model is just a call to that library (for standard\narchitectures).\n\nAbout the cost of the last layer for NLP: this is true. However, this\nproblem is already present in many NLP applications even without Alrao,\nand over the years a number of strategies have been developed to reduce\nsubstantially the last layer's parameter burden for large vocabulary\nsizes (see several references in Jozefowicz 2016).\n\nAbout a mathematical analysis on a simple problem: see the new Theorem\n1 in Appendix B. Namely, for logistic regression (or any convex\nfunction with fixed pre-classifier), with eta_min small enough, then\nAlrao eventually reaches small loss. Going beyond that simplified\nsituation would already require a general analysis of the precise\nSGD dynamics of neural networks with a single learning rate.\n\nAbout the impact of the learning rate range, and using [10^-5:10] for\nCIFAR versus [10^-3:10^2] for PennTreeBank: Fig 3 clearly shows that\nusing [10^-3;10^2] for CIFAR provides very similar results to [10^-5:10].\nSo we could have used the same interval. Our general prescription is to\nuse for Alrao the range of learning rates that would have classically\nbeen tested via grid search. Generally, recurrent networks tend to\nrequire larger learning rates than convolutional networks: we do not\nforbid the use of such expert knowledge in Alrao, and this is why we\ninitially tested [10^-3:10^2] for LSTMs.  These intervals are just the\nfirst that came to mind, and we did not tweak them to get better results.\n(Partly, the difference is just because these experiments were run by two\ndifferent coauthors).\n\nWhy use the same learning rate for all parts of an LSTM unit: The\nintuition is similar to fixing the learning rate per unit rather than per\nweight. We would rather have a mixture of fully-functioning units and\nnon-functioning units, than the presence of too-large weights inside\nevery unit, which may screw up all units.\n\nOther minor comments: thank you for pointing out these mistakes, they have\nbeen fixed.", "title": "Specific answers to your comments"}, "rklxDjxz07": {"type": "rebuttal", "replyto": "Hyly32Fqhm", "comment": "Thank you for your balanced review with pros and cons. Let us answer each\ncon and detailed comment in turn.\n\n#Cons:\n\nNo theoretical argument for convergence: The newly included theorem in Appendix B\nstates that for a simple cases such as logistic regression, with eta_min\nsmall enough, then Alrao eventually reaches the optimal loss.  (Going\nbeyond that simplified situation would already require a general analysis\nof the precise dynamics of neural networks with a single SGD learning rate,\nwhich is an open problem.)\n\nBeating Adam only once: We have introduced experiments on ImageNet with\nthree models. They show that Adam's behavior is less consistent than\nAlrao (including a model where Adam does not even start to learn). Thus,\nthe Alrao/Adam comparison on CIFAR is not an isolated occurrence.\n\nAlrao working only with SGD: We suspect a bad interaction between Alrao's\nlearning rate mechanism, and the adaptive learning rates used in\noptimizers fancier than SGD. We acknowledge this limitation; we have\nedited the text to introduce Alrao on SGD by default, not as a generic\nidea (though the tests with Alrao+Adam have been kept).\n\n#Detailed comments:\n\n(1) Efficiency compared to Adam: Please see the general comment above and\nthe new ImageNet experiments.\n\n(2) Influence of neurons with \"bad\" learning rates, too small or too\nlarge. 1/ Too small learning rates: they will result in neurons not\nchanging much from their initialization.  Such neurons effectively behave\nlike fixed random features. Our results in Appendix H show that including random\nfeatures does not hurt and indeed can improve training. 2/ Too large\nlearning rates: indeed we expect Alrao to work only if some mechanism\nprevents bad neurons from having an ubbounded influence on subsequent\nlayers. For instance, with sigmoid activation functions, the worse that\ncan happen is that a neuron gets stuck to activities 0 or 1 that are\ndecorrelated from the desired output signal. In that case, the subsequent\nlayers have to learn to just ignore these 0/1 activities, but the\nnetwork will not diverge. With ReLU and BatchNorm, activations keep a\nbounded variance thanks to the BatchNorm, and the worse that can happen\nis that activities become N(0,1) variables decorrelated from the output.\nThe effect would be comparable to noisy neurons, which can be ignored by\nsubsequent layers.\n\n(3) Details of architectures, experimental setup, data preprocessing: now\nadded in Appendix D.\n\n(4) Why a different mechanism on the last layer. On the last layer, each\nneuron corresponds directly to a class/label. If assigning one rate per class,\nan unlucky class could get a very small learning rate and never learn anything\nat all, or a large learning rate and immediately diverge. (For inner layers,\nthis is not a problem as a layer contains many neurons and the subsequent layers\ncan learn to listen to those neurons which have a good learning rate.)\nTo give another example, imagine that we are working on a regression problem,\nwith a unique output neuron: then, choosing a fixed learning rate at random for\nthis unique output neuron is clearly a bad idea. Our current method keeps\nseveral copies of the output, with several learning rates.\n\n(5) (a) and (b): fixed.\n\n(5) (c) Fig 2a plots a single run, Fig 2b plots an average of 3 runs. The\nresults are very consistent between runs, as shown by the standard deviations in Table 1.\nThe curves for VGG19 do look similar; we can add them in an appendix if you deem it necessary.\n\n(5) (d) Color code in Fig 3: After deliberation we have decided to keep a\nper-figure color code, which allows for better comparison of relative\nperformance inside each figure. (The scales for train error and for test\nerror can be quite different.)\n", "title": "Specific answers to your comments"}, "HyleC5eMR7": {"type": "rebuttal", "replyto": "S1fcnoR9K7", "comment": "We would like to thank the reviewers for their insightful questions and remarks, which spurred us towards \nincluding more substantial experimental support.\n\nAdditionally, a major concern of the reviewers seems to be that Alrao\nwould not be that useful in practice.  Let us point out that in all our\nexperiments with Alrao-SGD, not a single run failed to learn.  For us,\nAlrao's tuning-free robustness and its ability to provide results in a\nsingle run are major arguments in its favor.\n\nThe new version uploaded today adds to the text:\n- a series of experiments on ImageNet\n- a theoretical analysis of Alrao in a simple case (convex function, fixed preclassifier) \n- answers to the various remarks (detailed below)\n\nThe new Imagenet experiments involve three standard models (AlexNet,\nResNet50, DenseNet121). They largely confirm the robustness of Alrao,\nwith results comparable to best-adjusted SGD (and better in one\ninstance). [The table contains \"?\" and best bounds so far where some\nexperiments are still running. They will be completed in a few days.]\n\nOn the same models, default Adam has mitigated results: on one model it\ndoes not learn anything at all, and on the other two it first reaches\nvery good performance then diverges shortly thereafter.\n\nWe answer below to the more specific comments of each review.", "title": "Main changes in the revision: new experiments, and a theoretical analysis in a simple case"}, "rkg4HYxL27": {"type": "review", "replyto": "S1fcnoR9K7", "review": "In this paper the authors propose a method called \u201cAll learning rates at once\u201d (Alrao) which aims to save the time needed to tune learning rate for DNN models testing. The method sets individual learning rate to each feature in each layer of a network using the values sampled from truncated log-uniform distribution. The only cost of the method is the creation of several branches of the classifier layer. Each of the branches is trained with a predefined learning rate value, and the final predictions are obtained by model averaging. In the presented experiments Alrao demonstrates performance comparable to SGD with optimal learning rate and more stable results compared to Adam. The authors indicate limitations of Alrao caused by the overhead in the final layer which complicates the application of the method for models with large classifier layer.\n\nOverall, the paper is written clearly and organized well. However, Equation (2) needs to be corrected. The denominator in the normalizing constant of log-uniform distribution should be \\log\\eta_{max} - \\log\\eta_{min}.\n\nMy main concern is related to the experimental evaluation of the method. I find the experimental evidence for the effectiveness of Alrao insufficient. As the authors propose to employ the method to quickly evaluate models and select best models to further training it would be beneficial to have more results in order to ensure that the method is reliable in this setting. Other demonstrations which would show possibly that the method enhances performance of architecture search methods may emphasize significance of the proposed method. Also, more experiments comparing Alrao against sampling learning rates per weight are needed. Given the current results, it is still unclear whether the proposed method performs better. Finally, I recommend to include comments explaining how much more time is needed in practice to train model with Alrao compared to SGD training.", "title": "Interesting method, but more experimental results are needed", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}