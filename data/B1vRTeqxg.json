{"paper": {"title": "Learning Continuous Semantic Representations of Symbolic Expressions", "authors": ["Miltiadis Allamanis", "Pankajan Chanthirasegaran", "Pushmeet Kohli", "Charles Sutton"], "authorids": ["m.allamanis@ed.ac.uk", "pankajan.chanthirasegaran@ed.ac.uk", "pkohli@microsoft.com", "csutton@ed.ac.uk"], "summary": "Assign continuous vectors to logical and algebraic symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.", "abstract": "The question of how procedural knowledge is represented and inferred is a fundamental problem in machine learning and artificial intelligence. Recent work on program induction has proposed neural architectures, based on abstractions like stacks, Turing machines, and interpreters,  that operate on abstract computational machines or on execution traces. But the recursive abstraction that is central to procedural knowledge is perhaps most naturally represented by symbolic representations that have syntactic structure, such as logical expressions and source code. Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of mathematical and logical expressions. These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different. The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures. We perform an exhaustive evaluation  on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.\n", "keywords": ["Deep learning"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "As part of this meta-review, I read the paper and found some surprising claims, such as the somewhat poorly motivated claim that coercing the output of a sub-network be a unit vector my dividing it by its L2 norm is close to layer normalisation which is mathematically almost true, if the mean of of the activations is 0, and we accept a fixed offset in the calculation of the stddev, but conceptually a different form of normalisation. It is also curious that other methods of obtaining stable training in recursive networks, such as TreeLSTM (Zhu et al. 2015, Tai et al. 2015), were not compared to. None of these problems is particularly damning but it is slightly disappointing not to see these issues discussed in the review process.\n\nOverall, the reviews, which I found superficial in comparison to the other papers I am chairing, found the method proposed here sound, although some details lacked explanation. The consensus was that the general problem being addressed is interesting and timely, given the attention the topics of program induction and interpretation have been receiving in the community recently. There was also consensus that the setting the model was evaluated on was far too simple and unnatural, and that there is need for a more complex, task involving symbolic interpretation to validate the model. It is hard to tell, given all the design decisions made (l2-normalisation vs layer norm, softmax not working), whether the end product is tailored to the task at hand, and whether it will tell us something useful about how this approach generalises. I am inclined, on the basis of the reviewer's opinions of the setting and my own concerns outlined above, to recommend redirection to the workshop track."}, "review": {"Hk4_LdCUx": {"type": "rebuttal", "replyto": "S1jqpoGLe", "comment": "Sorry for the long wait. I just updated the paper including ROC and PR curves. You can find them in page 13 as Figures 7 and 8. As you can see from the curves, EqNet still performs very well compared to the other methods.", "title": "ROC and PR curves"}, "SkZ3y7tLl": {"type": "rebuttal", "replyto": "S1jqpoGLe", "comment": "I now understand your point. Yes, of course, a ROC/PR curve can be computed as you suggest. I need some time to get this evaluation going, but I'll try to update the paper adding these new graphs within the next few days.\n", "title": "ROC and PR curve"}, "r1o0SkaSg": {"type": "rebuttal", "replyto": "BkPLT2WVx", "comment": "\n\n>> \"I do not believe that you can really do better than the truth table for boolean expr\"\n\nHumans deciding on the equivalence of two expressions do not always need to build a truth table. Some times it is very easy to apply a number of standard transformations (for instance, De Morgan's laws in the case of boolean expressions) to convert one expressions to another. For instance, two very long expressions might become equivalent by just one application of a rule - in this case no truth table computation would be required. The neural networks might be learning to compute such transformations implicitly. \n\nWe agree that this is a simple setting. We chose it because we felt that any representation learning method for that attempts to approximately learn expression semantics should be able to handle it. As the reviewer notes, the method could learn to compute the truth table of symbolic expressions. Therefore we found it very striking that no standard architecture could learn to do this! The idea behind this paper is that finding a representation learning method that *can* handle this simple setting is a necessary first step to a continuous representation learning method for program semantics.\n", "title": "Response"}, "H1p9EJarl": {"type": "rebuttal", "replyto": "SyiqkXfNx", "comment": "Thank you for your comments. We will improve the motivation of subexpforce. We do compare between the model with and without the subexpforce loss in the text (page 7): \"When training the network with and without subexpression forcing, on average, the area under the curve (AUC) of the score k decreases by 16.8% on the SeenEqClass and 19.7% on the UnseenEqClass. This difference is smaller in the transfer setting of Figure 2b-i and Figure 2b-ii, where AUC decreases by 8.8% on average.\" Let us know if you'd be interested in seeing a lengthier comparison.\n\nRegarding our metric, \"score_k\" can be interpreted as recall at rank k. Since at test-time our problem has more of an unsupervised flavor, because unseen equivalence classes will be observed, it is hard to compute a ROC or a precision/recall metric. Can you clarify how would you suggest computing the precision/recall-ROC in this setting?\n\nWe understand your concern about imbalance of the equivalence classes. If we were to report both the macro-averaged score_k (first average per equivalence class and then average all equivalence classes) and the micro-averaged score_k (average the score_k of all points assigning equal weights to each expression), would that be sufficient to resolve your concern?\n", "title": "Response"}, "BJ4l4kpre": {"type": "rebuttal", "replyto": "Hkdc5tZVg", "comment": "Thanks for pointing the typo in Table 3 and the issue with Fig4. We'll address them.\n\nWe understand that the setting where the equivalence classes of two expressions are already known is limited. The reason that we focused on such a setting is that we found that it was already quite hard, and seemed to be a necessary first step to more realistic settings such as expression similarity in real compute programs. Although in this work we are learning semantic vector representations using equivalence, there are multiple ways of retrieving (training) expressions that are approximately equivalent (e.g. identical outputs on a set of identical inputs). However, even in those cases, current neural network architectures would still not be able to capture semantics. Hence, in this work we make the first step and search for architectures that capture semantics in a setting where equivalence is known. And although we show that EqNets perform the best, it suggests that we need future research to first solve this problem, before moving to noisy data.\n\nFor arbitrary expressions (e.g. code) the problem of determining equivalence is indeed undecidable, but for boolean and polynomial expressions it is not. To detect equivalence of boolean expressions we convert them to the canonical conjunctive normal form which is unique. We get the same behavior for polynomials where we can symbolically expand them and simplify them into the unique form v_0*a + v_1*b + v_2*c + v_3*a^2 + ...\n\nOur network is only \"residual-like\" (and does _not_ use the standard residual architecture) because the size of the input layer is different from the size of the output layer (Fig. 1b). For example, for a node with two children, the input would have size 2*D, while the output is of size D. Therefore, using the identity is not exactly possible. There are many possible design choices to solve this, so our choice is just one sensible option. Finally, although the multiplication with W_{o0,\\tau_n} contributes to diminishing gradients, we retain the important element of residual networks by _not_ using a non-linearity.\n", "title": "Response"}, "HJYCbEZEx": {"type": "rebuttal", "replyto": "rkzN3OeQg", "comment": "It is true that q_ei is unnormalized and with large norms the softmax can achieve high probabilities. In practice, we found that this leads to q_ei's that have norms ~10^6, which we believe that it causes overfitting explaining why the performance is a bit lower when we use the maximum likelihood objective, although the difference in performance was not huge.\n\nOur supervised objective is very similar to the classic classification objective on the seen equivalence classes, with the addition of having a margin.\n\nWe are not trying to capture inductive biases from data. Perhaps it would be more clear to say this. In large problems, especially those that are significantly larger from those we considered in the paper, it will be impossible to be exhaustive. So, the difference between learning-based methods for semantics and an exact (symbolic) solver\nfor the NP-hard SAT problem is that the learning-based method will be approximate in the sense that it will be more accurate for symbolic expressions that are more similar to those in the training data. Having said that, even for the small problems we considered in this paper, we found that it was very difficult to get good performance using standard neural architectures. This is why we had to implement new techniques that were better suited for semantic properties, i.e. that represent semantics compositionality, while still allowing high-curvature operations in the continuous semantic representation space (the resnet trick) and encouraging semantic representations to be approximately reversible (see previous answer on subexpforcing).\n\nIn principle, if our models were perfect, there would be a 1:1 correspondence between semantic locations and truth tables. In some cases, we can identify low-dimensional projections that correspond to single elements in the truth table (Fig 4a). However, note that we are able to generalize from training on small expressions to testing on deeper expressions (Figure 2b). So if there is a simple mapping between semantic representations and truth tables, then there is some evidence that we are learning a procedure for doing this, rather than a map that is specific to the examples in the training set.\n\nWe will further clarify these in the text.", "title": "Objective and Inductive biases"}, "r1ouWVW4l": {"type": "rebuttal", "replyto": "HyvImh17l", "comment": "Essentially, you may think of subexpforcing as a constraint that tries to enforce reversibility, when possible, encouraging the semantic representation of the child nodes to be predictable from the parent. This helps to unify the representations of semantically identical but syntactically different representations. The added effect of the subexpforce with the denoising autoencoder is that it forces the expression representation to lie on a low-rank manifold inducing a clustering-like behavior of the triplets (r_{left} op r_{right})=r_{out}.\n\nWe will try to illustrate this with an example. Imagine three expressions e_1=\"a\", e_2=\"a \\land (b \\lor \\lnot b)\" and e_3=\"c\". Notice that \"e_1 \\land e_3\" is identical to \"e_2 \\land e_3\" and assume that both expressions have the exact same SemVec r_{out} but the SemVecs of e_1 and e_2 are far away even though e_1 and e_2 belong to the same equivalence\nclass. We observed this situation in practice with the simple TreeNNs. With subexpforce, we ask the optimization procedure to push r_{e_1} and r_{e_2} close to each other. This is achieved by asking the model to predict the representation of X (X \\land e_3=e_{out}) given the representation of e_3 (r_3) and e_out (r_{out}), ie. to make this reversible.\nThe network then predicts a single representation for X, r_X and pulls r_1 and r_2 towards the same location r_X.\n\nWe will make the subexpforce explanation more clear in the paper too.", "title": "SubExpForce"}, "rkzN3OeQg": {"type": "review", "replyto": "B1vRTeqxg", "review": "I am not sure to get why maximizing the likelihood of the data (Eq. 1) would lead to bad performance. The output of TreeNN is normalized, but not q_ei, is that correct? So couldn't high probabilities be achieved with large q_ei?\n\nI also do not get why the supervised objective is different from classification at train time (I understand that at test time, unseen equivalence classes are considered).\n\nFinally, in the introduction, the authors state that the goal of the paper is to develop models that will capture \"inductive biases\" from the data. What kind of biases are considered? It seems to me that the generated data lead to hard problems: it would probably be hard for a human to solve the boolean tasks, without computing the truth table of the expressions. Is this what the model is implicitly doing?The goal of this paper is to learn vector representation of boolean and polynomial expressions, such that equivalent expressions have similar representations.\n\nThe model proposed in the paper is based on recursive neural network, as introduced by Socher et al. (2012). Given the syntactic parse tree of a formula (either boolean or polynomial), the representation for a node is obtained by applying a MLP on the representation of the children. This process is applied recursively to obtain the representation of the full expression. Contrary to Socher et al. (2012), this paper proposes to use more than one layer (this is especially important to capture XOR operation, which is not surprising at all). The paper also introduces a reconstruction error (called SubexpForce), so that the expression of children can be recovered from the expression of the parent (if I understood correctly). The model is trained using a classification loss, where the label of a given expression corresponds to its equivalence class. The method is then evaluated on randomly generated data, and compared to baselines such as tf-idf, GRU RNN or standard recursive neural network.\n\nWhile I do agree with the authors that learning good representation for symbolic expressions (and to capture compositionality) is an important task, I am not entirely convinced by the experimental setting proposed in this paper. Indeed, as stated in the paper, the task of deciding if two boolean expressions are equivalent is NP-hard, and I do not understand if the model can do better than implicitly computing the truth table of expressions. While sometimes a bit hard to follow, the paper is technically sound. In particular, the proposed model is well adapted to the problem and outperforms the baselines.\n\npros:\n - the model is relatively simple and sound.\n - using a classification loss over equivalence classes (should be compared with using similarity).\n\ncons:\n - not convinced by the setting: I do not believe that you can really do better than the truth table for boolean expr (or computing the value of the polynomial expression for randomly chosen points in [0, 1]^n).\n - some part of the paper are a bit hard to follow (e.g. justification of the SubexpForce, discussion of why softmax does not work, etc...).\n - comparison between classification loss and similarity loss is missing.", "title": "Inductive biases", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BkPLT2WVx": {"type": "review", "replyto": "B1vRTeqxg", "review": "I am not sure to get why maximizing the likelihood of the data (Eq. 1) would lead to bad performance. The output of TreeNN is normalized, but not q_ei, is that correct? So couldn't high probabilities be achieved with large q_ei?\n\nI also do not get why the supervised objective is different from classification at train time (I understand that at test time, unseen equivalence classes are considered).\n\nFinally, in the introduction, the authors state that the goal of the paper is to develop models that will capture \"inductive biases\" from the data. What kind of biases are considered? It seems to me that the generated data lead to hard problems: it would probably be hard for a human to solve the boolean tasks, without computing the truth table of the expressions. Is this what the model is implicitly doing?The goal of this paper is to learn vector representation of boolean and polynomial expressions, such that equivalent expressions have similar representations.\n\nThe model proposed in the paper is based on recursive neural network, as introduced by Socher et al. (2012). Given the syntactic parse tree of a formula (either boolean or polynomial), the representation for a node is obtained by applying a MLP on the representation of the children. This process is applied recursively to obtain the representation of the full expression. Contrary to Socher et al. (2012), this paper proposes to use more than one layer (this is especially important to capture XOR operation, which is not surprising at all). The paper also introduces a reconstruction error (called SubexpForce), so that the expression of children can be recovered from the expression of the parent (if I understood correctly). The model is trained using a classification loss, where the label of a given expression corresponds to its equivalence class. The method is then evaluated on randomly generated data, and compared to baselines such as tf-idf, GRU RNN or standard recursive neural network.\n\nWhile I do agree with the authors that learning good representation for symbolic expressions (and to capture compositionality) is an important task, I am not entirely convinced by the experimental setting proposed in this paper. Indeed, as stated in the paper, the task of deciding if two boolean expressions are equivalent is NP-hard, and I do not understand if the model can do better than implicitly computing the truth table of expressions. While sometimes a bit hard to follow, the paper is technically sound. In particular, the proposed model is well adapted to the problem and outperforms the baselines.\n\npros:\n - the model is relatively simple and sound.\n - using a classification loss over equivalence classes (should be compared with using similarity).\n\ncons:\n - not convinced by the setting: I do not believe that you can really do better than the truth table for boolean expr (or computing the value of the polynomial expression for randomly chosen points in [0, 1]^n).\n - some part of the paper are a bit hard to follow (e.g. justification of the SubexpForce, discussion of why softmax does not work, etc...).\n - comparison between classification loss and similarity loss is missing.", "title": "Inductive biases", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HyvImh17l": {"type": "review", "replyto": "B1vRTeqxg", "review": "Hi,\ncould you please motivate the intuition behind subexpforce better. I am having trouble following the end of section 2.1.The authors propose a new model to learn symbolic expression representations. They do a reasonably extensive evaluation with similar approaches and motivate their approach well.\n\nAs expressed in the preliminary questions, I think the authors could improve the motivation for their subexpforce loss.\n\nAt the top of page 6 the authors mention that they compare to two-layer MLP w/o residual connections. I think having a direct comparison between a model with and w/o the subexpforce loss would be helpful too and should be included (i.e. keep the residual connections and normalization).\n\nMy main concern is the evaluation \"score\". It appears to be precision on a per query basis. I believe a more standard metric, precision-recall or roc would be more informative. In particular the chosen metric is expected to perform better when the equivalence classes are larger, since this isn't taken into account in the denominator, but the likelihood of a random expression matching the query increases.", "title": "Clarification of SubExpForce", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SyiqkXfNx": {"type": "review", "replyto": "B1vRTeqxg", "review": "Hi,\ncould you please motivate the intuition behind subexpforce better. I am having trouble following the end of section 2.1.The authors propose a new model to learn symbolic expression representations. They do a reasonably extensive evaluation with similar approaches and motivate their approach well.\n\nAs expressed in the preliminary questions, I think the authors could improve the motivation for their subexpforce loss.\n\nAt the top of page 6 the authors mention that they compare to two-layer MLP w/o residual connections. I think having a direct comparison between a model with and w/o the subexpforce loss would be helpful too and should be included (i.e. keep the residual connections and normalization).\n\nMy main concern is the evaluation \"score\". It appears to be precision on a per query basis. I believe a more standard metric, precision-recall or roc would be more informative. In particular the chosen metric is expected to perform better when the equivalence classes are larger, since this isn't taken into account in the denominator, but the likelihood of a random expression matching the query increases.", "title": "Clarification of SubExpForce", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}