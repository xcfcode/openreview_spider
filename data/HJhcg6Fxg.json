{"paper": {"title": "Binary Paragraph Vectors", "authors": ["Karol Grzegorczyk", "Marcin Kurdziel"], "authorids": ["kgr@agh.edu.pl", "kurdziel@agh.edu.pl"], "summary": "Learning short codes for text documents with Binary Paragraph Vectors.", "abstract": "Recently Le & Mikolov described two log-linear models, called Paragraph Vector, that can be used to learn state-of-the-art distributed representations of documents. Inspired by this work, we present Binary Paragraph Vector models: simple neural networks that learn short binary codes for fast information retrieval. We show that binary paragraph vectors outperform autoencoder-based binary codes, despite using fewer bits. We also evaluate their precision in transfer learning settings, where binary codes are inferred for documents unrelated to the training corpus. Results from these experiments indicate that binary paragraph vectors can capture semantics relevant for various domain-specific documents. Finally, we present a model that simultaneously learns short binary codes and longer, real-valued representations. This model can be used to rapidly retrieve a short list of highly relevant documents from a large document collection.", "keywords": ["Natural language processing", "Transfer Learning"]}, "meta": {"decision": "Reject", "comment": "This paper proposes to binarize Paragraph Vector distributed representations in an end-to-end framework. Experiments demonstrate that this beats autoencoder-based binary codes. However, the performance is similar to using paragraph vectors followed by existing binarization techniques, failing to show an advantage of training end-to-end. Therefore, the novel contribution seems too limited for publication."}, "review": {"rkszh7ove": {"type": "rebuttal", "replyto": "Bk5etL8Dx", "comment": "Dear Reviewer,\n\nThank you for your comment.\n\nRegarding the simplicity of end-to-end training (i.e. Binary PV) vs. a two-stage approach (first infer real-valued paragraph vectors and then binarize with, e.g., ITQ), we would argue that the end-to-end training is simpler. From our experience, the training and inference time in Binary PV models is almost identical to that of the original paragraph vectors. We then obtain binary codes with no additional steps. Also, Binary PV are simple to implement - extending PV models to binary variants requires just adding a sigmoid layer with rounding.", "title": "RE: RE: Responses"}, "r1fbeLsLg": {"type": "rebuttal", "replyto": "ry0xzkNEx", "comment": "Dear Reviewer,\n\nThank you for your review.\n\nWe extended our experimental results with comparison against random hyperplane projection LSH and iterative quantization. These results were added to Table 2 and are discussed in the paragraph above the table. In short, our results show no benefit from using a separate algorithm for binarization.\n\nAs you suggested, we also evaluated several gradient estimators for stochastic binary neurons (SBN), namely:\n  - plain straight-through (ST) estimator (dSBN(a)/da = 1),\n  - modified ST estimator (dSBN(a)/da = sigma'(a)),\n  - slope-annealed variants of the above two (proposed by Chung et al. in \"Hierarchical Multiscale Recurrent Neural Networks\").\nThe best results in this settings were obtained with slope-annealed variant of the plain ST estimator. Specifically, we observed better top-hits precision on 20NG dataset (compared to deterministic Krizhevsky's binarization). However, this did not translate to an improved MAP, due to lower precision at higher recall levels. We did not observe any improvement over Krizhevsky's method on the larger RCV1 dataset. Therefore we left Krizhevsky's method as our binarization approach.\n\nRegarding the length of binary codes, in the original version of our paper we focused on indexing directly with short codes. To compare with semantic hashing, we also reported results for 128-bit codes. As you pointed out, codes longer then 32 bits are feasible, e.g. by using Norouzi et al. multi-index hashing. However, with longer codes we also need to use larger search radii. Norouzi et al report search radii of around 15-bits for 128-bit codes (assuming 10-NN lookup over a 1B database). This gives radius/code_size of approx. 0.11, at which point Norouzi's method scales with the square root of the database size. Summing up, we reworded parts of our paper where we discuss the code size, to not exclude codes larger than 32-bits.", "title": "Response"}, "r1M2xLo8l": {"type": "rebuttal", "replyto": "B1xDkjtBg", "comment": "Dear Reviewer,\n\nThank you for your review.\n\nWe agree that indexing data structures are important parts of information retrieval systems. However, in this work we are mainly interested in learning good binary representation of text. Such representation can be used for document retrieval (and indeed, this is a proxy to the code quality in our paper) but can also serve other purposes: e.g. can be used for fast comparison of hashed data items or fast duplicate detection. Also, while we do not discuss efficiency of binary codes in our work, this has been looked at by Salakhutdinov & Hinton in their semantic hashing paper (section 4.2). We do not rule out investigating indexing methods for paragraph vectors in future research.\n\nWe added information regarding training loss (which is a sampled softmax) on the figures depicting our models. The linear projection is not merged with embedding lookup, because in Real-Binary model we want to explicitly infer both short binary code and high-dimensional real-valued representation.\n\nIn Binary PV-DM model we may want to use a different number of dimensions for word vectors and binary codes (e.g. to infer short binary codes). When the context passed to the sigmoid layer is a concatenation of word vectors and (pre-sigmoid) document vectors, their dimensionality does not need to match. This wouldn't be the case if, instead, we used a sum of relevant vectors as the context passed to the sigmoid non-linearity.\n\nRegarding the experimental setup, we added a sentence at the end of 2nd paragraph of section 3, explaining that we use the same benchmark datasets and quality measures that were used in semantic hashing paper. We also explained in the caption of Table 4 that 300D real-valued vectors are compared using cosine similarity. Furthermore, Table 4 now also reports performance of high-dimensional embeddings (from Real-Binary model) without pre-filtering.\n\nRegarding the transfer learning, despite its wide scope, Wikipedia follows specific style and language, which is not representative for many other domains (e.g. newsgroup posts). In the same spirit networks pre-trained on ImageNet are frequently fine-tuned for domain-specific classification.", "title": "Response"}, "SkiH1UoUe": {"type": "rebuttal", "replyto": "HyEBcqGVe", "comment": "Dear Reviewer,\n\nThank you for your review.\n\nAs you suggested, a comparison against semantic hashing was added to Figure 4. Regarding your remark on the Reuters corpus used in the semantic hashing paper, Salakhutdinov & Hinton mistakenly identified it as RCV2 in the conference version of their paper. Journal version of the paper (published in 2009, and cited in out work) states that they use RCV1-v2 (which is a cleansed version of RCV1). This also agrees with the size of the corpus reported in both conference and journal versions. In our experiments we also use version 2 of RCV1. We've added that detail to the paper.\n\nRegarding your remark that it would be interesting to see how many bits are required to match the performance of the continuous representation, we were not able to exactly match the performance of real-valued representation with binary codes (by simply increasing code size). One strength of PV networks is that they are in fact simple, yet quite effective, log-linear models. In Binary PV we must add a binarized sigmoid non-linearity, which can make training harder.\n\nAs your requested, a comparison against the continuous PV-DBOW with bigrams was added to Table 1 and in Figure 4. Furthermore, we extended Table 4 with comparison between Real-Binary PV results (variant B therein) and an alternative approach, where filtering is carried out with plain Binary PV-DBOW codes and ranking is carried out using standard paragraph vectors (variant C in Table 4). As could be expected, variant C yielded higher NDCG. However, we point out that this retrieval strategy requires a model (PV-DBOW) with approximately 10 times more parameters than Real-Binary PV-DBOW. We added a text in the paragraph discussing these results, where we point to a possible improvement in retrieval precision at the cost of using a bigger model.", "title": "Response"}, "SkwZRBi8x": {"type": "rebuttal", "replyto": "HJhcg6Fxg", "comment": "We have just uploaded an updated version of our paper. Below we list the most important changes. We will then follow with responses to the reviews.\n\n1) AnonReviewer3 pointed out that, apart from the Krizhevsky's binarization, one can also employ stochastic neurons in the coding layer. When comparing these two approaches we carried out a more thorough investigation of the dropout in the coding layers. In the initial experiments we employed small, 10% dropout. However, larger dropout rates (up to 50%) turned out to improve performance on the validation sets (in both binary and real-valued models). We therefore updated final results to reflect the dropout rates selected in validation experiments.\n\n2) We extended the comparison with baseline methods by adding results for two hashing techniques, namely random hyperplane projection and iterative quantization.\n\n3) Visualization of Binary PV codes (Figure 5 in the previous version of the paper) along with the corresponding text was moved to an appendix.", "title": "New revision"}, "SJyOCyw7x": {"type": "rebuttal", "replyto": "HJHB1SkQx", "comment": "Dear Reviewer,\n\nThank you for your helpful comment.\n\nFollowing your suggestion, we experimented with two baseline approaches, where binary codes were inferred from real-valued paragraph vectors using: a) an autoencoder with sigmoid coding layer (and Krizhevsky's binarization) and b) Gaussian-Bernoulli RBM. We focused on networks with the number of parameters similar to that of Binary PV-DBOW models. The results from these experiments are included as Table 2 in an updated version of our manuscript (uploaded today) and discussed in the paragraph above the table. Our results show no benefit from using this two unsupervised models for binarization.\n\nIn addition to the experiments described above, we also used an autoencoder and an RBM to construct binary codes from 300-dimensional paragraph vectors. We did not observe clear improvement from this approach, even though it uses 10x more parameters than Binary PV models. Specifically, with the autoencoder we observed lower MAP on both test sets. The RBM improved MAP on TNG, but on RCV it yielded performance similar to Binary PV-DBOW.", "title": "Comparison with two baseline approaches"}}}