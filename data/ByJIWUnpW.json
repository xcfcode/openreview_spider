{"paper": {"title": "Automatically Inferring Data Quality for Spatiotemporal Forecasting", "authors": ["Sungyong Seo", "Arash Mohegh", "George Ban-Weiss", "Yan Liu"], "authorids": ["sungyons@usc.edu", "mohegh@usc.edu", "banweiss@usc.edu", "yanliu.cs@usc.edu"], "summary": "We propose a method that infers the time-varying data quality level for spatiotemporal forecasting without explicitly assigned labels.", "abstract": "Spatiotemporal forecasting has become an increasingly important prediction task in machine learning and statistics due to its vast applications, such as climate modeling, traffic prediction, video caching predictions, and so on. While numerous studies have been conducted, most existing works assume that the data from different sources or across different locations are equally reliable. Due to cost, accessibility, or other factors, it is inevitable that the data quality could vary, which introduces significant biases into the model and leads to unreliable prediction results. The problem could be exacerbated in black-box prediction models, such as deep neural networks. In this paper, we propose a novel solution that can automatically infer data quality levels of different sources through local variations of spatiotemporal signals without explicit labels. Furthermore, we integrate the estimate of data quality level with graph convolutional networks to exploit their efficient structures. We evaluate our proposed method on forecasting temperatures in Los Angeles.", "keywords": ["spatiotemporal data", "graph convolutional network", "data quality"]}, "meta": {"decision": "Accept (Poster)", "comment": "With an 8-6-6 rating all reviewers agreed that this paper is past the threshold for acceptance.\n\nThe  quality of the paper appears to have increased during the review cycle due to interactions with the reviewers. The paper addresses issues related to the quality of heterogeneous data sources. The paper does this through the framework of graph convolutional networks (GCNs). The work proposes a data quality level concept defined at each vertex in a graph based on a local variation of the vertex. The quality level is used as a regularizer constant in the objective function. Experimental work shows that this formulation is important in the context of time-series prediction.\n\nExperiments are performed on a dataset that is less prominent in the ML and ICLR community, from two commercial weather services Weather Underground and WeatherBug; however, experiments with reasonable baseline models using a \"Forecasting mean absolute error (MAE)\" metric seem to be well done.\n\nThe biggest weakness of this work was a lack of comparison with some more traditional time-series modelling approaches. However, the authors added an auto-regressive model into the baselines used for comparison. Some more details on this model would help.\n\nI tend to agree with the author's assertion that: \"there is limited work in ICLR  on data quality, but it is definitely one essential hurdle for any representation learning model to work in practice. \".\n\nFor these reasons I recommend a poster.\n\n"}, "review": {"Hk7kJzcxM": {"type": "review", "replyto": "ByJIWUnpW", "review": "Update:\n\nI have read the rebuttal and the revised manuscript. Paper reads better and comparison to Auto-regression was added. This work presents a novel way of utilizing GCN and I believe it would be interesting to the community. In this regard, I have updated my rating.\n\nOn the downside, I still remain uncertain about the practical impact of this work. Results in Table 1 show that proposed method is capable of forecasting next hour temperature with about 0.45C mean absolute error. As no reference to any state of the art temperature forecasting method is given (i.e. what is the MAE of a weather app on a modern smartphone), I can not judge whether 0.45C is good or bad. Additionally, it would be interesting to see how well proposed method can deal with next day temperature forecasting.\n\n---------------------------------------------\nIn this paper authors develop a notion of data quality as the function of local variation of the graph nodes. The concept of local variation only utilizes the signals of the neighboring vertices and GCN is used to take into account broader neighborhoods of the nodes. Data quality then used to weight the loss terms for training of the LSTM network to forecast temperatures at weather stations.\n\nI liked the idea of using local variations of the graph signals as quality of the signal. It was new to me, but I am not very familiar with some of the related literature. I have one methodological and few experimental questions.\n\nMethodology:\nWhy did you decide to use GCN to capture the higher order neighborhoods? GCN does so intuitively, but it is not clear what exactly is happening due to non-linearities. What if you use graph polynomial filter instead [1] (i.e. linear combination of powers of the adjacency)? It can more evidently capture the K-hop neighborhood of a vertex.\n\nExperiments:\n- Could you please formalize the forecasting problem more rigorously. It is not easy to follow what information is used for training and testing. I'm not quite certain what \"Temperature is used as a target measurement, i.e., output of LSTM, and others including Temperature are used as input signals.\" means. I would expect that forecasting of temperature tomorrow is solely performed based on today's and past information about temperature and other measurements.\n- What are the measurement units in Table 1?\n- I would like to see comparison to some classical time series forecasting techniques, e.g. Gaussian Process regression and Auto-regressive models. Also some references and comparisons are needed to state-of-the-art weather forecasting techniques. These comparisons are crucial to see if the method is indeed practical.\n\nPlease consider proofreading the draft. There are occasional typos and excessively long wordings.\n\n[1] Aliaksei Sandryhaila and Jos\u00e9 MF Moura. Discrete signal processing on graphs. IEEE transactions\non signal processing, 61(7):1644\u20131656, 2013.", "title": "Interesting work. Good novelty component. Uncertain about practical impact.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "r16AndOEf": {"type": "rebuttal", "replyto": "S1GlLvu4G", "comment": "Thanks for your pointing out. \n\nYes, the unit in Table 1 is Celcius (C). As you mentioned, we normalize features and responses, and feed them into our model. Once we get the predicted values (which are in a normalized range) from our model, we do denormalization (inverse normalization) to recover the Celcius unit to report MAE results.\nThe predicted values are denormalized by the constants (standard deviation and mean) from the training dataset.", "title": "Re: Measurement units in Table 1"}, "rJDUzhtxf": {"type": "review", "replyto": "ByJIWUnpW", "review": "The paper is an application of neural nets to data quality assessment. The authors introduce a new definition of data quality that relies on the notion of local variation defined in (Zhou and Sch\u00f6lkopf, 2004), and they extend it to multiple heterogenous data sources. The data quality function is learned using a GCN as defined in (Kipf and Welling, 2016).\n \n1) How many neighbors are used in the experiments? Is this fixed or is defined purely by the Gaussian kernel weights as mentioned in 4.2? Setting all weights less than 0.9 to zero seems quite abrupt. Could you provide a reason for this? How many neighbors fit in this range?\n2) How many data points? What is the temporal resolution of your data (every day/hour/minute/etc.)? What is the value of N, T? \n3) The bandwidth of the Gaussian kernel (\\gamma) is quite different (0.2 and 0.6) for the two datasets from Weather Underground (WU) and Weather Bug (WB) (sect. 4.2). The kernel is computed on the features (e.g., latitude, longitude, vegetation fraction, etc.). Location, longitude, distance from coast, etc. are the same no matter the data source (WU or WB). Maybe the way they compute other features (e.g., vegetation fraction) vary slightly, but I would guess the \\gamma should be (roughly) the same. \n4) Are the features (e.g., latitude, longitude, vegetation fraction, etc.) normalized in the kernel? If they are given equal weight (which is in itself questionable) they should be normalized, otherwise some of them will always dominate the distances. If they were indeed normalized, that should be made clear in the paper.\n5) Why do you choose the summer months? How does the framework perform on other months and other meteorological signals except temperature? The application is very nice and complex, but I find that the experiments are a little bit too limited. \n6) The notations w and W are used for different things, and that is slightly confusing. Usually one is used as a matrix notation of the other.\n7) I would tend to associate data quality with how noisy observations are at a certain node, and not heterogeneity. It would be good to add some discussion on noise in the paper. How do you define an anomaly in 5.2? Is it a spatial or temporal anomaly? Not sure I understand the difference between an anomaly and a bridge node. \n8) \u201cA bridge node highly likely has a lower data quality level due to the heterogeneity\u201d. I would think a bridge node is very relevant, and I don't necessarily see it as having a lower data quality. This approach seems to give more weight to very similar data, while discarding the transitions, which in a meteorological setting, could be the most relevant ?! \n9) Update the references, some papers have updated information (e.g., Bruna et al. - ICLR 2014, Kipf and Welling \u2013 ICLR 2017, etc.).\n\nQuality \u2013 The experiments need more work and editing as mentioned in the comments.  \nClarity \u2013 The framework is fairly clearly presented, however the English needs significant improvement. \nOriginality \u2013 The paper is a nice application of machine learning and neural nets to data quality assessment, and the forecasting application is relevant and challenging. However the paper mostly provides a framework that relies on existing work.\nSignificance \u2013 While the paper could be relevant to data quality applications by introducing advanced machine learning techniques, it has limited reach outside the field. Maybe publish it in a data quality conference/journal.", "title": "Nice idea and application, but needs significant work before publication.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ry07x_9xG": {"type": "review", "replyto": "ByJIWUnpW", "review": "Summary of the reviews:\nPros:\n\u2022\tA novel way to evaluate the quality of heterogeneous data sources.\n\u2022\tAn interesting way to put the data quality measurement as a regularization term in the objective function.\nCons:\n\u2022\tSince the data quality is a function of local variation, it is unclear about the advantage of the proposed data quality regularization versus using a simple moving average regularization or local smoothness regularization.\n\u2022\tIt is unclear about the advantage of using the Multi-layer graph convolutional networks versus two na\u00efve settings for graph construction + simple LSTM, see detailed comments D1\n\nDetailed comments:\nD1: Compared to the proposed approaches, there are two alternative na\u00efve ways: 1) Instead of construct the similarity graph with Gaussian kernel and associated each vertex with different types of time-series, we can also construct one unified similarity graph that is a weighted combination of different types of data sources and then apply traditional LSTM; 2) During the GCN phases, one can apply type-aware random walk as an extension to the deep walk that can only handle a single source of data. It is unclear about the advantage of using the Multi-layer graph convolutional networks versus these two na\u00efve settings. Either some discussions or empirical comparisons would be a plus.\n", "title": "This work proposed a new way to evaluate the quality of different data sources with the time-vary graph model, that is, the quality of data sources (e.g., time-series sequences) associated with each vertex is defined as a function of local variation at each vertex. In addition, the quality level is also used as a regularization term in the objective function for any general applications. The data quality evaluation is trained using the Multi-layer graph convolutional networks. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJCAdVTQM": {"type": "rebuttal", "replyto": "ByIwKojXM", "comment": "We really appreciate your comments and updated the following.\n\n\"The explanations added in the paper and in the authors comments are valuable and they should be included as much as possible in future versions of the paper, especially answers to questions 7 and 8 concerning the behavior of bridge nodes and anomalies.\"\n>> We added our explanation (Section 5.2 and 5.3) about the behavior of bridge nodes and anomalies in the discussion section (about Q7 and Q8) based on our responses to previous comments. Furthermore, we proofread our discussion to help make the explanation clear. \n\n\"The confusing sentence at Q8 has not been changed in the paper, but the authors explanation in the comments is very useful and should be added to the paper.\"\n>> As you suggested, we updated Section 5.3 to make it clear and less confusing. We added our explanation of how the data quality level is affected by neighboring nodes and quantitatively showed that the level is inferred correctly.\n\n\"Answer to Q6: I understand the notations\u2014my comment was a suggestion to increase readability of the paper by using different letters for different things, not just lowercase vs uppercase vs bold.\"\n>> About Q6: We changed the data quality level notation to $s_i$ instead of $w_i$ for readability. Bold $s$ and $s_i$ are used to point out the data quality level, and $W$ is only used for edge weights.\n\n\"Forecasting applications are extremely important and challenging, but the experimental section needs to be better explained and further developed. Some of the sentences are still confusing and this makes it hard to follow some of the arguments. Please consider having the manuscript edited by someone who is an English expert.\"\n>> We added details of our experiments (including experimental setting, discussion of results) and edited the manuscript carefully (especially the Abstract, Introduction, Experiments, Results, and Conclusion sections) with the help of a native English speaker to improve its readability. \n\nThank you for your comments", "title": "Re: Revised review"}, "rktRm4eQz": {"type": "rebuttal", "replyto": "rJDUzhtxf", "comment": "6) >>\nWe use the lowercase w for data quality levels for each station and the upper case W for edge weights. The bold w is the vector having N elements and w_i is the data quality level of an i-th weather station. We have fixed the confusions in the updated draft. \n\n7) >>\n- In our paper, we assume that if two connected nodes (weather stations) are located in similar spatial features, these nodes highly likely observe similar meteorological measurements. Thus, if two connected nodes provide very different observations, we think that these observations are not reliable and could be noisy. Furthermore, if there is a group of connected nodes and some nodes observe significantly different measurements with other nodes in the group, we can say that the measurements are too heterogeneous and not reliable. In other words, the data quality level is associated with noisy as well as heterogeneous observations.\n- For the anomaly detection, what we would like to propose is that the embeddings from our model can be used to visualize nodes based on their connectivity and observations. For example, in Figure 3, the red dot (v_22) is connected with the green dots (v_19, v_20, v_21, v_23, v_25, v_29). Since these nodes have similar spatial features and are connected, it is expected to have similar observations. At t=0, the distribution of the nodes seems like a cluster. However, v_25 is far away from other green nodes and the red node at t=4. There are two possible reasons. First, observations of v_25 at t=4 may be too different with those of other green nodes and the red node. Second, observations of a node (v_4) that is only connected to v_25 (not other green nodes and the red node) might be too noisy. In the first case, since it violates our assumption (v_25's observations should be similar with those of another green node.), the observations of v_25 at t=4 might be noisy or not reliable. In the second case, the observations of v_4 at t=4 might be noisy. Thus, we can detect potentially anomalous nodes by looking at the distribution of nodes. (A bridge node is not necessary to be an anomaly.)\n- The anomaly comes from temporal observations by comparing to neighbor observations. Thus, two aspects (spatial and temporal) are jointly considered. \n\n8) >> \nThanks for your pointing out. We agree that the sentence you quote can cause some confusion. A bridge node in our paper is considered as a node connecting two (or more) clusters that consist of nodes having similar features. As a result, a bridge node is affected by two (or more) different groups of nodes simultaneously, and thus, the quality level at the bridge node is more susceptible than those of other nodes. However, it doesn\u2019t directly mean that it is necessary for a bridge node to have a lower data quality level.\n\n9) >>\nThanks for your suggestion and we update the conference information in the revised version. \n\nQuality, Clarity, Originality, Significance >>\nWe do not agree with the comment on significance. Our paper proposes a novel solution to automatically infer data quality levels based on local variations of graph signals and demonstrate that the quality levels can be used to reduce forecasting error and detect potentially anomalous observations. It provides a new idea on inferring interpretable quantity without explicit labels. We agree that there is limited work in ICLR  on data quality, but it is definitely one essential hurdle for any representation learning model to work in practice. ", "title": "Re: Reviewer 1"}, "HyChXVl7G": {"type": "rebuttal", "replyto": "rJDUzhtxf", "comment": "We sincerely thank the reviewer for the insightful comments and suggestions. We would like to stress that assessing and mitigating heterogeneity of data quality is an extremely important but less-studied topic in machine learning. Our work aims at positioning the task and providing one possible solution to address this important problem. The novelty of the paper lies in this important contribution rather than purely the novelty of the proposed model.\n\nBelow is our response to the questions. \n1) and 3) >>\nThanks for pointing out this potentially confusing issues. To build a symmetric adjacency matrix, we did not set a fixed number of neighbors. Instead, we defined the neighbors by the Gaussian kernel weights with the weight threshold. Since the numbers of the weather stations in WeatherBug(WB) and Weather Underground(WU) are different (#WU:42, #WB:159), the average numbers of neighbors in the datasets are too different under the same bandwidth(\\gamma). We aim at evaluating our model under similar degree distribution (similar topology) and therefore, we adjust the bandwidth to make the average numbers of adjacent neighbors in two dataset be similar under the 0.9 threshold. The average degrees are 6.0 and 7.5 for WU and WB, respectively.\n\n2) >>\n- Each weather station in the data sources (WU and WB) has a different temporal resolution. Some weather stations have observed measurements in every 5 minutes but other weather stations have operated sensors in every 30 minutes. So, we fix the temporal granularity as 1 hour and aggregate observations in each hour by averaging them. \n- N is the total number of weather stations and #WU:42, #WB:159. \n- T is the total length of signals and it is 744(hours) (24(hours/day)*31(day)) for July and August.\n\n4) >>\nThis is a very good point. Yes, we are aware that features represented in a large number (e.g., latitude ~ 34 degree) can dominate features represented in a small number (e.g., vegetation fraction < 1.0) , which is exactly what we want to avoid. Thus, it is a must to normalize the features before using them for computing the distances. As you point out, equal weighting may be not perfectly correct, however, this is what we can do without any domain knowledge.\n\n5) >>\n- It is a very good point too. Interestingly, the Los Angeles area contains many microclimates, which means that the daytime temperatures can vary as much as 36\u00b0F (19\u00b0C) between inland areas such as the San Fernando Valley and the coastal Los Angeles Basin. The temperature differences between different areas are more obvious during the summer season (than other seasons). For example, the average high temperature (\u00b0F) can vary as much as 26\u00b0F for July and August on 9 different regions (Downtown LA, LAX, Santa Monica, Culver City, Long Beach, San Fernando Valley, Burbank, Santa Ana, and Anaheim). In contrast, it varies only 6\u00b0F for December and January in the same regions. Thus, it is more challenging to predict temperatures in the summer season, which is why we choose two months (7 and 8) that often shows extreme fluctuations of temperatures.\n- The bigger picture of our work is investigating urban heat island effect in Los Angeles areas. Temperature is the most important factor in urban heat island and exhibits more variations (and more difficult to predict). In contrast,  other observations, such as Relative Humidity or Precipitation, are very stable in Los Angeles areas. They are very easy to predict and therefore cannot justify for a complex model.\n\n", "title": "Re: Reviewer 1"}, "S1eWfNlmz": {"type": "rebuttal", "replyto": "Hk7kJzcxM", "comment": "Thank you for your comments and suggestions to improve the paper. Below are our responses to the main points of your comments:  \n\nMethodology\n>> It is commonly believed that the earth climate system is a complex one involving many nonlinear interactions between climate factors. Neural networks have proven to be an effective solution to capture nonlinear dependencies in many applications. To capture nonlinearity, we use a nonlinear activation function (ReLU). Furthermore, multiple layers are more effective to learn features at various levels of abstraction. \nGraph polynomial filter (Equation (7) in [1]) has the similar form as GCN filter operations, but it does not consider nonlinearity. GCN is also based on the polynomial of the adjacent matrix, it equivalently handles the K-hop neighborhood of a vertex.\n\nExperiments\n>> Could you please formalize the forecasting problem more rigorously.\nThanks for pointing out the confusion. For the sentence you quoted, we did intend to describe as your suggested - In other words, future temperatures are forecasted by looking at past temperatures as well as other meteorological observations. We have updated the expression more clearly.\n\n>> What are the measurement units in Table 1?\nWe have added the details of the climate dataset in the appendix in the updated draft. \n\n>>  I would like to see comparison to some classical time series forecasting techniques, e.g. Gaussian Process regression and Auto-regressive models. \nThanks for the baseline suggestions. We have added the results by auto-regressive for robust comparison in the updated draft. \n\nThanks for your comments, and we proofread the draft and make it more clear.", "title": "Re: Reviewer 3"}, "ry6rgEgXG": {"type": "rebuttal", "replyto": "ry07x_9xG", "comment": "Thank you for your comments and suggestions to improve the paper. Below are our responses to the main points of your comments:  \n\nCons 1.\n>> It is an interesting point.  A simple moving average or local smoothness regularization may improve the forecasting performance. However, these regularizations cannot infer the data quality level which is useful to understand time-varying graph signals.\n\nCons 2 and Detailed comments\n>> Thanks for the suggestions of more baselines. For the former one, we do not have a specific way to get the unified similarity graph based on the different types of time-series. That is why we only consider the spatial (static) features to construct the graph structure. But it could be an interesting direction to explore. \nIt is a great idea to think different ways to cover K-hop neighbor nodes. Random deep walk is one of the ways. Although we haven\u2019t compared our method to these random-walk-based methods, the multi-layer GCNs with the data quality networks are more flexible to learn latent interactions between temporal observations due to their nonlinearity and compositional property. It may be possible to improve forecasting quality by using the random walk method, however, we focus on the automatically inferring method of the data quality level, which is not easily achievable by other ways.", "title": "Re: Reviewer 2"}}}