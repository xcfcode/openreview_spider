{"paper": {"title": "Infinite-horizon Off-Policy Policy Evaluation with Multiple Behavior Policies", "authors": ["Xinyun Chen", "Lu Wang", "Yizhe Hang", "Heng Ge", "Hongyuan Zha"], "authorids": ["chenxinyun@cuhk.edu.cn", "luwang@stu.ecnu.edu.cn", "hangyhan@mail.ustc.edu.cn", "hengge@mail.sdu.edu.cn", "zhahy@cuhk.edu.cn"], "summary": "A new partially policy-agnostic method for infinite-horizon off-policy policy evalution with multiple known or unknown behavior policies.", "abstract": "We consider off-policy policy evaluation when the trajectory data are generated by multiple behavior policies. Recent work has shown the key role played by the state or state-action stationary distribution corrections in the infinite horizon context for off-policy policy evaluation. We propose estimated mixture policy (EMP), a novel class of partially policy-agnostic methods to accurately estimate those quantities. With careful analysis, we show that EMP gives rise to estimates with reduced variance for estimating the state stationary distribution correction while it also offers a useful induction bias for estimating the state-action stationary distribution correction. In extensive experiments with both continuous and discrete environments, we demonstrate that our algorithm offers significantly improved accuracy compared to the state-of-the-art methods.", "keywords": ["off-policy policy evaluation", "multiple importance sampling", "kernel method", "variance reduction"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors present a method to address off-policy policy evaluation in the infinite horizon case, when the available data comes from multiple unknown behavior policies.  Their solution -- the estimated mixture policy -- combines recent ideas from both infinite horizon OPE and regression importance sampling, a recent importance sampling based method.  At first, the reviewers were concerned about writing clarity, feasibility in the continuous case, and comparisons to contemporary methods like DualDICE.  After the rebuttal period, the reviewers agreed that all the major issues had been addressed through clarifications, rewriting, code release, and additional empirical comparisons.  Thus, I recommend to accept this paper."}, "review": {"SJx9Xl7aYB": {"type": "review", "replyto": "rkgU1gHtvr", "review": "After rebuttal:\nThank author for the clarification. The new version looks better and I tend to accept the paper in the current version.\n=========\nThis paper provides a algorithm to solve infinite horizon off policy evaluation with multiple behavior policies by estimate a mixed policy under regression, and follows the same method of BCH. The intuition of using an estimated policy comes from Hanna et al. (2019) which shows that an estimated policy ratio can reduce variance even it introduces additional bias. The authors provide theoretical proof on that and arguing that their method is not worse than BCH one. Empirical results show that in general their method performs as good as previous baseline. I believe this method is novel and natural and worth investigating.\n\nTechnical Concerns:\nThe major concern I have is in continuous case, it is almost impossible to pre-assume a model for learning the mixed policy $\\hat{\\pi_0}$. For example, if the sample policies $\\pi_j$ are all Gaussians, then $\\pi_0$ according to equation (4) would be a complicated mixture distribution (not even a Gaussian mixture since it involves ratio of $d_{\\pi_j}$ which is hard to compute). If the model is not precise, we cannot achieve the bias/variance tradeoff where the bias introduce by model mismatching can be arbitrarily large.\nAnd according to the experimental details in appendix E, I didn't find any useful model assumption to address that issue. So my question would be: what model are you using when doing regression for $\\pi_0$?\n\nClarity Concern:\nThe written of the paper is not satisfactory. The major contribution should be highlight in section 4, which from my first time reading is very unclear. The key observation of equation (4) uses a recursive definition, where we define $\\pi_0$ using an undefined $d_{\\pi_0}$. I think you should rewrite $d_{\\pi_0}$ as the weighted summation of $d_{\\pi_j}$. And you should avoid repeating similar equation like (2) (3) (6) and (7) where you can cite equation (2) in general or use a short notation for that equation, otherwise it is hard to contract the contribution of the paper.\nThe experiment part is also unclear. Here's some questions: 1) How many repetitions you apply for each figure? It seems not smooth enough. 2) Which estimator you use for you regression? Maximum Likelihood Estimation? Which model you are using for each environment (I know for tabular MLE is count based)? 3) What is $\\pi_k$ in section E.3 equation (12)? How do you compute KL divergence in this equation for empirical distribution?\n\nSome minor issues:\n1. You should replace 'for all' to $\\forall$ when writing equation, like equation before (2), equation (6) and equation in proposition 4.\n2. You'd better to separate legend with figure in order to make the legend larger and figure clearer.\n\nOverall, I think this work is very novel and natural, but should give more consideration on the model selection. I tend to reject the paper by the current version and encourage the authors to submit to another conference after the revision.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 3}, "SklbL9CnFB": {"type": "review", "replyto": "rkgU1gHtvr", "review": "  *Synopsis*:\n  The main contribution of this paper is the development of estimated mixture policy (EMP), which takes ideas from the new off-policy policy evaluation infinite horizon estimators (i.e. Liu) and from a recent development in more traditional importance weight approaches using regression importance sampling (i.e. Hannah). This new method provides a nice extension of Liu's algorithm to many policies, and to when the policy is unknown. The paper provides some nice analysis inspired by Hannah. Finally, they provide empirical results.\n\n\n  *Review*:\n\n  While I think the method has potential interest to the community, I found the empirical results lacking (particularly in missing competitors). I also have some concerns about the theory as there seems to be many typos and consistency issues making much of it hard to follow. Overall, I think this paper is not quite up for publication, but if the authors address my consistency/missing proofs issues and provide a comparison to DualDice I will increase my score.\n\n  1. I don't find the reason provided for not including DualDICE compelling and think it is an important competitor here, as there are many similarities between the two methods.\n     - It would also be interesting to reproduce the results provided by Liu et al with the model based approach, and the on-policy oracle. I don't think these are as pressing as DualDice but still interesting.\n\n  2. There are many consistency issues with respect to notation, and some odd notation choices as compared to the rest of the literature:\n     - What is script \\epsilon in the equation in 2.1? It looks like it should be an expectation over d_\\pi, but I've never seen this notation before.\n     - The indicies of sums and sets often change between one based and zero based indexing. This should be unified (preferably to one based). For example, section 2.1 \\pi_j(j=0,1,2,...,m) and m=1 for one policy doesn't work. Either \\pi_j(j=0,1,2,...m-1) or m=0. This occurs throughout the proofs in the appendix as well.\n     - What is script \\Epsilon_\\theta? Do you mean script F_\\theta? And then what does it mean for \\theta_0 \\in E_1? There seems to be many definitions missing.\n\n  2.5. There are also some issues with the presentation of the theory over the consistency issues already mentioned.\n     - The assumptions and conditions for the theorems presented in the main text should be clearly specified in the theory statement.\n     - The proof to theorem 2 (i.e. in the appendix) should be provided.\n     - The proof of proposition 4 seems to be missing as well.\n\n  *Questions*\n  Q1 What are the properties of SADL? Why was this used instead of DualDICE in the comparison?\n\n  Q2 How would BCH do if we used the mixture policy as the behavior policy in the multiple behavior policy case? How would it compare to your method? This could be an interesting comparison, just to test if the lower MSE argument holds up in the multiple behavior policy case.\n\n  Q3 What is the meaning of partially-policy-agnostic? It is unclear to me how it is different from the policy-agnostic approaches. If all that is different is you are estimating the behavior to use in the usual infinite-horizon approach, should this be in a separate category from policy-agnostic approaches? (I would say probably not, but I think you could make a case for it).\n\n  Q4 \"Then, a single (s,a,s') tuple simply follows the marginal distribution...\". Is this trivially true?\n\n  *Minor comments not taken into account in the review*\n  - section 2.1 \"target policy \\pi via a pre-collected...\" -> remove \"a\"\n  - The layout of the related works section is a bit hard to follow.\n  - \"Recently, Nachum et. al. (2019) proposes DualDice\": proposes->proposed\n  - \"by their estimated values in two folds\": do you mean in two ways? This is unclear.\n  - Section 3.1: \"notation abusion\" -> notation abuse\n  - Equation right after equation 1: \"d_\\pi_0(s)\\pi(a|s)\" -> \"d_\\pi_0(s)\\pi_0(a|s)\"\n  - \"The derivation of kernel method are put in...\" -> \"The derivation of the kernel method is put in...\"\n  - \"we need introduce more notation\" -> \"we need to introduce more notation\"\n  - Section 4: \"detailed description on the data sample\": \"on\"->\"of\"\n  - \"In this light by pooling the data together....\" These two sentences should be put together.\n  - I would like if your theorems were restated in the appendix, for ease of reading.\n\n-----------\nPost Rebuttal\n\nI'd like to thank the author for their thorough response! Given the major additions to the paper including empirical comparisons and clarity for the theory I've decided to update my score to reflect my new feelings (i.e. to a 6). I think this paper is well worth accepting in its current form.\n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 2}, "SJe5dmAijS": {"type": "rebuttal", "replyto": "SklbL9CnFB", "comment": "Response to Major Comments:\n1.  \n- We got the released code and carried out a comparison study with DualDICE.\n- We implemented  a variation of BCH method by Liu et al for mutliple-behavior-policy cases, called BCH (pooled), using the exact value of all behavior policies. We now report the comparison results of our method EMP, BCH and DualDICE in both single- and multi-behavior-policy settings.\n- We use the estimation results by on-policy oracle with large trajectory length and number as the \u201ctrue-value\u201d. This is how we compute the MSE for EMP and other benchmark OPPE methods.\n\n2. \n- We have correct this typo on page 2.\n- We have unified the notation so that all indicies start from 1.\n- We have correct these typos. $\\mathcal{E}_\\theta$ is the correct notation for parameter space.\n- We also went through the mathematical part of the paper and corrected the typos we have found.\n\n2.5\n- We now cite the assumptions in the statement of the theorems. The detailed assumptions are stated and explained in Appendix B. We didn\u2019t directly include the full assumptions in the statement of the theorem mainly because of the space limit. Most of the assumptions actually involve technical details about the kernel method that solves the min-max problems, and they need space of half to one page.\n- We have removed the SADL algorithm in the revision, as it was used as an substitute of DualDICE in the previous version of the paper. So we also removed (the original) Theorem 2, which follows a very similar proof as the kernel-method derivation in Liu et. al.\n- Proposition 4 (now 3) has been proved in Veach & Guibas (1995). We include it mainly for self-containedness and refer to the original paper for the proof. We now cite Veach & Guibas (1995) more explicitly in the statement of Proposition 3.\n\nResponse to Questions:\nQ1 What are the properties of SADL? Why was this used instead of DualDICE in the comparison? \n- We are not aware of the DualDICE code releasing when we first submitted this paper. So we use SADL as a substitute of DualDICE. They are both policy-agnoistic and learn the state-action joint distribution correction.\n\nQ2 How would BCH do if we used the mixture policy as the behavior policy in the multiple behavior policy case? How would it compare to your method? This could be an interesting comparison, just to test if the lower MSE argument holds up in the multiple behavior policy case. \n- We implemented a multiple-behavior-policy version of Liu et al, using the information of all behavior policies. We call it BCH (pooled) and report the comparison results of our method EMP, BCH (pooled) and DualDICE.\n\nQ3 What is the meaning of partially-policy-agnostic? It is unclear to me how it is different from the policy-agnostic approaches. If all that is different is you are estimating the behavior to use in the usual infinite-horizon approach, should this be in a separate category from policy-agnostic approaches? (I would say probably not, but I think you could make a case for it). \n-We call EMP a partially policy-agnostic method in the sense that, although EMP does not require any information on the \u201cphysical\u201d behavior policies, it learns a \u201cvirtual\u201d policy, which is the mixture policy, defined formally in Section 4.1, and contains aggregated information about the \u201cphysical\u201d behavior polices. As a consequence, the accuracy of the \u201cvirtual\u201d policy learning (conceptually, whether the algorithm can effectively extract the aggregated information about the behavior policies) will affect the performance of EMP. We now add this explanation to the introduction part.\n\nQ4 \"Then, a single (s,a,s') tuple simply follows the marginal distribution...\". Is this trivially true? \n- We added a new Subsection 4.1 to more formally state our assumptions on the sample data collected from different behavior policies and the relevant distributions.\n\nResponse to Minor Comments:\n- We have corrected all the typos accordingly. As to the related work part, we added a few sentences of explanation at several places to improve the logic flow.\n", "title": "Response to Review #1"}, "H1e6OxCjjH": {"type": "rebuttal", "replyto": "SJx9Xl7aYB", "comment": "Response to Technical Concerns:\nIn our experiment, we use a neural network to approximate the mixture policy in continuous case and estimate the model by MLE. We have added a paragraph explaining this for the general algorithm in Section 4.2 and for the numerical experiment in Appendix D.2. We agree with the reviewer that when model is not precise, the bias will overwhelmed the variance reduction. We explicitly explained that the performance of EMP relies on the accuracy of policy learning in the introduction part. We think the problem of model uncertainty is interesting and more challenging, and probably requires a different theoretic framework, such as robust optimization, so we will leave this for further study. Therefore, in most part of the paper, to study the effect of policy learning on OPPE performance, we would like to focus on the cases that the policy can be well approximated by some parametric model, especially for theoretic analysis. \n\nResponse to Clarity Concerns:\n- We have modified Section 4 according to the comments of the reviewers. We added a subsection to more formally define the mixture policy pi_M and the mixture distribution d_M (to better distinguish from the single-behavior policy, we have also changed the notation.) We also provided additional theoretic properties of \\pi_M and d_M to provide more intuition behind the algorithm design.\n- We have removed equation (3) and (6). But we keep equation (7) (now becomes (6)) to make the description of EMP algorithm more self-contained. \n- We have also modified the experiment part. First, we added new comparison results to the state-of-art policy-agnostic method DualDICE. Second, we reorganized experiment part to make it more consistent with the theoretic analysis, hope this could convey clearer messages of the numerical experiments.\n\nResponse to Questions: \n1)How many repetitions you apply for each figure? It seems not smooth enough. \n-we have increased the number of repetitions by 3 times and updated the numerical results.\n2)Which estimator you use for you regression? Maximum Likelihood Estimation? Which model you are using for each environment (I know for tabular MLE is count based)?\n- For the three discrete environment, we used count-frequency to estimate the policy. For the continuous environment, we used a neural network to model the policy and MLE to estimate the model parameters. We have added a paragraph explaining our model and estimation methods in Appendix D.2. (E.2 in the previous version).\n3)What is in section E.3 equation (12)? How do you compute KL divergence in this equation for empirical distribution? \n- Equation (12) is used to defined the adjusted proportion of data from each policy. It involves the KL-divergence, which is computed by estimating the behavior policy by a parametric model. We have added some explanation in Section D.3 (E.3 in the previous version.) \n\nResponse to Minor Issues: \n1.You should replace 'for all' to when writing equation, like equation before (2), equation (6) and equation in proposition 4. \n- We have replaced \u2018for all\u2019 with \u2018\\forall\u2019 in the equations.\n2.You'd better to separate legend with figure in order to make the legend larger and figure clearer. \n- We have changed the format of figures accordingly.", "title": "Response to Review #2"}, "Bkl8G1RsjB": {"type": "rebuttal", "replyto": "Sygmew3xqH", "comment": "- We have added the missing citation Prenup et al 2000. \n\n-  To certain extend, our method shows the performance improvement of OPPEval algorithm by using certain representation model (especially in continuous case) to learn the specific information about the underlying behavior policies from data. Besides, there are applications where policy evaluation is the ultimate goal in itself [1]. Policy evaluation algorithms are also important to study because they are often key parts of larger algorithms where the ultimate goal is to find an optimal policy (one such example is the class of actor-critic algorithms, see [2] for a survey). We feel that ICLR seems to be expanding its scope and covering more general topics in ML and the topic of this paper fits this trend. \n\n[1] P. Balakrishna, R. Ganesan, and L. Sherry, \u201cAccuracy of reinforcement learning algorithms for predictingaircraft taxi-out times: A case-study of tampa bay departures,\u201dTransportation Research Part C: EmergingTechnologies, vol. 18, no. 6, pp. 950\u2013962, 2010.\n\n[2]I. Grondman, L. Busoniu, G. A. Lopes, and R. Babuska, \u201cA survey of actor-critic reinforcement learning:Standard and natural policy gradients,\u201dIEEE Transactions on Systems, Man, and Cybernetics, Part C(Applications and Reviews), vol. 42, no. 6, pp. 1291\u20131307, 2012", "title": "Response to Review #3"}, "BJgZdaaoiB": {"type": "rebuttal", "replyto": "rkgU1gHtvr", "comment": "1. We carry out a comparison study with DualDice in the revision.\n2. We re-organize the theoretic derivation to highlight the key ideas of EMP algorithm. In particular, we explain more explicitly the definition of the mixture policy in the multiple-behavior-policy setting, and its connection with the sample data distribution, to better explain our notion of \"partially policy-agnostic\".\n3. We re-organize the experiment part to make it more consistent with the theoretic analysis results. We also did more experiment repetitions to get better MSE estimation.", "title": "Revision Summary"}, "Sygmew3xqH": {"type": "review", "replyto": "rkgU1gHtvr", "review": "The authors propose here a method for off-policy policy evaluation (OPPEval), to use the reinforcement reinforcement learning on infinite horizon problems from previously-collected trajectories. \n\nThe authors frame their work that much of the focus in the OPPEval field has been on, as they call, \"policy-aware\" methods that use information from the policy  to improve estimates  to cope with the mismatch between then behaviour and the estimated target policy (such as IS, WIS, etc) when computing state stationary distribution. These contrast \"policy agnostic\" methods (DualDICE, Nachum et al, 2019) that suffer from the curse of dimensionality in estimating the much higher dimensional state-action stationary distributions but do not depend on policy information. \nThe manuscripts novelty rests in a comparing and relating  these agnostic/aware approaches and propose a partially policy-agnostic method (EMP) that strives to combine advantages from both approaches by following a mixture approach (effectively a mixture of weighted policies). The authors provide a derivation of their methods bounds and show that their method outperforms policy-aware methods as well as policy-agnostic methods. In the comprehensive experiments they compare recent methods by Liu et al (BCH) and WIS (I suppose they mean weighted importance sampling following Prenup et al 2000, as no citation given) ), as well a a new policy-agnostic method they propose here (SADL). In all cases the results  favour the proposed new method (EMP). The results advance the field by providing a pathway to improved estimation results (lower uncertainty) by using policy mixtures.  \nWhile I have not checked the derivations line-by-line the results are consistent and interesting, although not entirely clear to me why this is an important contribution to a representational learning conference. \n\nA key question to this paper (and the OPPEval field) is to evaluate their methods  more consistently in closed-loop agent experiments - after training on the historical data. Perhaps for a representational learning conference this would be more appropriate way to convince one of the strength of the results.\n", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 4}}}