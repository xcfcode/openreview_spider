{"paper": {"title": "ShakeDrop regularization", "authors": ["Yoshihiro Yamada", "Masakazu Iwamura", "Koichi Kise"], "authorids": ["yamada@m.cs.osakafu-u.ac.jp", "masa@cs.osakafu-u.ac.jp", "kise@cs.osakafu-u.ac.jp"], "summary": "", "abstract": "This paper proposes a powerful regularization method named \\textit{ShakeDrop regularization}.\nShakeDrop is inspired by Shake-Shake regularization that decreases error rates by disturbing learning.\nWhile Shake-Shake can be applied to only ResNeXt which has multiple branches, ShakeDrop can be applied to not only ResNeXt but also ResNet, Wide ResNet and PyramidNet in a memory efficient way.\nImportant and interesting feature of ShakeDrop is that it strongly disturbs learning by multiplying even a negative factor to the output of a convolutional layer in the forward training pass.\nThe effectiveness of ShakeDrop is confirmed by experiments on CIFAR-10/100 and Tiny ImageNet datasets.\n", "keywords": []}, "meta": {"decision": "Reject", "comment": "The paper proposes a regularisation technique based on Shake-Shake which leads to the state of the art performance on the CIFAR-10 and CIFAR-100 dataset. Despite good results on CIFAR, the novelty of the method is low, justification for the method is not provided, and the impact of the method on tasks beyond CIFAR classification is unclear."}, "review": {"r1HFPmSgG": {"type": "review", "replyto": "S1NHaMW0b", "review": "This paper proposes a regularization technique for deep residual networks.  It is inspired by regularization techniques which disturb the training by applying multiplicative factors to the convolutional layer outputs e.g  Shake-Shake (Gastaldi '17) and PyramidDrop (Yamada '16).  The proposed approach samples a Bernoulli variable randomly to either follow the standard variant of Pyramid net, or applies a variant of shake-shake to pyramid net.\n\n+ Experimental results on CIFAR-10 and CIFAR-100 well-exceed exceed the existing \"vanilla\" techniques + regularizers.  \n- Clarity: some statements are not clear / not substantiated e.g. how does the proposed method overcome the memory problem that shake-shake has?  There are some minor issues wrt presentation, e.g. grammatical correctness of sentences, consistent usage of references, which can be fixed with more careful proofreading.\n- Quality: even though the experimental results are compelling, the paper lacks thorough analysis in understanding the effects of the regularizer.  The two experiments looks at (1) the training error, which the paper openly states does not explain why the proposed regularization works and (2) variance of the gradients throughout learning; the larger variance of gradients is speculated to be the cause, but this is almost expected, given that the method is designed to allow larger fluctuations and perturbations during training.", "title": "Compelling experimental results, analysis not totally clear", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "HkAAvk0gf": {"type": "review", "replyto": "S1NHaMW0b", "review": "The paper proposes a new form of regularization that is an extension of \"Shake-Shake\" regularization (Gastaldi, 2017). The original \"shake-shake\" proposes using two residual paths adding to the same output (so x + F_1(x) + F_2(x)), and during training, considering different randomly selected convex combinations of the two paths (while using an equally weighted combination at test time). However, this paper contends that this requires additional memory, and attempt to achieve similar regularization with a single path. To do so, they train a network with a single residual path, where the residual is included without attenuation in some cases with some fixed probability, and  attenuated randomly (or even inverted) in others. The paper contends that this achieves superior performance than choosing simply a random attenuation for every sample (although, this can be seen as choosing an attenuation under a distribution with some fixed probability mass at 1). Experiments show improved generalization on CIFAR-10 and CIFAR-100.\n\nI don't think the paper contains sufficiently novel elements to be accepted as a conference track paper at ICLR. While it is interesting that this works well (especially the \"negative\" weight on the residual), the proposed method is fundamentally a combination of prior work: dropout and \"shake-shake\" regularization. Moreover, the evaluation is somewhat limited---essentially, I feel there isn't conclusive proof that \"shake-drop\" is a generically useful regularization technique. For one, the method is evaluated only on small toy-datasets: CIFAR-10 and CIFAR-100. I think at the very least, evaluation on Imagenet is necessary. The proposed regularization is applied only to the \"PyramidNet\" architecture---which begs the question of whether the proposed regularization is useful only for this specific network architecture. It would have been more useful to see results with and without \"shake-drop\" on different architectures (the point being to show a consistent improvement with this regularization, rather than achieving 'state of the art' on CIFAR-10). Moreover, it would be interesting to see if the hyperparameter comparison shown in Tables 1 and 2 remained consistent across architectures.", "title": "Review", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyI9Lxf-z": {"type": "review", "replyto": "S1NHaMW0b", "review": "The paper proposes ShakeDrop regularization, which is essentially a combination of the PyramidDrop and Shake-Shake regularization. The procedure consists of essentially weighting the residual branch with a random weight, in the style of Shake-Shake, where the weight is sampled from a mixture of uniform distribution in [-1, 1] and delta at 1, such that the mixture of those two distributions varies linearly with layer depth, in the style of PyramidDrop. In the style of Shake-Shake, a different random weight (in [0, 1]) is used for the backward pass. The most surprising part is that that the forward weight can be negative thus inverting the output of a convolution. Apparently the goal is to \"disturb\" the training, and the procedure yields state-of-the-art results on CIFAR-10/100.\n\nPositives:\n\n- Results: state-of-the-art on CIFAR-10/100\n\nNegatives:\n\n1. No real motivation on why should this work. I guess the motivation is the mixture of PyramidDrop and Shake-Shake motivations, but the main surprising part (forward weight can be negative) is not motivated at all. There is a tiny bit of discussion at the very end, section 4.4, where authors examine the training loss (showing it's non-zero so less overfitting) and mean/variance of gradients (increased). However, this doesn't really satisfy me - it is clear that more disturbance will cause this behaviour, but that doesn't mean any disturbance is good, e.g. if I always apply the negative weight and make my model weights go in the wrong direction, I'm pretty sure training loss and gradients will be even larger, but it's a bad idea to do.\n\n2. I'm concerned with the \"weird trick that happens to work on CIFAR\" line of work (not saying that this paper is the only offender) - are these methods actually useful and generalizable to other problems, or are we overfitting on CIFAR and creating MNIST v2.0 ? It would be nice to demonstrate that this regularization works in at least one more problem, maybe ImageNet, though maybe regularization is not needed there but just find one more dataset that needs regularization and test this on that.\n\n3. The paper doesn't explain well what is the problem with Shake-Shake and memory. I see that the author of Shake-Shake has made a comment on this and that makes a lot of sense, i.e. there is no memory issue, just because there are 2x branches doesn't mean shake-shake needs 2x memory as it can use less capacity=memory to achieve the same performance. So it seems the main premise of the paper - \"let's apply Shake-Shake to deeper models but we need to come up with a modified method because Shake-Shake cannot be applied due to memory problems\" - seems wrong.\n\n4. The writing quality is quite bad, it is very hard to understand what authors mean in parts of the text. E.g. at two places \"it has almost the same residual block as Eqn. (1)\" - how is it \"almost\"? Below equation 5, it is never specified that alpha and beta are sampled uniformly(?) from those ranges, one could think that alpha and beta are fixed constants that take a specific value that is in that range. There are also various grammatical errors such as \"is expected to be powerful but slight memory overhead\" or \"which is introduced essence\", etc.\n\nSmaller comments:\n- Isn't it surprising that alpha in [-1, 1] and beta in [0, 1] works well, but alpha in [0, 1] and beta in [-1, 1] works much worse? The two important cases, (alpha negative, beta positive) and (alpha positive, beta negative), seem to me like they are conceptually very similar.\n- End of section 4.1, should it be b_l as p_L is a constant and b_l is what is sampled?\n- I don't like that exactly the same text is repeated 3 times (abstract, end of intro, end of 1.1) and in very short distance from each other - repeating the same words 3 times doesn't make the reader understand it better, slight rephrasing is much more beneficial.\n\nOverall:\nGood to know that this method sets the new state of the art on CIFAR-10/100, so as such it should be of interest to the community to be available online (arXiv). But with fairly little novelty (is a combination of 2 methods), very little insights of why this should work at all (especially the negative scaling coefficient which is the only extra thing that one learns from this paper, since the rest is a combination of PyramidDrop and Shake-Shake), no idea on whether the method would work outside of the CIFAR-world, and bad quality of the text - I don't think the manuscript is sufficiently good for ICLR.\n\n", "title": "Review", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Sy8VrBpXf": {"type": "rebuttal", "replyto": "S1NHaMW0b", "comment": "We have cleaned up \"bugs\" in the revised paper.", "title": "A new version of the paper is available"}, "HJ2pqLhQf": {"type": "rebuttal", "replyto": "BknEhzqmf", "comment": "We found some errors that should be corrected. Now we are revising it and will upload a further revised version of the paper today.", "title": "We will revise the paper again"}, "ryGlO75QG": {"type": "rebuttal", "replyto": "r1HFPmSgG", "comment": "Thank you very much for your review comments. \n\nHere are our responses to your comments.\n\n- Clarity: Based on your comments, we substantially improved the paper. In the revised paper, we more clearly state the motivation including the memory issue, the problem we tackled and its difficulty, idea to solve the problem, interpretation of Shake-Shake regularization to derive the proposed regularization method, and experimental results including the condition of base network architectures to apply the proposed ShakeDrop into greater details.\n\n- Quality: We found that the results in question do not have so informative to explain the phenomenon. So, in the revised paper, we added further consideration regarding the range parameters (alpha and beta) and the condition of base network architectures to apply the proposed ShakeDrop.\n", "title": "Response to review comment"}, "Skv0E75Xz": {"type": "rebuttal", "replyto": "HkAAvk0gf", "comment": "We appreciate your valuable feedback.\n\nWe found factual errors in your comment.\n(1) The proposed method is not a combination of dropout and shake-shake.\nWe agree that a combination of \u201cdropout\u201d and shake-shake regularization is trivial. But, it is not what we did. In the proposed ShakeDrop, we used ResDrop in a different usage from the usual. ResDrop is not used for dropping some layers as in the original paper. Instead, the mechanism of ResDrop is used as a probabilistic switch of two networks. We show in the paper that such usage of ResDrop contributes to stabilize a network hard to train. This is novel and must be informative in the community. Furthermore, in the revised paper, we present how the problem is not trivial and greater details about our interesting findings.\n(2) While this is not a clear error, we are afraid that we cannot get which method you mean by saying \u201cchoosing simply a random attenuation for every sample\u201d of the following sentence: \u201cThe paper contends that this achieves superior performance than choosing simply a random attenuation for every sample (although, this can be seen as choosing an attenuation under a distribution with some fixed probability mass at 1).\u201c\n\nWe found your comments are reasonable. So, based on your comments, we extended experiments in two aspects in the revised paper.\n(1) The proposed ShakeDrop has been successfully applied to ResNet (EraseReLU version), Wide ResNet (with batch normalization added in the end of residual blocks) and ResNeXt (EraseReLU version) since we found that batch normalization is required to be at the end of residual blocks.\n(2) In addition to CIFAR-10/100 datasets, we confirm the effectiveness of the proposed ShakeDrop through experiments on Tiny ImageNet dataset. Unfortunately, experiments on ImageNet dataset was not possible in time with our computational resources.\n\nWe hope you find our revised paper is valuable.\n", "title": "Response to review comment"}, "H11n1Xcmz": {"type": "rebuttal", "replyto": "HyI9Lxf-z", "comment": "We appreciate your detailed review comments. Based on your feedback, we substantially improved the paper.\nWe believe that our contribution is not limited to achieving the state of the art of CIFAR-10/100 but also providing interesting insight to the community.\n\nFirst of all, we would like to point out a factual error which might be caused by our paper quality.\nRegarding the novelty, we understand that the reviewer regards the proposed ShakeDrop as a simple combination of two methods (ResDrop and Shake-Shake). Though apparently it could be seen like that, it is not true. To clarify it, we enumerate what we contribute for proposing ShakeDrop.\n(1) As Shake-Shake does not work on a single residual branch, we proposed a new regularization method working on a single residual branch (used in the intermediate method (\u201c1-branch Shake\u201d; previously we called it PyramidShake)). While it is inspired by Shake-Shake, it is completely different one.\n(2) We used ResDrop in a different usage from the usual. In the original paper, ResDrop is used for dropping some layers. Instead, we used it as a switch of two networks. We demonstrated in the paper that such usage of ResDrop contributes to stabilize a network hard to train like \u201c1-branch Shake.\u201d\n\nThe following are responses to negative aspects of the paper in your comments.\n\n1. Motivation\nAt the time of the initial submission, our motivation was to propose an effective and memory efficient regularization method applicable to PyramidNet because PyramidNet was the best network architecture on CIFAR-10/100 datasets.\nAfter reading review comments, we slightly updated our motivation. That is, in the revised paper, the target network architectures are not only PyramidNet but also ResNet, Wide ResNet and ResNext.\nThough the negative forward weight could be sensational, it is not our central contribution (we explained in the revised manuscript into greater detail). We added consideration on why the negative forward weight works well on the proposed ShakeDrop.\n\n2. Experiments on different datasets\nIt is understandable reaction to request experiments on different datasets. We added experiments on Tiny ImageNet dataset. While we understand experiments on ImageNet dataset are better, we found it is not possible to complete them in time with our computational resources.\n \n3. Memory issue of Shake-Shake\nIt seems our explanation was not appropriate. As we responded to the post by the author of Shake-Shake, out intention is not taking into account only learnable parameters but the total memory consumption.\n\nAnyway, out intention is as follows (as is written in the revised paper). Shake-Shake is designed to take a weighted sum of outputs of two residual branches. So, it requires at least two branches in a layer to apply. Due to this, it can be applied only to ResNeXt (having multiple branches) and it requires more memory to make the network deep than networks with a single residual branch in a layer.\n\n4. Writing quality\nWe are very sorry about it. We tried our best to correct such issues.\n\nRespond to \u201cSmaller comments\u201d\n- (alpha negative, beta positive) and (alpha positive, beta negative) are conceptually same?\nIt is a reaction we expected. The answer is no. We tried our best to explain this in the revised paper.\n\n- End of section 4.1, should it be b_l as p_L is a constant and b_l is what is sampled?\nThanks for pointing this out. It is a typo. It should have been b_l.\n\n- Exactly the same text is repeated 3 times\nWe are sorry for this. It is also solved in the revised paper.\n", "title": "Response to review comment"}, "BknEhzqmf": {"type": "rebuttal", "replyto": "S1NHaMW0b", "comment": "We substantially improved the paper. In the revised paper, we updated as follows.\n- Added experiments on different network architectures; not only PyramidNet, but also ResNet, WRN and ResNeXt\n- Added experiments on a new dataset \u201cTiny ImageNet\u201d\n- Added consideration about parameters (alpha and beta)\n- Revised introduction to fit the updated; the paper does not focus only on PyramidNet anymore\n- The intermediate method specially focusing on PyramidNet, named PyramidShake, was deleted. Instead, we named an intermediate regularization method \u201c1-branch Shake\u201d\n- Improved paper writing quality\n", "title": "Revised paper is uploaded"}, "HkmBcxr-f": {"type": "rebuttal", "replyto": "Sy3adjgWz", "comment": "We are afraid that you don't correctly understand what we claim.\nWe know that we can keep the number of parameters by adjusting Cardinality and baseWidth on ResNeXt.\nBut, we don't talk about the number of parameters solely.\nInstead, we talk about memory consumption.\nAmount of required memory depends on not only the number of learnable parameters but also other factors (such as the input of each layer for calculation of gradients on the backward pass). They cause the overhead we pointed out.\n\nWe found that our paper in the current form is not correctly understood by readers. So, we are improving it. Please wait for a while for the revised version.\n\nAnyway, thanks for your interest to our paper.", "title": "That's not what we intend"}, "r1F6F5Ygf": {"type": "rebuttal", "replyto": "S1rHOM-gz", "comment": "Roughly speaking, Shake-Shake requires as twice the amount of memory as ResNet on a residual block due to twice the number of residual branches. ShakeDrop can solve the issue by using a single residual branch (this corresponds to PyramidShake). Since PyramidShake is unstable in learning, we combined it with ResDrop to stabilize it.", "title": "Reply to \"Memory issue clarification\""}, "B16xxZzJf": {"type": "rebuttal", "replyto": "SyrG-ql1f", "comment": "Thank you very much for your inquiry. \nThe answer of your question is the former.\nThat is, we sample b_l on the forward pass and reuse it on the backward pass.\n\nRegarding Fig. 2(d), yes, what you have pointed out is correct.\nWe will revise it on a later version.\nThank you very much for pointing out the mistake. ", "title": "Reply to \"Implementation Clarification\""}}}