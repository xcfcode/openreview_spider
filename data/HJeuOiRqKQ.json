{"paper": {"title": "Pooling Is Neither Necessary nor Sufficient for Appropriate Deformation Stability in CNNs", "authors": ["Avraham Ruderman", "Neil C. Rabinowitz", "Ari S. Morcos", "Daniel Zoran"], "authorids": ["aruderman@google.com", "ncr@google.com", "arimorcos@gmail.com", "danielzoran@google.com"], "summary": "We find that pooling alone does not determine deformation stability in CNNs and that filter smoothness plays an important role in determining stability. ", "abstract": "Many of our core assumptions about how neural networks operate remain empirically untested. One common assumption is that convolutional neural networks need to be stable to small translations and deformations to solve image recognition tasks. For many years, this stability was baked into CNN architectures by incorporating interleaved pooling layers. Recently, however, interleaved pooling has largely been abandoned. This raises a number of questions: Are our intuitions about deformation stability right at all? Is it important? Is pooling necessary for deformation invariance? If not, how is deformation invariance achieved in its absence? In this work, we rigorously test these questions, and find that deformation stability in convolutional networks is more nuanced than it first appears: (1) Deformation invariance is not a binary property, but rather that different tasks require different degrees of deformation stability at different layers. (2) Deformation stability is not a fixed property of a network and is heavily adjusted over the course of training, largely through the smoothness of the convolutional filters. (3) Interleaved pooling layers are neither necessary nor sufficient for achieving the optimal form of deformation stability for natural image classification. (4) Pooling confers \\emph{too much} deformation stability for image classification at initialization, and during training, networks have to learn to \\emph{counteract} this inductive bias. Together, these findings provide new insights into the role of interleaved pooling and deformation invariance in CNNs, and demonstrate the importance of rigorous empirical testing of even our most basic assumptions about the working of neural networks.", "keywords": ["Convolutional Neural Networks", "Deformation Stability", "Pooling", "Transformation Invariance"]}, "meta": {"decision": "Reject", "comment": "This paper studies the role of pooling in the success underpinning CNNs. Through several experiments, the authors conclude that pooling is neither necessary nor sufficient to achieve deformation stability, and that its inductive bias can be mostly recovered after training. \n\nAll reviewers agreed that this is a paper asking an important question, and that it is well-written and reproducible. On the other hand, they also agreed that, in its current form, this paper lacks a 'punchline' that can drive further research. In words of R6, \"the paper does not discuss the links between pooling and aliasing\", or in words of R4, \"it seems to very readily jump to unwarranted conclusions\". In summary, the AC recommends rejection at this time, and encourages the authors to pursue the line of attack by exploring the suggestions of the reviewers and resubmit.  "}, "review": {"SJlJYrBKA7": {"type": "rebuttal", "replyto": "SJlP_2N-67", "comment": "Thank you for your kind feedback.\n\n\u201cThis paper tries to argue that pooling is unnecessary for deformation invariance.\u201d\nPerhaps we should have made this clearer in our writing, but our claim is not that pooling is *never* necessary. Instead, our claim is that pooling is not *always* necessary and that there is an alternative mechanism that can lead to stability to deformation, namely smooth filters. Further, we show that on very commonly studied tasks, this mechanism is at play. Perhaps our choice of title lead to some confusion, but we were trying to say pooling is \u201cnot necessary\u201d by which we meant \u201cnot required\u201d rather than \u201cuneccessary\u201d which seems to imply \u201cnever helpful\u201d. We believe these are the common uses of these terms but perhaps we should have chosen a different title or made our assertion clearer.\n\n\u201cResults on CIFAR 10 show pooling has little effect but is it unnecessary for harder problems as well? What about human pose datasets where deformation is inherent?\u201d\nThank you for this suggestion, this indeed is an interesting question. However, at the same time, we do not believe this question needs to be answered to establish our main point in this paper. Our point is NOT that pooling is never helpful. Our point is that for tasks that benefit from deformation stability, it is possible to learn to be stable to deformation by learning smooth filters. We also show that this mechanism is at play on two of the most commonly studied computer vision tasks in machine learning.\n\n\u201cthe level of cosine sensitivity (shown as same with and without pooling) could very well be a steady state for the specific classification task.\u201d\nIt would be very helpful to us if you could expand on this statement. Are you claiming that perhaps deformation stability is merely correlated with good performance at the end of training rather than causing it? \n\n\u201cAlso as mentioned in the conclusion of the paper, the effect of pooling through the course of training would add more weight.\u201d\nThank you for the encouragement to pursue this line of work!\n", "title": "Thank you for your feedback"}, "HJg6gEBY0Q": {"type": "rebuttal", "replyto": "H1xY5EwIpm", "comment": "Thank you for your encouraging words regarding empirical studies that question some of the widely accepted dogmas of deep learning!\n\nWe wish to clarify a few points and ask of you to clarify some of your comments if possible:\n\n\u201cJust because smoother filters are more stable does not automatically mean that they are more desirable.\u201d\nIt seems that you concluded that we were claiming that more stability is always a desirable property. We have not asserted this and in fact have highlighted that often stability is *reduced* over the course of training. Further, as our title suggests, it is important not only to have \u201cmore deformation stability\u201d but rather the \u201cappropriate deformation stability\u201d.\nIn future we will try to make it clearer as to what we are asserting, and if you have any suggestions on how to improve this aspect we would greatly appreciate it.\n\n\u201cJust showing that one network is better than another wrt some arbitrarily defined simple class of deformations with no reference to actual recognition performance, speed of training, or interpretation of the nature of the deformations and the learned filters is not very convincing.\u201d\nCould you please clarify what you meant by this sentence. It is not clear what you think we are trying to convince you of when you say \u201cthis is not very convincing\u201d.\nYou assert that class of deformations is arbitrary. We spend the first few paragraphs of section 2.1 justifying the study of these deformations. It would be helpful for us if you could explain why this class of deformations still arbitrary to you.\n\n\u201cI would really like to understand what pooling actually does.\u201d It would be really helpful to us if you could expand on this and clarify what you are asking here.", "title": "Thank you for your feedback "}, "HygrNXrF0Q": {"type": "rebuttal", "replyto": "HJeFxf4oa7", "comment": "Thank you for your kind feedback about the writing and the importance of the question being addressed.\n\nWhile we agree the results are not particularly surprising in retrospect, reading the literature on pooling, we have not seen learned smooth filters as a proposed mechanism for deformation stability and thought that these results may be of interest to the community trying to understand convolutional networks and deformation stability.\n\nFurther, we believe this work gives us an important bit of information on the topic of building inductive biases into architecture. Our work shows that a common architectural decision (pooling), long believed to be helpful in conferring a particular inductive bias (stability to deformation) was not actually necessary and that the inductive bias built in was being \u201coverridden\u201d by the learning process.\n\nThank you for suggesting we discuss aliasing. This indeed looks like an important direction in which to expand this work. Also, thank you for pointing out the reference \u201cImpact of Aliasing on Deep CNN-Based End-to-End Acoustic Models\u201d, we were unaware of this work and it seems very relevant.", "title": "Thank you for your feedback"}, "HJeFxf4oa7": {"type": "review", "replyto": "HJeuOiRqKQ", "review": "This paper asks what is the role of pooling in the success story of CNNs applied to computer vision. \nThrough several experimental setups, the authors conclude that, indeed, pooling is neither necessary nor sufficient to achieve deformation stability, and that its effect is essentially recovered during training. \n\nThe paper is well-written, it is clear, and appears to be readily reproducible. It addresses an interesting and important question at the interface between signal processing and CNNs. \n\nThat said, the paper does not produce any clear novel results. It does not provide any theoretical result, nor any new algorithm. Its contributions consist of three empirical studies, demonstrating that (i) the benefits of pooling in terms of deformation stability can be achieved through supervised learning the filters instead (sec 3), (ii) the mechanism to obtain stability through learning essentially consists on reducing the bandwidth of (some) filters (sec4), and (iii) that this mechanism is data-dependent (sec 5). None of these studies strike the reviewer as particularly revealing. Moreover, the reviewer felt that the authors could have built on those findings to ask (and hopefully answer) a few interesting questions, such as:\n-- Nowhere in the paper there is a discussion about critical Nyquist sampling and the need to reduce the bandwidth of a signal prior to downsampling it in order to avoid aliasing. Average pooling provably does it, and learnt filters do it provided they indeed become bandlimited. What are the links between deformation stability and the ability to avoid aliasing? \n-- How many lowpass antialiasing filters are needed per layer to provide sufficient stability? \n-- Also, the authors should relate this study with similar works that do the same in speech (e.g. https://www.isca-speech.org/archive/Interspeech_2018/abstracts/1371.html). \n\nIn conclusion, my impression is that this paper requires a major iteration before it can be of widespread interest to the community. I encourage the authors to think about the above points. \n\n\n ", "title": "Serious empirical study, but somewhat unsurprising and expected conclusions. ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SJlP_2N-67": {"type": "review", "replyto": "HJeuOiRqKQ", "review": "The work does an analysis of impact of different pooling strategies on image classification with deformations. It shows different pooling strategies reach to similar levels of deformation stability after sufficient training. It also offers an alternative technique with smoothness filters with to CNNs more stable. \nPros:\nThe paper considers a wide variety of pooling strategies and deformation techniques for evaluation.  Fair experiments with conclusion of similar stability of different pool layers after training is very evident.\nCons:\ni) Results on CIFAR 10 show pooling has little effect but is it unnecessary for harder problems as well? What about human pose datasets where deformation is inherent?\niii) Although, the results presented on smoother filter initialization are interesting, but these results are not compared in a one to one setting to different pooling methods, convolutions or residual networks. \n\nThis paper tries to argue that pooling is unnecessary for deformation invariance, as title suggests, and proposes initialization based on smooth filters as an alternative. Results are presented on CIFAR 10 to show the same, albeit on a trained network. However, CIFAR 10 is not a difficult dataset and the level of cosine sensitivity (shown as same with and without pooling) could very well be a steady state for the specific classification task. Imagenet dataset doesn't seem to show ablative studies. So this little evidence is insufficient to conclude that pooling is unnecessary.  Also as mentioned in the conclusion of the paper, the effect of pooling through the course of training would add more weight. ", "title": "Ask good questions but need more insightful analysis", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "H1xY5EwIpm": {"type": "review", "replyto": "HJeuOiRqKQ", "review": "It is often argued that one of the roles of pooling is to increase the stability of neural networks to \ndeformations. This paper presents empirical evidence to contest this assertion, or at least qualify it.\n\nI appreciate empirical studies that question some of the widely accepted dogmas of deep learning. \nFrom this point of view, the present paper is certainly interesting.\n\nUnfortunately, the actual evidence presented is quite weak, and insufficient to draw far reaching \nconclusions. An obvious objection is the authors only consider two datasets, and a very small number of \nmore or less standard pooling methodologies. The effect of pooling is evaluated in terms of cosine \nsimilarlity, which is not necessarily a good proxy for the actual performance of a network.\n\nA more serious issue is that they seem to very readily jump to unwarranted conclusions. For example, \nthe fact that stability to deformations (by which I necessarily mean the specific type of deformations \nthat they consider) tends to decrease in the middle layers of neural networks during training does not \nmean that starting with a neural network with less stability would be better. Maybe some kind of \nspontaneous coarse-to-fine optimization is going on in the network. Similarly, it is obvious that smoother \nfilters are going to lead to more stable representations. However, they might be less good at discriminative \ntasks. Just because smoother filters are more stable does not automatically mean that they are more desirable.\n\nStability to deformations is an important but subtle topic in computer vision. For starters, it is difficult \nto define what kind of deformations one wants to be insensitive to in the first place. A useful model would \nlikely incorporate some notion of deformations at multiple different length scales. \n\nJust showing that one network is better than another wrt some arbitrarily defined simple class of deformations \nwith no reference to actual recognition performance, speed of training, or interpretation of the nature of \nthe deformations and the learned filters is not very convincing. I would particularly like to emphasize the \nlast point. I would really like to understand what pooling actually does, not just at the level of \"if you \nturn it off, then cosine similarity will decrease by this much or that much.\"", "title": "Not enough evidence to conclude much about pooling", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1eNZj4bpX": {"type": "review", "replyto": "HJeuOiRqKQ", "review": "The work does an analysis of impact of different pooling strategies on image classification with deformations. It shows different pooling strategies reach to similar levels of deformation stability after sufficient training. It also offers an alternative technique with smoothness filters with to CNNs more stable. \nPros:\nThe paper considers a wide variety of pooling strategies and deformation techniques for evaluation.  Fair experiments with conclusion of similar stability of different pool layers after training is very evident.\nCons:\ni) Results on CIFAR 10 show pooling has little effect but is it unnecessary for harder problems as well? What about human pose datasets where deformation is inherent?\niii) Although, the results presented on smoother filter initialization are interesting, but these are results are not compared in a one to one setting to different pooling methods, convolutions or residual networks. \n\nThis paper tries to argue that pooling is unnecessary for deformation invariance, as title suggests, and proposes initialization based on smooth filters as an alternative. Results are presented on CIFAR 10 to show the same, albeit on a trained network. However, CIFAR 10 is not a difficult dataset and the level of cosine sensitivity (shown as same with and without pooling) could very well be a steady state for the specific classification task. Imagenet dataset doesn't seem to show ablative studies. So this little evidence is insufficient to conclude that pooling is unnecessary.  Also as mentioned in the conclusion of the paper, the effect of pooling through the course of training would add more weight. ", "title": "Ask good questions but need more insightful analysis ", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}}}