{"paper": {"title": "Do Transformers Understand Polynomial Simplification? ", "authors": ["Vishesh Agarwal", "Somak Aditya", "Navin Goyal"], "authorids": ["t-viaga@microsoft.com", "~Somak_Aditya1", "~Navin_Goyal1"], "summary": "", "abstract": "Recently researchers have demonstrated that Transformers can be trained to learn symbolic tasks such as solving integration and differential equations in an end-to-end fashion. In these setups, for an input symbolic expression, the Transformer predicts the final solution in a single step. Since such tasks may consist of a sequence of logical steps, question remains whether such networks have understood and learnt individual steps to reach the solution. To take a deeper look, we consider the task of polynomial simplification. Polynomials can be written in a simple normal form as a sum of monomials which are ordered in a lexicographic order. For a polynomial which is not necessarily in this normal form, a sequence of simplification steps is applied to reach the fully simplified (i.e., in the normal form) polynomial. For this task, we describe a synthetic Polynomial dataset generation algorithm which generates polynomials with unique proof steps. Then, we conduct an extensive analysis of the Transformer\u2019s abilities to learn the polynomial simplification task along different dimensions.", "keywords": []}, "meta": {"decision": "Reject", "comment": "While the reviewers find the experiments in the paper somewhat interesting, they find that the paper does not sufficiently address whether the limitations shown for models in this paper translate to larger models and other, more realistic, tasks, or an artifact of the setup considered in the paper.  Overall the takeaways seem unclear from the paper and I believe it is not ready for acceptance.  Addressing the issues raised by reviewers and having a more clear discussion on connections to existing results will help the paper."}, "review": {"Zx8WRLxWsX": {"type": "review", "replyto": "yZkF6xqhfQ", "review": "The paper studies the capability of the transformer architecture to\nperform rewriting to normal form in a simplified polynomial setting.\n\nIt is a continuation of the research by Piotrowski et al (PUBK) in the\narea of using neural nets to do symbolic rewriting, followed later by\nLample&Charton (LC).\n\nSeveral datasets are generated, using various\nconstraints on the sizes of coefficients, etc. Using infix vs prefix\nnotation is also analyzed. The number of variables is either 1 or 2,\nwhich seems insufficient. Already PUBK shows that going from 2 to 3\n(poly 5 vs poly6) variables reduces the performance considerably.\n\nThe main difference to PUBK is that the unnormalized polynomials are\ngenerated in a simpler format (sum of products of factors) that makes\n(or should make) the normalization procedure very simple. And that the\nevaluation is done step-wise, similar e.g. to work of Gauthier [3].\n\nThe setting is a bit problematic when compared to the full setting\nalso by forcing the normalization to be done in a particular \"obvious\"\nway. A richer set of rewriting steps consisting e.g. of finding common\nfactors (as e.g. in 2*(2x+1)*(y+1)*(z+2) + 2*(2x+1)*(y+1)*(z+3)) could\nlead to shorter rewriting sequences. These two issues - very\nsimple polynomials and very constrained rewriting rules - imply that\nthe setting is insufficient to answer the question posed by the title.\n\nMy overall feeling is that the paper shows a lot of experimental data,\nbut it does not bring sufficiently interesting new insights.\n\nSome more comments:\n\nThe poor generalization (e.g. Table 16) of the transformer to symbolic\ndata generated differently is not very surprising. Still, I am missing\ninformation about testing transformers trained on fewer variables on\ndata with more variables.\n\nOn page 5 the authors say \"We make sure that the simplified versions\nof the input polynomial in the training batches, do not collide with\nany endpoints in the  test and validation set.\"  \n==> \nHow was this done? PUBK shows that one simple kind of replacement in data (CONST\ninstead of all digits) leads to very large train/test overlaps in\nLC. But PUBK says that this is initial and much more needs to be\ndone. A simple improvement suggested there is measuring the\nLevenshtein distance as in Wang et al., 2018. [1]. I would expect much\nmore on this topic given the previous work and their issues.\n\nCompared with Zombori et al. [2], none of the settings is ever\nshown to learn perfectly a usable (even if simple) algorithm.\n\np2: \"As a state-of-the-art model, we explore Transformers. While both Graph Neural Networks and\nTransformers have been used for single-step representation learning of symbolic theorems and single\nstep goal-theorem scoring, Transformer-based sequence-to-sequence networks have shown superior-\nity in end-to-end tasks in integration, differential equations\"\n==>\nVarious versions of tree neural nets have been used quite successfully by Gauthier for related symbolic tasks [3].  Similarly for guiding theorem provers, in particular in ENIGMA [4] .\n\nPrefix vs infix: see [5] for previous related work on this and more.\n\np2: symbolic re-write==> symbolic rewriting\n\np2: the facstep in the example is unclear/confusing - X^1 is replaced just by X.\n==> What is the underlying representation? Can you give a more illustrative example of facstep?\n\n\nReferences:\n\n[1] Qingxiang Wang, Cezary Kaliszyk, Josef Urban:\nFirst Experiments with Neural Translation of Informal to Formal Mathematics. CICM 2018: 255-270\n\n[2] Zsolt Zombori, Adri\u00e1n Csisz\u00e1rik, Henryk Michalewski, Cezary Kaliszyk, Josef Urban:\nTowards Finding Longer Proofs. CoRR abs/1905.13100 (2019)\n\n[3] Thibault Gauthier:\nDeep Reinforcement Learning for Synthesizing Functions in Higher-Order Logic. LPAR 2020: 230-248\n\n[4] Karel Chvalovsk\u00fd, Jan Jakubuv, Martin Suda, Josef Urban:\nENIGMA-NG: Efficient Neural and Gradient-Boosted Inference Guidance for E. CADE 2019: 197-215\n\n[5] Bartosz Piotrowski, Josef Urban:\nStateful Premise Selection by Recurrent Neural Networks. LPAR 2020: 409-422\n\n========================\n\nUPDATE\n\nThe response says:\n\n\"With the straightforward use of Transformers, where the model has only seen a single variable in training, there\u2019s no information for the model about what to do with the second variable and thus it will not generalize to the two variable case. Training on two variable-polynomials and testing on two variable-polynomials has relatively low accuracies in our experiment. This suggests that training on single variable-polynomials and testing on two variable-polynomials will result in even lower accuracies. With more work, one may be able to design a model with appropriate inductive bias that understands the concept of multiple variables. This is beyond our scope.\"\n\nI am afraid that this makes the study rather insufficient for me. The problem of representing variables, eigenvariables/skolems, and capturing structural similarity between different theories and signatures is ubiquitous in the ML-for-TP area. Practically all useful systems developed so far - both features-based and DL-based - have to address this. The authors' answer is \"our representation is unsuitable\". The observation that if you have no shared representation of variables, you will get little/no generalization is a no-brainer and there is hardly any need to publish negative papers about it. In particular, in a conference about *representations* and some 15 years after first useful systems dealing with such issues have been developed. There are many fixes to this - see e.g. Gauthier's representation of variables in his Tree NNs, etc.\n\nMy score will stand, but I would like to encourage the authors to dig deeper and follow the suggestions given in this and other reviews. The general topic of learnability of symbolic rewriting by various neural architectures is certainly interesting, potentially very useful, and far from well understood.\n", "title": "Many experiments but not sufficiently interesting new insights", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "oCR9tikJgzM": {"type": "rebuttal", "replyto": "IJb4K62msO", "comment": "\"It is also possible that a larger model would improve on handling long sequences, it would certainly be worth trying - even if indeed it shouldn't affect the main points.\" \n\nWe agree with your suggestion, and will experiment with it.", "title": "Thank you for your Response"}, "V8xJWLScLcR": {"type": "rebuttal", "replyto": "ANkct9HsvT", "comment": "Dear Reviewer, thank you for your encouraging comments. Please find our response below.\n\u201cFor one, the authors only train a 4-layer 4-head model, which is quite small as far as Transformers go.\u201d: We plan to perform these experiments in future. However, as mentioned in the general response, such a choice does not conflict with our premise and the obtained insights.\n\n\u201cCan the Transformers simplify polynomials with way more factors than trained on? With a higher number of variables? Higher degrees?\u201d: For OOD evaluation, we evaluate by increasing more factors, higher coefficients, and higher exponents. The comparison of MEDIUM COEFF, MEDIUM TERMS & MEDIUM DEGREE (Table 16) shows this data. For 2 variables and other settings, please check Tables 14, 15.\n\nFiner Steps in Arithmetic Multiplication question: You are correct in stating that we do not add such finer steps for arithmetic multiplication. However, our results show (Table 9 for annotated proofs) that Transformers suffer on longer proofs. Since, the above modification will generate even longer proofs, we suspect the setting will lead to even worse results. Having said that, we do plan to perform experiments where the difficulty in integer factorization is \u201coutsourced\u201d by making an external program (a standard integer multiplication program) perform integer multiplication and letting the Transformers handle only symbolic manipulation. As the experimentation over all variations will take time, we believe we will be able to include the results in the final version upon acceptance.", "title": "Response to AnonReviewer2"}, "F1RxdoOPHn": {"type": "rebuttal", "replyto": "e9ijauFNdTx", "comment": "Thank you for your reply. We will respond to the new points that you raise.\n\n\"So failure on highly synthetic tasks be a cause of concern but is not necessarily a show-stopper or super interesting, publishable insight\"\n\nRather than stopping the show, the intention is to examine more closely what has been achieved. Related to the example you mention, it's well-known that computer vision models are susceptible to adversarial attacks and often learn spurious features. These things can be demonstrated by artificial examples. Arguably, a large fraction of adversarial examples are artificial as they don\u2019t arise in natural settings. Synthetic artificial settings let one control the number of moving parts and simplify the problem.\n\n\"those negative results tend to be highly suspect, since there can be a lot of major bugs, suboptimal hyperparamaters, or other unexpected issues that can affect the validity of the results. This means that negative results should always be accompanied by extreme amount of evidence and also a clear qualitative description of the exact failure modes together with explicit attempts of trying to fix them.\"\n\nCompletely agreed. For the medium coefficient, prefix input, un-annotated proofs, the experiments were:\n1. learning rates: 1e-2, 1e-3, 1e-4, 5e-5 with embedding size 256. With 1e-2, loss did not seem to decrease steadily. 1e-3 resulted in low accuracy. 1e-4 worked well and 5e-5 had slow convergence.\n2. Setting learning rate as 1e-4, tried different embedding sizes: 128, 256, 512.\n    Accuracy wise, both 256 and 512 fared equally well and better than 128. \n3. With learning rate 1e-4, embedding size 256, tried dropout as 0 and 0.5. We got similar accuracy for both.\nAll other experiments were run with lr 1e-4, embedding size 256, dropout 0.\nWe think the extensive tables and discussion in the paper are pretty explicit description of failure modes.", "title": "Follow-up"}, "qBGebI1oP0Y": {"type": "rebuttal", "replyto": "TSJb-Pr1y4Y", "comment": "Dear Reviewer, Thank you for engaging with us and your thoughtful reply.\n\n\u201cThe paper addresses a task that can be relatively efficiently handled by other methods and there has been overwhelming evidence for Transformer's capability to perform one and multi-step reasoning tasks. ([Polu and Sutskever: GPT-f], [Lample and Charton], [Rabe et al Skip-tree training], [Wu et al: INT] etc.)\u201d\n\nOur paper is precisely calling into question what you term \u201coverwhelming evidence\u201d from some of the papers you cite and others. As we argued in the paper and our response, many of these deal with more complex types of reasoning than we do. Given that Transformers are unable to do two variable polynomial simplification well (within our setting), it raises concerns about what they are doing in those other settings where test errors are higher and the possibility of probing what\u2019s going on far more limited because of the complexity of the task and the human written datasets. Our work is only a first step in this direction and we believe further synthetic settings can be devised that test other aspects of reasoning in a systematic way.\n\nYou are right that there are multiple observations in the paper and none stands out. In our view, this is not a shortcoming but inherent in the nature of the problem: reasoning task is multifaceted where one necessarily has to deal with multiple phenomena and there are multiple failure modes. It is of course possible to study single phenomena (e.g. integer multiplication) which can be interesting in its own right, but that\u2019s a different problem and both types of studies are valuable. We found that to shoehorn the conclusions of such a study into one or two major technical conclusions is not possible. However, a high level one-line summary can be given: The situation with Transformer\u2019s mathematical reasoning abilities is muddy.\n\n\u201cIf it is the usefulness of curriculum learning, then we have more impressive measurement points: [Zombori et al], also the paper does not describe a generally usable strategy to create curricula.\u201d\n\nThe two works cannot be directly compared due to different settings (they work with more complex theorem proving problems) and different models (they use reinforcement learning).\n\n\"The Takeaway is still not clear\"\nAs a recount, we analyze multi-step reasoning capabilities of Transformers in a systematic and careful way spanning various dimensions. Our insights *together summarizes* the strengths and weaknesses that Transformers exhibit. Additionally, we also believe that the proposed style of analysis and derived insights will motivate the community to  *analyze across the discussed dimensions for any task that require multiple-step reasoning*. As we show from or results, we believe this is necessary before claiming such tasks are solved. Currently we have seen that, the end-to-end proof accuracy for integration/DEs (in LC), HOList (Bansal et al.) etc. or next-step prediction accuracy analysis do not capture such dimensions. ", "title": "The Takeaway"}, "L4KunL5ql5r": {"type": "rebuttal", "replyto": "dWBgvQvwmr8", "comment": "Thank you for your kind and encouraging comments. Please find our response for individual comments below. We put your comments in quotes, followed by our response.\n\n\u201cThe task is also not very useful as a prediction target, because it can be solved with a simple algorithm, as far as I can see. The paper essentially claims that this allows us to study Transformers better. What insights were produced here that couldn't have been produced on more challenging benchmark from the literature?\u201d : We agree that the final goal of this line of research is to design models that can learn to solve problems that we don't already know how to solve. Several recent works in ATP frameworks cited in the paper have attempted to do this. The accuracies achieved remain low. This could be for various reasons including deficiencies of the models and/or of data. To understand the situation better, it seems reasonable to first check if our models are able to solve much simpler problems where we have fine-grained control over various aspects of the dataset (this is impossible for ATP frameworks). For some examples of insights from our work, please see our common response.\n\nUnique Proofs: In keeping with our general philosophy of having a very simple setting, unique proofs simplify the task of the Transformer as at each step it has a unique step to perform. \n(If the proofs are not unique, then one could use RL/MCTS-based approaches as is done in many papers or learn the most probable next steps as in language models.)\n\n\u201cWhat is the point of \"establishing baselines\" if the task is essentially solved already with moderately-sized Transformers?\u201d: It is true that the task can be solved by a simple algorithm, however the task using Transformers as the model is not solved (Low accuracies in Table 1 and 2). Thus, our results can serve as a baseline for this problem.\n\n\u201cWhat can be learned from the curriculum learning experiments? The way the curriculum is defined here appears to essentially require a synthetic benchmark for which we can generate examples of different hardness levels. So how could this help for real problems?\u201d\nThis is a good point. Certainly new curriculums will be needed for real problems.\nCL Experiments: First a note that, by using synthetic dataset generation, our goal was to eliminate the concerns such as \u201clack of data\u201d.  Since, even in this setting, Transformers perform poorly, in many configurations, we wanted to employ traditional techniques such as CL to see whether such available techniques may improve the performance of Transformers. We observe that for many configurations, there is surprising improvement when CL is employed carefully. But, its inability to show a steady performance improvement again shows why more work is required for improving Transformers\u2019 performance even for a seemingly simpler task of polynomial simplification.\n\n\u201cIntroduction and Conclusion: The paper claims that training on the two variable case leads to better performance on the one variable case than training on it directly. This sounds very much like an artifact that could stem from the lack of training data or so. How is this possible?\u201d:\nWhile we don\u2019t have an explanation for this observation, we remark that:\nit can\u2019t be an artefact of the data. Both 1-var and 2-var models converge after similar number of steps (#Train column in Table 1 and 2). Thus, 1-var models see more single variable polynomials than 2-var models. But the 2-var model gets polynomials sampled from the 2 var distribution. (Table 1 and 2) It could happen that with 2 variables, the task of collecting variables together is better exemplified. In the sense that the tougher task model is performing good on a simpler task\n\n\u201cp.2, paragraph 1: \"we observe that the system can understand candidate sub-expressions ...\" I am always wary of the use of \"understand\" in the context of neural networks, as it is not very clear what it means.\u201d: We agree. A better title would have been \u201cCan Transformers Perform Polynomial Simplification.\u201d\n\n\u201cFor example, how many training steps/epochs did you train for?\u201d\nAs the data is generated on the fly, number of examples seen = number of training steps. #EE and #Train columns in the result tables correspond to the number of training steps. We will update the draft if we see more details are required for reproduction of the results.", "title": "Response to AnonReviewer4"}, "vyl0h9u1k5": {"type": "rebuttal", "replyto": "Zx8WRLxWsX", "comment": "Dear Reviewer, we appreciate your detailed comments on the work. We put your comments in quotes, followed by our responses. \n\u201cThe number of variables is either 1 or 2\u201d:  This relates to the main aim of the work. The main aim is to test Transformers on the polynomial simplification task  in a fine-grained manner; i.e. from step-wise understanding all the way down to basic operator level understanding. While it is true that we could have looked at more variables, our results for two variables already show considerable decrease in accuracy.\n\nClaimed similarity with Gauthier [3]: Indeed, many of the works on neural theorem provers cited in the paper produce stepwise proofs and Gauthier [3] appears to be in the same category. As explained in the paper, our setting is distinct from these in that no search is involved and thus the task is much simpler. \n\n\u201cThese two issues - very simple polynomials and very constrained rewriting rules - imply that the setting is insufficient to answer the question posed by the title.\u201d: We agree that the precise task that we study is a special type of polynomial simplification. However, we think the above comment is misplaced for several reasons:\n(1) While the title of the paper does mention polynomial simplification without qualification, the task is very clearly specified in the paper from the beginning and there's no claim that we are studying *the* polynomial simplification task. To our knowledge, there's no *canonical* polynomial simplification task. Indeed, there\u2019s no limit to how complex one can allow the rewrite steps to be. Given this, we think that the title is descriptive and not unreasonable as it is difficult to specify the details in the title. \n(2) Our negative results show that Transformers fail even for such a simple task in some of the settings (e.g., two variables results in Table 2). This strongly suggests that Transformers will fail if we increase the complexity of the task, e.g. by allowing richer rewrite rules or increasing the number of variables. For negative results, failure on a simpler task is a *stronger* result.\n(3) Allowing richer rewriting rules is interesting in its own right but not relevant for our purpose here which is to study what Transformers allow us to do in a very simple setting. Forcing the normalization to be done in the unique (or \u201cobvious\u201d) way is an important design feature of our study that enables us to do this. We can of course give up on uniqueness and generate a dataset without unique proofs, and increase the complexity of the task in other ways. We suspect that that will further degrade the performance of Transformers.\n\u201cit does not bring sufficiently interesting new insights\u201d: Please see our general response.\n\n\u201cStill, I am missing information about testing transformers trained on fewer variables on data with more variables.\u201d: With the straightforward use of Transformers, where the model has only seen a single variable in training, there\u2019s no information for the model about what to do with the second variable and thus it will not generalize to the two variable case. Training on two variable-polynomials and testing on two variable-polynomials has relatively low accuracies in our experiment. This suggests that training on single variable-polynomials and testing on two variable-polynomials will result in even lower accuracies. With more work, one may be able to design a model with appropriate inductive bias that understands the concept of multiple variables. This is beyond our scope.\n\nTrain Test Disjoint: We ensure that no unnormalized polynomial in train set simplifies to the same polynomial and one in the test set. This ensures that none of the intermediate steps could also be the same. \n\nCONST substitution by PUBK: The CONST substitution done by PUBK [1] on the LC [2] dataset is not applicable for the polynomial dataset as here arithmetic is an integral part of our task, which is not the case for integration. \n\n\u201cCompared with Zombori et al. [2], none of the settings is ever shown to learn perfectly a usable (even if simple) algorithm\u201d: This comment too seems to stem from a misunderstanding of our goals. We are not sure which result in [2] is being referred to here.\n\n \u201cWhat is the underlying representation? Can you give a more illustrative example of facstep?\u201d\nThe underlying representation is simply the inorder (infix) or preorder (prefix) traversal of the expression tree. So, 2 * x1 ^ 2 * x2 is written as [2, MUL, x1, EXP, 2, MUL, x2] in infix. \nFacstep example: 2 * x1 ^ 1  + 1 * x2 -> 2 * x1 + x2\nRepresentation: [2, MUL, x1, EXP, 1, ADD, 1, MUL, x2] -> [2, MUL, x1, ADD, x2]", "title": "Response to AnonReviewer3"}, "KTJ4xp9etKU": {"type": "rebuttal", "replyto": "yZkF6xqhfQ", "comment": "Previous work in symbolic math tasks such as PUBK [1] and LC [2] focus on the end-to-end setting, and get quite good performance. Previous work on logic tasks such as HOL [3], INT [4] focus on stepwise proof search, and achieve moderately good performance. However, generating stepwise proofs on symbolic tasks, which is interesting in its own right, isn\u2019t as well explored. \nIn this paper, we propose a very simple mathematical setting to evaluate Transformers' ability in generating proofs on the symbolic task of polynomial simplification. Thus, our goal is not to give new state-of-the-art models nor is our goal to extract a usable algorithm from the model. Our results map out, within the scope of our polynomial simplification problem, the strengths and weaknesses of Transformers. What we lose by making the task simple, we gain by being able to study the problem more extensively and probing the model along multiple axes. This can inform the future work on designing better models and training algorithms: \n\n(1) As a concrete example, we identify integer multiplication as one of the main bottlenecks. In general, neural nets struggle with integer multiplication. One way to make progress then could be to design a neuro-symbolic system where the multiplication part is performed by a symbolic algorithm. More generally, decomposing the task into components where the model works well and those where it does not can help design better systems. \n\n(2) As another example, we see that for some settings, curriculum learning results in significant benefits. \n\n(3) One general finding is that accuracy at end-point tasks is often significantly better than proof accuracy. Our models do not have a notion of proof, they only see individual proof steps. Thus incorporating the notion of proof in the model, perhaps by modifying the loss function to take the proof into account might help.\n\n[1] Bartosz Piotrowski, Josef Urban, Chad E. Brown, and Cezary Kaliszyk. Can neural networks learn symbolic rewriting?, 2019.\n\n[2] Guillaume Lample and Francois Charton. Deep learning for symbolic mathematics. In International Conference on Learning Representations, 2020.\n\n[3] Aditya Paliwal, Sarah M. Loos, Markus N. Rabe, Kshitij Bansal, and Christian Szegedy. Graph representations for higher-order logic and theorem proving. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 2967\u20132974. AAAI Press, 2020. URL https://aaai.org/ojs/index.php/AAAI/ article/view/5689.\n\n[4] Yuhuai Wu, Albert Jiang, Jimmy Ba, and Roger B. Grosse. INT: an inequality benchmark for evaluating generalization in theorem proving. CoRR, abs/2007.02924, 2020. URL https://arxiv.org/abs/2007.02924.", "title": "General Response"}, "8YIQrMyWtTx": {"type": "rebuttal", "replyto": "MMUFF9E2xmm", "comment": "Thank you for your review. Please see our general response, as most of the objections/concerns are related to the basic premise of the work. In short, our aim is not to test on harder problems, but to test the transformer's ability to do multi-step reasoning. Within the scope of a somewhat simpler setting of polynomial simplification tasks, our results map out the strengths and weaknesses of Transformers.", "title": "Response to AnonReviewer1"}, "dWBgvQvwmr8": {"type": "review", "replyto": "yZkF6xqhfQ", "review": "The paper \"Do Transformers Understand Polynomial Simplification?\" introduces a new reasoning task (convert polynomials into a normal form) and studies the performance and errors of Transformers on this task. The task itself is quite simple: given a randomly generated term involving small constants, variables, additions, and multiplications, bring the term into a well-defined normal form. Each task has to be solved in a unique sequence of steps. The authors study the performance of Transformers to either simplify the expressions step by step or to predict the simplified version directly.\n\nContributions I highly appreciate:\n- Contrasting step-wise generation of proofs vs end-point prediction.\n- Contrasting different representations of formulas: prefix notation appears to be better than infix notation.\n- Observation that transformers struggle with multiplication.\n\nQuestions to the authors:\n- The task is pretty simple compared to other (mathematical) reasoning tasks studied in the literature. The task is also not very useful as a prediction target, because it can be solved with a simple algorithm, as far as I can see. The paper essentially claims that this allows us to study Transformers better. What insights were produced here that couldn't have been produced on more challenging benchmark from the literature?\n- Why is it important for your paper that proofs are unique? Do we simply train Transformers to execute a simple algorithm? If so, how does that relate to the existing theorem proving and reasoning approaches where we usually have many possible proofs for a task.\n- What is the point of \"establishing baselines\" if the task is essentially solved already with moderately-sized Transformers?\n- What can be learned from the curriculum learning experiments? The way the curriculum is defined here appears to essentially require a synthetic benchmark for which we can generate examples of different hardness levels. So how could this help for real problems?\n- Introduction and Conclusion: The paper claims that training on the two variable case leads to better performance on the one variable case than training on it directly. This sounds very much like an artifact that could stem from the lack of training data or so. How is this possible?\n\n\nMinor comments:\n\np.1, pargraph 1: I believe Hahn et al also propose an \"end-to-end\" task using Transformers instead of embedding their approach in an existing neuro-symbolic system.\n\np.1, paragraph 2: The reference to Lample & Charton is slightly off: It was published in ICLR 2020.\n\np.2, paragraph 1: \"we observe that the system can understand candidate sub-expressions ...\" I am always wary of the use of \"understand\" in the context of neural networks, as it is not very clear what it means.\n\np2. related work: I do not understand how you contrast your work to the existing theorem proving works: There are a number of neural theorem provers (HOList, GamePad, GPT-f, and probably some more) that also generate proofs step by step. They might employ more advanced search ideas, but I think it would be good to state why your paper does not want to go in this direction.\n\np.2, Section 3: How does Sympy \"ensure correctness\"? There could be bugs in the code even if you didn't write the code yourself?\n\np.2, footnote: \"an unique\" -> \"a unique\"\n\np.3, last paragraph: remove \"Hence\".\n\nAdditional details about the training setup would be appreciated. For example, how many training steps/epochs did you train for?", "title": "Interesting study of the behavior of Transformers on symbolic tasks", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ANkct9HsvT": {"type": "review", "replyto": "yZkF6xqhfQ", "review": "The authors analyze the performance of Transformer models on simplifying polynomials and - importantly - generating proofs at the same time. This is a very nice idea that allows to study the performance of Transformers in depth and at the same time in an important setting where verification is performed as part of running the model. And the authors show a strong baseline, with models performing very well in a number of settings. A few areas seem to have been neglected though. For one, the authors only train a 4-layer 4-head model, which is quite small as far as Transformers go. Maybe it's irrelevant for this problem - but having at least one bigger model as a point of comparison would be good. Next, the out-of-distribution question warrants more experiments. Can the Transformers simplify polynomials with way more factors than trained on? With a higher number of variables? Higher degrees? The authors also show that one main problem for Transformers is learning to multiply the coefficients. But - assuming this reviewer understood correctly - the authors do not apply the proof requirement to multiplication. E.g., for \"12*3\" the model has to immediately output \"36\" rather than \"10*3 + 2*3 = 30 + 6 = 36\". Maybe this could help the Transformer learn and be more resilient to coefficient size? So while the current version of the paper is ok, there are a few areas for improvement which prevent it from being a clear accept.", "title": "Nice analysis with some more possibilities", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "MMUFF9E2xmm": {"type": "review", "replyto": "yZkF6xqhfQ", "review": "This paper studies the efficacy of transformers on a polynomial simplification tasks. \n\nThere are two main motivations for this work: Piotrowski et al and, Lample and Sarton (references in the paper). The paper is set out to explore the capability of transformer networks of creating muti-step proofs. \n\nOne of the contributions of the paper is the creation of dataset of polynomial simplifications. They use the method in Lample and Charleston to generate a large random dataset of polynomials represented as a sum of products. Each term in that product is a product of a small set of factors. So the basic question is: how do we represent this polynomial by a formula of minimum length, in which each operation is one of the +, - or *.  \n\nThis is a hard question for sure.\n\nBut is this question as hard as in Lample and Sarton?\n\nIs it hard as computing integrals of expressions?\n\nEverybofy knows that computing integrals is hard. In fact, it is much harder than computing partial derivatives.\n\nHow hard is it to give a representation of a sum of products? It might be hard. But is it as hard as the above?\n\nI can't tell, but this paper does not even specify what is a baseline. When do we believe that something is important.\n\nThis paper fails to specify what is an interesting message and fails to specify that message.\n", "title": "A somewhat interesting set of experiments with unclear takeaways.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}