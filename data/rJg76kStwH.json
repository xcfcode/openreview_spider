{"paper": {"title": "Efficient Probabilistic Logic Reasoning with Graph Neural Networks", "authors": ["Yuyu Zhang", "Xinshi Chen", "Yuan Yang", "Arun Ramamurthy", "Bo Li", "Yuan Qi", "Le Song"], "authorids": ["yuyu@gatech.edu", "xinshi.chen@gatech.edu", "yuanyang@gatech.edu", "arun.ramamurthy@siemens.com", "lbo@illinois.edu", "yuan.qi@antfin.com", "lsong@cc.gatech.edu"], "summary": "We employ graph neural networks in the variational EM framework for efficient inference and learning of Markov Logic Networks.", "abstract": "Markov Logic Networks (MLNs), which elegantly combine logic rules and probabilistic graphical models, can be used to address many knowledge graph problems. However, inference in MLN is computationally intensive, making the industrial-scale application of MLN very difficult. In recent years, graph neural networks (GNNs) have emerged as efficient and effective tools for large-scale graph problems. Nevertheless, GNNs do not explicitly incorporate prior logic rules into the models, and may require many labeled examples for a target task. In this paper, we explore the combination of MLNs and GNNs, and use graph neural networks for variational inference in MLN. We propose a GNN variant, named ExpressGNN, which strikes a nice balance between the representation power and the simplicity of the model. Our extensive experiments on several benchmark datasets demonstrate that ExpressGNN leads to effective and efficient probabilistic logic reasoning.", "keywords": ["probabilistic logic reasoning", "Markov Logic Networks", "graph neural networks"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper is far more borderline than the review scores indicate. The authors certainly did themselves no favours by posting a response so close to the end of the discussion period, but there was sufficient time to consider the responses after this, and it is somewhat disappointing that the reviewers did not engage.\n\nReviewer 2 states that their only reason for not recommending acceptance is the lack of experiments on more than one KG. The authors point out they have experiments on more than one KG in the paper. From my reading, this is the case. I will consider R2 in favour of the paper in the absence of a response.\n\nReviewer 3 gives a fairly clear initial review which states the main reasons they do not recommend acceptance. While not an expert on the topic of GNNs, I have enough of a technical understanding to deem that the detailed response from the authors to each of the points does address these concerns. In the absence of a response from the reviewer, it is difficult to ascertain whether they would agree, but I will lean towards assuming they are satisfied.\n\nReviewer 1 gives a positive sounding review, with as main criticism \"Overall, the work of this paper seems technically sound but I don\u2019t find the contributions particularly surprising or novel. Along with plogicnet, there have been many extensions and applications of Gnns, and I didn\u2019t find that the paper expands this perspective in any surprising way.\" This statement is simply re-asserted after the author response. I find this style of review entirely inappropriate and unfair: it is not a the role of a good scientific publication to \"surprise\". If it is technically sound, and in an area that the reviewer admits generates interest from reviewers, vague weasel words do not a reason for rejection make.\n\nI recommend acceptance."}, "review": {"183qWZC3oo4": {"type": "rebuttal", "replyto": "xKZA5JcijpQ", "comment": "Thanks for your interest in our work. With regard to the embedding size of GNN / tunable embeddings, we used grid search on the validation set, and the optimal setup is indeed 127-dimensional tunable embeddings plus 1-dimensional GNN embedding, as provided in the default command line. So we also noticed that on the Freebase dataset, higher dimensional GNN is not improving the performance. We assume the reason could be that the supervised learning signal from Freebase dataset is powerful, thus the tunable embedding part is fitting the data pretty well, while the topology of graph is playing less important role in this case. This is reasonable due to the nature of dataset. On the other datasets such as Cora, higher dimensional GNN is not hurting the performance (please refer to Table 2). As discussed in Section 5.1, the inductive GNN embeddings are designed to reduce the number of parameters (more compact model) without hurting the model capacity or expressiveness too much, which is a trade-off between model compactness and expressiveness.", "title": "Re: Question about the tunable embedding"}, "HwXdAlXVxUW": {"type": "rebuttal", "replyto": "t3Sq1jijkS1", "comment": "Thanks for reaching out again. We just started to work on the new experiments. Just to clarify our data setup, the original training data of FB15K-237 is randomly split into \"facts.txt\" and \"train.txt\", since we need a subset of training facts to generate first-order logic rules using NeuralLP. So we use \"facts.txt\" as the knowledge base and \"train.txt\" as the training data to generate rules.\n\nThe crux of our previous discussions seems to be the \"clean\" way of accessing the test data and updating the GNN parameters during the prediction (inference) phase. As clarified before, we would 1) learn the GNN parameters from supervised KG data with observed facts only; 2) perform query-by-query inference by sampling the query-related ground formulae and updating the GNN parameters in a \"sandbox\" only for this query, i.e., the update of parameters will not affect the inference of other queries. Please let us know if you have further questions about this new evaluation scheme.\n\nWe also noticed that a recent paper published in WWW'20 titled \"Probabilistic Logic Graph Attention Networks for Reasoning\" is very similar to our work ( https://dl.acm.org/doi/pdf/10.1145/3366424.3391265 ), which reports similar (slightly higher MRR and Hits@10) performance on FB15K-237 compared to our results. Since their paper does not cite our work, we assume they complete the work independently and this may help validate our experimental results.", "title": "Re: Reproducing the experiments without test data leakage "}, "HoVabnORN0g": {"type": "rebuttal", "replyto": "bw9iZgJ3f4", "comment": "Thanks for the follow-up questions. We further clarify these questions as follows.\n\n1. We would like to clarify that the training of MLN typically refers to learning the weights of logic formulae, while the inference of MLN is to predict the query (with the current formula weights). In fact, once the MLN is defined with a set of logic formulae and a set of observed facts, the probability of any latent variable (query) is already determined. The reason why we need to update GNN parameters during inference is because the exact inference of MLN is computationally infeasible, and we employ GNN as the variational posterior to perform approximate inference. For the knowledge graph used by GNN, it only contains observed facts and neither latent variables nor test query facts exist in the graph. So when we update the GNN parameters, there are no \u201ccreated query nodes\u201d in the graph that may leak test data information. We optimize the GNN parameters to make it a better posterior model, so that the variational inference can better approximate the underlying true probability distribution defined by the MLN.\n\n2. For each query in the Freebase dataset formed as (s,p,o), we perform inference of (s,p,?) and (?,p,o) sequentially, where we treat the queries as an input stream so that we do keep updating the GNN parameters during the inference. We assume that what you described is with regard to the parameter updating scheme here. Just to confirm with your points, what if we perform the inference of each query independently, i.e., we use the GNN parameters initially learned from supervised data with observed facts only, and perform the inference query by query by updating the GNN parameters always from the initially learned ones, would this way of evaluation rule out the possibility of test data leakage? If so, we can definitely try it out and we\u2019ll update the experimental results here.", "title": "Clarification on test data usage and inference approach"}, "IvKNzJ56b34": {"type": "rebuttal", "replyto": "RYjLpvoys3", "comment": "Thanks for your interest in our paper. We appreciate your detailed comments. We are truly sorry for being late to respond. Here we clarify our test data usage and details of our inference approach. Hopefully our response clarifies your question, which may also help other readers understand our paper and code.\n\n1. Potential leakage of validation and test data: 1) We confirm that the truth values of the validation and test data are not used anywhere during the inference and learning process; 2) Our graph neural network (GNN) is built on the knowledge graph, rather than the ground MLN, and we construct the knowledge graph only based on the observed facts in the training data (refer to Fig. 2 for a comparison of the ground MLN and the knowledge graph). That being said, there is no nodes of test set fact in the knowledge graph, thus there is no way of \"looking\" at the set of created variables and reduce the set of potential answers when updating the tunable embeddings and other trainable parameters of the GNN; 3) For the second part of our E-step objective function in Eq. 6, which is the supervised learning objective for training the GNN, it is only using the observed facts in the training data; 4) When predicting the query married_to(JohnDoe, ?), we consider all the entities in the knowledge graph to replace \"?\" to construct the test tuples, and predict the probabilities of all the constructed tuples for ranking and computing the evaluation metrics as MRR and Hits@N.\n\n2. Construction of MLN: The construction of the fully ground Markov Logic Network is computationally infeasible. With the mean-field approximation, we are able to decompose the global expectation over the entire MLN into local expectations over ground formulae. We perform the inference of MLN in a stochastic fashion: we sample mini-batches of ground formulae which may contain both observed and latent variables (facts). For each ground formula in the sampled batch, we take the expectation of the corresponding potential function w.r.t. the posterior of the involved latent variables (first term in Eq. 4), and compute a local sum of entropy using the posterior of the latent variables (second term in Eq. 4). Note that we not only have the facts in training, validation and test data, but also have all the latent variables used in the ground formulae. For example, given there is a logic formula Smoke(x) \u2227 Hypertention(x) => Cancer(x), and there's a test fact Cancer(David), then the corresponding variables Smoke(David) and Hypertention(David) in the ground formula will be used for inference and learning, no matter whether they are observed or latent. In summary, we construct MLN based on each ground formula, rather than just adding the facts in training, validation and test data.\n\n3. How we use test_fact_ls which contains the test set facts: During the inference process, we use test_fact_ls to guide the sampling of ground formulae. There are exponential number of all possible ground formulae, however, most of them are irrelevant to the query facts in the test data. Our sampling strategy is to focus on the ground formulae that contain at least one query fact as the latent variable. We also require that each sampled ground formula should have no truth values yet, i.e., the truth value of ground formula should depend on the truth values of the latent variables in it. This sampling strategy helps find relevant logic formulae that are relevant to the query, so that the inference can be more efficient. Note that there is no guarantee that the sampled formula can derive the correct test fact, since the logic formulae are auto-generated for FB15K-237 by NeuralLP and could be noisy. Our model has no access to the truth values of any latent variables in any ground formulae.\n\nPlease kindly let us know if you have any further questions. Thanks.", "title": "Clarification on test data usage and inference approach"}, "rkgx3TK2tr": {"type": "review", "replyto": "rJg76kStwH", "review": "In this paper the authors propose a system, called ExpressGNN, that combines MLNs and GNNs. This system is able to perform inference and learning the weights of the logic formulas.\n\nThe proposed approach seems valid and really intriguing. Moreover the problems it tackles, i.e. inference and learning over big knowledge graphs, are of foremost importance and are interesting for a wide community of researchers.\nI have just one concern and it is about the experiments for the knowledge graph completion task. In fact, this task was performed only on one KG. I think the proposed system should be evaluated on more KGs.\n\nFor these reasons I think the paper, after an extension of the experimental results, should be accepted.\n\n[Minor]\nPage 3. \u201cThe equality holds\u201d which equality are you talking about?\n", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 2}, "S1xWpo55or": {"type": "rebuttal", "replyto": "SkgLKByAtS", "comment": "Thanks for your comments. We briefly respond to a couple of points as follows.\n\n\n> The integration of variational EM and MLN has been explored in another work pLogicNet.\n\nWe have to clarify that our ExpressGNN work was proposed earlier than the pLogicNet. In fact, we have submitted an earlier version of our work to arXiv 15 days before the pLogicNet appeared on arXiv ( https://arxiv.org/abs/1906.08495 ). Due to the ongoing anonymous period, we could not provide the link of our arXiv submission here.\n\n\n> With pLogicNet, the contributions are not surprising or novel.\n\n1) As claimed above, we proposed the idea of integrating stochastic variational inference and MLN before the pLogicNet work appeared. As a concurrent and later work, pLogicNet also employs variational EM for MLN inference, which should not hurt the originality and novelty of our work.\n\n2) Compared to pLogicNet, our work employs GNNs to capture the structure knowledge that is implicitly encoded in the knowledge graph. For example, an entity can be affected by its neighborhood entities, which is not modeled in pLogicNet but can be captured by GNNs. Our work models such implicit knowledge encoded in the graph structure to supplement the knowledge from logic formulae, while pLogicNet has no graph structure knowledge and only has a flattened embedding table for all the entities.\n\n3) Our method is a general framework that can trade-off the model compactness and expressiveness by tuning the dimensionality of the GNN part and the embedding part. Thus, pLogicNet can be viewed as a special case of our work with the embedding part only.\n\n4) We compared our method with pLogicNet in the experiments. Please refer to Table 3 for the experimental results. Our method achieves significantly better performance than pLogicNet (MRR 0.49 vs 0.33, Hits@10 60.8 vs 52.8) on the FB15K-237 dataset.\n\nWe have updated the paper to incorporate the discussions above.", "title": "Response to Reviewer #1"}, "HJxrNxsciS": {"type": "rebuttal", "replyto": "HygnUyGAYH", "comment": "First of all, thank you for your valuable comments. We briefly respond to a couple of points as follows.\n\n\n> Why traditional MLN is computationally inefficient? Provide the inference time complexities.\n\nThe computational complexity of probabilistic MLN inference is known to be #P-complete when MLN was proposed [1]. To make it feasible, there are three categories of approximate inference methods: Monte Carlo methods, loopy belief BP, and variational methods [2]. Previous methods (including MCMC, BP, lifted BP) require to fully construct the ground Markov network before performing approximate inference, and the size of the ground Markov network is O(M^d) where M is the number of entities and d is the highest arity of the logic formula. Typically, there are a large number of entities in a practical knowledge graph, making the full grounding infeasible.\n\nWith mean-field approximation, our stochastic inference method avoids to fully construct the grounded Markov network, which only requires local grounding of the formulae in each sampled minibatch. Our method has constant time complexity for each sampled minibatch, and the overall time complexity is O(N) where N is the number of iterations. We have compared the inference efficiency on two benchmark datasets. Experimental results reported Fig. 4 show that our method is both more efficient and scalable than traditional MLN inference methods.\n\n\n> Does Lifted BP reduce the computational cost of grounding?\n\nLifted BP constructs the minimal lifted network via merging the nodes as the first step, and then performs belief propagation on the lifted network to save the computational cost. However, there is no guarantee that the lifted network is much smaller than the ground network. In the worst case, the lifted network can have the same size as the original ground network [2]. Moreover, the construction of the lifted network is also computationally expensive, which is even slower than the construction of the full network as reported in Table 3 of their paper [2]. In fact, our experiments demonstrate that Lifted BP is NOT efficient even on small dataset like UW-CSE and Kinship (please refer to Fig. 4 in our paper), and it certainly cannot scale up to the FB15K-237 dataset.\n\n\n> Why use Neural LP to learn the rules?\n\nThe FB15K-237 dataset is not designed for evaluating MLN inference / learning methods, and hence, have no logic formulae provided. Our work focuses on MLN inference and learning with a set of logic formulae, thus we need to generate the rules first. Similarly, recent work [3] uses simple brute-force search to generate the rules for MLN. However, brute-force rule search can be very inefficient on large-scale data. Instead, our method employs Neural LP to efficiently generate the rules. We use the training set only for rule learning, which guarantees that there is no information leakage during the evaluation on the test set.\n\n\n> Why not compare to BoostSRL?\n\nThe BoostSRL work uses MC-SAT as the inference method, which has been compared with our work in the experiments. According to the inference time reported in Fig. 4, our method is much more efficient and scalable than MC-SAT.\n\nMoreover, BoostSRL is not directly comparable to our method, since the task is completely different. Our method is designed for MLN inference and rule weight learning with logic rules provided, while BoostSRL was proposed for MLN structure learning, i.e., learning logic rules for MLN. We chose Neural LP instead of this method to generate the rules, since Neural LP has been demonstrated to be effective in rule induction on the Freebase dataset. In the updated paper, we have included BoostSRL as related work to supplement our literature review.\n\n\n> MLN is fairly general, does GNN result in any loss of expressivity?\n\nWe have discussed the expressive power of GNNs in our paper in the section titled \u201cWhy combine GNN and tunable embeddings\u201d. To make it more clear, in the updated paper, we change the section title to: \u201cExpressive power of GNN as inference network \u201d. In this section, we have shown an example in Fig. 3 where GNN produces the same embedding for nodes that should be distinguished. We have also formally proved the sufficient and necessary condition to distinguish any non-isomorphic nodes in the knowledge graph. Inspired by this, we augment GNN with additional tunable embeddings to trade-off the compactness and expressiveness of the model.\n\n\n> Related work should appear in the main paper.\n\nThanks for the suggestion. In the updated paper, we\u2019ve added the related work section right after the introduction to provide a clear background of statistical relational learning and Markov Logic Networks.\n\n\nReferences\n\n[1] Richardson, Matthew, and Pedro Domingos. \u201cMarkov Logic Networks.\u201d Machine Learning.\n\n[2] Singla, Parag, and Pedro M. Domingos. \u201cLifted First-Order Belief Propagation.\u201d AAAI.\n\n[3] Qu, Meng, and Jian Tang. \u201cProbabilistic Logic Neural Networks for Reasoning.\u201d arXiv.\n", "title": "Response to Reviewer #3"}, "BJlgEyicsr": {"type": "rebuttal", "replyto": "rkgx3TK2tr", "comment": "Thanks for your review comments. We briefly respond to your questions as follows.\n\n\n> The proposed system should be evaluated on more KGs.\n\nIn fact, our method is evaluated on four benchmark datasets with four different KGs: UW-CSE, Cora, Kinship, and Freebase. These knowledge graphs are of different knowledge types and data distributions, and are widely used as benchmark datasets to evaluate MLNs and knowledge graph reasoning methods.\n\n\n> Page 3. \u201cThe equality holds\u201d which equality are you talking about?\n\n\u201cThe equality holds\u201d points to the equality in Eq. (2). To make it more clear, we have added a reference to Eq. (2) in the updated paper.\n", "title": "Response to Reviewer #2"}, "SkgLKByAtS": {"type": "review", "replyto": "rJg76kStwH", "review": "This paper proposes a framework for solving the probabilistic logic reasoning problem by integrating Markov neural networks and graph neural networks to combine their individual features into a more expressive and scalable framework. Graph neural networks are used for learning representations for Knowledge graphs and are quite scalable when it comes to probabilistic inference. But no prior rules can be incorporated and it requires significant amount of examples per target in order to converge. On the other hand, MLN are quite powerful for logical reasoning and dealing with noisy data but its inference process is computationally intensive and does not scale. Combining these two frameworks seem to result in a powerful framework which generalizes well to new knowledge graphs, does inference and is able to scale to large entities. \n\nRegarding its contribution, the paper seems to consider a training process which is done using the variational EM algorithm. The variational EM is used to optimize the ELBO term (motivation for this is the intractability of the computing the partition term). In the E-step, they infer the posterior distribution and in the M-step they learn the weights. The integration of variational EM algorithm and MLN has been explored in another work (pLogicNet: Probabilistic Logic Neural Networks for Reasoning), but this paper proposes a new pipeline of tools: MLN, GNN and variational EM which seem to outperform all the existing baseline methods.The paper looks technically sound to me and the evaluations results are delivered neatly, however the flow of the paper makes it a bit difficult to follow sometimes due to many topics covered in it.\nRegarding the significance of the paper, it tries to combine logic reasoning and probabilistic inference which is of great interest among the researchers recently. ExpressGNN proves to generalise well and perform accurate inference due to the tunable embeddings added at the GNN.\n\nOverall, the work of this paper seems technically sound but I don\u2019t find the contributions particularly surprising or novel. Along with plogicnet, there have been many extensions and applications of Gnns, and I didn\u2019t find that the paper expands this perspective in any surprising way. \n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}, "HygnUyGAYH": {"type": "review", "replyto": "rJg76kStwH", "review": "The paper proposes to use graph neural networks (GNN) for inference in MLN. The main motivation seems to be that inference in traditional MLN is computationally inefficient. The paper is cryptic about precisely why this is the case. There is some allusion in the introduction as to grounding being exponential in the number of entities and the exponent being related to the number of variables in the clauses of the MLN but this should be more clearly stated (e.g., does inference being exponential in the number of entities hold for lifted BP?). In an effort to speed up inference, the authors propose to use GNN instead. Since GNN expressivity is limited, the authors propose to use entity specific embeddings to increase expressivity. The final ingredient is a mean-field approximation that helps break up the likelihood expression. Experiments are conducted on standard MLN benchmarks (UW-CSE, Kinship, Cora) and link prediction tasks. ExpressGNN achieves a 5-10X speedup compared to HL-MRF. On Cora HL-MRF seems to have run out of memory. On link prediction tasks, ExpressGNN seems to achieve better accuracy but this result is a bit difficult to appreciate since the ExpressGNN can't learn rules and the authors used NeuralLP to learn the rules followed by using ExpressGNN to learn parameters and inference.\n\nHere are the various reasons that prevent me from rating the paper favorably:\n\n- MLNs were proposed in 2006. Statistical relational learning is even older. This is not a paper where the related work section should be delegated to the appendix. The reader will want to know the state of inference and its computational complexity right at the very beginning. Otherwise, its very difficult to read the paper and appreciate the results.\n\n- Recently, a number of papers have been tried to quantify the expressive power of GNNs. MLN is fairly general, being able to incorporate any clause in first-order logic. Does the combination with GNN result in any loss of expressivity? This question deserves an answer. If so, then the speedup isn't free and ExpressGNN would be a special case of MLN, albeit with the advantage of fast inference.\n\n- Why doesn't the paper provide clear inference time complexities to help the reader appreciate the results? At the very least, the paper should provide clear time complexities for each of the baselines.\n\n- There are cheaper incarnations of MLN that the authors should compare against (or provide clear reasons as to why this is not needed). Please see BoostSRL (Khot, T.; Natarajan, S.; Kersting, K.; and Shavlik, J. 2011. Learning Markov logic networks via functional gradient boosting. In ICDM)", "title": "Official Blind Review #3", "rating": "1: Reject", "confidence": 3}}}