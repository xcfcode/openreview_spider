{"paper": {"title": "Minimal Random Code Learning: Getting Bits Back from Compressed Model Parameters", "authors": ["Marton Havasi", "Robert Peharz", "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"], "authorids": ["mh740@cam.ac.uk", "rp587@cam.ac.uk", "jmh233@cam.ac.uk"], "summary": "This paper proposes an effective method to compress neural networks based on recent results in information theory.", "abstract": "While deep neural networks are a highly successful model class, their large memory footprint puts considerable strain on energy consumption, communication bandwidth, and storage requirements. Consequently, model size reduction has become an utmost goal in deep learning. A typical approach is to train a set of deterministic weights, while applying certain techniques such as pruning and quantization, in order that the empirical weight distribution becomes amenable to Shannon-style coding schemes. However, as shown in this paper, relaxing weight determinism and using a full variational distribution over weights allows for more efficient coding schemes and consequently higher compression rates. In particular, following the classical bits-back argument, we encode the network weights using a random sample, requiring only a number of bits corresponding to the Kullback-Leibler divergence between the sampled variational distribution and the encoding distribution. By imposing a constraint on the Kullback-Leibler divergence, we are able to explicitly control the compression rate, while optimizing the expected loss on the training set. The employed encoding scheme can be shown to be close to the optimal information-theoretical lower bound, with respect to the employed variational family. Our method sets new state-of-the-art in neural network compression, as it strictly dominates previous approaches in a Pareto sense: On the benchmarks LeNet-5/MNIST and VGG-16/CIFAR-10, our approach yields the best test performance for a fixed memory budget, and vice versa, it achieves the highest compression rates for a fixed test performance.", "keywords": ["compression", "neural networks", "bits-back argument", "Bayesian", "Shannon", "information theory"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes a novel coding scheme for compressing neural network weights using Shannon-style coding and a variational distribution over weights.  This approach is shown to improve over existing schemes for LeNet-5 on MNIST and VGG-16 on CIFAR-10, strictly dominating them in terms of compression/error rate tradeoffs. Comparing to more baselines would have been helpful. Theoretical analysis based on non-trivial extensions of prior work by Harsha et al. (2010) and  Chatterjee & Diaconis (2018) is also presented. Overall, there was consensus among the reviewers that the paper makes a solid contribution and should be published.\n"}, "review": {"BJgeEAmlCm": {"type": "rebuttal", "replyto": "BkxIDtRY6m", "comment": "Yes, in principle one could recover the randomness used to sample from \\tilde{q} analogous to the bits-back argument (although it would require sharing the training set inputs and errors). However, in our experience, most of the probability mass is concentrated in one sample meaning that the entropy is close to 0 so the gains are unlikely to be significant. It would be interesting to see how the entropy of \\tilde{q} changes as the number of samples K increases. We expect that the entropy would be close to (log K - KL(q||p)).\n\nEdited.", "title": "Thank you for the question."}, "Ske6QqXeAm": {"type": "rebuttal", "replyto": "S1lV-njc2m", "comment": "Thank you for your review and the constructive feedback.\n\nA point of criticism raised in the review was that we built on existing works. However, we want to point out that Harsha et al. (2010) presented their work in a very different setting, i.e. a general bound on communication complexity. They provide a mathematical proof for the bound, which, however, does not translate to a feasible algorithm. We, on the other hand, propose a novel algorithm that achieves the bound on the encoding length and we provide performance guarantees using results from Chatterjee & Diaconis (2018) -- their results also do not directly translate to our setting (we consider an approximate sampling algorithm while they discuss the sample size required for importance sampling). As far as we know, neither of these works have received considerable attention in the machine learning literature.\n\nIn terms of baselines, our main goal was to compare against Bayesian compression, a state-of-the-art algorithm that is also motivated by the bits-back argument. We included further state-of-the-art baselines where they were available (VGG16/CIFAR10 was only reported in the Bayesian compression paper). We omitted compression algorithms that focus on improving the efficiency at runtime since these typically have significantly worse performance in terms of compression. However, exploiting our compression scheme for efficient inference (time, energy) is an important direction we are currently following.", "title": "Reply"}, "HJltv_7g0X": {"type": "rebuttal", "replyto": "r1eTYfKA2X", "comment": "Thank you for the constructive feedback. We hope that this reply addresses some of the concerns.\n\nIndeed, the paper has two key contributions. Firstly, the theoretical results that show that MIRACLE is a generalization of the Shannon-type coding schemes and secondly, the implementation of the algorithm that achieve state-of-the-art results on the two benchmark tasks.\n\nReviewer 2 points out that, while the basic algorithm encodes random samples, this is not the case for MIRACLE since it fixes the weights in each block and does further training on the rest. We argue that they are still random samples because by retraining after fixing each block, we are no longer sampling from a mean-field Gaussian, we are effectively sampling from a more flexible, autoregressive distribution (since each dimension is dependent on the previous ones). Indeed, if the retraining step was omitted, then we would be sampling from a less flexible, mean-field Gaussian distribution which leads to worse performance.\n\nRegarding the point that the origin of the gain is unclear, we can provide rough estimates. The difference in compression size between a mean-field Gaussian (without retraining) and the flexible distribution with retraining is about 2 times. The use of the hashing trick gives an improvement of about 1.5 times.\n\nThe size of the encoding of the standard deviation of the prior is trivial compared to the overall compression size. They take 32x(number of layers) bits in the final message. For LeNet-5, this is approximately 1.0 % of the overall compression size and for VGG-16, it is approximately 0.05 % of the overall compression size.\n\nUsing different distributions is an interesting avenue to explore. We believe, for example, that sparsity inducing priors could be beneficial. In this version of the paper, we settled with Gaussians because the reparameterization trick straight forwardly applies and the KL divergence has a closed form.\n\nRegarding the approximation error of algorithm 1, we have not done extensive experiments to quantify it. In our experience, exp(KL) samples perform well enough. Following the suggestion in the review, we ran an experiment on a toy example to see how the approximation error changes with the number of samples. It show that the total variation tends to 0 as the number of samples K increases, but it is still difficult to quantify how K affects the overall performance. Toy experiment link: https://imgur.com/a/oadzAT2", "title": "Reply"}, "ryxkJV7xRX": {"type": "rebuttal", "replyto": "BJe0AHhj3Q", "comment": "Thank you for your review and the constructive feedback.\n\nIndeed, the main contributions of our paper are:\n1) we propose a novel coding scheme for compressing neural network weights; in particular, we improve over the previously ubiquitous pruning-quantization pipeline and Shannon-style coding.\n2) our algorithm achieves the theoretical lower bound predicted by theory (based on Harsha et al.) and achieves the efficiency predicted by Hinton et al.'s bits-back argument.\n3) our algorithm allows, in contrast to previous work, to explicit control the trade-off between prediction quality and compression rate; in particular, please see our derived trade-off curves on Figure 1.", "title": "Summary of the contributions"}, "r1eTYfKA2X": {"type": "review", "replyto": "r1f0YiCctm", "review": "The authors come up with a surprisingly elegant algorithm (\"minimal random coding\") which encodes samples from a posterior distribution, only using a number of bits that approximates the KL divergence between posterior and prior, while Shannon-type algorithms can only do this if the posterior is deterministic (a delta distribution). It can also be directly used to sample from continuous distributions, while Shannon-type algorithms require quantization. In my opinion, this is the main contribution of the paper.\n\nThe other part of the paper that is specifically concerned with weight compression (\"MIRACLE\") turns out to be a lot less elegant. It is somewhat ironic that the authors specifically call attention to the their algorithm sending random samples, as opposed to existing algorithms, which quantize and then send deterministic variables. This is clearly true for the basic algorithm, but, if I understand correctly, not for MIRACLE. It seems clear that neural networks are sensitive to random resampling of their weights -- otherwise, the authors would not have to fix the weights in each block and then do further gradient descent for the following blocks. What would happen if the distributions were held constant, and the algorithm would be run again, just with a different (but identical) random seed in both sender and receiver? It seems this would lead to a performance drop, demonstrating that (albeit indirectly), MIRACLE also makes a deterministic choice of weights.\n\nOverall, I find the paper somewhat lacking in terms of evaluation. MIRACLE consists of a lot of parts. It is hard to assess how much of the final coding gain presented in table 1 is due to the basic algorithm. What is the effect of selecting other probability models, possibly different ones than Gaussians? Choosing appropriate distributions can have a significant impact on the value of the KL divergence. Exactly how much is gained by applying the hashing trick? Are the standard deviations of the priors included in the size, and how are they encoded?\n\nThis could be assessed more clearly by directly evaluating the basic algorithm. Theorem 3.2 predicts that the approximation error of algorithm 1 asymptotically zero, i.e. one can gain an arbitrarily good approximation to the posterior by spending more bits. But how many more are practically necessary? It would be fantastic to actually see some empirical data quantifying how large the error is for different distributions (even simple toy distributions). What are the worst vs. best cases?\n", "title": "Very interesting paper", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJe0AHhj3Q": {"type": "review", "replyto": "r1f0YiCctm", "review": "In this paper the authors propose to use MLD principle to encode the weights of NNs and still preserve the performance of the original network. The main comparison is from Han 2016, in which the authors use ad-hoc techniques to zero some coefficient and prune some connection + Huffman coding. In this case , the authors uses as a regularizer (See equation 3) a constraints that the weights are easy to compress. The results seem significant improvement with respect to the state of the art. \n\n", "title": "Interesting argument", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "S1lV-njc2m": {"type": "review", "replyto": "r1f0YiCctm", "review": "This paper considers the compression of the model parameters in deep neural networks. The authors propose minimal random code learning (MIRACLE), which uses a random sample of weights and the variational framework interpreted by the bits-back argument. The authors introduce two theorems characterizing the properties of MIRACLE, and demonstrate its compression performance through the experiments.\n\nThe proposed approach is interesting and the performance on the benchmarks is good enough to demonstrate its effectiveness. However, since the two main theorems are based on the existing results by Harsha et al. (2010) and Chatterjee & Diaconis (2018), the main technical contribution of this paper is the sampling scheme in Algorithm 1.\n\nAlthough the authors compare the performance trade-offs of MIRACLE with that of the baseline methods quoted from source materials, isn't it possible or desirable to include other competitors or other results for the baseline methods? Are there any other methods, in particular, achieving low error rate (with high compression size)? Little is discussed on why the baseline results are only a few. \n\nminor comment: \n- Eq.(4) lacks p(D) in front of dD.    \n\nPros:\n- Interesting approach based-on the bits back argument\n- Good performance trade off demonstrated through experiments\nCons:\n- Only a few baseline results, in particular, at high compression size\n", "title": "The proposed approach has interesting formulation and good performance tradeoff while the main theorems are based on existing works.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rkxLkNQ0q7": {"type": "rebuttal", "replyto": "Skea7Z0TcX", "comment": "Thank you for your comment, we are glad that you found our work interesting.\n\nIndeed it is true that the method does not require quantization. Perhaps we were not too explicit about this, but all of our results hold for continuous distributions. In particular, we used continuous (Gaussian) distributions in our experiments, which made optimization of the variational objective easy.\n\nIn earlier concepts, we actually did attempt to use quantization, resulting in discrete distributions. However, these models proved too difficult to optimize (we tried various gradient estimators such as Gumbel-Softmax, reinforce, straight through estimator), and we were unable to reach state-of-the-art performance with these approaches. This problem did not pertain to continuous distributions. A continuous distribution is straight-forward to train using SGD and the reparameterization trick. The training is stable and gives good performance.", "title": "Continuous distributions are easier to train"}}}