{"paper": {"title": "Neural Machine Translation with Universal Visual Representation", "authors": ["Zhuosheng Zhang", "Kehai Chen", "Rui Wang", "Masao Utiyama", "Eiichiro Sumita", "Zuchao Li", "Hai Zhao"], "authorids": ["zhangzs@sjtu.edu.cn", "khchen@nict.go.jp", "wangrui@nict.go.jp", "mutiyama@nict.go.jp", "eiichiro.sumita@nict.go.jp", "charlee@sjtu.edu.cn", "zhaohai@cs.sjtu.edu.cn"], "summary": "This work proposed a universal visual representation for neural machine translation (NMT) using retrieved images with similar topics to source sentence,  extending image applicability in NMT.", "abstract": "Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets, including the WMT'16 English-to-Romanian, WMT'14 English-to-German, WMT'14 English-to-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines.", "keywords": ["Neural Machine Translation", "Visual Representation", "Multimodal Machine Translation", "Language Representation"]}, "meta": {"decision": "Accept (Spotlight)", "comment": "This paper proposes using visual representations learned in a monolingual setting with image annotations into machine translation. Their approach obviates the need to have bilingual sentences aligned with image annotations, a very restricted resource. An attention layer allows the transformer to incorporate a topic-image lookup table. Their approach achieves significant improvements over strong baselines. The reviewers and the authors engaged in substantive discussions. This is a strong paper which should be included in ICLR. \n\n"}, "review": {"ryxpUP6RKr": {"type": "review", "replyto": "Byl8hhNYPS", "review": "Summary: This paper uses visual representation learned over monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs for multimodal NMT. Their approach enables visual information to be integrated into large-scale text-only NMT. Experiments on four widely used translation datasets show that the proposed approach achieves significant improvements over strong baselines.\n\nStrengths:\n- This paper is well motivated and well written. I especially like how they use external paired sentence-image data from Multi30k to learn weak pairs for sentences in machine translation.\n- Experimental results are convincing. I like how low-resource translation is included as a priority in their experiments.\n\nWeaknesses:\n- Do you have any explanations as to why the number of images, if too large, actually hurts translation performance? Is it because more images also leads to a higher chance of noisy images?\n- It would be nice to have an experiment that varies the size of the external paired sentence-image dataset and tested the impact on performance.\n- Please comment on the extra computation required for obtaining image data for MT sentences and for learning image representations.\n- Why are there missing BLEU scores and the number of parameters in Table 1?\n\n### Post rebuttal ###\nThank you for your detailed answers to my questions.", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}, "B1gYdNvCFS": {"type": "review", "replyto": "Byl8hhNYPS", "review": "This paper provides an approach to use visual information to improve text only neural machine translation systems. The approach creates a \"topic word to images\" map using an existing image aligned translation corpora. Given a source sentence, the model extracts relevant images, extracts their Resnet features and fuses them with the features generated from the word sequence. The decoder uses these fused representation to generate the target sentence. Overall, I like the approach, seems like it can be easily augmented to existing NMT systems. \n\nOne of the claims of the paper was to be able to use monolingual image aligned data. However image captioning datasets are not mentioned. It would make sense to use image captioning data to create the image lookup. Also, what will be the performance of a standard image captioning system on the task ? I believe it will not be great, but I think for completeness, you should add such a baseline.\n\nMinor comments: \n1. What is M in Algorithm 1 ? \n2. First paragraph in related work is very unrelated to the current subject, please remove.\n", "title": "Official Blind Review #2", "rating": "8: Accept", "confidence": 3}, "rJe9E2ITYr": {"type": "review", "replyto": "Byl8hhNYPS", "review": "The authors propose to augment NMT with a grounded inventory of images.  The intuition is clear and the premise is very tempting.  The key architectural choice is to allow the transformer to use language embeddings to attend into a topic-image lookup table.  The proportion is learned to balance how much signal comes from each source.    Figure 4, attempts to investigate the importance of this sharing and its effects on performance.\n\nWhile reviewing this paper I went back and read the EN-DE evaluation data for the last few years trying to see how often I could reason that images would help and I came up severely lacking.  For example, \"The old system of private arbitration courts is off the table\" from DE-EN 2016 Dev doesn't seem like it should benefit from this architecture.  It's then hard for me to square that with the +VR gains seen throughout this work on non-grounded datasets.  I trust that the authors did in fact achieve these results but I cannot figure out how or why.  This is all further confused by the semantic topics used for clustering the images which ignores stop words and therefore spatial relations or any grammatical nuances.  \n\nIn contrast, it does make sense that Multi30K would benefit from this architecture.  As a minor note, were different feature extractors compared? The recent flurry of papers on multimodal transformers indicate that deeper resnet stacks correspond to improved downstream performance.  Is that also true in this domain?", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "BylD4R7NiB": {"type": "rebuttal", "replyto": "Byl8hhNYPS", "comment": "We thank all reviewers so much for the valuable comments on improving the quality of this work. We have updated the paper according to the feedback and our latest evaluations. The major revisions are marked in red for easy reading.\n\n1)\tWe add a discussion (Analysis 6.1) about the contribution of the lookup table for the improved results. The comparisons of different feature extractors are also included in this section. Detailed demonstrations are in Appendix A.2.\n\n2)\tWe add the discussion (Analysis 6.2) to demonstrate the effects of the number of sentence-image pairs including splitting the Multi30K and adding external MS COCO image caption datasets for comparisons.\n\n3)\tWe add the discussion of external computation time in Appendix A.1.\n\n4)\tWe add a page of more retrieved images for sentences in WMT datasets in Appendix A.4.\n", "title": "Submission Update"}, "rJgabTX4sr": {"type": "rebuttal", "replyto": "rJe9E2ITYr", "comment": "Thanks for your insightful comments.\n\n1. How or why is the benefit.\nThis comment is insightful and we also considered about it. Intuitively, we would easily fall into the connections between each sentence and image. However, it is nearly impossible to pair sentence with images with completely the same meaning all the time. According to our investigation, we conclude that the major contribution would be more effective contextualized sentence encoding for better representation from the visual clue combination instead of single image enhancement for encoding each individual sentence or word. \n\nAccording to Distributional Hypothesis (Harris et al., 1954) which states that \u201cwords that occur in similar contexts tend to have similar meanings\u201d, we are inspired to extend the concept in multimodal world, \u201cthe sentences with similar meanings would be likely to pair with similar even the same images\u201d, where the consistent images (with similar topic) could play the role of topic or type clues for similar sentence modeling. For your example, the topic words are {private, courts, table}, which can be paired with relevant images and other sentences with the same (similar) topics will be paired with the same (similar) group of images.\n\nThis is also very similar to the idea of word embedding by taking each image as a \u201cword\u201d. Because we use the average pooled output of ResNet, each image is represented as 2400d vector. For all the 29,000 images, we have an embedding layer with size (29000, 2400). The \u201ccontent\u201d of the image is just like the embedding initialization. It indeed makes effects, but the capacity of the neural network is not up to it. In contrast, the mapping from text word to the index in the word embedding is critical. Similarly, the mapping of sentence to image in image embedding would be essential, i.e., the similar sentences (with the same topic words) tend to map the similar images. \n\nTo verify the hypothesis, we shuffle the image embeddings but keep the lookup table, to only exchange the features of each image but maintain the sentence-image mapping. Unsurprisingly, the BLEU score (EN-RO) is 33.53, which is very close to the reported one (33.78). In addition, we randomly initialize the image embedding instead of ResNet, the result is 33.28. In comparison, if we randomly retrieve unrelated images to break the lookup, the result is 32.14. These results verify the necessity of the lookup table. We have added a detailed discussion in the paper (please see Analysis 6.1). \n\nWe believe this finding would be suggestive for the future research since most previous work focused on the content of the image itself. As a different research line, we highlight the consistency among the mono-modality to bridge the gap of language and image modeling.\n\n2. Why stop words are ignored.\nAccording to the explanation above, we think the spatial relations or grammatical nuances would not be so important in this task if we take the images as topic guidance. Ignoring the stopwords can help us get rid of the disturbance of unnecessary high-frequency words (such as function words) being the topic, as the standard practice for TF-IDF topic extraction.\n\n3. Comparison of different feature extractors.\nYes. We compared with ResNet101 and ResNet152 on EN-RO. The BLEU scores are 33.63 and 33.87. It seems deeper ResNet indeed gives better results but the difference is not very significant. \n", "title": "Response to Reviewer #1"}, "r1eQFhX4ir": {"type": "rebuttal", "replyto": "B1gYdNvCFS", "comment": "Thanks for your constructive feedbacks! Please see our response below.\n\n1. About image captioning.\n\nYes. Image captioning dataset is absolutely available for creating the lookup table. As you suggest, we use MS COCO Image captioning dataset to learn a lookup table and apply it to the EN-RO translation task to do the quick evaluation. As a result, the BLEU score is (33.55), which is comparable to the current lookup table (33.78) based on Multi30K, and outperforms the Trans. (base) (32.66). \n\nRegarding the performance of the standard image captioning system, we train a caption model (Show, Attend, and Tell (Xu et al., 2015b)) with fine-tuned encoder (ResNet101) on the COCO dataset to encode the images. The result on EN-RO is 33.58.  We are a little bit uncertain if we have well understood this request because our task is text to text translation while image captioning is image to text. If not, we are glad to address further. \n\n2. About the minor comments.\n\n(1)\tThis is typo. It is Q. \n\n(2)\tYes. We will remove it following your suggestion. \n", "title": "Response to Reviewer #2"}, "rJezfnmEoB": {"type": "rebuttal", "replyto": "ryxpUP6RKr", "comment": "Thanks so much for your constructive feedbacks. Please see our response below.\n\n1. Influence of the number of images:\nYes. The reason might be the higher chance of noise. It would be very important to provide a group of images that share similar patterns or topics. However, too many images for a sentence would have greater chance of noise.\n\n2. Impact of paired sentence-image dataset:\nYes. We add the external MS COCO image caption training set and evaluate on the EN-RO task for quick evaluation. The BLEU scores are 33.55 and 33.71 respectively for COCO only and Multi30K+COCO. \n\nIn addition, we are also interested in the influence of the number of sentence-image pairs inspired by your suggestion. We randomly split the pairs of Multi30K into the proportion in [0.1, 0.3, 0.5, 0.7, 0.9], the corresponding BLEU scores are [33.07, 33.44, 34.01, 34.06, 33.80] respectively. These results indicate that a modest number of pairs would be beneficial.\n\n3. The extra computation:\nThe extra computation is negligible. \n\nThe time of obtaining image data for MT sentences for EN-RO dataset, for example, is approximately less than 1 minute by tensor operation in GPU. The lookup table is formed as the mapping of token (only topic words) index to image id. Then, the retrieval method is applied as the tensor indexing from the sentence token (only topic words) index to image ids, which is the same as the procedure of word embedding. The retrieved image ids are then sorted by frequency. \n\nLearning image representations takes only about 2 minutes for all the 29,000 images in Multi30K using 6G GPU memory for feature extraction and 8 threads of CPU for transforming images. The extracted features are formed as the \u201cimage embedding layer\u201d with the size of (29000, 2400) for quick accessing in neural network. \n\n4. Missing BLEU scores & the number of parameters:\nBecause those missing numbers (N/A) are not reported in the corresponding literature. \n", "title": "Response to Reviewer #3"}, "HJxJBYhb_S": {"type": "rebuttal", "replyto": "rygYkNlb_H", "comment": "Thanks for your interest and constructive comments!\n\nResponse-to-comment-1. In the introduction, we intend to indicate that visual representation has been shown beneficial by some studies. As you mentioned, we agree that it is an open question, as we discussed in Section 2 and Section 5.3. We will clarify that \u201cit is still an open question\u201d in the introduction part and add more discussions accordingly in the later version.\n\nIn previous literatures, visual information is primarily applied to the translation task over Multi30K dataset. Nevertheless, the conclusion about the benefit of the visual modality is still unclear. In this work, we propose to investigate the effectiveness in different and more universal scenarios. We are motivated to comprehensively explore and evaluate the potential of visual modality on more datasets that are in different scales. Our method relies only on image-monolingual annotations instead of the existing approach that depends on image-bilingual annotations, thus breaking the bottleneck of using visual information in NMT. Our experimental results and analysis verify the effectiveness of the visual representation. \n\nBased on the motivation above, we primarily focus on the evaluations of the stable improvements on the baselines (instead of SOTA comparisons), especially for the text-only NMT and low-resource NMT (Table 1) without manually-annotated text-image pairs. Therefore, we show the results of some public baseline models (including the transformer in Ive et al. (2019), instead of the deliberation network) for reference in Table 1-2, only to indicate that our implemented transformer models showed similar BLEU scores with other public reported transformers. As you mentioned, we will also add the SOTA performances in the revised version to show the effect of visual information in various scenarios. \n\nResponse-to-comment-2. That is the official baseline (text-only NMT) on WMT17-Multi30K 2017 test data in Elliott et al. (2017). We will make it clear by noting in the table caption to avoid confusing. In addition, we will add the result of EN-DE in Elliott and K\u00e1d\u00e1r (2017) for more comprehensive reference.\n", "title": "Response"}}}