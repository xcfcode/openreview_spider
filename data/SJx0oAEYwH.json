{"paper": {"title": "Cover Filtration and Stable Paths in the Mapper", "authors": ["Dustin L. Arendt", "Matthew Broussard", "Bala Krishnamoorthy", "Nathaniel Saul"], "authorids": ["dustin.arendt@pnnl.gov", "matthew.broussard@wsu.edu", "kbala@wsu.edu", "nat@riverasaul.com"], "summary": "A new filtration from a SINGLE cover, with applications to movie recommendations and explainable machine learning", "abstract": "The contributions of this paper are two-fold. We define a new filtration called the cover filtration built from a single cover based on a generalized Steinhaus distance, which is a generalization of Jaccard distance. We then develop a language and theory for stable paths within this filtration, inspired by ideas of persistent homology. This framework can be used to develop several new learning representations in applications where an obvious metric may not be defined but a cover is readily available. We demonstrate the utility of our framework as applied to recommendation systems and explainable machine learning.\n\nWe demonstrate a new perspective for modeling recommendation system data sets that does not require manufacturing a bespoke metric. As a direct application, we find that the stable paths identified by our framework in a movies data set represent a sequence of movies constituting a gentle transition and ordering from one genre to another.\n\nFor explainable machine learning, we apply the Mapper for model induction, providing explanations in the form of paths between subpopulations. Our framework provides an alternative way of building a filtration from a single mapper that is then used to explore stable paths. As a direct illustration, we build a mapper from a supervised machine learning model trained on the FashionMNIST data set. We show that the stable paths in the cover filtration provide improved explanations of relationships between subpopulations of images.\n", "keywords": ["cover and nerve", "Jaccard distance", "stable paths in filtration", "Mapper", "recommender systems", "explainable machine learning"]}, "meta": {"decision": "Reject", "comment": "The paper proposes a filtration based on the covers of data sets and demonstrates its effectiveness in recommendation systems and explainable machine learning. The paper is theory focused, and the discussion was mainly centered around one very detailed and thorough review. The main concerns raised in the reviews and reiterated at the end of the rebuttal cycle was lack of clarity, relatively incremental contribution, and limited experimental evaluation. Due to my limited knowledge of this particular field, I base my recommendation mostly on R1's assessment and recommend rejecting this submission."}, "review": {"HJxtGnD3jH": {"type": "rebuttal", "replyto": "ryeddrwhor", "comment": "Here are some more details for the paragraph next to Figure 10 (in Page 18, part of Appendix) in the paper:\n\n\n    We initially train a model using a very standard and naive approach. First, we reduce the dimensionality of the images to 100 using Principal Components Analysis and use this 100 dimensional space to train a LR model with L1 regularization on the FashionMNIST designated train set of 60k images. \n\n\n    This naive approach is satisfactory to demonstrate the use of the Mapper filtration. It provides us a mapping from a 100 dimensional space to a 10 dimensional space.\n\n\n    Unfortunately, Mapper is unable to operate on a lens of 10 dimensions because of the curse of dimensionality. For this reason, we must reduce this space to ~2 dimensions.  Here, we opt for the dimensionality reduction method de jour UMAP as it preserves some local connectivity in the data set. Any other density based dimensionality reduction would be suitable here as they tend to maintain local relationships of the data, which is most important for preserving when exploring topology. The parameters for UMAP are taken as the default.\n\n\n    For this example, we chose 40 bins with a 50% overlap.  The number of bins is best chosen to help with the visualization density of each node and should be scaled to the number of observations being plotted. In two dimensions, this results in 40*40= 1600 bins and expected 37.5 observations per bin.  This order of magnitude is digestible when viewing the Mapper. As for the overlap, we opt for a larger overlap than is usually used in the literature. This is because we can leverage the stability of paths in later steps to filter out noise resulting in the over connected from high overlap.\n\n\n    Finally, for the choice of clustering algorithm, we opt for DBSCAN as a default as it handles the density of data well. The parameters are taken as the default.\n\n\nPlease let us know if we could provide any more specific details.\n", "title": "FashionMNIST Experiment details"}, "SJldYkNnoS": {"type": "rebuttal", "replyto": "HklZVJ42ir", "comment": "\n4. Experiments\n\ni. As a general question, I would like to see how stable the\n  interpretations of the different sections are. There is always\n  a degree of stochasticity when training a neural network, so does\n  the output of Mapper stay stable?\n\n    -> We posit that the theoretical stability guarantees for our\n       framework provides some level of stability to such variations.\n\nii. I am not sure about the relevance of experiment in Section 4.1; it\n  does not really provide a link to an explainable model for me, but\n  instead is more along the lines of 'gentle interpolation in a latent\n  space'. I could see that this has its uses, but an evaluation requires\n  more than one example. I would be particularly interested in knowing\n  about cases in which result is unexpected. Do such cases exist?\n\n    -> We realize it is tricky to evaluate such recommendations, since\n       there is no \"ground truth\" to compare. We highlighted the\n       Mulan->Moulin Rouge example since it presented a highly\n       illustrative instance for the kind of paths we are looking\n       for. We explored a few other interesting cases, but have not\n       presented details due to space limitations.\n\n\niii. Please comment on the stability of the sampling procedure in Section 4.1\n\n    -> The sampling was performed mostly to ensure the overall\n       computational pipeline runs on a typical laptop in good\n       time. There is nothing suggesting outcomes would be drastically\n       different if we try the pipeline on much larger samples, or\n       even the whole dataset.\n\n       Once we received this round of reviews, we considered setting\n       up a run for the whole data set on AWS. But we were not able to\n       finish the experiment on time for the rebuttal. We plan to\n       pursue this experiment in the near future, though.\n\n\n\niv. I understand that Section 4.2 wants to show how algorithms like Mapper can\n  act as a middle ground between black-box and white-box models. Is this\n  correct? If so, the chosen example is _not_ sufficiently complex to be\n  compelling. Would it not be equally appropriate to rewrite this\n  experiment in terms of explaining paths in the latent space of\n  autoencoders? In this context, I see more of a need for these paths as\n  an alternative to paths that purely employ latent space geometry.\n\n    -> Thanks for the suggestion. We will explore this in future work.\n\n\nv. Adding to this, the autoencoder context would it also make possible to\n  compare paths generated using Mapper and the proposed as well as the\n  paths generated by the model. Such an experiment would be more\n  compelling, I think.\n\n    -> Thanks for the suggestion. We will explore this in future work.\n\n\nvi. In general, what are the implications of understanding a path as in\n  Figure 9? How could the model updated to account for this? If the\n  paper were to include an example of how to *fix* a model based on such\n  information, it would a highly compelling use case.\n\n    -> Again, great idea. Will explore in future work.\n       We have listed this problem in our Conclusions.\n\n5. Minor style issues\n\ni. Mapper should be capitalised consistently\n\n   ->Replaced all instances of 'mapper' with 'Mapper'\n\nii. I suggest removing the paragraph 'Organization' as it is redundant\n\n   ->Remove (Done)\n\niii. 'mod differences' --> 'modulo differences' (?)\n\n   ->Replace (Done)\n\niv. 'distanceof' --> distance of'\n\n   ->Replace (Done)\n\nv. 'on undirected graph' --> 'on an undirected graph'\n\n   ->Replace (Done)\n\nvi. 'Principle component analysis' --> 'Principal component analysis'\n\n   ->Replace (Done)\n", "title": "Responses to Review #1 - Part 4"}, "HklZVJ42ir": {"type": "rebuttal", "replyto": "S1efh072jB", "comment": "\nxxi. The algorithm in Figure 2 should include some comments that make the\n  procedure  more understandable. I found the conceptual leap to Pareto\n  frontiers confusing at first; maybe this could be motivated better.\n\n     -> We present the motivation and details in the para to the right\n        of Figure 2 (the algorithm).\n\n\n\nxxii. Figure 3 needs more explanations. In particular the connection to\n  persistence could be elucidated---I get that there's an immediate\n  connection (see the paper above) but this concept needs at least\n  a brief introduction. Moreover, if this connection is relevant, it\n  deserves to be elucidated in the paper explicitly.\n\n      -> We are not making any strong claims about the direct\n         correspondence to persistence. It is only a superficial\n         motivation, and hence we changed the language to say\n         \"...similar in a loose sense to computing persistent\n         homology\" (at the end of first para in this Section).\n\n\nxxiii. Figure 4 needs additional labels; I expect that 'distance/stability'\n  are shown in the caption. Is this correct?\n\n  In general, I found the path visualisation not so helpful; maybe it\n  would be better to show the full 1-skeleton (or an excerpt) and\n  highlight the corresponding paths?\n\n  -> We added \"Length/\\rho shown on top\" to the caption.\n\n     We highlight the portion of Mapper in Figure 8 on which the\n     stable/short paths from the corresponding Pareto frontier (Fig\n     10) are shown.\n\n\nxxiv. The language and motivation in Section 4 is somewhat informal; this is\n  not an issue for me, but I wanted to mention it as a something that\n  could potentially be rephrased.\n\n\n\nxxv. The additional application domains should be moved from Section 4 to\n  a discussion section.\n\n   ->  Moved to Conclusion.\n\n\nxxvi. The cover generation strategy could be understood more rapidly if\n  a small illustration was provided. Do I get it correctly that every\n  *cover set* constitutes a film, while the overlap between them is\n  based on whether the same user also rated another film?\n\n   -> That is correct.\n\n\n\nxxvii. The path selection procedure for the experiment in Section 4.1 needs\n  to be explained formally. I get that it is akin to selecting a point\n  on the 'elbow' of a curve, but this should be briefly explained.\n\n  -> We explain the path selection in the paragraph to the left of\n     Figure 7 (the Pareto frontier figure in this Section). There is a\n     marked increase in stability from the 3-edge path to 4-edge path,\n     and hence we choose the 4-edge path as the shortest path.\n\n\n\nxxviii. Figure 5 needs to be referenced more prominently in the text.\n\n   -> Moved to Appendix now, and mentioned in text right next to the\n      Figure.\n\n\nixxx. Section 4.2 is glossing over many important details of the definition\n  of Mapper. These are absolutely required, though, and any parameters\n  selected here should also be analysed to learn whether they affect the\n  results.\n\n   -> We have added some details about how we chose the # bins and\n      overlap % for the Mapper (below Figure 10).\n\nxxx. I am not sure about the utility of the paths in Figure 7\n\n   -> The Figure illustrates how length (in # edges) and stability get\n      traded off within the window highlighted in the Mapper in Figure\n      7 (old Figure 6).\n\nxxxi. Figure 9 needs more explanations; as mentioned below, why not compare\n  this path with a geometry-based path in the embedding? Moreover, why\n  are there not the same number of images for all classes?\n\n   -> The caption has been expanded. In particular, we added that \"\n      Columns with no shoes shown had no representative of that class\n      in the node.\"\n\n\nxxxii. The conclusion needs to be rewritten; the paper does *not* show\n  stability properties in the main text\n\n   -> Stability is presented in main text now.", "title": "Responses to Review #1 - Part 3"}, "S1efh072jB": {"type": "rebuttal", "replyto": "HJeJ8RXhjS", "comment": "ix.   Discussing stability of the paths/cover in the introduction is\n      misleading because these aspects are only discussed in the\n      appendix.  Same goes for some of the contributions in Section\n      1.1; the conjectures or connection to other filtrations are only\n      mentioned in the appendix.\n\n      This needs to be rewritten; I would pick a few contributions\n      that the main text can focus on and mention the rest in passing.\n\n  The paper lacks this structure at present and for a conference\n  submission, all main contributions should also be a part of the main\n  text.\n\n      ->We have now brought stability into main text, send some experimental\n        detail to the appendix.\n\n\nx. Stability is meant to reflect a property of a cover; this could be\n  clarified by means of extending Figure 1, for example. During my\n  first pass of the paper, some of the subsequent definitions were\n  lacking a clear motivation. I would suggest to _clearly_ state that\n  the goal is to circumvent issues with paths in a cover that only\n  depend on a few data points.\n\n      ->Bring discussion of overlaps containing single point into main\n         text, not just caption to figure 1?\n\n\nxi. Moreover, it would be interesting to point out to what extent the\n  method presented here is the _only_ one capable of doing this. It\n  occurred to me that there's a preprint that discusses how to stabilise\n  the calculations of persistent homology (with respect to picking the\n  _same_ creator simplex for a feature, for example):\n\n    Bendich, Paul et al.\n    Stabilizing the unstable output of persistent homology computations\n    https://arxiv.org/abs/1512.01700\n\n  It seems that one of the unique features of the method in this paper\n  is that the definition of path stability aligns exactly with the other\n  stability concept---the paths are thus indeed stable with respect to\n  perturbations of the underlying data set.\n\n  Maybe this could be mentioned as an interesting feature.\n\n  -> Thanks much for pointing us to this paper. We now cite it in the\n     3rd paragraph in Related Work, and also point out the unique\n     feature related to stability that out framework provides.\n\n\n\nxii. The paragraph 'In Section 4.1 [...]' in the related work section is\n  somewhat redundant; I would suggest putting it at the beginning of the\n  respective experimental section and merging it with the existing\n  description there.\n\n  ->We moved this para to the start of the Subsection on Rec Sys.\n\n\nxiii. The definition of the Steinhaus distance strikes me as unnecessary\n  complex; while it is mathematically pleasing to know that arbitrary\n  measures could be used, it seems that this is never exploited anywhere\n  in the paper.\n\n  -> We now present results on equivalence of our construction to\n     Cech/VR filtrations in the main text. The more general Steinhaus\n     distance is needed in that context.\n\n\nxiv. In the interest of clarity, maybe it would make sense to refer to the\n  distance as a 'generalised Jaccard distance' instead.\n\n  -> See above. We mention we use only the generalized Jaccard\n     distance mostly.\n\nxv. The proof of Theorem 2.5 could be moved to the appendix. Moreover,\n  I think it could be simplified by mentioning that the nerve is\n  constructed from the cover and that weights are assigned based on\n  the number of intersections; then the main result would follow from\n  monotonicity. (the current proof is of course fully correct, I just\n  had to rephrase this result in terms of concepts that I found easier\n  to grasp)\n\n     ->The proof is now moved to Appendix.\n\n\nxvi. Concerning the terminology, I find 'a most stable path' to be somewhat\n  confusing at the first read. I now understand what is meant by it, but\n  maybe 'highly stable' or 'maximally stable' would be better.\n\n     ->Replaced 'most stable path' with 'maximally stable path'\n\n\nxvii. In Section 3, the 1-skeleton should be explicitly mentioned.\n\n     ->We explicitly mention 1-skeleton now.\n\nxviii. Moreover, the notion of 'shortest path', which _could_ also employ\n  distances, should be distinguished from stable paths; if I understand\n  the argumentation correctly, shortest paths are defined in terms of\n  the number of edges, while stable paths are defined in terms of the\n  weights along those edges. I would suggest spelling this out directly\n  to make the concepts non-ambiguous.\n\n  -> Added clarification at start of the Section to this effect.\n\n\nixx. In Definition 3.1, it should be d_st, not D_st, I think.\n\n     ->Replaced. Also in Figure 2.\n\nxx. Definition 3.1 could also be intuitively summarised as 'the largest\n  edge weight along a path', right?\n\n     -> Yes. We've added a sentence to this effect right after the\n        Definition (Definition 5.1 now).\n", "title": "Responses to Review #1 - Part 2"}, "HJeJ8RXhjS": {"type": "rebuttal", "replyto": "SylLQHX9tH", "comment": "\nA. Missing clarity: the paper requires readers to be familiar with the\n   Mapper algorithm and glosses over important details. Several\n   results that are mentioned as core contributions in the main texts\n   are only stated in the supplementary materials.\n\n   -> We have reorganized the paper substantially. We now have\n      stability and equivalence results in the main text, and details\n      of experiments in Appendix\n\n\nB. Missing experimental depth: the experiments shown in the paper are\n   interesting, but only scratch the surface. To have a convincing\n   case study, a more quantitative analysis is required. At present,\n   the selection and depiction of results seems ad-hoc. I realise\n   that, when the foremost goal is interpretability, providing a\n   quantitative analysis is not always easily possible. Nevertheless,\n   each data set should be used for more than one case study; ideally,\n   multiple results that are 'surprising' can be reported. For\n   example, are there paths that do not match our intuition? What do\n   they tell us about the data set?\n\n   -> Please see our responses to specific comments below.\n\n\nC. Doubts about the technical correctness: the supervised learning\n   example requires an elaborate setup (dimensionality reduction, UMAP\n   for further reductions, setting up Mapper with additional\n   parameters). I am wondering how trustworthy any result obtained\n   from such an analysis can be. I would suggest discussing the\n   parameter selection process in more detail and ideally providing\n   more information about the impact of these choices.\n\n   -> Please see our responses to specific comments below.\n\n\nIn the following, I will comment on these issues in more detail.\n\n3. Clarity\n\ni.    Concepts such as 'cover' should be briefly introduced; already the\n      abstract presumes that readers are familiar with several TDA\n      concepts\n\n      -> We have added a 2D illustration of the default Mapper\n         construction in the Introduction, with a figure.\n\n\nii.   I would shorten the abstract to improve its flow; the paragraphs\n      'We demonstrate...' and 'For explainable...' could also be added\n      to the introduction to improve the exposition of the paper.\n\n      -> Abstract modified and shortened. Those details are presented\n         in the intro (last para under 1.1 Our Contributions).\n\n\niii.  The introduction already starts with TDA concepts; a brief\n      motivation would make the paper more accessible\n\n      -> We have added more details now.\n\n\niv.   I disagree with the statement that the existence of a metric on\n      the data is an implicit assumption of TDA. My perspective is\n      quite the opposite: TDA is rather flexible *because* of its\n      support for different metrics; Vietoris--Rips complex\n      calculation, for example, only requires a matrix of pairwise\n      distances. Hence, I would rewrite this sentence.\n\n      -> Rewritten. See response below.\n\n\nv.    To add to the previous point: the paper itself introduces a new\n      metric based on a generalised Jaccard distance. I feel that the\n      discussion of 'ill-fit or incomplete' metrics detracts from the\n      main message of the paper.\n\n      -> Removed the words \"ill-fit or incomplete\".\n\n\nvi.   The first explanation/definition of Mapper in the introduction is\n      rather complicated ('nerve of a refined pullback cover'); I\n      would suggest rephrasing this.\n\n      ->Changed it to make things more intuitive. Removed the jargon\n        sentence.\n\n\n\nvii.  The point about the 1-skeleton warrants more explanation: it is\n      my understanding that the paper uses paths that are defined\n      using this skeleton as well. My first pass of the paper slightly\n      confused me because I figured that the described paths would be\n      generalised over high-dimensional simplices as well. I would\n      suggest making this 'restriction' (it is not a proper\n      restriction because paths can be defined for arbitrary\n      simplices, but it is a restriction in terms of the\n      dimensionality) or property clear from the beginning.\n\n      ->We state explicitly that we work with 1-skeleton now.\n\n      \tBut technically, the framework could look at d-chains in the\n      \tMapper for d >= 2. Of course, it will be harder to interpret\n      \tsuch d-chains in the case of real data.\n\n\nviii. The discussion of hypercube covers is repeated multiple times in\n      the introduction and the related work section; I would suggest\n      mentioning this only once as it does not have to a large bearing\n      on the methods described in the paper anyway.\n\n      -> We mention it only once now.\n", "title": "Responses to Review #1 - Part 1"}, "H1xbfnm3sS": {"type": "rebuttal", "replyto": "Skl0rFWi9B", "comment": "  \n2. The method itself is built upon the SOTA method \u201cMapper\u201d. Authors\n   may want to clearly state the difference between the new method and\n   Mapper in writing.\n\n   -> We introduce Mapper using a picture in the Introduction now.\n\n\n3. The overall structure of this paper needs some improvement. Authors\n   attempt to introduce a new distance metric as an intermediate level\n   representation. This is not very clear at the beginning; instead,\n   most of the introduction is related to TDA and mapper, which may\n   confuse people not from this particular field.\n\n   -> We have comprehensively restructured the paper as detailed here\n      and in the our responses to Review #1.\n\n\n\n4. What is the meaning of D_St under definition 3.1? Is this the same\n   as d_St or not?\n\n   -> Changed  D_st to d_st in def 3.1\n\n\n5. It seems the restrictions in the experiments are strong, which can\n   be found from a few settings in the experiments. For example, the\n   authors claim that \u201cTo reduce computational expenses and noise, we\n   remove all movies with less than 10 ratings and then sample 4000\n   movies at random from the remaining movies.\u201d It seems the model may\n   not work well in a more general case...\n\n   -> Please see response to related comment from Review #1.\n\n\n\n6. The pre-processing steps for filtration cover seem verbose on\n   FashionMNIST \u201cThe model is evaluated at 93% accuracy on the\n   remaining 10,000 images. We then extract the 10-dimensional\n   predicted probability space and use UMAP (McInnes & Healy, 2018) to\n   reduce the space to 2 dimensions. This 2-dimensional space is taken\n   as the filter function of the Mapper, using a cover consisting of\n   40 bins along each dimension with 50% overlap between each bin.\u201d\n   Not sure if this method will work well without fine-tuning or\n   feature selection like this.\n\n   -> Please see response to related comment from Review #1.\n", "title": "Responses to Review #3"}, "SylLQHX9tH": {"type": "review", "replyto": "SJx0oAEYwH", "review": "1. Paper summary\n\nThis paper develops a novel filtration for the analysis of based on the\nidea of *covers* of data sets. The filtration employs the notion of\na generalised Jaccard distance to define stable paths within the nerve\nof the cover. These stable paths are then shown to be useful for the\ncreation of 'gentler' transitions for recommendation systems, as well\nas the development of explainable supervised machine learning models.\n\n2. Review summary\n\nI found this paper to be very interesting in terms of its new\nperspective on filtrations and its idea of the introducing a\n'stability' concept inspired by topological persistence.\nHowever, I recommend rejecting the paper in its current form due to the\nfollowing issues:\n\nA. Missing clarity: the paper requires readers to be familiar with the\n   Mapper algorithm and glosses over important details. Several results\n   that are mentioned as core contributions in the main texts are only\n   stated in the supplementary materials.\n\nB. Missing experimental depth: the experiments shown in the paper are\n   interesting, but only scratch the surface. To have a convincing case\n   study, a more quantitative analysis is required. At present, the\n   selection and depiction of results seems ad-hoc. I realise that,\n   when the foremost goal is interpretability, providing a quantitative\n   analysis is not always easily possible. Nevertheless, each data set\n   should be used for more than one case study; ideally, multiple\n   results that are 'surprising' can be reported. For example, are\n   there paths that do not match our intuition? What do they tell us\n   about the data set?\n\nC. Doubts about the technical correctness: the supervised learning\n   example requires an elaborate setup (dimensionality reduction,\n   UMAP for further reductions, setting up Mapper with additional\n   parameters). I am wondering how trustworthy any result obtained\n   from such an analysis can be. I would suggest discussing the\n   parameter selection process in more detail and ideally providing more\n   information about the impact of these choices.\n\nIn the following, I will comment on these issues in more detail.\n\n3. Clarity\n\n- Concepts such as 'cover' should be briefly introduced; already the\n  abstract presumes that readers are familiar with several TDA concepts\n\n- I would shorten the abstract to improve its flow; the paragraphs 'We\n  demonstrate...' and 'For explainable...' could also be added to the\n  introduction to improve the exposition of the paper.\n\n- The introduction already starts with TDA concepts; a brief motivation\n  would make the paper more accessible\n\n- I disagree with the statement that the existence of a metric on the\n  data is an implicit assumption of TDA. My perspective is quite the\n  opposite: TDA is rather flexible *because* of its support for\n  different metrics; Vietoris--Rips complex calculation, for example,\n  only requires a matrix of pairwise distances. Hence, I would rewrite this\n  sentence.\n\n- To add to the previous point: the paper itself introduces a new\n  metric based on a generalised Jaccard distance. I feel that the\n  discussion of 'ill-fit or incomplete' metrics detracts from the\n  main message of the paper.\n\n- The first explanation/definition of Mapper in the introduction is\n  rather complicated ('nerve of a refined pullback cover'); I would\n  suggest rephrasing this.\n\n- The point about the $1$-skeleton warrants more explanation: it is my\n  understanding that the paper uses paths that are defined using this\n  skeleton as well. My first pass of the paper slightly confused me\n  because I figured that the described paths would be generalised over\n  high-dimensional simplices as well. I would suggest making this\n  'restriction' (it is not a proper restriction because paths can be\n  defined for arbitrary simplices, but it is a restriction in terms of\n  the dimensionality) or property clear from the beginning.\n\n- The discussion of hypercube covers is repeated multiple times in the\n  introduction and the related work section; I would suggest mentioning\n  this only once as it does not have to a large bearing on the methods\n  described in the paper anyway.\n\n- Discussing stability of the paths/cover in the introduction is\n  misleading because these aspects are only discussed in the appendix.\n  Same goes for some of the contributions in Section 1.1; the\n  conjectures or connection to other filtrations are only mentioned in\n  the appendix.\n\n  This needs to be rewritten; I would pick a few contributions that the\n  main text can focus on and mention the rest in passing.\n\n  The paper lacks this structure at present and for a conference\n  submission, all main contributions should also be a part of the main\n  text.\n\n- Stability is meant to reflect a property of a cover; this could be\n  clarified by means of extending Figure 1, for example. During my\n  first pass of the paper, some of the subsequent definitions were\n  lacking a clear motivation. I would suggest to _clearly_ state that\n  the goal is to circumvent issues with paths in a cover that only\n  depend on a few data points.\n\n- Moreover, it would be interesting to point out to what extent the\n  method presented here is the _only_ one capable of doing this. It\n  occurred to me that there's a preprint that discusses how to stabilise\n  the calculations of persistent homology (with respect to picking the\n  _same_ creator simplex for a feature, for example):\n\n    Bendich, Paul et al.\n    Stabilizing the unstable output of persistent homology computations\n    https://arxiv.org/abs/1512.01700\n\n  It seems that one of the unique features of the method in this paper\n  is that the definition of path stability aligns exactly with the other\n  stability concept---the paths are thus indeed stable with respect to\n  perturbations of the underlying data set.\n\n  Maybe this could be mentioned as an interesting feature.\n\n- The paragraph 'In Section 4.1 [...]' in the related work section is\n  somewhat redundant; I would suggest putting it at the beginning of the\n  respective experimental section and merging it with the existing\n  description there.\n\n- The definition of the Steinhaus distance strikes me as unnecessary\n  complex; while it is mathematically pleasing to know that arbitrary\n  measures could be used, it seems that this is never exploited anywhere\n  in the paper.\n\n- In the interest of clarity, maybe it would make sense to refer to the\n  distance as a 'generalised Jaccard distance' instead.\n\n- The proof of Theorem 2.5 could be moved to the appendix. Moreover,\n  I think it could be simplified by mentioning that the nerve is\n  constructed from the cover and that weights are assigned based on\n  the number of intersections; then the main result would follow from\n  monotonicity. (the current proof is of course fully correct, I just\n  had to rephrase this result in terms of concepts that I found easier\n  to grasp)\n\n- Concerning the terminology, I find 'a most stable path' to be somewhat\n  confusing at the first read. I now understand what is meant by it, but\n  maybe 'highly stable' or 'maximally stable' would be better.\n\n- In Section 3, the 1-skeleton should be explicitly mentioned.\n\n- Moreover, the notion of 'shortest path', which _could_ also employ\n  distances, should be distinguished from stable paths; if I understand\n  the argumentation correctly, shortest paths are defined in terms of\n  the number of edges, while stable paths are defined in terms of the\n  weights along those edges. I would suggest spelling this out directly\n  to make the concepts non-ambiguous.\n\n- In Definition 3.1, it should be $d_{St}$, not $D_{St}$, I think.\n\n- Definition 3.1 could also be intuitively summarised as 'the largest\n  edge weight along a path', right?\n\n- The algorithm in Figure 2 should include some comments that make the\n  procedure  more understandable. I found the conceptual leap to Pareto\n  frontiers confusing at first; maybe this could be motivated better.\n\n- Figure 3 needs more explanations. In particular the connection to\n  persistence could be elucidated---I get that there's an immediate\n  connection (see the paper above) but this concept needs at least\n  a brief introduction. Moreover, if this connection is relevant, it\n  deserves to be elucidated in the paper explicitly.\n\n- Figure 4 needs additional labels; I expect that 'distance/stability'\n  are shown in the caption. Is this correct?\n\n  In general, I found the path visualisation not so helpful; maybe it\n  would be better to show the full 1-skeleton (or an excerpt) and\n  highlight the corresponding paths?\n\n- The language and motivation in Section 4 is somewhat informal; this is\n  not an issue for me, but I wanted to mention it as a something that\n  could potentially be rephrased.\n\n- The additional application domains should be moved from Section 4 to\n  a discussion section.\n\n- The cover generation strategy could be understood more rapidly if\n  a small illustration was provided. Do I get it correctly that every\n  *cover set* constitutes a film, while the overlap between them is\n  based on whether the same user also rated another film?\n\n- The path selection procedure for the experiment in Section 4.1 needs\n  to be explained formally. I get that it is akin to selecting a point\n  on the 'elbow' of a curve, but this should be briefly explained.\n\n- Figure 5 needs to be referenced more prominently in the text.\n\n- Section 4.2 is glossing over many important details of the definition\n  of Mapper. These are absolutely required, though, and any parameters\n  selected here should also be analysed to learn whether they affect the\n  results.\n\n- I am not sure about the utility of the paths in Figure 7\n\n- Figure 9 needs more explanations; as mentioned below, why not compare\n  this path with a geometry-based path in the embedding? Moreover, why\n  are there not the same number of images for all classes?\n\n- The conclusion needs to be rewritten; the paper does *not* show\n  stability properties in the main text\n\n4. Experiments\n\n- As a general question, I would like to see how stable the\n  interpretations of the different sections are. There is always\n  a degree of stochasticity when training a neural network, so does\n  the output of Mapper stay stable?\n\n- I am not sure about the relevance of experiment in Section 4.1; it\n  does not really provide a link to an explainable model for me, but\n  instead is more along the lines of 'gentle interpolation in a latent\n  space'. I could see that this has its uses, but an evaluation requires\n  more than one example. I would be particularly interested in knowing\n  about cases in which result is unexpected. Do such cases exist?\n\n- Please comment on the stability of the sampling procedure in Section 4.1\n\n- I understand that Section 4.2 wants to show how algorithms like Mapper can\n  act as a middle ground between black-box and white-box models. Is this\n  correct? If so, the chosen example is _not_ sufficiently complex to be\n  compelling. Would it not be equally appropriate to rewrite this\n  experiment in terms of explaining paths in the latent space of\n  autoencoders? In this context, I see more of a need for these paths as\n  an alternative to paths that purely employ latent space geometry.\n\n- Adding to this, the autoencoder context would it also make possible to\n  compare paths generated using Mapper and the proposed as well as the\n  paths generated by the model. Such an experiment would be more\n  compelling, I think.\n\n- In general, what are the implications of understanding a path as in\n  Figure 9? How could the model updated to account for this? If the\n  paper were to include an example of how to *fix* a model based on such\n  information, it would a highly compelling use case.\n\n- Moreover, would it be possible to detect 'problematic' regions such as\n  the one shown in Figure 9 automatically? If so, it would again provide\n  a highly compelling example. Otherwise, I think that the\n  interpretation of a model depends again on humans and are thus\n  restricted based on model complexity.\n\n5. Minor style issues\n\n- Mapper should be capitalised consistently\n- I suggest removing the paragraph 'Organization' as it is redundant\n- 'mod differences' --> 'modulo differences' (?)\n- 'distanceof' --> distance of'\n- 'on undirected graph' --> 'on an undirected graph'\n- 'Principle component analysis' --> 'Principal component analysis'", "title": "Official Blind Review #1", "rating": "1: Reject", "confidence": 4}, "Skl0rFWi9B": {"type": "review", "replyto": "SJx0oAEYwH", "review": "The paper presents an interesting filtration method to find staple maps, which proves effective in the recommendation system and explainable machine learning. The new framework is built upon a new concept termed cover filtration based on the generalized Steinhaus distance derived from Jaccard distance. A staple path discovery mechanism is then developed using this filtration based on persistent homology. Experiments on Movielense and FashionMNIST has quantitatively and qualitatively demonstrated the effectiveness of the new model. It also showcases the explainable factors with visualized samples from FashionMNIST dataset. In addition, theoretical discussion in the appendix makes the theory part of this work solid and convincing.\n\n* The method introduced in this paper is intuitive and demonstrates with meaningful outcomes.\n\n* The method itself is built upon the SOTA method \u201cMapper\u201d. Authors may want to clearly state the difference between the new method and Mapper in writing.\n\n* The overall structure of this paper needs some improvement. Authors attempt to introduce a new distance metric as an intermediate level representation. This is not very clear at the beginning; instead, most of the introduction is related to TDA and mapper, which may confuse people not from this particular field.\n\n* What is the meaning of D_St under definition 3.1? Is this the same as d_St or not?\n\n* It seems the restrictions in the experiments are strong, which can be found from a few settings in the experiments. For example, the authors claim that \u201cTo reduce computational expenses and noise, we remove all movies with less than 10 ratings and then sample 4000 movies at random from the remaining movies.\u201d It seems the model may not work well in a more general case...\n\n* The pre-processing steps for filtration cover seem verbose on FashionMNIST \u201cThe model is evaluated at 93% accuracy on the remaining 10,000 images. We then extract the 10-dimensional predicted probability space and use UMAP (McInnes & Healy, 2018) to reduce the space to 2 dimensions. This 2-dimensional space is taken as the filter function of the Mapper, using a cover consisting of 40 bins along each dimension with 50% overlap between each bin.\u201d Not sure if this method will work well without fine-tuning or feature selection like this.\n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}}}