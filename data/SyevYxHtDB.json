{"paper": {"title": "Prediction Poisoning: Towards Defenses Against DNN Model Stealing Attacks", "authors": ["Tribhuvanesh Orekondy", "Bernt Schiele", "Mario Fritz"], "authorids": ["orekondy@mpi-inf.mpg.de", "schiele@mpi-inf.mpg.de", "fritz@cispa.saarland"], "summary": "We propose the first approach that can resist DNN model stealing/extraction attacks", "abstract": "High-performance Deep Neural Networks (DNNs) are increasingly deployed in many real-world applications e.g., cloud prediction APIs. Recent advances in model functionality stealing attacks via black-box access (i.e., inputs in, predictions out) threaten the business model of such applications, which require a lot of time, money, and effort to develop. Existing defenses take a passive role against stealing attacks, such as by truncating predicted information. We find such passive defenses ineffective against DNN stealing attacks. In this paper, we propose the first defense which actively perturbs predictions targeted at poisoning the training objective of the attacker. We find our defense effective across a wide range of challenging datasets and DNN model stealing attacks, and additionally outperforms existing defenses. Our defense is the first that can withstand highly accurate model stealing attacks for tens of thousands of queries, amplifying the attacker's error rate up to a factor of 85$\\times$ with minimal impact on the utility for benign users.", "keywords": ["model functionality stealing", "adversarial machine learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposed an optimization-based defense against model stealing attacks.  A criticism of the paper is that the method is computationally expensive, and was not demonstrated on more complex problems like ImageNet.  While this criticism is valid, other reviewers seem less concerned by this because the SOTA in this area is currently focused on smaller problems.  After considering the rebuttal, there is enough reviewer support for this paper to be accepted."}, "review": {"SfHHKpONk-X": {"type": "rebuttal", "replyto": "SyevYxHtDB", "comment": "The code and data is available here: https://resources.mpi-inf.mpg.de/d2/orekondy/predpoison/", "title": "Code and Data"}, "BJx6LZr0tB": {"type": "review", "replyto": "SyevYxHtDB", "review": "This paper proposed an effective defense against model stealing attacks. \n\nMerits:\n1) In general, this paper is well written and easy to follow.\n2) The approach is a significant supplement to existing defense against model stealing attacks. \n3) Extensive experiments. \n\nHowever, I still have concerns about the current version. \nI will possibly adjust my score based on the authors' response. \n\n1) In the model stealing setting, attacker and defender are seemingly knowledge limited. This should be clarified better in Sec. 3.  It is important to highlight that the defender has no access to F_A, thus problem (4) is a black-box optimization problem for defense. Also, it is better to have a table to summarize the notations. \n\nAdditional questions on problem formulation:\na) Problem (4) only relies on the transfer set, where $x \\sim P_A(x)$, right? \nb) For evaluation metrics, utility and non-replicability, do they have the same D^{test}? How to determine them, in particularly for F_A? \nc) One utility constraint is missing in problem (4). I noticed that it was mentioned in MAD-argmax, however, I suggest to add it to the formulation (4).\n\n2)  The details of heuristic solver are unclear. Although the authors pointed out the pseudocode in the appendix, it lacks detailed analysis. \n\n3) In Estimating G, how to select the surrogate model? Moreover, in the experiment, the authors mentioned that defense performances are unaffected by choice of architectures, and hence use the victim architecture for the stolen model. If possible, could the author provide results on different architecture choices for the stolen model as well as the surrogate model?\n\n############## Post-feedback ################\nI am satisfied with the authors' response. Thus, I would like to keep my positive comments on this paper. Although the paper is between 6 and 8, I finally decide to increase my score to 8 due to its novelty in formulation and extensive experiments. ", "title": "Official Blind Review #1", "rating": "8: Accept", "confidence": 4}, "B1gudgK3jB": {"type": "rebuttal", "replyto": "B1eAb_gqiH", "comment": "We are happy our initial response helped address some concerns of AR4. \n\nNow, we hope to address the follow-up concerns.\n\n\n<< 1.b.i (a) attackers are fairly weak .. get mediocre results >>\n- Stronger attackers would further motivate the model stealing threat. Model stealing is an active research topic (Table A2), and we find recent approaches demonstrate results to warrant a concern -- models require an immense amount of money and labour to develop, but can be substantially replicated (0.72-1.0$\\times$ in our experiments) via prediction APIs for 10s-100s of dollars.\n\n\n<< 1.b.i (b) attackers require a lot of synthetic data >>\n- The data is easy to obtain (e.g., random natural images) and comparable to victim\u2019s training set size e.g., 1$\\times$ (for CIFAR10/CIFAR100), 2.1$\\times$ (Caltech256). Hence, we argue amount of data is not a limitation for attacks.\n- [Revisions] We remark on sizes of attacker\u2019s transfer set relative to victim\u2019s training set in Sec. 5 \u201cAttack Strategies\u201d.\n\n\n<< 1c. Suspect Imagenet results would be underwhelming \u2026 attack methods seem very easy to adapt to new datasets \u2026 would be nice to include this >>\n- Extending methods to Imagenet would help clarify attack results. But, the experiments would indeed be intractable to perform in the given duration. We plan to tackle it in the future.\n\n\n<< Caltech256 inference times a little concerning >>\n- We agree and we have already started working on this problem. Our current version reduces computation on Caltech256 to 0.3s by estimating $G$ using a more efficient surrogate architecture (ResNet-34 vs. VGG16 from before).\n- [Revisions] We indicate this in Appendix E.2. We will further discuss and update timing results.\n", "title": "Follow-up Response "}, "S1gADrHuir": {"type": "rebuttal", "replyto": "H1lmmrBdsS", "comment": "<< (4) Angular histogram plot crafted without knowledge of model\u2019s parameters .. would motivate the defense more >>\n- We appreciate the suggestion. We find our defense similarly introduces angular deviations, even without knowledge of the attacker\u2019s model parameters. \n- [Revisions] We now indicate this in our discussion in the main manuscript. We provide further details in Appendix F.4. \n\n[1] Tram\u00e8r, Florian, et al. \"Stealing machine learning models via prediction apis.\" USENIX 2016.\n[2] Jagielski, Matthew, et al. \"High-Fidelity Extraction of Neural Network Models.\" arXiv 2019.\n[3] Anonymous authors. \u201cThieves on Sesame Street! Model Extraction of BERT-based APIs.\u201d Under review at ICLR 2020.", "title": "(2/2) Response to Review #4"}, "H1lmmrBdsS": {"type": "rebuttal", "replyto": "r1ef7Qn-jr", "comment": "We thank all reviewers for their valuable feedback. We are glad that reviewers find our \u201cwell-motivated\u201d defense approach \u201ceffective\u201d against model stealing, \u201csignificantly supplement existing defenses\u201d, and further validated by \u201cextensive experiments\u201d. We are also pleased reviewers find the paper \u201cwell-written\u201d and \u201cvery readable\u201d.\n\nWe now address individual concerns of AnonReviewer4.\n\n<< (1a) MAD is comparable to random-noise on CIFAR100 \u2026 Defense performance gap reduces as the dataset becomes more difficult >>\n- In terms of the two utility metrics we use to evaluate defense performance:\n  (i) *utility = defender\u2019s accuracy* (y-axis of Fig. 4 - bottom; blue and green lines): the performance gap is nonetheless significant e.g., CIFAR100 MAD defender accuracy is 5% higher than random-noise at attacker accuracy (x-axis) = 44%; and\n  (ii) *utility = perturbation amount* (y-axis of Fig. 4 - top): we consistently find MAD significantly outperforms random-noise defense, even for difficult datasets e.g., CIFAR100 MAD defender introduces 1/3$\\times$ lesser perturbation than random-noise at attacker accuracy = 44%.\n- [Revisions] We introduced magnification insets of dense regions in Fig. 4 of the revised manuscript to better convey the gaps.\n\n\n<< (1b) Overall skeptical of the threat model: (i) requires large number of queries \u2026 (ii) attacker does not achieve great results >>\n- We believe the skepticism is unjustified, because:\n  (i) *Querying is cheap*: The bottleneck for stealing models is the *cost* of executing queries (e.g., money, latency) rather than the *number* of queries. The cost for querying in practise is cheap: 0.0015 USD per prediction on Google cloud prediction API for instance. \n  (ii) *Attack performance/results extracted per dollar is substantial*: Yes, the accuracy of attacker\u2019s stolen models are imperfect (0.72-1.0$\\times$ victim\u2019s accuracy, Table 1). However, the dollars spent per accuracy point (\u201cAP\u201d) [1, 2, 3] for the attacker is a fraction of the victim\u2019s. Consider Caltech256 for instance: (a) Victim = 11 USD per AP (22K images x 0.35 USD / 80 accuracy; 0.35 USD using Google\u2019s data labeling platform) ignoring costs to collect and curate data, engineer the model, etc.; and (b) Attacker = 1 USD per AP (50K images x 0.0015 USD / 74.6 accuracy) by stealing. \n- Consequently, we argue that model stealing attacks pose a severe threat when viewing the problem as information extracted by the attacker per dollar.\n\n\n<< (1c) Attack and defense results on ImageNet would be nice >>\n- While Imagenet evaluation would be interesting, the focus of our paper is defending existing attack models and on datasets that the attacks have proven to be effective. Unfortunately we are not aware of any existing model stealing attack on ImageNet, apart from a very recent arXiv paper [2] (Jagielski et al., Sep. 2019).\n\n\n<< (2) How long does this optimization procedure take? \u2026 unreasonable if it significantly lengthens the time to return outputs of queries >>\n- We find all our optimization procedures take under a second. Specifically: 6ms (MNIST), 7ms (FashionMNIST), 9ms (CIFAR10), 69ms (CIFAR100), 0.4s (CUB200), 0.8s (Caltech256). \n- [Revisions] We added a discussion on run-times in Appendix E.2.\n\n\n<< (3a) Would be nice if attacks were explained a bit more. Specifically, how are attacks tested? >>\n- Thanks for the suggestion. The attacks are evaluated on a common held-out victim test set. For a fair head-to-head comparison, the test set during evaluation is common to both the victim\u2019s model and attacker\u2019s stolen model.\n- [Revisions] Attacks are further clarified in the revised manuscript by (i) a visualization of attacker, defender and evaluation metrics in Appendix Figure A1; and (ii) extending our existing discussion on attack model details in Appendix D.\n\n\n<< (3b) Does the attacker have knowledge about the class-label space of the victim? >>\n- All attackers are aware of the *number* of output classes of the victim model. As for the *semantics* of output class labels: (i) {knockoff}: does not require this knowledge; and (ii) {jbda, jb-self, jb-top3}: has the knowledge.\n- [Revisions] We clarified this in our existing discussion on attack models in Appendix E.\n\n\n<< (3c) If the attacker is trained with some synthetic data/other dataset, do you then freeze the feature extractor and train a linear layer to validate on the victim\u2019s test set? >>\n- No. We evaluate the attacker\u2019s stolen model as-is on the victim\u2019s test set. To further elaborate, we: (a) construct a transfer dataset (image-posterior pairs, where image = synthetic/other) using the attack strategies; (b) train the attack model $F_A$ using the transfer set; and (c) evaluate $F_A$ on the victim\u2019s test set. Note that (a) and (b) are intertwined for some attacks.\n- [Revisions] We updated our manuscript to clarify this in Sec. 5 \u201cEffectiveness of Attacks\u201d and provided additional details in Appendix D \u201cEvaluating attacks.''", "title": "(1/2) Response to Review #4"}, "S1luN1BuoH": {"type": "rebuttal", "replyto": "BJx6LZr0tB", "comment": "We thank all reviewers for their valuable feedback. We are glad that reviewers find our \u201cwell-motivated\u201d defense approach \u201ceffective\u201d against model stealing, \u201csignificantly supplement existing defenses\u201d, and further validated by \u201cextensive experiments\u201d. We are also pleased reviewers find the paper \u201cwell-written\u201d and \u201cvery readable\u201d.\n\nWe now address individual concerns of AnonReviewer1.\n\n<< (1a) Problem (4) relies on the transfer set, where $x \\sim P_A(x)$, right?  >>\n- Yes. More precisely, problem (4) relies on a single input $x$ (queried by the adversary, sampled from an unknown $P_A(x)$).\n\n\n<< (1b) Do utility and non-replicability have the same $D^{test}$? \u2026 How to determine $D^{test}$ for F_A?  >>\n- Yes. For evaluation purposes we use the same test set accuracies (of the corresponding dataset e.g., MNIST) to evaluate both the defended victim model ($F^{\\delta}_V$) and attacker model ($F_A$). The setting allows for a fair head-to-head comparison of both models on a common test set. \n- [Revisions] We added an illustration (Fig. A1 of the appendix) and additional discussion (Appendix D \u201cEvaluating attacks\u201d) to better clarify this.\n\n\n<< (1c) Utility constraint of MAD-argmax missing in (4) \u2026 suggest adding it to (4)  >>\n- Thanks for pointing this typo out. (4-7) previously presented our optimization problem for approach \u201cMAD\u201d. \n- [Revisions] Our revised version also presents the additional constraint for variant \u201cMAD-argmax\u201d in Eq. (8) .\n\n\n<< (1d) Writing clarifications: (i) Better clarify that both attacker and defender are knowledge limited \u2026 (ii) highlight problem (4) is a black-box optimization problem for defense \u2026 (iii) Table to summarize notation >>\n- Thanks for the suggestions. \n- [Revisions] The revised manuscript addresses all the above:  (i) is clarified in Sec. 3 paragraphs \u201cKnowledge-limited Attacker\u201d and \u201cDefender\u2019s Assumptions\u201d accordingly; (ii) is highlighted immediately after presenting problem (4); and (iii) We summarized the notation in Table A1 of the appendix.\n\n\n<< (2) Details of the heuristic solver are unclear ...  Although the authors pointed out the pseudocode in the appendix, it lacks detailed analysis.  >>\n- We apologize for not making this clear. \n- [Revisions] We revised our paragraph \u201cHeuristic Solver\u201d in Sec. 4 of the manuscript and further elaborated on it in Appendix C.\n\n\n<< (3) In estimating $G$, how to select the surrogate model? Results on different choices of architectures >>\n- We select the surrogate model based on empirical observations for choices of:  \n(a) surrogate architectures: which has a negligible effect and is robust to choices of attacker architectures (Fig. A2); and \n(b) initialization of surrogate model: which plays a crucial role. Initializing the weights of the surrogate far from convergence (Fig. A3) provides a better gradient signal to poison the posteriors. Consequently, we choose a randomly-initialized model to estimate $G$.\n- [Revisions] We further clarify this in Section 4 (under \u201cEstimating $G$\u201d) of the revised manuscript and provide additional details (including results for choices of architectures and initializations) in Section E.1.\n", "title": "Response to Review #1"}, "Byg7Ei4OiH": {"type": "rebuttal", "replyto": "SygLUkwTFr", "comment": "We thank all reviewers for their valuable feedback. We are glad that reviewers find our \u201cwell-motivated\u201d defense approach \u201ceffective\u201d against model stealing, which \u201csignificantly supplement existing defenses\u201d, and is further validated by \u201cextensive experiments\u201d. We are also pleased reviewers find the paper \u201cwell-written\u201d and \u201cvery readable\u201d.\n\nWe appreciate AR3 for recognizing that our defenses \u201cadvances the research field\u201d, especially due to a lack of an effective defense. While stronger theoretical results would certainly aid this line of study, our primary focus in this paper was establishing the first effective defense against a range of existing attack models and settings. We would be happy to answer any further questions.\n", "title": "Response to Review #3"}, "r1ef7Qn-jr": {"type": "review", "replyto": "SyevYxHtDB", "review": "The paper proposes a new method for defending against stealing attacks.\n\nPositives:\n1) The paper was very readable and clear.\n2) The proposed method is straightforward and well motivated.\n3) The authors included a good amount of experimental results. \n\n\nConcerns: \n1) You note that the random perturbation to the outputs performs poorly compared to your method, but this performance gap seems to decrease as the dataset becomes more difficult (i.e. CIFAR100). I\u2019m concerned that this may indicate that the attackers are generally weak and this threat model may not be very serious. Overall, I\u2019m skeptical of this threat model - the attackers require a very large number of queries, and don\u2019t achieve great results on difficult datasets. Including results on a dataset like ImageNet would be nice. \n2) How long does this optimization procedure take? It seems possibly unreasonable for the victim to implement this defense if it significantly lengthens the time to return outputs of queries. \n3) Although this is a defense paper, it would be nice if the attacks were explained a bit more. Specifically, how are these attacks tested? You use the validation set, but does the attacker have knowledge about the class-label space of the victim? If the attacker trained with some synthetic data/other dataset, do you then freeze the feature extractor and train a linear layer to validate on the victim\u2019s test set? It seems like this is discussed in the context of the victim in the \u201cAttack Models\u201d subsection, but it\u2019s unclear what\u2019s happening with the attacker. \n4) It would be nice to see an angular histogram plot for a model where the perturbed labels were not crafted with knowledge of this model\u2019s parameters - i.e. transfer the proposed defense to a blackbox attacker and produce this same plot. This would motivate the defense more. \n\n\n", "title": "Official Blind Review #4", "rating": "3: Weak Reject", "confidence": 2}, "SygLUkwTFr": {"type": "review", "replyto": "SyevYxHtDB", "review": "This paper aims at defending against model stealing attacks by perturbing the posterior prediction of a protected DNN with a balanced goal of maintaining accuracy and maximizing misleading gradient deviation. The maximizing angular deviation formulation makes sense and seemingly correct. The heuristic solver toward this objective is shown to be relatively effective in the experiments. While the theoretical novelty of the method is limited, the application in adversarial settings may be useful to advance of this research field, especially when it is relatively easy to apply by practitioners.I recommend toward acceptance of this paper even though can be convinced otherwise by better field experts.\n\n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 1}}}