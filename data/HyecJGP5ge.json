{"paper": {"title": "NEUROGENESIS-INSPIRED DICTIONARY LEARNING: ONLINE MODEL ADAPTION IN A CHANGING WORLD", "authors": ["Sahil Garg", "Irina Rish", "Guillermo Cecchi", "Aurelie Lozano"], "authorids": ["sahilgar@usc.edu", "rish@us.ibm.com", "gcecchi@us.ibm.com", "aclozano@us.ibm.com"], "summary": "An online dictionary learning incorporates dynamic model adaptation, adding/deleting its elements in response to nonstationary data.", "abstract": "In this paper, we focus on online representation learning in non-stationary environments which may require continuous adaptation of model\u2019s architecture. We propose a novel online dictionary-learning (sparse-coding) framework which incorporates the addition and deletion of hidden units (dictionary elements), and is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus, known to be associated with improved cognitive function and adaptation to new environments. In the online learning setting, where new input instances arrive sequentially in batches, the \u201cneuronal birth\u201d is implemented by adding new units with random initial weights (random dictionary elements); the number of new units is determined by the current performance (representation error) of the dictionary, higher error causing an increase in the birth rate. \u201cNeuronal death\u201d is implemented by imposing l1/l2-regularization (group sparsity) on the dictionary within the block-coordinate descent optimization at each iteration of our online alternating minimization scheme, which iterates between the code and dictionary updates. Finally, hidden unit connectivity adaptation is facilitated by introducing sparsity in dictionary elements. Our empirical evaluation on several real-life datasets (images and language) as well as on synthetic data demonstrates that the proposed approach can considerably outperform the state-of-art fixed-size (nonadaptive) online sparse coding of Mairal et al. (2009) in the presence of nonstationary data. Moreover, we identify certain properties of the data (e.g., sparse inputs with nearly non-overlapping supports) and of the model (e.g., dictionary sparsity) associated with such improvements.", "keywords": ["Unsupervised Learning", "Computer vision", "Transfer Learning", "Optimization", "Applications"]}, "meta": {"decision": "Reject", "comment": "This paper combines simple heuristics to adapt the size of a dictionary during learning. The heuristics are intuitive: augmenting the dictionary size when correlation between reconstruction and inputs falls below a certain pre-determined threshold, reducing the dictionary size by adding a group-sparsity L1,2 penalty on the overall dictionary (the L1,2 penalty for pruning models had appeared elsewhere before, as the authors acknowledge).\n The topic of online adjustment of model complexity is an important one, and work like this is an interesting and welcome direction. The simplicity of the heuristics presented here would be appealing if there was more empirical support to demonstrate the usefulness of the proposed adaptation. However, the empirical validation falls short here:\n - the claim that this work offers more than existing off-line model adaptation methods because of its online setting is the crux of the value of the paper, but the experimental validation does not offer much in terms of how to adjust hyperparameters to a truly nonstationary setting. Here, the hyperparameters are set rather arbitrarily, and the experimental setting is a single switch between two datasets (where off-line methods would do well), so this obscures the challenges that would be presented by a less artificial level of non-stationarity\n - an extensive set of experiments is presented, but all these experiments are confined in a strange experimental setting that is divorced from any practical metrics: the \"state-of-the-art\" method of reference is Mairal et al 2009 which was focused on speeding learning of dictionaries. Here, the metrics offered are correlation / reconstruction error, and classification, over full images instead of patches as is usually done for large images, without justification. This unnecessarily puts the work in a vacuum in terms of evaluation and comparison to existing art. In fact, the reconstructions in the appendix Fig. 17 and 18 look pretty bad, and the classification performance 1) does not demonstrate superiority of the method over standard dictionary learning (both lines are within the error bars and virtually indistinguishable if using more than 60 elements, which is not much), 2) performance overall is pretty bad for a 2-class discrimination task, again because of the unusual setting of using full images.\n In summary, this paper could be a very nice paper if the ideas were validated in a way that shows true usefulness and true robustness to the challenges of realistic nonstationarity, but this is unfortunately not the case here.\n Ps: two recent papers that could be worth mentioning in terms of model adaptation are diversity networks for pruning neurons (Diversity Networks, Mariet and Sra ICLR 2016, https://arxiv.org/abs/1511.05077), and augmenting neural networks with extra parameters while retaining previous learning (Progressive Neural Networks, Rusu et al 2016, https://arxiv.org/pdf/1606.04671v3.pdf)."}, "review": {"HyZ-ci9Lg": {"type": "rebuttal", "replyto": "Syk3UmQEe", "comment": "We thank the reviewer for the insightful comments. Please consider our response to the review as below.\n\n1. \"On the other hand, the overall technique is not very novel. Although not exactly equivalent, similar ideas have been explored\"\n\nPlease see the discussion above in response to the Reviewer1, item 4 in the summary above, as well as a paragraph at the end of the introduction which hopefully emphasizes more clearly the  novelty of the proposed approach.  As discussed above, at some level of generality, the ideas of addition or deletion of variables have been explored, but never in online dictionary learning context.\n\nWe thank the reviewer for the reference on MDL approach to model selection in sparse coding, in offline setting; we cited and discussed this reference in the introduction, however, our approach is quite different as it involves an online setting and assumes nonstationary inputs.\n\nIf our arguments concerning novelty of our approach are still not sufficiently convincing, we would really appreciate any specific references on prior art proposing an online adaptive model-selection within any autoencoder setting (sparse dictionary or beyond), based on hidden units birth/death dynamics, with a thorough evaluation, empirical and theoretical, of situations where such adaptive approach is beneficial (or not) as compared vs non-adaptive baseline. We were unable to find any such prior work.  \n\n2. \" the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data. Which depending on the \"level\" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set.\"\n\nThis is a good point; as usual, we end up having a tunable parameter controlling the birth rate (here, a threshold on the representation error measured as 1 - correlation between actual and reconstructed input), and must resort to cross-validation to set it up, or potentially provide another layer of automation in order to tune this parameter online (e.g., an RL approach to parameter tuning in our method). However, we view this as a direction for future work, since the automated parameter tuning is a sufficiently involved procedure on its own.\n \nRegarding the earlier question about reinitialization of dead elements in the Mairal's original algorithm: we added new experiments in the Appendix, section B.7, Figure 19. First of all, we noticed that the fraction of \u201cdead\u201d elements was typically very small in our experiments when using the original ODL method of Mairal et al. (2009). We next followed the idea of reinitialization of dead elements in ODL, referring to this modification as \u201cODL*\u201d in Figure 19.  Specifically, we reinitialize the \u201cdead\u201d elements with random initializations, and then relearn the reinitialized elements, along with other elements, on the current batch of data. Fig. 19 extends Fig. 2 with additional plots for the baseline extension \u201cODL*\u201d of ODL; keeping the experiment settings same. We see that there is negligible difference in the performance of ODL and its extension \u201cODL*\u201d, and our method NODL maintains its superior performance w.r.t. ODL as well as \u201cODL*\u201d.\n", "title": "Reply to AnonReviewer3"}, "r1kHKicUe": {"type": "rebuttal", "replyto": "H1BU-VuEg", "comment": "We thank the reviewer for the insightful comments. Please consider our response to the review as below.\n\n1. \"the algorithm could be discussed more, to give a more solid view of the contribution\"\n\n A new section (3.1) was added to provide a detailed discussion. \n\n2. \"The technique is not novel in spirit\" \n\nPlease see the item 4 in the summary above. We are convinced that our approach is actually quite novel, in the context of online dictionary learning or more general autoencoders. Clearly, at a very general level, model selection is not a novel idea, and at the same level of generality, the ideas of adding or deleting variables during model selection may have appeared earlier in very different contexts (e.g., in learning Bayesian network structure, or in LASSO solved by active set methods such as LARS, just to name a few). We are not claiming the novelty at such extremely general level, but the dynamic addition/deletion ONLINE model selection in dictionary learning setting is a completely novel approach. As discussed in item 4 above, not only is our approach quite different from the corresponding related work, our key contribution is actually empirical and theoretical evaluation of situations/conditions which make such  online model selection beneficial for dictionary learning \u2013 the topic never explored in the sparse coding literature, to the best of our knowledge. We also added a bullet-point list of our contributions at the end of the introduction paragraph in the paper.\n\nIf the novelty of the proposed approach still does not seem convincing, we would really appreciate specific references to prior work which (1) implements birth/death of hidden units in an autoencoder model, in an online setting, outperforming state-of-art in nonstationary settings and (2) provides both empirical and theoretical evaluation of conditions under which adaptive approach is beneficial vs non-adaptive.\n\n 3. \"Is there a way to relate the organization of the data to the behavior of this method? In this paper, buildings are shown first, and natural images (which are less structured, more difficult) later. Is this just a way to perform curriculum learning? What happens when data simply changes in structure, with no apparent movement from simple to more complex (e.g. from flowers, to birds, to fish, to leaves, to trees etc.) In a way, it makes sense to see an improvement when the training data has such a structure, by going from something artificial and simpler to a more complex, less structured domain.\"\n\nThe objective here was to explore ANY nonstationary conditions, not necessarily the  \"curriculum learning\" setting which indeed would imply going from simpler to more complex concepts.  We are not convinced that the natural images are necessarily \"more difficult\" that buildings (e.g., a complicated city scene with multiple buildings can be seen, vice versa, more 'complex' than a flower, as in some of those images).\n\nIn fact, we performed additional experiments, added to the Appendix, section B.8, Figure 20, where we tried all possible permutations of the input datasets (buildings, flowers, animals). The results were very similar to the ones presented in the main paper when we flipped the order of input data (e.g., with flowers followed by building, or flowers followed by animals, our methods showed similar magnitude of improvements over the baseline method as it did for buildings followed by flowers or by animals). Overall, our general observation is that there is no processing order on datasets where baseline (ODL) could perform better than the proposed approach. We see a significant advantage of NODL over ODL if we have Oxford or Flowers data sets as the first domain data. However, this advantage is smaller when using the Animals dataset first, followed by flowers or buildings, so, empirically, it turns out that perhaps animal images were more 'complex', and then flowers and buildings looked 'easier' to learn afterwards, but this is just a speculation at this point. \n\n In the future, it would be interesting to investigate scenarios where the complexity of the input would be measured precisely and controlled for, e.g., using the compressed size of an image as a proxy for its complexity, or some similar 'exact' measure.\n\n4. \"The paper is interesting, the idea useful with some interesting insights. I am not sure it is ready for publication yet. \"\n\nCan you please make any specific suggestions regarding the steps that would make the paper ready for publication? Thank you!\n \n", "title": "Response to AnonReviewer1"}, "SkLwPs5Ul": {"type": "rebuttal", "replyto": "HyecJGP5ge", "comment": "We would like to thank all reviewers for their insightful comments that helped us to improve the paper; to address those comments, we modified the paper as follows:\n\n1.\tAdded a subsection 3.1 in section 3, describing various details of the proposed algorithm, as suggested by Reviewer1.\n\n2.\tAdded a section (Appendix, B9, Figures 21-26) demonstrating stability of our results (our method consistently outperforming the non-adaptive dictionary learning)  while varying all of the algorithm\u2019s parameters, one at a time, over a wide range of values.  \n\n3.\tAdded a section (Appendix, B7, Fig. 19) with new empirical results to address the concern of Reviewer3 regarding comparison with a version of the baseline method which involved re-initialization of ``dead\u2019\u2019 elements; we did not observe any significance difference in the performance of such augmented method versus the original baseline (the number of dead elements appearing in the baseline method was quite small); on the contrary, our method employing group-sparsity was removing larger number of ``weak\u2019\u2019 (low l2-norm) elements and yielded better performance due to such explicit regularization.\n\n4.\tAdded a section (Appendix, B8, Fig. 20) with new results evaluating our method under different ordering of the input data, to address the question of the Reviewer1 regarding the potential sensitivity of our results to the ordering of the training domains  (e.g., from easier to harder). No significant changes  were observed due to permutation of the input datasets (our method was still outperforming the baseline), i.e. buildings images were not necessarily \u2018simpler\u2019 than natural ones.\n\n5.\tA summary of our contributions is added at the end of the intro section. Our main point is that the contribution of this paper is truly novel and nontrivial. Indeed,\n\n    a.\tWe are the first to propose an online model-selection approach to dictionary learning (DL) based on dynamic addition and deletion of the elements (hidden units), which leads to significant performance improvements over the state-of-art online DL, especially in non-stationary settings. None of the reviewers have provided a reference that would contradict the above statement.\n\n    b.\tWhile some prior work (e.g., Bengio et al, NIPS 2009) involved deletion of dictionary elements, no addition was involved in that approach and, more importantly, their approach only concerned an offline dictionary learning, not online. Similarly, approaches to neural nets involving cascade correlations and other methods adding hidden units are lacking regularization (deletion) that we achieve via group sparsity constraint.    Finally, Reviewer3 mentions prior work on off-line MDL-based model selection in dictionary learning, but the major difference is, again, that our setting is online learning, while their approach is off-line.  \n\n    c.\tFurthermore, none of the prior approaches explored the interplay between addition and deletion, neither theoretically nor empirically, while we performed such evaluation and identified regimes when our online model-selection would outperform the non-adaptive baseline   (Section 5).\n\n\nThus, to the best of our knowledge, our proposed method is completely novel, and provides a considerable improvement over the state-of-art, especially when the input is nonstationary.\n  \n\nOur detailed replies to each review are put as separate comments below.\n", "title": "Authors' response to all the three reviews."}, "r1Okjo58l": {"type": "rebuttal", "replyto": "SkDONYuVx", "comment": "We would like to thank the reviewer for useful comments and constructive suggestions.\nIn response to the comment on insufficient details of the algorithm, we added a new section 3.1 fully devoted to discussing such details. However, as suggested by the reviewer, a further analysis of the algorithm, involving the evolving properties of the dictionary and performance changes due to changes in the data, remain the directions for future work.\n", "title": "Reply to AnonReviewer2"}, "rJssdHf7e": {"type": "rebuttal", "replyto": "ryxmHjgXl", "comment": "> In Mairal et al (2009), the authors discuss a way of reinitialising  'dead' atoms when they present little activity. They use samples \n> from the dataset (instead of noise). Are you using this feature? If  not, it seems like a reasonable baseline to include.\n\nThank you for pointing this out. Yes, we are aware of this feature mentioned in the Mairal's original paper; note, however, that Mairal's paper only mentioned this option, but did not provide any empirical evaluation/comparison of its benefits with the fixed-size version, while in our paper, one of our main goals is to evaluate fixed-size vs non-fixed-size dictionary learning approaches. So, the reinitialization version is perhaps less appropriate as a baseline.\n\nFurthermore, in our baseline (fixed-size Mairal) experiments, we hardly get 'dead' atoms except  for  the case of dense dictionary elements with highly sparse codes. In such settings, there can be a very small number of dead atoms in the dictionary (like less than 1%). Also, since the Pearson correlation  and the mean square error curves are plotted w.r.t. the final  dictionary size, i.e. accounting for only the non-dead atoms in a  dictionary (the ones with non-zero l2 norm), we are effectively  discounting for the dead atoms in our evaluation to give the  baseline an advantage.\n\n\n>In the case of sparse dictionary elements, is the online DL baseline include such constraint?  \n\nYes, when the dictionary elements are sparse, the constraint is  imposed for the online DL baseline also.\n ", "title": "Response to the question by AnonReviewer3"}, "ryxmHjgXl": {"type": "review", "replyto": "HyecJGP5ge", "review": "\nIn Mairal et al (2009), the authors discuss a way of reinitialising 'dead' atoms when they present little activity. They use samples from the dataset (instead of noise). Are you using this feature? If not, it seems like a reasonable baseline to include.\n\nIn the case of sparse dictionary elements, is the online DL baseline include such constraint? \n\nI'd like to thank the authors for their detailed response and clarifications.\n\nThis work proposes new training scheme for online sparse dictionary learning. The model assumes a non-stationary flow of the incoming data. The goal (and the challenge) is to learn a model in an online manner in a way that is capable of  adjusting to the new incoming data without forgetting how to represent previously seen data. The proposed approach deals with this problem by incorporating a mechanism for adding or deleting atoms in the dictionary. This procedure is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus. \n\nThe paper has two main innovations over the baseline approach (Mairal et al): (i) \u201cneuronal birth\u201d which represents an adaptive way of increasing the number of atoms in the dictionary (ii) \"neuronal death\", which corresponds to removing \u201cuseless\u201d dictionary atoms.\n\nNeural death is implemented by including an group-sparsity regularization to the dictionary atoms themselves (the group corresponds to a column of the dictionary). This promotes to shrink to zero atoms that are not very useful, keeping controlled the increase of the dictionary size.\n\nI believe that the strong side of the paper is its connections with the adult neurogenesis phenomenon, which is, in my opinion a very nice feature.\nThe paper is very well written and easy to follow.\n\nOn the other hand, the overall technique is not very novel. Although not exactly equivalent, similar ideas have been explored. While the neural death is implemente elegantly with a sparsity-promoting regularization term, the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data. Which depending on the \"level\" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set. Still, having adaptive dictionary size is very interesting.\n\nThe authors could also cite some references in model selection literature. In particular, some ideas such as MDL have been used for automatically selecting the dictionary size (I believe this work does not address the online setting, but still its a relevant reference to have). For instance,\n\nRamirez, Ignacio, and Guillermo Sapiro. \"An MDL framework for sparse coding and dictionary learning.\" IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927.\n", "title": "Question regarding baseline", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Syk3UmQEe": {"type": "review", "replyto": "HyecJGP5ge", "review": "\nIn Mairal et al (2009), the authors discuss a way of reinitialising 'dead' atoms when they present little activity. They use samples from the dataset (instead of noise). Are you using this feature? If not, it seems like a reasonable baseline to include.\n\nIn the case of sparse dictionary elements, is the online DL baseline include such constraint? \n\nI'd like to thank the authors for their detailed response and clarifications.\n\nThis work proposes new training scheme for online sparse dictionary learning. The model assumes a non-stationary flow of the incoming data. The goal (and the challenge) is to learn a model in an online manner in a way that is capable of  adjusting to the new incoming data without forgetting how to represent previously seen data. The proposed approach deals with this problem by incorporating a mechanism for adding or deleting atoms in the dictionary. This procedure is inspired by the adult neurogenesis phenomenon in the dentate gyrus of the hippocampus. \n\nThe paper has two main innovations over the baseline approach (Mairal et al): (i) \u201cneuronal birth\u201d which represents an adaptive way of increasing the number of atoms in the dictionary (ii) \"neuronal death\", which corresponds to removing \u201cuseless\u201d dictionary atoms.\n\nNeural death is implemented by including an group-sparsity regularization to the dictionary atoms themselves (the group corresponds to a column of the dictionary). This promotes to shrink to zero atoms that are not very useful, keeping controlled the increase of the dictionary size.\n\nI believe that the strong side of the paper is its connections with the adult neurogenesis phenomenon, which is, in my opinion a very nice feature.\nThe paper is very well written and easy to follow.\n\nOn the other hand, the overall technique is not very novel. Although not exactly equivalent, similar ideas have been explored. While the neural death is implemente elegantly with a sparsity-promoting regularization term, the neural birth is performed by relying on heuristics that measure how well the dictionary can represent new incoming data. Which depending on the \"level\" of non-stationarity in the incoming data (or presence of outliers) could be difficult to set. Still, having adaptive dictionary size is very interesting.\n\nThe authors could also cite some references in model selection literature. In particular, some ideas such as MDL have been used for automatically selecting the dictionary size (I believe this work does not address the online setting, but still its a relevant reference to have). For instance,\n\nRamirez, Ignacio, and Guillermo Sapiro. \"An MDL framework for sparse coding and dictionary learning.\" IEEE Transactions on Signal Processing 60.6 (2012): 2913-2927.\n", "title": "Question regarding baseline", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}