{"paper": {"title": "Tree-Structured Variational Autoencoder", "authors": ["Richard Shin", "Alexander A. Alemi", "Geoffrey Irving", "Oriol Vinyals"], "authorids": ["ricshin@cs.berkeley.edu", "alemi@google.com", "geoffreyi@google.com", "vinyals@google.com"], "summary": "", "abstract": "Many kinds of variable-sized data we would like to model contain an internal hierarchical structure in the form of a tree, including source code, formal logical statements, and natural language sentences with parse trees. For such data it is natural to consider a model with matching computational structure. In this work, we introduce a variational autoencoder-based generative model for tree-structured data. We evaluate our model on a synthetic dataset, and a dataset with applications to automated theorem proving. By learning a latent representation over trees, our model can achieve similar test log likelihood to a standard autoregressive decoder, but with the number of sequentially dependent computations proportional to the depth of the tree instead of the number of nodes in the tree.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper is clearly written, but the method isn't demonstrated to solve any problem much better than simpler approaches. To quote one reviewer, \"the work may well be significant in the future, but is currently somewhat preliminary, lacks motivation, chooses a tree structured encoder without particular motivation, and is lacking in wider comparisons.\""}, "review": {"SJgXBAb4x": {"type": "review", "replyto": "Hy0L4t5el", "review": "The approaches here do not currently seem practical for interesting problems. Is that the case, and if so is there reason to believe there will be obvious developments that will make it more practical?The authors propose a variational autoencoder for a specific form of tree-generating model.\n\nThe generative model for trees seems reasonable but is not fully motivated. If no previous references suggest this tree specification, then clear motivation for e.g. the extension beyond CFG should be given beyond the one sentence provided.\n\nGiven the tree model it may be natural to specify a tree model encoder, but the posterior distribution does not respect the structure of the prior (as the posterior distribution couples tree-distant variables), so there is in fact no good reason for this form, and a more general network could be compared with.\n\nThe approach provides sensible differentiable functions for encoding the network. The tests are indicative, but the results are very similar to the tested approaches, and it is not clear what the best evaluation metric ought to be.\n\nSignificance: the work may well be significant in the future, but is currently somewhat preliminary, lacks motivation, chooses a tree structured encoder without particular motivation, and is lacking in wider comparisons. There is also some lack of current motivation for the model, and no comparison with tractable models that do not need a variational autoencoder.\n\nOriginality: original, but at the moment it is not clear such originality is necessary.\n\nClarity: Good.\n\nExperiments: Sensible, but not extensive or conclusive.", "title": "Practicality", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJzUcCbNe": {"type": "review", "replyto": "Hy0L4t5el", "review": "The approaches here do not currently seem practical for interesting problems. Is that the case, and if so is there reason to believe there will be obvious developments that will make it more practical?The authors propose a variational autoencoder for a specific form of tree-generating model.\n\nThe generative model for trees seems reasonable but is not fully motivated. If no previous references suggest this tree specification, then clear motivation for e.g. the extension beyond CFG should be given beyond the one sentence provided.\n\nGiven the tree model it may be natural to specify a tree model encoder, but the posterior distribution does not respect the structure of the prior (as the posterior distribution couples tree-distant variables), so there is in fact no good reason for this form, and a more general network could be compared with.\n\nThe approach provides sensible differentiable functions for encoding the network. The tests are indicative, but the results are very similar to the tested approaches, and it is not clear what the best evaluation metric ought to be.\n\nSignificance: the work may well be significant in the future, but is currently somewhat preliminary, lacks motivation, chooses a tree structured encoder without particular motivation, and is lacking in wider comparisons. There is also some lack of current motivation for the model, and no comparison with tractable models that do not need a variational autoencoder.\n\nOriginality: original, but at the moment it is not clear such originality is necessary.\n\nClarity: Good.\n\nExperiments: Sensible, but not extensive or conclusive.", "title": "Practicality", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJb70jxXg": {"type": "review", "replyto": "Hy0L4t5el", "review": "\n1. Regarding the variable-sized latent VAE, how was the prior distribution over the number of latent variables specified? And during generation, how did you ensure that the resulting tree structure had the desired number of nodes? Did you generate the tree structure as described in 3.2 and simply draw z_{n_i} as necessary?\n2. Could you provide some more detail regarding the proof dataset? What sorts of functions and predicates did you use? How many types were there in the grammar? A simple example would also be helpful, perhaps showing a low-depth ground truth tree and a sample from the trained VAE.This paper introduces a novel extension of the variational autoencoder to arbitrary tree-structured outputs. Experiments are conducted on a synthetic arithmetic expression dataset and a first-order logic proof clause dataset in order to evaluate its density modeling performance.\n\nPros:\n+ The paper is clear and well-written.\n+ The tree-structure definition is sufficiently complete to capture a wide variety of tree types found in real-world situations.\n+ The tree generation and encoding procedure is elegant and well-articulated.\n+ The experiments, though limited in scope, are relatively thorough. The use of IWAE to obtain a better estimate of log likelihoods is a particularly nice touch.\n\nCons:\n- The performance gain over a baseline sequential model is marginal.\n- The experiments are limited in scope, both in the datasets considered and in the evaluation metrics used to compare the model with other approaches. Specifically: (a) there is only one set of results on a real-world dataset and in that case the proposed model performs worse than the baseline, and (b) there is no evaluation of the learned latent representation with respect to other tasks such as classification.\n- The ability of the model to generate trees in time proportional to the depth of the tree is proposed as a benefit of the approach, though this is not empirically validated in the experiments.\n\nThe procedures to generate and encode trees are clever in their repeated use of common operations. The weight sharing and gating operations seem important for this model to perform well but it is difficult to assess their utility without an ablation (in Table 1 and 2 these modifications are not evaluated side-by-side). Experiments in another domain (such as modeling source code, or parse trees conditioned on a sentence) would help in demonstrating the utility of this model. Overall the model seems promising and applicable to a variety of data but the lack of breadth in the experiments is a concern.\n\n* Section 3.1: \"We distinguish three types\" => two\n* Section 3.6: The exposition of the variable-sized latent state is slightly confusing because the issue of how many z's to generate is not discussed.\n* Section 4.2-4.3: When generating the datasets, did you verify that the test set is disjoint from the training set?\n* Table 1: Is there a particular reason why the variable latent results are missing for the depth 11 trees?", "title": "Pre-review questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Byq5r5bNl": {"type": "review", "replyto": "Hy0L4t5el", "review": "\n1. Regarding the variable-sized latent VAE, how was the prior distribution over the number of latent variables specified? And during generation, how did you ensure that the resulting tree structure had the desired number of nodes? Did you generate the tree structure as described in 3.2 and simply draw z_{n_i} as necessary?\n2. Could you provide some more detail regarding the proof dataset? What sorts of functions and predicates did you use? How many types were there in the grammar? A simple example would also be helpful, perhaps showing a low-depth ground truth tree and a sample from the trained VAE.This paper introduces a novel extension of the variational autoencoder to arbitrary tree-structured outputs. Experiments are conducted on a synthetic arithmetic expression dataset and a first-order logic proof clause dataset in order to evaluate its density modeling performance.\n\nPros:\n+ The paper is clear and well-written.\n+ The tree-structure definition is sufficiently complete to capture a wide variety of tree types found in real-world situations.\n+ The tree generation and encoding procedure is elegant and well-articulated.\n+ The experiments, though limited in scope, are relatively thorough. The use of IWAE to obtain a better estimate of log likelihoods is a particularly nice touch.\n\nCons:\n- The performance gain over a baseline sequential model is marginal.\n- The experiments are limited in scope, both in the datasets considered and in the evaluation metrics used to compare the model with other approaches. Specifically: (a) there is only one set of results on a real-world dataset and in that case the proposed model performs worse than the baseline, and (b) there is no evaluation of the learned latent representation with respect to other tasks such as classification.\n- The ability of the model to generate trees in time proportional to the depth of the tree is proposed as a benefit of the approach, though this is not empirically validated in the experiments.\n\nThe procedures to generate and encode trees are clever in their repeated use of common operations. The weight sharing and gating operations seem important for this model to perform well but it is difficult to assess their utility without an ablation (in Table 1 and 2 these modifications are not evaluated side-by-side). Experiments in another domain (such as modeling source code, or parse trees conditioned on a sentence) would help in demonstrating the utility of this model. Overall the model seems promising and applicable to a variety of data but the lack of breadth in the experiments is a concern.\n\n* Section 3.1: \"We distinguish three types\" => two\n* Section 3.6: The exposition of the variable-sized latent state is slightly confusing because the issue of how many z's to generate is not discussed.\n* Section 4.2-4.3: When generating the datasets, did you verify that the test set is disjoint from the training set?\n* Table 1: Is there a particular reason why the variable latent results are missing for the depth 11 trees?", "title": "Pre-review questions", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkcrLJAfe": {"type": "review", "replyto": "Hy0L4t5el", "review": "Did the authors try the model on tree representations of natural language sentences? I'm curious about possible applications to dependency or constituency trees. The method overall seems to be a very interesting structural approach to variational autoencoders, however it seems to lack motivation as well as the application areas sufficient to prove its effectiveness.\n\nI see the attractiveness of using structural information in this context and I find it more intuitive than using a flat sequence representation, especially when there is a clear structure in the data. However experimental results seem to fail to be convincing in that regard.\n\nOne issue is the lack of a variety of applications in general, the experiments seem to be very limited in that regard, considering that the paper itself speaks about natural language applications. It would be interesting to use the latent representations learned with the model for some other end task and see how much it impacts the success of that end task compared to various baselines.\n\nIn my opinion, the paper has a potentially strong idea however in needs stronger results (and possibly in a wider variety of applications) as a proof of concept.", "title": "Applications to language", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rk4TQQf4g": {"type": "review", "replyto": "Hy0L4t5el", "review": "Did the authors try the model on tree representations of natural language sentences? I'm curious about possible applications to dependency or constituency trees. The method overall seems to be a very interesting structural approach to variational autoencoders, however it seems to lack motivation as well as the application areas sufficient to prove its effectiveness.\n\nI see the attractiveness of using structural information in this context and I find it more intuitive than using a flat sequence representation, especially when there is a clear structure in the data. However experimental results seem to fail to be convincing in that regard.\n\nOne issue is the lack of a variety of applications in general, the experiments seem to be very limited in that regard, considering that the paper itself speaks about natural language applications. It would be interesting to use the latent representations learned with the model for some other end task and see how much it impacts the success of that end task compared to various baselines.\n\nIn my opinion, the paper has a potentially strong idea however in needs stronger results (and possibly in a wider variety of applications) as a proof of concept.", "title": "Applications to language", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}