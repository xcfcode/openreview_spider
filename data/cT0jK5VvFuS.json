{"paper": {"title": "Uncertainty in Neural Processes", "authors": ["Saeid Naderiparizi", "Kenny Chiu", "Benjamin Bloem-Reddy", "Frank Wood"], "authorids": ["~Saeid_Naderiparizi1", "kenny.chiu@stat.ubc.ca", "benbr@stat.ubc.ca", "~Frank_Wood2"], "summary": "", "abstract": "We explore the effects of architecture and training objective choice on amortized posterior predictive inference in probabilistic conditional generative models.  We aim this work to be a counterpoint to a recent trend in the literature that stresses achieving good samples when the amount of conditioning data is large.  We instead focus our attention on the case where the amount of conditioning data is small.  We highlight specific architecture and objective choices that we find lead to qualitative and quantitative improvement to posterior inference in this low data regime.  Specifically we explore the effects of choices of pooling operator and variational family on posterior quality in neural processes.  Superior posterior predictive samples drawn from our novel neural process architectures are demonstrated via image completion/in-painting experiments.", "keywords": ["Neural Processes", "Meta-learning", "Variational Inference"]}, "meta": {"decision": "Reject", "comment": "This paper analyzes some design choices for neural processes, paying particular attention to their small-data performance, uncertainty, and posterior contraction.  This is certainly a worthwhile project, and R3 found the analysis interesting, giving the paper a score of 8.  However, R1, R2, and R4 found the experimental validation to be incomplete and insufficient to support the paper's broader recommendations.  As the paper is investigating the various combinations of implementations, I tend to agree with R1, R2, and R4 that this paper---while having some interesting ideas---needs a bit more precision and breadth to its experiments."}, "review": {"57Le8gn2iQj": {"type": "review", "replyto": "cT0jK5VvFuS", "review": "Summary: This paper is an empirical investigation into the role of architecture and objective choices in Neural Process (NP) models when the amount of conditioning data is limited. Specifically, they investigate the question of well-calibrated uncertainty. \n\nClarity: The overall quality of the writing is clear, but missing details/explanations leave some claims unjustified and therefore lines of argument difficult to follow.\n\nOriginality: Limited -- the paper is mostly an empirical investigation, with the modifications to existing NPs being (a) max-pooling and (b) SIVI on the posterior z\u2019s.\n\nSignificance: Seems limited (see more on \u201cCons\u201d section and \u201cQuestions/comments\u201d), though I am wondering whether the authors could have done more to emphasize the main contributions of this work. The paper\u2019s significance for me has been hampered by lack of details and clear takeaway/implications of the results.\n\nPros: There\u2019s been a lot of work on NPs and their variants, but less so on empirically investigating how and when they work well. There is also a range of systematic empirical evaluations both in the main text and supplement, which I appreciated.\n\nCons: There are some assumptions/statements that would be beneficial to elaborate upon in the paper. First, calibrated uncertainty is defined to be \u201chigh sample diversity for small conditioning sets; and sharp-looking, realistic images for any size of conditioning set.\u201d This statement, introduced early on in the paper without much justification, is a point that the authors repeatedly return to, and I found myself wondering why (especially since NPs are built for prediction/regression, so a lot of the prior work on calibration for classification models should hold here such as (Murphy 1973), (Gneiting et. al 2007), (Kuleshov et al., 2018)). What is the benefit of reasoning about calibration in the generative modeling case (e.g. with Inception Scores)?\n\nAdditionally, why should a more flexible approximate posterior be more beneficial for better calibrated uncertainty? (Is this a log-likelihood argument, since the log-likelihood decomposes to calibration + \u201csharpness\u201d?) More broadly, my biggest problem was that the paper makes claims about improving calibration (e.g. samples are obtained from \u201cwell calibrated posteriors\u201d in Figure 6) without formally defining how to evaluate \u201cgood calibration,\u201d what it means to have \u201ccalibrated uncertainty,\u201d etc. I would appreciate if the authors could clear up any misunderstandings I may have had about the work.\n\nQuestions/comments:\n- (Heusel et. al 2017) show that the inception score (IS) is not a valid metric for CelebA -- would the authors report FID scores instead?\n- Regarding the comment in Section 4 about posterior contraction: aren\u2019t NPs exchangeable by design via the iid latent variable z (Korshunova et. al 2019, among many others)? I thought that invoking (conditional) de Finetti via the iid latent variable is what allows NPs to model (exchangeable) stochastic processes.\n- It would be helpful to formally define \u201cposterior contraction\u201d for the reader -- is it referring to a reduction in posterior uncertainty?\n- Intuitively, why would sample diversity decrease as the size of the conditioning set grows? For example, if I have a dataset of 10 examples of only cats and dogs and increase its size to 1000 (which say, also includes examples of sheep), shouldn\u2019t that also increase my sample diversity as well?\n- Shouldn\u2019t the arrow going from z -> y_i in Figure 2 be reversed?\n\n-------------\nUPDATE: I have read the authors' rebuttal and revised draft and have raised my score to a 5.", "title": "contributions not clear", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rlh7ZGAw2Eg": {"type": "rebuttal", "replyto": "92G-M0wTWN", "comment": "Thank you for the follow-up and raising insightful questions and concerns.\n\nRegarding (b): The hierarchical encoder should be compared to models that optimize Eq. 2 in the paper (which we refer to as the ELBO objective), and the results in Appendix I show that SIVI models indeed outperform their ELBO counterparts. These results provide empirical evidence that the hierarchical encoder should be beneficial. It is also because of these results that we expect the overall performance drop for CelebA is due to other factors (in particular, the ELBO objective). The results in Appendix I do appear to support this hypothesis as the hierarchical encoder model at least achieves the best results among the ELBO models, but overall the models with the NP objective perform better than the models with the ELBO objective.\n\nRegarding (c): We argue that because of the lack of theoretical justification for the additional approximation in the NP objective, the better-known ELBO objective may be more desirable despite its worse empirical performance in some experiments. It is possible that a model which uses the NP objective with a hierarchical encoder would empirically perform better than NP+max, though a tractable objective would first need to be derived and this is better left as future work.\n\nWe agree with the reviewer that it is important to figure out what exactly causes our proposed model to perform worse than NP+max on CelebA. Possible sources that we are currently aware of include\n(regarding the inception score results) the underlying classifier used to compute the inception score, as the classifier does not perform as well as the MNIST and FashionMNIST classifiers. Related questions include whether the classifier was well-trained in the first place, what happens if the number of classes is changed, etc. There is also the bigger question of whether the inception score/GAN objective is actually an appropriate measure of what we desire in a model.\ncomplexity of the CelebA latent space. More levels to the hierarchy or a more expressive variational family may be required. There are many other options for obtaining a more expressive variational family (including the mentioned normalizing flows and their conditional variants).\nGiven that this list is already non-trivial and that there are likely other sources to consider, we believe a deeper investigation of the cause is better suited for follow-up work. \n\nRegarding the implications of using Eq. 2: as mentioned in Section 4, the NP objective can be written as two terms that both encourage posterior contraction. It is true that Eq. 2 loses the interpretation of the second term (the KL between the posterior conditioned on the target and the posterior conditioned on the context). However, we stress that these are merely interpretations of the objective that help explain why contraction occurs. It is clear from all of our experiments that posterior contraction still occurs with Eq. 2 as the objective. It is just that the objective does not provide an intuitive explanation for the contraction in this case.\n", "title": "Reply to reviewer 2"}, "V_ghkTA9jKB": {"type": "rebuttal", "replyto": "nZUeu9cKub", "comment": "We thank the reviewer for the thorough comments. Here is our response:\n\n__SIVI__: As mentioned in our response to _AnonReviewer2_, we have added additional results to the supplementary material investigating the effect of SIVI. Furthermore, we have added a new experiment on FashionMNIST supporting our hypothesis that a 1-level hierarchical encoder achieves more performance gain on a dataset with a finite set of well-separated classes. Also, our inception scores are more reliable on such datasets.\n\n__Mixed pooling__: The idea of using both max and mean pooling is interesting and we have actually tried it previously, but it did not perform better than the models with only max pooling. We have added inception scores plots to the appendices (see Appendix J)\n\n__ANP and ConvNP__: We have done experiments with ANP with max pooling and ANP with SIVI and max pooling, but these do not mitigate the lack of sample diversity in ANP. We are not sure of how Convolutional NPs would perform. We have added this statement to the paper.\n\n__Max pooling and posterior mean__: We argue that in the case of small conditioning sets, the posterior uncertainty is more relevant than the posterior mean. With large conditioning sets, the posterior mean begins to dominate. Even in the case of large conditioning sets, our experimental results suggest that max pooling does not do worse (if not better) than mean pooling in capturing the posterior mean (most evident comparing the samples in the right-most columns of Fig. 6a and Fig. 6e where the SIVI with max pooling samples resemble the original image more closely than the mean pooling samples).\n\n__NP+max results__: For space constraints, Fig. 6 only shows the \"standard\" models i.e. models proposed in the previous works. Qualitative results from NP+max are reported in Appendix L, and results from some other non-standard configurations are reported in the Appendix C.", "title": "Response to reviewer 1"}, "y9cAwCx21uR": {"type": "rebuttal", "replyto": "vbuB6iBc4Z6", "comment": "We thank the reviewer for the positive review and highlighting what they found to be conceptually interesting.", "title": "Response to reviewer 3"}, "mXK41eML6y": {"type": "rebuttal", "replyto": "ftdzTUyETGf", "comment": "We appreciate the reviewer\u2019s thoughtful comments and questions. Here is our response:\n\n__Contribution of SIVI__: To see the effect of hierarchical proposals in isolation, it is not enough to compare SIVI+max with NP+max or SIVI+mean with NP+mean, because SIVI models optimize a lower bound to $p(y_\\mathcal{T} | x_\\mathcal{T})$ (see Eq. 2 in the paper) while NP models target an approximation to the lower bound to $p(y_\\mathcal{T} | x_\\mathcal{T}, x_\\mathcal{C}, y_\\mathcal{C})$. Therefore, we should compare SIVI models with models that optimize Eq. 2, where SIVI outperforms the baseline on both datasets. We argue that although the NP objective empirically achieves better results, the additional approximation in its objective (that replaces $p(z|x_\\mathcal{C}, y_\\mathcal{C})$ with $q(z|x_\\mathcal{C}, y_\\mathcal{C})$) has not yet been justified theoretically. We have added a section to the appendices where the effect of SIVI is discussed in more detail and inception score plots for different models are provided.\n\nAdditionally, we have added an additional experiment on FashionMNIST and added the results to the paper. SIVI+max outperforms the other methods on this dataset as well. We conjecture that SIVI achieves a bigger performance gain when the dataset has a finite set of well-separated classes, like MNIST and FashionMNIST. This means that perhaps SIVI is able to better separate the different classes in the latent space. In addition, since we train classifiers on each dataset to compute inception score, having such well-behaved datasets results in more reliable trained classifiers and more accurate inception scores. It is possible that on datasets without well-defined classes like CelebA, a hierarchical encoder with more levels of hierarchy might be able to capture the multi-modality of the posterior, though with an additional approximation of the ELBO with each level of additional hierarchy.\n\nWe completely agree with the reviewer regarding the other suggestions and have updated the paper to include the mutual information view of inception score and choice of SIVI.\n\nWe thank the reviewer again for the helpful comments and related work suggestions.\n", "title": "Response to reviewer 2"}, "k2MKS4r1uCl": {"type": "rebuttal", "replyto": "57Le8gn2iQj", "comment": "We would like to thank the reviewer for the comments and constructive criticism. Here are our responses to your concerns:\n\n__Calibrated uncertainty__: The definition of \u201ccalibrated uncertainty\u201d in the papers that you listed is conceptually the same as what we use, i.e., that predictive uncertainty should be consistent with the data generating distribution (and using the empirical distribution as a proxy). However, the problem considered here is fundamentally different (and more challenging): we seek calibrated uncertainty on a per-image, per-conditioning set basis, which renders the approaches in those papers either inapplicable or infeasible. GAN-type evaluations of model performance are not perfect, but they are more appropriate for this problem. Progress in this area is important, but not the main point of the paper.\n\nThanks for pointing out this source of confusion; we have added a short background section discussing previous work on uncertainty calibration.\n\n\n__Contributions__: To clarify the confusion about the contributions of our work, we state our two main contributions here. First, we identify a key performance limitation of neural processes and explore the nature of that limitation empirically and conceptually. Specifically, we investigate how uncertainty is represented in neural process models as a function of the size of the context set. Second, we explore architecture and objective choices, and empirically show which choices lead to models that have better calibrated uncertainty. \n\n__FID__: Regarding your question about IS vs. FID, FID measures how similar two sets of (real and generated) images are. Therefore, FID requires access to \u201creal\u201d images. It is easily applicable to GANs where the goal is to learn the distribution of real objects from which a large dataset of i.i.d. samples is available. However, in our experiments, the \u201creal\u201d data distribution is a conditional distribution and we do not have access to samples from it. However, IS is computed purely from the generated samples and hence is more practical in our setting.\n\nAdditionally, as mentioned in the paper, we understand that the standard inception score gives misleading results on CelebA or MNIST, but that is because the standard score is computed using a network trained on ImageNet. That is why we compute the scores using classification networks trained on CelebA/MNIST images.\n\n__Posterior contraction__: As mentioned in the paper, \u201cposterior uncertainty (as measured by entropy) decreases for increasing context size\u201d is what we mean by posterior contraction. This is standard terminology in Bayesian statistics.\n\n__Exchangeability of NPs__: Unfortunately, there is confusion in the NP literature about stochastic processes and the so-called \u201cconditional de Finetti\u2019s theorem.\u201d Much of the confusion seems to stem from an incorrect interpretation that the consistency requirements of Kolmogorov\u2019s Extension Theorem are the same as exchangeability. They are not the same. (As a simple check, note that an infinite-length discrete-time Markov chain is a properly defined stochastic process whose existence is guaranteed by Kolmogorov\u2019s Extension Theorem, but it is not exchangeable.) The upshot is that NP models are *constructed* to have some useful conditional independence properties that ultimately are the same as those *assumed* in Gaussian Process regression (i.e., conditionally independent observations given a random function), but they are not exchangeable.\n\n\n__Graphical model__: Fig. 2 is the generative graphical model for both context and target sets. That is why the arrow is from z -> y_i. We have updated the figure caption to make it clear.", "title": "Response to reviewer 4"}, "nZUeu9cKub": {"type": "review", "replyto": "cT0jK5VvFuS", "review": "The authors propose an extension to Neural Processes (NPs), where they use max-pooling instead of mean-pooling as the aggregation function and use a mixture distribution with hierarchical VI for the decoder. They show that this slightly improves the predictive likelihoods, but crucially strongly improves the diversity of posterior samples when the conditioning set is small, which they argue is an important feature of such models.\n\nMajor comments:\n- What is the actual impact of using the SIVI? In most experiments, it looks like NP+max performs just as well.\n- What would happen if one would use mean-pooling AND max-pooling and just concatenate the two to yield an aggregated representation? Wouldn't that combine the best of both worlds and the downstream decoder could learn which representation to use for the mean and the variance prediction?\n- Could these ideas (SIVI, max-pooling) also be combined with more modern NP architectures (like Attentive NPs or Convolutional NPs)?\n\nMinor comments:\n- It is argued that the max-pooling is naturally better at capturing useful information for estimating the posterior variances. But what about the posterior mean? Shouldn't the mean-pooling be better for that?\n- In Tab. 1, \"NP+max\" seems to be the best-performing model. Why is it not shown in Tab. 6?\n\nSummary:\nI think the focus on the diversity of posterior samples is very interesting and highlights some important property of these kinds of models. However, given the relative simplicity of the proposed extensions, I feel like they are not studied extensively enough. For the paper to provide a clear value for the community, I think it would be good to extend the experiments to cover the whole combination space of {NP, ANP, SIVI} x {mean-pooling, max-pooling, mean+max-pooling}, so that it becomes clearer what the influence of the different design choices are in combination with each other.", "title": "Interesting idea, but the evaluation is not quite thorough enough", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ftdzTUyETGf": {"type": "review", "replyto": "cT0jK5VvFuS", "review": "The paper aims at increasing the sample diversity of neural processes when the condition set is small, while maintaining visual fidelity. The low-data regime is arguably where neural processes are most interesting, and in that regard the paper is right to turn to this setting. The discussion on how different aggregation functions affect the predictive uncertainty of the neural process is also appreciated, as is the experiment on regressing the size of the condition set based on the latent embedding.\n\nUnfortunately, the experiments section does not paint a clear enough picture. While the experiments show us that the proposed modifications have some benefits, it is not so clear how much each part contributes. Especially the contribution of the SIVI bound is hard to judge. As it stands the paper feels a bit incomplete in this regard. For that reason I cannot recommend accepting the paper at this stage, though I am willing to revise my score based on the authors' response. Specifically, I'd appreciate if the paper could make a clear case for adopting the hierarchical latent variable structure and the SIVI bound, as these add complexity to the method (while the max-aggregator does not).\n\n### Pros\n\n* The paper deals with a relevant issue. Neural processes are most interesting when the condition set is small, and this scenario has so far been largely ignored.\n* The discussion on the choice of aggregator is useful, as are the experiments on the variational posterior entropy and the prediction of the context set size.\n\n### Cons\n\n* It is unclear how much each modification (SIVI and max-pooling) contribute. The experimental results compare SIVI+max pooling with NP+max pooling, but SIVI+mean is omitted. It's also notable that NP+max seems to work better than SIVI+max in the CelebA dataset. Some discussion would be helpful here, as I don't see any reason why a hierarchical latent structure should hurt in any case, barring optimization difficulties.\n* I am not familiar with SIVI and I don't expect the average reader to be either. I'd appreciate some discussion on the choice of using it.\n\n### Other comments\n\n* The inception score sounds like the mutual information between the class label and the generated image. I expect that stating this would help some readers.\n* Perhaps it would help to look at each component in isolation and in different settings, e.g. outside the image domain. I can see how max-pooling might be good for images, while other aggregation methods might have an edge in, e.g. a dataset of robot joint trajectories. Other people have investigated the choice of aggregation method, and reading this work reminded me of work by [Soelch et al.](https://arxiv.org/abs/1903.07348), which might be interesting to the authors.", "title": "This paper proposes to improve neural processes by changing the aggregation operator and using a hierarchical latent structure. These decisions are aimed at improving the performance in the low-data regime.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "vbuB6iBc4Z6": {"type": "review", "replyto": "cT0jK5VvFuS", "review": "This paper proposes an improvement of the standard NP by using a mixture distribution \\q_{\\phi}, semi-implicit variational inference, and max pooling to capture the multimodel structure of the posterior distribution. Replacing one normal Gaussian distribution with a mixture (of Gaussians, normally) is a widely-adopted idea in latent variable models including NP; the adopted semi-implicit variational inference was originally developed in Yin and Zhou ICML 2018, and no further improvement on this inference method is proposed in this manuscript; max pooling is one of three commonly used pooling methods, i.e., max, min, and mean pooling. using one of them to replace another is simple but the explanation of the reason why max pooling is better is interesting and profound. So, the improvement is weak although it is shown to be effective by the empirical study. More importantly, the authors have investigated the posterior contraction of NP. It is interesting. The relationship between the two parts of the objective function of NP has been discussed related to the posterior contraction, both parts have contributed to the contraction apart from their classical explanation on reconstruction and regularization. To my best knowledge, it is the first work to discuss the posterior contraction of NP. It is a classical property in Bayesian and this link will enable further theoretical analysis for NP. ", "title": "This paper proposes an improvement of the standard NP, and the authors have investigated the posterior contraction of NP. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}