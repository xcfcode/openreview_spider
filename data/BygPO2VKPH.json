{"paper": {"title": "Sparse Coding with Gated Learned ISTA", "authors": ["Kailun Wu", "Yiwen Guo", "Ziang Li", "Changshui Zhang"], "authorids": ["wukl14@mails.tsinghua.edu.cn", "guoyiwen.ai@bytedance.com", "liza19@mails.tsinghua.edu.cn", "zcs@mail.tsinghua.edu.cn"], "summary": "We propose gated mechanisms to enhance learned ISTA for sparse coding, with theoretical guarantees on the superiority of the method. ", "abstract": "In this paper, we study the learned iterative shrinkage thresholding algorithm (LISTA) for solving sparse coding problems.  Following assumptions made by prior works, we first discover that the code components in its estimations may be lower than expected, i.e., require gains, and to address this problem, a gated mechanism amenable to theoretical analysis is then introduced. Specific design of the gates is inspired by convergence analyses of the mechanism and hence its effectiveness can be formally guaranteed. In addition to the gain gates, we further introduce overshoot gates for compensating insufficient step size in LISTA. Extensive empirical results confirm our theoretical findings and verify the effectiveness of our method.", "keywords": ["Sparse coding", "deep learning", "learned ISTA", "convergence analysis"]}, "meta": {"decision": "Accept (Spotlight)", "comment": "The paper extends LISTA by introducing gain gates and overshoot gates, which respectively address underestimation of code components and compensation of small step size of LISTA. The authors theoretically analyze these extensions and backup the effectiveness of their proposed algorithm with encouraging empirical results. All reviewers are highly positive on the contributions of this paper, and appreciate the rigorous theory which is further supported by convincing experiments. All three reviewers recommended accept.\n"}, "review": {"SJlzRVohKH": {"type": "review", "replyto": "BygPO2VKPH", "review": "1. Summary\nThe authors propose extensions to LISTA with the goal of addressing underestimation (by introducing \u201cgain gates\u201d) and including momentum (by introducing \u201covershoot gates\u201d). The authors provide theoretical analysis for each step of their LISTA augmentations, showing that it improves convergence rate. Their theoretical statements are empirically validated and then a numerical comparison is performed between the most interesting LISTA variants. Their proposed GLISTA performs favorably, especially for networks of depth greater than 10.\n2. Decision and arguments\nThe theory given is quite comprehensive and seems solid. Moreover it\u2019s motivated by and helps real problems, namely L1\u2019s well-known underestimation and LISTA\u2019s lack of momentum. On top of that, the numerical results not only show your GLISTA outperforming others, but that by adding gates to other LISTA variants you can get better results. On the other hand (see details in the Questions section), there is some critical information missing from the empirical section which makes the results non-repeatable. \nTraining is not described, i.e. hyperparameter searches and stopping criteria. A couple of the plots validating theory are difficult to understand. The datasets used are synthesized and tiny, only 1000 samples (training/testing/validation sets are not described) and 250x500 dimensions. That means your network has many more parameters than data samples unless I am mistaken.\nSo I have given weak reject because the paper has strong theory but weak experiments making it hard to trust the conclusions. I really want to hear back about the questions raised below.\n3. Questions \na) Just before eqn 12 you say that gains greater than 1 can be more appropriate. Am I understanding this correctly: *before* shrinkage you want to apply a gain on code elements that are \u201ctruly\u201d nonzero, in order to cancel out the imminent L1 penalization? And you will learn to predict those code elements via parameters Lambda?\nb) Just before Section 4, you describe the difference between yours and Moreau & Bruna\u2019s momentum taps. If I understand correctly they have a matrix called W_m^(k) which is multiplied onto the previous iterate\u2014but for each layer / time unit (k), the matrix may vary by learning (note that in the main body of their paper they don\u2019t explicitly use ^(k) notation, but it is explained to be the same as the other LISTA parameter matrices which vary with time, and is made explicit in the appendix). So I think their momentum is in fact time-varying. Moreover it is certainly dependent on the previous iterate, using the function f(z^(k-1)) = W_m^(k) * z^(k-1). Did you mean something else by that? In what sense do you have a higher capacity\u2014do you have more parameters?\nc) If I understand correctly, in Figure 5a, 5b, for each point you have trained independently an N-layer GLISTA. Then you observe properties of your learned parameters.\nFor 5a: how do you compute the \u201coutput\u201d of the gain gates? What is the input that gives this output?\nFor 5b: which \u2018t\u2019 is used to calculate ||W(t)D-I+U(t)A||? Or did you just train a single 15-layer GLISTA, and the x-axis is just \u2018t\u2019?\nd) Could you tell us more about the hyperparameter tests for every method? How do we know it was a fair comparison? There are no error bars, but from experience I know that training LISTA type networks can be a pain. What algorithms did you use? Stopping criteria? Some plots start at zero layers (4c, 5b, 6a-c) and some start at 1 layer\u2026 \ne) Interesting that in your real-data example, the sparse vector e has more non-zeros than zeros, is that really sparse? What dictionary A did you use, then? By your own description this task does not fit the sparse coding model you have analyzed\u2026. Am I misunderstanding?\n4. Additional feedback/ minor comments\na) Add reference for DOA estimation application\nb) You should put something like \u201cwe prove this in the supplement\u201d for props and theorems. Otherwise to readers less familiar with the literature it will seem like you forgot to put a reference.\nc) It seems unnecessary to put ||epsilon||<= 0 , instead of epsilon=0.\nd) Basically I think you should explain what your gate is before providing proofs about it. It would be useful to see a plot of the function g and/or kappa so provide some intuition about what you are doing to the architecture (nonlinear? Linear? Threshold? Etc.). Capital \u201cLambda\u201d is not defined until after theorems are provided about the functions g, kappa, f. In fact it seems like kappa is only useful for the sake of proofs. The first time through, I over-read the statement \u201cAll the learnable parameters are thus collected as Lambda\u2026\u201d just before Section 3.1.1 because I was thinking \u201cWhy isn\u2019t Lambda in the definitions of f/g/kappa??\u201d. Anyway it makes perfect sense when you define it clearly as after Eqn 18.\ne) At the beginning of Section 3.2 you have a clause \u201cthe over-shoot gates act more like on the output\u201d, which doesn\u2019t make sense.\nf) Figures should be approximately self-explanatory\u2014but \u201cRatio of lower components\u201d is not explained in the caption of Fig 4b. Although I greatly appreciate that you have made the actual plot lines/markers very easy to see! Should these results be averaged over many training attempts with error bars?\n", "title": "Official Blind Review #2", "rating": "8: Accept", "confidence": 3}, "BkeidSdTFr": {"type": "review", "replyto": "BygPO2VKPH", "review": "Summary:\n\nThis paper is focused on solving sparse coding problems using LISTA-type\nnetworks. It discusses the weakness of the ``no false positive'' assumption in\nprevious works and the weakness results in underestimated code components. The\nauthors propose a ``gain gating function'' to mitigate the weakness. Moreover,\nthe paper incorporates another ``overshoot'' gating function inspired by\nmomentum-based methods. Both contributions are supported with theoretical and\nempirical results. Numerical experiments show that the proposed model is\nsuperior to previous works especially in cases with high measurement noises or\nill-composed basis matrix. The paper is well written and easy to follow, and the\nempirical results are impressive.\n\nI really like the relaxation of ``no false positive'' assumption as in real world\napplication, learning-based algorithms may find it difficult to keep satisfying\nthe assumption while maintaining good empirical performance. And this relaxation\ncontributes to guarantee the convergence in a more real scenario.\n\nI think in (Chen et al., 2018), the support selection technique serves to\nmitigate the problem of underestimated code components as it bypasses the\nthresholding function for codes with large magnitudes. The empirical results\nalso show that in many cases LISTA with support selection has comparable\nperformance. I don't know if LISTA with support selection also suffers from\nthis underestimation problem severely.\n\nQuestions:\n\n1. The theorems in Section 3.1 hold without overshooting mechanism, i.e. eta==1.\nProp. 2 says that for one step of iteration, the optimal updates requires\novershoot. When the gain and overshoot mechanisms are combined, however, will\nthe theoretical convergence still hold?\n\n2. I don't have a very good understanding of why GLISTA performs so well given\nill-composed basis matrix compared to previous works. Could there be some\n(intuitive) explanation as to this based on the theoretical results?\n\n3. More details about the training process should be included. For example, I\ncan guess the loss function used for the training. But it is confusing not\nmentioning it at all in the paper. Some might think the objective funciton in\neqn (2) but the convergence analysis is with respect to the ground truth sparse\nvector. Also, the training scheme and some hyperparameter selection should also\nbe stated at least in Appendix.\n\n\nOverall I hold very positive attitude towards this paper due to its theoretical\ncontributions and good empirical results.\n\n================================\nUpdate: the authors addressed my questions well and I will keep my positive decision on this paper.", "title": "Official Blind Review #1", "rating": "8: Accept", "confidence": 3}, "SklsjVbtoH": {"type": "rebuttal", "replyto": "BkeidSdTFr", "comment": "Thanks for the positive feedback. We are very excited that our theoretical studies for relaxing the assumption of \u201cno-false-positive\u201d are endorsed. If possible, we would also like to study the support selection technique more to see whether the underestimation is theoretically mitigated in future studies.\n\nResponse to your questions:\n\n1. Good question! We analyzed the scenario when gain gates are introduced and eta>1 (i.e., with constant overshoots) and found that our theoretical results of linear convergence hold as well. For learned overshoot as described in Eq. (17) and function o_t, the theoretical analysis is nontrivial and we would like to consider it carefully in the future.\n\n2. In our understanding, one difference between ill-posed dictionary matrices and well-posed ones is their different values of $\\mu(A)$, and according to the theoretical results, ill-posed $A$ leads to slower convergence and possibly requires more sophisticated gains. We conjecture that our learnable mechanism endows higher capacity to adapt the model to such scenarios. From another perspective, we know from [1] that the convergence of LISTA with noise fulfills $$\\|x^{(t)}-x_s\\|_2\\leq \\alpha^t sB +\\frac{C^\\ast}{1-\\alpha},$$ in which $\\alpha=(2s-1)\\mu(A)$ is large given an ill-posed dictionary matrix and $C^\\ast>0$ does not vary with $\\mu(A)$. For a deep sparse coding network with noise, the second term in the above bound dominates the convergence speed. Since our gain gates operate as decreasing $\\alpha$ (when incorporated into LISTA), its superiority may be more significant with an originally large $\\alpha$.\n\n3. We appreciate the suggestions for including more experimental details. Explanations about our objective function, training policy, stoping criteria, optimizer, and hyper-parameter selection have now been added into Section 4 and 8.\n\n[1] Chen X, Liu J, Wang Z, et al. Theoretical linear convergence of unfolded ISTA and its practical weights and thresholds. NeurIPS, 2018.", "title": "Response to AnonReviewer #1"}, "Byxyxc7njH": {"type": "rebuttal", "replyto": "BygPO2VKPH", "comment": "We thank all the reviewers for their time and effort in reading and reviewing our paper. We have revised the paper following constructive suggestions and comments from the reviewers and we are more than glad to answer any further questions. In particular:\n\n* We have squeezed more experiments and discussions for the overshoot and gain gate into the main body of the paper.\n\n* We have shown standard deviations and error bars along with average performance over five runs in empirical comparisons.\n\n* We have explained more about our experimental details in Section 4 and 8 in the paper.\n", "title": "General response from authors"}, "rkgpyBlYjB": {"type": "rebuttal", "replyto": "S1eZe6Sa9H", "comment": "Thanks for the positive feedback and constructive suggestions. We have revised the paper accordingly. Some overshoot gate related experimental results have been squeezed into Section 4.1 in the main body of the paper. The effectiveness of a direct combination of the overshoot and gain gates has been highlighted. Also, the progress and limitation of asymptotic behaviors with overshoots are explained carefully in Section 3.2.", "title": "Response to AnonReviewer #3"}, "SkgYZ9sOsH": {"type": "rebuttal", "replyto": "SJlzRVohKH", "comment": "We appreciate the positive feedback on our theoretical discoveries and suggestions about clarifying more implementation details. We have revised the paper (especially Section 4) to introduce our experimental settings clearer, including but not limited to the objective function, training policy, stoping criteria, optimizer, and hyper-parameter selection. Our training strategies are kept the same as prior works for fair comparisons, following a progressive training principle on streaming data [1][2][3]. In principle, the training set \u201cgrows\u201d as training proceeds and is by no means tiny. The test set and validation set each consist of 1000 samples and the same sets are used for evaluating different methods. Our code will be made publicly available to make the results more reproducible. \n\nResponses to majors concerns and questions:\n\na. The development of our gain gains is motivated by rigorous analyses in Proposition 1. Since we know that, for non-zero code elements, the absolute value of estimations are always smaller or at most equal to that of the ground-truth and they share the same signs, we aim to enlarge such estimations before feeding them into the next processing units. Indeed, the estimations are enlarged via learnable gate functions in our method. \n\nb. We are aware of the ^(k) notation in the appendix of LFISTA, but what we tried to explain right before Section 4 is that, the major difference between our method and some classical momentum-based methods is whether the scaling factors (multiplied on previous estimations) in the momentum taps are dependent on the current input(s). Note that the \u201ccurrent input(s)\u201d here indicate not only the previous estimation x^(t-1), but also the observation y. We have revised the content a bit in the paper for better clarity on this point.\n\nc. Actually, unlike in Figure 4c, 6, and 7 we train a single 16-layer network for Figure 5a/5b and observe properties on each of its intermediate/final estimations. Therefore, for the output of the gain gates, we just average the obtained gain values over the whole test set and calculate it on all intermediate/final layers (from t=0 to t=d-1=15) for plotting in Figure 5a. Figure 5b is similarly obtained. We have revised the figure caption to make it clearer.\n\nd. 1) We have revised Section 4 in the paper to add more training details and avoid possible confusion. Hyper-parameters in competitors are set/tuned following their official codes on the validation set, and similarly for the ones in our method. 2) We now report standard derivations as well in Table 1, 3, and 4, along with the average performance over 5 runs which has already been reported in our previous version. All the figures except for Figure 5 now illustrate average results along with error bars as suggested. 3) Some plots start at the \u201cfirst\u201d layer instead of the \u201czeroth\u201d layer since some estimations may not make much sense with an initial estimation, e.g., for \"false positive rate\" within non-zero code components shown in Figure 4a, we don\u2019t calculate it on x^(0)=0.\n\ne. There seems to be a typo in our Section 4.2. Actually we let 40% of the elements of e be non-zero (rather than be \u201czero\u201d as described in the previous version). We also tried with higher sparsity with only 20% of the elements being non-zero and our method still outperforms the others \n(ours: 5.19e-3$\\pm$0.53e-3, LISTA:  0.0402$\\pm$0.0050, if q=15; \n ours: 1.76e-3$\\pm$0.84e-3, LISTA:   0.0204$\\pm$0.0032, if q=25; \n ours: 3.79e-4$\\pm$0.39e-4, LISTA:  7.51e-3$\\pm$2.97e3, if q=35). \nAs suggested, we also give a clearer description of the photometric stereo analysis problem in Section 4.2, including its corresponding A and y, to show that the problem well fits the model we have analyzed. \n\nFor minor comments, we thank the reviewer for pointing out and we have revised the paper accordingly.\n\n[1] Chen X, Liu J, Wang Z, et al. Theoretical linear convergence of unfolded ISTA and its practical weights and thresholds. NeurIPS, 2018.\n\n[2] Liu J, Chen X, Wang Z, et al. ALISTA: Analytic weights are as good as learned weights in LISTA, ICLR, 2019.\n\n[3] Borgerding M, Schniter P, Rangan S. AMP-inspired deep networks for sparse linear inverse problems. IEEE Transactions on Signal Processing, 2017, 65(16): 4293-4308.", "title": "Response to AnonReviewer #2"}, "S1eZe6Sa9H": {"type": "review", "replyto": "BygPO2VKPH", "review": "This paper proposed two novel gates to improve the convergence speed of the learned ISTA (LISTA) algorithm for sparse coding. The first gate is designed to address the problem that the output of the LISTA algorithm usually has a lower magnitude compared against the ground-truth. To address this, the paper proposed a gain gate to increase the magnitude of a layer in the LISTA algorithm. The second gate is designed to further improve the convergence with a technique similar to the momentum but with a time-varying coefficient. \n\nThis paper provides rigorous theoretical justifications for the observations that motivate the two gates in two propositions. It also provides theoretical guarantees for the first gain gate under practical assumptions. \n\nThorough synthetic experiments are conducted to empirically validate the proposals as well as the theorems. The effectiveness of the gates is also proved in a real-world computer vision task, photometric stereo. \n\nGiven the thoroughness of the theoretical and experimental justifications, I strongly recommend the acceptance of the paper.\n\nDespite the strength of the paper, I have the following suggestions and questions regarding the two gates proposed by the paper:\n\n1. The second proposed gate, the overshoot gate, lacks theoretical justification and is not as well studied as the gain gate. The experiments for this gate are not included in the main text but got placed in the appendix. I suggest squeezing some of them into the main text to show a complete picture about the overshoot gate. If there are more theoretical results about the behavior of the overshoot gate, the author should include it in the paper; otherwise, the author should clearly states this limitation in the main text of the paper.\n\n2. The author proposed two gates to address two different issues of the original LISTA algorithm. However, the two gates are only applied seperately to modify the LISTA algorithm; they have not been integrated into a single concise algorithm. It would strengthen the paper if the author could provide a further discussion on how the two gates can be combined to address the two issues in one algorithm.\n", "title": "Official Blind Review #3", "rating": "8: Accept", "confidence": 3}}}