{"paper": {"title": "Coupled Recurrent Models for Polyphonic Music Composition", "authors": ["John Thickstun", "Zaid Harchaoui", "Dean P. Foster", "Sham M. Kakade"], "authorids": ["thickstn@cs.washington.edu", "zaid@uw.edu", "sham@cs.washington.edu", "dean@foster.net"], "summary": "New recurrent generative models for composition of rhythmically complex, polyphonic music.", "abstract": "This work describes a novel recurrent model for music composition, which accounts for the rich statistical structure of polyphonic music. There are many ways to factor the probability distribution over musical scores; we consider the merits of various approaches and propose a new factorization that decomposes a score into a collection of concurrent, coupled time series: \"parts.\" The model we propose borrows ideas from both convolutional neural models and recurrent neural models; we argue that these ideas are natural for capturing music's pitch invariances, temporal structure, and polyphony.\n\nWe train generative models for homophonic and polyphonic composition on the KernScores dataset (Sapp, 2005), a collection of 2,300 musical scores comprised of around 2.8 million notes spanning time from the Renaissance to the early 20th century. While evaluation of generative models is know to be hard (Theis et al., 2016), we present careful quantitative results using a unit-adjusted cross entropy metric that is independent of how we factor the distribution over scores. We also present qualitative results using a blind discrimination test.\n", "keywords": ["music composition", "music generation", "polyphonic music modeling"]}, "meta": {"decision": "Reject", "comment": "This paper proposes novel recurrent models for polyphonic music composition and demonstrates the approach with qualitative and quantitative evaluations as well as samples. The technical parts in the original write-up were not very clear, as noted by multiple reviewers. During the review period, the presentation was improved. Unfortunately the reviewer scores are mixed, and are on the lower side, mainly because of the lack of clarity and quality of the results."}, "review": {"H1lqh2AHAX": {"type": "rebuttal", "replyto": "BJghCWtqh7", "comment": "Thank you for your extensive comments, and in particular for drawing our attention to the Johnson paper. Our relative pitch weight-sharing is the same idea as Johnson\u2019s tied parallel networks, and we have made sure to recognize this in the new revision of the paper.\n\nWe\u2019ve made an effort to clean up many of your specific comments regarding clarity (see our new top-level post for a summary of changes to the paper) and we also hope that the new Appendix B gives a more holistic response to your requests for clarification. Specifically regarding the models in Table 5: all models in Table 5 address the single-part (homophonic) task, so there is no global model. All models in Table 3 follow the coupled (standard) run architectures described in Equations (2), (3), and (4).\n\nRegarding piano scores: we hope Appendix B provides some clarification, in particular subsection B.4. A piano score like the one in Figure 5 can be handle as two homophonic parts (bass staff and treble staff). The example given in Appendix A is a particularly complicated case where the lines split into 4-part polyphony; our point is that KernScores gives us labels to decompose these more complicated cases into homophonic parts that can be modeled using coupled architectures.\n\nRegarding the \u201cunusualness\u201d of the melodies, we point out that about half our corpus consists of Renaissance music. This may account for some of the difference in model output compared to models trained on corpora consisting entirely of music from the canonical classical era (17th-19th centuries).\n\nWe have added single-part and two-part scores on the demos page, with the caveat that there are no single-part scores in our dataset and very few two-part scores, so these scores are somewhat out-of-sample.\n\nRegarding data augmentation: we found in our experiments that, for the relative pitch models, pitch-shifting data augmentation doesn\u2019t improve log-loss. Likewise, given weight-sharing for parts, shuffling the order of the parts doesn\u2019t improve log-loss.\n\nFor the listening test, excerpts were chosen at random locations in the score. The fact that participants struggle to distinguish between training data and model outputs puts at least a lower bound on the quality of generated output. But we agree that listening to these excerpts without context can often make very little sense. On the other hand, we make no claim to model long-term dependencies in music, so presenting listeners with long clips doesn't elicit informative feedback. We are open to ideas about better ways to evaluate these models.", "title": "re: many comments"}, "rkgk2iABA7": {"type": "rebuttal", "replyto": "rJexNSvj3X", "comment": "Thank you for your feedback. We have revised the paper to clarify the notational and definitional issues you identified. \n\nRegarding the number of bits needed for a piano roll: we have included further discussion of this point in Appendix B. We draw your attention in particular to Figures 5 and 7, which clarify the need for a second bit to distinguish between the onset of a note and continuation of a note from an earlier frame.", "title": "re: writing and piano roll representation"}, "BkldQoArAm": {"type": "rebuttal", "replyto": "rkxg76-an7", "comment": "Seven out of our twenty study participants self-identified as musically educated. Conditioned on that group, we found the following results:\n\nClip Length: 10, 20, 30, 40, 50\nAverage: 4.9, 6.0, 6.4, 6.9, 7.0\n\nSo there was no significant distinction in results based on musical education. \n\nOne thing to keep in mind is that a substantial fraction of our corpus is Renaissance music, which even well-educated classical musicians may be less familiar with. We informed participants of the scope of the training data prior to the listening test, but the bias towards Renaissance patterns in both the training data and model output could make classical music education less informative for discrimination.\n", "title": "re: qualitative evaluation"}, "Hyl5P90rCX": {"type": "rebuttal", "replyto": "r1V0m3C5YQ", "comment": "We have updated the paper. This comment summarizes the substantial changes.\n\n(*) We have included an additional Appendix B that precisely describes several popular factorizations of the distribution over scores, including the one used in this paper (Appendix B.5).\n\n(*) Some clarifications of the cross-entropy metric in Section 3.\n\n(*) Multiple revisions in Section 4 to clarify notation.\n\n(*) Clarification of the distinction between a monophonic and homophonic prediction task in Section 4.\n\n(*) Revision to Table 3: renaming \u201cbottom\u201d and \u201ctop\u201d to \u201cpart\u201d and \u201cglobal\u201d respectively, along with clarification in the caption regarding the meaning of these terms.", "title": "revision summary"}, "rkxg76-an7": {"type": "review", "replyto": "r1V0m3C5YQ", "review": "The paper is well written and presented, giving a good literature review and clearly explaining the design decisions and trade-offs. The paper proposes a novel factorisation approach and uses recurrent networks. \n\nThe evaluation is both quantitative and qualitative. The qualitative experiment is interesting, but there is no information given about the level of musical training the participants had. You would expect very different results from music students compared to the general public. How did you control for musical ability/ understanding?\n\nThe paper has a refreshing honesty in its critical evaluation of the results, highlighting fundamental problems in this field.\n\nOverall, while I am not an expert in musical composition and machine learning, the paper is clear, and appears to be advancing the art in a reliable fashion.", "title": "well-written paper", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BJghCWtqh7": {"type": "review", "replyto": "r1V0m3C5YQ", "review": "PROs\n-seemingly reasonable approach to polyphonic music generation: figuring out a way to splitting the parts, share parameters appropriately, measuring entropy per time, all make sense\n-the resulting outputs tend to have very short-term harmonic coherence (e.g. often a \u2018standard chord\u2019 with some resolving suspensions, etc), with individual parts often making very small stepwise motion (i.e. reasonable local voice leading)\n-extensive comparison of architectural variations\n-positive results from listening experiments\n\nCONs\n-musical outputs are *not* clearly better than some of the polyphonic systems described; despite the often small melodic steps, the individual lines are quite random sounding; this is perhaps a direct result of the short history\n-I do not hear the rhythmic complexity that is described in the introduction\n-the work by Johnson (2015) (ref. provided below) should be looked at and listened to; it too uses coupled networks, albeit in a different way but with a related motivation, and has rhythmic and polyphonic complexity and sounds quite good (better, in my opinion) \n-some unclear sections (fixable, especially with an appendix; more detail below)\n-despite the extensive architectural comparisons, I was not always clear about rationale behind certain choices, eg. if using recurrent nets, why not try LSTM or GRU? (more questions below)\n-would like to have heard the listening tests; or at least read more about how samples were selected (again, perhaps in an appendix and additional sample files)\n\n quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).\n\nQuality -- In this work, various good/reasonable choices are made. The quality of the actual output is fine. It is comparable to-- and to my ears not better than-- existing polyphonic systems such as the ones below (links to sample audio are provided here):\n\n-Bachbot - https://soundcloud.com/bachbot (Liang et al 2017)\n- tied parallel nets - http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/ (Johnson 2015, ref below)\n-performanceRNN - https://magenta.tensorflow.org/performance-rnn - (Simon & Oore 2017)\n..others as well..\n\n\nClarity -- Some of the writing is \"locally\" clear, but one large, poorly-organized section makes the whole thing confusing (details below). It is very helpful that the authors subsequently added a comment with a link to some sample scores; without that, it had been utterly impossible to evaluate the quality. There are a few points that could be better clarified:\n\t-p5\u201da multi-hot vector of notes N\u201d. It sounds like N will be used to denote note-numbers, but in fact it seems like N is the total number of notes, i.e. the length of the vector, right? What value of N is used?\n-p5 \u201ca one-hot vector of durations D\u201d. It sounds like D will be used to denote durations, but actually I think D is the length of the 1-hot vector encoding durations right? What value of D is used, and what durations do the elements of this vector represent?\n-similarly, does T represent the size of the history? This should really be clarified.\n\t-p5 Polyphonic models.\n\t\t-Eq (2), (3), (4): Presumably the h\u2019s are the hidden activations layers?\n\t\t-the networks here correspond to the blue circles in Fig 1, right? If so, make the relationship clear and explicit \n\t\t-Note that most variables in most equations are left undefined       \n\t\t-actually defining the W\u2019s in Eq(2-4)  would allow the authors to refer to the W\u2019s later (e.g. in Section 5.2) when describing weight-sharing ideas. Otherwise, it\u2019s all rather confusing. For example, the authors could write, \u201cThus, we can set W_p1 = W_p2 = W_p3 = W_p4\u201d (or whatever is appropriate). \n\t-Generally, I found that pages 5-7 describe many ideas, and some of them are individually fairly clearly described, but it is not always clear when one idea is beginning, and one idea is ending, and which ideas can be combined or not. On my first readings, I thought that I was basically following it, until I got to Table 5, which then convinced me that I was in fact *not* quite following it. For example, I had been certain that all the networks described are recurrent (perhaps due to Fig1?), but then it turned out that many are in fact *not* recurrent, which made a lot more sense given the continual reference to the history and the length of the model\u2019s Markov window etc. But the reader should not have had to deduce this. For example, one could write, \n\t\u201cWe will consider 3 types of architectures: convolutional, recurrent, .... In each architecture, we will have [...] modules, and we will try a variety of combinations of these modules. The modules/components are as follows:\u201d. It\u2019s a bit prosaic, but it can really help the reader. \n-Appendices, presented well, could be immensely helpful in clarifying the exact architectures; obviously not all 22 architectures from Table 5 need to be shown, but at least a few of them shown explicitly would help clarify. For example, in Fig1, the purple boxes seem to represent notes (according to the caption), but do they actually represent networks? If they really do represent notes, then how can \u201cnotes\u201d receive inputs from both the part-networks and the global network? Also, I was not entirely clear on the relationship of the architecture of the individual nets (for the parts) to that of the global integrating network. E.g. for experiment #20, the part-net is an RNN (with how many layers?? with regular or LSTM cells?) followed by a log-linear predictor (with one hidden layer of 300 units right? or are there multiple layers sometimes?), but then what is the global network? Why does the longest part-history vector appear to have length 10 based on Table 5, but according to Table 3 the best-performing history length was 20? Though, I am not sure the meaning of the \u201cbottom/top\u201d column was explained anywhere, so maybe I am completely misunderstanding that aspect of the table? Etc.\n-Many piano scores do not easily deconstruct into clean 4-part polyphony; the example in Appendix A is an exception. It was not clear to me how piano scores were handled during training. \n-Terminology: it is not entirely clear to me why one section is entitled \u201chomophonic models\u201d, instead of just \u201cmonophonic models\u201d. Homophonic music usually involves a melody line that is supported by other voices, i.e. a sort of asymmetry in the part-wise structure. Here, the outputs are quite the opposite of that: the voices are independent, they generally function well together harmonically, and there is usually no sense of one voice containing a melody. If there\u2019s some reason to call it homophonic, that would be fine, but otherwise it doesn\u2019t really serve to clarify anything. However, the authors do say that the homophonic composition tasks are a \u201cminor generalization of classic monophonic composition tasks\u201d, so this suggests to me that there is something here that I am not quite understanding.\n\nThe last sentence of Section 5.3 is very confusing-- I don\u2019t understand what lin_n is, or 1_n is, or how to read the corresponding entries of the table. The first part of the paragraph is fairly clear. \n\nTable 4: \u201cThe first row\u201d actually seems like it is referring to the second row. I know what the authors mean, but it is unnecessarily confusing to refer to it in this way. One might as well refer to \u201cthe zeroth row\u201d as listing the duration of the clip :)\n\nThe experimental evaluation: I would like to hear some of the paired samples that were played for subjects. Were classical score excerpts chosen starting at random locations in the score, or at the beginning of the score? It is known that listening to a 10-second excerpt without context can sometimes not make sense. I would be curious to see the false positives versus the false negatives. Nevertheless, I certainly appreciate the authors\u2019 warning to interpret the listening results with caution.\n\n\n\n\nOriginality & Significance -- So far, based both on the techniques and the output, I am not entirely convinced of the originality or significance of this particular system. The authors refer to \u201crhythmically simple polyphonic scores\u201d such as Bachbot, but I cannot see what is rhythmically fundamentally more sophisticated about the scores being generated by the present system. One nice characteristic of the present system is the true and audible independence of the voices.\n\nOne of the contributions appears to be the construction of models that explicitly leverage with shared weights some of the patterns that occur in different \u201cplaces\u201d (pitch-wise and temporally) in music. This is both very reasonable, and also not an entirely novel idea; see for example the excellent work by Daniel Johnson, \u201cGenerating Polyphonic Music Using Tied Parallel Networks\u201d (paper published 2017, first shared online, as far as I know, in 2015: links to all materials available at http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/  )\nAnother now common (and non exclusive) way to handle some of this is by augmenting the data with transposition. It seems that the authors are not doing this here. Why not? It usually helps. \n\nAnother contribution appears to be the use of a per-time measure of loss. This is reasonable, and I believe others have done this as well. I certainly appreciated the explicit justification for it, however.\n\nNote that the idea of using a vector to indicate metric subdivision was also used in (Johnson 2015).\n\nPlaying through some of the scores, it is clear that melodies themselves are often quite unusual (check user studies), but the voices do stay closely connected harmonically, which is what gives the system a certain aural coherence. I would be interested to hear (and look at) what is generated in two-part harmony, and even what is generated-- as a sort of baseline-- with just a single part. \n\nI encourage the authors to look at and listen to the work by Johnson:\n-listening samples: http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/\n-associated publication: http://www.hexahedria.com/files/2017generatingpolyphonic.pdf\n\nOverall, I think that the problem of generating rhythmically and polyphonically complex music is a good one, the approaches seem to generally be reasonable, although they do not appear to be particularly novel, and the musical results are not particularly impressive. The architectural choices are not always clearly presented.\n\t\t\t\n\t\t\n", "title": "sections almost made sense, output almost sounded good, ... (\"4: Ok but not good enough\")", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJexNSvj3X": {"type": "review", "replyto": "r1V0m3C5YQ", "review": "\nComposing polyphonic music is a hard computational problem. \nThis paper views the problem as modelling a probability distribution \nover musical scores that is parametrized using convolutional and recurrent \nnetworks. Emphasis is given to careful evaluation, both quantitatively and qualitatively. The technical parts are quite poorly written.\n\nThe introduction is quite well written and it is easy to follow. It provides a good review that is nicely balanced between older and recent literature. \n\nUnfortunately, at the technical parts, the paper starts to suffer due to sloppy notation. The cross entropy definition is missing important details. What does S exactly denote? Are you referring to a binary piano roll or some abstract vector valued process? This leaves a lot of guess work to the reader. \nEven the footnote makes it evident that the authors may have a different mental picture -- I would argue that a piano roll does not need two bits. Take a binary matrix: Roll(note=n, time=t) = 1 (=0) when note n is present (absent) at time t. \n\nI also think the term factorization is sometimes used freely as a synonym for representation in last paragraphs of 4 and first two paragraphs of 5 -- I find this misleading without proper definitions.\n\nThe models, which are central to the message of the paper, are not described clearly. Please\ndefine function a(\\cdot) in (2), (3), (4), : this maybe possibly a typesetting issue (and a is highly likely a sigmoid) but what does x_p W_hp x x_pt etc stand for? Various contractions? You have only defined the tensor as x_tpn. Even there, the proposed encoding is difficult to follow -- using different names for different ranges of the same index (n and d) seems to be avoiding important details and calling for trouble. Why not just introduce an order 4 tensor and represent everything in the product space as every note must have a duration? \n\nWhile the paper includes some interesting ideas about representation of relative pitch, the poor technical writing makes it not suitable to ICLR and hard to judge/interpret the extensive simulation results.\n\nMinor:\n\nFor tensors, 'rank-3' is not correct use, please use order-3 here if you are referring to the number of dimensions of the multiway array. \n\nWhat is a non-linear sampling scheme? Please be more precise.\n\nThe Allan-Williams citation and year is broken:\nMoray Allan and Christopher K. I. Williams. Harmonising Chorales by Probabilistic Inference. Advances in Neural Information Processing Systems 17, 2005.\n", "title": "Contains a good overview and extensive simulations. Unfortunately poor technical writing.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BygOalnNnQ": {"type": "rebuttal", "replyto": "r1V0m3C5YQ", "comment": "We've sampled some scores from the model described in the paper and released them anonymously here:\n\nhttp://ec2-18-219-197-207.us-east-2.compute.amazonaws.com/\n\nCode for loading the KernScores dataset discussed in the paper will be made available once this submission is de-anonymized.", "title": "Demos"}}}