{"paper": {"title": "Sorting out Lipschitz function approximation", "authors": ["Cem Anil", "James Lucas", "Roger B. Grosse"], "authorids": ["cem.anil@mail.utoronto.ca", "jlucas@cs.toronto.edu", "rgrosse@cs.toronto.edu"], "summary": "We identify pathologies in existing activation functions when learning neural networks with Lipschitz constraints and use these insights to design neural networks which are universal Lipschitz function approximators.", "abstract": "Training neural networks subject to a Lipschitz constraint is useful for generalization bounds, provable adversarial robustness, interpretable gradients, and Wasserstein distance estimation. By the composition property of Lipschitz functions, it suffices to ensure that each individual affine transformation or nonlinear activation function is 1-Lipschitz. The challenge is to do this while maintaining the expressive power.  We identify a necessary property for such an architecture: each of the layers must preserve the gradient norm during backpropagation. Based on this, we propose to combine a gradient norm preserving activation function, GroupSort, with norm-constrained weight matrices. We show that norm-constrained GroupSort architectures are universal Lipschitz function approximators. Empirically, we show that norm-constrained GroupSort networks achieve tighter estimates of Wasserstein distance than their ReLU counterparts and can achieve provable adversarial robustness guarantees with little cost to accuracy.", "keywords": ["deep learning", "lipschitz neural networks", "generalization", "universal approximation", "adversarial examples", "generative models", "optimal transport", "adversarial robustness"]}, "meta": {"decision": "Reject", "comment": "This paper presents an interesting and theoretically motivated approach to imposing Lipschitz constraints on functions learned by neural networks. R2 and R3 found the idea interesting, but R1 and R2 both point out several issues with the submitted version, including some problems with the proof--probably fixable--as well as a number of writing issues. The authors submitted a cleaned-up revised version, but upon checking revisions it appears the paper was almost completely re-written after the deadline. I do not think reviewers should be expected to comment a second time on such large changes, so I am okay with R1's decision to not review the updated version. Future reviewers of a more polished version of the paper will be in a better position to assess its merits in detail."}, "review": {"SkxeNOzqfV": {"type": "rebuttal", "replyto": "B1geukTDGV", "comment": "Posting publicly because the AC has not responded to our private comment.\n\nI get that it's fun to engage in debates about our community's publication standards, and that you want to defend your decision against criticism. But I wish you would be a bit more careful in this context. OpenReview is archival, and you are posting in your official capacity as AC, so any comment you make will be interpreted as referring to our submission. Your (I assume inadvertent) implication that our submission was the sort of thing the process needs to disincentivize is careless and misleading.\n\nOur original submission was a finished paper, and all of the algorithms and mathematical results were already in more or less their final form. In the revision, we added a bunch of new experiments, and did a global rewrite for clarity (which I think is what the revision period is intended for). Our original submission was not perfect by any means, and two of the three reviewers gave us insightful and constructive feedback that helped us improve the paper. But my students and I simply do not submit half-baked work.\n\nThe anonymous commenter does raise an important issue with the review process, namely that R1 did not take the time to read the paper even once. This wasn't because it was \"incomplete\", but because it had some typos scattered throughout, such as \\citet vs. \\citep. I would never, as an AC, endorse this as a legitimate reason not to write a proper review. I did have some papers in my AC batch which were genuinely incomplete (e.g. 6 pages), and even then I still insisted the reviewers read the paper and write real reviews.\n\n- Roger\n", "title": "OpenReview is not the place to debate hypotheticals"}, "rylEXpJnl4": {"type": "rebuttal", "replyto": "Hkx58AXfgV", "comment": "We understand that R1 is probably very busy and did not want to read our paper twice. But we would have appreciated if they could find the time to read it once.\n", "title": "\\citet vs. \\citep is no excuse not to write a real review"}, "BJx3P-AvpX": {"type": "rebuttal", "replyto": "HkgdKwx93Q", "comment": "Thank you for your detailed comments. We have uploaded a revised version of the paper which we believe addresses the majority of your concerns. You can find more detailed responses below. \n\nConcern 1: Is GroupSort leading to bad networks? (Integrate the topology of inputs)\n\nCould you clarify what you mean by \u201cintegrate the topology of inputs\u201d? Interpreting this as \u201cis GroupSort a niche activation?\u201d, we respond with the following: GroupSort is able to recover many common activation functions, for example ReLU, MaxOut, Concatenated ReLU, absolute value (now detailed in Appendix A). Importantly, it is often able to do this even with norm constrained weights (note that ReLU cannot recover GroupSort in this case). The main difference then will be how difficult GroupSort networks are to train. We have found practically that GroupSort networks are typically as easy to train as their ReLU counterparts. We trained wide ResNets using MaxMin and achieved comparable performance to ReLU. We also trained CelebA WGANs using MaxMin activations in the critic network without any issues. Importantly, in each case we used the suggested optimization hyperparameters tuned for ReLU and found that MaxMin worked too.\n\nConcern 2: Proof of Theorem 1 is incorrect.\n\nThank you for taking the time to carefully investigate this result. While we are confident that the result is correct, we have rewritten the proof in an attempt to make it clearer.\n\nTo address your points directly: we have modified the statement to hold almost everywhere, in which case we need not discuss sub differentials and may use differentiability directly. For your comment about the Cauchy-Schwarz inequality, note that the product cannot be larger than 1, as each individual component of the product has to be less than or equal to 1 (by the 1-Lipschitz constraint). Hence, each component must itself be 1. We have made this explicit in the revised proof, by bounding the product of norms above and below by 1. We have also removed the three-line result expressed in the appendix and instead baked it into the proof as part of the induction step. Finally, we have extended this result to hold in the setting of vector-valued inputs. Thank you for pointing out these issues to us. We hope that the improvements we\u2019ve presented will clarify the proof for you but would be happy to discuss this further.\n\nConcern 3: Why not use GroupSort only at the end of the network?\n\nThe universal construction must use GroupSort for the intermediate layers as well. We construct the final network by taking the max/min of increasingly wide and deep networks  (which are themselves max/mins). The final result is a network which uses MaxMin throughout and is able to represent the max/min of arbitrarily complicated Lipschitz functions.\n\nConcern 4: Table 3 shows FullSort doing worse\n\nThis is true and perhaps not particularly surprising. In Section 4.1 (Section 3.2 in old version) we state that while FullSort and MaxMin are equally expressive, the former leads to a more challenging optimization problem. The full-sort activation sorts the entire activation vector. We were surprised that the network was able to learn anything reasonable at all (especially with dropout!) and presented this column as a surprising observation - we do not suggest that practitioners adopt FullSort for classification as it is harder to optimize and more computationally expensive. We would be happy to clarify this further in the paper.\n\nWe hope that our responses above adequately resolve your concerns. Although we believe the current revision does a much better job of presenting these arguments, we warmly encourage you to provide any criticisms that may help us further express these points more clearly.\n", "title": "Feedback integrated into revised version. Thank you!"}, "HkgwSlAPpQ": {"type": "rebuttal", "replyto": "ryxY73AcK7", "comment": "We thank each of the reviewers for their time and comments. We have uploaded a revised version of our paper which addresses the notes from each reviewer and includes substantial improvements to the writing. The new version provides improved presentation of theoretical content and some new additions to the experiments section. We emphasize that the scope of the paper has not changed at all. Alongside these changes, we have also modified the title of our paper to \u201cSorting out Lipschitz function approximation\u201d.\n\nWe have also responded to each of the reviewers in kind and welcome further discussion!", "title": "Uploaded revision and individual comments"}, "rkeWyGAwp7": {"type": "rebuttal", "replyto": "H1gaveTEn7", "comment": "We are deeply sorry that you felt the paper was not in a position to be given a complete review. We acknowledge that the paper was certainly lacking polish (as also noted by reviewer 2) and accept that this may have made the paper difficult to read in places.\n\nWe have uploaded a revised version which is tidier and without so many of the unfortunate errors you spotted previously. The revised version also presents the theoretical results more cleanly with some substantial improvements to the experiments. We hope that you will provide a more complete review at this time.\n", "title": "Sorry for the poor presentation. Please, take another look!"}, "Hkx-jx0v6Q": {"type": "rebuttal", "replyto": "rJxFhEd63Q", "comment": "Thank you for your kind feedback!\n\nWe agree with your comments on the empirical results presented in the original paper. We are pleased to present several improvements in our revised version. We include much improved adversarial robustness results which contain provable robustness guarantees and strong empirical evidence that MaxMin leads to significantly more expressive networks than ReLU (see Fig. 8 in revised version). We also compared MaxMin to ReLU on Wide ResNets and found that MaxMin had comparable performance over the training schemes we explored (we used a limited hyperparameter search around the optimal ReLU settings). Finally, we used MaxMin to train a WGAN-GP model on CelebA and generated images qualitatively on-par with the carefully tuned Leaky-ReLU model. We believe that these new additions show that MaxMin is more than just a niche activation function and in Lipschitz-constrained settings may lead to significant practical gains.", "title": "Thank you for the feedback"}, "rJxFhEd63Q": {"type": "review", "replyto": "ryxY73AcK7", "review": "This paper introduces GroupSort. The motivation is to find a good way to impose Lipschitz constraint to the learning of neural networks. An easy approach is \"atomic construction\", which imposes a norm constraint to the weight matrix of every network layer. Although it guarantees the network to be a Lipschitz function, not all Lipschitz functions are representable under this strong constraint. The authors point out that this is because the activation function of the network doesn't satisfy the so called Jacobian norm preserving property.\n\nThen the paper proposes the GroupSort activation which satisfies the Jacobian norm preserving property. With this activation, it shows that the network is not only Lipschitz, but is also a universal Lipschitz approximator. This is a very nice theoretical result. To my knowledge, it is the first algorithm for learning a universal Lipschitz function under the architecture of neural network. The Wasserstein distance estimation experiment confirms the theory. The GroupSort network has stronger representation power than the other networks with traditional activation functions.\n\nAdmittedly I didn't check the correctness of the proof, but the theoretical argument seems like making sense.\n\nDespite the strong theoretical result, it is a little disappointing to see that the GroupSort doesn't exhibit any significant advantage over traditional activation function on image classification and adversarial learning. This is not surprising though.", "title": "Review of \"Universal Lipschitz Functions\"", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HkgdKwx93Q": {"type": "review", "replyto": "ryxY73AcK7", "review": "summary:\n\nA paper that states that a new activation function, which sorts coordinates in a vector by groups, is better than ReLU for the approximation of Lipschtiz functions.\n\npros:\n\n- interesting experiments\n- lots of different problems evaluated with the technique\n\ncons:\n\n- the GroupSort activation is justified from the angle of approximating Lipschitz transformations. While references are given why Lip is good for generalisation, I cannot see why GroupSort does not go *against* the ability of deep architectures to integrate the topology of inputs (see below).\n- the proof of Theorem 1 requires polishing (see below)\n- experiments require some polishing\n\ndetail:\n\n* The proof of Theorem 1 has three problems, first in the main file argument: since ReLU is not differentiable, you cannot use the partial derivative. Maybe a sub differential ? Second, in the RHS after the use of the Cauchy-Schwartz inequality (no equation numbering\u2026) you claim that the product of all three norms larger than 1 implies *each* of the last two is 1. This is wrong: it tell nothing about the the value of each, only about the *product* of each, which then make the next two identities a sufficient *but not necessary* condition for this to happen and invalidates the last identity. Last, the Theorem uses a three lines appendix result (C) which is absolutely not understandable. Push this in the proof, make it clear.\n\nSection D.1 (proof of Theorem 2) the proof uses group size 2 over a vector of dimension 2. This, unless I am mistaken, is the only place where the group sort activation is used and so the only place where GroupSort can be formally advocated against ReLU. If so, what about just using ReLUs and a single group sort layer somewhere instead of all group sort ? Have the authors tried this experimentally ?\n\nIf I strictly follow Algorithm 1, then GroupSort is carried out by *partitioning* the [d] indexes in g groups of the same size. This looks quite arbitrary and for me is susceptible to impair the capacity of deep architectures to progressively integrate the topology of inputs to generalise well. Table 3 tends to display that this is indeed the case as FullSort does much worse than ReLU.\n\n* Table 5: replace accuracies by errors, to be consistent with other tables.\n\n* in the experiments, you do not always specify the number of groups (Table 4)\n", "title": "Interesting paper but missing details and some formal polishing required", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1gaveTEn7": {"type": "review", "replyto": "ryxY73AcK7", "review": "The paper proposes a new \"sorting\" layer in neural networks that offers\nsome theoretical properties to be able to learn network which are 1-Lipschitz\nfunctions.\n\nThe paper contains what seems to be a nice contribution but the manuscript\nseems to have been written in a rush which makes it full of typos\nand very hard to read. This unfortunately really feels like unfinished work.\n\nJust to name a few:\n\n- Please check the use of \\citep and \\citet. See eg Szegedy ref on page 3.\n\n- Unfinished sentence \"In this work ...\" page 3.\n\n- \"]\" somewhere at the bottom of page 4.\n\n- \"Hence, neural network has cannot to lose Jacobian norm... \" ???\n\netc...\n\nAlthough I would like to offer here a comprehensive review I consider\nthat the authors have not done their job with this submission. ", "title": "Potentially interesting but unfinished work", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJgZYz8167": {"type": "rebuttal", "replyto": "B1evMmjA2X", "comment": "Thank you for bringing this preprint to our attention! OPLU is indeed identical to GroupSort with a grouping size of 2 (which we call MaxMin). We will cite this paper and credit it for proposing MaxMin and observing that it is norm-preserving.\n\nThe focus of the OPLU paper is to preserve the norm of gradients during backpropagation to allow the training of extremely deep networks. In our latest revision of the paper, we also discuss this property in terms of dynamical isometry [1]. In our work, our primary focus is on training expressive Lipschitz-constrained architectures and we identify gradient norm preservation as an important condition for which MaxMin is one such suitable activation. We also prove that using MaxMin we are able to recover universal approximation of Lipschitz functions (which other common activations fail to achieve).\n\nThe revised version and our response to the reviewers will be posted soon. \n\n[1]: Pennington et al. \u201cResurrecting the sigmoid in deep learning through dynamical isometry: theory and practice\u201d https://arxiv.org/abs/1711.04735", "title": "Thank you for bringing this to our attention"}}}