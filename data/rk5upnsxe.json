{"paper": {"title": "Normalizing the Normalizers: Comparing and Extending Network Normalization Schemes", "authors": ["Mengye Ren", "Renjie Liao", "Raquel Urtasun", "Fabian H. Sinz", "Richard S. Zemel"], "authorids": ["mren@cs.toronto.edu", "rjliao@cs.toronto.edu", "urtasun@cs.toronto.edu", "fabian.sinz@epagoge.de", "zemel@cs.toronto.edu"], "summary": "", "abstract": "Normalization techniques have only recently begun to be exploited in supervised learning tasks. Batch normalization exploits mini-batch statistics to normalize the activations. This was shown to speed up training and result in better models. However its success has been very limited when dealing with recurrent neural networks. On the other hand, layer normalization normalizes the activations across all activities within a layer. This was shown to work  well in the recurrent setting. In this paper we propose a unified view of  normalization techniques, as forms of divisive normalization, which includes layer and batch normalization as special cases. Our second contribution is the finding that a small modification to these normalization schemes, in conjunction with a sparse regularizer on the activations, leads to significant benefits over standard normalization techniques. We demonstrate the effectiveness of our unified divisive normalization framework  in the context of convolutional neural nets and recurrent neural networks, showing  improvements over baselines in image classification, language modeling as well as super-resolution.  ", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "On the one hand, the topic is considered important and the paper is technically correct. On the ohter hand, novelty and theoretical depth are a bit lacking. Overall, this is a borderline paper. \n\nStill, the Program Chairs recommend it for a poster presentation given the importance of the topic."}, "review": {"B1B9xL9xb": {"type": "rebuttal", "replyto": "rk5upnsxe", "comment": "Caffe cifar10 network can obtain around 82 % accuracy without any normalization, just by setting the learning rate schedules to {60K, 65K and 70K} starting from 0.001, 0.0001, 0.00001. Therefore, the advantages of L1 regularization are not very convincing.\n\n", "title": "Table 3 results seem to be more dependent on the learning rate scheduling than on the proposed normalization   "}, "ry1TEwxDe": {"type": "rebuttal", "replyto": "HJ_8mZdLg", "comment": "\nWe tried VGG-16 on ImageNet using top-1 accuracy with center crop. We apply\nnormalization (BN and DN) on the first convolutional layer of each stage of\nthe same feature map size. The baseline without any normalization scheme\nreaches 68.63%. The BN version reaches 69.19%, and the DN version also gets\n69.24%. Due to time limit, we have not yet finished training the version with\nan L1 sparse regularizer. DN performs better than our trained baseline network\nand on par with BN. This shows that DN (which, in contrast to BN, normalizes\non just a single image) is still useful to improve the accuracy over baseline\neven on larger problems. \n\nHowever, we acknowledge that these numbers are still\nbelow the public released VGG weights which has 71% top-1 accuracy. The\noriginal VGG paper did not release the exact learning rate schedule so we\ncouldn\u2019t fully reproduce the reported number within our time limit.", "title": "Updates on training VGG networks on ImageNet"}, "ryyg5IlPg": {"type": "rebuttal", "replyto": "Hyw2wWu8g", "comment": "\nWe run more than 30 experiments with different random seeds on DN ResNet-32 on\nboth CIFAR-10 and CIFAR-100 to investigate the pattern of learned sigmas are\nevery layer. In particular, we plot the relationship between sigma and input\nscale (local standard deviations of activations). The points are color coded\nby the layer number (from purple to yellow). We make several observations:\n\n1) At lower layers (dark purple), sigma is usually between medium to large to\nscale down the input values.\n\n2) At higher layers (light yellow), sigma goes to zero (as activation scale\nmight not matter for classification problem).\n\n3) In the middle, there is usually a reliable relationship between sigma and\nthe input scale, which maybe useful to make the parameter search more\nefficient.\n\nFig 2. Link to view the images\nhttp://imgur.com/a/muBFj\nLearned Sigma vs. Input Scale on ResNet-32", "title": "Further study on the effect of sigma on ResNet-32"}, "Hyw2wWu8g": {"type": "rebuttal", "replyto": "HyhJVhb4l", "comment": "\n------------------------------------------------------------------------------\nQ: Divisive normalization, is used extensively in Krizhevsky12 (LRN). The\nclaim that DN is less explored is questionable.\n\nA: Please see our general response above. In brief, the difference to\nKrizhevsky is not just constants but the spatial dimension we sum over.\nFurthermore, Krizhevsky et al. use normalization for one particular task while\nwe systematically explore it on several benchmarks. \n\n------------------------------------------------------------------------------\nQ: It is not clear whether the Divisive normalization does subtract the mean \nfrom the activation as there is a contradiction in its definition in equation \n1 and 3.\n\nA: Equation (1) is the divisive normalization model as used on the \nneuroscience literature. Equation (3) is our divisive normalization model. We \ndo subtract the mean in all cases. The reason is that divisive normalization \ncan be seen as a non-linear radial transformation which has strong links to \nredundancy reduction and density estimation on natural images and sound (see \nreferences Schwartz/Simoncelli, Lyu/Simoncelli, Sinz/Bethge, Balle/Simoncelli).\nHowever, these links rely on zero mean data (or elementw-ise transformations \nthereof). Therefore, even the neuroscience version often implicitly assumes \nmean centered data.  In practice, we found that mean subtraction is important \nas it helps accelerate the training. We also tried a variant with only mean-\nsubtraction (on BN, LN and DN) and found that the performance is consistently\nlower than divisive normalization, (1% decrease on CIFAR-10, and 3% decrease\non CIFAR-100).\n\n------------------------------------------------------------------------------\nQ: This questions whether the \"General Formulation of Normalization\" is\ncorrect.\n\nA: Our model technically includes the case where the mean is not subtracted by\nchoosing A to be empty. As we show in Table 1, different choices of sigma, A,\nand B recover BN, LN, or DN. One exception is the form used in Jarrett et al:\nx |-> x/max(||x||, c), which is not a special case of our model. However, DN\ncan be seen as a smooth, invertible approximation of it, since the function\nvalues for ||x|| -> infty and ||x||->0 are the same and DN is bounded and\nmonotonically increasing (i.e. sigmoidal) in between.\n\n------------------------------------------------------------------------------\nQ: It seems that Divisive normalization is used also in Jarrett09, called\nContrast Normalization, with a definition more similar to equation 3\n(subtracting the mean).\n\nA: As we explain in our general points above, our DN differs in important\naspects from Jarrett et al. and Krizhevsky et al. In brief, Krizhevsky et al.\nis invertible, but does not use spatial summation and does not use mean\ncentering (like we do). Jarrett et al. use mean centering but is not generally\ninvertible. As we argue above, we think that these choices matter for\nclassification performance.\n\n------------------------------------------------------------------------------\nQ: In case of the RNN experiments, BN may be inferior to DN due to a small \nbatch size.\n\nA: We experimented BN with larger batch size, which helped  BN achieve a\nslightly better performance which was  still worse than DN models. Varying\nbatch size was not considered in our experiments as the original learning rate\nschedule was tied to the number of epochs, and change batch size will end up\nsearching for the best learning rate schedule. In the new set of experiments,\nwe searched for the best learning rate schedule for BN with larger batch size.\nWe include a comparison here, but note that it may not be fair again since DN\nresults were based on smaller mini-batches.\n\nFor ReLU RNN models, we are able to increase the learning by 10 by having a\nbatch size of 120, which leads to the most improvement.\n\nReLU RNN (large batch=120)\nBN large batch:       128\nBN* large batch:      124\nDN small batch:       118 (In the paper)\nDN* small batch:      117 (In the paper)\n\nTanh RNN (large batch=120)\nBN large batch:       131 (Adding L1 did not help)\nDN small batch:       132 (In the paper)\nDN* small batch:      123 (In the paper)\n\nLSTM RNN (large batch=60)\nBN* large batch:      116\nDN* small batch:      102\n\n------------------------------------------------------------------------------\nQ: It is unclear what and how are measured the results shown in Table 10. Also\nit is unclear what are the sizes of the suppression/summation fields for the\nCIFAR and Super Resolution experiments.\n\nA: The plots in table 10 show representative 2d histograms of activations in \nthe first layer before normalization along with the average pairwise mutual\ninformation (MI) and correlation. This is to show that BN increases the\ndependencies between the activations on early layers and that the smoother and\nthe L1 regularizer can decrease those. This could be one of the reasons for\ntheir better performance.\n\nOn CIFAR, the suppression/summation fields are 5x5, 3x3, 3x3 for each\nconvolutional layer.\n\nOn super resolution, the suppression field is 5x5 and and the suppression\nfield is 7x7 for the first two convolutional layers.\n\n------------------------------------------------------------------------------\nQ: It is usually better to pick a stronger baseline for the tasks (e.g. the\nwidely available ResNet).\n\nA: Since we submitted the paper, we have been experimenting with ResNet-32 as\na much larger architecture, on CIFAR-10 and CIFAR-100. The original\narchitecture uses BN by default. If we remove BN, the architecture is very\ndifficult to train or converges to a poor solution. We first reproduced\noriginal BN ResNet-32, with 92.6% on CIFAR-10, and 69.8% on CIFAR-100.\n\nWe then train DN using the same hyperparameters. Importantly, the effect of\nsigma (2.5% gain on CIFAR-100) and L1 regularizer (0.5% gain on CIFAR-100) is\nstill found, even in the presence of other regularization techniques such as\nvarious data augmentation and weight decay in the training. This suggests that\nboth the smoother and L1 regularizer should be included when applying divisive\nnormalized models.\n\nSo far our best DN model  achieves 91.3% on CIFAR-10 and 66.6% on CIFAR-100.\nWhile this performance is lower than the original ResNet, there is certainly\nroom to improve as we have not performed any hyperparameter optimization, and\ninstead have used the same hyperparameters of the original BN ResNet. These\nwere found via extensive tuning with BN, but other hyperparameter settings are\nlikely better for DN.\n\nDuring training the DN network we made an interesting observation:  Since the\nnumber of sigma hyperparameters scales with the number of layers, we found\nthat setting sigma as a learnable parameter for each layer helps the\nperformance (1.3% gain on CIFAR-100). Note, that training this parameter is\nnot possible in the formulation by Jarrett et al. The learned sigma shows a\nclear trend: it tends to decrease with depth (see Fig. 1), and the sigma in the\nlast convolution layer is approaching to 0.\n\nReasons for that effect could be that the overall scale of the data changes\nover layers. Since there seems to be an optimal range for sigma, that range\nwould shift as well. Another reason could be that preserving the scale of the\ndata becomes less and less relevant towards deeper layers classification layer\nand the non-invertible (sigma=0) solution is more helpful in the final\nclassification layer.\n\nFig 1. Link to view the image: http://imgur.com/a/8l7lz\nLearned sigma in ResNet-32 on CIFAR-100 dataset.\n", "title": "Response to AnonReviewer3"}, "Bkc_z-_8g": {"type": "rebuttal", "replyto": "rk5upnsxe", "comment": "\nWe thank the reviewers for their constructive comments. Before we address each\nitem individually, we would like to emphasize a few general points.\n\nOur normalization scheme is the first to work well for both DNNs and RNNs.\nPreviously batch normalization (BN) was problematic for RNNs while layer\nnormalization (LN) did not perform well for DNNs. We demonstrate that our\nnormalization scheme performs on par or outperforms BN and LN on several\nbenchmark problems. While our scheme is similar in spirit to the ones used by\nJarrett et al. or Krizhesvky et al., it differs in important aspects that we\nbelieve are important in practice.\n\nOne important feature not present in current normalization schemes is the\nsmoother in the denominator which makes the normalization equation\ninvertible/information preserving (unlike Jarrett, like Krizhevsky). One\ncontribution of our paper is to show that the smoother boosts performance of\nthe normalization scheme (e.g. Figure 5 in Appendix A). While we use an\ninvertible normalization equation of the form x/sqrt(||x||^2 + c), Jarrett et\nal. use a clipping equation of the form x/max(||x||, c) which is only\ninvertible for very small signals (||x||<c).\n\nWe subtract the mean before normalizing (like Jarrett, unlike Krizhevsky). For\nmean centered responses, divisive normalization is a non-linear radial\ntransformation that has strong links to redundancy reduction on natural\nsignals (see references by Schwartz/Wainwright/Simoncelli, Lyu/Simoncelli,\nSinz/Bethge). We show that the L1 regularizer in combination with\nnormalization improves performance for all normalization schemes. We give\npotential explanations for why L1 regularization helps (e.g. Table 10).\n\nThus, while our normalization scheme uses existing elements, we empirically\ndemonstrate which elements are important in practice by means of a systematic\ncomparison of normalization schemes on a diverse set of problems. Our proposed\nnormalization scheme DN* yields better performance on several benchmark tasks,\nhas a wider application range than BN (i.e., RNN, super resolution), and is\neasier to use in practice since it does not require to keep track of\nstatistics during training.\n\nIndividual responses will be posted as direct replies under the comments.", "title": "General response to reviewers"}, "HJ_8mZdLg": {"type": "rebuttal", "replyto": "rJ3Df4vVe", "comment": "\n------------------------------------------------------------------------------\nQ: What the difference is between the new DN method and standard cross-channel\nlocal contrast normalization.\n\nA: The main difference between standard cross-channel contrast normalization\nand DN is the smoother (sigma) in the denominator. Contrast normalization can\nbe recovered by setting sigma=0 and choosing the summation and suppression\nfields appropriately. However, as we show in Figure 5 in Appendix A, non-zero\nvalues of sigma consistently yield better performances than sigma=0 (contrast\nnormalization). Thus, the difference between local contrast normalization and\nDN matters for the performance in practice.\n\n------------------------------------------------------------------------------\nQ: Conclusions might not hold on larger, stronger tasks, like ImageNet, and\nwith larger deeper models...\n\nA: We are currently training VGG networks on ImageNet, and we will update our\nresponse once we get results.", "title": "Response to AnonReviewer2"}, "HkLdFbuIe": {"type": "rebuttal", "replyto": "rkLxFcgNg", "comment": "\n------------------------------------------------------------------------------\nQ: \"Local Contrast Normalization\" is not said anywhere, as it is a common\nterminology in the neural network and vision literature.\n\nA: Please see our response to AnonReviewer2 for a comparison to Local Contrast\nNormalization. We will reference local contrast normalization papers in an\nupdated version of the paper.\n\n------------------------------------------------------------------------------\nQ: It is unclear to me why you chose to pair L1 regularization of the\nactivation and normalization. They seem complementary. Would it make sense to\napply L1 regularization to the baseline to highlight it is helpful on its own.\nOverall, it seems the only thing that brings a consistent improvement across\nall setups.\n\nA: We did add L1 to the baseline in our paper submission, see CIFAR-10/100\ntable results. Adding L1 without mean centering does not seem to be a sensible\nchoice. We also tried to add L1 to mean centered models (without having the\ndenominator). They consistently underperform DN* models (1% decrease on\nCIFAR-10 and 3% decrease on CIFAR-100).\n\nWe also added a potential explanation why L1 regularization helps and what our\nmotivation was to consider it in an updated version of the paper (also see our\nresponse below).\n\n------------------------------------------------------------------------------\nQ: On related work, maybe it would be worthwhile to insist that Local Contrast\nNormalization (LCN) used to be very popular [Pinto et al, 2008, Jarrett et al\n2009, Sermanet et al 2012; Quoc Le 2013] and effective. It is great to connect\nthis literature to current work on layer normalization and batch\nnormalization. Similarly, sparsity or group sparsity of the activation has\nshown effective in the past [Rozell et al 08, Kavukcuoglu et al 09] and need\nmore exposure today.\n\nA: Thanks for the suggestions. We added them to an updated version of the\npaper.\n\nAnonReviewer4: Finally, since dropout is so popular but interact poorly with\nnormalizer estimates, I feel it would be worthwhile to report results with\ndropout beyond the baseline and discuss how the different normalization scheme\ninteract with it.\n\nWe tried dropout in combination with normalization schemes. It did not seem to\nyield any improvement in performance and therefore we abandoned it later. The\nnormalization was only on convolutional layers, whereas dropout is applied on\nfully connected layers. It may be that dropout and normalization do not have\ntoo much interaction since they are applied at difference places.", "title": "Response to AnonReviewer4"}, "H1DU9KEEe": {"type": "rebuttal", "replyto": "SJf04_kQg", "comment": "\nQ: There does not seem to be consistently the best normalization method\nand the L1 regularization seems to be rather important. Do you have\nany intuition why BN* performs the best on CIFAR while DN* on the\nsuper-resolution task?\n\nA: We think that the reason why DN* outperforms BN* on super-resolution\nis that BN applies the same statistics to all patches of one\nimage which causes some overall intensity shift. DN, on the other\nhand, use a per image statistic. While this is useful for\nsuper-resolution having a larger pool of data to average over might be\nmore advantageous in the case of CIFAR. DN* only averages over a\nbatchsize of one, although it uses all channels. In the case of CIFAR\nthe bigger batch size for BN* might just be more helpful. Also, CIFAR\nimages are pretty small. Therefore, the difference in the spatial\naveraging range between BN* and DN* might not be relevant here.\n\n---\nQ: Do you think that it would be possible to see similar effects on\nlarger datasets? Such as ILSVRC etc.\n\nA: Yes, we expect that. Larger datasets with more classes and variety\nrequire bigger networks, so regularization and normalization is just\nas important there.\n---\nQ: What are the radii of the summation R_A and suppression fields R_B?\n\nThe radius of DN for CIFAR and RNN experiments are mentioned in the \npaper. Cifar: Local window size 5x5, 3x3, 3x3 (for each conv layer), \nand for RNN, 30 for LSTM and 60 for RNN.\n\n---\nQ: Would it be possible to elaborate more about what is visualized in\ntable 10? Is there some intuition why the BN and LN have almost\ndiagonal structure, while additional L1 regularization removes that?\n\nA: We visualize the joint histogram of filter responses (the v's) for\n4 pairs of filters to visualize correlations between those responses\non lower levels. We do not yet understand why the unregularized\nnormalization schemes can lead to such strong correlations. We do,\nhowever, have an intuition why the L1 regularizer helps decrease them.\nA possible explanation for this effect is that the L1 regularizer\nmight have a similar effect as maximum likelihood estimation of an\nindependent Laplace distribution. To see that, let $\np_v\\left(\\mathbf{v}\\right) \\propto\\exp\\left(-\\left\\Vert\n\\mathbf{v}\\right\\Vert _{1}\\right) $ and $\\mathbf{x}=W^{-1}\\mathbf{v}$\nwith full rank invertible matrix $W$. Under this model\n$p_x\\left(\\mathbf{x}\\right) =p_v\\left(W\\mathbf x\\right)\\left|\\det\nW\\right|$.  Then, minimization of the L1 norm of the activations under\nthe volume conserving constraint $\\det A=\\text{const.}$ corresponds to\nmaximum likelihood on that model, which would encourage decorrelated\nresponses. We do not enforce such a constraint, and the filter matrix\nmight even not be invertible. However, the supervised loss function of\nthe network benefits from having diverse non-zero filters. This\nencourages the network to not collapse filters on the same direction\nor put them to zero, and might act as a relaxation of the volume\nconserving constraint.", "title": "Reply of Pre-review Questions"}, "SJf04_kQg": {"type": "review", "replyto": "rk5upnsxe", "review": "- There does not seem to be consistently the best normalization method and the L1 regularization seems to be rather important. Do you have any intuition why BN* performs the best on CIFAR while DN* on the super-resolution task?\n- Do you think that it would be possible to see similar effects on larger datasets? Such as ILSVRC etc.\n- What are the radii of the summation R_A and suppression fields R_B?\n- Would it be possible to elaborate more about what is visualized in table 10? Is there some intuition why the BN and LN have almost diagonal structure, while additional L1 regularization removes that?This paper empirically studies multiple combinations of various tricks to improve the performance of deep neural networks on various tasks. Authors investigate various combinations of normalization techniques together with additional regularizations. \n\nThe paper makes few interesting empirical observations, such that the L1 regularizer on top of the activations is relatively useful for most of the tasks. \n\nIn general, it seems that this work can be significantly improved by providing more precise study of existing normalization techniques. Also, studying more closely the overall volumes of the summation and suppression fields (e.g. how many samples one needs to collect for a robust enough normalization) would be useful.\n\nIn more detail, the work seems to have the following issues:\n* Divisive normalization, is used extensively in Krizhevsky12 (LRN). It is almost exactly the same definition as in equation 1, however with slightly different constants. Therefore claiming that it is less explored is questionable.\n* It is not clear whether the Divisive normalization does subtract the mean from the activation as there is a contradiction in its definition in equation 1 and 3. This questions whether the \"General Formulation of Normalization\" is correct.\n* In seems that Divisive normalization is used also in Jarrett09, called Contrast Normalization, with a definition more similar to equation 3 (subtracting the mean).\n* In case of the RNN experiments, it would be more clear to provide the absolute size of the summation and suppression field as BN may be inferior to DN due to a small batch size.\n* It is unclear what and how are measured the results shown in Table 10. Also it is unclear what are the sizes of the suppression/summation fields for the CIFAR and Super Resolution experiments.\n\nMinor, relatively irrelevant issues:\n* It is usually better to pick a stronger baseline for the tasks. The selected CIFAR model from Caffe seems to be quite far from the state of the art on the CIFAR dataset. A stronger baseline (e.g. the widely available ResNet) would allow to see whether the proposed techniques are useful for the more recent models as well.\n* Double caption for Table 7/8.", "title": "Pre-review Questions", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyhJVhb4l": {"type": "review", "replyto": "rk5upnsxe", "review": "- There does not seem to be consistently the best normalization method and the L1 regularization seems to be rather important. Do you have any intuition why BN* performs the best on CIFAR while DN* on the super-resolution task?\n- Do you think that it would be possible to see similar effects on larger datasets? Such as ILSVRC etc.\n- What are the radii of the summation R_A and suppression fields R_B?\n- Would it be possible to elaborate more about what is visualized in table 10? Is there some intuition why the BN and LN have almost diagonal structure, while additional L1 regularization removes that?This paper empirically studies multiple combinations of various tricks to improve the performance of deep neural networks on various tasks. Authors investigate various combinations of normalization techniques together with additional regularizations. \n\nThe paper makes few interesting empirical observations, such that the L1 regularizer on top of the activations is relatively useful for most of the tasks. \n\nIn general, it seems that this work can be significantly improved by providing more precise study of existing normalization techniques. Also, studying more closely the overall volumes of the summation and suppression fields (e.g. how many samples one needs to collect for a robust enough normalization) would be useful.\n\nIn more detail, the work seems to have the following issues:\n* Divisive normalization, is used extensively in Krizhevsky12 (LRN). It is almost exactly the same definition as in equation 1, however with slightly different constants. Therefore claiming that it is less explored is questionable.\n* It is not clear whether the Divisive normalization does subtract the mean from the activation as there is a contradiction in its definition in equation 1 and 3. This questions whether the \"General Formulation of Normalization\" is correct.\n* In seems that Divisive normalization is used also in Jarrett09, called Contrast Normalization, with a definition more similar to equation 3 (subtracting the mean).\n* In case of the RNN experiments, it would be more clear to provide the absolute size of the summation and suppression field as BN may be inferior to DN due to a small batch size.\n* It is unclear what and how are measured the results shown in Table 10. Also it is unclear what are the sizes of the suppression/summation fields for the CIFAR and Super Resolution experiments.\n\nMinor, relatively irrelevant issues:\n* It is usually better to pick a stronger baseline for the tasks. The selected CIFAR model from Caffe seems to be quite far from the state of the art on the CIFAR dataset. A stronger baseline (e.g. the widely available ResNet) would allow to see whether the proposed techniques are useful for the more recent models as well.\n* Double caption for Table 7/8.", "title": "Pre-review Questions", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}