{"paper": {"title": "Data Noising as Smoothing in Neural Network Language Models", "authors": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng"], "authorids": ["zxie@cs.stanford.edu", "sidaw@cs.stanford.edu", "jiweil@stanford.edu", "danilevy@cs.stanford.edu", "anie@cs.stanford.edu", "jurafsky@stanford.edu", "ang@cs.stanford.edu"], "summary": "Derive data noising schemes for neural network language models corresponding to techniques in n-gram smoothing.", "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in n-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.", "keywords": ["Natural language processing", "Deep learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The reviewers are reasonably supportive of this paper. The ideas presented in the paper are nice and the results are encouraging. The authors should consider, for the final version of this work, providing comparisons to other approaches on the text8 corpus (or on the 1 Billion Words corpus, Chelba et al.)."}, "review": {"HJrzkOELg": {"type": "rebuttal", "replyto": "H1VyHY9gg", "comment": "We thank the anonymous reviewers again for their valuable feedback! MT results for source-only and target-only noising have been added to Table 4.\n\nWe wish to emphasize that the work\u2019s strength is the principled theoretical connection we derive between noising and smoothing. This connection allows us to incorporate generative assumptions into the discriminative sequence prediction setting, and provides an explanation for the empirical gains by several other papers.\n\nTo paraphrase Reviewer 4, we then adapt smoothing methods from n-gram models to continuous language models. We describe analogues to not only basic methods such as interpolation but also Kneser-Ney.\n\nFinally, as all reviewers seem to agree, the method is effective and simple to apply.\n", "title": "Response to Reviews"}, "r1YV-osEx": {"type": "rebuttal", "replyto": "BJz3eHiVg", "comment": "Thanks for the feedback. We agree the simplicity of the method is a strength.\n\nRegarding theoretical justification: Please see our response to Reviewer 3, where we summarize the high-level theoretical justification in the paper. We also contribute the connection between smoothing and noising, which provides a theoretical explanation for the empirical results of several papers (last paragraph of Related Work).\n\nWe would also like to emphasize that we do examine incorporating generative assumptions such as discounting and diverse histories in the paper. Thanks again for the suggestions.", "title": "Response"}, "B1wr1oi4g": {"type": "rebuttal", "replyto": "rJK-1M5Eg", "comment": "Thanks for the feedback. We fully agree that the method is simple and improves performance.\n\nWe maintain that there is theoretical justification for why noising should work for RNN models, though perhaps this was vaguely put in the paper:\n1. RNN LMs trained using maximum likelihood often overfit. Consider a large RNN that achieves near the minimum training cross-entropy loss -- it then behaves like an n-gram model.\n2. We show the noising schemes result in pseudocounts corresponding to interpolation smoothing (Section 3.3), which helps avoid overfitting since the RNN LM is trained (again, using maximum likelihood) on the noised data.\n\nResponse to other comments:\n- p. 3&4 We\u2019ve tried to make more explicit the connection between blank noising and interpolation in the text (this seems like an easier way to explain not overfitting to specific contexts). We\u2019ve also annotated the general form equations on p. 4, which hopefully makes them more clear. More feedback always appreciated!\n- p. 5 Thanks for this helpful suggestion. We\u2019ll update the MT results with numbers where the source/target are not noised.", "title": "Response: theoretical justification, comments"}, "B1DQHmA7e": {"type": "rebuttal", "replyto": "rkFlM317x", "comment": "Thanks for the questions! We agree that scale of data is an important consideration. PTB contains 929k training tokens (42K sentences), 73k validation tokens, and 82k test tokens. For the IWSLT dataset, training contains 190K sentence pairs with ~5.4M tokens in total. We\u2019ve updated the paper accordingly.\n\nIt is possible to run the smoothing methods on more data. We've updated the paper with some results on Text8, which has 15.3M training tokens, 848K validation tokens, and 855K test tokens, with a 42K vocabulary in our experiments. As you suggest, as the amount of data grows, it may be necessary to use higher order n-gram statistics with more bookkeeping to obtain gains.\n\nRegarding computational costs / speed: Even for larger values of gamma and larger vocabularies (e.g. in the Text8 experiments), we do not observe significant changes to tokens/sec processed when using cumulative sums with bisection to sample (~2.5microsec/sample for a 42K vocabulary on our machine); runtime remains dominated by GPU operations for the RNN. It is true that noised models usually must be trained for more epochs, as shown in Figure 1, and this depends on optimization and annealing settings. As mentioned in the paper, only training time is affected by noising; test time is unaffected unlike in some other methods. Hope this clarifies things!\n", "title": "amount of data"}, "rJGO47RQx": {"type": "rebuttal", "replyto": "rymMxH1Qe", "comment": "Thanks for the reference, which we unfortunately missed in our survey of related work. It\u2019s difficult for us to compare our gains in perplexity/BLEU with WER reductions, though it would be great to see the effects of noising on the LM/decoder for applications such as speech transcription. Regarding NAT vs. multi-style training, it would also be interesting to model different LM noise types and levels as hidden variables, though we did not explore that direction in this work. Thanks for the ideas/suggestions!", "title": "comparison with noising in speech"}, "rkFlM317x": {"type": "review", "replyto": "H1VyHY9gg", "review": "Can you provide the detailed information of the data size (number of words and sentences) for both Penn Treebank and IWSLT2015 corpus so that readers can figure out the data size range of experiments?\nSince the method is motivated by a smoothing method, and the discussion of the smoothing method depends on the scale of data (and the order of n-gram), it would be interesting to have such information.\nProviding the computational costs of the proposed method would be more helpful.\nAlso, is it possible to apply it to larger scale data?\n\nThis paper proposes a data noising technique for language modeling. The main idea is to noise a word history by using a probabilistic distribution based on N-gram smoothing techniques. The paper is clearly written and shows that such simple techniques improve the performance in various tasks including language modeling and machine translation. May main concern is that the method is too simple and sounds ad hoc, e.g., there is no theoretical justification of why n-gram smoothing based data noising would be effective for recurrent neural network based language modeling.\n\nComments: \n- p. 3 \u201ccan be seen has a way\u201d -> \u201ccan be seen as a way\u201d (?)\n- p. 3. In general, the explanation about blank noising should be improved. Why does it avoid overfitting on specific contexts?\n- p. 4. It would be better to provide more detailed derivations for a general form of unigram and blank noising equations.\n- p. 5, Section 3.6: Is there any discussions about noising either/both input and output sequences with some numbers? This would be helpful information.\n", "title": "Amount of data.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJK-1M5Eg": {"type": "review", "replyto": "H1VyHY9gg", "review": "Can you provide the detailed information of the data size (number of words and sentences) for both Penn Treebank and IWSLT2015 corpus so that readers can figure out the data size range of experiments?\nSince the method is motivated by a smoothing method, and the discussion of the smoothing method depends on the scale of data (and the order of n-gram), it would be interesting to have such information.\nProviding the computational costs of the proposed method would be more helpful.\nAlso, is it possible to apply it to larger scale data?\n\nThis paper proposes a data noising technique for language modeling. The main idea is to noise a word history by using a probabilistic distribution based on N-gram smoothing techniques. The paper is clearly written and shows that such simple techniques improve the performance in various tasks including language modeling and machine translation. May main concern is that the method is too simple and sounds ad hoc, e.g., there is no theoretical justification of why n-gram smoothing based data noising would be effective for recurrent neural network based language modeling.\n\nComments: \n- p. 3 \u201ccan be seen has a way\u201d -> \u201ccan be seen as a way\u201d (?)\n- p. 3. In general, the explanation about blank noising should be improved. Why does it avoid overfitting on specific contexts?\n- p. 4. It would be better to provide more detailed derivations for a general form of unigram and blank noising equations.\n- p. 5, Section 3.6: Is there any discussions about noising either/both input and output sequences with some numbers? This would be helpful information.\n", "title": "Amount of data.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}