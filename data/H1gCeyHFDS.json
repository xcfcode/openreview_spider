{"paper": {"title": "Gram-Gauss-Newton Method: Learning Overparameterized Neural Networks for Regression Problems", "authors": ["Tianle Cai*", "Ruiqi Gao*", "Jikai Hou*", "Siyu Chen", "Dong Wang", "Di He", "Zhihua Zhang", "Liwei Wang"], "authorids": ["caitianle1998@pku.edu.cn", "grq@pku.edu.cn", "1600010681@pku.edu.cn", "siyuchen@pku.edu.cn", "wangdongcis@pku.edu.cn", "di_he@pku.edu.cn", "zhzhang@math.pku.edu.cn", "wanglw@cis.pku.edu.cn"], "summary": "A novel Gram-Gauss-Newton method to train neural networks, inspired by neural tangent kernel and Gauss-Newton method, with fast convergence speed both theoretically and experimentally.", "abstract": "First-order methods such as stochastic gradient descent (SGD) are currently the standard algorithm for training deep neural networks. Second-order methods, despite their better convergence rate, are rarely used in practice due to the pro- hibitive computational cost in calculating the second-order information. In this paper, we propose a novel Gram-Gauss-Newton (GGN) algorithm to train deep neural networks for regression problems with square loss. Our method draws inspiration from the connection between neural network optimization and kernel regression of neural tangent kernel (NTK). Different from typical second-order methods that have heavy computational cost in each iteration, GGN only has minor overhead compared to first-order methods such as SGD. We also give theoretical results to show that for sufficiently wide neural networks, the convergence rate of GGN is quadratic. Furthermore, we provide convergence guarantee for mini-batch GGN algorithm, which is, to our knowledge, the first convergence result for the mini-batch version of a second-order method on overparameterized neural net- works. Preliminary experiments on regression tasks demonstrate that for training standard networks, our GGN algorithm converges much faster and achieves better performance than SGD.", "keywords": ["Deep learning", "Optimization", "Second-order method", "Neural Tangent Kernel regression"]}, "meta": {"decision": "Reject", "comment": "The article considers Gauss-Newton as a scalable second order alternative to train neural networks, and gives theoretical convergence rates and some experiments. The second order convergence results rely on the NTK and very wide networks. The reviewers pointed out that the method is of course not new, and suggested that comparison not only with SGD but also with methods such as Adam, natural gradients, KFAC, would be important, as well as additional experiments with other types of losses for classification problems and multidimensional outputs. The revision added preliminary experiments comparing with Adam and KFAC. Overall, I think that the article makes an interesting and relevant case that Gauss-Newton can be a competitive alternative for parameter optimization in neural networks. However, the experimental section could still be improved significantly. Therefore, I am recommending that the paper is not accepted at this time but revised to include more extensive experiments. \n"}, "review": {"BJeshEY55r": {"type": "review", "replyto": "H1gCeyHFDS", "review": "The authors propose a scalable second order method for optimization using a quadratic loss. The method is inspired by the Neural Tangent kernel approach, which also allows them to provide global convergence rates for GD and batch SGD. The algorithm has a computational complexity that is linear in the number of parameters and requires to solve a system of the size of the minibatch.  They also show experimentally the advantage of using their proposed methods over SGD. \n\n\t- The paper is generally easy to read except section 3.1 which could be clearer when establishing the connexion between the proposed algorithm and NTK.\n\n\t- The proposed algorithm seems to be literally a regularized Gauss-Newton with Woodbury matrix inversion lemma applied to equation (7). Additional simplifications occur due to the pre-multiplication by the jacobian and give (9). However, this is not clear in the paper, instead section 3.1, is a bit vague about the derivation of (9).\n\t- In terms of theory, the proofs of thm 1 and 2 seem sound. They rely essentially on the convergence results established for NTK in [Jacot2018, Chizat2018]. The main novelty is that the authors provide faster rates for the Gauss-Newton pre-conditioner which leads to second-order convergence. The second theoretical contribution is to extend the proof to batched gradient descent. Both are somehow expected, although the second one is more technical.\n\t- However, the convergence rates provided for batched gradient descent (thm 2) rely on a rather unrealistic assumption: the size of the network should grow as n^18 where n is the sample size. This makes the result less appealing as in practice this is highly unlikely to be the case.\n\t-  The convergence analysis for the NTK dynamics, which is essential in the proof, relies on a particular scaling 1/sqrt(M) of the function with the number of parameters. In [Chizat2018], it is discussed that although it leads to convergence in the training loss, generalization can be bad. Is there any reason to think in this case, things would be different?\n\n\t- Experiments: Experiments were done on two datasets to solve a regression task. They show that training loss decreases indeed faster than SGD and finds better solutions. A more fair comparison would be against other second-order optimizers like KFAC.\n\t- How was the learning rate chosen for the other methods? Was the same lr used?\n\t- The authors say that the algorithm has the same cost of one backward pass, could they be more specific about the implementation?\n\t- What are the test results for the second dataset? Could they be reported somewhere (in the appendix?)\n        - Both tasks are univariate regression, can the method be applied successfully in a multivariate setting? \nI don't see how the proposed method is different from exactly doing regularized gauss newton, so to me the algorithm is not novel in itself. Besides the method seems to require a quadratic loss function which limits its application.\n\n\n----------------------------------------------------------------------------------\nRevision:\n\n\n I've read the author's response and other reviews. I think the paper will be stronger if extended to more general cases (multivariate output + more general losses), thus I encourage the authors to resubmit the paper with stronger experiments.\n\n\n\n\n", "title": "Official Blind Review #4", "rating": "3: Weak Reject", "confidence": 3}, "SkgoyRG6KH": {"type": "review", "replyto": "H1gCeyHFDS", "review": "Post-rebuttal: I've read author's response and other reviews. As pointed out by other reviewers, the proposed algorithm is restricted to single-output regression and the claim \"accelerate convergence without much computational overhead\" might not be true in general multi-output regression tasks. I believe the lack of multi-output regression experiments makes the paper a bit weak, therefore I changed my score to 3 and vote for rejection.\n\nThat being said, I do find the algorithm interesting and the theoretical results impressive. I encourage the authors to include experiments on multi-output regression tasks (or tone down the claim about computational overhead) and resubmit the paper.\n\n------------------------------------------------------------------------------------------------------------------------------------------------------------------\nBased on recent progress on the connection between neural network training and kernel regression of neural tangent kernel, this paper proposes a Gram-Gauss-Newton (GGN) algorithm to train deep neural networks for regression problems with square loss. For overparameterized shallow networks, the authors proved global convergence of the proposed algorithm in both full-batch and mini-batch setting. To my knowledge, the proof of global convergence in the mini-batch setting is novel and might be of independent interest for other work.\n\nOverall, this paper is well-written and easy to follow. It's interesting to see that the proposed algorithm can achieve quadratic convergence while most previous papers only get linear convergence. \nGiven that, I'd like to give a score of 6 and I'm willing to increase my score if the authors can resolve my concerns below.\n\nConcerns:\n- For the algorithm, if I understand correctly, it's actually same as natural gradient descent with generalized inverse. I think the authors should make the connection clear. I would like to see more discussions with natural gradient descent or Newton methods in the next revision.\n- The authors claim that the proposed GGN algorithm only has minor computational overhead compared to first-order methods. I doubt if it's true in general. In section 3.3, the authors argue that computing individual Jacobian matrices for every example in the minibatch has roughly the same computation as the backpropagation in SGD. As far as I know, it's not true in practice. In addition, the inverse of the Gram matrix can also be expensive when the output dimension (the dimension of y) is large.\n\nMinor Comments:\n- In the paper, the theoretical results are based on the assumption of smooth activation function. I wonder if it is possible to include the case of ReLU activation as it's the most popular activation function in deep learning.\n- I don't have a good understanding about why mini-batch version would converge after reading the paper. To me, second-order methods with mini-batch estimation of the preconditioner would lead to biased gradient estimation. Could you comment on that?", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 3}, "rkgi-irhiB": {"type": "rebuttal", "replyto": "SkgoyRG6KH", "comment": "Thank you for your valuable comments. We have addressed the most common issues in the general response above. Here we answer the additional questions raised by the reviewer.\n\n--Activation functions. We believe it is possible to change our proof to ReLU activations based on the techniques in [1].\n\n--Proof techniques of mini-batch GGN. As mentioned in Section 1, though conventional wisdom may suggest that applying mini-batch scheme to second-order methods will introduce a biased estimation of the accelerated gradient direction, we can prove that mini-batch GGN converges on overparametrized networks. Our proof only entails the decrease of the loss after performing a whole cycle of updates. This is significantly different from the former techniques used to prove the convergence of SGD, which uses a small learning rate to force the decrease of expected loss at each step.\n\n[1] Gradient descent provably optimizes over-parameterized neural networks, Du et al.", "title": "Response"}, "SkxosqS2sB": {"type": "rebuttal", "replyto": "rygJfehGqB", "comment": "Thank you for your valuable comments. We have addressed the most common issues in the general response above. Here we answer the additional questions raised by the reviewer.\n\n--The RL paper. Thanks for a good reference. The independent work on reinforcement learning aims to precondition the Q-learning update rule with linear approximation, so similar to natural gradient analyzed in [1] , there is still a learning rate term $\\alpha$ in the algorithm. However, our method is motivated by solving NTK regression, which does not introduce the step size term (or can be understood as suggesting the learning rate to be 1 as mentioned in the related work section). We added the reference to related work section in the revision.\n\n--Convergence result considering a large learning rate. Thanks for pointing out the misleading expression on large learning rate. As a second-order method, GGN does a Newton-type update without learning rate. Thus, unlike the papers mentioned by the reviewer which need to bound the learning rate by a quantity related to the smoothness to ensure a similar behavior as gradient descent, we show that mini-batch GGN can converge without forcing a specific small step size, which is totally different from the convergence of gradient descent. We modified the expression in the revision.\n\n[1] Fast convergence of natural gradient descent for overparameterized neural networks, Zhang et al.", "title": "Response"}, "BkgxE9rhoH": {"type": "rebuttal", "replyto": "rJxaGCqN9S", "comment": "Thank you for your valuable comments. Most of the issues are addressed in the general response above. ", "title": "Response"}, "B1gDZ5B3jS": {"type": "rebuttal", "replyto": "BJeshEY55r", "comment": "Thank you for your valuable comments. Most of the issues are addressed in the general response above. ", "title": "Response"}, "Bkew6KBhsr": {"type": "rebuttal", "replyto": "BJxhNaz2qH", "comment": "Thank you for your valuable comments. Most of the issues are addressed in the general response above. ", "title": "Response"}, "HJxKytr2oS": {"type": "rebuttal", "replyto": "H1gCeyHFDS", "comment": "We thank all the reviewers for the valuable comments. We address the major issues here which are mentioned multiple times by the reviewers.\n\n--Novelty of the algorithm. The equivalence of natural gradient descent and Gauss-Newton algorithm has been well studied. However, exact solving wasn\u2019t tractable before and people had to rely on approximation methods like K-FAC. We believe that the novelty of GGN lies in the specific implementation of an exact solution by the Gram matrix over a mini-batch scheme, which can be practically useful. \n\n--Limitations of the algorithm. We acknowledge that the current GGN algorithm is still limited to the single-output regression problem. Then again, regression is a fundamental problem in machine learning and already has a large number of application scenarios. As for classification and multivariable regression tasks, as discussed in section 5, the direct application of GGN requires a linear scaling of the size of Jacobian w.r.t. the number of classes. There are possible ways to address this issue, like making some modifications of the network output. This is an important future work, and we are already doing experiments on classification tasks like CIFAR and Imagenet.  \n\n--Computational complexity and implementation. Though modern frameworks like PyTorch and TensorFlow don't give an easy way to compute per-example derivatives efficiently, we re-implement the backpropagation process for different type of layers, e.g. convolutional layers, linear layers which makes the computation of Jacobian efficient. We note that a concurrent work [https://openreview.net/forum?id=BJlrF24twB ](Sec 2.2) gives some examples of efficient implementation. We\u2019re still working on making the code cleaner and will release the code if the paper is accepted. \n\n--Experimental results. As requested by the reviewers, we have added the comparison with Adam and K-FAC, as well as the generalization result of AFAD-LITE, in Appendix D.\n\nIn general, our paper aims to propose an algorithm that makes use of second-order information to accelerate convergence without much computational overhead, and both the theoretical and experimental results demonstrate its effectiveness. We agree with the reviewer that we should do more experiments that scale and generalize to different tasks in order to demonstrate the full potential of GGN, and we are still working hard on it.\n", "title": "Response to all reviewers"}, "rygJfehGqB": {"type": "review", "replyto": "H1gCeyHFDS", "review": "The paper presents a second order optimization algorithm, along with convergence proof of the algorithm in both batch and minibatch setting. The effectiveness of the method is demonstrated on two regression tasks. My overall assessment is that the method is still quite limited and the method itself is not novel, but I am willing to change my score to accept if my concerns have been addressed.\n\n(1) The method is not novel. The same algorithm was proposed and applied to the RL setting [1]. \n(2) The method is still quite limited to 1-output function scenario, where the NTK matrix is easy to compute. This limitation though is not mentioned in the paper. I hope the author should have a discussion on this and admit this limitation.\n(3) Also due to (2), the experiments shown in the paper are on toy data and hence lack of strong empirical support.\n(4) The method doesn't scale up to large batch size.\n(5) In the theoretical section, the paper states \n\"However, to our knowledge, no convergence result considering large learning rate (e.g. has the same scale with the\nupdate of GGN) has been proposed.\"\nThis is not true. Here are some papers: [2,3,4]\n(6) Lack of some second order optimization baselines, e.g., KFAC.\n\nMisc:\n(1) For section 3.3, first of all, (B) costs at least half of (A) as it requires a backward pass. \n(2) For section 3.3, the authors write:\n\" What is different is that GGN also, for every input data, keeps track of the output\u2019s derivative for the parameters; while in\nSGD the derivatives for the parameters are averaged over a batch of data.\"\nIs there a simple way of implementing/computing the gradient for *every* input data on GPU? How is that compared to computing the average? I wish to see more evidence of showing they're the same as authors claimed.\n\n[1] Towards Characterizing Divergence in Deep Q-Learning.\n[2] The Power of Interpolation: Understanding the Effectiveness of SGD in Modern Over-parametrized Learning.\n[3] Fast and Faster Convergence of SGD for Over-Parameterized Models (and an Accelerated Perceptron).\n[4] Fast Convergence of Stochastic Gradient Descent under a Strong Growth Condition", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 4}, "BJxhNaz2qH": {"type": "review", "replyto": "H1gCeyHFDS", "review": "Summary: The authors propose the Gram-Gauss-Newton method for training neural networks. Their method draws inspiration from the connection between the neural network optimization and kernel regression of neural tangent kernel. \n\nTheir method is described in Algorithm 1, but to summarize it, they use the Gauss-Newton method to train neural networks, and prove quadratic convergence for the full-batch training. They also have a mini-batch version of GGN, the practical version, and this is proven to have linear convergence.\n\nThe authors also provide experiments that includes the usual loss v. epoch, but also loss v. wallclock time (which is nice when proposing second-order-like methods where extra computations are necessary), and a test error v. epoch (which is again nice for second-order methods as explained below).\n\nStrengths: The paper has nice proofs of the theorems, and they show a method with quadratic convergence (but full-batch training) without having to invert the full Jacobian matrix whose size depends on the number of parameters, but rather inverting the Gram matrix, whose size depends on the number of training data.\n\nDue to the seeming extra computational cost of the method, (the method requires computing the full Jacobian matrix which depends on the number of neural network weights) I am grateful that they provided comparisons with wallclock time to SGD.\n\nAnd there is this notion that second-order methods have been shown to not generalize as well as first-order methods, and thus it was nice to see that they had an experiment where they tested generalization.\n\nThe background information was also nice to read.\n\nWeaknesses: They do not compare it with other methods optimization methods, such as Adam (a first-order method) or natural gradient (a second-order method), and I would have thus liked to have seen comparisons to these.\n\nI would have also liked to see a test loss v. time/epoch for the AFAD-LITE task as well (they only have it for the RSNA Bone Age task), at least provided in the appendix if there was not enough space.\n\nIn the references, there are numerous citations of the arXiv versions of papers, but I suggest the authors replace them with the conference/journal versions if those papers were accepted in conferences/journals (and I spot some that were).\n\nOther comments: (i) In the first sentence of 3.3., I think one should replace \u201cGGN has quadratic convergence rate\u201d with \u201cfull-batch GGN has quadratic convergence rate,\u201d as in the subsequent sections you are discussing mini-batch GGN.", "title": "Official Blind Review #5", "rating": "6: Weak Accept", "confidence": 2}, "rJxaGCqN9S": {"type": "review", "replyto": "H1gCeyHFDS", "review": "Authors propose minimizing neural network using kernel ridge regression. (Formula 9 and Algorithm 1). Main difference of this method is compared to Gauss-Newton, is that it uses JJ' as curvature, which has dimensions b-by-by (batch size b), instead of J'J as curvature, which has dimensions m-by-m (number of parameters m).\n\nWhen b is much smaller than m, this matrix is tractable to represent exactly. Related approach is taken by KKT (see Figure 1 of https://arxiv.org/pdf/1806.02958.pdf) which also replaces J'J with more tractable JJ'.\n\nThere is a long history of authors trying to extend second order methods to deep learning and and finding that curvature estimated on a small batch is extremely noisy, requiring large batches (see papers by Nocedal's group). Authors propose a method that estimates curvature from small batches. Given the history of failures in small-batch curvature estimation, the bar is high to show that small-batch curvature estimation works.\n\nBulk of the paper is dedicated to theoretical convergence and connections between concepts. Since the focus of the paper is on a new optimization method for deep learnning, I feel like convergence proofs can be moved to Appendix, and more of the paper should focus on practical aspects of the method. Also the connections to other concepts (ie, tangent kernel) are not essential to the paper and could be better left over for a tutorial paper.\n\nI'm not convinced that their method works well enough to have practical impact.\n\n- Their method seems to be limited to neural network with one output (ie, univariate regression task). This is a serious limitation and paper should highlight this more on this, given that vast majority of applications and benchmarks involve more than output variable.\n\n- Practical implementation details are skimmed over. Section 3.3 brings up that to compute Jacobian, one needs to keep track of the output derivative on per-example basis. How is this accomplished? Modern frameworks like PyTorch and TensorFlow don't give an easy way to compute per-example derivatives efficiently.\n\n- Experiments are performed on two tasks that are not well known in the literature. The choice is somewhat understandable given that their method performs for univariate regression, but also this makes it hard to evaluate whether the method works. SGD vs Gram-Gauss evaluation use parameter settings which are not comparable, so it's impossible to tell whether the improvement are due to better choice of hyper-parameters.\n\n\nThe changes needed to make this paper acceptable are extensive, and I would recommed a reject.\n\nI would recommend authors attempt the following changes for future submission:\n\n1. Make it work for multivariate regression. There's a conversion technique to represent multivariate regression in the same form as univariate regression (see Section 2.4 of \"Rao, Toutenberg\" Linear Models). Essentially it comes down concatenating o output Jacobians (o output classes) along the batch dimension.\n\n2. Use this to evaluate the method on standard benchmarks like MNIST and CIFAR and show that it doesn't cause a significant worsening in quality. Given that similar approach (KKT paper) found bigger improvement on RNN task, an RNN task may be useful.\n\n3. Give more details on implementation. How was Jacobian calculation implemented? Which framework? How was the per-example computation made tractable? Making small-scale experiments reproducible through anonymous github submission would also help ", "title": "Official Blind Review #1", "rating": "1: Reject", "confidence": 3}}}