{"paper": {"title": "Adversarial Machine Learning at Scale", "authors": ["Alexey Kurakin", "Ian J. Goodfellow", "Samy Bengio"], "authorids": ["kurakin@google.com", "ian@openai.com", "bengio@google.com"], "summary": "", "abstract": "Adversarial examples are malicious inputs designed to fool machine learning models.\nThey often transfer from one model to another, allowing attackers to mount black\nbox attacks without knowledge of the target model's parameters.\nAdversarial training is the process of explicitly training a model on adversarial\nexamples, in order to make it more robust to attack or to reduce its test error\non clean inputs.\nSo far, adversarial training has primarily been applied to small problems.\nIn this research, we apply adversarial training to ImageNet.\nOur contributions include:\n(1) recommendations for how to succesfully scale adversarial training to large models and datasets,\n(2) the observation that adversarial training confers robustness to single-step attack methods,\n(3) the finding that multi-step attack methods are somewhat less transferable than single-step attack\n      methods, so single-step attacks are the best for mounting black-box attacks,\n      and\n(4) resolution of a ``label leaking'' effect that causes adversarially trained models to perform\n      better on adversarial examples than on clean examples, because the adversarial\n      example construction process uses the true label and the model can learn to\n      exploit regularities in the construction process.\n", "keywords": ["Computer vision", "Supervised Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper is an empirical study of adversarial training in a large-scale regime. \n Its main contributions are to demonstrate that Imagenet models can be made robust to adversarial examples using relatively efficient constructions (so-called 'one-step' methods). Along the way, the authors also report the 'label leaking' effect, a flaw in previous adversarial constructions that limits the effectiveness of adversarial examples at regularizing training. \n \n The reviewers were consensual that this contribution is useful to the field, and the label leaking phenomena is an important aspect that this paper helps address and mitigate. The authors responded promptly to reviewers questions. Despite limited novelty and lack of quantitative analysis, I recommend accept as a poster."}, "review": {"r1TyL13Lx": {"type": "rebuttal", "replyto": "Sky6A6QEg", "comment": "Thanks for the review.\n\nAs you suggested we added appendix E, which contains experiments of training with various values of fractions of clean/adversarial examples, including experiments with no clean examples.\nAs could be seen from these results, increasing the fraction of adversarial examples in the minibatch leads to a decrease of accuracy on clean examples and to an increase of accuracy on adversarial examples. Having more than half of adversarial examples in the minibatch (corresponding to k=16 in our case) leads to almost no increase of accuracy on adversarial examples, at the same time it could lead to up to 1% decrease of accuracy on clean examples. This was the reason we chose k=16 for the main experiments done in the paper.\n\nRegarding part [2], do you have specific things in mind where we should add better theoretical explanation?", "title": "Reply to the review"}, "SkjXLJ38x": {"type": "rebuttal", "replyto": "HyX7I5VNg", "comment": "Thanks for the review.\n\nWe would like to add that while sections 4.3 and 4.4 are not specific to large-scale dataset, all experiments in these sections were done on a large scale dataset. Thus these results are valid for large scale datasets as well. In other words these results do cover the case of at least one large scale datasets.\n\nAlso as a way to address cons we could consider changing title of the paper to limit its scope.\nPossible new titles which we though of:\na) \u201cAdversarial machine learning at scale: a case study\u201d\nb) \u201cAdversarial object recognition at scale\u201d\nc) \u201cAn example of adversarial machine learning at scale\u201d", "title": "Reply to the review"}, "rJJzUknLx": {"type": "rebuttal", "replyto": "HyD8pKrNl", "comment": "Thank for the review.", "title": "Reply"}, "rJ6UTI0Qg": {"type": "rebuttal", "replyto": "Byi8XV0ze", "comment": "We added a citation to \u201cLearning with a Strong Adversary\u201d and discussed a comparison of the findings in the paper.\n\n\u201cThe key contribution of scalable adversary training is \u2018examples to be grouped into batches containing both normal and adversary examples\u2019. \u201c\n\tRecommendation to use both clean and adversarial examples in the minibatch was one of the observations made in our experiments. However there are other contributions of the paper (as stated in Sec 1), including successful adversarial training on ImageNet dataset,  the discovery of the \u201clabel leaking\u201d effect, empirical study of transferability and importance of model capacity.\n\nRegarding training with only adversarial examples:\n\tThis strategy corresponds to the method described in \u201cExplaining and Harnessing Adversarial Examples\u201d, Goodfellow et al, 2014, with alpha set to 0 rather than 0.5. It is not a completely distinct training procedure. When writing \u201cExplaining and Harnessing Adversarial Examples\u201d, Ian introduced the alpha parameter after finding that the alpha=0 variant did not perform acceptably. \u201cImproving back-propagation by adding an adversarial gradient\u201d (N\u00f8kland 2015) advocated setting alpha to 0 before \u201cLearning with a Strong Adversary\u201d did so.\n\tIn the present work, training on only adversarial examples corresponds to k=32 (given that minibatch size of 32). For our experiments on ImageNet we have done hyperparameter search to find optimal value of k. We observed that increase of k lead to decrease of accuracy on clean examples and does not necessarily provide significant improvement of accuracy on adversarial examples. Overall for our experiments we have chosen k=16.\n\nNote that in \u201cLearning with a Strong Adversary,\u201d the VGG network is not applied to ImageNet, but rather to MNIST and CIFAR-10. Our most important contribution is experimentation at ImageNet scale.", "title": "Reply to reviewer's question"}, "B1HGaIAmg": {"type": "rebuttal", "replyto": "SJqB0tkQl", "comment": "> For the results in Table 3, what is the value of \\epsilon for the adversarial training?\n\nDuring adversarial training \\epsilon was chosen randomly for each example in the minibatch from truncated normal distribution (as described in the last paragraph of section 3). This includes results in Table 3.\n\n> For the results in Figure 2, when \\epsilon is fixed (say \\epsilon=8), have you compared how many successful adversarial examples are generated (and thus passed to the target network)? Will it be another factor of this transferability rate?\n\nTransfer rate is computed as \u201cnumber of examples fooling both source and target networks\u201d / \u201cnumber of examples fooling only source network\u201d.\nSo yes, number of successful adversarial examples for source network is indeed one of the factors affecting transferability rate.\nHowever conclusions of section 4.4 are valid if you take into account this observation.\n\nIn particular if you fix epsilon and compare only numerator of transfer rate for different methods then it will be higher for one step methods, lower for basic iterative method and the lowest for iterative least likely class.\nOverall numerator of transfer rate will behave similarly to error rate on adversarial images generated for one network and classified by another. These numbers are discussed in appendix C, table 6 and figure 5.\n\nIf your question is about change of epsilon with fixed method.\nIn such case both numerator and denominator of transfer rate will increase with increase of epsilon. However we observed that numerator (number of transferred examples) increasing much faster than denominator (number of examples which fooled source network).\nFor example when epsilon growth from 8 to 16, denominator increases by less than 1% while numerator increases by more than 20%.\nWe added this clarification to the paper.\n\n\n> Also, do we have an estimation of the variance of the estimation of the transferability rate? How does this variance depend on the number of the successful adversarial examples?\n\nWe estimated that 95% confidence interval for the transfer rate is within \u00b11.3 percent point of value reported in Table 4. 1.3% is worst case estimation, for many values in this table actual 95% confidence interval is even smaller.\n\n\n> In the paragraph below Table 3, what does the following sentence mean? \"The most labels are leaked wit \\epsilon=0.3 *****, the model leaks 79 labels on the test set\" How this number 79, is calculated?\n\nN_correct_adv_label - N_correct_adv_pred\nwhere N_correct_adv_label is the number of test examples that are\ncorrectly classified when perturbed using the true label to generate\nthe perturbation, and N_correct_adv_pred is the number of test\nexamples that are correctly classified when perturbed using the\nmodel's prediction of the label.\n\n\n> Shouldn't all the true label have been leaked, since they all have been used to construct adversarial examples.\n\nNo, every label had the opportunity to be leaked, but not all of them\nwere successfully leaked.\n\nWe updated the paper to clearly define the term \"leak.\" We say a\nlabel has been leaked if and only if the model classifies an\nadversarial example correctly when that adversarial example is\ngenerated using the true label but misclassifies a corresponding\nadversarial example that was created without using the label.\n\nWe think your question is based on a less restrictive definition of\n\"leak\", where a label is considered leaked if the perturbation is a\nfunction of the label.\n\nRecall that the model gets to observe only the adversarial example\nx_adv, not the original example x_clean, so the trivial way to recover\nthe label by enumerating all possible calls to\nmake_adversarial_example(x_clean, label) is not possible.\n\n\n> Lastly, for the observations in section 4.2, 4.3, and 4.4, are they particular for only large scale dataset, or similar phenomenons happen in small scale dataset?\n\nLabel leaking was observed on small dataset (MNIST) as well after careful re-examination of results (section 4.2).\n\nWe haven\u2019t specifically studied how \\epsilon affect transferability and how model capacity affect robustness to adversarial examples on small dataset. But there are no fundamental reason why it would be different.\n\nIn response to this question, we ran the MNIST tutorial for cleverhans 1.0 (the paper for 1.0 isn\u2019t available yet, but 0.1 is here: https://arxiv.org/abs/1610.00768) for 100 epochs with different depths of model.\n\nDefault depth, included with cleverhans:\nClean train, clean eval: 0.9922\nClean train, adv eval: 0.0503\nAdv train, clean eval: 0.9928\nAdv train, adv eval: 0.9347\n\nDouble depth:\nClean train, clean eval:  0.9915\nClean train, adv eval: 0.1754\nAdv train, clean eval: 0.9924\nAdv train, adv eval:  0.9683\n\nWe see that increasing the depth of the model increased the resistance to adversarial examples, whether or not adversarial training was used.", "title": "Reply to reviewer's question"}, "H1G63U0mg": {"type": "rebuttal", "replyto": "HyieU6kQx", "comment": "To clarify, FGSM could be used for adversarial *training* and it does provide robustness to adversarial examples. However we do not recommend to use FGSM with true labels *to evaluate the model* because it might be unclear whether an increase of accuracy is caused by label leaking or due to actual increase in robustness.\n\n> Could you comment on how robust models trained on step l.l. (or another comparable method) adversarial examples are to FGSM examples?\n\nWe discovered that it\u2019s enough to train model on \u201cstep l.l.\u201d method to become robust to FGSM adversarial examples. Quantitative results are provided in Appendix A, fig. 3.\n\n>  Do you have any other proposals for how to deal with making models robust to these kinds of examples?\n\nYes, we briefly discussed in the paper the following ways to increase robustness to adversarial examples:\n1. As discussed in section 4.3., a dramatic increase of model capacity, far beyond the model sizes currently in use, might lead to increased robustness to adversarial examples.\n2. As shown in appendix D, certain non-linear activation function (so called ReluDecay) could increase robustness to various types of adversarial examples.\nHowever these two methods do not yet provide full robustness to adversarial examples. Also, they work better in conjunction with adversarial training.\n", "title": "Reply to the reviewer's question"}, "HyieU6kQx": {"type": "review", "replyto": "BJm4T4Kgx", "review": "Great work! Just one question: \n\nTable 4 shows that FGSM produces examples that are most transferable to other models, implying they are the examples mostly likely to be used in an 'attack'. However, you mention that adversarial training on FGSM examples does not work due to the label leaking effect. Could you comment on how robust models trained on step l.l. (or another comparable method) adversarial examples are to FGSM examples? Do you have any other proposals for how to deal with making models robust to these kinds of examples?This paper has two main contributions: \n(1) Applying adversarial training to imagenet, a larger dataset than previously considered \n(2) Comparing different adversarial training approaches, focusing importantly on the transferability of different methods. The authors also uncover and explain the label leaking effect which is an important contribution.\n\nThis paper is clear, well written and does a good job of assessing and comparing adversarial training methods and understanding their relation to one another. A wide range of empirical results are shown which helps elucidate the adversarial training procedure. This paper makes an important contribution towards understand adversarial training and believe ICLR is an appropriate venue for this work.", "title": "pre-review question about transferability ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HyD8pKrNl": {"type": "review", "replyto": "BJm4T4Kgx", "review": "Great work! Just one question: \n\nTable 4 shows that FGSM produces examples that are most transferable to other models, implying they are the examples mostly likely to be used in an 'attack'. However, you mention that adversarial training on FGSM examples does not work due to the label leaking effect. Could you comment on how robust models trained on step l.l. (or another comparable method) adversarial examples are to FGSM examples? Do you have any other proposals for how to deal with making models robust to these kinds of examples?This paper has two main contributions: \n(1) Applying adversarial training to imagenet, a larger dataset than previously considered \n(2) Comparing different adversarial training approaches, focusing importantly on the transferability of different methods. The authors also uncover and explain the label leaking effect which is an important contribution.\n\nThis paper is clear, well written and does a good job of assessing and comparing adversarial training methods and understanding their relation to one another. A wide range of empirical results are shown which helps elucidate the adversarial training procedure. This paper makes an important contribution towards understand adversarial training and believe ICLR is an appropriate venue for this work.", "title": "pre-review question about transferability ", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJqB0tkQl": {"type": "review", "replyto": "BJm4T4Kgx", "review": "Hi,\n\nI have two questions about the experiments.\n1. For the results in Table 3, what is the value of \\epsilon for the adversarial training?\n2. For the results in Figure 2, when \\epsilon is fixed (say \\epsilon=8), have you compared how many successful adversarial examples are generated (and thus passed to the target network)? Will it be another factor of this transferability rate? Also, do we have an estimation of the variance of the estimation of the transferability rate? How does this variance depend on the number of the successful adversarial examples?\n\nAnother question about a statement.\nIn the paragraph below Table 3, what does the following sentence mean?\n\"The most labels are leaked wit \\epsilon=0.3 *****, the model leaks 79 labels on the test set\"\nHow this number 79, is calculated? Shouldn't all the true label have been leaked, since they all have been used to construct adversarial examples.\n\nLastly, for the observations in section 4.2, 4.3, and 4.4, are they particular for only large scale dataset, or similar phenomenons happen in small scale dataset?This paper investigate the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet. While the final conclusions are still vague, this paper raises several noteworthy finding from its experiments.\nThe paper is well written and easy to follow. Although I still have some concerns about the paper (see the comments below), this paper has good contributions and worth to publish.\n\nPros:\nFor the first time in the literature, this paper proposed the concept of \u2018label leaking\u2019. Although its effect only becomes significant when the dataset is large, it should be carefully handled in the future research works along this line.\nUsing the ratio of 'clean accuracy' over \u2018adversarial accuracy\u2019 as the measure of robust is more reasonable compared to the existing works in the literature. \n\nCons:\nAlthough the conclusions of the paper are based on the experiments on ImageNet, the title of the paper seems a little misleading. I consider Section 4 as the main contribution of the paper. Note that Section 4.3 and Section 4.4 are not specific to large-scale dataset, thus emphasizing the \u2018large-scale\u2019 in the title and in the introduction seems improper. \nBasically all the conclusions of the paper are made based on observing the experimental results. Further tests should have been performed to verify these hypotheses. Without that, the conclusions of the paper seems rushy. For example, one dataset of imageNet can not infer the conclusions for all large-scale datasets. ", "title": "Label leaking, and the transferability of the adversarial examples", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyX7I5VNg": {"type": "review", "replyto": "BJm4T4Kgx", "review": "Hi,\n\nI have two questions about the experiments.\n1. For the results in Table 3, what is the value of \\epsilon for the adversarial training?\n2. For the results in Figure 2, when \\epsilon is fixed (say \\epsilon=8), have you compared how many successful adversarial examples are generated (and thus passed to the target network)? Will it be another factor of this transferability rate? Also, do we have an estimation of the variance of the estimation of the transferability rate? How does this variance depend on the number of the successful adversarial examples?\n\nAnother question about a statement.\nIn the paragraph below Table 3, what does the following sentence mean?\n\"The most labels are leaked wit \\epsilon=0.3 *****, the model leaks 79 labels on the test set\"\nHow this number 79, is calculated? Shouldn't all the true label have been leaked, since they all have been used to construct adversarial examples.\n\nLastly, for the observations in section 4.2, 4.3, and 4.4, are they particular for only large scale dataset, or similar phenomenons happen in small scale dataset?This paper investigate the phenomenon of the adversarial examples and the adversarial training on the dataset of ImageNet. While the final conclusions are still vague, this paper raises several noteworthy finding from its experiments.\nThe paper is well written and easy to follow. Although I still have some concerns about the paper (see the comments below), this paper has good contributions and worth to publish.\n\nPros:\nFor the first time in the literature, this paper proposed the concept of \u2018label leaking\u2019. Although its effect only becomes significant when the dataset is large, it should be carefully handled in the future research works along this line.\nUsing the ratio of 'clean accuracy' over \u2018adversarial accuracy\u2019 as the measure of robust is more reasonable compared to the existing works in the literature. \n\nCons:\nAlthough the conclusions of the paper are based on the experiments on ImageNet, the title of the paper seems a little misleading. I consider Section 4 as the main contribution of the paper. Note that Section 4.3 and Section 4.4 are not specific to large-scale dataset, thus emphasizing the \u2018large-scale\u2019 in the title and in the introduction seems improper. \nBasically all the conclusions of the paper are made based on observing the experimental results. Further tests should have been performed to verify these hypotheses. Without that, the conclusions of the paper seems rushy. For example, one dataset of imageNet can not infer the conclusions for all large-scale datasets. ", "title": "Label leaking, and the transferability of the adversarial examples", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Byi8XV0ze": {"type": "review", "replyto": "BJm4T4Kgx", "review": "The key contribution of scalable adversary training is \"examples to be grouped into batches containing both normal and adversary examples\". \n\nCould you explain the why it is important? \n\nAlso I suggest to compare with other adversary training algorithm, such as in [1], training without clear sample. In experiment of [1], with VGG network, similar regularization effect was found, and they provided proof for linear model.\n\n[1] Huang, Ruitong, et al. \"Learning with a strong adversary.\" CoRR, abs/1511.03034 (2015).This paper is a well written paper. This paper can be divided into 2 parts:\n1.Adversary training on ImageNet \n2.Empirical study of label leak, single/multiple step attack, transferability and importance of model capacity\n\nFor part [1], I don\u2019t think training without clean example will not make reasonable ImageNet level model. Ian\u2019s experiment in \u201cExplaining and Harnessing Adversarial Examples\u201d didn't use BatchNorm, which may be important for training large scale model. This part looks like an extension to Ian\u2019s work with Inception-V3 model. I suggest to add an experiment of training without clean samples.\n\nFor part [2], The experiments cover most variables in adversary training, yet lack technical depth.  The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM, if we do careful observe on some special MNIST FGSM example, we can find augmentation effect on numbers, which makes grey part on image to make the number look more like the other numbers. Although this effect is hard to be observed with complex data such as CIFAR-10 or ImageNet, they may be related to the authors' observation \"FGSM examples are most transferable\".  \n\nIn this part the authors raise many interesting problems or guess, but lack theoretical explanations. \n\nOverall I think these empirical observations are useful for future work. \n\n", "title": "Prereview question about scalable adversary training", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Sky6A6QEg": {"type": "review", "replyto": "BJm4T4Kgx", "review": "The key contribution of scalable adversary training is \"examples to be grouped into batches containing both normal and adversary examples\". \n\nCould you explain the why it is important? \n\nAlso I suggest to compare with other adversary training algorithm, such as in [1], training without clear sample. In experiment of [1], with VGG network, similar regularization effect was found, and they provided proof for linear model.\n\n[1] Huang, Ruitong, et al. \"Learning with a strong adversary.\" CoRR, abs/1511.03034 (2015).This paper is a well written paper. This paper can be divided into 2 parts:\n1.Adversary training on ImageNet \n2.Empirical study of label leak, single/multiple step attack, transferability and importance of model capacity\n\nFor part [1], I don\u2019t think training without clean example will not make reasonable ImageNet level model. Ian\u2019s experiment in \u201cExplaining and Harnessing Adversarial Examples\u201d didn't use BatchNorm, which may be important for training large scale model. This part looks like an extension to Ian\u2019s work with Inception-V3 model. I suggest to add an experiment of training without clean samples.\n\nFor part [2], The experiments cover most variables in adversary training, yet lack technical depth.  The depth, model capacity experiments can be explained by regularizer effect of adv training;  Label leaking is novel; In transferability experiment with FGSM, if we do careful observe on some special MNIST FGSM example, we can find augmentation effect on numbers, which makes grey part on image to make the number look more like the other numbers. Although this effect is hard to be observed with complex data such as CIFAR-10 or ImageNet, they may be related to the authors' observation \"FGSM examples are most transferable\".  \n\nIn this part the authors raise many interesting problems or guess, but lack theoretical explanations. \n\nOverall I think these empirical observations are useful for future work. \n\n", "title": "Prereview question about scalable adversary training", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}