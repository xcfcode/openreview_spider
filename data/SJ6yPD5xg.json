{"paper": {"title": "Reinforcement Learning with Unsupervised Auxiliary Tasks", "authors": ["Max Jaderberg", "Volodymyr Mnih", "Wojciech Marian Czarnecki", "Tom Schaul", "Joel Z Leibo", "David Silver", "Koray Kavukcuoglu"], "authorids": ["jaderberg@google.com", "vmnih@google.com", "lejlot@google.com", "schaul@google.com", "jzl@google.com", "davidsilver@google.com", "korayk@google.com"], "summary": "", "abstract": "Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880\\% expert human performance, and a challenging suite of first-person, three-dimensional \\emph{Labyrinth} tasks leading to a mean speedup in learning of 10$\\times$ and averaging 87\\% expert human performance on Labyrinth.", "keywords": []}, "meta": {"decision": "Accept (Oral)", "comment": "There was broad consensus by the reviewers that this paper should be accepted. There was also a good deal of discussion about detailed aspects of the paper. I think the directions in which this paper points are going to be of interest to many in the community. As one reviewer put is, the idea here seems to involve taking advantage of the \"possibility\" of control that an agent may have over the environment. This is formulated in terms of auxiliary control and auxiliary prediction tasks, which share an underlying CNN and LSTM representation.\n \n The revision posted by the author's addresses a number of the questions, suggestions and concerns of reviewers."}, "review": {"HkH2DooIg": {"type": "rebuttal", "replyto": "SJ6yPD5xg", "comment": "We thank the reviewers and everyone else who provided comments for their feedback. The most recent revision of the paper incorporates a number of the suggestions and aims to address the most serious concerns.\n\nAnonReviewer3\n\n- Can authors comment about the computational resources needed to train the UNREAL agent?\n\nPlease see the answer to a previous question below.\n\n- The overall architecture is quite complicated. Are the authors willing to release the source code for their model?\n\nThe UNREAL architecture was quite straightforward to implement. We will add pseudocode to the appendix of the camera ready version to clarify how the model works. We don\u2019t currently have plans to release the code but we\u2019re happy to assist any open source reimplementation effort.\n\nAnonReviewer4\n\n- I believe you should say something about the extra computational cost of optimizing these auxiliary tasks. How much do you lose in terms of training speed? Which are the most costly components?\n\nPlease see the answer to a previous question below. The wall clock speedup of UNREAL vs A3C is currently about 8x.\n\n- If possible, please try to make it clearer in the abstract / intro that the agent is learning different policies for each task. When I read in the abstract that the agent \"also maximises many other pseudo-reward functions simultaneously by reinforcement learning\", my first understanding was that it learned a single policy to optimize all rewards together, and I realized my mistake only when reaching eq. 1.\n\nThanks for pointing this out. We\u2019ve tried to clarify this in the abstract.\n\n- The \"feature control\" idea is not validated empirically (the preliminary experiment in Fig. 5 is far from convincing as it only seems to help slightly initially). I like that idea but I am worried by the fact the task is changing during learning, since the extracted features are being modified. There might be stability / convergence issues at play here.\n\nThat is a fair point. We have updated the paper with improved results for feature control. We used a target network to make the features being controlled change less frequently during training. Feature control now works roughly as well as pixel control.\n\n- Since as you mentioned, \"the performance of our agents is still steadily improving\", why not keep them going to see how far they go? (at least the best ones)\n\nWe haven\u2019t done this because we were following the training protocol for A3C. It would be interesting to see where the scores saturate.\n\n- Why aren't the auxiliary tasks weight parameters (the lambda_*) hyperparameters to optimize? Were there any experiments to validate that using 1 was a good choice?\n\nWe do include the lambda for pixel control in the hyperparameter search. The parameter ranges are given in the Appendix.\n\n- Please mention the fact that auxiliary tasks are not trained with \"true\" Q-Learning since they are trained off-policy with more than one step of empirical rewards (as discussed in the OpenReview comments)\n\nWe do say that they are trained with n-step Q-learning. Do you mean that it is not a true off-policy learning method?\n\nAnonReviewer5\n\n- I think one weakness of the paper (or rather, considering the number of things that can fit in a paper, crucially needed future work) is that there is very little experimental analysis of the effect of the auxiliary tasks appart from their (very strong) effect on performance. In the same vein, pixel/feature control seems to have the most impact, in Labyrinth just A3C+PC beats anything else (except UNREAL), I think it would have been worth looking at this, either in isolation or in more depth, measuring more than just performance on RL tasks.\n\nWe have noticed some qualitative differences between the learned policies of the UNREAL and A3C agents. The policies learned by UNREAL tend to \u201cpay attention\u201d to where they are going, for example they look straight ahead when moving forward while agents trained with A3C sometimes look in less informative directions (e.g. looking at the sky). They also seem to lead to agents which prefer to stay in the middle of corridors, bump much less frequently into walls.", "title": "Response to reviewers"}, "Skqzhj5rl": {"type": "rebuttal", "replyto": "Sklbud2me", "comment": "Plots provided already average over multiple episodes, thus take into account stochasticity of the environment. The hyperparameter search is not part of the algorithm. As we mentioned, we verified that rerunning the same hyperparameters with multiple seeds leads to essentially the same performance. We added an appendix section showing this. In particular, we include the plot showing correlation between hyperparameters and performance (which would not be there if seeding significantly affects performance).", "title": "Authors' response"}, "BkTNoj9Sg": {"type": "rebuttal", "replyto": "HJhbCfyEx", "comment": " - in feature control, why not also learn to maximize change? Or vice-versa, in pixel control why not learn to maximally activate a pixel? I can think of my own reasons but I am curious as to your reasoning. (by maximizing value instead of maximizing change, you need to learn to control the direction as well, which requires more knowledge, so intuitively maximizing value seems better)\n\nWe did experiment with controlling feature values and controlling feature changes in feature control, but we did not see a big difference between the two. We could include both results for consistency. We did not experiment with maximally activating pixels in pixel control. It would be interesting to try doing this on the RGB channels but we haven\u2019t experimented with that.\n\n- Is the method 10x faster on average for top-3 experiments or across hyperparameters? (sec 4.1.1)\n\n10x speed up refers to top3 experiments. The average speedup across all hyperparameters is around 12x. We also added a plot to the appendix C to show results for this setting.\n\n- One can guess (or maybe I missed it), but it's not explicitly said whether UNREAL uses Feature Control or only Pixel Control (the loss name L_PC suggests so). If not, I can see from figure 5 the Feature Control doesn't seem to have that much effect, is that why you didn't include it in UNREAL?\n\nThat's true, UNREAL currently uses Pixel Control and no Feature Control. We decided not to put it into the main agent at this point since it is an ongoing research direction, with more preliminary results (as shown on Figure 5).\n\n- section 4.2 references figure 7 but it seems like it should be figure 6\n\nThanks for pointing this out, it is now corrected.", "title": "Response"}, "ByUrQsqrl": {"type": "rebuttal", "replyto": "SJ6yPD5xg", "comment": "Dear authors,\n\ncongratulations on this very interesting paper; using auxiliary tasks is a great idea that opens a lot of future research.\n\nHowever I'm having troubles understanding the exact objective of the UNREAL agent. In 3.4, you state that the agent optimises \"a single combined loss function\" (equation (2)) but that \"in practise, the loss is broken down into separate components\". So when exactly is which part optimised? \n\nFrom how I understand the paper now, it seems that in practise, the individual loss functions on the right-hand side of equation (2) are updated separately, i.e., during training, you switch between loss functions and this way optimise them \"simultaneously in parallel\" (p.4). Do I understand this correctly? Because then I don't understand how the lambdas come into play; in what way do they influence the learning (or evaluation) of the agent? Do you ever actually look at L_UNREAL, or evaluate how the agent performs on the auxiliary tasks? In figure 3, you're evaluating \"performance\", so I guess that's the score received while playing the game (which indirectly corresponds to L_A3C for the UNREAL agent)?", "title": "Question about loss functions"}, "S1pD9uh7e": {"type": "rebuttal", "replyto": "rk1bpXizl", "comment": "Am I correct in understanding that you compute Q-function gradients for the pixel control tasks using off-policy, n-step roll-outs drawn from a replay buffer which is filled by (previous versions of) the policy for the primary task?\n\nIf so, it's interesting that divergence between the sampling distribution for the MC (i.e. non-critic-estimated) rewards and the distributions induced by the pixel control policies is not an issue. I can see why divergence between the behaviour policy and estimation policy wouldn't be a problem for async 20-step Q-learning as in the A3C paper, i.e. the divergence would generally be small and due only to staleness of the thread-local parameters. In your current setting though, it seems like this divergence could be significant. Do you have any hypotheses about why the behaviour/estimation policy divergence might not be a problem in your setting?", "title": "further clarification"}, "Sklbud2me": {"type": "rebuttal", "replyto": "ryGh-vnQl", "comment": "Perhaps then it would make the paper stronger to instead/in addition show the results averaged over multiple random seeds for the best hyperparameters, instead of the 3 best runs from the hyperparameter search (which were with the same seed? or not?)? Basically, the question this raises is whether the hyperparameter search itself is part of the algorithm. Furthermore, by reporting only top-3, isn't the evaluation confounding the effect of random seed from the effect of hyperparameters? In a standard evaluation, you get to choose the hyperparameter, but not the random seed, especially in stochastic environments.", "title": "Evaluation metric"}, "Hy-ZGDnQe": {"type": "rebuttal", "replyto": "rywtZonfe", "comment": "Thank you for your comments. To answer your questions:\n\n- 10x less steps to train a model is great, but how much longer does each step take? Since you train on many things simultaneously, is there still a wall-time speedup?\n\nAn agent-environment step with UNREAL compared to A3C is 1.2 times slower in terms of wall-time with our unoptimised research code. The resulting wall-time speedup in learning is over 8x. We should stress that in many applications that we are interested in, it is the number of environment steps that is important to reduce, as the environment is the wall-time bottleneck.\n\n- It seems like feature control is almost the same thing as pixel control, but from the \"viewpoint\" of the \"hidden state\". Did you try having stacked recurrent layers and doing this feature control at each layer? If not, do you think it would be beneficial? \n\nYes, we see feature control as the equivalent of doing pixel control on higher-level visual features. One benefit of controlling spatial feature maps (or pixels directly) is that it\u2019s possible to take advantage of spatial correlations by using deconvolutional output layers. We agree that controlling other layers of the network could also be beneficial.\n\n- In the total absence of external rewards, the base policy would not learn anything meaningful. Did you attempt to do on-policy learning of some variants of pixel or feature control? Maybe as some kind of pretraining phase?\n\nWe have thought about acting with the pixel/feature control policies but we\u2019re leaving this for future work.\n\n- You used n=20-step return, what \\gamma value accompanies this?\n\nGamma was set to 0.99 for the behaviour policy and 0.9 for the pixel control policies.", "title": "Authors' response"}, "ryGh-vnQl": {"type": "rebuttal", "replyto": "rkNuXSnXl", "comment": "We run a single job per hyperparameter setting and sample 45 hyperparameters per agent. It is important to note, that even though algorithm is stochastic, results are reproducible (once best hyperparameters were obtained we ran these experiments again to see whether the performance matches, and it does). The robustness plot (Figure 3, top right) also shows that UNREAL essentially strictly dominates the performance of A3C when compared on exactly the same set of hyperparameters.", "title": "Authors' response"}, "rkNuXSnXl": {"type": "rebuttal", "replyto": "SJ6yPD5xg", "comment": "The authors report \"performance over last 100 episodes of the top-3 jobs at every point in training.\" While there is no information on how many jobs total are used, this does not seem to be a statistically sound evaluation metric, since it is not measuring average case performance. The paper essentially reports a max over a large number of stochastic runs of the algorithm. In this case, wouldn't an algorithm that simply injects noise into the gradients eventually attain the best result given enough jobs? A more generous interpretation is that the authors reported the top-3 jobs for *entire* training run, but even that seems not particularly sound, since the claim is that the proposed method achieves *faster* learning -- in that case, shouldn't you also factor in the cost of all the other jobs that did not succeed?", "title": "Evaluation metric"}, "rywtZonfe": {"type": "review", "replyto": "SJ6yPD5xg", "review": "- \"Labyrinth also supports continuous motion unlike the Minecraft platform of (Oh et al., 2016), which is a 3D grid world.\" Not true anymore, the latest Minecraft platform (Malmo) allows getting continuous motion. (although it is true that this specific paper uses the 3D grid)\n- 10x less steps to train a model is great, but how much longer does each step take? Since you train on many things simultaneously, is there still a wall-time speedup?\n- It seems like feature control is almost the same thing as pixel control, but from the \"viewpoint\" of the \"hidden state\". Did you try having stacked recurrent layers and doing this feature control at each layer? If not, do you think it would be beneficial? \n- In the total absence of external rewards, the base policy would not learn anything meaningful. Did you attempt to do on-policy learning of some variants of pixel or feature control? Maybe as some kind of pretraining phase?\n- You used n=20-step return, what \\gamma value accompanies this?\n\nThanks!This work proposes to train RL agents to also perform auxiliary tasks, positing that doing so will help models learn stronger features.\nThey propose two pseudo-control tasks, control the change in pixel intensity, and control the activation of latent features. They also propose a supervised regression task, predict immediate reward following a sequence of events. The latter is learned offline via a skewed sampling of an experience replay buffer in order to balance seeing reward or not to 1/2 chance.\nSuch agents perform significantly well on discrete-action-continuous-space RL tasks, and reach baseline performance in 10x less iterations. \n\nThis work contrasts with traditional \"passive\" unsupervised or model-based learning. Instead of forcing the model to learn a potentially useless representation of the input, or to learn the possibly impossible (due to partial observability) task-modelling objective, learning to control local and internal features of the environment complements learning the optimal control policy.\n\nTo me the approach is novel and proposes a very interesting alternative to unsupervised learning that takes advantage of the \"possibility\" of control that an agent has over the environment.\nThe proposed tasks are explained at a rather high level, which is convenient to understand intuition, but I think some lower level of detail might be useful. For example L_PC should be explicitly mentioned, before reaching the appendix. Otherwise this work is clear and easily understandable by readers familiar with Deep RL.\nThe methodology is sound, on one hand hand the distribution of best hyperparameters might be different for A3C and UNREAL, but also measuring top-3 ensures that, presuming that the both best hyperparameters for A3C and for UNREAL are within the explored intervals, the per-method best hyperparameters are found.\nI think one weakness of the paper (or rather, considering the number of things that can fit in a paper, crucially needed future work) is that there is very little experimental analysis of the effect of the auxiliary tasks appart from their (very strong) effect on performance. In the same vein, pixel/feature control seems to have the most impact, in Labyrinth just A3C+PC beats anything else (except UNREAL), I think it would have been worth looking at this, either in isolation or in more depth, measuring more than just performance on RL tasks.\n\n", "title": "A few questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkL5To-Vg": {"type": "review", "replyto": "SJ6yPD5xg", "review": "- \"Labyrinth also supports continuous motion unlike the Minecraft platform of (Oh et al., 2016), which is a 3D grid world.\" Not true anymore, the latest Minecraft platform (Malmo) allows getting continuous motion. (although it is true that this specific paper uses the 3D grid)\n- 10x less steps to train a model is great, but how much longer does each step take? Since you train on many things simultaneously, is there still a wall-time speedup?\n- It seems like feature control is almost the same thing as pixel control, but from the \"viewpoint\" of the \"hidden state\". Did you try having stacked recurrent layers and doing this feature control at each layer? If not, do you think it would be beneficial? \n- In the total absence of external rewards, the base policy would not learn anything meaningful. Did you attempt to do on-policy learning of some variants of pixel or feature control? Maybe as some kind of pretraining phase?\n- You used n=20-step return, what \\gamma value accompanies this?\n\nThanks!This work proposes to train RL agents to also perform auxiliary tasks, positing that doing so will help models learn stronger features.\nThey propose two pseudo-control tasks, control the change in pixel intensity, and control the activation of latent features. They also propose a supervised regression task, predict immediate reward following a sequence of events. The latter is learned offline via a skewed sampling of an experience replay buffer in order to balance seeing reward or not to 1/2 chance.\nSuch agents perform significantly well on discrete-action-continuous-space RL tasks, and reach baseline performance in 10x less iterations. \n\nThis work contrasts with traditional \"passive\" unsupervised or model-based learning. Instead of forcing the model to learn a potentially useless representation of the input, or to learn the possibly impossible (due to partial observability) task-modelling objective, learning to control local and internal features of the environment complements learning the optimal control policy.\n\nTo me the approach is novel and proposes a very interesting alternative to unsupervised learning that takes advantage of the \"possibility\" of control that an agent has over the environment.\nThe proposed tasks are explained at a rather high level, which is convenient to understand intuition, but I think some lower level of detail might be useful. For example L_PC should be explicitly mentioned, before reaching the appendix. Otherwise this work is clear and easily understandable by readers familiar with Deep RL.\nThe methodology is sound, on one hand hand the distribution of best hyperparameters might be different for A3C and UNREAL, but also measuring top-3 ensures that, presuming that the both best hyperparameters for A3C and for UNREAL are within the explored intervals, the per-method best hyperparameters are found.\nI think one weakness of the paper (or rather, considering the number of things that can fit in a paper, crucially needed future work) is that there is very little experimental analysis of the effect of the auxiliary tasks appart from their (very strong) effect on performance. In the same vein, pixel/feature control seems to have the most impact, in Labyrinth just A3C+PC beats anything else (except UNREAL), I think it would have been worth looking at this, either in isolation or in more depth, measuring more than just performance on RL tasks.\n\n", "title": "A few questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ByaSg4szg": {"type": "rebuttal", "replyto": "B1K8po2be", "comment": "1. I think Sahil Sharma has answered this well for us but to reiterate, the idea is that hidden unit activations correspond to spatial or temporal abstractions of the current state of the environment. By training a policy to maximise the activation of a hidden unit requires the agent to understand how to control certain aspects of the environment that are more abstract than pixels. Those results in the paper are preliminary but even still provide evidence of feature control being useful.\n\n2. We have tried playing with the skewed sampling but found minimal effect, as along as there is sufficient skew towards rewarding states. Setting at 0.5 works consistently across all levels.", "title": "Authors' Response"}, "By8WAQozl": {"type": "rebuttal", "replyto": "Hy1dqoBfe", "comment": "Thank you for pointing out the typo. We will correct the equation. We will also remove the reference to supplementary materials.", "title": "Authors' response"}, "rk1bpXizl": {"type": "rebuttal", "replyto": "SJk3aeLGe", "comment": "We use asynchronous n-step Q-learning with n=20. We agree that it is somewhat of a hybrid between on-policy and off-policy learning, but it led to better results than standard (1-step) Q-learning in our initial experiments. It may be beneficial to use true off-policy return-based methods (e.g. \"Safe and efficient off-policy reinforcement learning\", Remi Munos, Thomas Stepleton, Anna Harutyunyan, Marc G. Bellemare), but we haven't experimented with them yet.", "title": "Authors' response"}, "Hy1dqoBfe": {"type": "rebuttal", "replyto": "SJ6yPD5xg", "comment": "Very interesting paper! It is quite stunning to see these improvements over A3C.\n\nPerhaps the choice of \\lambda_c in equation 1 in front of the sum is somewhat confusing, because the use of a lower case c seems to suggest that \\lambda_c is actually different for each task (although then it should not be in front of the sum). If I understood things correctly (also regarding equation 2), there is only a single weighting \\lambda parameter for the entire set of auxiliary control tasks. So wouldn't \\lambda_\\mathcal{C} be a better name for this parameter or am I misinterpreting things?\n\nAlso, the appendix mentions 'Further details are included in the supplementary materials'. Perhaps I missed something, but if not, when/where will they be available?  ", "title": "Equation 1 and supplementary material"}, "SJk3aeLGe": {"type": "review", "replyto": "SJ6yPD5xg", "review": "Hi, quick question: can you please clarify how you perform n-step Q-Learning on auxiliary tasks (unless you use n=1?), since it has to be off-policy? Thanks!This paper is about improving feature learning in deep reinforcement learning, by augmenting the main policy's optimization problem with terms corresponding to (domain-independent) auxiliary tasks. These tasks are about control (learning other policies that attempt to maximally modify the state space, i.e. here the pixels), immediate reward prediction, and value function replay. Except for the latter, these auxiliary tasks are only used to help shape the features (by sharing the CNN+LSTM feature extraction network). Experiments show the benefits of this approach on Atari and Labyrinth problems, with in particular much better data efficiency than A3C.\n\nThe paper is well written, ideas are sound, and results pretty convincing, so to me this is a clear acceptance. At high level I only have few things to say, none being of major concern:\n- I believe you should say something about the extra computational cost of optimizing these auxiliary tasks. How much do you lose in terms of training speed? Which are the most costly components?\n- If possible, please try to make it clearer in the abstract / intro that the agent is learning different policies for each task. When I read in the abstract that the agent \"also maximises many other pseudo-reward functions simultaneously by reinforcement learning\", my first understanding was that it learned a single policy to optimize all rewards together, and I realized my mistake only when reaching eq. 1.\n- The \"feature control\" idea is not validated empirically (the preliminary experiment in Fig. 5 is far from convincing as it only seems to help slightly initially). I like that idea but I am worried by the fact the task is changing during learning, since the extracted features are being modified. There might be stability / convergence issues at play here.\n- Since as you mentioned, \"the performance of our agents is still steadily improving\", why not keep them going to see how far they go? (at least the best ones)\n- Why aren't the auxiliary tasks weight parameters (the lambda_*) hyperparameters to optimize? Were there any experiments to validate that using 1 was a good choice?\n- Please mention the fact that auxiliary tasks are not trained with \"true\" Q-Learning since they are trained off-policy with more than one step of empirical rewards (as discussed in the OpenReview comments)\n\nMinor stuff:\n- \"Policy gradient algorithms adjust the policy to maximise the expected reward, L_pi = -...\" => that's actually a loss to be minimized\n- In eq. 1 lambda_c should be within the sum\n- Just below eq. 1 r_t^(c) should be r_t+k^(c)\n- Figure 2 does not seem to be referenced in the text, also Figure 1(d) should be referenced in 3.3\n- \"the features discovered in this manner is shared\" => are shared\n- The text around eq. 2 refers to the loss L_PC but that term is not defined and is not (explicitly) in eq. 2\n- Please explain what \"Clip\" means for dueling networks in the legend of Figure 3\n- I would have liked to see more ablated versions on Atari, to see in particular if the same patterns of individual contribution as on Labyrinth were observed\n- In the legend of Figure 3 the % mentioned are for Labyrinth, which is not clear from the text.\n- In 4.1.2: \"Figure 3 (right) shows...\" => it is actually the top left plot of the figure. Also later \"This is shown in Figure 3 Top\" should be Figure 3 Top Right.\n- \"Figure 5 shows the learning curves for the top 5 hyperparameter settings on three Labyrinth navigation levels\" => I think it is referring to the left and middle plots of the figure, so only on two levels (the text above might also need fixing)\n- In 4.2: \"The left side shows the average performance curves of the top 5 agents for all three methods the right half shows...\" => missing a comma or something after \"methods\"\n- Appendix: \"Further details are included in the supplementary materials.\" => where are they?\n- What is the value of lambda_PC? (=1 I guess?)\n\n[Edit] I know some of my questions were already answered in Comments, no need to re-answer them", "title": "n-step Q-Learning for auxiliary tasks", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1BCqSMNl": {"type": "review", "replyto": "SJ6yPD5xg", "review": "Hi, quick question: can you please clarify how you perform n-step Q-Learning on auxiliary tasks (unless you use n=1?), since it has to be off-policy? Thanks!This paper is about improving feature learning in deep reinforcement learning, by augmenting the main policy's optimization problem with terms corresponding to (domain-independent) auxiliary tasks. These tasks are about control (learning other policies that attempt to maximally modify the state space, i.e. here the pixels), immediate reward prediction, and value function replay. Except for the latter, these auxiliary tasks are only used to help shape the features (by sharing the CNN+LSTM feature extraction network). Experiments show the benefits of this approach on Atari and Labyrinth problems, with in particular much better data efficiency than A3C.\n\nThe paper is well written, ideas are sound, and results pretty convincing, so to me this is a clear acceptance. At high level I only have few things to say, none being of major concern:\n- I believe you should say something about the extra computational cost of optimizing these auxiliary tasks. How much do you lose in terms of training speed? Which are the most costly components?\n- If possible, please try to make it clearer in the abstract / intro that the agent is learning different policies for each task. When I read in the abstract that the agent \"also maximises many other pseudo-reward functions simultaneously by reinforcement learning\", my first understanding was that it learned a single policy to optimize all rewards together, and I realized my mistake only when reaching eq. 1.\n- The \"feature control\" idea is not validated empirically (the preliminary experiment in Fig. 5 is far from convincing as it only seems to help slightly initially). I like that idea but I am worried by the fact the task is changing during learning, since the extracted features are being modified. There might be stability / convergence issues at play here.\n- Since as you mentioned, \"the performance of our agents is still steadily improving\", why not keep them going to see how far they go? (at least the best ones)\n- Why aren't the auxiliary tasks weight parameters (the lambda_*) hyperparameters to optimize? Were there any experiments to validate that using 1 was a good choice?\n- Please mention the fact that auxiliary tasks are not trained with \"true\" Q-Learning since they are trained off-policy with more than one step of empirical rewards (as discussed in the OpenReview comments)\n\nMinor stuff:\n- \"Policy gradient algorithms adjust the policy to maximise the expected reward, L_pi = -...\" => that's actually a loss to be minimized\n- In eq. 1 lambda_c should be within the sum\n- Just below eq. 1 r_t^(c) should be r_t+k^(c)\n- Figure 2 does not seem to be referenced in the text, also Figure 1(d) should be referenced in 3.3\n- \"the features discovered in this manner is shared\" => are shared\n- The text around eq. 2 refers to the loss L_PC but that term is not defined and is not (explicitly) in eq. 2\n- Please explain what \"Clip\" means for dueling networks in the legend of Figure 3\n- I would have liked to see more ablated versions on Atari, to see in particular if the same patterns of individual contribution as on Labyrinth were observed\n- In the legend of Figure 3 the % mentioned are for Labyrinth, which is not clear from the text.\n- In 4.1.2: \"Figure 3 (right) shows...\" => it is actually the top left plot of the figure. Also later \"This is shown in Figure 3 Top\" should be Figure 3 Top Right.\n- \"Figure 5 shows the learning curves for the top 5 hyperparameter settings on three Labyrinth navigation levels\" => I think it is referring to the left and middle plots of the figure, so only on two levels (the text above might also need fixing)\n- In 4.2: \"The left side shows the average performance curves of the top 5 agents for all three methods the right half shows...\" => missing a comma or something after \"methods\"\n- Appendix: \"Further details are included in the supplementary materials.\" => where are they?\n- What is the value of lambda_PC? (=1 I guess?)\n\n[Edit] I know some of my questions were already answered in Comments, no need to re-answer them", "title": "n-step Q-Learning for auxiliary tasks", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hy64RUxGx": {"type": "rebuttal", "replyto": "B1K8po2be", "comment": "I think the reason why one would like to control firing of features is because they represent different task-specific abstractions learnt by the agent. Features typically fire if a particular spatial or temporal abstraction has recently been encountered. By learning to control how these features fire, the agent effectively learns to \"go towards\" such abstractions (could for example be an enemy in a game like Sea Quest).\nHaving said that, it is unclear to me as to how exactly were the features controlled. The paper mentions using n-step async Q-learning for auxiliary tasks control. But I do not understand what the actions would be in this case of learning to control features.", "title": "first comment"}, "rk2xfoAZx": {"type": "rebuttal", "replyto": "SJ6yPD5xg", "comment": "Hi, I believe the first line of the abstract: \"Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward\" is slightly misleading. Due to reward clipping done by most DRL algorithms, what is maximized is discounted rewards frequency and not discounted cumulative rewards.", "title": "Misleading first line of abstract"}, "B1K8po2be": {"type": "rebuttal", "replyto": "SJ6yPD5xg", "comment": "Hi, \nVery nice work, with a neat jump in performance! Just had a couple questions:\n1. I understand the idea behind 'pixel control' but I don't quite get the motivation behind adding the 'feature control' task. Could you provide some intuition as to why maximizing the hidden unit activations would help learning? Also, from figure 5(c), it looks like this doesn't help much?\n2. Have you tried playing with the skewed sampling parameter (0.5)? We had experimented with a very similar scheme, prioritized sampling (Narasimhan et al., 2015), and found that tuning this parameter did give us some gains. \n\nReferences:\nKarthik Narasimhan, Tejas Kulkarni, Regina Barzilay. Language Understanding for Text-based Games Using Deep Reinforcement Learning. Proceedings of EMNLP, 2015 ", "title": "Questions"}}}