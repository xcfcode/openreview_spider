{"paper": {"title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning", "authors": ["Justin Fu", "Aviral Kumar", "Ofir Nachum", "George Tucker", "Sergey Levine"], "authorids": ["~Justin_Fu1", "~Aviral_Kumar2", "~Ofir_Nachum1", "~George_Tucker1", "~Sergey_Levine1"], "summary": "A benchmark proposal for offline reinforcement learning.", "abstract": "The offline reinforcement learning (RL) problem, also known as batch RL, refers to the setting where a policy must be learned from a static dataset, without additional online data collection. This setting is compelling as it potentially allows RL methods to take advantage of large, pre-collected datasets, much like how the rise of large datasets has fueled results in supervised learning in recent years. However, existing online RL benchmarks are not tailored towards the offline setting, making progress in offline RL difficult to measure. In this work, we introduce benchmarks specifically designed for the offline setting, guided by key properties of datasets relevant to real-world applications of offline RL. Examples of such properties include: datasets generated via hand-designed controllers and human demonstrators, multi-objective datasets where an agent can perform different tasks in the same environment, and datasets consisting of a mixtures of policies. To facilitate research, we release our benchmark tasks and datasets with a comprehensive evaluation of existing algorithms and an evaluation protocol together with an open-source codebase. We hope that our benchmark will focus research effort on methods that drive improvements not just on simulated tasks, but ultimately on the kinds of real-world problems where offline RL will have the largest impact. ", "keywords": ["reinforcement learning", "deep learning", "benchmarks"]}, "meta": {"decision": "Reject", "comment": "This paper proposes benchmark tasks for offline reinforcement learning.  The paper has major strength and weakness, and it has resulted in very active discussion among reviewers, authors, and other participants.\n\nThe major strength includes the following:\n- The proposed benchmark is already heavily used in the community\n- Offline reinforcement learning is very important to solve reinforcement learning tasks in the real world\n- The paper covers a range of tasks and provides through evaluation of existing methods to be used as baselines\n\nThe major weakness is that it is not sufficiently convincing that the methods that perform well in the proposed benchmark tasks will perform well in the offline reinforcement learning tasks in the real world.  \n\nThis is partly due to the nature of the benchmark tasks of offline reinforcement learning, which require simulators to evaluate the policies learned with offline reinforcement learning.  This means that one cannot simply collect datasets from real world tasks and provide them as benchmark datasets.  \n\nAlthough one cannot do much about simulators, benchmark tasks for offline reinforcement learning still have many design choices.  In particular, how should the datasets in the benchmark be collected (i.e., behavior policies)?\n\nWhile the datasets in the proposed benchmark are collected with various behavior policies including humans, it is not necessarily convincing that the resulting benchmark tasks are good for the purpose of evaluating offline reinforcement learning to be used in the real world.\n\nIn addition to the suggestions given by the reviewers, a possible direction to improve the paper is to focus on the choice of behavior policies used to generate the datasets in the proposed benchmark.  One might then be able to provide some convincing arguments as to why performing well in the benchmark might imply good performance in the real world by relating it to the choice of behavior policies."}, "review": {"iBlZrFsxYPE": {"type": "review", "replyto": "px0-N3_KjA", "review": "Summary:\nIn this paper a test suite of data sets and corresponding benchmarks for offline reinforcement learning is introduced.\nSeveral existing RL benchmarks are used, the results of several algorithms are presented.\nThe authors claim that the benchmarks were specifically designed for the offline setting and are guided by the key properties of datasets in real-world applications of offline RL.\n\nStrong points:\nThe present paper has already been cited and the benchmarks suite has already been used by other publications. Obviously there is a need for offline RL test suites.\n\nWeak points:\nThe authors' claim that such a benchmark for offline RL should \"be composed of tasks that reflect challenges in real-world applications of data-driven RL\" is only partially met by the paper in its present form. The area of robotics, with deterministic dynamics, is comparatively well represented, but there is no real, industrial application. In particular it seems that so far no benchmark has been included that has the ambition to have the characteristics and complexity of a real application. \n\nRecommendation:\nOn the one hand, the really realistic benchmarks are missing, so that a publication seems premature. On the other hand, the current status is already used by the research community, since there seems to be no test suite for offline RL apart from \"RL unplugged: Benchmarks for offline reinforcement learning\". I therefore recommend to accept the paper.\n\nQuestions:\nTo what extent are the current benchmarks stochastic?\nAre there bi- or multimodal transition probabilities?\n\nAdditional feedback with the aim to improve the paper:\nIn Table 2 and Table 3 average results are reported over only 3 random seeds. This seems to me to be clearly too little, especially since the policy performance of Q-function based algorithms often fluctuates strongly. Since no uncertainties, e.g. in the form of standard error, are given, the reliability of the results cannot be assessed.\nThe way policies are selected before they are tested should be described more clearly. My impression was that in each case the policy is used that results for a considered algorithm and random seed after 500K training iterations or gradient steps. Since different algorithms require different computational efforts this approach does not seem to be in the sense of a real-world application. In most cases there should be the willingness to use much more computational effort for especially good policies. According to the motto: computing power is cheap, data is expensive. \n\nI like the formulation \u201cEffective offline RL algorithms must handle [\u2026] data collected via processes that may not be representable by the chosen policy class.\u201c This expresses the, in my opinion, correct view of the real situation well, while the assumption that there is a \"behavior policy\" that generated the data is not true in general. It may have been different people at different times who performed the actions while the data set was recorded.\n\nPlease check the bibliography for accidental lower case, like \u201emarkov\u201c, \u201eadobeindoornav\u201c\n\n\n----------------------------------\n(Dec 3) Taking into account the other reviews, the authors' responses and the changes made by the authors, as well as the extensive and controversial discussion, I rate the paper still with a score of 6.\n", "title": "Premature or high time?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "jzLbCMbTa1": {"type": "rebuttal", "replyto": "wq-dAjsJFEs", "comment": "> \u201cWhat could be made clear is whether there are novel contributions in the way behavior policies are selected, whether the choice of the behavior policies is the best with the current state of the art, or whether the choice of behavior policies is unsatisfactory.\u201d\n\nWe can attempt clarify on the novelty of the way behavior policies are selected, and other reviewers can comment if they feel this is not accurate.\n\nThere have been several behavior policies proposed in prior work which we incorporate, which include using data contained in the replay buffer, and using data collected from RL-trained policies of varying performance. As prior papers typically evaluate on a subset of these problems, we decided to include all of these data collection methods in the benchmark, and evaluate each one. These are primarily incorporated in the Gym-MuJoCo domain.\n\nIn terms of datasets that have not believe have been proposed in prior work, we believe the following data collection methods have not been widely incorporated in offline RL:\n- **Human data.** The Adroit and FrankaKitchen domains contain datasets acquired from human demonstrations. Utilizing human data is likely an important step for making progress towards real applications of offline RL.\n- **Hand-designed controllers.** We also include data collected from fixed, hand-designed controllers which are importantly not RL-trained policies (navigation domains, Flow). As discussed in the text, this can potentially stress-test the ability of algorithms to handle behavior policies outside of the function class.\n- **Behavior policies not aimed at solving the task of interest** (i.e. task-unrelated behavior). On the FrankaKitchen domain, the behavior policy consists of behaviors that attempt to perform auxiliary tasks such as opening cabinets, the microwave, moving the kettle, etc. however not all of these are actually needed to, or relevant towards optimizing the reward function. This is indicative of real-world applications where we wish to utilize general-purpose task-agnostic data to optimize a given reward function.\n- **Passively logged data**. On our navigation domains (Maze2D, AntMaze, CARLA), we include data observed from random, undirected navigation, which are retroactively labeled with a task reward. We believe passive logging will be similar to how many large datasets will be collected in realistic applications.\n- **Mixture datasets**. We include datasets (on the Gym domain) which consist of mixtures of behavior policies. This can potentially stress-test the ability of algorithms to handle multi-model state, action and trajectory distributions.", "title": "On behavior policies."}, "_Sn87qXh3el": {"type": "review", "replyto": "px0-N3_KjA", "review": "The paper proposes a standardized benchmark for offline RL research. The data collection is well motivated from real world scenarios covering many important design factors. I really appreciate that human demonstration and hand-crafted controllers are also included. The evaluation protocol and the API looks clear and easy to use. The benchmark of existing methods is thorough and provides many useful insights. I believe this work will have a high impact on the offline RL community. I can expect this benchmark will be used by many papers in the future and will function as the starting point for many offline RL research.\n\nHowever, I notice that this dataset may not be accessible for underrepresented groups. I therefore vote to reject. As the authors note in the paper, each task consists of a dataset for training and a simulator for evaluation. In my understanding, half of the six tasks (Maze2D, AntMaze, Gym-mujoco) depend heavily on the MuJoCo simulator, which is a commercial software and is not free even for academic use. A personal MuJoCo license costs 500 USD per year. I am concerned that MuJoCo is not accessible for most underrepresented researchers. It is not clear when MuJoCo becomes a dominating benchmark for online RL research, though there are indeed free, open-sourced alternatives, e.g. PyBullet (https://github.com/bulletphysics/bullet3). In online RL, we need the simulator for training. One reason MuJoCo becomes popular may be because it's more stable and faster than PyBullet. However, in offline RL, a simulator is used only for evaluation not for training. So the high reliability of MuJoCo may no longer be so necessary. I therefore view offline RL as a good opportunity for the community to get rid of commercial simulation softwares, making RL research more accessible for underrepresented groups. If accepted, this paper will indeed greatly promote the use of MuJoCo given its potential high impact, making RL more privileged.  \n\nOverall, I really enjoy reading the paper and am glad to see a standardized benchmark for offline RL. I am happy to raise my score if the accessibility issue is addressed, e.g., by using PyBullet as the physical engine.\n\n====================\n\n(Nov 24) I appreciate the effort put into the Bullet reimplementation and therefore increase my score from 3 to 6.\n", "title": "Concerns about accessibility for underrepresented groups ", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "5kcd4sIfh1H": {"type": "rebuttal", "replyto": "px0-N3_KjA", "comment": "We appreciate the constructive feedback from the reviewers. In response, we:\n- are nearly complete reimplementing the Gym-MuJoCo and Maze2D tasks using the PyBullet simulator. Progress so far is available at https://sites.google.com/view/d4rl-anonymous/. \n- are in the process of running more seeds (10 seeds) to provide stronger statistical significance.\n- have revised the text to clarify that D4RL includes real behavior data from humans and includes tasks that use models of realistic systems that are accepted as such by domain experts (Flow, Franka & Adroit, Carla).\n\nThe reviewers have pointed out areas for improvement and discussion. However, no benchmark is perfect and compared to the closest similar work, D4RL makes significant contributions in important areas:\n- includes real behavior data and synthetic behavior data that incorporates narrow data distributions, multitask data, and non-representable policies. In prior work, the predominant method for evaluating offline RL algorithms was to generate data from online RL agents on Atari or Gym tasks. As we show in our empirical evaluation, this does not exercise important dimensions of the problem.\n- is reproducible: all our code and data is open-source, we include scripts for generating our datasets or describe how our datasets were generated (e.g. in the case of our human datasets), and can be used by anyone (PyBullet, CARLA, Flow do not require a paid license, and MuJoCo has a free student license). Previous work either does not open source the datasets and/or the behavior policies used to generate the datasets.\n- provides a comprehensive evaluation of 11 RL methods. Previously, no such extensive evaluation on a common dataset had been done in offline RL. \n", "title": "Summary of changes"}, "1bCUo-S6mf": {"type": "rebuttal", "replyto": "h2LJ6inSEHJ", "comment": "We are pleased to report that we are nearing completion for reimplementing the Gym-MuJoCo and Maze2D tasks using the Bullet simulator. The dataset URLs and environment code are included in the code on our website at https://sites.google.com/view/d4rl-anonymous/. We will be continuing to work on integrating this into the rest of the library, and are planning out how to reimplement AntMaze next as well.", "title": "Progress Update on Bullet Re-implementation"}, "mCoFbaMnYvQ": {"type": "rebuttal", "replyto": "L0o4qw789AQ", "comment": "Thank you for the reply, and you raise good points. We appreciate your comment about the importance of realistic evaluation domains, and we agree that (1) synthetic datasets that enable standardized benchmarks and (2) realistic datasets are both important for evaluating offline RL methods. However, D4RL already accounts for this.\n\n**1. D4RL already includes real data.** For \u201creal human behavior\u201d, the FrankaKitchen domain and Adroit domain consist of real human demonstrations collected via teleoperation, as mentioned in Section 5. This behavior is contained in the adroit-*-human tasks for the Adroit domain, and all FrankaKitchen tasks.\n\nIn terms of including mathematical models of real behavior as you suggest, we utilize the \u201cIntelligent driver model\u201d [1] in our Flow tasks as outlined in Section 5. This is a well-cited and widely used model of human driving behavior within the operations research and traffic communities, keeping in line with our design principle of using well-vetted, realistic models and simulations. We labeled these tasks as flow-\\*-controller, but perhaps it would be better to name them flow-\\*-idm to make this fact more clear. \n\n[1] \u201cCongested traffic states in empirical observations and microscopic simulations.\u201d Treiber et. al, 2000\n\n**2. We do not claim that D4RL evaluates real-world applications.** While D4RL includes realistic data sources, such as data from humans, it is meant to be a mechanism for benchmarking offline RL algorithms, not for evaluating whether such algorithms can actually solve real-world applications. This is in line with previous benchmarks in the RL community (e.g., ALE, OpenAI gym, etc.). We disagree with the statement that \"The authors are currently focusing their paper on real behavioral datasets\" -- while D4RL includes real data, this is not the focus, and we do not claim that D4RL is anything but a synthetic benchmark and our design choices reflect this. However, if you believe that the current draft makes this claim, we would be happy to correct this issue and revise accordingly.\n\n> \u201cOn the other hand, the authors also don't generate good synthetic behaviors. Their behaviors are not reproducible..\u201d\n\nWe are not clear on why our behaviors are \u201cnot reproducible\u201d, and why our synthetic datasets are not \u201cgood\u201d - would it be possible to clarify on this point? For most of our datasets, we either include scripts for generating our datasets, or describe how our datasets were generated (e.g. in the case of our human datasets). We believe this should be sufficient for reproducing the majority of the datasets in our benchmark.\n\n> \u201cIn case of synthetic datasets, the authors should provide clean, simple datasets (even tabular), make the behaviors easy to reproduce, and make a notion of optimality clear.\u201d\n\nWe have included a Gridworld environment based on Minigrid (https://github.com/maximecb/gym-minigrid) for debugging purposes. These can be found in the code attached on our website (https://sites.google.com/view/d4rl-anonymous/), as the minigrid-fourrooms-random and minigrid-fourrooms tasks. Minigrid-fourrooms-random is collected via random actions, and minigrid-fourrooms uses the same collection method as the maze2d and antmaze environment via planning routes to randomly selected goals.\n \nWe would be happy to make adjustments as per your request to improve D4RL, but your current criticisms do not provide a way of doing this. Your comment asks for: (a) real data (we already have this); (b) tabular domains (now added); (c) ease of reproducibility (this is already addressed -- all code is open-sourced and reproducible, but if there is any particular thing you would like us to do to improve reproducibility, please tell us). While we would be happy to improve D4RL to address your suggestions, currently the main issue you raise is already addressed (e.g., human data, mathematical models of real behavior).\n \nLastly, we would conclude by noting that the RL community does have a standard for benchmarks. We believe that D4RL meets or exceeds this standard. While certainly there is more that could be done -- for example, developing higher fidelity simulators, tasks that cover other domains such as healthcare and education, tasks that cover additional properties such as casual structures, etc., we believe that D4RL already adds a lot in terms of allowing the community to standardize around an effective evaluation standard for offline RL methods. It is substantially better than the previous widely accepted standard, which was to use ad-hoc data generated from RL policies on MuJoCo gym locomotion tasks. If you have concerns that D4RL is a step in the wrong direction, please tell us what you think would be better, but so far we do not see a clear path for improvement from your comments.\n", "title": "On the use of real data"}, "cJZZqMTrso5": {"type": "rebuttal", "replyto": "gzbW7diZEDA", "comment": "Hello, we appreciate the constructive feedback on the paper. We are wondering if our response addresses your concerns, or if you think additional changes are needed. Thank you!", "title": "Additional concerns?"}, "h2LJ6inSEHJ": {"type": "rebuttal", "replyto": "_Sn87qXh3el", "comment": "Thank you for your comments, and raising awareness on this important issue. We are glad to hear that you think this benchmark has potential to have a high impact on the RL community and will be used by many papers to come. \n\nWe agree that using free, open-source simulators would benefit the community and the introduction of a new benchmark is a great place to make that shift in the community. We have begun implementing a PyBullet version of our tasks, and will add an update here on the progress before the rebuttal deadline. \n\nNevertheless, MuJoCo is already used by the community including many RL benchmark papers such as RLLAB [1], RLUnplugged [2], Metaworld [3], and Gym [4]. However, adding PyBullet versions to the current tasks seems like the best path forward. We also emphasize that several of our domains, such as CARLA and Flow, use simulators that do not require a paid license. \n\nWe note that the authors are not affiliated with the company that sells MuJoCo. MuJoCo does offer free licenses on its website for personal use, for projects not \u201cpart of employment\u201d and not already receiving financial support.\n\n[1] \u201cBenchmarking Deep Reinforcement Learning for Continuous Control\u201d Duan et. al. ICML 2016\n\n[2] \u201cRL Unplugged: Benchmarks for Offline Reinforcement Learning\u201d Gulcehre et. al. NeurIPS 2020\n\n[3] \u201cMeta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning\u201c Yu et. al. CoRL 2019\n\n[4] \u201cOpenAI Gym\u201d Brockman et. al. 2016\n\n", "title": "Author's response"}, "n_k675ju96r": {"type": "rebuttal", "replyto": "iBlZrFsxYPE", "comment": "Thank you for your feedback. We believe your main concern is in the real-world ramifications of the tasks used in the benchmark, to which we respond to below, along with several other clarifications. Please let us know if you have any additional questions, and if this addresses your concerns.\n\n> \u201cThe authors' claim that such a benchmark for offline RL should \"be composed of tasks that reflect challenges in real-world applications of data-driven RL\" is only partially met by the paper in its present form. The area of robotics, with deterministic dynamics, is comparatively well represented, but there is no real, industrial application\u2026 on the one hand, the really realistic benchmarks are missing\u201d \n\nWe agree that incorporating tasks with real-life implications is important for a benchmark, and therefore we leveraged some of the most realistic simulated domains which are widely available for research use. For example, the Adroit and FrankaKitchen domains are models of Shadow Hand and Franka robots, respectively, and leverage real human demonstrations collected via motion capture. Likewise, CARLA is a photorealistic simulator used in the autonomous driving community (e.g. [1]) and Flow is a traffic simulator used in the operations research/transportation community (e.g. [2]). Robotic manipulation, autonomous driving, and traffic control are all domains with real, challenging applications and significant implications for everyday life. \n\nWe considered other domains with real datasets, such as healthcare (MIMIC-III) or recommender systems (various, across industry), but for the purposes of constructing a benchmark, we do not have a reliable method for evaluating the performance of algorithms on these tasks aside from costly real-world evaluation.\n\n[1] \u201cEnd-to-end driving via conditional imitation learning.\u201d Codevilla et. al. ICRA 2018\n\n[2] \u201cStabilizing traffic with autonomous vehicles\u201d Wu et. al. ICRA 2018\n\n> \u201cTo what extent are the current benchmarks stochastic? Are there bi- or multimodal transition probabilities?\u201d\n\nThe current benchmarks are stochastic in the initial state (i.e. in the navigation environments the starting location is randomized), and through partial observability in the case of CARLA. Stochasticity is however, a property that is not explored in-depth compared to the other properties we outlined in Section 4. We added additional discussion of this point in Section 7.\n\n> \u201c\u2026 average results are reported over only 3 random seeds. This seems to me to be clearly too little, especially since the policy performance of Q-function based algorithms often fluctuates strongly. \u2026 In most cases there should be the willingness to use much more computational effort for especially good policies. According to the motto: computing power is cheap, data is expensive.\u201d\n\nThe cost for evaluating this benchmark was already quite expensive due to its scope, with 11 algorithms evaluated and 42 tasks (for a total of 462 evaluations), including several which require GPU access. We will evaluate additional seeds, but this may not be complete during the rebuttal period. \n", "title": "Rebuttal for reviewer 4"}, "voVAs1xjoZZ": {"type": "rebuttal", "replyto": "woE-2zxOdE0", "comment": "> \u201cThe dataset is mostly simulated. While I understand the difficulty of collection real-data, it does raise some concerns that a simulated dataset can be generated by researchers themselves.\u201d\n\nThank you for your comments. We made a very conscious choice in the design of the benchmark to keep everything in simulation, and select domains for which realistic, accurate simulations exist and have been vetted by the research community. We would be open to suggestions which allow both accurate evaluation and a convincing degree of realism in order to improve the benchmark.\n\nWe would also like some additional clarification on what concerns you have with using a dataset generated by a simulator, and we can add this to the discussion in the main text.\n", "title": "Clarifications on simulation"}, "gzbW7diZEDA": {"type": "rebuttal", "replyto": "_H97HrjdmeR", "comment": "Thank you for your comments and feedback. We respond to individual comments below. Please let us know if this addresses your concerns.\n\n> \u201cIf the authors bring together datasets that generate original ideas that have never been previously explored, then I believe it's more likely that this paper could be accepted in future venues.\u201d\n\nWe believe that benchmark and evaluation papers should be measured on their potential impact and insights provided. Constructing a systematic evaluation of existing methods, providing insights into their shortcomings to guide future research (Section 6), and implementing an easy-to-use research platform (see our website and code: https://sites.google.com/view/d4rl-anonymous/) are all contributions that further advance this field. \n\nD4RL has already facilitated standardized evaluation and benchmarking, and moving beyond reliance on MuJoCo locomotion tasks and datasets obtained from replay buffers of online RL algorithms. This work has been used as a starting point for novel ideas as evidenced by the rapid adoption of D4RL, and the number of papers submitted to this conference that use it as their primary evaluation task such as:\n- \u201cOffline Policy Optimization with Variance Regularization\u201d\n- \u201cUncertainty Weighted Offline Reinforcement Learning\u201d\n- \u201cRisk-Averse Offline Reinforcement Learning\u201d\n- \u201cAddressing Distribution Shift in Online Reinforcement Learning with Offline Datasets\u201d\n- \u201cBRAC+: Going Deeper with Behavior Regularized Offline Reinforcement Learning\u201d\n- \u201cFine-Tuning Offline Reinforcement Learning with Model-Based Policy Optimization\u201d\n- \u201cEMaQ: Expected-Max Q-Learning Operator for Simple Yet Effective Offline and Online RL\u201d\n- \u201cOPAL: Offline Primitive Discovery for Accelerating Offline Reinforcement Learning\u201d\n- \u201cModel-Based Offline Planning\u201d\n\n And in NeurIPS 2020 papers:\n- \u201cConservative Q-Learning for Offline Reinforcement Learning\u201d. Kumar et. al.\n- \u201cModel-Based Offline Policy Optimization\u201d. Yu et al.\n- \u201cContinual Learning of Control Primitives: Skill Discovery via Reset-Games\u201d. Xu et. al.\n\n> \u201cReal datasets present many more real-world problems, including: Non-stationary data\u2026 Real policies\u2026 Causal structures\u2026 Reduction from real datasets\u2026 Changing and/or very large action sets\u2026 Non-robotic environments\u201d\n\nWhile we agree that these are important properties of real-world datasets, no single benchmark can cover every real world problem. Our benchmark tasks provide coverage of several important real-world properties that we suspected have a large impact on offline RL and have not previously been evaluated in offline RL. For example, the Adroit/Franka domains contain real policies obtained by teleoperation. We have added a discussion of these properties and the limitations of our benchmark to emphasize their importance and future opportunity in Section 7.\n\n> \u201cWhile the dataset provided by the authors may be a useful resource... this paper does not introduce any new or novel ideas. Overall the main contribution of this work is the gathering of offline data in one place\u201d\n\nWe agree that our primary contribution is not novel methods or analyses, however, you understate our contributions. In the Gym/Franka/Adroit domains, we do use existing datasets, however, for the remaining domains, we either construct datasets for tasks that have not previously been studied in offline RL or in the case of Maze2D/AntMaze, construct novel domains to study specific properties. Appendix C details which datasets we borrowed as part of the construction of this benchmark.\n\nPrior to our work, the predominant method for evaluating offline RL algorithms was to generate data from online RL agents on Atari or Gym tasks. As discussed in [2] and shown in our empirical evaluation, this does not exercise important dimensions of the problem. Our data generation procedures bring attention to important and neglected properties in offline RL (e.g., narrow data distributions, multitask data, and non-representable policies) that have a significant impact on performance and have already guided followup research.\n\nFor example, recent work [1, 2] has provided contradicting evidence on the importance of the data generating distribution in offline RL. D4RL contains multiple domains where this problem can be explored in depth, across a wide variety of evaluated algorithms. Our empirical evaluations in Table 1 reveal clear performance differences under different data generation settings.\n\nFinally, we provide a systematic evaluation of 11 RL methods, including state-of-the-art algorithms. Previously, no such extensive evaluation on a common dataset had been done in offline RL. The standardization and thorough evaluation of a benchmark with appealing properties (including several that you listed as important real-world properties) is a valuable contribution.\n\n[1] \u201cAn Optimistic Perspective on Offline Reinforcement Learning\u201d Agrawal et. al. 2020\n\n[2] \u201cBehavior Regularized Offline Reinforcement Learning\u201d Wu et. al. 2019\n", "title": "Clarifications on contribution"}, "_H97HrjdmeR": {"type": "review", "replyto": "px0-N3_KjA", "review": "This paper offers a new set of challenges for batch reinforcement learning coupled with a set of benchmarks, including autonomous robotics and driving domains. All domains also come with simulation environments.\n\nWhile the dataset provided by the authors may be a useful resource for researchers in offline-RL, this paper does not introduce any new or novel ideas. Overall the main contribution of this work is the gathering of offline data in one place, reducing the time needed for other researchers to do so. It does not seem as if the authors did any non-trivial work other than annotating the data. Furthermore, there have been many previous work which have already collected offline datasets as part of their work.\n\nI do not underestimate the importance the authors' hard work, nor the importance of the provided datasets to the RL community. Nevertheless, I do not believe this work should be published in a high-end conference without presenting any ideas that are not trivial or known to other researches. \n\nThe authors propose to use the following design factors in their offline datasets: narrow distributions, multi-task data, sparse rewards, suboptimal data, and partially observable policies. While these factors may indeed be good for testing offline-RL algorithms, they do not provide a complete picture of real world datasets. Real datasets present many more real-world problems, including:\n1. Non-stationary data. Many real datasets are non-stationary. The non-stationary behavior could be mimicked or simulated from real behavior.\n2. Real policies. Real datasets don't involve policies that were trained by RL agents. The authors could create datasets that are constructed by real human beings (for example, humans playing atari games, with mixed or different expertise). In a controlled setup, the datasets could be constructed so that policies are categorized (e.g., \"level of non Markovianess`\"). \n3. Causal structures. Real policies may act according to some causal structure in the background that is not necessarily known.\n4. Reduction from real datasets. One could collect or use large amounts of high quality datasets from the real world and add certain corruptions to the data as to lower its quality (e.g., removing certain trajectories). If the initial data is of high quality, the corrupted data could be controlled well.\n5. Changing and/or very large action sets. Real world datasets have changing and large datasets. As an example, consider ad placement, or text based tasks.\n6. Non-robotic environments, including games but also real world problems.\n\nIf the authors bring together datasets that generate original ideas that have never been previously explored, then I believe it's more likely that this paper could be accepted in future venues.\n\n\n\n", "title": "A collection of offline datasets", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "woE-2zxOdE0": {"type": "review", "replyto": "px0-N3_KjA", "review": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning\nreview:\n\nsummarization:\n\nIn this paper, the authors consider the problems of offline reinforcement learning problems,\n and has a focus of dataset and shareable code base.\nWhile no novel algorithms are proposed in this project,\na systematic evaluation of existing algorithms (offline RL algorithms) is proposed.\n\nPros:\n1. Offline RL is a hot topic this year, \nwith a lot of papers exploring efficient and robust ways to utilize offline collected data.\nThis paper, while using simulated data, \nprovides a general platform to benchmark these algorithms.\n\n2. The project provides a comprehensive evaluation and discussion on existing considerations in offline RL.\nIt provides several interesting directions in offline RL.\n\n3. The paper is well-written.\nIt is very clear what the purpose of the project is,\nand it is very clear why the authors make the dataset in the way they did.\n\nCons:\n\n1. The dataset is mostly simulated.\nWhile I understand the difficulty of collection real-data, \nit does raise some concerns that a simulated dataset can be generated by researchers themselves.\n\nSummary:\nWhile the dataset is simulated,\nI do think there\u2019s value in the dataset and the shared code-base to facilitate recent progress in offline RL research.\n", "title": "Review from reviewer 3", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}