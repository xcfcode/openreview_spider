{"paper": {"title": "Lossy Image Compression with Compressive Autoencoders", "authors": ["Lucas Theis", "Wenzhe Shi", "Andrew Cunningham", "Ferenc Husz\u00e1r"], "authorids": ["ltheis@twitter.com", "wshi@twitter.com", "acunningham@twitter.com", "fhuszar@twitter.com"], "summary": "A simple approach to train autoencoders to compress images as well or better than JPEG 2000.", "abstract": "We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.", "keywords": ["Computer vision", "Deep learning", "Applications"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper optimizes autoencoders for lossy image compression. Minimal adaptation of the loss makes autoencoders competitive with JPEG2000 and computationally efficient, while the generalizability of trainable autoencoders offers the added promise of adaptation to new domains without domain knowledge.\n The paper is very clear and the authors have tried to give additional results to facilitate replication. The results are impressive. While another ICLR submission that is in the same space does outperform JPEG2000, this contributions nevertheless also offers state-of-the-art performance and should be of interest to many ICLR attendees."}, "review": {"SJtXRzJFx": {"type": "rebuttal", "replyto": "SkcvBsmIl", "comment": "Thank you for the correction.\n\nIs it possible for you to share the raw data plotted in figure 4?\n\nThis would help greatly in making comparative plots.", "title": "Thanks"}, "Skg3ZD8Ox": {"type": "rebuttal", "replyto": "H1H2aOove", "comment": "Hi Johannes,\n\n> But did you also use our method of approximating the rate (i.e., additive uniform noise *without* quantization; piecewise linear densities)?\n\nYes, in this figure I was only concerned with the different ways of dealing with the quantization during training, and I used uniform noise without quantization as in your paper. I did not use piecewise linear densities, as I consider that a separate design decision.\n\n> In general, due to all these differences, it seems to me it's too early to call out a \"winner\", and I'm not quite following your claim that your method is better.\n\nI only meant to claim that redefining derivatives works better than uniform noise when everything else is kept the same as in our paper (training procedure, network architecture, etc), which is of course the only thing the figure shows. It may well be that adding uniform noise works better in other contexts, e.g., when the architecture or the entropy model are different (GSMs vs piecewise linear). Sorry if I did not make that clear somewhere. Your overall algorithm clearly performs better, and it will be interesting to figure out the deciding differences.\n\nRegarding RD plots, I am not doubting that different ways of visualizing the same results can influence the perception of these results (this seems true even for the simplest results, e.g. classification accuracy vs classification error), and I agree that having this discussion is important. But it does not mean that the summary plots are categorically invalid or uninformative, only that readers should be made aware of exactly what they are looking at. Only providing results for individual images has its own obvious downsides, namely being harder to digest by not being concise. I will post results for individual images later (or provide them via email for anyone who's interested).\n\n> Why don't you show images with somewhat lower bitrates (and more images in general)? I find that it is very hard to see any differences at all in the images you show in the paper.\n\nWe were focusing on quantitative analyses and realized too late that we need to bring down the bit rates much more to get into the regime of highly visible artefacts (which arguably is not that relevant from a production point of view, but certainly nicer to show in a paper). That said, the images do look visibly different, as confirmed by our MOS test. Note that the differences may be more or less visible depending on the PDF viewer and screen you use. I did not include more images in the paper because the PDF was already approaching 20 MB, but I am happy to provide a full set of compressed images on request.\n\nThanks for your feedback!", "title": "Re: Some clarifications"}, "H1H2aOove": {"type": "rebuttal", "replyto": "BJCMqXBUg", "comment": "Hi Lucas,\n\nto answer your question: yes, we optimize a separate set of transforms for each lambda and always pick one of them ahead of encoding, whereas you appear to optimize three different sets of transforms and then test and select one of them during runtime, which is a form of signal adaptiveness (similar to our use of adaptive entropy coding).\n\nRe figure 10: I'm still unsure how to interpret the figure. I assume that you trained a set of transforms for each method, and then generated the curves in the same way you did before for your method. When you trained the models, you certainly substituted our method of relaxing the distortion term. But did you also use our method of approximating the rate (i.e., additive uniform noise *without* quantization; piecewise linear densities)? If you substitute only the distortion term, you're not comparing to our method, but to some hybrid method, which might be worse than both of our proposals.\n\nWe also didn't mention that the optimization method is somewhat different (mostly due to your \"incremental\" scheme). What if our method just requires slower training, etc.? In general, due to all these differences, it seems to me it's too early to call out a \"winner\", and I'm not quite following your claim that your method is better. (I do think both papers have some unique aspects to contribute to the field and would be happy if the reviewers decide to accept both of them.)\n\nWith respect to the rate-distortion plots: We found that averaging over identical rates versus identical distortions can distort the appearance of the RD-plot in a way that it overemphasizes or underemphasizes high bit rates vs. low bit rates (see appendix of our paper). Thus, even if you take care that you generate the plot in exactly the same way for all methods, it might mislead you to think that some method is particularly well suited for the high- or low-bitrate regime, etc. Have you taken that into account? Also, note that starting with the very first version of our paper, we documented the way we generated the plot in the appendix, but some other references haven't done that. So there's a chance people can come to the wrong conclusion comparing plots across papers. We decided to enable a comparison based on individual images and stay away from averaged plots for now, until there is an agreed-upon solution.\n\nFinally, regarding that last point: Why don't you show images with somewhat lower bitrates (and more images in general)? I find that it is very hard to see any differences at all in the images you show in the paper.\n\nBest,\nJohannes.\n", "title": "Some clarifications"}, "SJATQCkwe": {"type": "rebuttal", "replyto": "rJiNwv9gg", "comment": "Dear reviewers, we made the following changes to our paper:\n\n\u2013 added direct comparison with VAE/quantization approximation of Balle et al. (Figure 10)\n\u2013\u00a0added another control to Figure 3 (incremental training vs fixed small learning rate)\n\u2013\u00a0added a motivation and reference for MOS tests\n\u2013\u00a0added more detail to appendix to make reimplementation easier\n\u2013 improved caption of Figure 3A\n\nMinor:\n\u2013\u00a0added number of scales used in Gaussian scale mixtures\n\u2013\u00a0fixed reference to Figure 4 (which pointed to Figure 9 before)\n\nToderici et al. kindly provided us with their results which include entropy encoding*. We will rerun MOS experiments and intend to update quantitative comparison figures before the meeting.\n\n* https://drive.google.com/file/d/0B-n7IsUzdeWOb0dNSm42aEp5aE0/view", "title": "Revision"}, "BJCMqXBUg": {"type": "rebuttal", "replyto": "SkHHgWE8e", "comment": "Hi Eero,\n\nI appreciate your feedback!\n\nRegarding \u201cIn our paper, the substitution of additive uniform noise in place of quantization is done *only* for optimization\u201d. This is true for the figure I posted: For evaluation, actual quantization was used and no noise added. I am sorry if I didn\u2019t make this clear, it didn\u2019t occur to me that there was an alternative way of doing it.\n\nRegarding differences between your implementation and ours: Note that I mentioned two of the three differences you described. I didn\u2019t go into the neural network architecture, since I was mainly concerned with differences affecting qualitative performance, and you state in your paper that similar compression can be achieved using an architecture similar to ours, albeit less efficiently [1]. In preliminary experiments we found 5-10% reduction in bit rate using very simple context-free adaptive entropy encoding, so I assumed your use of CABAC would play a bigger role than the updated version of your paper shows.\n\nWhere I am not sure about your implementation is whether you compute a separate encoder/decoder pair for each lambda. If that is the case, a further difference would be the number of transformations used.\n\nRegarding summary plots, I agree that it\u2019s easy to get these wrong, although I am less pessimistic about their value if the same averaging is applied to all methods. If all curves are averaged at the same rate and also plotted against rate, the curves are easy to interpret as each point along the curve simply corresponds to the average distortion at that rate. To be able to measure distortion at arbitrary rates, we interpolated the curves for each image first. Interpolating and averaging has its own intricacies, which can be taken care of by measuring the actual rate-distortion at enough points for each method (we used 20 points for JPEG and JPEG 2000).\n\n[1] \u201cPreliminary results indicate that qualitatively similar results are achievable with other activation functions we tested, but that rectified linear units generally require a substantially larger number of model parameters to achieve the same rate\u2013distortion performance as the GDN/IGDN nonlinearities\u201d", "title": "Re: Clarifications regarding Balle et. al."}, "SkHHgWE8e": {"type": "rebuttal", "replyto": "H1RGutpNx", "comment": "Dear Lucas & AnonReviewer1,\n\nSince our ICLR submission (https://openreview.net/forum?id=rJxdQ3jeg) has been discussed in this thread, we felt we should comment on a few of the claims that have been made.  Note that none of us are reviewers, official or unofficial, for this paper.  In our view, both papers offer interesting contributions, and we think the ICLR meeting would benefit from having both presented!\n\n[Theis, 23dec]: Ball\u00e9 et al. submitted closely related work to ICLR (https://openreview.net/forum?id=rJxdQ3jeg). The main difference lies in their treatment of quantization. Together with adaptive entropy encoding, they achieve clear gains over JPEG 2000. We find that when everything else is the same, our treatment of quantization works better (https://ndownloader.figshare.com/files/7260857). Together, these results show that it is possible to significantly outperform JPEG 2000 using our approach.\n\nOur implementation differs in a number of ways from that of the current paper: we use a 3-stage generalized divisive normalization transform to map the image data into a space suitable for (uniform) quantization, an objective function based on continuous relaxation of the quantizer to an additive additive noise process, and an adaptive entropy coder.  This coder is adaptive to the statistics of individual images, but only weakly spatially adaptive within images (unlike JPEG 2000, which uses a much more sophisticated form of spatially adaptive coder).  In response to one of the reviews of our paper, we computed the entropy of the encoded test images from the Kodak set, assuming the probability model learned from the training set. As we expected, these entropies, which should closely approximate the performance of a *non-adaptive* entropy coder, are only slightly worse (on average) than the bit rates obtained with our adaptive entropy coder (see http://www.cns.nyu.edu/~balle/kodak-entropy2-psnr.pdf).  We conclude that most of the rate-distortion performance of our system arises from the 3-stage GDN transform.\n\nDespite this, we want to caution against over-interpreting averaged/summary rate-distortion results, and emphasize that compression results are best evaluated on individual images.  In our case, the average JPEG2000 R-D curve was computed over images encoded at the same *rate*, whereas the average R-D curves for our coder were computed over images encoded at the same *lambda* (R-D slope).  Although this makes no difference for results computed on individual images, it does affect the summary plot.  To illustrate this, we re-computed the average performance of JPEG2000 over images encoded at the same *distortion* level (the implementation allows specification of either distortion or the rate), and found that it gave a different curve, albeit still significantly worse than that of our coder (this plot is included in the appendix of the revised paper).  This discrepancy serves to illustrate the difficulty of providing a \u201ccorrect\u201d method of summarizing coder performance.  We suggest looking at the results for individual images (for our coder, these were made available at http://www.cns.nyu.edu/~lcv/iclr2017).\n\n[Theis, 25dec]: I would appreciate it if the reviewer could take another look at Balle et al.'s paper, as their approach is in fact exactly to replace quantization with additive noise (Equation 5, $\\delta y$ is i.i.d. uniform noise). Thus, our plot is a comparison of Balle's approach (uniform noise) and the CAE.\n\nIn our paper, the substitution of additive uniform noise in place of quantization is done *only* for optimization (i.e., to create a continuously differentiable objective function).  All compression results (the rate-distortion curves and decoded images, both in the paper and online) are computed using actual quantization, followed by entropy coding.  This was explicitly stated in the paper, but might easily have been missed (we\u2019ve emphasized it more in the revision).  It appears that the rate-distortion plot linked in the response paragraph above, and also the images presented in figure 1 of this paper, may have been computed using additive uniform noise, and if so, are not a correct characterization of our coding method.  In the end, what matters is rate-distortion performance given actual quantization, not the rate-distortion performance under the proxy objective.", "title": "Clarifications regarding Balle et. al."}, "SkcvBsmIl": {"type": "rebuttal", "replyto": "B1gxqOM8e", "comment": "Thanks for your feedback and catching a mistake. The beta values we reported indeed do not match the loss function in Equation 2. The correct values to plug into that equation are (1/0.01 - 1), (1/0.05 - 1), (1/0.2 - 1). (The loss function we used is \\beta L + (1 - \\beta) D, hence the numbers.) \n\nRegarding the y-axis in Figure 3 and Figure 4. For the loss function the log-likelihood was measured in nats per input dimension, and the MSE was scaled by 1/1000 so that the beta parameter is in a more intuitive range. I.e., to reimplement the training one needs to scale the two terms in a way not specified in the paper. We didn't include those details to keep the paper uncluttered, but will try to add this information to the appendix to make reimplementation easier.", "title": "Correct beta values"}, "B1gxqOM8e": {"type": "rebuttal", "replyto": "rJiNwv9gg", "comment": "Thank you for the important work. I'm trying to reproduce the results.\n\nThe loss function in eq 2. scales the distortion by a beta hyper parameter. In your experiment you mention training three variants, high bit-rate: beta=0.01, medium: 0.05 and low: 0.2\n\nSo, the high bit-rate variant weights the distortion the least? That does not make sense to me. Did you mean 1/0.01, 1/0.05 and 1/0.2 or something else?\n\nAlso, figure 3 B shows the loss on the y-axis. This goes from 0.01 to 0.03. How can the loss be so low if it includes -logQ, which from figure 4 seems to be around 0.3 to 2.0 (bpp), if I'm interpreting this right.\n\nAlso, you mention you use mean squared error, but not on which scale. Do you calculate mse on the normalized images or on the input/output uint8 images?", "title": "Loss function"}, "B1MXl1_Hl": {"type": "rebuttal", "replyto": "BJ7G-SpVl", "comment": "Thanks!\n\nThe cross-entropy was the missing piece for me. We're not minimizing H(Q), but H(P, Q) which is an upper bound on H(P). H(P,Q) = H(P) + KL(P||Q)\n\nYou're right, in order to compress the z's, we'll need a probabilistic model of z, which is our Q model, and we can achieve H(P,Q)+2 bits compression using arithmetic coding.\n\nI guess a further reason to not use the histograms of the z's from P to estimate the entropy is that it underestimates the entropy of P.\n\nAgain, thanks. I might actually understand this paper now.", "title": "Thanks"}, "BJdFyK64l": {"type": "rebuttal", "replyto": "HyY4AIpEg", "comment": "Yes, we use Gaussian scale mixtures while Balle et al. use a piece-wise linear density. This is in no way means that Balle et al. optimized the encoder for adaptive encoding, as claimed earlier by the reviewer.\n\nI would appreciate it if the reviewer could take another look at Balle et al.'s paper, as their approach is in fact exactly to replace quantization with additive noise (Equation 5, $\\delta y$ is i.i.d. uniform noise). Thus, our plot is a comparison of Balle's approach (uniform noise) and the CAE.\n\nInstead of a comparison with generative models, we provide a much more relevant comparison with Toderici et al. (2016b), for which code and results on the Kodak dataset are available. We went to great lengths and unlike Balle et al. or Toderici et al. even performed a psychophysical evaluation.", "title": "Re: Wrt Balle et al."}, "BJ7G-SpVl": {"type": "rebuttal", "replyto": "SJcJxESEl", "comment": "E[-log Q(z)] is an upper bound on the entropy H[P(z)] = E[-log P(z)] (https://en.wikipedia.org/wiki/Cross_entropy). While this doesn't guarantee that we are minimizing the entropy, it guarantees that the entropy is always smaller than the estimated rate.\n\nNote that from a practical point of view, the entropy is not as interesting as E[-log Q(z)], since the former only corresponds to the theoretically achievable compression performance, while the latter corresponds to the actual compression performance.\n\nUsing histograms, we can estimate E[-log Q(z_i)] without range coding (but not the entropy of the very high-dimensional z). The result is only slightly smaller than using range coding, but performing range coding provides a fairer comparison with other compression algorithms which also have to perform an encoding.", "title": "Upper bound"}, "H1ZTc4T4e": {"type": "rebuttal", "replyto": "rJOI9C9Ve", "comment": "The reviewer is mistaken. Just as in our work, Balle et al. optimize for an independent code (bottom at page 3; Equation 5). During optimization, only the marginals are modeled and adaptive entropy coding, which exploits dependencies, is not taken into account. We can apply adaptive entropy encoding posthoc, just as in their work.\n\nThe reviewer describes Balle et al.\u2019s main contribution as their ability to back-propagate [through] the rate. I am confused by this statement since the main contribution of our paper is exactly that. Furthermore, Balle\u2019s approach was published in an earlier paper, so this is in fact not even a contribution of their current submission. Our approach is novel and works better (again, this is for two deep models using the same training procedure, only swapping the way rates are back-propagated: https://ndownloader.figshare.com/files/7260857).\n\nBoth our paper and Balle\u2019s paper discuss under which cases their approach is equivalent to a variational autoencoder. What does the reviewer mean by \u201cdiscriminatively trained\u201d? For certain choices, optimizing the lower bound of a variational autoencoder is exactly optimizing a certain rate-distortion trade-off as done by Balle. We can expect any deviation from those choices to only make the generative model worse, since it wouldn\u2019t be optimizing a rate-distortion trade-off.", "title": "Re: Wrt Balle et al."}, "SyleoaqNe": {"type": "rebuttal", "replyto": "ByWOTZMNe", "comment": "Dear Reviewer 1, thanks again for reviewing our paper. We would be grateful if you could reconsider your score after taking into account the following points addressing some of your concerns:\n\n1. While the encoder is non-linear, we will include estimates of the filters of a linear approximation to the encoder so that they can be compared with JPEG 2000. (Preliminary noisy estimates of some of the filters: https://ndownloader.figshare.com/files/7260815\u00a0\u2013\u00a0note that the estimates are noisy, not the filters themselves).\n\n3. In a sense we are giving JPEG 2000 an unfair advantage, since its representations are encoded using adaptive entropy coding, and CAE\u2019s are encoded using range coding with simple i.i.d. assumption. Additional gains can be achieved by adding adaptive entropy coding, but unfortunately no easy-to-use implementation exists (we are working on it).\n\nBall\u00e9 et al. submitted closely related work to ICLR (https://openreview.net/forum?id=rJxdQ3jeg). The main difference lies in their treatment of quantization. Together with adaptive entropy encoding, they achieve clear gains over JPEG 2000. We find that when everything else is the same, our treatment of quantization works better (https://ndownloader.figshare.com/files/7260857). Together, these results show that it is possible to significantly outperform JPEG 2000 using our approach.\n\nWe will add this comparison to the paper. Since Ball\u00e9\u2019s approach is equivalent to a particular VAE, this should also qualify as a comparison to a well-crafted generative model. Ball\u00e9\u2019s submission also shows that this topic is of strong interest to the ICLR community.", "title": "Beating JPEG 2000"}, "SJcJxESEl": {"type": "rebuttal", "replyto": "B1HPClkEg", "comment": "Thank you for the explanation. That was indeed the source of confusion.\n\nThis leaves me with a somewhat related question though\n\nIn the following I'm assuming what we'll send to the decoder at test time is the integers from P(z), i.e. the integers actually produced by the encoder (after rounding).\n\nIf Q(z) is a parametric approximation to P(z), then do we have any kind of guarantees that we'll minimize the entropy of P(z) by minimizing the bound in eq. 8?\n\nFor example, in the first iteration, the Q model is probably a very bad estimator of P(z), and I can construct an encoder such that the entropy of P(z) is both greater and less than the the bound in eq. 8 (e.g. Encoder1(z|x) = uniform(Z) (maximum entropy) and Encoder2(z|x) = 0 (zero entropy)).\n\nKnowing Q(z) can we say anything about P(z)?\n\nAlso, why do you use an implementation of a range coder to measure the entropy of the codes? Can't you calculate the (empirical) entropy of P(z) from the histograms over the training or test set and use that, since this should be the lower bound of compressibility?", "title": "Thanks"}, "B1N_F-yNx": {"type": "rebuttal", "replyto": "BJ9AS_CQe", "comment": "Thanks!\n\n1. The scale parameters are trained using Adam (like the other parameters) while keeping the encoder and decoder fixed (but not the GSMs). Some details can be found at the bottom of page 6.\n\nFigure 3A shows the results of this training. Each blue dot corresponds to the (logarithm of a) scale parameter of a coefficient. The row of blue dots near zero was trained for the same rate-distortion trade-off that the rest of the parameters were trained for. The log-scales are therefore log(1) = 0, since no correction is necessary. We will update the caption of Figure 3A to make it more clear.\n\nThe x-axis corresponds to the coefficient index (the scales/coefficients appear ordered because of the incremental training).\n\n2. I don\u2019t have a clear answer to this question, but the intuition behind incremental training stems from the different coefficients having very different variances, which causes problems for gradient descent not fully addressed by Adam. There are probably other ways to deal with this problem (e.g., Newton-like methods or variance normalization similar to batch normalization).\n\nYes, the training becomes more stable due to the decrease in learning rate. Using the smaller learning rate from the start makes the performance worse (we will add another curve to the figure).\n\n3. We used 6 scales (we will add this detail to the paper). The parameters of the Gaussian scale mixtures were simply optimized together with the other parameters (encoder and decoder or scales) using Adam; no EM or other special treatment. To ensure positivity, we optimized log-scales rather than scales.", "title": "1. Adam; 2. different variances of coefficients; 3. 6 scales, Adam/no EM"}, "B1HPClkEg": {"type": "rebuttal", "replyto": "BybZPt0Xl", "comment": "Thank you for your question.\n\nEquation 7 is not an assumption but a definition. Note that Q(z) is a model distribution and not the actual distribution over coefficients (if this was the cause of confusion, we will try to make it more clear in a revision). If for entropy coding we use the discrete distribution which results from integration, then there is no further approximation.\n\nFor simplicity, we use histograms at test time, so there is in fact a small discrepancy. However, this discrepancy is not necessary. We could parametrize q so that Q corresponds to a histogram or integrate the GSMs to compute Q.\n\nThe KL divergence between Q(z) = int q(z + u) du and the data distribution over z, P(z), corresponds to the number of bits we waste by not modeling the distribution of coefficients perfectly (e.g. ignoring remaining dependencies). However, adding this KL divergence would not be helpful, since we are not trying to optimize the theoretically achievable compression performance, but the number of bits we would actually get using our model (e.g. making independence assumptions).\n\nLet me know if this explanation isn\u2019t clear or I got your interpretation of our equations wrong.", "title": "Q is a model distribution, not the data distribution"}, "BJ9AS_CQe": {"type": "rebuttal", "replyto": "rJiNwv9gg", "comment": "Great and very interesting work. I have some questions.\n\n1. It seems that the introduced scale parameters for different bit-rates are also trainable. How do you train and update the scale parameters for different bit-rates? Could you explain it in more details? Also, could you elaborate more about Figure 3A? What does the x-axis of the graph means? \n\n2. What was the intuition for incremental training? Do you have some insights why incremental manner is beneficial for training? Also, for figure 3B, it seems that the loss are not quite stable before 100 thousand iterations but suddenly the loss become very stable. Is this due to decreased learning rate at that point? \n\n3. Regarding the GSM modeling of quantized coefficients, how many mixtures did you use and how did you train GSM (e.g., EM algorithm) ? Also, isn't it a burden to train GSM to fit the quantized coefficients at every iteration? \n\nThanks.", "title": "Some questions regarding the paper"}, "BybZPt0Xl": {"type": "rebuttal", "replyto": "rJiNwv9gg", "comment": "Thank you for this important and inspiring work.\n\nI have some questions regarding the bound in eq. 8.\n\nMy understanding:\n\nThe bound in eq. 8 is used in the loss function as a proxy for the non-differentiable entropy of the codes. If the bound is minimized, a bound on the entropy is minimized.\n\nThe bound in eq. 8 comes directly from the equality in eq. 7, by applying Jensen's inequality. So, the bound only holds if the equality in eq. 7 holds.\n\nHowever, q(z+u) is a parametric approximation (the GSM model) of Q(z), not an equality.\n\nDoes the bound still hold? Under which circumstances?\n\n(I have a feeling that the missing term on the RHS of eq. 8 to make the bound an equality is a KL term between int q(z+u) du and Q(z), in which case the bound still holds as the KL is positive, but I have not been able to show it.)\n", "title": "Regarding eq 8"}, "S1LJvvEXl": {"type": "rebuttal", "replyto": "Bya4TEyXx", "comment": "Dear Reviewer 3, thank you for taking the time to review our paper.\n\nThe motivation for mirror-padding in the pixel space is that this leads to an image with more natural statistics than zero-padding. Similarly, zero-padding in the space of coefficients is natural, since the coefficients are sparsely distributed. In our paper we used zero-padded convolutions in each layer of the decoder. However, a better motivated choice would have been to (analogously to the encoder) zero-pad the coefficients once and then use \u201cvalid convolutions\u201d. We find that both options work equally well, but might update the paper with this slightly more elegant although slightly slower architecture.\n\nWe considered annealing but didn\u2019t experiment enough with it to be able to say anything about it\u2019s effects on training performance.\n\nWe agree that there would be better ways to evaluate perceptual quality. The motivation for using mean opinion scores with a 1-5 rating is that this is (unfortunately) the industry standard. We hope to replace it in future work after gaining experience with other psychophysical paradigms.", "title": "Mirror-padding/zero-padding leads to more natural statistics"}, "S1OnIwV7l": {"type": "rebuttal", "replyto": "HJKOo0ZXg", "comment": "Dear reviewer 2, thank you for taking the time to review our paper.\n\nInstead of the identity, we tried using a particular smooth approximation to the rounding function, and increasing the steepness/closeness of the approximation over the course of the training. The performance was the same as the performance achieved using the identity.\n\nRegarding batch size, not that the first sentence of Section 3.2 says that \u201cAll models were trained using [\u2026] batches of 32 images\u201d.\n\nRegarding incremental training, note that we already included a comparison of incremental training and non-incremental training Figure 3B.\n\nRegarding MOS scores, the main motivation for using mean opinion scores with a rating from 1-5 is that this is a widely used industry standard. We agree that there should be better ways to do perceptual evaluations (e.g. 2AFC tasks, ranking), and we hope to explore these and replace MOS testing in the future. We will explain our motivation for using MOS tests in a revised manuscript.\n\nThe raters were instructed not to zoom into the pictures, but for natural viewing conditions were otherwise free to view the images for however long or from whatever distance they liked.", "title": "Batch size and numbers for incremental training"}, "Hk-IUDEmx": {"type": "rebuttal", "replyto": "SyghNnJmg", "comment": "Dear Reviewer 1, thanks for taking the time to review our paper.\n\nYes, it should be Figure 4 instead of Figure 9 on page 7, thanks for catching that.\n\nRegarding your first comment, first note that we do not claim that CAE is always better than JPEG 2000. We claim that we are able to \u201ctrain deep autoencoders competitive with JPEG 2000\u201d (abstract), and that \u201cwe achieve performance similar to or better than JPEG 2000 when evaluated for perceptual quality\u201d (introduction). We also point out that \u201cin terms of PSNR, our method performs [\u2026] slightly worse at low and medium bit rates\u201d (Section 3.3). Secondly, note that the ultimate test for image compression has to be perceptual quality, and PSNR and SSIM should only be viewed as an approximation to it. In terms of perceptual quality as measured by a MOS test, we found a statistically significant albeit small improvement over JPEG 2000. That said, further improvements could easily be gained by using adaptive entropy encoding (as done by JPEG 2000) instead of simple range coding as we do now.\n\nRegarding your second point, we agree that it would be interesting to evaluate the effect of artefact removal on the rate-distortion curve and we will try to include such a comparison in a later version of our manuscript. However, note that the achievement described in this paper is not a new state-of-the-art encoding scheme (better algorithms than JPEG 2000 exist), but a simple solution to the unsolved problem of learning a useful encoder at all. This is a non-trivial contribution in itself, since it is not clear how to best deal with the non-differentiability in compression. It enables the use of deep learning where before a lot of expertise in signal processing was necessary to come up with a new encoder.\n\nWe will add encoding and decoding times to the paper. However, fair comparisons of any two algorithms are tricky, since the running time depends so heavily on the implementation. Some things to consider are that highly optimized and specialized (but not necessarily widely used) implementations of JPEG 2000 exist, while we used a general purpose framework designed for training but not for deployment.\n\nFinally, while our main contribution is conceptual, we are confident that our work will lead to state-of-the-art compression algorithms. We are currently exploring architectures and training better tuned for the low bit rate regime as well as introducing more sophisticated entropy encoding schemes (as used by other compression algorithms such as JPEG 2000). Speedups may be possible at little performance loss, for example through training separable filters.", "title": "Reply"}, "HJKOo0ZXg": {"type": "review", "replyto": "rJiNwv9gg", "review": "You say \"Empirically, we found the identity r(y)=y to work as well as more sophisticated choices\" -- would it be possible to explain which approaches did you actually try, and how well did they perform under your architecture?\n\nWhen talking in 3.2 about the number of updates, do you assume a batch size of 1? If not, it may be worth mentioning what was the batch.\n\nAlso in 3.2, you mention \"incremental bit enabling\" and very briefly say that it works \"better\". Do you have any numbers we could compare? It's not clear \"how much better\" does this trick achieve, otherwise.\n\nRegarding the MOS setup:\n1. What motivated using a discrete scoring scale with no reference? \n2. Have you considered using triplets?\n3. What conditions did the raters have? Was there a strict protocol in place to ensure a constant distance from the monitor? \n4. Was the monitor and lighting conditions the same for all raters?This work proposes a new approach for image compression using auto encoders. The results are impressive, besting the state of the art in this field.\n\nPros:\n+ Very clear paper. It should be possible to replicate these results should one be inclined to do so.\n+ The results, when compared to other work in this field are very promising. I need to emphasize, and I think the authors should have emphasized this fact as well: this is very new technology and it should not be surprising it's not better than the state of the art in image compression. It's definitely better than other neural network approaches to compression, though.\n\nCons:\n- The training procedure seems clunky. It requires multiple training stages, freezing weights, etc.\n- The motivation behind Figure 1 is a bit strange, as it's not clear what it's trying to illustrate, and may confuse readers (it talks about effects on JPEG, but the paper discusses a neural network architecture, not DCT quantization)", "title": "Some more questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SkJi7LQEg": {"type": "review", "replyto": "rJiNwv9gg", "review": "You say \"Empirically, we found the identity r(y)=y to work as well as more sophisticated choices\" -- would it be possible to explain which approaches did you actually try, and how well did they perform under your architecture?\n\nWhen talking in 3.2 about the number of updates, do you assume a batch size of 1? If not, it may be worth mentioning what was the batch.\n\nAlso in 3.2, you mention \"incremental bit enabling\" and very briefly say that it works \"better\". Do you have any numbers we could compare? It's not clear \"how much better\" does this trick achieve, otherwise.\n\nRegarding the MOS setup:\n1. What motivated using a discrete scoring scale with no reference? \n2. Have you considered using triplets?\n3. What conditions did the raters have? Was there a strict protocol in place to ensure a constant distance from the monitor? \n4. Was the monitor and lighting conditions the same for all raters?This work proposes a new approach for image compression using auto encoders. The results are impressive, besting the state of the art in this field.\n\nPros:\n+ Very clear paper. It should be possible to replicate these results should one be inclined to do so.\n+ The results, when compared to other work in this field are very promising. I need to emphasize, and I think the authors should have emphasized this fact as well: this is very new technology and it should not be surprising it's not better than the state of the art in image compression. It's definitely better than other neural network approaches to compression, though.\n\nCons:\n- The training procedure seems clunky. It requires multiple training stages, freezing weights, etc.\n- The motivation behind Figure 1 is a bit strange, as it's not clear what it's trying to illustrate, and may confuse readers (it talks about effects on JPEG, but the paper discusses a neural network architecture, not DCT quantization)", "title": "Some more questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SyghNnJmg": {"type": "review", "replyto": "rJiNwv9gg", "review": "In the last para on page 7, it should read Figure 4 instead of 9 right ?\n\nThe curves in Fig 4 alone do not seem to back up the claim that CAE does better than JPEG2000---the only noticeable difference appears to be in SSIM at higher bit-rates, and I feel the difference may be too small to be meaningful (especially for values > 0.9). Therefore, I think the paper should back up the claim with more visual examples (like the last two rows of Fig 5). It wouldn't be a bad idea to add error bars to Fig 4 as well.\n\nMoreover, I feel a fair comparison would be to the outputs of neural network trained to reconstruct images from their JPEG2000 encoded versions. You want to do this to show that the network's actually learning a better coding scheme, and not just applying better \"denoising\" during reconstruction.\n\nFinally, what's the running time of compression and decompression vs JPEG2000 ?The paper proposes a neural approach to learning an image compression-decompression scheme as an auto-encoder. While the idea is certainly interesting and well-motivated, in practice, it turns out to achieve effectively identical rates to JPEG-2000.\n\nNow, as the authors argue, there is some value to the fact that this scheme was learned automatically rather than by expert design---which means it has benefits beyond the compression of natural images (e.g., it could be used to automatically learning a compression scheme for signals for which we don't have as much domain knowledge). However, I still believe that this makes the paper unsuitable for publication in its current form because of the following reasons---\n\n1. Firstly, the fact that the learned encoder is competitive---and not clearly better---than JPEG 2000 means that the focus of the paper should more be about the aspects in which the encoder is similar to, and the aspects in which it differs, from JPEG 2000. Is it learning similar filters or completely different ones ? For what kinds of textures does it do better and for what kinds does it do worse (the paper could show the best and worst 10 patches at different bit-rates) ?\n\n2. Secondly, I think it's crucial that the paper demonstrate that the benefits come from a better coding scheme (as opposed to just a better decoder), as suggested in my initial pre-review question. How would a decoder trained on JPEG-2000 codes (and perhaps also on encoded random projections) do worse or better ?\n\n3. Finally, I think the fact that it does as well/worse than JPEG-2000 significantly diminishes the case for using a 'deep' auto-encoder. JPEG-2000 essentially uses a wavelet transform, which is a basis that past studies have shown could be recovered using a simple sparse dictionary algorithm like K-SVD. This is why I feel that the method needs to clearly outperform JPEG-2000, or show comparisons to (or atleast discuss) a well-crafted traditional/generative model-based baseline.", "title": "Vs JPEG2000", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ByWOTZMNe": {"type": "review", "replyto": "rJiNwv9gg", "review": "In the last para on page 7, it should read Figure 4 instead of 9 right ?\n\nThe curves in Fig 4 alone do not seem to back up the claim that CAE does better than JPEG2000---the only noticeable difference appears to be in SSIM at higher bit-rates, and I feel the difference may be too small to be meaningful (especially for values > 0.9). Therefore, I think the paper should back up the claim with more visual examples (like the last two rows of Fig 5). It wouldn't be a bad idea to add error bars to Fig 4 as well.\n\nMoreover, I feel a fair comparison would be to the outputs of neural network trained to reconstruct images from their JPEG2000 encoded versions. You want to do this to show that the network's actually learning a better coding scheme, and not just applying better \"denoising\" during reconstruction.\n\nFinally, what's the running time of compression and decompression vs JPEG2000 ?The paper proposes a neural approach to learning an image compression-decompression scheme as an auto-encoder. While the idea is certainly interesting and well-motivated, in practice, it turns out to achieve effectively identical rates to JPEG-2000.\n\nNow, as the authors argue, there is some value to the fact that this scheme was learned automatically rather than by expert design---which means it has benefits beyond the compression of natural images (e.g., it could be used to automatically learning a compression scheme for signals for which we don't have as much domain knowledge). However, I still believe that this makes the paper unsuitable for publication in its current form because of the following reasons---\n\n1. Firstly, the fact that the learned encoder is competitive---and not clearly better---than JPEG 2000 means that the focus of the paper should more be about the aspects in which the encoder is similar to, and the aspects in which it differs, from JPEG 2000. Is it learning similar filters or completely different ones ? For what kinds of textures does it do better and for what kinds does it do worse (the paper could show the best and worst 10 patches at different bit-rates) ?\n\n2. Secondly, I think it's crucial that the paper demonstrate that the benefits come from a better coding scheme (as opposed to just a better decoder), as suggested in my initial pre-review question. How would a decoder trained on JPEG-2000 codes (and perhaps also on encoded random projections) do worse or better ?\n\n3. Finally, I think the fact that it does as well/worse than JPEG-2000 significantly diminishes the case for using a 'deep' auto-encoder. JPEG-2000 essentially uses a wavelet transform, which is a basis that past studies have shown could be recovered using a simple sparse dictionary algorithm like K-SVD. This is why I feel that the method needs to clearly outperform JPEG-2000, or show comparisons to (or atleast discuss) a well-crafted traditional/generative model-based baseline.", "title": "Vs JPEG2000", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Bya4TEyXx": {"type": "review", "replyto": "rJiNwv9gg", "review": "* What is the motivation for using mirror-padding and valid convolutions in the encoder but zero-padded convolutions in the decoder?\n* In figure 3B there are large jumps in the loss even for non-incremental training. Do you have any insight into what the cause might be?\n* The masked incremental training is somewhat reminiscent of the common practice for VAEs of layering in the KL term. Did you try the analogous approach of multiplying the log Q term of Eq. 2 by a scalar that transitions from 0 to 1 over the course of training?\n* In the human evaluation, is there a particular reason for conducting a 1-5 rating experiment as opposed to a more direct experiment such as pairwise comparison or ranking of the images?This paper proposes an autoencoder approach to lossy image compression by minimizing the weighted sum of reconstruction error and code length. The architecture consists of a convolutional encoder and a sub-pixel convolutional decoder. Experiments compare PSNR, SSIM, and MS-SSIM performance against JPEG, JPEG-2000, and a recent RNN-based compression approach. A mean opinion score test was also conducted.\n\nPros:\n+ The paper is clear and well-written.\n+ The decoder architecture takes advantage of recent advances in convolutional approaches to image super-resolution.\n+ The proposed approaches to quantization and rate estimation are sensible and well-justified.\n\nCons:\n- The experimental baselines do not appear to be entirely complete.\n\nThe task of using autoencoders to perform compression is important and has a large practical impact. Though directly optimizing the rate-distortion tradeoff is not an entirely novel enterprise, there are enough differences (e.g. the quantization approach and sub-pixel convolutional decoder) to sufficiently distinguish this from earlier work. I am not an image compression expert but the approach and results both seem compelling. The main shortcoming is that the implementation of Toderici et al. 2016b appears to be incomplete, and there is no comparison to Balle et al. 2016. Overall, I feel that the fact that this architecture achieves competitive performance with JPEG-2000 while simultaneously setting the stage for future work that varies the encoder/decoder size and data domain means the community will find this work to be of significant interest.\n\nI have no further specific comments at this time as they were answered sufficiently in the pre-review questions.", "title": "Pre-review questions", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SyYe3NX4g": {"type": "review", "replyto": "rJiNwv9gg", "review": "* What is the motivation for using mirror-padding and valid convolutions in the encoder but zero-padded convolutions in the decoder?\n* In figure 3B there are large jumps in the loss even for non-incremental training. Do you have any insight into what the cause might be?\n* The masked incremental training is somewhat reminiscent of the common practice for VAEs of layering in the KL term. Did you try the analogous approach of multiplying the log Q term of Eq. 2 by a scalar that transitions from 0 to 1 over the course of training?\n* In the human evaluation, is there a particular reason for conducting a 1-5 rating experiment as opposed to a more direct experiment such as pairwise comparison or ranking of the images?This paper proposes an autoencoder approach to lossy image compression by minimizing the weighted sum of reconstruction error and code length. The architecture consists of a convolutional encoder and a sub-pixel convolutional decoder. Experiments compare PSNR, SSIM, and MS-SSIM performance against JPEG, JPEG-2000, and a recent RNN-based compression approach. A mean opinion score test was also conducted.\n\nPros:\n+ The paper is clear and well-written.\n+ The decoder architecture takes advantage of recent advances in convolutional approaches to image super-resolution.\n+ The proposed approaches to quantization and rate estimation are sensible and well-justified.\n\nCons:\n- The experimental baselines do not appear to be entirely complete.\n\nThe task of using autoencoders to perform compression is important and has a large practical impact. Though directly optimizing the rate-distortion tradeoff is not an entirely novel enterprise, there are enough differences (e.g. the quantization approach and sub-pixel convolutional decoder) to sufficiently distinguish this from earlier work. I am not an image compression expert but the approach and results both seem compelling. The main shortcoming is that the implementation of Toderici et al. 2016b appears to be incomplete, and there is no comparison to Balle et al. 2016. Overall, I feel that the fact that this architecture achieves competitive performance with JPEG-2000 while simultaneously setting the stage for future work that varies the encoder/decoder size and data domain means the community will find this work to be of significant interest.\n\nI have no further specific comments at this time as they were answered sufficiently in the pre-review questions.", "title": "Pre-review questions", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}