{"paper": {"title": "Network of Graph Convolutional Networks Trained on Random Walks", "authors": ["Sami Abu-El-Haija", "Amol Kapoor", "Bryan Perozzi", "Joonseok Lee"], "authorids": ["haija@google.com", "ajk2227@columbia.edu", "bperozzi@acm.org", "joonseok@google.com"], "summary": "We make a network of Graph Convolution Networks, feeding each a different power of the adjacency matrix, combining all their representation into a classification sub-network, achieving state-of-the-art on semi-supervised node classification.", "abstract": "Graph Convolutional Networks (GCNs) are a recently proposed architecture which has had success in semi-supervised learning on graph-structured data. At the same time, unsupervised learning of graph embeddings has benefited from the information contained in random walks. In this paper we propose a model, Network of GCNs (N-GCN), which marries these two lines of work. At its core, N-GCN trains multiple instances of GCNs over node pairs discovered at different distances in random walks, and learns a combination of the instance outputs which optimizes the classification objective. Our experiments show that our proposed N-GCN model achieves state-of-the-art performance on all of the challenging node classification tasks we consider: Cora, Citeseer, Pubmed, and PPI. In addition, our proposed method has other desirable properties, including generalization to recently proposed semi-supervised learning methods such as GraphSAGE, allowing us to propose N-SAGE, and resilience to adversarial input perturbations.", "keywords": ["Graph Convolution", "Deep Learning", "Network of Networks"]}, "meta": {"decision": "Reject", "comment": "This paper proposes a multiscale variant of Graph Convolutional Networks (GCN) , obtained by combining separate GCN modules using powers of normalized adjacency as generators. The model is tested on several node classification semi-supervised tasks obtaining excellent numerical performance.\n\nReviewers acknowledged the good empirical performance of the model, but all raised the issue of limited novelty, relative to the growing body of literature on graph neural networks. In particular, they missed an analysis that compares random walks powers to other multiscale approaches and justifies its performance in the context of semi-supervised learning. Overall, the AC believes this is a good paper, but it can be significantly stronger with an extra iteration that addresses these limitations. "}, "review": {"BJququIlf": {"type": "review", "replyto": "SkaPsfZ0W", "review": "The paper proposes a novel graph convolutional network in which a variety of random walk steps are involved with multiple GCNs.\n\nThe basic idea, introducing long rage dependecy, would be interesting. Robustness for the feature remove is also interesting.\n\nThe validation set would be important for the proposed method, but for creating larger validation set, labeled training set would become small. How the good balance of training-and-validation can be determined?\n\nDiscussing choice of the degree would be informative. In introducing many degrees (GCNs) for small labeled nodes semi-supervised setting seems to cause over-fitting.", "title": "possibly interesting ideas", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "H1kTclOlf": {"type": "review", "replyto": "SkaPsfZ0W", "review": "In this work a new network of GCNs is proposed. Different GCNs utilize different powers of the transition matrix to capture varying neighborhoods in a graph. As an aggregation mechanism of the GCN modules two approaches are considered: a fully connected layer on top of stacked features and attention mechanism that uses a scalar weight per GCN. The later allows for better interpretability of the effects of varying degree of neighborhoods in a graph.\n\nProposed approach, as authors noted themselves, is quite similar to DCNN (Atwood and Towsley, 2016) and becomes equivalent if the combined GCNs have one layer each. While comparison to vanilla GCN is quite extensive, there is no comparison to DCNN at all. I would be curious to see at least portion of the experiments of the DCNN paper with the proposed approach, where the importance of number of GCN layers is addressed. DCNN did well on Cora and Pubmed when more training samples were used. It also was tested on graph classification datasets, but the results were not as good for some of the datasets. I think that comparison to DCNN is important to justify the importance of using multilayer GCN modules.\n\nSome questions and concerns:\n- I could not quite figure out how many layers did each GCN have in the experiments and how impactful is this parameter \n- Why is it necessary to replicate GCNs for each of the transition matrix powers? In section 4.3 it is mentioned that replication factors r = 1 and r = 4 were used, but it is not clear from Table 2 what are the results for respective r.\n- Early stopping implementation seems a bit too intense. \"We invoke many runs over all datasets\" - how many? Mean and standard deviation are reported for top 3 performers, which is not enough to get a sense of standard deviation and mean. Kipf and Welling (2017) report results over 100 runs without selecting top performers if I understood correctly their setup. Could you please report mean and standard deviation of all the runs? Given relatively small performance improvement (comparatively to GCN), more than 3 (selected) runs are needed for comparison.\n- I liked the attention idea and its interpretation in Fig. 2. Could you please add the error bars for the attention weights. It is interesting to see them shifting towards higher powers of the transition matrix, but also it is important to know if this phenomena is statistically significant.\n- Following up on the previous item - did you try not including self connections when computing transition matrix powers? This way the effect of different degrees of neighborhoods in a graph could be understood better. When self-connections are present, each subsequent transition matrix power contains neighborhoods of lower degrees and interpretation becomes not as apparent.\n\nMinor comments:\n- Understanding of this paper quite heavily relies on the reader knowing Kipf and Welling (2017) paper. Particularly, the comment about approximations derived by Kipf and Welling (2017) in Section 3.3 and how directed graph was converted to undirected (Section 4.1) require a bit more details.\n- I am not quite sure why Section 2.3 is needed. Connection to graph embeddings is not given much attention in the paper later on (except t-SNE picture).\n- Typo in Fig. 1 caption - right and left are mixed up.\n- Typo in footnote on page 3.", "title": "Interesting approach, but lacks justification", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H12U-wYgG": {"type": "review", "replyto": "SkaPsfZ0W", "review": "The paper presents a Network of Graph Convolutional Networks (NGCNs) that uses\nrandom walk statistics to extract information from near and distant neighbors\nin the graph.\n\nThe authors show that a 2-layer Graph Convolutional Network, with linear\nactivation and W0 as identity matrix, reduces to a one-step random walk.\nThey build on this notion to  introduce the idea to make the GCN directly operate\non random walk statistics to better model information across distant nodes.\n\nGiven that it is not clear how many steps of random walk to use a-priori it is\nproposed to make a mixture of models whose outputs are combined by a\nsoftmax classifier, or by an attention based mixing (learning the mixing coefficients).\n\nI find that the comparison can be considered slightly unfair as NGCN has k-times\nthe number of GCN models in it. Did the authors compare with a deeper GCN, or\nsimply with a mixture of plain GCN using one-step random walk?\nThe datasets used for comparison are extremely simple, and I am glad that the\nauthors point out that this is a significant issue for benchmark driven research.\nHowever, doing calibration on a subset of the validation nodes via gradient\ndescent is not very clean as by doing it one implicitly uses those nodes for training.\nThe improvement of the calibrated model on 5 nodes per class (Table 3) seems\nto hint that this peeking into the validation is indeed happening.\n\nThe authors mention that feeding explicitly the information on distant nodes\nmakes learning easier and that otherwise such information it would be hard to\nextract from stacking several GCN layers. While this is true for the small datasets\nusually considered it is not clear at all whether this still holds when we will\nhave large scale graph benchmarks.\n\nExperiments are well conducted but lack a comparison with GraphSAGE and MoNet,\nwhich are the reference models for the selected benchmarks. A comparison would have made the contribution stronger in my opinion. Improvements in performance are minor\nexcept for decimated inputs setting reported in Table 3. In this last case though\nno statistics over multiple runs are shown.\n\nOverall I like the interpretation, even if a bit forced, of GCN as using one-step\nrandom walk statistics. The paper is clearly written.\nThe main issue I have with the approach is that it does not bring a very novel\nway to perform deep learning on graphs, but rather improves marginally upon\na well established one.\n", "title": "Interesting idea to boost GCN.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJGeK9pXG": {"type": "rebuttal", "replyto": "H1kTclOlf", "comment": "We added experiments for PPI dataset (we downloaded it from the GraphSAGE paper).\n\nfrom SAGE authors:\nSAGE-LSTM gets 61.2\nSAGE [i.e. pooling] gets 60.0\n\nOur implementation of SAGE gets 59.8\nOur method (N-SAGE) gets 65.0\n\nResults are added to the table.", "title": "Added PPI dataset (used by GraphSAGE)"}, "HkZAY5T7G": {"type": "rebuttal", "replyto": "H12U-wYgG", "comment": "We added experiments for PPI dataset (we downloaded it from the GraphSAGE paper).\n\nThe PPI dataset has about ~20 times more edges than our previously-largest dataset.\n\nfrom SAGE authors:\nSAGE-LSTM gets 61.2\nSAGE [i.e. pooling] gets 60.0\n\nOur implementation of SAGE gets 59.8\nOur method (N-SAGE) gets 65.0\n\nThis shows (unmodified) random walk indeed help increase performance. Results are added to the table.", "title": "Added PPI dataset, large dataset used by GraphSAGE"}, "SkeGD2hff": {"type": "rebuttal", "replyto": "BJfkzLgxf", "comment": "Thomas,\n\nWe now added experiments to GraphSAGE and also to Network of GraphSAGE. We only used one version of GraphSAGE, which is the mean pooling aggregation, as the authors of GraphSAGE mention that it performs on-par with their max-pooling aggregation model -- we did not try their LSTM aggregation.\n\n\nTLDR:\n\n* GraphSAGE performs better than GCN, when training data is very scarce (e.g. 5 or 10 labeled nodes per class).\n\n* GCN out-performs GraphSAGE with more training data (e.g. >= 20 labeled nodes per class).\n\n* Network of GraphSAGE (N-SAGE) is better than GraphSAGE in all scenarios.", "title": "Added GraphSAGE and a Network of GraphSAGE (N-SAGE)"}, "HJeR42nGG": {"type": "rebuttal", "replyto": "H12U-wYgG", "comment": "Thank you for your review! It made our work much better!\n\n* It is unfair to compare N-GCN which has k-times more parameters to GCN.\n\nWe tried deeper GCN and the results were worse. We also tried >16 hidden dimensions (e.g. 32, 64, 512), and the results were also worse. Potentially because these datasets over-fit, reaching 100% accuracy on training set in all cases.\nNonetheless, we tried mixture of experts on GCNs (i.e. K=1 but r>1), and it is better than K = r = 1, but not as good as ours using random walks. For example, look at appendix and scroll down, comparing every (K=1,r=4; i.e. mixture of GCN) with (K=4,r=1; ours), and you will find that ours is better in all cases, showing that random walks indeed help.\n\n\n* Calibration is not clean:\n\nYou are right. We got excited about the \"calibration\" paper. Now we removed calibration, as it deviates from our story, which gave us more room to experiment with GraphSAGE (SAGE) and DCNN, and show that we can build a Network of GraphSAGE (N-SAGE).\n\n\n* Does this hold for large datasets?\n\nThere are many benchmarks on the datasets we use, including at least a handful of concurrent submissions to ICLR. We said that we will tackle more datasets in \"future work\". We will do our best to do so by the end of the rebuttal cycle.\n\n\n* Experiments with GraphSAGE and MoNet?\n\nWe added experiments with GraphSAGE and DCNN. Our models still outperform. We plan to try-out MoNet but perhaps after adding another (larger) dataset.\n\n\n* Not much novelty?\n\nAs we added experiments for GraphSAGE (SAGE), we decided to wrap SAGE in a network and train it with random walks, showing that Network of SAGE (N-SAGE) is better than SAGE. We feel that this generalization makes our work novel enough, and we hope that you agree.\n", "title": "Compared with Mixture of GCNs, added GraphSAGE, removed Calibration text and experiments"}, "BkNk82nMz": {"type": "rebuttal", "replyto": "BJququIlf", "comment": "Thank you for taking the time to review our work!\n\n* Balance on training and validation:\n\nWe re-use the splits created by Planetoid paper (including train, validate, test) and we do not control it in this paper.\n\n\n* Degree of GCNs:\n\nWe assume that you meant the \"capacity\" (e.g. number of parameters) of GCNs. We now conduct experiments in Appendix on GCNs when we give them more parameters, and we show that they perform worse than our models, showing that our methods are out-performing because of random walks, and not necessarily more parameters.\n\n", "title": "Data splits are not ours. Added experiments with Mixture of GCNs to give it more capacity ."}, "BJkjr22MG": {"type": "rebuttal", "replyto": "H1kTclOlf", "comment": "Thank you for your review! It made our work much better!\n\n* Compare with DCNN:\nWe added experiments to DCNN, and clearly explained how they are a special case of ours in new Section 3.6. We outperform them in the \"standard\" setup that was used by Kipf and Planetoid (i.e. 20 nodes per class). However, DCNN is showing more power than GCN's with more training data (e.g. see table on Pubmed, up to 100 labeled nodes).\n\n\n* Layers of GCN?\nThanks! We now made it clear in writing. Our GCN and SAGE modules for both our models and baselines use 2 layers.\n\n\n* Why replication r > 1?\nWe added extensive evaluation in Appendix. More \"r\" helps, similar to \"ensemble of classifiers\" (e.g. mixture of experts). This seems to help on validation+test but not on train accuracy, as all models reach ~100% accuracy on training anyway.\n\n\n* Early stopping and \"many runs\" for validation.\nWe beleive that model selection we do is acceptable. We choose models based on *validation* accuracy. In fact, we now choose the top 1 model based on validation accuracy and report its test accuracy, which is the true practical setting. We do many runs (total == thousands, for all parameter sweeps) and put mean and standard deviation in appendix. Also, we now re-ran all experiments without early stopping.\n\n\n* Add error bars to attention.\nGood idea. Now done. Thank you!\n\n\n* Self-connections:\nWe add self connections, and already mentioned it in at least 2 places as we follow Kipf's setup.\n\n\n* Understanding paper requires knowing Kipf's work\nWe tried to explain what it means that approximations \"still valid\". Is it better now? we tried our best to make the paper stand on its own and will continue doing so before the camera ready (in hopes it gets accepted)\n\n\n* Section 2.3 not needed.\nWe feel that it gives the reader background of embeddings on adjacency VS embeddings using random walks. It also gives us defines \\mathcal{T} in terms of D and A (which might be known by many readers and they could skim that section).\n\n\n* Typos:\nWe fixed them. Thank you for pointing them out.\n", "title": "Added DCNN experiments"}, "H1rQ5HmgG": {"type": "rebuttal", "replyto": "BJfkzLgxf", "comment": "Thomas,\n\nThanks for your kind words about our work!\n\nWe share your feelings about the challenges of assessing work that uses the benchmark splits, and we agree that testing on more datasets (e.g. graphs introduced in GraphSAGE) would further test if our model can generalize to other settings which hopefully do not suffer from the train VS validation size variance.\n\nWe were not aware of GraphSAGE at the time of our work (it is recent, to appear in NIPS). Nonetheless, it should be a one-line addition to our baseline (Kipf's GCN) and our model (NGCN), as it is just a layer-norm transformation (https://arxiv.org/abs/1607.06450).\n\nWe hope to add some additional experimental results during the rebuttal phase, as we are also quite interested in understanding the impact of newer models (e.g. GraphSAGE and/or mixture of CNNs) in the context of our proposed method. This should strengthen our work -- Thank you for the suggestion!", "title": "RE: Validation set and comparison to GraphSAGE"}}}