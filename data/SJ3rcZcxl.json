{"paper": {"title": "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic", "authors": ["Shixiang Gu", "Timothy Lillicrap", "Zoubin Ghahramani", "Richard E. Turner", "Sergey Levine"], "authorids": ["sg717@cam.ac.uk", "countzero@google.com", "zoubin@eng.cam.ac.uk", "ret26@cam.ac.uk", "svlevine@eecs.berkeley.edu"], "summary": "We propose Q-Prop, a novel policy gradient method with an off-policy critic as control variate, that is more sample efficient than TRPO-GAE and more stable than DDPG, the state-of-the-art on-policy and off-policy methods.", "abstract": "Model-free deep reinforcement learning (RL) methods have been successful in a wide variety of simulated domains. However, a major obstacle facing deep RL in the real world is their high sample complexity. Batch policy gradient methods offer stable learning, but at the cost of high variance, which often requires large batches. TD-style methods, such as off-policy actor-critic and Q-learning, are more sample-efficient but biased, and often require costly hyperparameter sweeps to stabilize. In this work, we aim to develop methods that combine the stability of policy gradients with the efficiency of off-policy RL. We present Q-Prop, a policy gradient method that uses a Taylor expansion of the off-policy critic as a control variate. Q-Prop is both sample efficient and stable, and effectively combines the benefits of on-policy and off-policy methods. We analyze the connection between Q-Prop and existing model-free algorithms, and use control variate theory to derive two variants of Q-Prop with conservative and aggressive adaptation. We show that conservative Q-Prop provides substantial gains in sample efficiency over trust region policy optimization (TRPO) with generalized advantage estimation (GAE), and improves stability over deep deterministic policy gradient (DDPG), the state-of-the-art on-policy and off-policy methods, on OpenAI Gym's MuJoCo continuous control environments.", "keywords": ["Deep learning", "Reinforcement Learning"]}, "meta": {"decision": "Accept (Oral)", "comment": "This paper presents a nice contribution to the RL literature, finding an intermediate point between the high-variance (but unbiased) gradient estimates from policy optimization methods, and low(er)-variance (but biased) gradient estimates from off-policy actor-critic methods like DDPG. The basic idea (as I'm interpreting it, similar to one of the reviewers below), that that we can use an action-dependent baseline, based upon off-policy learning, to lower the variance of the gradient, assuming we also correct for it in the gradient computation. The experiments clearly show the benefit of the proposed approach. The work is a nice combination/unification of two prominent trends in RL (with the overarching goal of reducing sample complexity, which is of course crucial here), and I believe is absolutely worth accepting. The authors also did an excellent job responding to reviewer concerns and adjusting the manuscript to address any issues raised.\n \n Pros:\n + Nice contribution combining off-policy and on-policy methods, with a novel and compelling algorithm\n + Good evaluation on a wide variety of control tasks\n \n Cons:\n - Somewhat difficult to understand (the interpretation I give above is not quite how the paper is presented, though I believe they are equivalent), and the given presentation is somewhat dense at time"}, "review": {"HkoyInUUW": {"type": "rebuttal", "replyto": "Sk63xNS8Z", "comment": "1. Because Q = A - V, where V only depends on the state. It's the standard baseline trick in policy gradient, applied to both Qhat and Qbar. \n2. The comparison is difficult, since the original ACER code is not open sourced, and it includes a variety of additional design choices. We did some follow-up analysis on this paper: https://arxiv.org/abs/1706.00387, under simplified conditions. Overall, how to integrate critic-based policy gradient is different, and we could also apply Q-Prop on top of ACER algorithm.  ", "title": "Reply"}, "Sk63xNS8Z": {"type": "rebuttal", "replyto": "SJ3rcZcxl", "comment": "1. Why the Q can be directly replaced with A from equation 7 to equation 8?\n\n2. And what do you think the pros and cons of Q-prop compared with ACER(https://arxiv.org/abs/1611.01224)?", "title": "From equation 7 to equation 8."}, "rkluF7Ncl": {"type": "rebuttal", "replyto": "HJT0sL79g", "comment": "no problem!\n\nyou have \" in the equal for\" which should be \"in the equation for\".", "title": "another one"}, "SkA8eN8jx": {"type": "rebuttal", "replyto": "SyrILFSig", "comment": "I see, my bad. Thank you", "title": "you are right"}, "SyrILFSig": {"type": "rebuttal", "replyto": "S1_tJyY9g", "comment": "We used the property described in the first equation of Appendix A, since the expectation of score function is wrt a_t, all terms not involving a_t have 0 expectation and can be thought of as a part of state-dependent baseline. ", "title": "derivation clarification"}, "S1_tJyY9g": {"type": "rebuttal", "replyto": "SJ3rcZcxl", "comment": "One thing confuse me is that in the appendix equation (14), when author derive the 1st order Taylor expansion, from line 2 to line 3 in g(theta). The author derive that the term \"f(s_t,bar(a_t))+grad(f(s_t,bar(a_t)))*(a_t-bar(a_t))\" is equal to \"grad(f(s_t,bar(a_t)))*a_t\". This is problematic not only for general f(.), but also when replace it with Q function.", "title": "paper equations"}, "B1gk_7X5x": {"type": "rebuttal", "replyto": "SJ3rcZcxl", "comment": "the equation under (14) has a typo: missing left bracket ')' after \\hat{f}", "title": "typo in appendix"}, "HJT0sL79g": {"type": "rebuttal", "replyto": "B1gk_7X5x", "comment": "thanks for pointing out!", "title": "thank you!"}, "rydEliF8e": {"type": "rebuttal", "replyto": "r1MiVT18g", "comment": "Thank you for your feedback, and we're glad to hear that our response addressed your concerns. Since your answer indicates that you've reevaluated the paper based on our responses and additional experiments, we would appreciate a revision to the original score as well, since your current score is substantially lower than that of the other reviewers. And of course we would appreciate any other feedback if, e.g., we have not addressed all of your concerns in enough detail.", "title": "Re-evaluation"}, "rJ5evvULl": {"type": "rebuttal", "replyto": "SklQBhLNe", "comment": "Thank you for mentioning this interesting work. GProp proposes a TD-based approach for learning value gradient directly and analyzes its stability property from deterministic policy gradient theorem, and appears an interesting extension/related work to DPG (Silver et. al., 2014), DDPG (Lillicrap et. al., 2016), and Dueling network (Wang et. al., 2016). \n\nGProp, however, is orthogonal to Q-Prop. In fact, Q-Prop can be used with GProp easily, by substituting \\nabla_a Q_w(s,a)|_{a=\\mu(s)} in Eq. 8 in the Q-Prop paper with G_w(s,\\mu(s)) in the GProp paper. Besides the analysis provided from the DPG theorem perspective, predicting the value gradient directly may provide better gradient estimate than differentiating a NN-fitted action-value function and improve current Q-Prop implementation with standard TD-based Q-fitting.\n\nWhile GProp analyzes compatibility conditions in detail, it makes approximation when deriving a TD-based algorithm (see Section 3.4 L_BGE Equation in GProp) and its policy gradient is biased. By contrast, Q-Prop is guaranteed to be as unbiased as standard Monte Carlo policy gradient methods such as REINFORCE (see Section 2.1, 3, 3.1 in Q-Prop). It is interesting to therefore evaluate stability across DDPG, GProp, Q-Prop with DDPG-style critic learning, Q-Prop with GProp.   \n\nSimilarity in namings is by coincidence. We took inspiration from MuProp (Gu et. al., 2016). ", "title": "Q-Prop vs GProp"}, "HJ6FQLI8g": {"type": "rebuttal", "replyto": "S1hUM3FVx", "comment": "Thank you for your review and constructive feedback! \n\nFigure 2.a shows that on HalfCheetah-v1 the conservative variant of Q-Prop (TR-c-Q-Prop) learns more stably than the standard (TR-Q-Prop) and aggressive (TR-a-Q-Prop) variants, and empirically appears more robust toward unreliable critic estimate. This is expected from the derivation, since the conservative variant effectively turns off Q-Prop and back-off to TRPO for states that are likely to have bad critic estimates. We also updated Section 3.4 second paragraph to include discussions on the robustness to bad critics.\n\nWe performed hyperparameter search on both Q-Prop and DDPG as detailed in Appendix D and showing that the best attainable performance is more stable for Q-Prop than DDPG as shown in Table 1. Our claim is that Q-Prop is more robust to bad critics and less sensitive to hyperparameters than DDPG. In Table 1, it is important to weigh significances of task results based on task difficulties. While DDPG could solve tasks like HalfCheetah and Swimmer (which have 17 and 8 state dimensions and 6 and 2 action dimensions respectively), Figure 3.b and Table 1 show that Q-Prop achieves significantly better performance within the same number of episodes on the much harder Humanoid task (376 state dimensions and 17 action dimension). Note that Q-Prop was able to solve all tasks as well as TRPO, though sometimes slightly slower than DDPG, while DDPG was unable to achieve good performance on Humanoid. Empirically, we observe that the harder the tasks become (e.g. 2D HalfCheetah -> 3D Humanoid, see curves comparing TR-c-Q-Prop and DDPG in Figures 3.a, 3.b and Table 1), the benefit of Q-prop over purely off-policy method like DDPG becomes more obvious. A crucial point about DDPG from our experiment is that given a reasonable hyperparameter search done in Appendix D for DDPG, DDPG couldn\u2019t find good solutions for some of the harder problems. This practically makes DDPG difficult to be applied for solving hard problems, even though it sometimes can solve simpler tasks better than TRPO and Q-Prop. \n\nWe also ran additional experiments to validate the hyperparameter sensitivity further, as available in https://docs.google.com/document/d/1ow_BIrKYt11r4BXMbM7w9qBxbedhedKrBrvMhnXm7nI/edit?usp=sharing. It builds on the discussion on Figure 3.a, further demonstrating that Q-Prop is less sensitive to reward scales than DDPG.", "title": "Response to AnonReviewer1"}, "ByDfXUI8g": {"type": "rebuttal", "replyto": "r11-EhWBx", "comment": "Thank you for your review and positive feedback!\n1) compute time: We expanded current discussion on computation time in Section 3.4. The running time of Q-Prop is effectively time(TRPO) + time(DDPG)/2, if we use the same parameter update per step of experience as DDPG (i.e. one update per one step). If the simulator is very fast, time(DDPG)/2 >> time(TRPO), so effectively the run-time per episode of Q-Prop is about half of DDPG and slower than TRPO. However, for applications where we care about sample-efficiency, the experience collection is often the bottleneck, and in such cases, the off-policy critic learning and the on-policy data collection and policy update can be parallelized, and will run at about the same speed as both TRPO and DDPG. \n2) limitations of this technique: We added Section 3.4 to discuss the main limitations. \n\nWe also clarified the confusion in Eq (4). Q without subscript represents the target network.", "title": "Response to AnonReviewer3"}, "r1M0fULUl": {"type": "rebuttal", "replyto": "rJVKwJMSe", "comment": "Thank you for your detailed reviews and pointing to aspects of the paper that can be improved!\n- compute time: We expanded current discussion on computation time in Section 3.4. The running time of Q-Prop is effectively time(TRPO) + time(DDPG)/2, if we use the same parameter update per step of experience as DDPG (i.e. one update per one step). If the simulator is very fast, time(DDPG)/2 >> time(TRPO), so effectively the run-time per episode of Q-Prop is about half of DDPG and slower than TRPO. However, for applications where we care about sample-efficiency, the experience collection is the bottleneck, and in such cases, the off-policy critic learning and the on-policy data collection and policy update can be parallelized, and will run at about the same speed as both TRPO and DDPG. \n- sensitivity experiment: We ran additional experiments to validate the hyperparameter sensitivity further, as available in https://docs.google.com/document/d/1ow_BIrKYt11r4BXMbM7w9qBxbedhedKrBrvMhnXm7nI/edit?usp=sharing. It builds on the discussion on Figure 3.a, further demonstrating that Q-Prop is less sensitive to reward scales than DDPG.", "title": "Response"}, "HJXW1TXUe": {"type": "rebuttal", "replyto": "rkXaJfGIl", "comment": "Thank you for your recommendation. The run-time per episode of Q-Prop is time(TRPO) + time(DDPG)/2. If simulation is very fast, then it\u2019s roughly time(DDPG)/2. As you mentioned, it takes a long time to run until 100k episodes just like DDPG. This is a limitation of Q-Prop which we discussed in Section 3.3 last paragraph, but it should not be a concern if collecting samples is the bottleneck, which is the case in most real-world applications (e.g. physical robots). We are in the process of running those experiments for longer episodes and hope to post available videos after.\n\nDuan et. al. 2016 is the main benchmark paper we aimed to compare with, and seems to have different reward scales/results from the Gym website. We primarily followed their example to compare numbers across many domains, rather than specifics in videos which are difficult to quantify.  ", "title": "Response"}, "rkXaJfGIl": {"type": "rebuttal", "replyto": "ryhaysWIx", "comment": "Thanks for the response. In light of what you said, I have the following recommendations:\n\n1. Make public a video of all your policies (and policies that you have with TRPO-GAE and DDPG). As mentioned before, numbers aren't very helpful. A visual comparison between the different policies will be much more revealing. \n\n2. Include wall clock times in Table 1. The selling point seems time, but it need not necessarily be correlated with sample complexity. For instance, some algorithms run on a batch setting (update once per say 10k time-steps) while others run on some approximate online setting (update every 100 or so steps). This is why DDPG is slower (in wall-clock sense) than TRPO, even though the former could be more sample efficient. Where does Q-prop stand in this regard?\n\n3. Why not run for more episodes (say 100k) and then compare performances? This will likely give a better idea about asymptotic performance. The results reported in the gym website (e.g. https://gym.openai.com/evaluations/eval_W27eCzLQBy60FciaSGSJw) take only 5-20 hours, which isn't that bad. At least, run Q-prop alone for 100k episodes and compare to assymptotic performance reported on the gym website.\n\nPlease note that this isn't a comment against the ideas presented in the paper. I also appreciate the efforts of author to compare against other algorithms. However, I think the above suggestions are easy enough to implement and would be much more revealing of the strengths and weaknesses of Q-prop, that I highly recommend them.", "title": "Some recommendations"}, "ryhaysWIx": {"type": "rebuttal", "replyto": "HJD3gzgIx", "comment": "Thank you for your suggestions. We are using the same reward definitions and default settings as given in OpenAI Gym. In addition, our TRPO-GAE and DDPG benchmarks are run with TensorFlow implementations in https://github.com/openai/rllab. Since the authors of the code moved from Theano to TensorFlow, they have not fully benchmarked the codes, and small bugs were fixed on my end as well. We used LinearFeatureBaseline for all Monte Carlo policy gradient algorithms (including Q-Prop), primarily because the NN Baseline was not available in their open-source TF implementation. We believe our results are sufficiently close; however, some discrepancies may be due to Linear baseline vs NN baseline. \n\nIn Table 2, all algorithms were run for 30k episodes. Note that the OpenAI gym website reports reward up to e.g. 100k episodes, which we were unable to do for all methods due to computational constraints. However, since the point of the evaluation is to evaluate speed of learning, we believe it is reasonable to choose a fixed episode horizon for all methods. Our results are comparable with the gym website results until 30k episodes, e.g. better than gym on Humanoid-v1, Ant-v1 and slightly worse but comparable on HalfCheetah-v1. \n\nWhile we tried our best to benchmark our results to be similar as Duan et. al. 2016 paper, their paper appears to use different MuJoCo domain settings from the OpenAI Gym ones, e.g. the reward scale for HalfCheetah-v1 is quite off, and they do not have videos available. \n\nFor HalfCheetah-v1, a setting of DDPG (see Figure 3.a \u201cDDPG-r0.1\u201d curve) appears to outperform all the results in OpenAI Gym website in terms of both sample efficiency and final performance. For Humanoid-v1, Figure 3.b shows that TRPO performs about as well for the 30k episodes as the gym website (https://gym.openai.com/evaluations/eval_i3x1JpReRukTZrznxypCw, where performance increases only much later), so the results are in fact similar. We confirm that our Humanoid-v1 policy with reward~3500 at 30k episodes does walk but a bit less elegantly than the gym policy with reward~6000 at 80-90k episodes. We only ran the methods up to 30k episodes, so the final performances are lower than the best reported after 80k-90k episodes. However, within the first 30k episodes ours is better than their result, and given that both our Q-Prop and TRPO curves learn monotonically, we feel the current setting was sufficient to demonstrate the relative performance with and without the Q-Prop control variate. \n\nWe appreciate you bring up important challenges in RL: how to have fair benchmarks. We have tried our best in the paper, using the open-source domains/codes without any modification. We also have open-sourced our codes here: https://github.com/shaneshixiang/rllabplusplus. ", "title": "Reward functions are the same as the tasks in OpenAI gym suite"}, "HJD3gzgIx": {"type": "rebuttal", "replyto": "SJ3rcZcxl", "comment": "The results reported seem very different from what is available on the gym website. Are you using different reward definitions, and if so, you should make this very transparent. \n\nFor instance, look at Table~1.\nFor hopper, TRPO can attain a reward close to 3600, much higher than what is reported for TRPO/Q-Prop: https://gym.openai.com/evaluations/eval_W27eCzLQBy60FciaSGSJw\nSimilarly, for the humanoid-v1, TRPO-GAE/DDPG can obtain orders of magnitude more return than results reported in this paper: https://gym.openai.com/evaluations/eval_i3x1JpReRukTZrznxypCw\n\nIf your environment definitions are different, you should re-run your experiments on the definitions packaged with the gym suite. At the very least, you should take your policies and test them on the reward functions with the gym tasks. As is, it's very hard to interpret your numbers and make any conclusions, since we don't know your reward function. Do you have videos for the different policies? What does a reward of 3500 mean? Is the system moving, or just standing still, or just just applying less control effort to get a higher score?\n\nThe idea seems promising, but I appeal to the authors to do the above \"sanity check\" experiments, without which the paper would be incomplete. Also, for the TRPO experiments, which value function baseline did you use (linear, NN -- if so, how many layers etc)? You just have a pointer to Duan's paper, but it would be good to be explicit, since they have multiple options available in their code base.", "title": "Are the reward functions different from the tasks in OpenAI gym suite?"}, "r1MiVT18g": {"type": "rebuttal", "replyto": "H1WxcN6Hx", "comment": "Thank you for your quick response. Yes, the implementation of TRPO-GAE is exactly the same as Q-Prop if \\eta_{t,e}=0. We will include the sentence about equivalence in the paper. \n\nThe codes are available: https://github.com/shaneshixiang/rllabplusplus. Q-Prop codes are directly written on top of the open-sourced TRPO-GAE TensorFlow implementation (e.g. sandbox/rocky/tf/algos/batch_opt.py). ", "title": "Q-Prop implementation"}, "B1NxP19re": {"type": "rebuttal", "replyto": "SyJakD7Bl", "comment": "Thank you for valuable feedback. We hope we could engage in constructive discussions to fully clarify and address your concerns and questions. We have incorporated some of your suggestions into the paper, available on OpenReview. Below are our initial responses to your seven comments. \n\n1. Given your feedback, we renamed Section 2.1 as \u201cMonte Carlo policy gradient methods\u201d and Section 2.2 as \u201cpolicy gradient methods with function approximation\u201d, added a discussion in Section 2, and clarified some references to \u201cpolicy gradient\u201d through the paper. We decided to use \u201cMonte Carlo policy gradient\u201d over \u201cREINFORCE\u201d because we would like it to cover a more general class of algorithms including REINFORCE/vanilla policy gradient, TRPO, etc. Please let us know if you have other suggestions.\n\n2. We agree that a detailed evaluation of the components of the algorithm is important, and we endeavored to provide these in Figure 2.a (see curves comparing variants of Q-Prop) and Figure 3.a (see Q-Prop variants with and without trust-region, i.e. TR-c-Q-Prop and v-c-Q-Prop). Note, however, that Q-Prop is a general control variates approach that can be combined with a number of prior techniques, including TRPO-GAE and DDPG,  as well as natural gradient, standard REINFORCE, and standard (on-policy) actor-critic algorithms. We've clarified this in the second-to-last paragraph in Section 3.1. While the particular implementation used in our experiments has a number of design choices, these are inherited from the prior methods. We believe it is therefore outside the scope of this paper (and outside of the conference paper length limits) to evaluate each of these decisions, but further discussion of these points can be found in the corresponding prior work: DQN (Mnih et. al. 2015), DDPG (Lillicrap et. al. 2016), TRPO-GAE (Schulman et. al., 2016). Below we respond to specific lines in this comment.\n\n\u201cIs the improved performance due to the use of the action-dependent control variate? Would the same setup but using a state-value baseline still perform just as well? Are the performance benefits due to the use of an off-policy advantage estimation algorithm, GAE(lambda)\u201d\nAll our policy gradient/REINFORCE-based benchmark methods defined in section 5 first paragraph are implemented with GAE(lambda=0.97), which itself includes a state-value baseline, to give a fair comparison. Q-prop is implemented on top of these, so all improvements seen come from the use of our action-dependent baseline (see Figures 2.b, 3.a, 3.b and Table 1, e.g. compare Figure 2.b. \"TRPO-5000\" vs \"TR-c-Q-Prop-5000\", which indicates the benefit of Q-Prop over TRPO using the same exact estimator, with the only difference being the action-dependent baseline). In Figure 3.a, we also compared REINFORCE with and without Q-Prop (see \u201cVPG\u201d and \u201cv-c-Q-Prop\u201d curves). Note that REINFORCE here also uses the GAE state-dependent baseline, making this a reasonable REINFORCE comparison of action dependent vs state dependent baselines. Lastly to clarify a detail, the original GAE paper uses on-policy advantage estimation and does not use replay buffer. This was in fact an important motivation for us to explore Q-Prop with a critic learned off-policy. We are in the process of running these experiments, but preliminary results suggest that an off-policy action-independent baseline (GAE with off-policy value function) performs worse than both an off-policy action-dependent baseline (Q-Prop) and an on-policy action-independent baseline (GAE).\n\n\u201cOr, would performance have been similar with an on-policy advantage estimation algorithm? What about if a different off-policy advantage estimation algorithm was used, like Retrace(lambda), GTD2, ETD, or WIS-LSTD? Or, is the improved performance due to the use of a replay buffer?\u201d \nWe do not think Q-Prop with on-policy advantage estimation will provide much benefit over GAE because GAE already performs on-policy advantage estimation via an on-policy state-value function estimator. Using Q-Prop with an on-policy action-value function estimator is a possible alternative to investigate, which we are currently working to add to the experiments. That said, we specifically focused on a simple off-policy action-value function fitting approach in this paper because we were aiming to improve sample-efficiency and wanted a simple demonstration to show using off-policy data in on-policy REINFORCE-style algorithms helps. We believe that good alternative off-policy advantage estimation algorithms are likely to do equally well or better, and we\u2019ve added a discussion in the last paragraph of Section 4 on this. Comparing with all prior off-policy estimation approaches like Retrace(lambda), GTD2, ETD, or WIS-LSTD is interesting empirical work for the future; however, we can only present a limited number of experimental results in a conference format, so we instead chose to implement Q-Prop on top of the current state-of-the-art deep RL policy gradient and actor-critic methods (TRPO and DDPG-style Q-function estimation), so as to cleanly illustrate the improvement provided by an action-dependent control variate obtained from off-policy data.\n\n\u201cComparisons are not performed between variants of Q-Prop that show the importances of these different components.\u201d\nFigure 2.a is dedicated to comparing various variants of Q-Prop: no-Q-Prop (\u201cTRPO\u201d) vs. standard Q-Prop (\u201cTR-Q-Prop\u201d) vs conservative (\u201cTR-c-Q-Prop\u201d) vs aggressive (\u201cTR-a-Q-Prop\u201d). We felt this is more important than comparing the off-policy learning component with all previous variants in the limited space available in a conference paper. The results in Figure 2.a that TRPO-GAE+conservative Q-Prop performed most reliably and is indeed what we expected.\n\n\u201cFor example, after reading this paper, it is not clear whether having the action-dependent baseline (or using the first order Taylor approximation for the baseline) is beneficial or not - it could be that the strong performance comes from GAE(lambda) or the use of a replay buffer.\u201d\nAgain, Q-Prop is implemented on top of GAE, and comparing TR-c-Q-Prop curves with TRPO curves in Figures 2.b, 3.a, 3.b and Table 1 clearly show that Q-Prop further improves upon GAE.\n\n\nTo summarize, the Q-Prop method itself is very simple, and most implementation designs are directly inherited from prior state-of-the-art method papers (TRPO-GAE and DDPG). Q-Prop offers considerable flexibility on what on-policy and off-policy advantage learning techniques, and we have added discussion in the second-to-last paragraph in Section 3.1. Given that the current off-policy advantage learning is based on prior techniques (e.g. DDPG), more sophisticated methods (e.g. Retrace(lambda)) will likely lead to further improvements, and we\u2019ve added a discussion of this to Section 4 last paragraph.\n\n3. Thank you for pointing this out. It can be made unbiased either if gamma = 1 or if gamma^t multiplies the gradient at each time step, which is generally undesirable. This is general for most policy gradient/REINFORCE methods. We have included the discussion on this in Section 2.1 below Eq. 2, and appropriately qualified all claims of unbiasedness in the paper.\n\n4. Thank you for pointing this out. Indeed, the paper can be written differently to emphasize the point that this is not specific to deep neural networks. However, our immediate goal in this paper is to make deep RL work better, and the design decisions in the algorithm are tailored to specifically addressed the difficulties of training deep architectures. Our aim in focusing on deep RL is to properly scope out the contribution of the paper. We are NOT claiming to propose a better RL algorithm for everything, because we don't have evidence for this, so while it's true that the contribution is not deep network specific, the evaluation is, and the focus of the paper reflects this limitation of the evaluation (due to the natural limitations of a fixed-size conference paper). We believe this scope is reasonable for ICLR, as evidenced by a number of other deep RL submissions that also propose improvements not specific to deep networks. It would be an interesting direction for future (e.g. an extended journal version) to evaluate the method in other, non-deep RL settings.\n\n5. Thank you. We will update the discussion about importance sampling to include those references. If you would like to suggest a comparison to an importance sampling method that is suitable for large continuous state and action spaces, we would be happy to compare to it. \n\n6. We already compare to a natural gradient method, namely TRPO, which is a variant of natural gradient with a specific step size rule (see Section 7 in Schulman et. al. 2015). On the tasks in our experiments, TRPO was shown to perform as well or better than standard natural policy gradient methods (see Figure 4 in Schulman et. al., 2015, and see Table 1 and Section 6 \u201cTNPG and TRPO\u201d in Duan et. al., 2016), and we therefore picked TRPO as the baseline in our experiments. In our paper, we show that adding Q-Prop further improves TRPO (see TR-c-Q-Prop curves with TRPO curves in Figures 2.b, 3.a, 3.b and Table 1). Thus, we have compared our method with a natural gradient algorithm and shown improvement.\n\n7. We fixed the problem by modifying rho_pi definition in Eq 2 and fixed the Q, A typos in Section 2.1 and \\hat{Q} typo in Section 3.1. Thank you!\n\nWe appreciate that you find the idea of Q-Prop interesting. We made the decision to prioritize comparisons with the state-of-the-art deep RL methods because our priority in this paper is to improve upon deep RL methods; however, we look forward to making more concrete analytical and empirical comparison with a broader class of RL techniques in future work, which we agree will be another valuable research contribution. \n", "title": "Rebuttal"}, "B1v5dVxBx": {"type": "review", "replyto": "SJ3rcZcxl", "review": "No questionsThe paper proposes using a first-order Taylor expansion as a control variate in policy gradient-style methods. Empirical results in dynamical control tasks suggest that this algorithm reduces the sample complexity, while the theoretical results presented suggest the algorithm is unbiased but of lower variance.\nThe use of control variates is very important and the present paper is an interesting approach in this direction. I am not fully convinced of the approach, because it is one of many possible, and the theoretical analysis relies on an approximation of the variance rather than exact calculations, which makes it less compelling. However, this paper is a step in the right direction so it is worth accepting. In the experiments, a few things need to be discussed further:\n- What is the running time of the proposed approach? The computation of the extra terms required looks like it could be expensive. running time comparison in addition to sample comparison should be included\n- The sensitivity to parameter settings of the proposed algorithm needs to be illustrated in separate graphs, since this is one of the main claims in the paper\n- It would be nice to have a toy example included in which one can actually compute exact values and plot learning curves to compare more directly bias and variance. It would especially be nice to do this with a task that includes rare states, which is the case in which variance of other methods (eg importance sampling) really becomes significant.", "title": "No questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJVKwJMSe": {"type": "review", "replyto": "SJ3rcZcxl", "review": "No questionsThe paper proposes using a first-order Taylor expansion as a control variate in policy gradient-style methods. Empirical results in dynamical control tasks suggest that this algorithm reduces the sample complexity, while the theoretical results presented suggest the algorithm is unbiased but of lower variance.\nThe use of control variates is very important and the present paper is an interesting approach in this direction. I am not fully convinced of the approach, because it is one of many possible, and the theoretical analysis relies on an approximation of the variance rather than exact calculations, which makes it less compelling. However, this paper is a step in the right direction so it is worth accepting. In the experiments, a few things need to be discussed further:\n- What is the running time of the proposed approach? The computation of the extra terms required looks like it could be expensive. running time comparison in addition to sample comparison should be included\n- The sensitivity to parameter settings of the proposed algorithm needs to be illustrated in separate graphs, since this is one of the main claims in the paper\n- It would be nice to have a toy example included in which one can actually compute exact values and plot learning curves to compare more directly bias and variance. It would especially be nice to do this with a task that includes rare states, which is the case in which variance of other methods (eg importance sampling) really becomes significant.", "title": "No questions", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SklQBhLNe": {"type": "rebuttal", "replyto": "SJ3rcZcxl", "comment": "Could you explain how Q-Prop is related to GProp from Compatible Value Gradients for Reinforcement Learning of Continuous Deep Policies by Balduzzi & Ghifary (http://arxiv.org/abs/1509.03005)?", "title": "Related work"}}}