{"paper": {"title": "Mixup Inference: Better Exploiting Mixup to Defend Adversarial Attacks", "authors": ["Tianyu Pang*", "Kun Xu*", "Jun Zhu"], "authorids": ["pty17@mails.tsinghua.edu.cn", "kunxu.thu@gmail.com", "dcszj@mail.tsinghua.edu.cn"], "summary": "We exploit the global linearity of the mixup-trained models in inference to break the locality of the adversarial perturbations.", "abstract": "It has been widely recognized that adversarial examples can be easily crafted to fool deep networks, which mainly root from the locally non-linear behavior nearby input examples. Applying mixup in training provides an effective mechanism to improve generalization performance and model robustness against adversarial perturbations, which introduces the globally linear behavior in-between training examples. However, in previous work, the mixup-trained models only passively defend adversarial attacks in inference by directly classifying the inputs, where the induced global linearity is not well exploited. Namely, since the locality of the adversarial perturbations, it would be more efficient to actively break the locality via the globality of the model predictions. Inspired by simple geometric intuition, we develop an inference principle, named mixup inference (MI), for mixup-trained models. MI mixups the input with other random clean samples, which can shrink and transfer the equivalent perturbation if the input is adversarial. Our experiments on CIFAR-10 and CIFAR-100 demonstrate that MI can further improve the adversarial robustness for the models trained by mixup and its variants.", "keywords": ["Trustworthy Machine Learning", "Adversarial Robustness", "Inference Principle", "Mixup"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposed a mixup inference (MI) method, for  mixup-trained models, to better defend adversarial attacks.  The idea is novel and is proved to be effective on CIFAR-10 and CIFAR-100.  All reviewers and the AC agree to accept the paper."}, "review": {"Hye3IBo9oH": {"type": "rebuttal", "replyto": "Bkloiyi5iS", "comment": "Thank you again for your kind suggestions, which really help a lot to improve the original version of the paper. We deeply appreciate it!", "title": "Thank you!"}, "Hylb_3L5uS": {"type": "review", "replyto": "ByxtC2VtPB", "review": "Notes: \n\n-Paper claims that adversarial examples stem from locally non-linear behavior.  However, wasn't this the exact opposite of the conclusion of \"Explaining and Harnessing Adversarial Examples\" (Goodfellow 2015)?  It is cited in this paper but I think the proper conclusion is the opposite of what is written here.  \n\n-Linearity however may still be an important component since it simplifies the problem of adversarial robustness.  \n\n-Many techniques try to introduce adversarial robustness through transformations during inference time.  \n\n-Paper claims that adversarial training induces locally linear behavior - but I'm rather skeptical of this claim.  Think about something like KNN with k=1 and euclidean distance.  This should be L2-robust on the training set, yet very non-linear.  \n\n-I understand the contents of Figure 1, but I'm not sure exactly what conclusion I should draw from it.  \n\n-The novel procedure presented is \"Mixup Inference\".  This involves classifying using interpolations of test inputs.  \n\n-Two mechanisms are proposed for why this could help.  One is that the magnitude of the perturbation will shrink after doing mixup (although the signal in the original image will also shrink, so I'm not sure if I like this argument).  The second argument is that the adversarial perturbation will have to appear with different random examples which will force the attack to be more \"universal\" to succeed.  This second argument I find much more compelling.  \n\n-The notation $y_s \\sim p_s(y_s)$ is rather abusive since the random variable and the same have the same name but this is common in machine learning.  \n\n-The technique if I understand correctly (Algorithm 1) amounts to using an average of the prediction at the original point along with an average of the mixes going to all other points in the dataset.  \n\n-The paper is a bit slow to explain what distribution lambda will come from - but it effects the algorithm a lot (especially if the distribution is symmetric or asymmetric).  \n\nComments: \n\n-There is another paper (Shimada 2019) that also uses interpolation at test time and should be cited here, although I admit that paper is written in a confusing way so the connection may not be immediately obvious: https://arxiv.org/pdf/1906.08412v1.pdf\n\n-I have a suggestion for the organization of the paper that I think would improve it.  I would suggest to first introduce the method in a clear fashion (after motivating it), along with the equations.  Then, *after that*, clearly and separately introduce the analysis of the \"optimal linear model\": \"a well mixup-trained model F can be denoted as a linear function H on the convex combinations of clean examples\".  I think that would make the paper much clearer.  \n\n-For example, how can we actually know what G_k() is unless we know the adversarial perturbation (which shouldn't generally be possible during inference)?  I found this discussion to be rather confusing (basically section 3.1) although admittedly it might be my own fault.  \n\n-Why does mixup-inference hurt the clean accuracy by 10% (table 2)?  This seems like quite a lot to me.  Still the degree of robustness does seem impressive.  And I also believe that the obtained robustness of this technique along with AT is state of the art.  \n\n-It might be nice to see examples of attacks on the resulting model, especially the one with the best robustness.  It's possible that Linf-bounded attacks against *this model* will be perceptible, which would support lowering the epsilon-attack-budget (which is actually a good thing for the research field as it's evidence that maybe this epsilon shouldn't be considered practically imperceptible).  \n\nReview: \n\nThis paper was interesting, because it has nice experimental results and seems like a good idea, but I feel like the paper needs to be improved.  The biggest issue is that the paper repeatedly claims that adversarial robustness can be improved by making networks more linear, yet I believe that this is the opposite of what prior work has found.  I also found the exposition of the idea to be confusing as it simultaneously introduces the idea and an analysis of the technique - I would much prefer if the technique were introduced first and the analysis under some optimal assumptions moved to a different section.  ", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 4}, "HklUuXwqor": {"type": "rebuttal", "replyto": "ByxtC2VtPB", "comment": "Dear Reviewers,\n\nThank you again for your valuable comments and suggestions, which are really helpful for us. We have uploaded new revisions and posted responses to the proposed concerns and questions.\n\nWe totally understand that this is a quite busy period of time, since the reviewers may be preparing the rebuttal for their own submissions or rushing for the deadline of the recent conferences.\n\nSo we deeply appreciate it if the reviewers can take some time to return further feedbacks on whether our responses and extra experiment results solve their concerns. If there is any other question, we will try our best to provide satisfactory answers. \n\nBest,\nThe authors", "title": "Looking forward to further feedbacks"}, "SkxDmETLoB": {"type": "rebuttal", "replyto": "SkgiTp66YB", "comment": "We train a Resnet-50 with the mixup on ImageNet, and the top-1 accuracy on 50,000 validation examples is 73.7%. In the adversarial setting, we test on 1,000 randomly selected examples in the validation set. The accuracy results (%) on the sampled validation subset are shown below:\n\n        Method     || Clean Acc| $\\text{PGD}_{10}^{\\textbf{tar}}$ |$\\text{PGD}_{10}^{\\textbf{un}}$ | $\\text{PGD}_{50}^{\\textbf{tar}}$|$\\text{PGD}_{50}^{\\textbf{un}}$ ||\n         Mixup       ||      88.3     |      1.3      |      3.6     |      0.5      |      3.4    ||\nMixup + MI-OL ||      82.2     |      67.2    |     35.0   |      64.1    |     24.6   ||\n\nHere $\\epsilon=4/255$, number of execution in MI is $N=30$, and $\\lambda_{\\text{OL}}=0.4$. ", "title": "More results on ImageNet"}, "B1xw_i0lor": {"type": "rebuttal", "replyto": "Hylb_3L5uS", "comment": "Thank you for the valuable review.\n\nWe upload the revision according to your suggestions, which includes:\n1. To be more rigorous, we reclaim that the adversarial examples mainly root from locally unreasonable or unstable behavior, and the adversarial training method induces locally stable behavior.\n\n2. We check the paper of Schimada et al. (2019) and find that it is indeed a quite related paper. We cite it and mention the difference between our work in Introduction.\n\n3. We modify the organization as you suggested in Section 3, i.e., we first introduce the method and then provide the theoretical analysis. We also explain the distribution of lambda ahead.\n\n4. We modify the notations like $y_{s}\\sim p_{s}(y_{s})$ to $y_{s}\\sim p_{s}(y)$, and $x_{s}\\sim p_{s}(x_{s}|y_{s})$ to $x_{s}\\sim p_{s}(x|y_{s})$.\n\n5. We provide adversarial examples crafted by adaptive attacks against the IAT + MI (best robustness) in Figure 5 (in Appendix part).\n\n \n \nBelow we further clarify the remain questions.\n \nQuestion 1. About the linearity:\nAs to the relationship between linearity and robustness, actually there is not a final conclusion. Goodfellow et al. (2015) claimed that the vulnerability of neural networks comes from their linear nature mainly because they found that FGSM can successfully attack some networks, and FGSM is based on the linear assumption of classifiers. However, there are many cases where FGSM fails but iterative attacks like PGD can still evade the models, which means adversarial examples do not necessarily stem from the linear nature. Besides, recent work [*1] finds that linearity can improve robustness, which contradicts the conclusion in Goodfellow et al. (2015). So as you suggested, in the revision, we do not claim the relationship between model linearity and the existence of adversarial examples.\n \n[*1] Qin et al. Adversarial Robustness through Local Linearization. NeurIPS 2019\n \n \nQuestion 2. The shrinkage mechanism in MI:\nIt is true that the signal in the original image will also shrink after performing MI. Intuitively, we intend to improve the \u2018signal-to-noise ratio\u2019 fed into the classifier, so what we expect is that the effect of adversarial noise shrinks faster than it caused by the original signal. This is actually the property represented in Eq.(12) and Eq.(15), where the left parts of the inequalities indicate the change of the adversarial effect, and the right part indicates the change of the original clean effect.\n \n \nQuestion 3. About the function $G_{k}$:\nWe cannot have a closed-form representation on $G_{k}$, since the neural network is a black-box model. So what we can claim in the theoretical analyses is that MI can improve robustness on the attacks satisfying Eq.(12) and Eq.(15), i.e., the decay of the adversarial effect is enough to compensate the decay of the original clean effect. Then in Figure 2, we empirically show that strong attacks like PGD satisfy this property and such that MI can better defend them.\n \n \n\nAnswers on other detailed notes and comments:\n(1)  In Algorithm 1, the average is performed on the $N$ randomly sampled points in the dataset with label $y_{s}$. In the experiments, we follow the setting in previous work (Xie et al. ICLR 2018) to set $N=30$ for fair comparisions.\n\n(2)  In Table 2, we choose the hyperparameter setting with comparable clean accuracy for each method to perform a fair comparison. As shown in Figure 3(b), there is a trade-off between clean accuracy and adversarial accuracy depending on the hyperparameters in each method, and MI can lead to better trade-off.\n", "title": "Thank you for the valuable review"}, "SJljWoRgjS": {"type": "rebuttal", "replyto": "SkgiTp66YB", "comment": "Thank you for the supportive review.\n\nShowing the average and standard errors is a good suggestion, we are running the experiments and will add them in the final version.\n \nWe also have some initial results on ImageNet, where applying MI can improve the adversarial accuracy of mixup-trained models from 3.6% to 35%, under PGD-10 attacks with $\\epsilon=4/255$.", "title": "Thank you for the supportive review"}, "ByxZpqCgjr": {"type": "rebuttal", "replyto": "B1g-lU5W9B", "comment": "Thank you for the supportive review.\n\nQuestion 1. The case of mislabeling:\nThe improvement on decreasing output mislabels is mainly finished in the training phase by mixup or other better training mechanisms. The main effect of MI is to improve adversarial robustness, where the original clean counterpart is correctly classified. It is possible that MI can correctly classify the adversarial examples crafted on mislabeled clean images, but this is not guaranteed by theoretical analyses.\n \n \nQuestion 2. More efficient inference:\nMI can still have good performance when $N=1$. A higher number of $N$ can stabilize the inference progress, but is not necessary. In our experiment, we choose $N=30$ to follow the setting in previous work (Xie et al. ICLR 2018).\n \n \nQuestion 3. About MI-Combined:\nThe MI-Combined method consists of MI-PL in the detection phase and MI-OL in the classification phase. These two phases are separately justified in Eq.(14) and Eq.(15).\n \n\nQuestion 4. Related work:\nThank you for pointing out, we have added the reference of Tokozume et al. (ICLR 2018) in the uploaded revision.\n", "title": "Thank you for the supportive review"}, "SkgiTp66YB": {"type": "review", "replyto": "ByxtC2VtPB", "review": "This paper proposes a novel use of mixup, which is originally a data augmentation method incorporating two training samples and their corresponding labels. The authors utilize mixup not for training but for inference (MI; Mixup Inference). Experimental results on Cifar 10, and Cifar 100 show that MI can boost the classification performance in combination with interpolated AT (Adversarial  Training) and mixup.\n\nI lean to accept this paper. The proposed method is simple but effective, moreover well-motivated. The experimental results, including several ablation studies, show a high versatility with existing methods.\n\nMy minor concerns are, however, consisting of two points.\n- The authors should repeat the experiments several times and show the averages and standard errors to make the significance clear.\n- Both Cifar 10 and Cifar 100 are relatively small scale datasets. I would like the authors to investigate larger ones.", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 4}, "B1g-lU5W9B": {"type": "review", "replyto": "ByxtC2VtPB", "review": "This paper introduces a novel method for an adversarial attack named mixup inference (MI).  Most of the work focuses on embedding mixup mechanism in the training phase, but MI uses the mixup in the inference phase. MI method has two main effects for the adversarial attack: one is perturbation shrinkage, and the other one is input transfer because MI can exploit\nthe induced global linearity. The experimental results show that MI can return more reliable predictions under different threat models.\n\nThis paper should be accepted because the proposed method is super simple but effective for defending from adversarial attacks under different threat conditions. This paper is well-written, including theoretical insights on why the MI method works.\n\nThe reviewer has some questions or comments to clarify the paper:\n1) In the explanation of the MI method, the authors assume only the cases where the input data is correctly classified if it is clean, or wrongly classified if it is adversarial. In a realistic situation, the classifier sometimes outputs mislabels. Thus is the discussion in Sec.3 valid if the clean input data misclassified?\n\n2) To predict the category of the input, MI methods must perform inference N times. It is not efficient. Are there any ideas to reduce the number of inferences?\n\n3) MI-Combined seems ad-hoc. It would be better to state its justification by theory.\n\n4) The same idea of mixup was proposed at the same conference (ICLR2018). It should be cited.\nTokozume et al., Learning from Between-class Examples for Deep Sound Recognition. ICLR, 2018.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 4}, "rygsYPl9tB": {"type": "rebuttal", "replyto": "SkeFK-RuYS", "comment": "Thank you for your recommendation on these interesting work, we read the linked papers to check if they are related to our work. As indicated by Reviewer1, our paper focuses on using mixup at inference time for better robustness.\n\nFirst of all, we find that none of the three linked papers is based on the mixup method (Zhang et al. ICLR 2018), or mentions the mixup method in their related work.\n\nThe three linked papers propose and analyze one similar method, named weighted nonlocal Laplacian (WNLL) layer as the output layer for DNNs. The claimed advantages of the WNLL layer is mainly on data-efficient learning and robustness in the semi-supervised setting, while we consider supervised learning. Besides, the WNLL layer benefits the learning in the training phase, while our mixup-inference (MI) method further improves the robustness of mixup-trained models in the inference phase.\n\nThanks again for your kind suggestions, but it seems that the linked papers are not quite related to our work. Please feel free to let us know if we misunderstand some parts of your linked papers.", "title": "The linked papers seem not quite related"}}}