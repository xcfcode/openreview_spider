{"paper": {"title": "Multi-source Multi-view Transfer Learning in Neural Topic Modeling with Pretrained Topic and Word Embeddings", "authors": ["Pankaj Gupta", "Yatin Chaudhary", "Hinrich Sch\u00fctze"], "authorids": ["pankaj_gupta96@yahoo.com", "yatinchaudhary91@gmail.com", "hinrich@hotmail.com"], "summary": "Transfer learning in Neural Topic Modeling using Pretrained Word and Topic Embeddings jointly from one or many sources to improve quality of topics and document representations in sparse-data settings ", "abstract": "Though word embeddings and topics are complementary representations, several\npast works have only used pretrained word embeddings in (neural) topic modeling\nto address data sparsity problem in short text or small collection of documents.\nHowever, no prior work has employed (pretrained latent) topics in transfer learning\nparadigm. In this paper, we propose a framework to perform transfer learning\nin neural topic modeling using (1) pretrained (latent) topics obtained from a large\nsource corpus, and (2) pretrained word and topic embeddings jointly (i.e., multiview)\nin order to improve topic quality, better deal with polysemy and data sparsity\nissues in a target corpus. In doing so, we first accumulate topics and word representations\nfrom one or many source corpora to build respective pools of pretrained\ntopic (i.e., TopicPool) and word embeddings (i.e., WordPool). Then, we identify\none or multiple relevant source domain(s) and take advantage of corresponding\ntopics and word features via the respective pools to guide meaningful learning\nin the sparse target domain. We quantify the quality of topic and document representations\nvia generalization (perplexity), interpretability (topic coherence) and\ninformation retrieval (IR) using short-text, long-text, small and large document\ncollections from news and medical domains. We have demonstrated the state-ofthe-\nart results on topic modeling with the proposed transfer learning approaches.", "keywords": ["Neural Topic Modeling", "Transfer Learning", "Unsupervised learning", "Natural Language Processing"]}, "meta": {"decision": "Reject", "comment": "This paper presents a transfer learning framework in neural topic modeling. Authors claim and reviewers agree that this view of transfer learning in the realm of topic modeling is novel.\n\nHowever, after much deliberation and discussion among the reviewers, we conclude that this paper does not contribute sufficient novelty in terms of the method. Also, reviewers find the experiments and results not sufficiently convincing.\n\nI sincerely thank the authors for submitting to ICLR and hope to see a revised paper in a future venue."}, "review": {"S1xyosr2iH": {"type": "rebuttal", "replyto": "SkgKHY_aKS", "comment": "Thanks for increasing your rating and leaning towards accept!\n\nThanks for acknowledging contribution of our proposed transfer learning approaches in topic modeling.", "title": "\"Enough contribution of transfer learning\""}, "HyenXIMZoH": {"type": "rebuttal", "replyto": "HyeGN5C85B", "comment": "Thanks for your reviews and positive comments, e.g., \"well written\". \n\nThe extensive experimental results (Table 5, 6 and 7) have shown significant improvements in terms of perplexity (PPL), topic coherence (COH) and IR scores using 7 datasets.  The improvements are EXPLICITLY mentioned in Tables 5, 6 and 7 (see \"Gain%\"). Also, see plots 2 (a,b,c,d,e), where our proposed model outperforms all the baselines at all the fractions in terms of retrieval precision.\n\nBeyond perplexity, we have also shown large gains in topic coherence scores due to improved topic quality and noticeable gains in precision for IR task.  \n\nFollowing are the 20 (some) EVIDENCES of significant improvements:\n\n\"Gain% vs DocNADE baseline\" (Table 5): \nOn 20NSshort: 10.9% (COH), 8.28% (IR)\nOn TMNtitle: 7.22% (PPL), 6.06% (COH) and 9.21% (IR)\nOn 20NSsmall: 37.9% (COH), 20.7% (IR).\nOn Ohsumedtitle: 4.01% (PPL) and 13.8% (IR) (Table 7)\nOn Ohsumed: 12.3% (PPL) and 4.35% (IR) (Table 7)\n\n\n\"Gain% vs DocNADEe baseline\" (Table 6): \nOn 20NSshort: 9.95% (COH), 8.84% (IR)\nOn TMNtitle: 4.60% (COH) and 7.04% (IR)\nOn 20NSsmall: 39.3% (COH). \nOn Ohsumedtitle: 17.3% (PPL) and 4.0% (IR) (Table 7)\nOn Ohsumed: 8.5% (PPL) and 4.91% (IR) (Table 7)\n\nAdditionally, #R4 and #R2 have acknowledged the noticeable gains achieved in this paper.", "title": "\"20 Evidences of significant improvements using 7 datasets (small/large) across 3 evaluation measures\""}, "HylGDaWujB": {"type": "rebuttal", "replyto": "ByxODxHYwB", "comment": "Dear Reviewers,\n\nThanks again for reviewing our paper! We have responded to your queries and we are looking forward to discuss further. \n\nEven though there is NO negative/critical criticism, the ratings are NOT positive. We mostly found clarification queries that we have addressed in our response. We have also highlighted our contributions and SIGNIFICANT GAINS that our proposed methods achieved. \n\nWe would appreciate if the reviewers could participate in the rebuttal and raise further questions, if any.  \n\nAlso, we would acknowledge if you could justify your negative ratings or update them accordingly based on our response below.\n\nThanks!", "title": "Rejection without a single constructive/negative comment? Please justify negative scores!"}, "SkgKHY_aKS": {"type": "review", "replyto": "ByxODxHYwB", "review": "On the basis of existing topic modelling approaches, the authors apply a transfer learning approach to incorporate additional knowledge to topic models, using both word embeddings and topic models. The underlying idea is that topic models contain a global view that differs on a thematic level, while word embeddings contain a local, immediate contextual view. The combination of both local and global view transfer to enhance a topic model is the main contribution of this paper, especially when using multiple sources (therefore the title: multi-source multi-view transfer).\nGiven a document collection, DocNADE is used to generate the topic-word matrix. In the local view transfer step, the pre-trained WordPool is used, from which knowledge is transferred on the target document. The global view transfer is done by transferring knowledge from the pre-trained TopicPool to the target. As described in Algorithm 1 in the paper, both Word- and TopicPool are jointly used in the transfer learning process. \nFor evaluation, three different measures are taken into account: Perplexity, Topic Coherence and Precision (Information Retrieval). In comparison to a DocNADE only approach, all values are better in the settings that use the transfer learning approach. Compared to DocNADE + word embeddings, the results are competitive as well. In both experiments, the multi-source setting evaluates best overall.\n\nIn conclusion, the paper shows that exploiting multiple sources and views in transfer learning leads to an overall improvement in the given tasks. The main contribution is the usage topic models in a transfer learning framework. Additionally the use of multi-source word embeddings is novel too, especially in the joint setting with the topic model transfer. The paper shows how the DocNADE approach is enhanced to make use of both local and global view transfer and how this enhancement leads to improved performance on various related tasks. \nStill, the overall contribution is mostly in combining existing methods and can be judged as rather incremental.\n\nMinor note: A small mistake has been found in Table 5. The best perplexity value in the first column is not the bold 638, but the 630 in the local-view transfer setting.\n\nEdit after rebuttal: In my review I did not value the contribution of the transfer learning approach enough. So, when also considering the extensive evaluation I am now leaning towards accept.", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 3}, "BJlcKuzbjr": {"type": "rebuttal", "replyto": "SkgKHY_aKS", "comment": "Thanks for your reviews, positive comments about \"novelty\" and acknowledging gains obtained by our proposed modeling.\n\nAs far as we know, this is the first/novel work that introduces:\n(1) single-source pre-trained topic embeddings,\n(2) single-source joint pre-trained word and topic embeddings, and  \n(3) multi-source transfers using pre-trained topics and word embeddings jointly in neural topic modeling under transfer learning paradigm.\n\nThis work DOES NOT focus on introducing a new topic model; however, we focus on introducing a novel transfer learning mechanism in neural topic modeling using complementary representations. Therefore, we have used the existing neural topic model, i.e., DocNADE to address data sparsity issues. \n\nThe experimental results have clearly shown noticeable gains in topic modeling due to the proposed transfer learning methodology using 7 target datasets from several domains (e.g., news, medical, etc.), evaluated using perplexity, topic coherence and information retrieval task.  \n\nThanks for the minor comment. We will correct it and will update the gain% as well. :)", "title": "About \"combining existing methods and can be incremental\": First Work to perform Multi-view and Multi-source transfer learning in Neural topic modeling"}, "HJeZYDGWir": {"type": "rebuttal", "replyto": "HyeGN5C85B", "comment": "As far we we know, we have covered all the experimental settings, where we have clearly/individually shown contributions of each of the components. \n\nSee Table 5, 6 and 7, where the scores are reported due to: \n(1) only single-source word embedding transfer, i.e., LVT, \n(2) multi-source word embedding transfer, i.e., MST+LVT, \n(3) only single-source topic embedding transfer, i.e., GVT, \n(4) multi-source topic embedding transfer, i.e., MST+GVT, \n(5) single-source joint word and topic embeddings transfers, i.e., MVT=LVT+GVT, and \n(6) multi-source joint word and topic embeddings transfers, i.e., MST+ MVT. \n\nNotice that the topic-embedding transfer is performed via the regulalrization term. Also, mentioned in algorithm #1.\n\nWe are happy to answer if something is still not clear. Please point out precisely. ", "title": "About the \"regulariser and clear contribution of each component\""}, "BJl1wBf-iH": {"type": "rebuttal", "replyto": "SygTrGoa9r", "comment": "Thanks for your (emergency) reviews.\n\nThanks for your positive comments on experimental setup and acknowledging that our transfer learning approaches introduced in neural topic modeling clearly outperform several baselines.\n\n>> \"Word Embedding Alignment\"\nYes, we do.\nPlease see section 3, page 6 in \"Reproducibility\" paragraph (line 3). Also, mentioned in caption of figure 6 as well as in Appendix C.4 (the last paragraph).\n\nWe perform the word embeddings alignment in all the \"+Glove\" settings (Table 6) to \n(1) overcome the DocNADEe (baseline topic model) limitation (word-embedding size must be same as the number of topics), and \n(2) align vector spaces of word-embeddings obtained from several sources as well as from several different training processes, e.g., from Glove, FastText and word embeddings from topic models. \n\nThe focus of our work is to demonstrate the joint word and topic embeddings transfer in neural topic models from one or many sources. ", "title": "About \"Word Embedding Alignment\": Yes, we do align Word Embeddings (mentioned 3 times in the paper)"}, "HyeGN5C85B": {"type": "review", "replyto": "ByxODxHYwB", "review": "The paper proposes a multi-source and multi-view transfer learning for neural topic modelling with the pre-trained topic and word embedding. The method is based on NEURAL AUTOREGRESSIVE TOPIC MODELs --- DocNADE (Larochelle&Lauly,2012). DocNADE learns topics using language modelling framework. DocNADEe (Gupta et al., 2019) extended DocNADE by incorporating word embeddings, the approach the authors described as a single source extension of the existing method.\n\nIn this paper, the proposed method adds a regularizer term to the DocNADE loss function to minimize the overall loss whereas keeping the existing single-source extension. The authors claimed that incorporating the regularizer will facilitate learning the (latent) topic features in the trainable parameters simultaneously and inherit relevant topical features from each of the source domains and generate meaningful representations for the target domain. The analysis and evaluation were presented to show the effectiveness of the proposed method. However, the results are not significantly improved than the based line model DocNADE. \n\nOverall, the paper is written well. However, it is not clear to me that the improved results are resulted due to multi-source multi-view transfer learning or for the better leaning of the single-source model due to the incorporation of the regularizer. \n\n\n", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 3}, "SygTrGoa9r": {"type": "review", "replyto": "ByxODxHYwB", "review": "This is an emergency review.\n\nThis work proposes a novel method to use pre-trained topic embeddings and pre-trained word embeddings obtained from various corpora in the transfer learning framework. \n\nTheir model architecture is based on DocNADE, unsupervised neural-network based topic model, and the authors propose two strategies to use pre-trained topic embeddings and pre-trained word vectors.\n1) Addition of a weighted sum of pre-trained word embeddings and the hidden vector of DocNADE.\n2) L2-Regularization term between topic embedding of DocNADE and pre-trained topic embeddings. They propose to align these two embeddings by multiplying align matrix \"A\" to the topic embedding of DocNADE.\n\nThey show the transfer learning performance of their model on various source/target domain datasets, including medical target corpora, and verify that their model outperforms on a short text and small document collection.\n\nStrengths.\n1. Comparison with the data augmentation baseline shows the performance gain is not only from bigger training data. Even though comparison with the naive baseline (data augmentation) seems too obvious, I think the results clearly show their claim about the importance of using transfer learning in neural topic modeling domain.\n2. As the first approach that introduces a novel transfer learning framework with pre-trained topic embeddings, they show tons of experimental results with various datasets and metrics to show the specification of their method. Their experimental setting is well designed.\n\nWeaknesses and comments:\nTheir method to combine pre-trained word embeddings and pre-trained topic embeddings is too simple. Since this is the first approach to use topic embedding in the transfer learning field, the simplicity of the proposed method is somewhat necessary. However, a weighted sum of pre-trained topic/word vectors seems not enough to transfer multisource knowledge. For instance, word vectors obtained from individual training processes do not share embedding vector space. As you apply the alignment method to topic embeddings from various sources, you should align word embeddings too.", "title": "Official Blind Review #4", "rating": "6: Weak Accept", "confidence": 2}}}