{"paper": {"title": "Mesh-Free Unsupervised Learning-Based PDE Solver of Forward and Inverse problems", "authors": ["Leah Bar", "Nir Sochen"], "authorids": ["barleah.libra@gmail.com", "sochen@tauex.tau.ac.il"], "summary": "Solving PDEs with deep learning techniques in an unsupervised fashion with regularizers for forward and inverse problems.", "abstract": "We introduce a novel neural network-based partial differential equations solver for forward and inverse problems. The solver is grid free, mesh free and shape free, and the solution is approximated by a neural network. \nWe employ an unsupervised approach such that the input to the network is a points set in an arbitrary domain, and the output is the\nset of the corresponding function values.  The network is trained to minimize deviations of the learned function from the PDE solution and \nsatisfy the boundary conditions. \nThe resulting solution in turn is an explicit smooth differentiable function with a known analytical form. \n \nUnlike other numerical methods such as finite differences and finite elements, the derivatives of the desired function can be analytically calculated to any order. This framework therefore, enables the solution of high order non-linear PDEs. The proposed algorithm is a unified formulation of both forward and inverse problems\nwhere the optimized loss function consists of few elements: fidelity terms of L2 and L infinity norms, boundary conditions constraints and additional regularizers. This setting is flexible in the sense that regularizers can be tailored to specific \nproblems. We demonstrate our method on a free shape 2D second order elliptical system with application to Electrical Impedance Tomography (EIT). ", "keywords": ["PDEs", "forward problems", "inverse problems", "unsupervised learning", "deep networks", "EIT"]}, "meta": {"decision": "Reject", "comment": "This paper proposes modifying the training loss for neural net-based PDE solvers, by adding an L_infty (max) term to the standard L_2 loss.  The motivation for this loss is sensible in that it matches the definition of a strong solution, but this is only a heuristic motivation, and is missing a theoretical analysis.\n\nThis paper's lack of novelty and polish, as well as the lack of clarity in the implementation details, makes this a narrow reject."}, "review": {"Hkem-N2DFH": {"type": "review", "replyto": "rklv-a4tDB", "review": "In this work, the authors propose an approach to solve forward and inverse problems in partial differential equations (PDEs) using neural networks. In particular, they consider a specific type of second-order differential operator combined with Dirichlet boundary conditions, and suggest how neural networks with suitable training objectives can be used to solve both types of problems. For a specific elliptic system relevant for Electrical Impedance Tomography (EIT), they then present numerical results obtained with their method and compare those to high-resolution finite element solutions and previous work.\n\nThe paper addresses a very general and important problem with wide ranging applications. People have solved PDEs using spectral, finite difference, finite volume or finite element methods for decades and there is a huge body of literate on this subject. The neural network based approach proposed in this paper seems general and simple with encouraging experimental results. However, there are several important points missing in the current paper based on which I would recommend rejecting it. However, I would be willing to increase the score if these points were addressed in sufficient detail.\n\nMajor comments:\n\n1) Choice of problems\n\nThere are many important problems in the literature that belong to the general type of PDE considered in this work (e.g. in electrostatics). To appreciate and understand the merits and limitations of the proposed approach better, it would be highly necessary to apply it to a wider range of problems, including ones with known analytical solution. \n\n2) Convergence tests\n\nConvergence tests are an important part that is missing in the current paper. In particular, how does the error in the neural-network solution decay with respect to the number of random points used to train the network? Plots comparing a suitable error norm versus the training dataset size would be very informative. Furthermore, what are the conditions, if any, to guarantee convergence to the exact solution? The authors mention in the last paragraph of the Discussion that a more rigorous analysis will be published elsewhere. However, as error analysis is such an integral part of the study, I think it needs to be addressed, at least to some extent, already in the current paper.\n\n3) Related work\n\nI don\u2019t think the current work is put in sufficient contrast with existing work. Several related papers are mentioned in the introduction and the experimental section includes one other method (GDM method of Sirignano & Spiliopoulos) for comparison. However, a more thorough discussion on how the current approach complements existing literature on neural network based PDE solvers would be in order. \n\nMinor comments:\n\ni) There are quite a few typos and grammar mistakes in the paper that need to be fixed. To give a few examples:\n\u2018the natural structure presents\u2019 -> \u2018the natural structure present\u2019\n\u2018Shr\u00f6dinger\u2019 -> \u2018Schr\u00f6dinger\u2019\n\u2018approximated by neural network\u2019 -> \u2018approximated by a neural network\u2019\n\u2018on circular and\u2019  -> \u2018on a circular and\u2019\n\u2018forces the equation\u2019 -> \u2018enforces the equation\u2019\netc...\n\nii) Eq. (7): There is a parenthesis missing on the LHS.\n\niii) The authors show many plots in Figs 3-9 but don\u2019t comment much on the results in the main text. A lot of the results presented could therefore go into an appendix. I would find it better to present fewer results/plots and discuss those in more detail.\n\niv) It would be good to define the Nabla operator in Eq. 5 for completeness.\n\nv) Please put references to equations and papers in parentheses or, at least, separate them otherwise from the sentence, to avoid confusion. For example:\n- the reference to \u2018Evans (2010)\u2019 below Eq. 5, or\n- the list of dangling references before the last paragraph on page 3,\n- the reference to Sirignano & Spiliopoulos in the caption of Table 1,\netc...\n\nvi) Please expand the captions in Fig. 3 to make the figure meaningful as a stand-alone, without having to read the entire text to understand what 'phantom' means. The same applies to all other captions.\n\nvii) Perhaps it would be clearer to use \u2018row\u2019 instead of \u2018line\u2019 when referring to the results in the table.\n\nviii) In Sec. 7 \u2018s\u2019 in \u2018Ns\u2019 should be a subscript.\n\nix) Explain what the acronym PSNR means.\n\nx) Please elaborate on why you take the top k values in Eq. (7). What happens if you take more/less?\n\nxi) First paragraph in Discussion: I am not sure the robustness w.r.t. the hyperparameters is really so surprising given that you apply the solver to very similar problems. If you could show that the same hyperparameters worked on a completely different problem, that would be much more interesting.\n\nxii) Please add references for the first paragraph of the introduction.\n\n\n*********************************************************\nI increased my rating based on the revisions. \n*********************************************************", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "SJx53NmviB": {"type": "rebuttal", "replyto": "rklv-a4tDB", "comment": "We uploaded a revised version of the manuscript.\nThanks to the constructive remarks of the reviewers, we considerably improved the text.\nWe highlighted our contribution, especially the promotion of strong solutions to the PDEs by our framework compared to recent approaches. We added comparisons of other inverse problems to Xu and Darve(2019).\n\nWe improved the notations, figure captions and discussion. Our modifications are marked in blue text. \n\n\nThank you very much!", "title": "General comments"}, "rygxXEafiB": {"type": "rebuttal", "replyto": "Hyg9zSZCYr", "comment": "We thank the reviewer for the constructive comments.\n\n1. We added an explanation for the L_inft term in the text. The point here is that the loss function is defined in the weak sense via a L_2 norm since we average the loss function over the minibatch The integral is insensitive \nto a point jump in the value of the integrand.  This manifest itself by  point discontinuities in u(x) that are seen empirically as well. We added the L_infty terms to have a solution in the strong sense.  We showed that this term is empirically crucial as  well. \n\nWhile supervised learning use directly the data as prior, unsupervised methods that tackle ill-posed problems must use a prior to steer the solution to the right subspace of possible solutions. The exact structure of the loss function and the choice of prior is not, in our eyes,  a \u201csimple addition\u201d but a major modeling issue which can handle noisy measurement and compensate ill-posed cases. \n\n2. A discussion on the inverse problem  will be added in the paper.  \nThanks. This point is now  clarified in the paper. In Eq. 3 we demonstrate the solution of the forward problem so the coefficient is known in the domain and on the boundary.  In eq. 4 the inverse problem is done and there we need to use the boundary information of the parameter. \n\n3. Thanks. We added two inverse problem examples: diffusion and wave equations in the presence of additive Gaussian noise with comparison to a Xu and Darve(2019).\n\n4. Thanks for referring us to this publication that evaded our search. We  added now  comparisons to this paper. We exemplified our method in two dimensions in the presence of additive Gaussian noise.\n\nMinor issues:\nWe appreciate the reviewer for carefully reading the manuscript and for providing these corrections. . It will be corrected shortly.", "title": "Answers to the reviewer's concerns"}, "rJxcwUpzir": {"type": "rebuttal", "replyto": "Hkem-N2DFH", "comment": "We thank the reviewer for the constructive and supportive comments.\n\n\n1. Thanks. We added two inverse  problem examples:  diffusion and wave equations in the presence of additive Gaussian noise and compared our results to Xu and Darve (2019).\n\n2. Thanks. We added a plot that demonstrates the performance as a function of the number of points in the training phase.  As expected, with large number of points we obtain a plateau while with small and intermediate number (up to 50000) the error is not monotonic due to local minima. \n\n3. .We added an approximation result by Shen et al. that gives a bound on the representation of a function in terms of the number of layers and the number of neurons in each layer.  The problem of bounding the error of approximation by a network is notoriously open problem. In our case the problem is more complex since the network approximates the function indirectly via the PDE. This is under current study. \n\n4. Related work will be added to the introduction.\n\nMinor concerns:\nWe highly appreciate the reviewer for carefully reading the manuscript  and for providing these corrections. . It will be corrected shortly.\n\n\n", "title": "Answers to the reviewer's concerns"}, "H1xBYbTGiS": {"type": "rebuttal", "replyto": "BJeTZ3a0tH", "comment": "We thank the reviewer for the constructive remarks\n\n1. We compared to the main publication that solved the forward problem. \nReviewer 3 referred us to a novel work in inverse problems. We prepared two more inverse problem examples. Diffusion and wave equations. We compared our results to Xu and Darve (2019).  We solved the inverse problems in 2D with several Gaussian noise levels. The results and comparisons are reported in Table 4 and figures 9,10 in the revised paper.\n\n2. We would like to add that the main challenge is to perform a full tomography from data that is defined only on the boundary for the function and the parameter sigma.  For this to be feasible a good forward and inverse algorithm should be available and in the same framework.  The full tomography is under current study and we will have publishable results in a couple of months. \n\n3. Quote: \"And when you try to use the solution, \nor to compare with some ground truth generated by traditional methods, usually we still need to make the solution discrete to use it. \"\nAnswer:\nWe should have been clearer on that point. The network is continuous in the input . In order to compare with the ground truth generated by traditional method we need to compare it on the grid where the traditional method is defined. The network value than was calculated on these grid points. The function value can be evaluated at any arbitrary point x. It was not discretized. An explanation for this will be added to the text. \n\n4. We added comparisons for an inverse solver as well. We address diffusion and wave equations in two dimensions in the presence of Gaussian noise.\n\n5. We  extended the explanation of the L_infty term in the text. The point here is that the loss function is defined in the weak sense via a L_2 norm since we average the loss function over the minibatch. The integral is insensitive \nto a point jump in the value of the integrand.  This manifest itself by point discontinuities of  u(x) that are seen empirically as well. We added the L_infty terms to have a solution in the strong sense. \n\n\n\n", "title": "Answers to the reviewer's concerns"}, "rylo9Rnzir": {"type": "rebuttal", "replyto": "rklv-a4tDB", "comment": "We thank the reviewers for their constructive comments.  We added more examples and comparisons and the first revised version was uploaded.\nThe final version will  be completed in few days.\n\nThe changes are marked by blue text.\n", "title": "General comment"}, "Hyg9zSZCYr": {"type": "review", "replyto": "rklv-a4tDB", "review": "This paper presents an unsupervised learning approach to solve forward and inverse problems represented by partial differential equations. A framework is proposed based on the minimisation of loss functions that enforce the boundary conditions of the PDE solution and promote its smoothness. The method is then applied to solve forward and inverse problems in electrical impedance tomography.\n\nFlexible machine learning approaches to solving partial differential equations is a subject of ongoing research. While the paper presents an elegant solution folding forward and inverse problems into a single framework, the presentation is missing a few important details which difficult the assessment of the contribution and favour a rejection of the paper. The main issues are insufficient experimental comparisons and a lack of theoretical support for the method.\n\nMajor issues:\n\n1. When compared to DGM (Sirignano & Spiliopoulos, 2017), for the forward problem, the method only differs in the form of the loss function, which is almost identical, with the exception of the additional L-infinity term and the optional user-defined regularisers. The argument for the inclusion of the L-infinity loss is its high sensitivity to outliers, enforcing a smooth solution. (a) Why PDE solutions learned using the original loss from DGM, which also yields continuous functions, should present outliers in the first place? Moreover, the possibility of adding a user-defined regularizer seems to be a relatively simple extension. (b) What should the theoretical or practical implications for the extra user-defined regularisation term be?\n\n2. The loss function for the inverse problem, which seems to be one of the paper\u2019s contributions, misses a dedicated discussion. An important detail in this loss is the third term, which enforces boundary conditions for the coefficients at the boundary of the domain. In Equation 2, however, the coefficients only affect the PDE through the Lu term over the domain, not its boundary. So what does \\theta_0 mean in Equation 4 then?\n\n3. The paper proposes a general framework, but experimental results are presented for only one specific problem, the electrical impedance tomography. The generalisability of the method to more complex problems, such as PDEs with time components and a high-dimensional spatial domain, cannot be inferred. Adding experimental comparisons on higher-dimensional domains would strengthen the paper.\n\n4. Experiments only present comparisons to relevant state-of-the-art methods (DGM) in the forward problem. There are no comparisons against other methods for the inverse and the free-shape geometry problems. For example, have the authors considered the method in [A]?\n[A] Xu, Kailai, and Eric Darve. \"The neural network approach to inverse problems in differential equations.\" arXiv preprint arXiv:1901.07758 (2019).\n\nMinor issues:\n\n1. The background on PDEs is relatively short for a machine learning conference. (a) There lacks an explanation on what the operator \\mathcal{L} means. (b) Equation 1 lacks an explicit use of \u201cu(x)\u201d, instead of simply \u201cu\u201d, causing confusion with the dependence of the coefficients on \u201cx\u201d. (c) The meaning of the index subscripts on the partial derivatives is also not made clear, especially if \u201cu\u201d could be interpreted as a vector-valued function for someone unfamiliar with PDEs. Replacing \u201csome u\u201d by \u201csome u:R^d\\to\\R\u201d would already help.\n\n2. What does \u201cn\u201d mean in the electrical current equations in Sec. 3?\n\n3. The derivative of a scalar \u201cu(x)\u201d with respect to a vector \u201cx\u201d should be a vector. So what are the plots in figures 4, 5 and 8 showing when referring to du/dx? Is that the magnitude of the vector or the partial derivative with respect to a single spatial component?\n\n4. What does \u201cPSNR\u201d stand for?\n\n5. Indirect citations in the text should be enclosed by brackets using something like the \u201c\\citep\u201d command from the package \u201cnatbib\u201d.\n\n6. In Table 1, there is a typo: \u201cGDM\u201d->\u201dDGM\u201d.\n\n7. The context contains a few minor grammatical issues that can be distracting at times, but should be solvable by revision.", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 2}, "BJeTZ3a0tH": {"type": "review", "replyto": "rklv-a4tDB", "review": "Summary: In the paper, the authors purpose to use neural networks to model both the function $u$ and the parameters and in a sense, unify the forward and inverse problems. The authors demonstrate the work on Electrical Impedance Tomography problem. The authors also purpose a new regularizer, the sum of L2 and L_inf norm of the differential operator.\n\nConcerns: there have been plenty of works that use neural networks to model the function $u$ for forward problems and another bunch of works that use neural networks to model parameters to do inverse problems.\n\nIt is not clear to me if combining the two will really give us benefits, since we are still doing these two problems separately. If we are doing some alternating training, unifying them could be useful.\n\nThe mesh free part is less interesting in my opinion. Works using feed forward neural networks are mesh free in general. And when you try to use the solution, or to compare with some ground truth generated by traditional methods, usually we still need to make the solution discrete to use it. \n\nThe experiments in the paper is limited. It compares with only one work in the forward task, but no comparison in the inverse problem. It is hard to evaluate its performance.\n\nThe theory is also needed. It is not very clear why L2 + L_inf regulation terms will help us. \nAfter all, in computer science, conference papers are considered as final publications. So a more extensive studied is expected. I would suggest submit this work for a workshop. \n\nDecision: This work need further experiments and theoretical analyze. I suggest weakly reject this paper.", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 2}}}