{"paper": {"title": "Measuring and mitigating interference in reinforcement learning", "authors": ["Vincent Liu", "Adam M White", "Hengshuai Yao", "Martha White"], "authorids": ["~Vincent_Liu3", "~Adam_M_White1", "~Hengshuai_Yao2", "~Martha_White1"], "summary": "", "abstract": "Catastrophic interference is common in many network-based learning systems, and many proposals exist for mitigating it. But, before we overcome interference we must understand it better. In this work, we first provide a definition and novel measure of interference for value-based control methods such as Fitted Q Iteration and DQN. We systematically evaluate our measure of interference, showing that it correlates with forgetting, across a variety of network architectures. Our new interference measure allows us to ask novel scientific questions about commonly used deep learning architectures and develop new learning algorithms. In particular we show that updates on the last layer result in significantly higher interference than updates internal to the network. Lastly, we introduce a novel online-aware representation learning algorithm to minimize interference, and we empirically demonstrate that it improves stability and has lower interference.", "keywords": ["Reinforcement Learning", "Representation Learning"]}, "meta": {"decision": "Reject", "comment": "The paper investigates interference in reinforcement learning and introduces a novel measure that can be used in value-based methods.\nAlthough the reviewers acknowledge that the paper has merits (the topic is relevant and the paper is well written), they feel that the contribution is not sufficiently supported by either a theoretical or empirical analysis. The authors' responses have solved some of the reviewers' concerns, but they agree that this paper is not ready for publication in its current form.\nI encourage the authors to update their paper following the reviewers' suggestions, in particular by improving the empirical analysis where comparisons with alternative methods (e.g., AVI/API methods that introduce regularization) need to be added."}, "review": {"7bxwste9euB": {"type": "review", "replyto": "26WnoE4hjS", "review": "This paper studies the reason for interference, aka catastrophic forgetting, when using parametric models for Reinforcement Learning. The authors draw the connection with previous methods and introduce some reasonable measure of interference. Then, they introduce a method to explicitly address the problem of interference, showing some good empirical results w.r.t. selected baselines.\n\nI think that this paper studies an interesting problem, but its analysis is a bit superficial and not supported by rigorous theoretical analysis. The proposed measures of interference make sense, but it seems to me that they are not reliable measures, especially in the considered online learning setting. In fact, the TD-error may increase significantly across several iterations because of the agent exploring unvisited states, and not only because of interference. Previous works, e.g. Prioritized Experience Replay (Schaul et al, 2016), show that a higher TD-error is actually desirable to guide exploration, so I'm unsure how the motivation behind this work relates with the literature. Moreover, since catastrophic forgetting is mostly problematic in deep RL, I'd have liked a stronger focus on deep RL, where the author could have tested the benefit of their approach on several algorithms based on the TD error, e.g. DQN, DDPG, TD3, SAC. It is also confusing how the authors refer to FQI as an online algorithm. FQI is known to be a batch RL algorithm, i.e. an offline algorithm where a fixed dataset of transitions collected by another agent is available, even though it can be used for iterative policy updates, but this is not the typical scenario of FQI.\n\nWithout an extensive empirical analysis, and considering the absence of a rigorous theoretical analysis motivating the proposed interference measure, I think this paper is not ready for publication and I encourage the authors to improve it, especially showing stronger and more significant empirical evidence over representative baselines, perhaps even considering some multi-task and/or lifelong learning problems where interference constitutes a bigger issue.\n\nPros\n------\n* The considered problem is interesting for a broad group of researchers in RL;\n* The presentation is clear enough.\n\nCons\n-------\n* The proposed measures are intuitive, but not supported by strong theoretical guarantees. In particular, the effect of exploration on the behavior of the TD error is not accurately discussed;\n* Absence of deep RL experiments to show the effectiveness of the proposed approach on more challenging problems.\n\n\nPost-rebuttal feedback\n-------------------------------\nI thank the authors for their reply. I still think that this paper has the major problem of presenting results about interference that are not strong enough to be published. In particular, I agree when the author say that \"the paper tries to bring conceptual clarity to this important topic, and provide a clear empirical methodology to measure interference and understand correlations to forgetting\", but still I think that the overall contribution consists \"just\" of one (of several other possible ones) intuitive method to measure interference, without a solid motivation. To me, this paper is promising but should present more significant results to have a stronger impact. I think the options are only two: stronger theoretical motivation, larger empirical analysis especially considering deep RL. Between the two, I consider the second option the best.", "title": "Interesting work, with weak theoretical motivation", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "NuMclkgiSnM": {"type": "review", "replyto": "26WnoE4hjS", "review": "Summary\n\nThe paper studies interference and forgetting in the context of reinforcement learning (RL).  On the example of the Iterative Value Estimation family of algorithms, the authors define interference as the increase of the true Q target prediction error after updating Q function parameters. Since the true Q target is usually unknown, the authors propose to use the difference of squared TD-errors between updates as a proxy for interference. The paper further defines forgetting as the difference between the current agent performance and the best performance across all previous updates. On CartPole and Acrobot environments, the authors show a positive correlation between the proposed measures of interference and forgetting. They further qualitatively demonstrate that updates of the last layer weights result in higher interference compared to updates of intermediate layers. Finally, the authors propose an algorithm based on meta-learning for learning representations that minimize interference resulting in more stable return plots on Acrobot. \n\nStrengths\n- The topic brought in the paper is important: the distribution shift and moving targets in RL are indeed problematic and often lead to instabilities during training and forgetting of high-return policies.\n- The paper is written clearly and generally easy to follow.\n\nWeaknesses\n- The proposed algorithm is designed to explicitly minimize interference. However, seeking for minimizing interference / forgetting on its own might prohibit exploration. For example, an agent that achieves the worst possible returns and is not learning at all will have zero interference / forgetting. (Perhaps it is more reasonable to seek monotonic policy improvement?)\n- The finding that changes in parameters of the last layer result in higher interference seems unsurprising as, generally, changes in the final layer parameters affect the output of a neural network more than changes in intermediate layers.\n- The proposed method for learning representations is based on meta-learning. It is unclear whether the learning curves on Acrobot are more stable due to claimed minimization of interference or due to using a more powerful optimization method. An ablation study will improve the quality of the paper.\n- The authors should consider using harder environments to make the experimental results more convincing.\n\nPositioning relative to the literature\n- One of the main methodological contributions of the paper is using squared TD-error as a proxy for measuring interference. At the end of section 4, the authors mention related methods based on first-order approximation of interference. However, no comparison with the methods was provided.\n- Similarly, the authors do not compare with other methods that address the forgetting in the RL context. For example, [2] uses simple weight averaging for minimizing forgetting and achieves learning curves similar to the ones in Figure 4.\n- Perhaps the authors should reconsider the term \u201cinterference\u201d. The cited paper [1] uses the term \u201cinterference\u201d for an inner product between gradient estimates evaluated at different data points and seeks, in contrast, to maximize interference.\n\nRecommendation\n\nThe reviewer leans towards rejecting the paper. The discussed problem is important, however, the findings of the paper do not seem generally surprising. The authors claim that \u201cthere is no established online measure of interference for RL\u201d but the resulting measure of interference is simply a difference between squared TD-errors before and after an update. The proposed measure of forgetting, being a difference of current returns and previous best returns, has limited novelty too. Moreover, the paper lacks a comparison with related work. Addressing the outlined weaknesses might increase the assigned score.\n\n**\n\nPost-rebuttal update: the score is increased from 4 to 5. See the response to the authors' comments.\n\n**\n\n[1] Bengio, Emmanuel, Joelle Pineau, and Doina Precup. \"Interference and Generalization in Temporal Difference Learning.\" arXiv preprint arXiv:2003.06350 (2020).\n\n[2] Nikishin, Evgenii, Pavel Izmailov, Ben Athiwaratkun, Dmitrii Podoprikhin, Timur Garipov, Pavel Shvechikov, Dmitry Vetrov, and Andrew Gordon Wilson. \"Improving stability in deep reinforcement learning with weight averaging.\" In Uncertainty in artificial intelligence workshop on uncertainty in Deep learning. 2018.\n", "title": "An important problem but the contributions have limited novelty, no comparison to the related work", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "joVH9YoD9wT": {"type": "rebuttal", "replyto": "_KQJcZmBRE_", "comment": "> \u201cI encourage the authors to improve it, especially showing stronger and more significant empirical evidence over representative baselines.\u201d\n\nThe dichotomy given here is that we should either have a rigorous theory paper or a large empirical paper demonstrating the utility of the algorithm. Instead, we have a third option. The paper tries to bring conceptual clarity to this important topic, and provide a clear empirical methodology to measure interference and understand correlations to forgetting. This first small empirical study itself is primarily a case study, but also has some insights about the role of buffer size, input type and hidden layer size on interference and forgetting. We then use the conceptual clarity on interference to design a new algorithm, directly mitigating interference, and demonstrate that such an approach is promising for stabilizing algorithms. Here again an important (and maybe surprising) result is just how unstable API is, and how much use a learned representation to mitigate interference increases stability (see Figure 4). \n\nFurther, when it comes to representative baselines, we make the same comment as to Reviewer 1. In the second part of the paper, we study interference for representation learning algorithms. Note that our goal is to investigate if mitigating interference can improve performance, rather than to compare more generally to algorithms designed to reduce forgetting. To the best of our knowledge, we do not know of any online representation learning methods that have been shown to reduce interference. Most representation learning approaches have used offline representation learning, including SRNN and OML. Our framework is a generic approach to incorporate these offline approaches, into an online setting. \n", "title": "Response to R4 (2/2)"}, "_KQJcZmBRE_": {"type": "rebuttal", "replyto": "7bxwste9euB", "comment": "Thank you for the comments. \n\n> \u201cI think that this paper studies an interesting problem, but its analysis is a bit superficial and not supported by rigorous theoretical analysis.\u201d\n\nThe RL community has yet to converge on a clear definition of interference. For example a recent ICML paper proposes several different ways to measure interference (https://arxiv.org/pdf/2003.06350.pdf). We provide a formalization and definition that can be measured, correlated with other performance measures and optimized (in addition to reward). No previous work has done these things.\n \nThis is a relatively unexplored topic in RL. It confuses us why one paper should be on the hook to provide a definition, demonstrate it makes sense, show it helps as part of an objective---and demonstrate the new algorithm at scale and provide a new theoretical framework.\n \nTheory linking interference to control performance is absolutely of interest. However, we as yet do not even have clear definitions or empirical work to provide clarity on the role of interference. It is hard to even know where to start theoretically, what questions to answer. We believe a purely empirical work, formalizing how to measure interference and examining its role, is an important first step.\n\n> \u201cThe TD-error may increase significantly across several iterations because of the agent exploring unvisited states, and not only because of interference.\u201d \n \nFirst we would like to point out (as we detailed in the introduction) that we study interference in the policy evaluation step of policy improvement---not in the usual online RL setting of prior work. We explicitly choose to investigate interference in the GPI setting where the agent alternates between estimating the value function from data (policy evaluation) and greedifiing the policy and collecting a new batch of data. This avoids the complexities of the policy and value function changing on each step, and the confounding factor of exploration. \n\nFor policy evaluation, the average TD-error across state-actions does not have to increase irrespective of the exploratory behavior of the policy. This is the primary motivation behind defining interference in the policy evaluation setting. \n\n> \u201cPrioritized Experience Replay (Schaul et al, 2016), show that a higher TD-error is actually desirable to guide exploration.\u201d \n\nIt\u2019s unclear what this means. PER, primarily, makes learning more sample efficient by focusing on transitions that have high TD error. The PER paper does talk about the potential of using signals from PER for guiding exploration in section 6 (the argument being that if experience generated from exploration strategy x is found to be more useful than that generated by y, x is a better exploration strategy), but they do not empirically verify if their proposal is useful and as a result, never show that \u201ca higher TD-error is actually desirable to guide exploration.\u201d \n\n> \u201cMoreover, since catastrophic forgetting is mostly problematic in deep RL, I'd have liked a stronger focus on deep RL, where the author could have tested the benefit of their approach on several algorithms based on the TD error, e.g. DQN, DDPG, TD3, SAC.\u201d\n\nCatastrophic forgetting has been clearly demonstrated in one-hidden layer neural networks. In fact, the term catastrophic forgetting predates deep-RL or even deep learning (see literature by R.M French or McCloskey). Moreover, DQN, TD3, and SAC introduce additional challenges for measuring interference. It is better to investigate a problem in the simplest setting it arises in. Our first step in the paper is to study interference in the GPI setting. Other methods, including DDPG, TD3 and SAC, are beyond the scope of the paper and they are a next step to extend beyond our setting. \n", "title": "Response to R4 (1/2)"}, "wW3Hj5HFoAw": {"type": "rebuttal", "replyto": "xjV-sP0ouSt", "comment": "Thank you for the comments. \n\n> \u201cI'm assuming forgetting is a more commonly used measure (please correct me if I'm wrong).\u201d\n\nThe intuitive definition of forgetting is the difference between the best performance achieved before time t and performance at time t. Some variants of the intuitive definition are used in the multi-task supervised learning community (e.g., [1], [2]). In RL, forgetting can be defined in terms of the agent\u2019s control performance, instead of prediction accuracy, and that is the forgetting measure we used in the paper. We will explain these connections, and add these citations, in the paper.\n\n[1] Chaudhry, Arslan, Puneet K. Dokania, Thalaiyasingam Ajanthan, and Philip HS Torr. \"Riemannian walk for incremental learning: Understanding forgetting and intransigence.\" \n\n[2] Serra, Joan, Didac Suris, Marius Miron, and Alexandros Karatzoglou. \"Overcoming catastrophic forgetting with hard attention to the task.\" \n\n> \u201cBut I don't get why we cannot use the forgetting measure for regularization, maybe some clarifications needed here.\u201d \n\nNot all measurable quantities are reasonable to optimize. We introduce forgetting because it is of interest to measure, but it makes little sense to optimize. Optimizing forgetting is relative to the agent\u2019s own performance in the past, which is somewhat circular. But in any case, it is optimized by simply trying to find the optimal policy, which is at least as good as any policy in the past. Optimizing forgetting would in fact tell the agent nothing about interference or generalization, and our goal is to obtain an objective to reduce interference and increase positive generalization. ", "title": "Response to R3"}, "xuVBmtDeb1G": {"type": "rebuttal", "replyto": "zeh6EzlSmIA", "comment": "> \u201cSo what's point to define interference with different granularity if only one is used in the end?\u201d\n\nIn order to define Interference Across Iterations, we need to define Iteration Interference, to summarize interference over time, and Update Interference, to summarize interference over state-action pairs. It conceptually leads to the one we measure. It is still worthwhile highlighting these differences, even if we don't measure at that granularity. When others use these definitions, they in fact could measure at that granularity. \n\n> \u201cAs the experiments show that internal layers contribute much less to interference, why it is better to adjust internal layers by representation loss than adjust the last layer which has much more influence on interference?\u201d \n\nThe interference in the last layer can be attributed to the features used by the last layer, and the weights of the last layer. The meta-objective, as a result, can be minimized by (1) updating the features, (2) updating the weights of the last layer, or (3) updating both the features and the weights. We chose to only optimize the features by minimizing interference because earlier work has shown that interference in the last layer can be greatly reduced by just learning the right features. This was demonstrated by SRNN [1], OML [2] and ANML [3] that showed that for the right feature representations, the last layer can learn complicated tasks with essentially no interference. \n\nAdditionally, minimizing interference is not the only goal of the agent. The actual goal of the agent is to learn the optimal value function while minimizing interference. This means that two different objectives --- one minimizing the td error and the other minimizing the increase in td-error  --- must be optimized simultaneously.\n\nNow in principle, we could have optimized the weights of the last layer to minimize interference as well, as suggested by the reviewer. But given the strongs results of [1], [2] and [3] and the fact that the last layer is learning the value function --- the primary goal of the agent ---  we concluded that it is better to update only the features using the meta-objective. \n\n[1] Liu, Vincent, et al. \"The utility of sparse representations for control in reinforcement learning.\" Proceedings of the AAAI Conference on Artificial Intelligence. 2019.\n\n[2] Javed, Khurram, and Martha White. \"Meta-learning representations for continual learning.\" Advances in Neural Information Processing Systems. 2019.\n\n[3] Beaulieu, Shawn, et al. \"Learning to continually learn.\" arXiv preprint arXiv:2002.09571. 2020.\n\n> The authors also introduce three different types of representation loss and OML obviously outperforms the others, but there is no enough analysis about why OML is supreme.\n\nIn section 6.2, we show that OML implicitly minimizes the interference measure but the other two representations do not. We believe that explains the stable performance of OML. \n", "title": "Response to R2"}, "xGVpH98YYJP": {"type": "rebuttal", "replyto": "N6aqgFESdjH", "comment": "> \u201cSimilarly, the authors do not compare with other methods that address the forgetting in the RL context.\u201d \n\nOur goal is to investigate if mitigating interference using representation learning can improve performance and reduce forgetting, rather than to compare more generally to algorithms designed to reduce forgetting. To the best of our knowledge, we do not know of any online representation learning methods that have been shown to reduce interference. Most representation learning approaches have used offline representation learning, including SRNN and OML. Our framework is a generic approach to incorporate these offline approaches, into an online setting. \n\nThe cited work from Nishkin et al. is not attempting to mitigate interference. Rather, it tries to mitigate forgetting similar by using weight averaging. There are several strategies that can be taken to reduce forgetting, including learning many neural networks and freezing them; lots and lots of replay with a very large buffer; identifying parameters important for performance and regularizing them from changing (Elastic weight consolidation, Synaptic intelligence); learning multiple networks and selecting the best one for making predictions and learning (Mixture of experts); and learning a parametric model of the data distribution (e.g., GAN) and using it to generate data for replay. Likely a final solution to forgetting will involve some combination of ideas, but understanding each in isolation is worthwhile. In this work, we limit our scope to reducing forgetting by focusing on interference from the representation. \n\n> \u201cThe cited paper [1] uses the term \u201cinterference\u201d for an inner product between gradient estimates evaluated at different data points and seeks, in contrast, to maximize interference.\u201d\n\nThat is one of the multiple different interference measures used in [1]. In Section 4 of our paper, we show that our interference measure is approximately proportional to negative gradient alignment, which is one of the interference measures used in [1] (e.g., see Section 5.2 of [1]). ", "title": "Response to R1 (2/2)"}, "N6aqgFESdjH": {"type": "rebuttal", "replyto": "NuMclkgiSnM", "comment": "Thank you for the comments. \n\n> \u201cThe proposed algorithm is designed to explicitly minimize interference. However, seeking for minimizing interference / forgetting on its own might prohibit exploration. For example, an agent that achieves the worst possible returns and is not learning at all will have zero interference / forgetting.\u201d \n\nWe agree that minimizing interference alone is not sufficient for learning, and the real goal is to minimize interference while maximizing generalization. Our proposed algorithm is doing exactly that. First, our algorithm is updating the value estimates using API or FQI using a non-zero step-size at every-step. This forces the system to minimize interference while also learning. Secondly, API-OML minimizes Update Interference metric that is not clipped to be non-negative. This allows our meta-objective to go beyond making interference zero (because it can make it negative) and avoid the pitfall of minimizing interference by turning off learning. \n\nWe concede that this was unclear from the write-up, as we defined Update Interference metric to be non-negative but then used the unclipped version. We will fix this issue.\n\nThat being said, we want to clarify that minimizing interference would not prohibit exploration in our setting. This is because within an iteration, the policy is fixed (Policy evaluation). Even if the policy is highly exploratory, the average td error across the state-space could go down for the right representation.  Exploration is, in-fact, tangential to interference for policy evaluation. \n\n> \u201cThe finding that changes in parameters of the last layer result in higher interference seems unsurprising as, generally, changes in the final layer parameters affect the output of a neural network more than changes in intermediate layers.\u201d\n\nEven if the findings are unsurprising, they have not been shown by prior work, to the best of our knowledge. Verifying untested ideas is an important aspect of research and science even if the said ideas appear intuitively true. If you can point to prior work that has a clear empirical demonstration of this phenomenon in RL we are excited to discuss it with you.\n\n> \u201cIt is unclear whether the learning curves on Acrobot are more stable due to claimed minimization of interference or due to using a more powerful optimization method. An ablation study will improve the quality of the paper.\u201d\n\nWe are not sure what the reviewer meant by \"more powerful optimization method.\" The meta-objective is being optimized using SGD, similar to the baselines. \n\nPerhaps the reviewer meant that the meta-learning algorithm requires more compute and memory and is therefore a more powerful method? If so, then while we agree that the API-OML requires more memory and compute, that is not necessarily bad. An algorithm that can benefit from more resources and learn better is a more scalable and useful algorithm. One possible option would be to run API for more iterations, to benefit from more compute and see if this closes the gap. However, we can already see in later learning that API remains much less stable than API-OML.\n\n\n> \u201cThe authors should consider using harder environments to make the experimental results more convincing.\u201d\n\nWe designed our experiments to study interference in the simplest cases it arises. This is a common approach, see Sutton\u2019s black and white world used to study tracking [1] and b-suite [2]. They used small environments because they were the right choice for their scientific questions. Larger and more complex domains almost certainly introduce confounding factors that are both unknown and difficult to control. The environments we used cause failures in API/FQI, making them a suitable place to investigate these measures of interference. Scaling up is one scientific question, but not the only one and not always the first one to ask. \n\n[1] Sutton, Richard S., Anna Koop, and David Silver. \"On the role of tracking in stationary environments.\"\n[2] Osband, Ian, Yotam Doron, Matteo Hessel, et al. \"Behaviour suite for reinforcement learning.\"\n\n> \u201cThe authors mention related methods based on first-order approximation of interference. However, no comparison with the methods was provided.\u201d\n\nWe mention the disadvantage of gradient alignment in the last paragraph of Section 4. It is actually a first-order approximation of the difference and more costly to compute. There is already an efficient approximation of interference, so we think it is unnecessary to compare to expensive methods introducing further approximation. \n", "title": "Response to R1 (1/2)"}, "xjV-sP0ouSt": {"type": "review", "replyto": "26WnoE4hjS", "review": "This paper studies the the interference problem under the API and FPI setting. It designs new measure for interference, and shows that the interference measure is correlated with forgetting. With the help of the interference measure, the paper studies the importance of the final layer of the neural network and proposes a new algorithm to mitigate interference. \n\nOverall, the paper is well-written but I have some questions regarding the logic of the paper. First, it's not super clear to me what is the role of the forgetting measure. Since the validation of interference measure is evaluated by forgetting, I'm assuming forgetting is a more commonly used measure (please correct me if I'm wrong). \n- If this is the case, then it seems the importance of interference measure is to come up with the SRNN and OML variants. But I don't get why we cannot use the forgetting measure for regularization, maybe some clarifications needed here. If the interference measure is not necessary for OML and SRNN, I don't quite get the necessarily of the interference measure. \n- If this is not the case, then why forgetting is an interesting measure to compare with? \n\nIn addition, I think it may be easier to follow if the unused definitions can be moved to Appendix, e.g., Pointwise Interference and Interference Across Iterations are only mentioned when they're defined, On the other hand, I think it'll be helpful to move some contents from Appendix to Sec 6.2, since it is the place that explicitly take advantage of the interference measure. \n\nLastly, there two minor points: \n- Line 4. the definition of iid is missing\n- Algorithm 1: it seems the the definition of b_k is missing", "title": "An interesting paper studying the interference under an API and FPI setting, but more clarification required.  ", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "zeh6EzlSmIA": {"type": "review", "replyto": "26WnoE4hjS", "review": "This paper proposes new measures to quantify interference in reinforcement learning at different granularity and show the correlation between interference and forgetting, and suggests to use extra representation loss to reduce interference. \n\nIn general, the definition of  interference at different granularity looks reasonable and the relation with forgetting matches intuition. However, there are no experiments to show any difference between those interference, such as update interference,  iteration interference, and interference across iterations.  So what's point to define interference with different granularity if only one is used in the end?  \n\nThe authors propose using extra representation loss to reduce interference, but the motivation is rather vague.  As the experiments show that internal layers contribute much less to interference, why it is better to adjust internal layers by representation loss than adjust the last layer which has much more influence on interference?  Is there more solid justification to connect representation loss and interference?  \n\nThe authors also introduces three different types of representation loss and OML obviously outperforms the others, but there is no enough analysis about why OML is supreme and then no guidance of how to choose a proper representation loss.\n\nAs I'm not an expert in reinforcement learning, I will leave the novelty and significance to other reviewers to decide. \n\nSome minor issues:\n1. Equations should be numbered.\n2. Could authors elaborate a bit on how to get the last equation on page 4? As the last term in the right side hasn't appeared before. \n3. The coefficients of forgetting and interference in the subfigures of Fig. 1 should be provided.\n \n", "title": "Probably interesting idea but not convincing enough", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}