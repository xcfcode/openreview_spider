{"paper": {"title": "ProbGAN: Towards Probabilistic GAN with Theoretical Guarantees", "authors": ["Hao He", "Hao Wang", "Guang-He Lee", "Yonglong Tian"], "authorids": ["haohe@mit.edu", "hwang87@mit.edu", "guanghe@mit.edu", "yonglong@mit.edu"], "summary": "A novel probabilistic treatment for GAN with theoretical guarantee.", "abstract": "Probabilistic modelling is a principled framework to perform model aggregation, which has been a primary mechanism to combat mode collapse in the context of Generative Adversarial Networks (GAN). In this paper, we propose a novel probabilistic framework for GANs, ProbGAN, which iteratively learns a distribution over generators with a carefully crafted prior. Learning is efficiently triggered by a tailored stochastic gradient Hamiltonian Monte Carlo with a novel gradient approximation to perform Bayesian inference. Our theoretical analysis further reveals that our treatment is the first probabilistic framework that yields an equilibrium where generator distributions are faithful to the data distribution. Empirical evidence on synthetic high-dimensional multi-modal data and image databases (CIFAR-10, STL-10, and ImageNet) demonstrates the superiority of our method over both start-of-the-art multi-generator GANs and other probabilistic treatment for GANs.", "keywords": ["Generative Adversarial Networks", "Bayesian Deep Learning", "Mode Collapse", "Inception Score", "Generator", "Discriminator", "CIFAR-10", "STL-10", "ImageNet"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes a new method that builds on the Bayesian modelling framework for GANs and is supported by a theoretical analysis and an empirical evaluation that shows very promising results. All reviewers agree, that the method is interesting and the results are convincing, but that the model does not really fit in the standard Bayesian setting due to a data dependency of the priors. I would therefore encourage the authors to reflect this by adapting the title and making the differences more clear in the camera ready version."}, "review": {"rJeRLZfI14": {"type": "rebuttal", "replyto": "rJg-57eh07", "comment": "Following is our response to your updated comments.\n\n=== Bayesian GAN prior in toy experiment ===\n\nWe agree that using a broad normal prior would be a better choice. We have rerun our experiment with the normal prior (with mean 0, std 1) and got the similar results as in the uniform prior case. We will certainly include the new result in our next version of the paper.\n\nFollowing is a remark on the modification we make on the toy model to use normal prior. We reparameterize the generator and discriminator. Generator with parameter theta^g produces a data distribution p(x_i; theta^g) = exp(theta^g_i) / sum_j exp(theta^d_j). Under a normal prior N(0,1), its prior probability is p(theta^g) = \\prod_j exp(- theta^g * theta^g / 2). Discriminator with parameter theta^d has a score function D(x_i; theta^d) = sigmoid(theta^d_i). Under a normal prior N(0,1), its prior probability is p(theta^d) = \\prod_j exp(- theta^d * theta^d / 2).\n\n=== why to strip the normal prior ===\n\nIt is a very good point. Thank you for your insightful comment on it. \n\nIt is true that the prior is crucial for a Bayesian model and should encode domain knowledge. What our empirical analysis shows is that a Gaussian prior is not helpful in the task of Bayesian GANs. At least, the normal prior does not show an advantage over the non-informative prior. Intuitively, putting a normal prior is very similar to have an L2 regularization when training a neural network. It looks helpful in the sense of robust to overfitting. However, we remark that, unlike typical supervised learning where model fitting is connected to generalization performance, an \"overfitting\" model is desirable for GANs that matches the data distribution perfectly.\n\nSince the normal prior does not work, we need a more involved prior that makes the Bayesian modeling work. Our solution is the \u201cunorthodox\u201d generator prior as you mentioned. Although it looks rather \u201cunorthodox\u201d at first blush, this generator prior is standard in the following senses: (1) As we previously explained to R3, our Bayesian model actually includes two separate models, one for the generator and one for the discriminator. Hence from the generator\u2019s perspective, the generator is the \u2018model\u2019 and the discriminator is the \u2018data\u2019 and the other way around from the discriminator's perspective. Note that the real data distribution we want to learn is actually a third-party component; it is, therefore, proper to involve the real data in the prior. (2) Our generator prior encodes our prior belief in the sense that the generator distribution should be stable if the discriminator cannot distinguish the synthetic data and the real data well.\n\n=== robustness to overfitting ===\n\nWe want to emphasize that the setting we are handling with is different from the traditional prediction settings where Bayesian methods are applied commonly. When dealing with classification or regression, robustness to overfitting is quite important. However, in the GAN computation setting, the overfitting issue is not the main concern since the goal is to produce a distribution that matches the real data distribution perfectly, rather than generalizing to unseen data.", "title": "Thanks for your helpful comments."}, "ryxAXLZYhm": {"type": "review", "replyto": "H1l7bnR5Ym", "review": "Summary\n=========\nThe paper extends Bayesian GANs by altering the generator and discriminator parameter likelihood distributions and their respective priors. \nThe authors further propose an SGHMC algorithm to collect samples of the resulting posterior distributions on each parameter set and evaluate their approach on both a synthetic and the CIFAR-10 data set. \nThey claim superiority of their method, reporting a higher distance to mode centers of generated data points and better generator space coverage for the synthetic data set and better inception scores for the real world data for their method.\n\nReview\n=========\nAs an overall comment, I found the language poor, at times misleading.\nThe authors should have their manuscript proof-read for grammar and vocabulary.\nExamples: \n- amazing superiority (page 1, 3rd paragraph)\n- Accutally... (page 1, end of 3rd paragraph)\n- the total mixture of generated data distribution (page 3, mid of 3.1)\n- Similarity we define (page 3, end of 3.1)\n- etc.\nOver the whole manuscript, determiners are missing.\n\nThe authors start out with a general introduction to GANs and Bayesian GANs in particular, \narguing that it is an open research question whether the generator converges to the true data generating distribution in Bayesian GANs.\nI do not agree here. The Bayesian GAN defines a posterior distribution for the generator that\nis proportional to the likelihood that the discriminator assigns to generated samples.\nThe better the generator, the higher the likelihood that the discriminator assign to these samples.\nIn the case of a perfect generator, here the discriminator is equally unable to distinguish real and generated samples and consequently degenerates to a constant function.\nUsing the same symmetry argument as the authors, one can show that this is the case for Bayesian GANs.\n\nWhile defining the likelihood functions, the iterator variable t is used without introduction.\n\nFurther, I a confused by their argument of incompatibility.\nFirst, they derive a Gibbs style update scheme based on single samples for generator and discriminator parameters using\nposteriors in which the noise has been explicitly marginalized out by utilizing a Monte Carlo estimate.\nSecond, the used posteriors are conditional distributions with non-identical conditioning sets.\nI doubt that the argument still holds under this setting.\n\nWith respect to the remaining difference between the proposed approach and Bayesian GAN,\nI'd like the authors elaborate where exactly the difference between expectation of objective value\nand objective value of expectation is.\nSince the original GAN objectives used for crafting the likelihoods are deterministic functions,\nrandomness is introduced by the distributions over the generator and discriminator parameters.\nI would have guessed that expectations propagate into the objective functions.\n\nIt is, however, interesting to analyze the proposed inference algorithm, especially the introduced posterior distributions.\nFor the discriminator, this correspond simply to the likelihood function.\nFor the generator, the likelihood is combined with some prior for which no closed form solution exists.\nIn fact, this prior changes between iterations of the inference algorithm.\nThe resulting gradient of the posterior decomposes into the gradient of the current objective and the sum over all previous gradients.\nWhile this is not a prior in the Bayesian sense (i.e. in the sense of an actual prior belief), it would be interesting to have a closer look at the effect this has on the sampling method.\nMy educated guess is, that this conceptually adds up to the momentum term in SGHMC and thus slows down the exploration of the parameter space and results in better coverage.\n\nThe experiments are inspired by the ones done in the original Bayesian GAN publication.\nI liked the developed method to measure coverage of the generator space although I find the\nterm of hit error misleading.\nGiven that the probabilistic methods all achieve a hit rate of 1, a lower hit error actually points to worse coverage.\nI was surprised to see that hit error and coverage are only not consistently negatively correlated.\nAdding statistics over several runs of the models (e.g. 10) would strengthen the claim of superior performance.", "title": "Stripping the priors from Bayesian GANs", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hkxok8co27": {"type": "review", "replyto": "H1l7bnR5Ym", "review": "PRIOR COMMENT:   This paper should be rejected based on the experimental work.\nExperiments need to be reported for larger datasets.  Note the MGAN\npaper reports results on STL-10 and ImageNet as well.\n\nNOTE:  this was addressed by the 27/11 revision, which included good\n   results for these other data sets, thus I now withdraw the comment\n\nNote, your results on CIFAR-10 are quite different to those in the\nMGAN paper.  Your inceptions scores are worse and FIDs are better!!  I\nexpect you have different configurations to their paper, but it would\nbe good for this to be explained.  NOTE:   explained in response!\n\nNOTE:  this was addressed by the 27/11 revision\n\nI thought the related work section was fabulous, and as an extension\nto BGAN, the paper is a very nice idea.  So I benefited a lot from reading\nthe paper.\n\nI have some comments on Bayesian treatment.  In Bayesian theory, the\ntrue distribution $p_{data}$ cannot appear in any evaluated formulas,\nas you have it there in Eqn (1) which is subsequently used in your\nlikelihood Eqn (2).  Likelihoods are models and cannot involve \"truth\".\n\nLemma 1:  Very nice observation!!  I was trying to work that out,\nonce I got to Eqn (3), and you thought of it. \n\nAlso, you do need to explain 3.2 better.  The BGAN paper, actually, is\na bit confusing from a strict Bayesian perspective, though for\ndifferent reasons.  The problem you are looking at is not a\ntime-series problem, so it is a bit confusing to be defining it as\nsuch.  You talk about an iterative Bayesian model with priors and\nlikelihoods.  Well, maybe that can be *defined* as a probabilistic\nmodel, but it is not in any sense a Bayesian model for the estimation\nof $p_{model}$.\n\nNOTE:  anonreviewer2 expands more on this\n\nWhat you do with Equation (3) is define a distribution on\n$q_g(\\theta_g)$ and $q_d(\\theta_d)$ (which, confusingly, involves the\n\"true\" data distribution ... impossible for a Bayesian formulation).\nYou are doing a natural extension of the BGAN papers formulation in\ntheir Eqs (1) and (2).  This, as is alluded to in Lemma 1.  Your\nformulation is in terms of two conditional distributions, so\nconditions should be given that their is an underlying joint\ndistribution that agrees with these.  Lemma 1 gives a negative result.\nYou have defined it as a time series problem, and apparantly one wants\nthis to converge, as in Gibbs sampling style.  Like BGAN, you have\njust arbitrarily defined a \"likelihood\".\n\nTo me, this isn't a Bayesian model of the unsupervised learning task,\nits a probabilistic style optimisation for it, in the sense that you are defining a probability\ndistribution (over $q_g(\\theta_g)$ and $q_d(\\theta_d)$) and sampling\nfrom it, but its not really a \"likelihood\" in the formal sense.  A\nlikelhood defines how data is generated.  Your \"likelihood\" is over\nmodel parameters, and you seem to have ignored the data likelihood,\nwhich you define in sct 3.1 as $p_{model}()$.\n\nAnyway, I'm happy to go with this sort of formulation, but I think you\nneed to call it what it is, and it is not Bayesian in the standard sense.  The theoretical\ntreatment needs a lot of cleaning up.  What you have defined is a\nprobabilistic time-series on $q_g(\\theta_g)$ and $q_d(\\theta_d)$.\nFair enough, thats OK.  But you need to show that it actually works in\nthe estimation of $p_{model}$.  Because one never has $p_{data}$, all\nyour Theorem 1 does is show that asymptotically, your method works.\nUnfortunately, I can say the same for many crude algorithms, and most\nof the existing published work.  Thus, we're left with requiring a\nsubstantial empirical validation to demonstrate the method is useful.\n\nNow my apologies to you: I could make somewhat related statements\nabout the theory of the BGAN paper, and they got to publish theirs at\nICLR!  But they did do more experimentation.\n\nOh, and some smaller but noticable grammar/word usage issues.\n\nNOTE:  thanks for your good explanation of the Bayesian aspects of the model ...\nyes I agree, you have a good Bayesian model of the GAN computation , but it\nis still not a Bayesian model of the unsupervised inference task.  This is a somewhat\nminor point, and should not in anyway influence worth of the paper ... but clarification\nin paper would be nice.", "title": "experimental work now conclusive", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Skg9rHqqC7": {"type": "rebuttal", "replyto": "Hkxok8co27", "comment": "#1 You have a good Bayesian model of the GAN computation, but it is still not a Bayesian model of the unsupervised inference task. \n\nYes, you are right. In this work, we aim to develop a better Bayesian model of the GAN computation. Generally, Bayesian models for unsupervised inference tasks could be a larger topic.\n\n#2 I want to see results on the big data sets.\n\nThanks for being positive of our work. We have included the results on STL-10 and ImageNet in our revision of the paper (e.g., Table 4 and Figure 4 of Section 5.2). As mentioned in the general response above, our model does provide better performance on both datasets with significant improvements of FID scores. We hope that with the additional results, the experimental work in the current version is more conclusive.\n", "title": "Thanks again for your thoughtful comments."}, "Hyx9hN59A7": {"type": "rebuttal", "replyto": "H1l7bnR5Ym", "comment": "We thank all the reviewers for the insightful comments and helpful suggestions. Here we summarize the major changes we did in the revision of our paper.\n\n1.  Adding experiment results on STL-10 and ImageNet.\n\nWe follow R3\u2019s suggestion to compare our models and baselines (MGAN, BGAN) on the larger datasets. Our model does provide better performance on both datasets. Especially, the improvement of FID scores looks significant. We include the new experiment results (Table 4 and Figure 4) in Section 5.2.\n\n2. Updating Inception score and FID results on CIFAR-10.\n\nThanks to R3\u2019s help, we find the discrepancy between FIDs given by the PyTorch model and the Tensorflow model. We have switched to the official Tensorflow model for evaluation and updated all results in Table 3 (Section 5.2). We also put a remark in Section B.1 (of the appendix) to make it clearer.\n\n3. Emphasizing the difference between our model and Bayesian GAN.\n\nR2 suggests that we elaborate more about the difference between our likelihood design (objective value of expectation) and Bayesian GAN\u2019s likelihood (expectation of objective value). We revise Section 4.2 to explain the differences both in the likelihood and in the prior more clearly.\n\n4. Adding a toy experiment to demonstrate different convergence behavior of our model and Bayesian GAN (Figure 1).\n\nWe include a new toy experiment on categorical distributions as empirical support for the superior convergence property of our model over the Bayesian GAN.  \n\nIn our toy experiment,  the data is sampled from a finite discrete space (more specifically, a categorical distribution). It is ideal to examine the Bayesian formulation in a finite case since the posterior can then be computed analytically and does not have error caused by inference algorithms. We try different combinations of likelihoods and priors in the experiment and compare their learned distributions. \n\nIn Figure 1, we visualize the generated data distributions of different models after they converge. The results show that only when using the combination of our likelihood and our prior can the model converge to the correct equilibrium. The full details of the experiment are included in Section D (of the appendix). This example also serves as an illustration of the convergence issue of Bayesian GAN.\n\nMinor changes:\n\n1. Change the term \u2018hit error\u2019 to \u2018hit distance\u2019 (e.g., in Table 2) to avoid the potential misunderstanding of its meaning.\n\n2. Add a few sentences in Section 4.1 to explain why Theorem 1 does not hold for Bayesian GAN.\n", "title": "General Response"}, "B1g1aeEapX": {"type": "rebuttal", "replyto": "ryxAXLZYhm", "comment": "\nDear AnonReviewer2,\n\nThank you for the feedback. \nFollowing is our response to your concerns.\n\n=== convergence of Bayesian GAN ===\n\nThe convergence of Bayesian GAN is indeed a problem, which is one of our key contributions. Bayesian GAN has a subtle difference from the original GANs during learning. To compute the posterior, Bayesian GAN cannot be learned by vanilla gradient descent methods, but is learned by SGHMC. In SGHMC framework, the gradient is always adulterated by white noises. Thus if the gradient from discriminator is always zero, the generator distribution will converge to a Gaussian distribution instead of staying unchanged.\n\nIn contrast, we fix this issue by a well-crafted prior for the generator distribution. Intuitively, the gradient from the prior helps combat with the noise and prevent degeneracy of the generator distribution towards a Gaussian distribution. Please note theorem 1 does not hold without introducing suitable prior for the generator.\n\n\n=== expectation of objective value v.s. objective value of expectation ===\n\nThis difference is another very critical improvement from Bayesian GAN. We will make it more clear in the revision of the paper.\n\nAs shown in Eqn 8, to compute likelihood, Bayesian GAN takes expectation after computing the GAN objective value. While as shown in Eqn 2, we compute GAN objective value after the expectation. The subtle adjustment is crucial. Theorem 1 will not hold if the likelihood is defined as the expectation of loss value as Bayesian GAN did. Intuitively, because the expectation \\E_{q_g} p_{gen}(x;\\theta_g)) is equivalent to the data distribution p_model(x) produced by the generator distribution, it makes sense to compute GAN objective over it instead of the reversed order (in Bayesian GAN). Besides, it\u2019s easy to see the gradients of the two different likelihoods is different since, for a given function f, the gradient of \\sum_i f(x_i) is usually different from that of f(\\sum_i x_i).\n\n=== clarification on incompatibility ===\n\nThe incompatibility corresponds to the incompatibility between two conditional distributions that can not belong to the same joint distribution. We identify a theoretical flaw of Bayesian GAN under a very simple setting (when only using single Monte-Carlo sample) that leads to incompatible conditionals of generator and discriminator. Moreover, we are not very certain about the concern \u201cthe used posteriors are conditional distributions with non-identical conditioning sets. I doubt that the argument still holds under this setting.\u201d Further explanation about \u201cnon-identical conditioning sets\u201d will be appreciated.\n\n=== relationship between hit error and coverage ===\n\nBy our definition, \u2018hit error\u2019 is the averaged distance between the generated data points (projected into a low dimensional space) and the low dimensional hyperplane that the ground truth mode lies in. While the \u2018coverage error\u2019 measures the similarity between the distribution of projected data points and the ground truth data distribution which is uniform.\n\nNote that these two metrics are actually orthogonal to each other, due to the fundamental difference between projection distances (\u2018hit error\u2019) and how the projections are distributed (\u2019coverage error\u2018). It\u2019s possible to get the same projection distances in a scattered or dense way. It\u2019s also possible to get the same projections from different projection distances. \n\nWe will change the terminology \u2018hit error\u2019 to \u2018hit distance\u2019 to make it clearer in our revision.\n\n=== further analyze of our inference algorithm ===\n\nThe momentum explanation seems an interesting direction to yield a formal explanation of such approximations, but we do not have a concrete analysis yet and leave it as future work. \n", "title": "Response to  AnonReviewer2"}, "Hkgyn9ICTm": {"type": "rebuttal", "replyto": "Hkxok8co27", "comment": "Dear AnonReviewer3,\n\nThank you for the insightful comments.\nFollowing is our response to your concerns.\n\n=== experiments ===\n\nWe will include results on STL-10 and ImageNet in the revision, or a later version if our machines cannot catch up the rebuttal deadline. Compared with Bayesian GAN, actually, we did a more thorough study on the choice of objective function, and our synthetic dataset is harder and more illustrative.\n\nHere we clarify the discrepancy between our quantitative evaluation of MGAN and that of the original paper. We actually use the official open-sourced code of MGAN with the same configurations (model architectures, training data). The discrepancy comes from the inception model used to compute FID. We compute FID with PyTorch Inception model (https://github.com/mseitzer/pytorch-fid.). The original MGAN paper did not say which inception model they have used. Our guess is that they used the Tensorflow inception model (https://github.com/bioinf-jku/TTUR). We observed FID computed by PyTorch model is much lower than that computed by the Tensorflow model, because of the different weights of the pre-trained models. A similar phenomenon has been recently observed for Inception Score [1]. To favor a more complete comparison, we will update our FID results by switching to the Tensorflow version.\n\nWe had posted the updated results in the comment. In our experiments, the MGAN with GAN-NS objective has the same setting with original MGAN. The Inception score and FID we get are 7.25 and 27.55 which are both worse than the scores reported in the original paper, 8.33 and 26.7. We train MGAN with the officially released code under the configuration reported in the MGAN paper (Table 4 in the appendix). The scores we reported is the best we can get via several training trials.\n\n[1] Barratt, Shane, and Rishi Sharma. \"A Note on the Inception Score.\" arXiv preprint arXiv:1801.01973 (2018).\n\n=== Bayesian formulation ===\n\nOur method has two separate Bayesian models, one for the generator and one for the discriminator. Take the Bayesian perspective for the generator as an example. The likelihood defined in the first equation of Eqn 2 gives the probability of observing some fixed discriminator distribution for some generator parameter, i.e., p(D^{(t)} | \\theta_g). Composite with the prior of the generator parameter q^{(t)}(\\theta_g), it is a Bayesian model from a strict perspective. Indeed, to see the correspondence of \u2018model parameter\u2019 and \u2018data\u2019  in classic Bayesian theory, our generator is the \u2018model\u2019 and the discriminator is the \u2018data\u2019. We estimate generator distribution by the observed discriminator distribution.\n\nThe novelty from classic Bayesian models is on the inference procedure. We integrate the two standard Bayesian models into a dynamical system: each Bayesian problem is solved alternatingly. From a game-theoretic point of view, each optimization problem is the best response strategy of the corresponding player, and the equilibrium presents a generator distribution that produces the target data distribution. \n\n=== Why time-series modelling ===\n\nThe problem is not a time-series problem. We simply solve it in an iterative manner. (akin to SGD that can iteratively solve both time-series and non-time-series problems). Our goal is to find the equilibrium of generator and discriminator distributions, where they satisfy each other\u2019s posterior under our Bayesian criterion. It is, however, possible to find the equilibrium via an iterative scheme. We will make this part more clear in the revision.\n\n=== A clarification about theorem 1 ===\n\nIt is indeed true that Theorem 1 only shows an analysis of the optimal solution in an asymptotic scenario. Unfortunately, it is, to our best knowledge, the best property that has been obtained in recent literature on GANs [2, 3, 4, 5, 6]. However, please note that Bayesian GAN does not even possess such asymptotic property and the difficulty of avoiding such problem as revealed by our analysis in Section 4.2. In contrast, our method is to the first Bayesian method to establish such property. \n\n[2] Goodfellow, Ian, et al. \"Generative adversarial nets.\" Advances in neural information processing systems. (NIPS 2014)\n[3] Hoang, Quan, et al. \"MGAN: Training generative adversarial nets with multiple generators.\" (ICLR 2018)\n[4] Arjovsky, Martin, Soumith Chintala, and L\u00e9on Bottou. \"Wasserstein generative adversarial networks.\"(ICML 2017)\n[5] Mao, Xudong, et al. \"Least squares generative adversarial networks.\" Computer Vision (ICCV), 2017 IEEE International Conference on. IEEE, 2017.\n[6] Zhao, Junbo, Michael Mathieu, and Yann LeCun. \"Energy-based generative adversarial network.\" (ICLR 2017)\n", "title": "Response to  AnonReviewer3"}, "B1lq-8vRa7": {"type": "rebuttal", "replyto": "H1l7bnR5Ym", "comment": "Previously, our FID results are computed using a PyTorch Implementation (https://github.com/mseitzer/pytorch-fid). Note that there exists a large discrepancy between the FID results conducted by PyTorch Inception model and Tensorflow model. Hence, to facilitate the comparison with previous paper, we decide to reevaluate by the official Tensorflow FID computation code (https://github.com/bioinf-jku/TTUR). \n\nHere are the updated results.\n\n                       Inception scores (higher is better)\n                  GAN-MM  & GAN-NS & WGAN & LSGAN \nDCGAN    &   6.53     &        7.21 &      7.19 &      7.36 \nMGAN      &   7.19     &        7.25 &      7.18 &      7.34\nBGAN       &   7.21    &        7.37 &       7.26 &      7.46\nours-PSA  &  7.75     &       7.53  &      7.28 &      7.36\n\n                             FIDs (lower is better)\n                  GAN-MM  & GAN-NS & WGAN & LSGAN \nDCGAN    &      35.57 &     27.68 &    28.31 &    29.11 \nMGAN      &      30.01 &     27.55 &    28.37 &    30.72\nBGAN       &      29.87 &     24.32 &    29.87 &    29.19\nours-PSA  &     24.60  &    23.55 &     27.46 &    26.90\n\nNote that we are reporting the results with the highest \u2018Inception score - 0.1 FID\u2019 for each model. Thus the Inception scores results are also updated.\n", "title": "Updating Inception and Frechet Inception Distance Results on CIFAR10. (Table 3 in the paper)"}, "Byg6_RX6Tm": {"type": "rebuttal", "replyto": "SJxci3qu3m", "comment": "Dear AnonReviewer1,\n\nThank you for agreeing with the significance of our contribution and voting to accept our paper. We will address the typos.\n\nWe make an additional remark here, which might be interesting. Bayesian modeling has been introduced in several mini-max problems in the deep learning community, such as adversarial (robust) learning [1] and GANs. However, most prior works pose Bayesian method as a heuristic without theoretical analysis. This work presents an important initial step toward a rigorous study of modernized Bayesian approaches. \n\n[1] Nanyang Ye, Zhanxing Zhu. Bayesian Adversarial Learning. 32nd Annual Conference on Neural Information Processing Systems (NIPS 2018)\n", "title": "Response to  AnonReviewer1"}, "SJxci3qu3m": {"type": "review", "replyto": "H1l7bnR5Ym", "review": "Mode collapse in the context of GANs occurs when the generator only learns one of the \nmultiple modes of the target distribution. Mode collapsed can be tackled, for instance, using Wasserstein distance instead of Jensen-Shannon divergence. However, this sacrifices accuracy of the generated samples.\n\nThis paper is positioned in the context of Bayesian GANs (Saatsci & Wilson 2017) which, by placing a posterior distribution over the generative and discriminative parameters, can potentially learn all the modes. In particular, the paper proposes a Bayesian GAN that, unlike previous Bayesian GANs, has theoretical guarantees of convergence to the real distribution.\n\nThe authors put likelihoods over the generator and discriminator with logarithms proportional to the traditional GAN objective functions. Then they choose a prior in the generative parameters which is the output of the last iteration. The prior over the discriminative parameters is a uniform improper prior (constant from minus to plus infinity). Under this specifications, they demonstrate that the true data distribution is an equilibrium under this scheme. \n\nFor the inference, they adapt the Stochastic Gradient HMC used by Saatsci & Wilson. To approximate the gradient of the discriminator, they take samples of the generator parameters. To approximate the gradient of the generator they take samples of the discriminator parameters but they also need to compute a gradient of the previous generator distribution. However, because this generator distribution is not available in close form they propose two simple approximations.\n\nOverall, I enjoyed reading this paper. It is well written and easy to follow. The motivation is clear, and the contribution is significant. The experiments are convincing enough, comparing their method with Saatsci's Bayesian GAN and with the state of the art of GAN that deals with mode collapse. It seems an interesting improvement over the original Bayesian GAN with theoretical guarantees and an easy implementation.\n\nSome typos:\n\n- The authors argue that compare to point mass...\n+ The authors argue that, compared to point mass...\n\n- Theorem 1 states that any the ideal generator\n+ Theorem 1 states that any ideal generator\n\n- Assume the GAN objective and the discriminator space are symmetry\n+ Assume the GAN objective and the discriminator space have symmetry\n\n- Eqn. 8 will degenerated as a Gibbs sampling\n+ Eqn. 8 will degenerate as a Gibbs sampling", "title": "A Bayesian GAN with where data distribution is an equilibrium", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}