{"paper": {"title": "What does it take to generate natural textures?", "authors": ["Ivan Ustyuzhaninov *", "Wieland Brendel *", "Leon Gatys", "Matthias Bethge"], "authorids": ["ivan.ustyuzhaninov@bethgelab.org", "wieland.brendel@bethgelab.org", "leon.gatys@bethgelab.org", "matthias.bethge@bethgelab.org"], "summary": "Natural textures of high perceptual quality can be generated from networks with only a single layer, no pooling and random filters.", "abstract": "Natural image generation is currently one of the most actively explored fields in Deep Learning. Many approaches, e.g. for state-of-the-art artistic style transfer or natural texture synthesis, rely on the statistics of hierarchical representations in supervisedly trained deep neural networks. It is, however, unclear what aspects of this feature representation are crucial for natural image generation: is it the depth, the pooling or the training of the features on natural images? We here address this question for the task of natural texture synthesis and show that none of the above aspects are indispensable. Instead, we demonstrate that natural textures of high perceptual quality can be generated from networks with only a single layer, no pooling and random filters.", "keywords": ["Deep learning", "Unsupervised Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The authors agreed that the paper presented a solid contribution and interesting (and somewhat surprising findings). The experiments are thorough and convincing, and while some reviewers raised concerns about a lack of comparisons of methods on a clear quantifiable objective, this is unfortunately a common issue in this field. Overall, this paper is a solid contribution with findings that would be interesting to the ICLR audience."}, "review": {"BJqLJzrIe": {"type": "rebuttal", "replyto": "H1SCAZJ4e", "comment": "We thank the reviewer for the positive review and extensive comments! \n\n> The main limitation of the paper is that it does not systematically compare different \n> methods against a quantifiable objective.\n\nThe current gold standard for testing parametric texture models is human evaluation of perceptual quality and diversity, and this is indeed an unsatisfying state of affairs. We strived for a more quantifiable measure of perceptual quality using a state-of-the-art VGG-based model, but are yet unable to quantifiably measure diversity. In the end, however, we feel that any measure should roughly reflect the human evaluation of perceptual quality and diversity. The insights of this paper solely hinge on the fact that shallow networks synthesise pleasing as well as diverse textures, but are independent of an exact quantitative comparison against deep texture models.\n", "title": "Response to the reviewer"}, "rkUl1MrUg": {"type": "rebuttal", "replyto": "HyAwcr-4g", "comment": "Thanks for the positive review! We very much agree that better quantitative evaluation methods for parametric texture synthesis models are needed and we actively strive to develop such methods. Establishing a new method will require profound psychophysical testing (the current gold standard) and thus warrants its own publication.", "title": "Response to the reviewer"}, "HkVo0bB8l": {"type": "rebuttal", "replyto": "ry3uL67Eg", "comment": "We thank the reviewer for the critical comments and the overall positive evaluation of the manuscript.\n\n> Firstly, all of generated samples by both VGG and single layer model are not perfect \n> and much worse than the results from non-parametric methods.\n\nWe agree that non-parametric resampling techniques are capable of producing high quality natural textures very efficiently. However, they do not define an actual model for natural textures but rather give a mechanistic procedure for how one can randomise a source texture without changing its perceptual properties. Parametric models, on the other hand, formulate a comprehensive statistical description of a texture and are in principle able to generate new samples that cannot be reached by resampling techniques.\n\n> Besides VGG-based model seems to do better in inpainting task in Figure 7.\n\nWe agree that overall the performance of the VGG-based model is still better then the random shallow networks. However, we would like to emphasise again that the goal of this paper is not to reach the best parametric texture model, but rather to demonstrate that even very simple single-layer networks are able to generate very good natural texture samples.\n\n> Last but not least, would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently?\n\nWe experimented comprehensively with multi-layer architectures and random filters. Synthesizing samples is indeed faster, but the perceptual quality is equal or lower than the shallow multi-scale architecture. A major goal of this paper is to find the simplest possible architecture, and so we prefer to leave out the discussion of multi-layer networks for the sake of clarity. We added a comment on multi-layer networks in the discussion.", "title": "Response to the reviewer"}, "SJV03JyEl": {"type": "review", "replyto": "BJhZeLsxx", "review": "I will post my thoughts in the final review, but other reviewers had already asked most of what I need for clarifications.This paper provides an interesting analysis of the conditions which enable generation of natural looking textures. The results is quite surprising, and analysis is quite thorough. \nI do think the evaluation methods require more work, but as other reviewers mentioned this could be an interesting line of work moving forwards and does not take too much from this current paper which, I think, should be accepted.", "title": "No further questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HyAwcr-4g": {"type": "review", "replyto": "BJhZeLsxx", "review": "I will post my thoughts in the final review, but other reviewers had already asked most of what I need for clarifications.This paper provides an interesting analysis of the conditions which enable generation of natural looking textures. The results is quite surprising, and analysis is quite thorough. \nI do think the evaluation methods require more work, but as other reviewers mentioned this could be an interesting line of work moving forwards and does not take too much from this current paper which, I think, should be accepted.", "title": "No further questions", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HkzVKW0mx": {"type": "rebuttal", "replyto": "ryuuM3AGl", "comment": "1. First let's make sure that there is no confusion about the goal of this paper: We do not aim to outperform VGG, we want to understand which components of VGG are critical for achieving the high texture synthesis performance that VGG achieves. To this end, we test how much VGG can be simplified without losing performance. Surprisingly, we find that we neither need a deep network nor does it require learning of the filters. \n\n2. Obviously VGG is not perfect but given that we here take it as reference it is natural to use the VGG loss as quantitative measure to evaluate how the random shallow networks compare to VGG.\n\n3. Regarding SSIM, we would like to point out that VGG loss is a much better metric to judge perceptual texture similarity than SSIM (see the similarity matrix we added to the appendix of the paper)\n\n4. We agree that inpainting is another interesting way of evaluating how well a texture synthesis model can cope with variable context. We now included an example in the appendix.", "title": "Clarifications about usage of the VGG-loss, structural similarity index and inpainting for texture evaluation"}, "SJU5ImJXg": {"type": "rebuttal", "replyto": "Sk1g_b8fe", "comment": "> The optimisation algorithm is local, so it is possible that, by restarting the VGG-19 \n> optimisation, a result with a lower energy could be obtained. Is this possible?\n\nThat\u2019s a nice suggestion! We tested it by initializing the VGG-19 optimization with the synthesis result from the shallow network. At the time of initialization, the loss is typically on the order of the original VGG loss (as shown in Fig. 4). After subsequent optimization with respect to the VGG objective, the loss decreases up to two orders of magnitude and the perceptual quality further increases. We included this result in the appendix (please see the updated paper).\n\n> since it is clear that only reproducing an image statistics is meaningless (as the best\n> possible result is the original image), can the authors discuss how \n> this family of texture generation methods should, at least in principle, be tested?\n\nIn principle, one would need to estimate the entropy of the samples. This is hard for optimization-based texture synthesis models. Alternatively, one could ask humans or train a classifier to judge how easily one can tell apart (multiple) samples drawn from the model from (multiple) patches taken from the original texture. Similarly, in a 2AFC task humans could be asked to pick between a patch and an inpainting result from the model.\n\nIn this paper we emphasise the surprising quality of textures synthesised from very simple shallow and random networks, and to this end we employ simple human judgement. We have been doing human psychophysics on texture discrimination before [1,2] and are planning new experiments in this direction, but this goes far beyond the scope of the presented work.\n\n[1] H. E. Gerhard and M. Bethge, Towards a rigorous study of artistic style: a new psychophysical paradigm. Art and Perception, 2, 23-44, 2014 \n\n[2] H. E. Gerhard, F. A. Wichmann, and M. Bethge, How sensitive is the human visual system to the local statistics of natural images? PLoS Computational Biology, 9(1), 2013\n\n> As discussed in detail in the last section of the paper, a texture method that just copies the \n> reference texture image is useless; the aim is instead to draw \"fair\" samples from the texture \n> intended as an image distribution. Is it possible that by fitting the energy function less, the \n> texture sampling properties are actually better?\n\nFor finite-size patches, the summary statistics are never matched exactly, and we thus introduced an extension of the common maximum entropy framework to allow a more \u201cfuzzy\u201d matching of the constraints (last part of discussion). Matching the constraints within some region of radius epsilon > 0 increases the set of preimages and thus likely increases the variability of the samples. The degree of variability (i.e. the size of epsilon) can be estimated from the data (which is related to a suggestion by Joan Bruna).\n\n> Several filter types are compared (Fourier, random, etc). However, the differences are not \n> discussed very explicitly. The main conclusion is random filters are, sometimes, just as good \n> as filters trained from data. What about e.g. the Fourier filters?\n\nWe picked Fourier filters based on the hypothesis that the failure of some texture models such as ICA is caused by large metric changes. A basis of fourier filters, on the other hand, is  orthogonal (thus metric-preserving) , orientation selective and multi-scale. We tested many other filters based on other specific hypothesis, some of which we have not shown in the paper (e.g. Haar wavelets), but none of them lead to any notable increase in the quality of the synthesised textures (compared to random filters). Of course, this empirical approach does not allow us to draw general conclusions; there might be data-dependent filters that substantially increase synthesis quality. Our empirical insights are important nonetheless since many \u201cdevils in the details\u201d are easy to overlook without sufficient empirical experience.", "title": "clarifications"}, "ryuuM3AGl": {"type": "review", "replyto": "BJhZeLsxx", "review": "The texture evaluation (described in eq. 6) could not measure the perceptual quality for me. Some generated textures looks broken (the bottom-right in Figure 1), which were seen as an successful example. Why not do comparison on texture inpainting like in [1] and you could use structural similarity index [2] to measure the perceptual quality more properly? \n\n[1] Heess, N., Williams, C. K. I. and Hinton, G. E.Learning generative texture models with extended Fields-of-Experts.\n[2] Wang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P. (2004). Image quality assessment: From error visibility to structural similarity. IEEE TRANSACTIONS ON IMAGE PROCESSING, 13(4), 600\u2013612.This work proposed a simple but strong baseline for parametric texture synthesis. In empirical experiments, samples generated by the baseline composed by multi-scale and random filters sometime rival the VGG-based model which has multi-layer and pre-trained filters. The authors concluded that texture synthesis does not necessarily depend on deep hierarchical representations or the learned feature maps. \nThis work is indeed interesting and insightful. However, the conclusions are needed to be further testified (especially for deep hierarchical representations). Firstly, all of generated samples by both VGG and single layer model are not perfect and much worse than the results from non-parametric methods.  Besides VGG-based model seems to do better in inpainting task in Figure 7. Last but not least, would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently? ", "title": "The used method of texture evaluation seems to be suspicious. Why not try compare different networks on texture inpainting task?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ry3uL67Eg": {"type": "review", "replyto": "BJhZeLsxx", "review": "The texture evaluation (described in eq. 6) could not measure the perceptual quality for me. Some generated textures looks broken (the bottom-right in Figure 1), which were seen as an successful example. Why not do comparison on texture inpainting like in [1] and you could use structural similarity index [2] to measure the perceptual quality more properly? \n\n[1] Heess, N., Williams, C. K. I. and Hinton, G. E.Learning generative texture models with extended Fields-of-Experts.\n[2] Wang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P. (2004). Image quality assessment: From error visibility to structural similarity. IEEE TRANSACTIONS ON IMAGE PROCESSING, 13(4), 600\u2013612.This work proposed a simple but strong baseline for parametric texture synthesis. In empirical experiments, samples generated by the baseline composed by multi-scale and random filters sometime rival the VGG-based model which has multi-layer and pre-trained filters. The authors concluded that texture synthesis does not necessarily depend on deep hierarchical representations or the learned feature maps. \nThis work is indeed interesting and insightful. However, the conclusions are needed to be further testified (especially for deep hierarchical representations). Firstly, all of generated samples by both VGG and single layer model are not perfect and much worse than the results from non-parametric methods.  Besides VGG-based model seems to do better in inpainting task in Figure 7. Last but not least, would a hierarchical model (instead of lots of filters with different size) handle multi-scale more efficiently? ", "title": "The used method of texture evaluation seems to be suspicious. Why not try compare different networks on texture inpainting task?", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Sk1g_b8fe": {"type": "review", "replyto": "BJhZeLsxx", "review": "The framework of Gatys ea al. shows that correlation statistics (empirical gram matrices) of deep feature responses provides an excellent characterisations of visual texture. This paper investigates in detail which kind of deep (or shallow) features may work well in this framework. \n\nOne of the main results is that that very shallow networks, consisting of a number of parallel one-layer filter banks with random weights, work surprisingly well, and for simple and regular textures tend to produce results which are visually better than using very complex data-adapted filters such as the ones in VGG-19.\n\nThe contribution is a nice one, interesting due to its fairly surprising findings, and possibly more interesting due to the caveats that it explicitly and implicitly raises about such methods for texture synthesis.\n\nI have a few questions for the authors. Figure 4 show that direct optimisation of shallow filters results in texture images that have a lower VGG-19 loss. This is imputed to the difficulty of optimising the highly non-linear VGG-19, which is a reasonable explanation. In fact, if by optimising the simpler statistics we can obtain a copy of the input texture, obviously the VGG-19 loss would also be zero. However:\n\n- The optimisation algorithm is local, so it is possible that, by restarting the VGG-19 optimisation, a result with a lower energy could be obtained. Is this possible?\n\n- As discussed in detail in the last section of the paper, a texture method that just copies the reference texture image is useless; the aim is instead to draw \"fair\" samples from the texture intended as an image distribution. Is it possible that by fitting the energy function less, the texture sampling properties are actually better?\n\n- Connected to the latter point, since it is clear that only reproducing an image statistics is meaningless (as the best possible result is the original image), can the authors discuss how this family of texture generation methods should, at least in principle, be tested?\n\n- Several filter types are compared (Fourier, random, etc). However, the differences are not discussed very explicitly. The main conclusion is random filters are, sometimes, just as good as filters trained from data. What about e.g. the Fourier filters?\n\n\n\nThe framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.\n\nFigure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.\n\nThe main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.\n\nThe authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.\n\nOverall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.\n\n\n\n", "title": "Clarifications and discussion about evaluation methodology", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "H1SCAZJ4e": {"type": "review", "replyto": "BJhZeLsxx", "review": "The framework of Gatys ea al. shows that correlation statistics (empirical gram matrices) of deep feature responses provides an excellent characterisations of visual texture. This paper investigates in detail which kind of deep (or shallow) features may work well in this framework. \n\nOne of the main results is that that very shallow networks, consisting of a number of parallel one-layer filter banks with random weights, work surprisingly well, and for simple and regular textures tend to produce results which are visually better than using very complex data-adapted filters such as the ones in VGG-19.\n\nThe contribution is a nice one, interesting due to its fairly surprising findings, and possibly more interesting due to the caveats that it explicitly and implicitly raises about such methods for texture synthesis.\n\nI have a few questions for the authors. Figure 4 show that direct optimisation of shallow filters results in texture images that have a lower VGG-19 loss. This is imputed to the difficulty of optimising the highly non-linear VGG-19, which is a reasonable explanation. In fact, if by optimising the simpler statistics we can obtain a copy of the input texture, obviously the VGG-19 loss would also be zero. However:\n\n- The optimisation algorithm is local, so it is possible that, by restarting the VGG-19 optimisation, a result with a lower energy could be obtained. Is this possible?\n\n- As discussed in detail in the last section of the paper, a texture method that just copies the reference texture image is useless; the aim is instead to draw \"fair\" samples from the texture intended as an image distribution. Is it possible that by fitting the energy function less, the texture sampling properties are actually better?\n\n- Connected to the latter point, since it is clear that only reproducing an image statistics is meaningless (as the best possible result is the original image), can the authors discuss how this family of texture generation methods should, at least in principle, be tested?\n\n- Several filter types are compared (Fourier, random, etc). However, the differences are not discussed very explicitly. The main conclusion is random filters are, sometimes, just as good as filters trained from data. What about e.g. the Fourier filters?\n\n\n\nThe framework of Gatys et al. demonstrated that correlation statistics (empirical Gram matrices) of deep feature responses provide an excellent characterisation of visual textures. This paper investigates in detail which kind of deep or shallow networks may work well in this framework. One of the main findings is that that very shallow nets, consisting of a single filter bank with random weights, work surprisingly well, and for simple and regular textures may produce results which are visually superior to complex data-adapted filters such as the ones in networks like VGG-19. More broadly, the paper contains an interesting and informative discussion on the strength and limitations on such methods for texture synthesis.\n\nFigure 4 shows that the optimisation of images with respect to shallow filter banks may result in texture images that have a lower VGG-19 loss than optimising the VGG-19 objective directly. This is imputed to the difficulty of optimising the highly non-linear VGG-19 cost function, which is a reasonable explanation. In the new supplementary material, the authors show that better optimisation results can be obtained by initialising the VGG-19-based optimisation with the shallow network optimisation results, which is a useful complement to the original experiments.\n\nThe main limitation of the paper is that it does not systematically compare different methods against a quantifiable objective. It is trivial to define image statistics that would allow to simply generate an exact copy of any reference texture, hence with very good visual quality. Such trivial statistics would also be very shallow. The aim is instead to capture a texture distribution, and measuring how well a method meets this challenge remains an open problem. Hence, while the empirical results seem to confirm the intuition that simple statistics are good enough for texture synthesis both in terms of quality and diversity (when compared to more complex statistics), it is difficult to conclusively confirm that this is the case.\n\nThe authors indicate that diversity could be measured in terms of entropy. This is reasonable, but, as they acknowledge in their answers to questions, difficult to do in practice. Furthermore, this would still not account for the other aspect of the problem, namely visual quality. They also suggest to perform a psychophysical assessment, which may be the only practical way of addressing this problem, but deem that to be material for future work.\n\nOverall, since evaluation of image generation is such an hard problem, I think the paper still has sufficient strengths to warrant publication in ICLR. Still, some form of psychophysical assessment would be useful to confirm the intuitions that, at present, can only be obtained by inspecting the figure sin the paper and in the supplementary material.\n\n\n\n", "title": "Clarifications and discussion about evaluation methodology", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}