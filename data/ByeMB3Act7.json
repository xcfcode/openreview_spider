{"paper": {"title": "Learning to Screen for Fast Softmax  Inference on Large Vocabulary Neural Networks", "authors": ["Patrick Chen", "Si Si", "Sanjiv Kumar", "Yang Li", "Cho-Jui Hsieh"], "authorids": ["patrickchen@g.ucla.edu", "sisidaisy@google.com", "sanjivk@google.com", "liyang@google.com", "chohsieh@cs.ucla.edu"], "summary": "", "abstract": "Neural language models have been widely used in various NLP tasks, including machine translation, next word prediction and conversational agents. However, it is challenging to deploy these models on mobile devices due to their slow prediction speed, where the bottleneck is to compute top candidates in the softmax layer. In this paper, we introduce a novel softmax layer approximation algorithm by exploiting the clustering structure of context vectors. Our algorithm uses a light-weight screening model to predict a much smaller set of candidate words based on the given context, and then conducts an exact softmax only within that subset. Training such a procedure end-to-end is challenging as traditional clustering methods are discrete and non-differentiable, and thus unable to be used with back-propagation in the training process. Using the Gumbel softmax, we are able to train the screening model end-to-end on the training set to exploit data distribution. The algorithm achieves an order of magnitude faster inference than the original softmax layer for predicting top-k words in various tasks such as beam search in machine translation or next words prediction. For example, for machine translation task on German to English dataset with around 25K vocabulary, we can achieve 20.4 times speed up with 98.9% precision@1 and 99.3% precision@5 with the original softmax layer prediction, while state-of-the-art (Zhang et al., 2018) only achieves 6.7x speedup with 98.7% precision@1 and 98.1% precision@5 for the same task.", "keywords": ["fast inference", "softmax computation", "natural language processing"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper introduces an approach for improving the scalability of neural network models with large output spaces, where naive soft-max inference scales linearly with the vocabulary size. The proposed approach is based on a clustering step combined with per-cluster, smaller soft-maxes. It retains differentiability with the Gumbel softmax trick. The experimental results are impressive. There are some minor flaws, however there's consensus among the reviewers the paper should be published.\n"}, "review": {"HkeASf47AX": {"type": "rebuttal", "replyto": "HyehHMVFnm", "comment": "We want to thank the reviewer for the useful suggestions!!\n\n-- about larger vocabulary experiment:\n\nWe have added an experiment with a much larger dataset --- Wikitext103 with vocabulary size of 80k. The result of prediction time speedup versus accuracy is shown in Figure 9 in the new version. As you can see from the figure, we can achieve more than 15x speedup with accuracy of 99.8%. In addition, in Table 3, we show the result on DE-EN, an NMT task with vocabulary size around 25k. We summarize the vocabulary size of all the datasets in Table 1. \n\n-- about result on speed-up of L2S over full softmax with respect to the vocabulary size\n\nWe have included an experiment of prediction time speed-up versus vocabulary size on PTB dataset. Results are summarized in Figure 8. In this figure, we could observe that our method can achieve higher speed-up with larger vocabulary size.\n\n-- about clustering parameters and label sets\n\nWe have added Table 7 to show the label sets learned from our method. We observe some interesting clusters---some words with similar meanings are in the same cluster.\n\n", "title": "Response to Reviewer 1"}, "HyetbGEm0m": {"type": "rebuttal", "replyto": "ByxDcLsR2X", "comment": "Thanks for your comments and that you enjoyed reading the paper!  \n\nResponses to questions:\n\n-- about larger vocabulary experiment:\n\nWe have added an experiment with a much larger dataset --- Wikitext103 with vocabulary size to be 80k. The result of prediction time speedup versus accuracy is shown in Figure 9 in the new version. As you can see from the figure, we can achieve more than 15x speedup with accuracy of 99.8%. In addition, in Table 3, we show the result on DE-EN, an NMT task with vocabulary size around 25k. We summarize the vocabulary size of all the datasets in Table 1. \n\n-- about perplexity and probability estimation\n\nThis is a great point. We agree that our method tends to generate better approximation of ranking of the words instead of probability of that word. The main reason for the reduced gain for PPL is that to compute PPL, after performing our method (L2S), we need an additional step to assign a probability to words that are not located in the predicted cluster, although this is a rare case (less than 5% chance). There are several potential ways to model this rare case and we chose to use SVD to approximate probability (same as svd softmax [Kyuhong Shim et.al in NIPS 2017]); however, SVD itself has lots of computational overhead. Therefore prediction time speedup is less pronounced for PPL than for the accuracy results. \n\nOn the other hand, we get reasonable probability estimation when the word is within the predicted cluster (usually they are top-k predicted words). Therefore we still achieve very good (>10x) speed up in NMT tasks with beam search (see Table 3). \n\n\n-- about qualitative analysis \n\nWe have added two qualitative analyses in the new version. Firstly, we show the words from different clusters learned from our method in Table 7, and observe some interesting structures--some words with similar meanings are in the same cluster. Secondly, examples of translation pairs by our method compared with original softmax results are shown in Table 8. \n", "title": "Response to Reviewer 3"}, "SJl5TbNQAQ": {"type": "rebuttal", "replyto": "S1gF35P_nQ", "comment": "We are thankful for the constructive comments!!\n\n-- about word clusters are not continuous and training end to end \n\nThere are several ways to make word clusters continuous such as using soft clustering, however, these strategies on the other hand will increase the prediction time. Even though word clusters representation is not continuous in L2S, our model can still train end-to-end in the sense that the clustering stage and the label selection are trained jointly with the gumbel technique. Our algorithm back-propagates the gradient to the clustering weights to update both clustering partition and label sets simultaneously. \n\n-- about speeding up training time\n\nWe focus on speeding up prediction in this work. We could potentially use the same idea--clustering+learning candidate words, to speed up training as well since we could narrow down the update on a few candidate words instead of the entire vocabulary when updating softmax\u2019s weight matrix. This is certainly an interesting future direction to work on.\n\n-- qualitative examples\n\nWe have added two qualitative analyses in the new version. Firstly, we show the words from different clusters learned from our method in Table 7, and observe some interesting structures---some words with similar meanings are in the same cluster. Secondly, examples of translation pairs by our method compared with full softmax results are shown in Table 8. \n", "title": "Response to Reviewer 2"}, "Hyx4lWEXRX": {"type": "rebuttal", "replyto": "ByeMB3Act7", "comment": "Hi all,\n\nWe appreciate the constructive feedback from the reviewers and the community.  And thanks for the patience for waiting our responses. We have made the following main changes to the current version to make our paper more complete.\n\n1. For NMT task, we apply our method on a new dataset EN-VE translation with vocabulary size of 22749. Results are summarized in Table 3. For this task, our method can achieve 20x speedup with BLEU score of 25.27, and the original softmax\u2019s BLEU is 25.35.\n\n2. Besides additional NMT experiment, we perform our algorithm on a larger vocabulary dataset Wikitext-103, a language model dataset with 80k vocabularies. Results are summarized in Figure 9. For this task, our method can achieve more than 15x speedup with P@1 at 99.8%.\n\n3. We also include an experiment on prediction time speed-up versus vocabulary size on PTB dataset. In this experiment, we vary the vocabulary size and show the speedup and accuracy. Results are summarized in Figure 8, showing that our method achieves higher speed-up with larger vocabulary size. \n\n4. We add two qualitative analysis in the appendix. Firstly, we show the words from different clusters learned from our method in Table 7, and observe some interesting structures--some words with similar meanings are in the same cluster. Secondly, examples of translation pairs by our method compared with full softmax results are shown in Table 8. Please look through those interesting examples!\n", "title": "Summary of Changes"}, "ByxDcLsR2X": {"type": "review", "replyto": "ByeMB3Act7", "review": "This paper proposes a novel method to speedup softmax computation at test time. Their approach is to partition the large vocabulary set into several discrete clusters, select the cluster first, and then do a small scale exact softmax in the selected cluster. Training is done by utilizing the Gumbel softmax trick.\n\nPros:\n1. The method provides another way that allows the model to learn an adaptive clustering of vocabulary. And the whole model is made differentiable by the Gumbel softmax trick. \n2. The experimental results, in terms of precision, is quite strong. The proposed method is significantly better than baseline methods, which is a really exciting thing to see. \n3. The paper is written clearly and the method is simple and easily understandable. \nCons:\n1. I\u2019d be really expecting to see how the model will perform if it is trained from scratch in NMT tasks. And I have reasons for this. Since the model is proposed for large vocabularies, the vocabulary of PTB (10K) is by no terms large. However, the vocabulary size in NMT could easily reach 30K, which would be a more suitable testbed for showing the advantage of the proposed method.  \n2. Apart from the nice precision results, the performance margin in terms of perplexity seems not as big as that of precision. And according to earlier discussions in the thread, the author confirmed that they are comparing the precision w.r.t. original softmax, not the true next words. This could raise a possible assumption that the model doesn\u2019t really get the probabilities correct, but somehow only fits on the rank of the words that was predicted by the original softmax. Maybe that is related to the loss? However, I believe sorting this problem out is kind of beyond the scope of this paper.  \n3. In another scenario, I think adding some qualitative analysis could better present the work. For example, visualize the words that got clustered into the same cluster, etc. \n\nIn general, I am satisfied with the content and enjoys reading the paper. \n", "title": "a nice method accelerating softmax for prediction in large vocabulary at test time", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyehHMVFnm": {"type": "review", "replyto": "ByeMB3Act7", "review": "This paper presents an approximation to the softmax function to reduce the computational cost at inference time and the proposed approach is evaluated on language modeling and machine translation tasks. The main idea of the proposed approach is to pick a subset of the most probable outputs on which exact softmax is performed to sample top-k targets. The proposed method, namely Learning to Screen (L2S), learns jointly context vector clustering and candidate subsets in an end-to-end fashion, so that it enables to achieve competitive performance.\n\nThe authors carried out NMT experiments over the vocabulary size of 25K. It would be interesting if the authors provide a result on speed-up of L2S over full softmax with respect to the vocabulary size. Also, the performance of L2S on larger vocabularies such as 80K or 100K needs to be discussed.\n\nAny quantitative examples regarding the clustering parameters and label sets would be helpful.\nL2S is designed to learn to screen a few words, but no example of the screening part is provided in the paper.", "title": "Fast and accurate approximation to softmax, but more in-depth analysis results would be required", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "S1gF35P_nQ": {"type": "review", "replyto": "ByeMB3Act7", "review": "The paper proposes a way to speed up softmax at test time, especially when top-k words are needed. The idea is clustering inputs so that we need only to pick up words from a learn cluster corresponding to the input. The experimental results show that the model looses a little bit accuracy in return of much faster inference at test time. \n\n* pros: \n- the paper is well written. \n- the idea is simple but BRILLIANT. \n- the used techniques are good (especially to learn word clusters). \n- the experimental results  (speed up softmax at test time) are impressive. \n\n* cons: \n- the model is not end-to-end because word clusters are not continuous. But it not an important factor. \n- it can only speed up softmax at test time. I guess users are more interesting in speeding up at both test and training time.\n- it would be better if the authors show some clusters for both input examples and corresponding word clusters.\n\n\n", "title": "I like the pape", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}