{"paper": {"title": "Density estimation using Real NVP", "authors": ["Laurent Dinh", "Jascha Sohl-Dickstein", "Samy Bengio"], "authorids": ["dinh.laurent@gmail.com", "jaschasd@google.com", "bengio@google.com"], "summary": "Efficient invertible neural networks for density estimation and generation", "abstract": "Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.", "keywords": ["Deep learning", "Unsupervised Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper clearly lays out the recipe for a family of invertible density models, explores a few extensions, and demonstrates the overall power of the approach. The work is building closely on previous approaches, so it suffers a bit in terms of originality. However, I expect this overall approach to become a standard tool for model-building.\n \n The main weakness of this paper is that it did not explore the computational tradeoffs of this approach against related methods. However, the paper already had a lot of content."}, "review": {"B1JXfvaUx": {"type": "rebuttal", "replyto": "ryV5E2-4e", "comment": "Thanks for your input!\n\"Given that inference and generation are both efficient and exact, and given that this represents a main advantage over other models, it would be great if the authors could provide some motivating example applications where this is needed/would be useful.\"\nEfficient inference is necessary for tractable training in most probabilistic models and efficient sampling is useful for application that would require fast sampling, e.g. music and speech synthesis and real-time rendering. Having exact inference allows us to accurately and exactly evaluate our models, and could be highly beneficial for tasks such as semi-supervised learning, which require accurately estimating a latent state. Having exact sampling as opposed to MCMC sampling efficiently prevents us from having highly correlated samples (and poor diversity of samples).\nWe have added this to the submission as an additional argument.\n\n\"The authors claim that \u201cunlike both variational autoencoders and GANs, our technique is able to learn a semantically meaningful latent space which is as high dimensional as the input space.\u201d Where is the evidence for this claim? I didn\u2019t see any analysis of the semantic meaningfulness of the latent space learned by real NVP. Stronger evidence that the learned representations are actually useful for downstream tasks would be nice.\"\nThank you for pointing out this claim. The aim of this sentence was to focus more on the \"as high dimensional as the input space\" aspect as opposed to VAEs and GANs, which previously used effectively low dimensional latent codes for their models.\nAs for showing that the latent space is semantically meaningful, based on your feedback we have added an additional experiment to the paper (Appendix F), showing that in class-conditional image generation latent variables have a consistent semantic meaning across classes.\n\n\"I still think the author\u2019s intuitions around the \u201cfixed reconstruction cost of L2\u201d are very vague. The factorial Gaussian assumption itself does not limit the generative model, it merely smoothes an otherwise arbitrary distribution, and to a degree which can be arbitrarily small, p(x) = \\int p(z) N(x | f(z), \\sigma^2) dz. How a lose lower bound plays into this is not clear from the paper.\"\nWhat you are saying is true but your statement is about the class of generative models associated to Variational Auto-Encoders. The Variational Auto-Encoder learning also includes an encoder for the approximate posterior. Often this approximation is not enough which often results in under exploiting the capacity that these generative models (the decoders) have. The issue here is trainability.\nThis lack of trainability is due to the gap between the approximate posterior and the real posterior. As the reconstruction cost becomes more flexible, the *real* posterior p(z|x) = p(z)p(x|z)/p(x) becomes more able to fit the family of approximate posterior considered.\nSince the problem of using fixed form reconstruction cost in VAEs is already mentioned in \"Autoencoding beyond pixels using a learned similarity metric\", we chose not to put a focus on explaining this issue.", "title": "Answer to AnonReviewer2"}, "Sk7bzoLUl": {"type": "rebuttal", "replyto": "HkxVRCfNg", "comment": "Hi,\n\nThanks you for reviewing our paper. \n\"The paper presents a lot of interesting experiments showing the capabilities of the proposed technique. Making the code available online will certainly contribute to the field. Is there any intention of releasing the code?\"\nOur code is now included in the Tensorflow models open source repository (https://github.com/tensorflow/models/tree/master/real_nvp), in addition to that, our model has been reimplemented  by Taesung Park on Github: https://github.com/taesung89/real-nvp\n\nWe will definitely correct the typo you mentioned.", "title": "Answer to AnonReviewer3"}, "BJBMQVnQe": {"type": "rebuttal", "replyto": "SJRCCZ0Ge", "comment": "Hi, \n\nI used cosine interpolation because, in high dimension, the mass of the gaussian distribution is mainly concentrated on a sphere (the norm-2 of a multivariate standard gaussian variable follows a chi-squared distribution). Hence, a cosine interpolation would mainly travel in area of high probability according to the prior distribution.\nWe are using batch normalization with moving average only on CIFAR-10 because this is where it had noticeable effect, the choice of the batch normalization scheme for the other datasets did not seem to make a difference.\nThe train and valid results remained close during training on large scale datasets, which seems to suggest that we are not overfitting.", "title": "Re: Latent space interpolation, batch norm with moving average and close train/valid results"}, "ryP23EnXe": {"type": "rebuttal", "replyto": "S1Ivr4kXl", "comment": "Hi, \n\nTheir objective functions _can be_ very similar indeed as Real NVP optimizes the exact log-likelihood whereas VAE optimizes the Evidence Lower Bound of the log-likelihood and their closeness depends on how much the approximate posterior q(z|x) matches the true posterior p(z|x)=p(z)p(x|z)/p(x). The Variational Lossy Autoencoder paper provides further discussion on the subject: https://openreview.net/forum?id=BysvGP5ee. \n\nHowever, we are referring to the standard VAE algorithms from (Kingma and Welling, 2013) and (Rezende et al., 2014) where p(x|z) is factorial. This results in a fixed form reconstruction loss -log(p(x|z)), a L2 loss for a conditional gaussian p(x|z). Even if you have a small decoder noise, i.e. p(x|z) has small entropy, this reconstruction loss remains fixed nonetheless. I agree that it would not be the case if p(x|z) was a more complex conditional distribution as in \"PixelVAE: A Latent Variable Model for Natural Images\" and \"Variational Lossy Autoencoder\", two papers that you will find among ICLR submissions this year. \n\nRelying on fixed form reconstruction cost is not an issue in an infinite capacity setting for either algorithm. ", "title": "Re: Likelihood vs L2 reconstruction"}, "S1Ivr4kXl": {"type": "review", "replyto": "HkpbnH9lx", "review": "The authors claim that \u201cas opposed to [variational autoencoders], real NVP does not rely on fixed form reconstruction cost like an L2.\u201d I can see where the authors get their intuition, nevertheless this statement seems overly simplified/wrong. For small decoder noise, a larger latent space and assuming a strong enough encoder so that the lower bound is close to the likelihood, the differences between what real NVP and a VAE optimize seem to become very small*. Both use a very similar generative model and (approximately) optimize its likelihood. Am I missing something/can you clarify what you mean?\n\n* Note that I am not trying to say that VAE and real NVP are the same, just that their objective functions _can be_ very similar.Building on earlier work on a model called NICE, this paper presents an approach to constructing deep feed-forward generative models. The model is evaluated on several datasets. While it does not achieve state-of-the-art performance, it advances an interesting class of models. The paper is mostly well written and clear.\n\nGiven that inference and generation are both efficient and exact, and given that this represents a main advantage over other models, it would be great if the authors could provide some motivating example applications where this is needed/would be useful.\n\nThe authors claim that \u201cunlike both variational autoencoders and GANs, our technique is able to learn a semantically meaningful latent space which is as high dimensional as the input space.\u201d Where is the evidence for this claim? I didn\u2019t see any analysis of the semantic meaningfulness of the latent space learned by real NVP. Stronger evidence that the learned representations are actually useful for downstream tasks would be nice.\n\nI still think the author\u2019s intuitions around the \u201cfixed reconstruction cost of L2\u201d are very vague. The factorial Gaussian assumption itself does not limit the generative model, it merely smoothes an otherwise arbitrary distribution, and to a degree which can be arbitrarily small, p(x) = \\int p(z) N(x | f(z), \\sigma^2) dz. How a lose lower bound plays into this is not clear from the paper.", "title": "Likelihood vs L2 reconstruction", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryV5E2-4e": {"type": "review", "replyto": "HkpbnH9lx", "review": "The authors claim that \u201cas opposed to [variational autoencoders], real NVP does not rely on fixed form reconstruction cost like an L2.\u201d I can see where the authors get their intuition, nevertheless this statement seems overly simplified/wrong. For small decoder noise, a larger latent space and assuming a strong enough encoder so that the lower bound is close to the likelihood, the differences between what real NVP and a VAE optimize seem to become very small*. Both use a very similar generative model and (approximately) optimize its likelihood. Am I missing something/can you clarify what you mean?\n\n* Note that I am not trying to say that VAE and real NVP are the same, just that their objective functions _can be_ very similar.Building on earlier work on a model called NICE, this paper presents an approach to constructing deep feed-forward generative models. The model is evaluated on several datasets. While it does not achieve state-of-the-art performance, it advances an interesting class of models. The paper is mostly well written and clear.\n\nGiven that inference and generation are both efficient and exact, and given that this represents a main advantage over other models, it would be great if the authors could provide some motivating example applications where this is needed/would be useful.\n\nThe authors claim that \u201cunlike both variational autoencoders and GANs, our technique is able to learn a semantically meaningful latent space which is as high dimensional as the input space.\u201d Where is the evidence for this claim? I didn\u2019t see any analysis of the semantic meaningfulness of the latent space learned by real NVP. Stronger evidence that the learned representations are actually useful for downstream tasks would be nice.\n\nI still think the author\u2019s intuitions around the \u201cfixed reconstruction cost of L2\u201d are very vague. The factorial Gaussian assumption itself does not limit the generative model, it merely smoothes an otherwise arbitrary distribution, and to a degree which can be arbitrarily small, p(x) = \\int p(z) N(x | f(z), \\sigma^2) dz. How a lose lower bound plays into this is not clear from the paper.", "title": "Likelihood vs L2 reconstruction", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJRCCZ0Ge": {"type": "review", "replyto": "HkpbnH9lx", "review": "Why use a cosine interpolation instead of a linear one (Fig. 6)?\nWhy aren't you using your batch normalization with moving average on other datasets than CIFAR-10 (Appendix E)?\nSome train and valid results in Table 1 seem pretty close. Are they statistically different?This paper proposes a new generative model that uses real-valued non-volume preserving transformations in order to achieve efficient and exact inference and sampling of data points.\nThe authors use the change-of-variable technique to obtain a model distribution of the data from a simple prior distribution on a latent variable. By carefully designing the bijective function used in the change-of-variable technique, they obtain a Jacobian that is triangular and allows for efficient computation.\n\nGenerative models with tractable inference and efficient sampling are an active research area and this paper definitely contributes to this field.\n\nWhile not achieving state-of-the-art, they are not far behind. This doesn't change the fact that the proposed method is innovative and worth exploring as it tries to bridge the gap between auto-regressive models, variational autoencoders and generative adversarial networks.\n\nThe authors clearly mention the difference and similarities with other types of generative models that are being actively researched.\nCompared to autoregressive models, the proposed approach offers fast sampling.\nCompared to generative adversarial networks, Real NVP offers a tractable log-likelihood evaluation.\nCompared to variational autoencoders, the inference is exact.\nCompared to deep Boltzmann machines, the learning of the proposed method is tractable.\nIt is clear that Real NVP goal is to bridge the gap between existing and popular generative models.\n\nThe paper presents a lot of interesting experiments showing the capabilities of the proposed technique. Making the code available online will certainly contribute to the field. Is there any intention of releasing the code?\n\nTypo: (Section 3.7) We also \"use apply\" batch normalization", "title": "Latent space interpolation, batch norm with moving average and close train/valid results", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkxVRCfNg": {"type": "review", "replyto": "HkpbnH9lx", "review": "Why use a cosine interpolation instead of a linear one (Fig. 6)?\nWhy aren't you using your batch normalization with moving average on other datasets than CIFAR-10 (Appendix E)?\nSome train and valid results in Table 1 seem pretty close. Are they statistically different?This paper proposes a new generative model that uses real-valued non-volume preserving transformations in order to achieve efficient and exact inference and sampling of data points.\nThe authors use the change-of-variable technique to obtain a model distribution of the data from a simple prior distribution on a latent variable. By carefully designing the bijective function used in the change-of-variable technique, they obtain a Jacobian that is triangular and allows for efficient computation.\n\nGenerative models with tractable inference and efficient sampling are an active research area and this paper definitely contributes to this field.\n\nWhile not achieving state-of-the-art, they are not far behind. This doesn't change the fact that the proposed method is innovative and worth exploring as it tries to bridge the gap between auto-regressive models, variational autoencoders and generative adversarial networks.\n\nThe authors clearly mention the difference and similarities with other types of generative models that are being actively researched.\nCompared to autoregressive models, the proposed approach offers fast sampling.\nCompared to generative adversarial networks, Real NVP offers a tractable log-likelihood evaluation.\nCompared to variational autoencoders, the inference is exact.\nCompared to deep Boltzmann machines, the learning of the proposed method is tractable.\nIt is clear that Real NVP goal is to bridge the gap between existing and popular generative models.\n\nThe paper presents a lot of interesting experiments showing the capabilities of the proposed technique. Making the code available online will certainly contribute to the field. Is there any intention of releasing the code?\n\nTypo: (Section 3.7) We also \"use apply\" batch normalization", "title": "Latent space interpolation, batch norm with moving average and close train/valid results", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJrtbGMze": {"type": "rebuttal", "replyto": "HJ5ml-Gfl", "comment": "Thanks for noticing that!\n\nThere are two views of batch normalization. One is the non-linear mapping from a batch of inputs to a batch of outputs.  In this case, the Jacobian determinant should indeed include the dependencies of the batch statistics with respect to the input x. However, this function is not bijective, so this interpretation cannot be used when learning a bijective map.\n\nThe other view of batch normalization is a linear mapping whose parameters are estimated at every iteration using the current batch statistics. During evaluation time, the quantities mu and sigma are replaced by accumulated statistics over the whole dataset. \n\nWe are using the second view: we consider batch normalization as a linear mapping with parameters estimated from the batch statistics. We therefore do not include the functional dependence of mu and sigma on x in the log determinant of the Jacobian when computing the log likelihood. During test time, batch norm is applied using accumulated values for mu and sigma, with no x dependence.\n\nNote that during training when we compute the gradient of the log likelihood w.r.t. the parameters, the dependence of mu and sigma on x *is* taken into account via automatic differentiation.\n\nWe will clarify this in the next version of the text. ", "title": "There are different interpretations of batch normalization"}, "HJ5ml-Gfl": {"type": "rebuttal", "replyto": "HkpbnH9lx", "comment": "In equations 17-18 the quantities $\\mu$ and $\\sigma$ depend on the current batch $x$. Shouldn't their derivatives with respect to $x$ be part of the Jacobian?", "title": "Equation 18 - missing derivatives of batch statistics?"}}}