{"paper": {"title": "The Dual Information Bottleneck", "authors": ["Zoe Piran", "Naftali Tishby"], "authorids": ["zoe.piran@mail.huji.ac.il", "tishby@cs.huji.ac.il"], "summary": "A new dual formulation of the Information Bottleneck, optimizing label prediction and preserving distributions of exponential form.", "abstract": "The Information-Bottleneck (IB) framework suggests a general characterization of optimal representations in learning, and deep learning in particular. It is based on the optimal trade off between the representation complexity and accuracy, both of which are quantified by mutual information. The problem is solved by alternating projections between the encoder and decoder of the representation, which can be performed locally at each representation level. The framework, however, has practical drawbacks, in that mutual information is notoriously difficult to handle at high dimension, and only has closed form solutions in special cases. Further, because it aims to extract representations which are minimal sufficient statistics of the data with respect to the desired label, it does not necessarily optimize the actual prediction of unseen labels. Here we present a  formal dual problem to the IB which has several interesting properties. By switching the order in the KL-divergence between the representation decoder and data, the optimal decoder becomes the geometric rather than the arithmetic mean of the input points. While providing a good approximation to the original IB, it also preserves the form of exponential families, and optimizes the mutual information on the predicted label rather than the desired one. We also analyze the critical points of the dualIB and discuss their importance for the quality of this approach.", "keywords": ["optimal prediction learning", "exponential families", "critical points", "information theory"]}, "meta": {"decision": "Reject", "comment": "Main content:\n\nBlind review #1 summarizes it well:\n\nThis paper introduces a variant of the Information Bottleneck (IB) framework, which consists in permuting the conditional probabilities of y given x and y given \\hat{x} in a Kullback-Liebler divergence involved in the IB optimization criterion.\n\nInterestingly, this change only results in changing an arithmetic mean into a geometric mean in the algorithmic resolution.\n\nGood properties of the exponential families (existence of non-trivial minimal sufficient statistics) are preserved, and an analysis of the new critical points/information plane induced is carried out.\n\n--\n\nDiscussion:\n\nThe reviews generally agree on the elegant mathematical result, but are critical of the fact that the paper lacks any empirical component whatsoever.\n\n--\n\nRecommendation and justification:\n\nThe paper would be good for ICLR if it had any decent empirical component at all; it is a shame that none was presented as this does not seem very difficult."}, "review": {"ByeFBnz3sH": {"type": "rebuttal", "replyto": "Byl0gW8RFH", "comment": "We thank the reviewer for the\u00a0helpful comments.\u00a0We were delighted to see that the reviewer\u00a0agrees with\u00a0our understanding of\u00a0the contribution and innovations\u00a0that the formalism\u00a0suggest in comparison to the known $\\rm{IB}$.\u00a0\n\nWe will now relate to specific points raised be the reviewer:\n\n-\u00a0Motivation:\u00a0 We refer the reviewer to the general official comment posted.\n\n-\u00a0Interpretation of the results:  Following the reviewers comment we have added examples to previous application of the$\\rm{IB}$  (sec. 1.1 \u201cThe Information Bottleneck method\u201d) to tie the general framework into context and believe that now the dualIB framework can be tested and applied to on similar tasks.\n\n-\u00a0Figures explanation:\u00a0 In addition we understood that the text was lacking an explanation of the experiments and the interpretations of the results, Thus we added an explanation regarding the results and settings for the problem presented in figure 1(a),2(a).\u00a0\n\n- We thank\u00a0the reviewer for pointing out the typo at Equation 3 (fixed in the newly submitted version).\n", "title": "Response to Review #3"}, "S1lGehG3iS": {"type": "rebuttal", "replyto": "HyxXpukrjH", "comment": "We wish to thank to reviewer for his response, while we stand behind it being a theoretical paper (a point we soon relate to in detail) we agree that the paper did not convey the motivation behind the new framework presented and with that the advantages it has. We have revised the paper accordingly as elaborated in the general official comment posted.\n\nWe will now relate to specific points raised be the reviewer:\n\n-\u00a0Theory-based paper:\u00a0\u00a0We agree with the\u00a0reviewer\u00a0that this paper is theory-based as it aims to reveal a new framework, the\u00a0Dual Information Bottleneck\u00a0($\\rm{dualIB}$), and lay down the grounds for future applications of it. As such we find it necessary to analyse it\u2019s analytical/theoretical properties and compare it to the original form, the\u00a0Information Bottleneck ($\\rm{lIB}$).\u00a0\nHaving said that, we understand the need of empirical experiments to demonstrate the claims and have thus added a new example that is analysed in the text, see sec. 3 \u201cThe Exponential Family dualIB\u201d. This is still not an application of a\u00a0concrete representation learning task yet it provides a visual evidence to the claimed behaviour.\u00a0\n\n-\u00a0Motivation:\u00a0 We refer the reviewer to the general official comment posted.\n\n-\u00a0Computational complexity:\u00a0\u00a0The computation complexity reduction emerges from the analytical result (Theorem 7) in which we show that the encoder-decoder are given by the sufficient statistics of the input distribution. Thus, the task is now to track these rather than the full distribution. This result is appealing to many problems which have a natural parametrization (i.e  a physical problem defined by a given Hamiltonian thus a Gibbs distribution) but are intractable using the IB.\n\n-\u00a0Output prediction:\u00a0We clarify that in this we refer to the maximization of the information on the\u00a0newly added\u00a0prediction variable, $\\hat{Y}$, as it rises naturally from the new Lagrangian. Thus the theory behind this lays in the formalism of the problem. in the revised version a visualization of the superiority is shown for the new setting and the phenomenon is clearly, see Figure 4(b).\nAs suggested by the reviewer we further evaluate now the prediction ability over the examined cases by displaying the $\\mathcal{L}_1$-expected loss which makes it evident that the $\\rm{dualExpIB}$ outperforms the IB minimizing the loss for all values of the trade off parameter, see Figure 4(c).\n\n-\u00a0Data from exponential families or\u00a0\u201calmost\u201d: \u00a0We thank the\u00a0reviewer\u00a0for pointing out that the motivation for choosing\u00a0the exponential families was unclear and made an\u00a0attempt to\u00a0clarify this in the text (see Sec. 3 \u201cThe Exponential Family dualIB\u201d ). From a statistical point of view a natural\u00a0way of\u00a0parametrizing data is\u00a0obtained by a distribution from the\u00a0exponential family as\u00a0most of the commonly used distributions form an exponential family or subset of an exponential family. \nMoreover the exponential family form emerges when asking what is the\u00a0maximum-entropy\u00a0distribution consistent with given constraints on expected values.\nWithin the scope of the paper we did not relate to the quality of the exponential family approximation of the data\u2019s distribution.\n\n-\u00a0Application to deep learning:\u00a0We thank the referee for pointing the lack of further discussion. We have clarified that in this we mean that current applications of the $\\rm{IB}$ in the DL regime can be done using the new $\\rm{dualIB}$ formalism, with the latter conserving natural parametrization and symmetries of the original problem. \n\n-\u00a0Deterministic IB: The Deterministic IB aims to better capture the notion of compression whereas our framework focuses on the parametrization of the data and prediction, thus they concern a different task.\n\n-\u00a0\u201cpreserves the low dimensional sufficient statistics of the data\": \u00a0As stated by the reviewer the $\\rm{dualExpIB}$ decoder (and encoder) are now of exponential form and given by sufficient statistics which are given by the one\u2019s of the original distribution, $A_{r}(x)$, as given in equation 16.  Thus it is given that the dimensionality is preserved, with $d$ such variables. The dependance on $\\hat{x}$ is obtained by considering the expectations, $\\sum_{x} A_{r}(x) p_{\\beta}(x\\mid\\hat{x})$, of the same functions, hence preserves the values mean. \n\nWe realize that the $\\rm{dualIB}$ framework is currently given from a theoretical point of view, yet we hope that having made the motivation for it along with it\u2019s advantages clearer now, including the addition of an experiment visualizing superiority of it, that the reviewer finds it relevant to the ICLR conference.\n", "title": "Response to Review #2"}, "rJgaJbzhsH": {"type": "rebuttal", "replyto": "S1xNKglJcH", "comment": "We were glad to see that the conceptual idea along with the innovation presented in the Dual Information Bottleneck ($\\rm{dualIB}$) framework was well understood. \nWe thank the reviewer for raising critical points we failed to address and/or properly explain, we have revised the paper accordingly as elaborated in the general official comment posted and will now relate to specific points raised by the reviewer:\n\n- Motivation: We refer the reviewer to the general official comment posted.\n\n- Definitions and notations: Following the\u00a0reviewer's remark we have added more background defining the Mutual Information (sec. 1.1 \u201cThe Information Bottleneck method\u201d) and adapted the notations of the expected value to the ML preferred formalism ($\\mathbb{E}$).\n\n- The $\\rm{dualIB}$form: The\u00a0reviewer's comment has brought to our attention that the rationale behind the obtained\u00a0dual\u00a0form, that is the permutation\u00a0of the probabilities distribution in the rate distortion function was not made clear in the text, we have added an explanation in the text (sec. 3 \u201cThe Exponential Family dualIB\u201d) and would like to elaborate it here:\nGiven the above motivation, we looked for the minimization problem preserving the\u00a0structure on the decoder, $p(y | \\hat{x})$, an exponential form (not the Bayes optimal we get by minimizing the IB Lagrangian).\nA natural alteration achieving this goal is the geometric mean (rather than an arithmetic one) from which it follows that the distortion function minimizing it is the geometric dual distortion; the KL divergence given by the permutation of the distributions.\u00a0\n\n- VAEs: We agree that this could be interesting but unfortunately within the scope of the paper we could not include this analysis.\n\n- New empirical results: While this work is mainly theoretical and intends to lay down the formalism of the $\\rm{dualIB}$ framework we agree that an empirical result could better convey it\u2019s advantages and thus we added a new experiment in sec. 3 \u201cThe Exponential Family dualIB\u201d:\n\nIn the new experiment we apply the $\\rm{IB}$ and $\\rm{dualExpIB}$ to data sampled from a Normal distribution. \nThe visual results, given in Figure 4(a) now showcase the benefits and improvements of the framework in this scenario.\u00a0\nIn Figure 4(a) we provide examples of the predicted distributions in which one can see the ability of the $\\rm{dualExpIB}$ to better generalize and capture the original distribution even though it is given the empirical sufficient statistics of the sample.\nNext, we aim to visualize the superiority in prediction; we wish clarify that this in this we referred to the maximization of the information on the\u00a0newly added\u00a0prediction variable, $\\hat{Y}$, as it rises naturally from the new Lagrangian.\u00a0\nIn Figure 4(b) we show the $\\hat{Y}$\u2019s information plane it for the new example. The superiority of $\\rm{dualExpIB}$ over the $\\rm{IB}$  is apparent.\nFurthermore, we evaluate the prediction ability over the examined cases using the $\\mathcal{L}_1$-expected loss, testing the accuracy between the obtained predictions of each method to the original population distribution, see Figure 4(c).\nWe see that for all values of the trade off parameter, $\\beta$, the loss is minimized by the $\\rm{dualExpIB}$. \u2028\n\nTo conclude, We hope that we were able to portray the advantages and interest in the $\\rm{dualIB}$\u00a0framework. \nThe framework is given from a theoretical point of view but we hope that with a clearer view of the motivation along with visual examples and elaboration on\u00a0possible applications,\u00a0it\u2019s importance and relevance to the ICLR conference is apparent.\n\u2028", "title": "Response to Review #1"}, "rkewlAW2jr": {"type": "rebuttal", "replyto": "B1xZD1rtPr", "comment": "We thank the reviewers for their thoughtful reviews. While we were glad to see that generally the conceptual idea behind the new Dual Information Bottleneck ($\\rm{dualIB}$) framework was conveyed we feel that that the reviewers pointed out a major flaw in out presentation - lack of properly explaining the\u00a0motivation\u00a0behind the dualIB framework and from there on it's\u00a0superiority over\u00a0the original information\u00a0bottleneck ($\\rm{IB}$).\n\u2028\nWe have thus modified the text as follows: \n- Revised the abstract\n- Added a subsection to the introduction, sec 1.5 \u201cdrawbacks of the IB\u201d.\n- Extended the discussion in sec. 1.6 \u201ccontributions of this work\u201d.\n- Performed an additional experiment specifically relating to the suggested advantages (see sec. 3, Figure 4) of our framework. \n\nWith that, we would like to bring the gist of the motivation here as well:\n\nThe main theoretical and practical drawback of the $\\rm{IB}$\u00a0is its reliance on the explicit knowledge of the joint distribution between the input variable $X$ and label $Y$, $p(x,y)$, in a completely non-parametric manner.\u00a0Often times, the data has natural parametrization (for example biological data-set that is known to follow an analytical model) or that given a finite sample of the data assumptions regarding the \"features\" can be made,\u00a0finding the sufficient statistics\u00a0representation.\u00a0\nSuch parametrization allows for a\u00a0dimensionality reduction (instead of tracking the entire joint\u00a0distribution\u00a0$p(x,y)$\u00a0we may track the\u00a0sufficient statistics) and for generalization to patterns we haven\u2019t observed in the given sample.\u00a0\nHaving that the IB is non-parametric, our motivation was to find an\u00a0approximation to it\u00a0that\u00a0uses\u00a0this information to (a) simplify the task and (b) obtain a better generalization as it follows the inner structure of\u00a0the distribution.\nGiven this aim, by requiring that the decoder will preserve a parametric structure a new\u00a0rate-distortion problem has emerged, the Dual Information\u00a0Bottleneck ($\\rm{dualIB}$).\nAs we show in the text the\u00a0framework is\u00a0applicable to any problem; obtaining a good approximation to the IB and it follows the same\u00a0characteristic behaviour. \u00a0\nThe dualIB's superiority emerges in the\u00a0case that the rule distribution, $p(y|x)$, can be approximated by an exponential family.\nIn the\u00a0exponential dual information bottleneck\u00a0scenario ($\\rm{dualExpIB}$) we show that the obtained solutions,\u00a0encoder-decoder, follow the same\u00a0exponential form with the parameters defined by the original parameters through expectations. This achieves the desired property of preserving the natural form of the distribution thus obtaining a better generalization, as we now emphasize by providing new experimental results.\u00a0\n\n", "title": "A general response clarifying revisions"}, "HyxXpukrjH": {"type": "review", "replyto": "B1xZD1rtPr", "review": "Description:\n\nThe information bottleneck (IB) is an information theoretic principle for optimizing a mapping (encoding, e.g. clustering) of an input, to trade off two kinds of mutual information: minimize mutual information between the original input and the mapped version (to compress the input), and maximize mutual information between the mapped input and an output variable. It is related to a minimization of an (expected) Kullback-Leibler divergence betwen conditional distributions of an output variable.\n\nIn this paper, instead of the original IB, authors consider a previously presented dual problem of Felice and Ay, where the Kullback-Leibler divergence is minimized in the reverse direction: from the conditional distribution of output given encoding, p(y|hat x), to the conditional distribution given the original input, p(y|x).\nThe \"dual problem\" itself has a more complicated form than the original IB, authors claim this is \"a good approximation\" of the original bottleneck formulation, and aim to prove various \"interesting properties\" of it. \n\n- An iterative algorithm (Algorithm 1) similar to the original IB algorithm but with a few more steps is provided.\n\n- A theorem about critical points where cardinality of the representation changes is given, similar to the IB critical points, and another theorem about difference of curves on an information plane between the IB and dual-IB solutions.\n\n- Authors also show that if the true conditional distribution of outputs given inputs has an exponential-family form, the dual-IB decoder also has a form in the same family, which is said to reduce computational complexity of the algorithm.\n\n\n\nEvaluation:\n\nThis is an entirely theory-based paper; although an algorithm is given, it is not instantiated for any concrete representation learning task, and no experiments at all are demonstrated.\n\nOverall, I feel the motivation is not clear and strong enough. The abstract does not illustrate the importance of the mentioned \"interesting properties\" well enough for concrete tasks. Reading the paper, the clearest motivations seem to be improving computational complexity, and having a clearer connection to output prediction in cases where the predictor may be sub-optimal. However, authors do not quantify these well:\n\n- The computational complexity improvement is not made clear (quantified) in a concrete IB optimization task: it seems it is only for exponential families, and even for them only affects one part of the algorithm, reducing its complexity from dim(X) to d; the impact of this is not tried out in any experiment.\n\n- For output prediction, authors motivate that dual-IB could have a more direct connection e.g. \"due to finite sample, in which it can be very different from the one obtained from the full distribution\"). Authors further claim that the dual-IB formulation can \"improve the generalization error when trained on small samples since the predicted label is the one used in practice\". However, this is not tested at all: no prediction experiments, no quantification of generalization error, and no comparisons are done, thus the impact of the clearer connection to output prediction is not tested at all, and no clear theorems are given about it either.\n\nThe property that the algorithm \"preserves exponential form of the original data distribution, if one exists\" is interesting in principle, but it is unclear if any real data would anyway precisely have such a distribution; what happens if the data is not exactly in an exponential family?\n\nIn its current state the paper, although based on an interesting direction, in my opinion does not make a sufficient impact to be accepted to ICLR.\n\nOther comments:\n\n\"Application to deep learning\" mentioned in Section 1.5 is only a sincle sentence in the conclusions.\n\nThere have been some other suggested alternative IB formulations, for example the Deterministic IB of Strouse and Schwab (Neural Computation 2017) which also claim improved computational efficiency. How does the method compare to those?\n\nSection 1.5 claims the algorithm \"preserves the low dimensional sufficient statistics of the data\": it is not clear what \"preserves\" means here, certainly it seems the decoder in Theorem 7 uses the same kinds of sufficient statistics as in the original data, but it is not clear that hat(x) would somehow preserve the same values of the sufficient statistics.\n", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 3}, "Byl0gW8RFH": {"type": "review", "replyto": "B1xZD1rtPr", "review": "This paper proposes a new \"dual\" variant of the Information Bottleneck framework. The IB framework has been the subject of many papers in past years, with a focus on understanding the inner workings of deep learning. The framework poses machine learning as optimizing an internal representation to tradeoff between retaining less information about the input features and retaining more information about the output label (prediction). The existing framework measures the retained information about the prediction via mutual information, which can be expressed as a KL divergence. The new dual framework reverses the arguments of this divergence.\n\nThe paper shows that this dual IB closely mirrors the original IB while having several additional nice mathematical properties. In particular,  it can be shown that for exponential families the dual IB representation retains the exponential form.\n\nOverall, I think this paper adds a meaningful new perspective to the IB framework and the analysis appears to be thorough. As such, I support acceptance. \n\nThe paper could be improved by giving more interpretation of the formal results and experiments - i.e. tying this framework back to the higher-level questions and explaining what the quantities mean.\n\nFigures 1, 2 part a: The meaning of both axes is unclear. The horizontal axis beta is a lagrangian parameter with no meaning outside the framework. The vertical axis Pr[y=0|\\hat x] has no semantics since the problem has not been defined. What is the reader meant to take away from these figures?\n\nEquation 3, part i: The numerator should have a beta subscript. The meaning of the denominator is not clear (it is a normalizing constant).\n\n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}, "S1xNKglJcH": {"type": "review", "replyto": "B1xZD1rtPr", "review": "This paper introduces a variant of the Information Bottleneck (IB) framework, which consists in permuting the conditional probabilities of y given x and y given \\hat{x} in a Kullback-Liebler divergence involved in the IB optimization criterion.\n\nInterestingly, this change only results in changing an arithmetic mean into a geometric mean in the algorithmic resolution.\n\nGood properties of the exponential families (existence of non-trivial minimal sufficient statistics) are preserved, and an analysis of the new critical points/information plane induced is carried out.\n\nThe paper is globally well written and clear, and the maths are rigorously introduced and treated. Two minor comments would concern the definition of the Mutual Information (MI), which could be recalled to help the unfamiliar reader and improve self-containedness, and the notation of the expected value (\\langle \\rangle_p), unusual in Machine Learning where \\mathbb{E} is often preferred.\n\nAnother point that could be enhanced is the intuition behind the IB and dualIB criteria: a small comment on their meaning/relevance as well as the rationale/implication of the probabilities permutation would be valuable addition.\n\nExhibiting a link with variational autoencoders (VAEs), and expliciting the differences in a VAE framework between the two criteria could also represent an interesting parallel, especially for machine learning oriented readers.\n\nOne of the main drawback of the present paper is however the lack of convincing experiments. Graphs showing the good behavior of the introduced framework are ok, but the clear interest of using dualIB rather than IB could be emphasized more. In particular, it does not seem that the issues about IB raised in the abstract (curse of dimensionality, no closed form solution) are solved with dualIB. Furthermore, dualIB optimizes the MI of the predicted labels, which is claimed to be beneficial contrary to the MI on the actual labels. However, no empirical demonstration of this superiority is produced, which is a bit disappointing.", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}}}