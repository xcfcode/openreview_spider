{"paper": {"title": "Continual Learning via Explicit Structure Learning", "authors": ["Xilai Li", "Yingbo Zhou", "Tianfu Wu", "Richard Socher", "Caiming Xiong"], "authorids": ["xli47@ncsu.edu", "yingbo.zhou@salesforce.com", "tianfu_wu@ncsu.edu", "rsocher@salesforce.com", "cxiong@salesforce.com"], "summary": "", "abstract": "Despite recent advances in deep learning, neural networks suffer catastrophic forgetting when tasks are learned sequentially. We propose a conceptually simple and general framework for continual learning, where structure optimization is considered explicitly during learning. We implement this idea by separating the structure and parameter learning. During structure learning, the model optimizes for the best structure for the current task. The model learns when to reuse or modify structure from previous tasks, or create new ones when necessary. The model parameters are then estimated with the optimal structure. Empirically, we found that our approach leads to sensible structures when learning multiple tasks continuously. Additionally, catastrophic forgetting is also largely alleviated from explicit learning of structures. Our method also outperforms all other baselines on the permuted MNIST and split CIFAR datasets in continual learning setting.", "keywords": ["continuous learning", "catastrophic forgetting", "architecture learning"]}, "meta": {"decision": "Reject", "comment": "The paper presents a promising approach for continual learning with no access to data from the previous tasks. For learning the current task, the authors propose to find an optimal structure of the neural network model first (select either to reuse, adapt previously learned layers or to train new layers) and then to learn its parameters. \n\nWhile acknowledging the originality of the method and the importance of the problem that it tries to address, all reviewers and AC agreed that they would like to see more intensive empirical evaluations and comparisons to state-of-the-art models for continual learning using more datasets and in-depth analysis of the results \u2013 see details comments of all reviewers before and after rebuttal. \nThe authors have tried to address some of these concerns during rebuttal, but an in-depth analysis of the results (evaluation in terms on accuracy, efficiency, memory demand) using different datasets still remains a critical issue.\n\nTwo other requests to further strengthen the manuscript:\n1) an ablation study on the three choices for structural learning (R3), and especially the importance of \u2018adaptation\u2019 (R3 and R1)\nThe authors have tried to address this verbally in their responses but a proper ablation study would be desirable to strengthen the evaluation.\n2) Readability and proofreading of the manuscript is still unsatisfying after revision.\n"}, "review": {"Hygcmic3AX": {"type": "rebuttal", "replyto": "BkekeLd52X", "comment": "Are they fair comparisons (evaluation only in terms of accuracy)? Different methods expand the network different amount. Hence, they should be compared on this metric too.\n\nAs mentioned in the paper, we make sure that all methods use similar amount of parameters. In particular, we make sure that all other methods at least match the number of parameters for our final model (after 10 tasks). In other words, all compared methods has same or more capacity as compared to our model, and we believe this comparison represents a fair comparison.\n\nWe agree that the expansion amount is also an important metric, and we will this metric in the final version.", "title": "Response after update"}, "r1e_1DrcAX": {"type": "rebuttal", "replyto": "ByxWsETU07", "comment": "We have added variational continual learning result. The result was not added in the first version because running the VCL with more parameters uses a lot of memory, and thus can only run on CPU, which is a bit slow.\n\nWe tried deep generative replay, however we are not able to get reasonable results on permuted MNIST with 10 permutations. We tried various hyper-parameter settings, and performance was reasonable when the number of tasks is within five (average performance at around 96%). When number of tasks go beyond five, performance drops on previous tasks is quite significant, some tasks dropped to ~60%\n\nWe have added suggested references in related work.`", "title": "Response"}, "HygXZSH50m": {"type": "rebuttal", "replyto": "rygRL-QRTX", "comment": "We have added one more experiment on split CIFAR-100. As reviewers suggested, MNIST dataset may not be a strong evaluation set, and therefore we added CIFAR-100 experiments since it represent a more realistic settings.", "title": "2nd paper revision"}, "BkekeLd52X": {"type": "review", "replyto": "ryxsS3A5Km", "review": "The paper considers the problem of sequential learning where data access for the previous tasks is completely prohibited. Authors propose a conceptually simple framework to learn structures (it is the selection of reusing, adapting previously learned layers or training new layers) as well as corresponding parameters in the sequential learning.\n\nThe paper is potentially interesting and providing possibly important framework for life-long learning. It is well written in most of cases and easy to follow (however I got the impression that the paper was rushed in the last minute; there are some trivial typos and very low resolution images etc.)\n\nHowever, I have a huge concern about the empirical evaluations.  This area is really huge and has attracted lots of interest from many researchers, meaning that we lots of methods to compare. Nevertheless, authors only focus on providing insights on effects of different components of the propose model. This is also critical but comparing against state-of-the-arts is also very important. Especially, comparing against Lee et al 2017 seems essential. I can see the difference against that paper from the authors' argument in the related work, but that is the difference not comparison. It would be great to compare the performances as well as the number of increased memory sizes as the number of task increases.\n\nMoreover, the details should be provided; for instance provide the explicit form of R(s). \n\n---------------------------------------------\n\nThanks for the update. But are they fair comparisons (evaluation only in terms of accuracy)? Different methods expand the network different amount. Hence, they should be compared on this metric too.", "title": "Review of \"Continual Learning via Explicit Structure Learning\"", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJxiRZ7A6Q": {"type": "rebuttal", "replyto": "HkeheEjO3X", "comment": "Thanks for your feedback. We have added additional results comparing our method with other more recent and relevant methods. Please refer to the updated manuscript.\n\nRegarding the questions:\n- What's the intuition behind implementing the \u201cadapt\u201d operator as additive bias over the previous weights, rather than just copying the previous weights and fine tuning?\n\nThe role of adaptor is to strike a balance between number of parameters and performance. As mentioned in the end of section 3.1, we have different cost for select each option. Adaptor provides a way of using and modifying previous representation without incurring any forgetting by adding a relatively small amount of parameter overhead.\n\n- In the general case, if the architecture search is a continuous relaxation (softmax combination of operators), why is the \"adapt\" operator necessary? Wouldn't this already be a linear combination of new and old parameters? (In the example case of a 1\u00d71 adaptor it makes sense, but this is a special restricted case which adapts with a smaller set of parameters)\n\nIn the adaptor case, when searching the combination of the old parameters with 1x1 conv forms an option. For example, in case we have two options, reuse and adaptor, the softweight is over the original parameter and the original parameter plus adaptor combined, so here the second part is treated as one option. To some extend what you are suggesting is true, however, this does not exactly corresponds to what is happening (as we explained above).\n\n- How is the structure regulariser backpropagated into the parameters of each layer? As I understand, it is composed of a constant discrete term z (number of parameters in each option), multiplied by architecture softmaxes alpha; the gradient with respect to each alpha is a constant, and so this has the effect of scaling the gradients of each operator.\n\nIn our implementation, the structure regularizer does not backprop to the parameters of each layer. Instead, the regularizer serves as a penalty for different choices, and thus has effect on the magnitude of alphas. Since alpha controls the weight for different options, this would influence the choice of different options during structure learning.\n\n- For the \"reuse - tuned\" case, isn\u2019t the model effectively maintaining a new network for each task?\n\nNo. When the model is reused, the parameters are tuned, and the tuned parameter is used both for current tasks that it is finetuned on as well as all previous tasks.\n", "title": "Response"}, "Skx9o-X0TQ": {"type": "rebuttal", "replyto": "B1e--EHYnQ", "comment": "Thanks for your feedback. We have added additional results comparing our method with other more recent and relevant methods. Please refer to the updated manuscript.\n\nRegarding the questions:\n-\tIn the equation (4), I wonder that, in the model, the hyperparameter(lambda_i or beta_i) of regularizer looks different according to the task, is it correct?\n\nYes they can be different for each tasks, this is more of a design choice. However, in our implementation and experiments, to make things easier, we just used the same hyperparameter for all tasks.\n\n-\tAs shown in the Fig. 2) three choice-reuse, adaptation, and, new, is decided in the layer level. But with a semantic intuition, such that two different task can share specific features and simultaneously each of them requires the different neural space to learn discriminative ones at layer l, it seems better if the model could search structure much flexible. Is there some of experimental trial or plan about these kind of joint-adoption?\n\nThis is a very good point. Ideally we would like to be able to do more finer grained search, and that is definitely desired. In practice, we could only make the search space more restricted so that the search can be done in a more efficient manner. Of course one is not restricted to use only the options that we provided in our implementation. More finer grained and search is definitely possible, for example, learning to share at filter/neuron level instead of layer level. This is more of a balance between training efficiency and final performance. The current implementation highlights the importance of taking structure into account. However, one should not limit themselves with only the options that we demonstrated. As long as the search space is reasonably sized and operations are plausible, it could be incorporated in our framework. This leads to interesting future work directions.\n\n-\tWhat is the main contribution of adaptation? I wonder that only reuse and new can work well including the role of adaptation, or not.\n\nThe role of adaptation is to strike a balance between number of parameters and performance. As mentioned in the end of section 3.1, we have different cost for select each option. Adaptor provides a way of using and modifying previous representation without incurring any forgetting by adding a relatively small amount of parameter overhead.\n", "title": "Response"}, "ryxJKZm067": {"type": "rebuttal", "replyto": "BkekeLd52X", "comment": "Thanks for your feedback. We have added additional results comparing our method with other more recent and relevant methods. Please refer to the updated manuscript.\n", "title": "Response"}, "rygRL-QRTX": {"type": "rebuttal", "replyto": "ryxsS3A5Km", "comment": "We thank all reviewers for providing constructive feedback that further improves the paper. We have highlighted the changes in text by using blue color. Minor editing changes are not marked. In this update revision we did following changes.\n\n1) We added more analysis on forgetting, which we think provides more insights into the method. In addition to use simple L-2 based regularization we finetuned our model without using any regularization, and we still obtained interesting result where the forgetting is minimal. This further suggests the importance of structure learning when learning continual tasks.\n\n2) As all reviewers suggested, we added more comparisons to more recent, existing methods. In particular, we compared ours with the more recent methods such as  dynamically expandable network, incremental moment matching, progressive network, hard attention to task, etc on permuted MNIST dataset. We show that our method is performs competitive or better as compared to all these method.\n\n3) Provided more details in appendix\n\n4) Corrected editorial errors as pointed out by reviewers.\n\nDue to the time limit, we only completed experiments on permuted MNIST. Additionally we are also running experiment on split MNIST so that we have more comparisons, and we will update another version with those results before the deadline.\n", "title": "Paper revision summary"}, "HkeheEjO3X": {"type": "review", "replyto": "ryxsS3A5Km", "review": "The proposed approach aims to mitigate catastrophic forgetting in continual learning (CL) problems by structure learning: determining whether to reuse or adapt existing parameters, or initialise new ones, when faced with a new task. This is framed as an architecture search problem, applying ideas from Differentiable Architecture Search (DARTS). The approach is verified on the Permuted MNIST dataset and evaluated on the Visual Decathlon, showing an improvement.\n\nI think this is an interesting idea with potential, and is worth exploring, and the paper is well-structured and easy to follow.\n\nUnfortunately, I feel the paper fails to consider recent work on CL, both in terms of discussion and benchmarking. The only previous work that is compared is EWC, on permuted MNIST, and the Visual Decathlon performance is only compared to simple baselines (such as adding an adapter or fine tuning) which makes it difficult to gauge the contribution.\nThere are recent works, some with better results on more difficult problems, such as Variational Continual Learning [1], Progress and Compress [2], or (Variational) Generative Experience Replay [3][4].\nGiven the approach is based on dynamically adding parameters or modules, Progressive Networks and Dynamically Expandable Networks (both cited) are especially relevant and should be compared (I believe the former may be related to the \u201cadapter\u201d baseline, but this should be made explicit).\n\nI have some questions / discussion points:\n- What's the intuition behind implementing the \u201cadapt\u201d operator as additive bias over the previous weights, rather than just copying the previous weights and fine tuning?\n- In the general case, if the architecture search is a continuous relaxation (softmax combination of operators), why is the \"adapt\" operator necessary? Wouldn't this already be a linear combination of new and old parameters? (In the example case of a 1\u00d71 adaptor it makes sense, but this is a special restricted case which adapts with a smaller set of parameters)\n- How is the structure regulariser backpropagated into the parameters of each layer? As I understand, it is composed of a constant discrete term z (number of parameters in each option), multiplied by architecture softmaxes alpha; the gradient with respect to each alpha is a constant, and so this has the effect of scaling the gradients of each operator.\n- For the \"reuse - tuned\" case, isn\u2019t the model effectively maintaining a new network for each task?\n\nI also have a number of other comments:\n- Reference to figure in page 6 should be figure 4, not 5.\n- I think the readability of the paper would benefit from another few proofreads; there are a number of grammatical issues throughout, and several sentence fragments, eg. in the top para of page 2: \u201c..., it has the potential to encourage information sharing. Since now the irrelevant part can be handled\u2026\u201d.\n\nI would encourage the authors to strengthen the experimental comparison by incorporating stronger, external baselines, and improving some of the minor writing issues.\n\n[1] Nguyen, Cuong V., et al. \"Variational Continual Learning.\" ICLR, 2018.\n[2] Schwarz, Jonathan, et al. \"Progress & Compress: A scalable framework for continual learning.\" ICML, 2018.\n[3] Shin, Hanul, et al. \"Continual learning with deep generative replay.\" NIPS, 2017.\n[4] Farquhar, Sebastian, and Yarin Gal. \"Towards Robust Evaluations of Continual Learning.\" arXiv, 2018.", "title": "Interesting idea, but needs a stronger experimental justification", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1e--EHYnQ": {"type": "review", "replyto": "ryxsS3A5Km", "review": "\nThis paper proposes a new approach to mitigate the catastrophic forgetting for continual learning. The model is composed to the neural architecture search and parameter learning based on the intuition that largely different tasks should allow to use different network structure to train them. In structure learning, they introduce three candidate to decide network architecture, reuse, adaptation and new. In the experiments, they show that their model outperforms SGD and EWC.\n\nBasically, the intuition of structure learning and the validation of that is straight forward and easy to follow. However, I\u2019m not sure that the proposed model can outperform the recent continual learning methods, such as IMM(Lee et al, 2017), DEN or  RCL(Ju Xu et al, 2018). There is only a relatively weak(and old) comparison with l2, and EWC.\n\n-\tIn the equation (4), I wonder that, in the model, the hyperparameter(lambda_i or beta_i) of regularizer looks different according to the task, is it correct?\n-\tAs shown in the Fig. 2) three choice-reuse, adaptation, and, new, is decided in the layer level. But with a semantic intuition, such that two different task can share specific features and simultaneously each of them requires the different neural space to learn discriminative ones at layer l, it seems better if the model could search structure much flexible. Is there some of experimental trial or plan about these kind of joint-adoption?\n-\tWhat is the main contribution of adaptation? I wonder that only reuse and new can work well including the role of adaptation, or not.\n-\tIs there any experiments to compare the recent continual learning methods(as I mentioned), in terms of AUC(or accuracy) and the network capacity?\n\nMinor remarks,\nPage 3: \t\u201cis been\u201d -> is\n\t\u201cunlikely\u201d-> unlike\nPage 4: \t\u201csharealbe\u201d -> shareable\nPage 5: \t\u201c, After\u201d -> , after\n\t\u201cpermuated\u201d -> permuted\nPage 6:\t\u201cFig. 5\u201d -> Fig. 4\n\n", "title": "review", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}