{"paper": {"title": "Neural Architecture Search with Reinforcement Learning", "authors": ["Barret Zoph", "Quoc Le"], "authorids": ["barretzoph@google.com", "qvl@google.com"], "summary": "", "abstract": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines.  Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214.", "keywords": []}, "meta": {"decision": "Accept (Oral)", "comment": "This was one of the most highly rated papers submitted to the conference. The reviewers all enjoyed the idea and found the experiments rigorous, interesting and compelling. Especially interesting was the empirical result that the model found architectures that outperform those that are used extensively in the literature (i.e. LSTMs)."}, "review": {"BkfxJnkQx": {"type": "review", "replyto": "r1Ue8Hcxg", "review": "This paper is intriguing.\n\nFor a question more out of curiousity than anything else, why did you use CPUs for training the language modeling task rather than GPUs? Just an excess number of CPUs at your disposal?\n\nRegarding the recurrent cell search, you force some of the cells, such as c_t and c_t-1 / h_t and h_t-1. Did you think of or experiment with allowing the model to add these itself?\n\nThis may be my lack of knowledge, but what's the reasoning for some of the reward choices? Last five epochs _cubed_ or c / validation_ppl**2? Obviously these are all generally ad hoc in RL but I'd just not seen these as standard choices in the past.\n\nWhat was the BPTT length for the word level language modeling? Was it 35 as in Zaremba et al.?\n\nFinally, you tested generalization on the character level Penn Treebank task, though that is quite small and odd in some ways (being a letter for letter version of the word level PTB, it also has a vocabulary of 10,000 words, no numbers / punctuation, and <unk> being a character by character \"word\"). Testing the generalization of the architecture to essentially the same dataset but at a character level doesn't seem an extreme generalization. Were you interested in running experiments over larger datasets such as text8 (character level), One Billion LM (word level, sentential context), or WikiText-103 (word level with context)?\n\nOverall I'm excited for the potential that policy gradient based architecture search may bring!This paper explores an important part of our field, that of automating architecture search. While the technique is currently computationally intensive, this trade-off will likely become better in the near future as technology continues to improve.\n\nThe paper covers both standard vision and text tasks and tackle many benchmark datasets, showing there are gains to be made by exploring beyond the standard RNN and CNN search space. While one would always want to see the technique applied to more datasets, this is already far more sufficient to show the technique is not only competitive with human architectural intuition but may even surpass it. This also suggests an approach to tailor the architecture to specific datasets without resulting in hand engineering at each stage.\n\nThis is a well written paper on an interesting topic with strong results. I recommend it be accepted.", "title": "Why CPUs, reward choices, generalization, BPTT length, dataset choice", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HksjMBGNe": {"type": "review", "replyto": "r1Ue8Hcxg", "review": "This paper is intriguing.\n\nFor a question more out of curiousity than anything else, why did you use CPUs for training the language modeling task rather than GPUs? Just an excess number of CPUs at your disposal?\n\nRegarding the recurrent cell search, you force some of the cells, such as c_t and c_t-1 / h_t and h_t-1. Did you think of or experiment with allowing the model to add these itself?\n\nThis may be my lack of knowledge, but what's the reasoning for some of the reward choices? Last five epochs _cubed_ or c / validation_ppl**2? Obviously these are all generally ad hoc in RL but I'd just not seen these as standard choices in the past.\n\nWhat was the BPTT length for the word level language modeling? Was it 35 as in Zaremba et al.?\n\nFinally, you tested generalization on the character level Penn Treebank task, though that is quite small and odd in some ways (being a letter for letter version of the word level PTB, it also has a vocabulary of 10,000 words, no numbers / punctuation, and <unk> being a character by character \"word\"). Testing the generalization of the architecture to essentially the same dataset but at a character level doesn't seem an extreme generalization. Were you interested in running experiments over larger datasets such as text8 (character level), One Billion LM (word level, sentential context), or WikiText-103 (word level with context)?\n\nOverall I'm excited for the potential that policy gradient based architecture search may bring!This paper explores an important part of our field, that of automating architecture search. While the technique is currently computationally intensive, this trade-off will likely become better in the near future as technology continues to improve.\n\nThe paper covers both standard vision and text tasks and tackle many benchmark datasets, showing there are gains to be made by exploring beyond the standard RNN and CNN search space. While one would always want to see the technique applied to more datasets, this is already far more sufficient to show the technique is not only competitive with human architectural intuition but may even surpass it. This also suggests an approach to tailor the architecture to specific datasets without resulting in hand engineering at each stage.\n\nThis is a well written paper on an interesting topic with strong results. I recommend it be accepted.", "title": "Why CPUs, reward choices, generalization, BPTT length, dataset choice", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "By96T49y-": {"type": "rebuttal", "replyto": "rJ8xa16Ax", "comment": "We have fixed this in our updated version.", "title": "Response"}, "rJ8xa16Ax": {"type": "rebuttal", "replyto": "r1Ue8Hcxg", "comment": "The abstract says: \"Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than\nthe previous state-of-the-art model\" while the last paragraph of section one has: \"Our CIFAR-10 model achieves a 3.84 test set error, while being 1.2x faster than the current best model.\". Should these be the same numbers or what is the reason for the difference (or is there a newer revision of the paper) ?", "title": "Inconsistent performance numbers between abstract and section 1 for CIFAR-10 network ? "}, "By45Fnn0x": {"type": "rebuttal", "replyto": "S1SGw63Mg", "comment": "Can you also give details on how many of these GPUs/CPUs were used (e.g. was it a single dual processor / dual GPU machine or rather a cluster) -- I think this is interesting information for readers to get an idea how feasible the task is with the computational resources they have access to.", "title": "Request for details on running time and number of machines"}, "H1qazwYOg": {"type": "rebuttal", "replyto": "r1Ue8Hcxg", "comment": "In the paper, you wrote that the controller is auto-regressive and that \"each prediction is fed into the next time step as input\". What is the input to the controller for the prediction of the first token - a random seed or constant?\n\nAdditionally, is the controller an extension of the sequence decoder used in sequence-to-sequence learning? What are the important changes between the NAS controller and the S2S decoder?\n\nThanks for the excellent paper and congratulations on the acceptance!", "title": "Auto-regressive model"}, "B1URCqgOx": {"type": "rebuttal", "replyto": "r1Ue8Hcxg", "comment": "Thanks for a very interesting paper!\n\nIt is nice to see that the authors will make the code for running the models found by their controller available.\nI am wondering whether the code for the design space will also be available. I.e., the spaces from which the policy samples, for CIFAR and Penn Treebank -- e.g. as code with free choices. That would make it much more likely that someone else can reproduce these results. Thanks!\n", "title": "Code available for the design space (not just the final models)?"}, "rJS4B5FLl": {"type": "rebuttal", "replyto": "HJoC77Tre", "comment": "Yes this is what we do.", "title": "Response"}, "rJvkBcKUg": {"type": "rebuttal", "replyto": "rJ0FDEw4x", "comment": "Thank you for pointing out these two related papers. We will include a comparison in our related work.", "title": "Response"}, "Hk9jVqYIx": {"type": "rebuttal", "replyto": "BJlICsTXe", "comment": "Thank you for mentioning this related work to us. We will include a comparison in the related work section of the paper.", "title": "Response"}, "rJJPimaSx": {"type": "rebuttal", "replyto": "r1Ue8Hcxg", "comment": "Thank you for the really interesting paper. I have a few questions about the control experiments.\n\nIn control experiment 1, how well did the model with the max/sin functions do in terms of perplexity? That seems perhaps more important than how similar the architecture itself is.\n\nWhat was the distribution over architectures used by random search in control experiment 2? Did you also do grid finetuning of hyperparameters with random search?", "title": "Control experiments"}, "HJoC77Tre": {"type": "rebuttal", "replyto": "r1Ue8Hcxg", "comment": "Dear Authors,\n\nThank you for sharing such an impressive work. I just have a question, in your experimental part, \"We then run a small grid search over learning rate, weight decay,\nbatchnorm epsilon and what epoch to decay the learning rate.\" May I know that in such a grid search, is the validation accuracy also computed on your 5,000 validation samples from which the reward (used in REINFORCE) is computed?\n\nThanks a lot!", "title": "On validation dataset"}, "rJ0FDEw4x": {"type": "rebuttal", "replyto": "r1Ue8Hcxg", "comment": "Overall, I find it very exciting that the actor-critic framework can produce such good results. \n\nThere is some related work in Bayesian optimization for architecture search that the authors may not know about. The authors are correct in stating that standard Bayesian optimization methods are limited to a fixed-length space, but there are also some Bayesian optimization methods that support so-called conditional parameters that allow them to effectively search over variable-depth architectures. Two particularly relevant papers are: \n\n1. http://jmlr.org/proceedings/papers/v28/bergstra13.pdf\nBergstra, Yamins, and Cox: Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures (ICML 2013). This allowed for different numbers of layers and used the Bayesian optimization algorithm TPE to search for the best model in a space of up to 238 hyperparameters.\n\n2. http://www.jmlr.org/proceedings/papers/v64/mendoza_towards_2016.pdf\nMendoza, Klein, Feurer, Springenberg, and Hutter: Towards Automatically-Tuned Neural Networks (AutoML@ICML 2016). \nThis did joint architecture search and hyperparameter optimization in the special case of feed-forward neural networks using the Bayesian optimization algorithm SMAC and generated custom per-dataset architectures that won two datasets in the AutoML competition against human experts.\n\nThe authors' approach has clear advantages over these methods (I believe especially in terms of generality) but also seems to have some disadvantages (I believe especially in terms of data efficiency). It would be nice if the authors could briefly contrast these approaches in their paper. Thanks!", "title": "Some relevant related work"}, "H1v5kck4g": {"type": "rebuttal", "replyto": "r1T0ms0mg", "comment": "Hello thank you for the nice review. We tried to address the issue of testing the generality of our architectures on different datasets. Our most recent revision contains more experiments where we ran the cell our neural architecture search algorithm found on a very benchmarked machine translation task, in addition to new experiments on character level language modeling.", "title": "Response"}, "BJlICsTXe": {"type": "rebuttal", "replyto": "r1Ue8Hcxg", "comment": "They obtain a performance which is worse that reported in your work, however they train their model on a single GPU in contrast to 800 used in the proposed work.\n\n", "title": "How do you position yourself to: https://arxiv.org/abs/1606.02492 \"Convolutional Neural Fabrics\""}, "Bk2GuyFQg": {"type": "rebuttal", "replyto": "SJ-SVi8Xx", "comment": "1. You are correct that 20 shards are probably more than we needed since the model is small, this choice was somewhat arbitrary.\n\n\n2. We used a base of 8 to be sure the search space was interesting enough for our search algorithm. A LSTM actually has a base of 4, but we believe our comparisons to be fair because the cells all have the same number of parameters and virtually the same amount of computation being done.", "title": "Response"}, "rJkCwyY7x": {"type": "rebuttal", "replyto": "B1qPmqBXl", "comment": "The wording for that section was confusing and we revised the paper to make this more clear. What we meant was that we trained a total of 12,800 different architectures.", "title": "Response"}, "ryj9wJYQg": {"type": "rebuttal", "replyto": "BkfxJnkQx", "comment": "\"For a question more out of curiousity than anything else, why did you use CPUs for training the language modeling task rather than GPUs? Just an excess number of CPUs at your disposal?\"\nYes we used CPUs because they were more at our disposal and the models were small enough that the speed degradation was minimal from switching from a GPU to CPU.\n\n\"Regarding the recurrent cell search, you force some of the cells, such as c_t and c_t-1 / h_t and h_t-1. Did you think of or experiment with allowing the model to add these itself?\"\nYes, we have thought and designed a model to learn these, but we did not experiment with it yet.\n\n\"This may be my lack of knowledge, but what's the reasoning for some of the reward choices? Last five epochs _cubed_ or c / validation_ppl**2? Obviously these are all generally ad hoc in RL but I'd just not seen these as standard choices in the past.\"\nYes these choices are just ways to try and get the reward for models to lie in the 0-1 range.\n\n\"What was the BPTT length for the word level language modeling? Was it 35 as in Zaremba et al.?\"\nIt was 35, the same in Zaremba et al.\n\n\"Finally, you tested generalization on the character level Penn Treebank task, though that is quite small and odd in some ways (being a letter for letter version of the word level PTB, it also has a vocabulary of 10,000 words, no numbers / punctuation, and <unk> being a character by character \"word\"). Testing the generalization of the architecture to essentially the same dataset but at a character level doesn't seem an extreme generalization. Were you interested in running experiments over larger datasets such as text8 (character level), One Billion LM (word level, sentential context), or WikiText-103 (word level with context)?\"\nYes we are interested in trying this out in other datasets. We currently have experiments going on text8 and WMT translation tasks.\n\n\n", "title": "Response"}, "HkPR8Jtme": {"type": "rebuttal", "replyto": "SkcgSiJmx", "comment": "1. We believe this method to be very generalizable and only choose these two tasks because we believed they were sufficiently different and challenging. They are also two of the most benchmarked datasets in deep learning. Though it can be extended to, our method may struggle for problems where model types are not defined yet (e.g., graphs, multimodal data).\n\n\n2. Yes we believe this would definitely help performance, but decided against doing this in this paper to try and show we could start with less and still achieve good results.\n", "title": "Response"}, "SJ-SVi8Xx": {"type": "review", "replyto": "r1Ue8Hcxg", "review": "1. Why does the distributed systems use S shards? It seems that the model does not have a huge number of parameters.\n\n2. In section 4.2, why does the model use base number of 8, not 2? It seems that it is more fair to compare with LSTM with base number 2.\nThis paper proposed to use RL and RNN to design the architecture of networks for specific tasks. The idea of the paper is quite promising and the experimental results on two datasets show that method is solid.\nThe pros of the paper are:\n1. The idea of using RNN to produce the description of the network and using RL to train the RNN is interesting and promising.\n2. The generated architecture looks similar to what human designed, which shows that the human expertise and the generated network architectures are compatible.\n\nThe cons of the paper are:\n1. The training time of the network is long, even with a lot of computing resources. \n2. The experiments did not provide the generality of the generated architectures. It would be nice to see the performances of the generated architecture on other similar but different datasets, especially the generated sequential models.\n\nOverall, I believe this is a nice paper. But it need more experiments to show its potential advantage over the human designed models.", "title": "several questions", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "r1T0ms0mg": {"type": "review", "replyto": "r1Ue8Hcxg", "review": "1. Why does the distributed systems use S shards? It seems that the model does not have a huge number of parameters.\n\n2. In section 4.2, why does the model use base number of 8, not 2? It seems that it is more fair to compare with LSTM with base number 2.\nThis paper proposed to use RL and RNN to design the architecture of networks for specific tasks. The idea of the paper is quite promising and the experimental results on two datasets show that method is solid.\nThe pros of the paper are:\n1. The idea of using RNN to produce the description of the network and using RL to train the RNN is interesting and promising.\n2. The generated architecture looks similar to what human designed, which shows that the human expertise and the generated network architectures are compatible.\n\nThe cons of the paper are:\n1. The training time of the network is long, even with a lot of computing resources. \n2. The experiments did not provide the generality of the generated architectures. It would be nice to see the performances of the generated architecture on other similar but different datasets, especially the generated sequential models.\n\nOverall, I believe this is a nice paper. But it need more experiments to show its potential advantage over the human designed models.", "title": "several questions", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "B1qPmqBXl": {"type": "rebuttal", "replyto": "r1Ue8Hcxg", "comment": "This is very interesting work. \nI am wondering how many networks you trained in total. For CIFAR-10, you say you trained the controller for 12,800 iterations. Since every controller used m=8 child replicas in every iteration, does this mean you trained 12.800 * 8 = 102.400 networks in total for CIFAR-10? Thanks!", "title": "How many networks were trained in total? 102.400 for CIFAR-10?"}, "SkcgSiJmx": {"type": "review", "replyto": "r1Ue8Hcxg", "review": "Quite interesting paper!\n\n1. Can you please comment on how generalizable the method is? Is there anything inherent to the two selected problems that make it work? What types of problems would be difficult for this method?\n2. The method is clearly computationally very intensive. Would it help to bootstrap it with an initial human-architected DNN? This paper presents search for optimal neural-net architectures based on actor-critic framework. The method treats DNN as a variable length sequence, and uses RL to find the target architecture, which acts as an actor. The node selection is an action in the RL context, and evaluation error of the outcome architecture corresponds to reward. A auto-regressive two-layer LSTM is used as a controller and critic. The method is evaluated on two different problems, and each compared with number of other human-created architectures.\n\nThis is very exciting paper! Hand selecting architectures is difficult, and it is hard to know how far from optimal results the hand-designed networks are. The presented method is  novel. The authors do an excellent job of describing it in detail, with all the improvements that needed to be done. The tested data represents well the capability of the method. It is very interesting to see the differences between the generated architectures and human generated ones. The paper is written very clearly, and is very accessible. The coverage and contrast with the related literature is done well.\n\nIt would be interesting to see the data about the time needed for training, and correlation between time/resources needed to train and the quality of the model. It would also be interesting to see how human bootstrapped models perform and involve.\n\nOverall, an excellent and interesting paper.", "title": "Generalization", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rkeMJc-Ee": {"type": "review", "replyto": "r1Ue8Hcxg", "review": "Quite interesting paper!\n\n1. Can you please comment on how generalizable the method is? Is there anything inherent to the two selected problems that make it work? What types of problems would be difficult for this method?\n2. The method is clearly computationally very intensive. Would it help to bootstrap it with an initial human-architected DNN? This paper presents search for optimal neural-net architectures based on actor-critic framework. The method treats DNN as a variable length sequence, and uses RL to find the target architecture, which acts as an actor. The node selection is an action in the RL context, and evaluation error of the outcome architecture corresponds to reward. A auto-regressive two-layer LSTM is used as a controller and critic. The method is evaluated on two different problems, and each compared with number of other human-created architectures.\n\nThis is very exciting paper! Hand selecting architectures is difficult, and it is hard to know how far from optimal results the hand-designed networks are. The presented method is  novel. The authors do an excellent job of describing it in detail, with all the improvements that needed to be done. The tested data represents well the capability of the method. It is very interesting to see the differences between the generated architectures and human generated ones. The paper is written very clearly, and is very accessible. The coverage and contrast with the related literature is done well.\n\nIt would be interesting to see the data about the time needed for training, and correlation between time/resources needed to train and the quality of the model. It would also be interesting to see how human bootstrapped models perform and involve.\n\nOverall, an excellent and interesting paper.", "title": "Generalization", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1SGw63Mg": {"type": "rebuttal", "replyto": "SJMOWnjMx", "comment": "For training the Cifar10 models we used K80 GPUs and for training the Penn TreeBank models we used Intel Haswell CPUs. The Cifar jobs took 3 - 4 weeks and the Penn TreeBank jobs took 2 - 3 weeks.", "title": "Request for details on running time and number of machines"}, "SJMOWnjMx": {"type": "rebuttal", "replyto": "r1Ue8Hcxg", "comment": "This is a very interesting paper! Can you please provide some information on the amount of time that was required in order to train your model and the hardware that you used?", "title": "Request for details on running time and number of machines"}}}