{"paper": {"title": "DecayNet: A Study on the Cell States of Long Short Term Memories", "authors": ["Nicholas I.H. Kuo", "Mehrtash T. Harandi", "Hanna Suominen", "Nicolas Fourrier", "Christian Walder", "Gabriela Ferraro"], "authorids": ["u6424547@anu.edu.au", "mehrtash.harandi@monash.edu", "hanna.suominen@anu.edu.au", "nicolas.fourrier@devinci.fr", "christian.walder@data61.csiro.au", "gabriela.ferraro@csiro.au"], "summary": "We present a LSTM reformulation with a monotonically decreasing forget gate to increase LSTM interpretability and modelling power without introducing new learnable parameters.", "abstract": "It is unclear whether the extensively applied long-short term memory (LSTM) is an optimised architecture for recurrent neural networks. Its complicated design makes the network hard to analyse and non-immediately clear for its utilities in real-world data. This paper studies LSTMs as systems of difference equations, and takes a theoretical mathematical approach to study consecutive transitions in network variables. Our study shows that the cell state propagation is predominantly controlled by the forget gate. Hence, we introduce DecayNets, LSTMs with monotonically decreasing forget gates, to calibrate cell state dynamics. With recurrent batch normalisation, DecayNet outperforms the previous state of the art for permuted sequential MNIST. The Decay mechanism is also beneficial for LSTM-based optimisers, and decrease optimisee neural network losses more rapidly.\n\nEdit status: Revised paper.", "keywords": ["Long short term memory", "Recurrent neural network", "Dynamical systems", "Difference equation"]}, "meta": {"decision": "Reject", "comment": "there is a disagreement among the reviewers, and i am siding with the two reviewers (r1 and r3) and agree with r3 that it is rather unconventional to pick learning-to-learn to experiment with modelling variable-length sequences (it's not like there's no other task that has this characteristics, e.g., language modelling, translation, ...) "}, "review": {"SklBChmu0m": {"type": "rebuttal", "replyto": "SyeE1XoaiX", "comment": "R1C03:\n\"all the values of forget gates and input gates in Figure 2 are manually set as *conceptual observations*\"\n\nAll parametric values are justifiable and are selected with specific purposes. In addition, the studied behaviours are not rare / uncommon scenarios, nor are they representatives of an unsuccessful training. They were, however, sadly, not well studied.\n\nIn Subsection 3.3.1, we stated that the first row of Figure 2 follows experimental results shown in Appendix B. In this appendix, we present Figure 6, time series of arbitrary forget gate dimensions. The presented forget gates belong to a LSTM trained to perform binary string addition. The purpose is to show that, for the simplest tasks, LSTMs possess forget gates with values that oscillate in a fashion that is hard to interpret.\n\nFigure 6 illustrates 16-instance-long chaotic time series in arbitrary dimensions of the forget gate. Repeatedly, chaos in time series takes the form of sudden shiftings, of either hills or valleys, from low to high values, and vice versa. Hence, we extract the repeating chaotic nature with the settings as follows. The first two frames of the first row of Figure 2 address changes in cell state dynamics where forget gates, with values selected to, increase sharply from small to large (equivalently, a hill in Figure 6). Then, the latter two frames depict the alternative cell state dynamics under forget gates which, with values chosen to, drop from large to small (equivalently, a valley in Figure 6). \n\nThe second and the third rows of Figure 2 serve to show the opposite scenarios of the first row - we fix the forget gates, as either extremely large values, or as extremely small values, throughout time. These settings serve to provide a stark contrast, for the fact, which is acknowledged by AnonReviewer3, for emphasising that \"When the forget is large, say 0.9, the input gate can be anywhere in the range [0.5, 1.0] and still produce growth in the cell value. For forget gate = 0.25, an even larger range of input gate values all produce a shrinking cell value.\"\n\nIn order to provide further clarification, we reworded Subsection 3.3 to include the statement of\n'We use Figure 2 to support these inferences. The first row simulates the near-random dynamics of a vanilla LSTM cell state. We present rows two and three to show stark contrasts, and advantages, of generating predictable cell state propagation with controllable forget gate values.'\n\nR1C04:\n\" the formula for the forget-polar input p\\_k, looks heavily hand-crafted\"\n\nAs mentioned in R1C01, all modification therein Section 4, including the forget-polar input, are motivated on the study presented in Subsection 3.2.2. The conceptual visualisation, which the forget-polar input is based on, is assisted by Figure 2. \n\nAs mentioned before in R1C01, the core idea is to construct a function that decays an equivalent portion in the forget gate for each iterative instance of the LSTM. This is to ensure each iterative instance is of equivalent importance to the network. In addition, the forget-polar input makes LSTM mechanics more interpretable. AnonReviewer3 commented on this with \"Each forget neuron will decrease at a different rate through the processing of a sequence, leading to sections of the cell state which will decay slowly and sections which will decay quickly.\"\n\nR1C05:\n\" both datasets are not designed in nature for sequential models like LSTMs\"\n\nBased on your comment, our revised paper now includes the meta-learning task of learning-to-learn (Andrychowicz et al.,2016). The new task cast LSTMs as optimisers to infer the updating rules of MLP-optimisees on MNIST classification. This is a naturally sequential task with an arbitrary length. Details are provided therein the General Comment.\n\n[Andrychowicz et al. (2016)]\nLearning to learn by gradient descent by gradient descent.\n", "title": "To AnonReviewer1 (Part 3) "}, "H1xuhhQOC7": {"type": "rebuttal", "replyto": "SyeE1XoaiX", "comment": "R1C02:\n\" Plots in Figure 2 only account for LSTMs' propagation within 3 steps\"\n\nR1C02 and R1C03 both concern Figure 2 - a collection of plots that present conceptual dynamical motions of the LSTM cell state. This figure consists nine subplots, in three rows and in three columns. Each row corresponds to a distinct time series of forget gate values, with behaviours invoked from, row one - uncontrolled forget gate values, row two - a large constant forget gate value, and row three - a small constant forget gate value, respectively. We visualise these through the 3 columns, the 3 time steps, from left to right, for the consecutive forward propagation of time t = 1, 2, 3. (Originally presented as k = 1, 2, 3; see R3C02 for the reason of changing k to t for denoting instances.)\n\nThough one can certainly continue the analysis for a longer time, we feel that three instances are sufficient to illustrate all major dynamics. The purpose of the first row is to address abrupt catches and sudden releases in vanilla LSTM cell states. An abrupt catch, shown in the transition from first row column one to first row column two, is resulted from a large decline in the gradient of the characteristic outline; whereas a sudden release, shown in the transition from first row column two to first row column three, is resulted from a large increase in the gradient of the characteristic outline. The purpose of the second and the third rows is to show that the forget gate act as the most influential component in the update of the cell state, and that dynamical alterations that are hard to interpret can be removed via controlled forget gate values. Subplots in row two show that cell states continue to grow with large constant forget gates; whereas those in row three show that cell states diminish to zero under small constant forget gates.\n", "title": "To AnonReviewer1 (Part 2) "}, "BkxB5nmO0m": {"type": "rebuttal", "replyto": "SyeE1XoaiX", "comment": "R1C01:\n\" many heuristic guesses in sec3\"\n\nPlease note that the core idea is to construct a function that decays an equivalent portion of forget gate values for each iterative instance of the LSTM. This specific design is chosen to ensure that all instantaneous decays are of equivalent importance. The resulting form (which may be perceived too heuristic) is merely a bounding mechanism for the sine function. Regarding the sine function, Subsection 3.2.2 provides motivations by analyzing the dynamics of the forget gate. \n\nThe novelty of our approach is to address dynamics of LSTMs. The other reviewers also showed support for the rationales behind the study. For example, as pointed out by AnonReviewer3, the particular set-up does not \"just randomly 'reset' the gates\" and is based on \"an analysis of the cell-state updating scheme of LSTM and realizes that it is mainly controlled by the forget gate\". AnonReviewer2 echoed this by stating that \"the forget gate seemingly having a stronger effect than the (input gate * input) component, and the authors propose to hard-wire the forget gate to produce a continuous and monotonic decrease\". We are saying all these to express that the whole design should not be seen as a heuristic crafting. While we have put immense effort in preparing our paper, if the reviewer feels clarifications on some parts can help getting our message across, we will gratefully follow. \n\nWith that said, we reworded Section 4 with a statement that reads\n'The forget gate is initialised as a vector of ones, with equivalent portions of the said initial ones set to be lost over the iterative instances of the LSTM.'\nin hope of clarification.\n", "title": "To AnonReviewer1 (Part 1)"}, "ByxYO5XuA7": {"type": "rebuttal", "replyto": "H1xL3Xuv27", "comment": "R3C01:\n\"should this be sigma\\_s?\"\n\nThank you for your careful eyes with this typo. We have fixed this in our revised paper. \n\nR3C02:\n\" The notation sigma\\_t could then be replace with tanh.\"\n\nBased on your suggestion, we have changed the notation to make our work more accessible. We have also changed k, which we initially used to denote for instances, into t .\n\nR3C03:\n\" ... to simply initialise LSTM forget gate biases to 1...\"\n\nBased on your suggestion, we included this general practice in Section 5 -- Related Studies. The text now reads as\n\"DecayNets initialise their forget gate values as 1s, which is similar to the effect of the common practice of setting LSTM forget gate biases to 1s (Gers et al., 2000), for yielding large initial forget values and for 'remembering more by default'.\"\n\n[Gers et al. (2000)]\nLearning to forget: Continual prediction with LSTM.\n\nR3C04:\n\" The first experiment is described as `image classification\" and\n\" It is not clear what the 'time' dimension is in how the RNNs are applied here ...\" \n\nThe experiment set-up follows the guidelines of Metz et al. (2016) (see Appendix C). We have now revised the text to make this experiment more accessible. In particular, we have added the explanation of\n\"The images are processed row-by-row, sequentially, from top to bottom. Thus, there are 28 instances and the inputs are vectors of 28 dimensions.\"\n\nAs addressed in the General Comment, the first experiment of the original submission is now placed in Appendix C of the revised paper. \n\n[Metz et al. (2016)]\nUnrolled  generative  adversarial networks.\n\nR3C05:\n\" A much wider variety of experiments ...\" and\n\" All the experiments (as far as I can tell) work on fixed length sequences. One advantage of an LSTM is that can run online on arbitrary length data\"\n\nWe have now added the arbitrary-length task of learning-to-learn (Andrychowicz et al., 2016) in Subsection 6.2 of the revised paper. This challenging task casts LSTMs as optimisers for updating the learning rule for another optimisee neural network. The LSTM-based optimisers are used to aggregate optimisee gradients for updates of arbitrary lengths, until the optimisee losses converge. Please refer to the General Comment for details.\n\n[Andrychowicz et al. (2016)]\nLearning to learn by gradient descent by gradient descent.", "title": "To AnonReviewer3"}, "HygFxYm_0X": {"type": "rebuttal", "replyto": "HkgwgV2q2X", "comment": "R2C01:\n\"Maybe there could also be a DecayNet-GRU\"\n\nThe updating scheme in GRUs is different and more complicated, therefore, the DecayNet cannot be applied there verbatim. While modifying GRUs with a deterministic feature similar to the Decay mechanism is a viable research direction, we believe it goes beyond the scope of our current work. Having said this, we have reflected your comment in Section 7 -- Conclusion. \n\nR2C02:\n\"instead of 'reformulation', would clearly write that DecayNet is an addition to the LSTM architecture, it might be more clear.\"\n\nWe have rephrased to address your comment in Section 1 - Introduction. We have now noted that\n\"We introduce DecayNets as an addition to the LSTM architecture \u2013-- DecayNets have monotonic decays in forget gates and enjoy smooth transitions in their cell state values.\"", "title": "To AnonReviewer2"}, "r1lmr_XO0m": {"type": "rebuttal", "replyto": "BkgYIiAcFQ", "comment": "We thank all reviewers, for spending time to review our paper, and for their constructive feedback. We have taken all their comments on board and revised our work accordingly. In order to assist the reviewers with the changes, the new/revised text are in colour brown in the revised-submission; the change of colours will be removed if the paper gets accepted.\n\nWe have added a new experiment to our paper by incorporating DecayNets to address a meta-learning problem; see Subsection 6.2 where DecayNets are used for the problem of learning to learn (Andrychowicz et al, 2016). This is to address the comment of AnonReviewer1 and AnonReviewer3 regarding more evidence for naturally sequential tasks where the length of the sequence is not fixed. We have also tightened our language according to the reviewers' comments; we fixed typos and proofread our work. Below, we address the common  concern of AnonReviewer1 and AnonReviewer3 for this additional experiment. Whereas point-by-point responses to reviewers' specific comments can be found as replies to the reviewer's feedback.\n\nAnonReviewer1 \n\"Both datasets are not designed in nature for sequential models like LSTMs.\"\nand \nAnonReviewer3\n\"All the experiments (as far as I can tell) work on fixed length sequences\"\n\nFollowing Andrychowicz et al. (2016),  we cast DecayNets and LSTMs as optimisers to update weights of MLP-optimisees on the task of MNIST classification. This task has an arbitrary length nature  which requires to aggregate the gradient information in a way that the optimizees can be optimized with minimum steps.\n\nThe MNIST images, of size 28 x 28, are presented as vector inputs of size 784 (= 28 x 28) to the optimisees. The optimisees are two-layer MLPs with dimensionality 784 x 20 x 10; the second MLP layer is a softmax layer where cross entropy losses need to be minimised.\n\nDuring the training phase, weights of the optimisees, along their corresponding gradients, are passed as inputs to LSTM-optimisers. For every instance, LSTM-optimisers prepare updating rules for the weights of the optimisees.\n\nThe Decay mechanism allows LSTMs to update their hidden variables for multiple times before generating the updating rules. For every instance, DecayNet forget gates are first reset as 1s, then, the inputs monotonically decay the forget gates, and update their hidden variables, for two instances. From our experiments, we found that the Decay mechanism helps the already successful LSTM-optimiser decrease MLP losses more rapidly, and yield optimised MLPs with lower and less varied losses.\n\nIn order to make space for the new experiment, we reconstructed Section 6. To be precise, we put \nthe experiment on image classification with row-by-row inputs in Appendix C of the revised paper.\n\n[Andrychowicz et al. (2016)] \nLearning to learn by gradient descent by gradient descent.", "title": "General Comment"}, "HkgwgV2q2X": {"type": "review", "replyto": "BkgYIiAcFQ", "review": "This paper performs an analysis of the cell-state updating scheme of LSTM and realizes that it is mainly controlled by the forget gate. Based on these outcomes they reformulate the functions (interpreted as differential equations) to add a decay-behaviour in the forget gates, finally called DecayNet-LSTM.\n\n\nThe theoretical analysis in this paper is very welcome and goes beyond observations which we made in the past, i.e., we often saw similar behavior in our experiments and as the authors also state in Section 5, there have been previous observations and approaches. In 2016, I have seen an idea called Random Activation Preservation (RAP) (https://ieeexplore.ieee.org/abstract/document/7487778 ) which just randomly \"resets\" the gates. However, they only show empirical outcomes, not a sophisticated analysis as in this paper.\n\nIn the experiments it is shown, that the DecayNet-LSTM performs similarly, or sometimes better than simple LSTM on standard tasks. On more difficult tasks, such as Perm-SeqMNIST, a state-of-the-art performance is achieved.\n\nMinor comments:\nPlease note, it should be Long Short-Term Memory (with hyphen between short and term)\nYou call the contribution DecayNet; And in the paper sometimes refer to it as DecayNet-LSTM; Maybe there could also be a DecayNet-GRU, ... If you, instead of \"reformulation\", would clearly write that DecayNet is an addition to the LSTM architecture, it might be more clear.", "title": "Theoretical Analysis of the forget gate behaviour leading to a nice novel contribution", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H1xL3Xuv27": {"type": "review", "replyto": "BkgYIiAcFQ", "review": "This paper analyses the internal dynamics of an LSTM, focusing on the cell state as being the most important component, and analyses what directly influences the contents of the cell state using difference equations. The authors note that at any timestep, the output cell state is the sum of (previous cell state * forget) and (input gate * input). The former can only shrink or maintain the cell value, which the authors label 'catch' and the latter can increase the magnitude, labelled 'release'.\n\nThe authors show that for a single neuron, with chosen values for the forget gate and inputs, consistent growth or consistent shrinking of the cell state can be observed. When the forget is large, say 0.9, the input gate can be anywhere in the range 0.5, 1.0] and still produce growth in the cell value. For forget gate = 0.25, an even larger range of input gate values all produce a shrinking cell value.\n\nDue to the forget gate seemingly having a stronger effect than the (input gate * input) component, the authors propose to hard wire the forget gate to produce a continuous and monotonic decrease, producing the DecayNet. The rate of this decay is controlled by a learned function of the input and previous hidden state, with some shifting in order to maintain a monotonic decrease. Each forget neuron will decrease at a different rate through the processing of a sequence, leading to sections of the cell state which will decay slowly and sections which will decay quickly.\n\nThe authors perform two sets of experiments. The second is sequence classification with the standard 'pixel by pixel' permuted sequential MNIST, in which they show a new SOTA with using Recurrent Batch Norm combined with DecayNet. They also demonstrate a DecayNet with fewer parameters producing roughy the same median performance as a normal LSTM but with lower variance.\n\nThe first experiment is described as \"image classification\", with MNIST and Fashion-MNIST. This section is unclear to me, I had initally assumed that the data would be fed in one pixel at a time, but due to the presence of the other experiments I presume this is not the case. It is not clear what the 'time' dimension is in how the RNNs are applied here, if not through some ordering of pixels. If the entire image is presented as a flattened input, and the time dimension is iterating through the dataset, then there is no reason to use an RNN here. More detail must be added here to make it clear exactly how these RNNs are being applied to images - the text says the softmax layer is produced from the final hidden state, but without the information about how the different hidden states are produced for a given training example this not meaningful. I can imagine that both tasks are pixel by pixel, and the only difference is whether to apply the permutation.. but that is my guesswork.\n\nIn general I find this paper an interesting idea, reasonably well communicated but some parts are not clear. All the experiments (as far as I can tell) work on fixed length sequences. One advantage of an LSTM is that can run onnline on arbitrary length data, for example when used in a RL Agent. In those circumstances, does learning a fixed monotonic delay on the forget gate make sense? I would guess not, and therefore I think the paper could be more explicit in indicating when a DecayNet is a good idea.\n\nThere are definitely tasks in which you want to have the forget gate drop to zero, to reset the state, and then go back up to 1 in subsequent timesteps to remember some new information. Presumably the monotonic delay would perform poorly.\n\nIs DecayNet appropriate only when you have fixed length sequences, where the distribution of 'when does relevant information appear in the input' is fixed? These questions make me doubt the generality of this approach, whereas \"this reformulation increases LSTM modelling power ... and also yields more consistent results\" from the abstract reads like this is a strictly better LSTM. A much wider variety of experiments would be required to justify this sentence. \n\nIt would be interesting to see some diagrams of forget gate / cell state changes throughout a real task, ie a graph with `k` on the x axis. The presentation of the new forget gate in \"System 2\" is clear in terms of being able to implement it, but it's not intuitive to me what this actually looks like. The graphs I suggest might go a long way to providing intuition for readers.\n\n\n\nOverall while I like the spirit of trying to understand and manipulate LSTM learning dynamics I am recommending reject. I do not think the paper sufficiently motivates why a monotonic decay should be good, and while the new SOTA on permuted MNIST is great, I'm concerned that the first experiments are not reproducable, as detailed previously in this review. All hyperparameters appear to be present, so this paper would be reproducable, except for the NIST experiments.\n\n\nGeneral recommendations for a future resubmission:\n\n* Clarify description of first MNIST experiments, and how they are different from permuted MNIST.\n* Experiments on a wider variety of canonical RNN tasks - Penn Treebank is an obvious contender.\n* Some mention of in what situations this is obviously not a good model to use (RL?)\n* More intuition / visualisations as to what the internal dynamics inside DecayNet look like, vs normal LSTM.\n* Devote less space to the initial dynamics analysis, or modify to be representative of a real task. This part was interesting on first read, but the only thing I think it really proves is 'when we artificially choose the input values things get bigger or smaller'. The important thing is, what actually happens when training on a task that we care about - if the same catch and release dynamics are observable, then that makes the idea more compelling.\n\n\nNotes and suggestions:\nI feel the notation would be clearer if instead of k = 1 .. D, this index was t = 1 ... T. This makes it cleare that s_k is not the k'th item in the array, but rather than whole activation array at a specific time. The notation \\sigma_t could then be replace with \\tanh.\n\n\"We replace \\sigma_t with sin for an ergodic delay over time\": as this is a new gate for the forget gate, should this be \\sigma_s?\n\nOne DL rule of thumb heard relatively often is to simply initialise LSTM forget gate biases to 1, to \"remember more by default\". As this is a (much simpler) way of trying to influence the behaviour of the gate, and it anecdotally improves data efficiency, it is worth mentioning in the paper.", "title": "Unclear general utility, insufficiently explained experiments.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyeE1XoaiX": {"type": "review", "replyto": "BkgYIiAcFQ", "review": "This paper provide a modification on the classical LSTM structure. Specifically, it reformulate the forget gate with a monotonically decreasing manner, using sinusoidal function as the activation function. \n\nHowever, both the motivation and experimental results on such modification are not convincing enough. \n\n1. While there are many heuristic guesses in sec3, important supports of these guesses are missed. For example, Figure 2 is designed to provide supports for the claim that we need controlled forget gates.  However, all the values of forget gates and input gates in Figure 2 are manually set as *conceptual observations*, which provides limited insight on what will happen in the real cases. While the reformulation in sec4 is based on the observations in Figure 2, it is important to plot the real cell propagation after the reformulation, and see whether the real observation meets the conceptual observations in Figure 2.\nBTW, Plots in Figure 2 only account for LSTMs' propagation within 3 steps, but in real cases there are way more steps. \n\n2. The authors claim monotonic propagation in the constant forget gates is more interpretable than those of the vanilla-LSTM, as no abrupt shrinkage and sudden growth are observed. But it isn't straightforward to get the relations between abrupt shrinkage and sudden growth on forget gates and the expressive power of the vanilla-LSTM. Also, it's hard to say the monotonic propagation is more interpretable because we don't know what's the meaning of such propagation on the behaviors of LSTMs in applications. \n\n3. The reformulation in sec 4, especially the formula for the forget-polar input p_k, looks heavily hand-crafted, without experimental supports but statements such as \"we ran numerous simulations\", which is not convincing enough. \n\n4. Experiments are applied on MNIST and Fashion-MNIST. While both datasets are not designed in nature for sequential models like LSTMs. There are better datasets and tasks for testing the proposed reformulation.   e.g. sentence classification, text generation, etc.  No explanation on the choice of datasets.  In addition, the difference between vanilla-LSTM and DecayNet-LSTM is small and it's hard to say it isn't marginal. Maybe larger-scale datasets are needed. \n\n5. Lacking of explanation on specific experimental settings. E.g. training all methods for *only one epoch*, which is very different from the standard practice.  \n\n6. More qualitative interpretations for real cell states in both vanilla LSTM  and DecayNet-LSTM are needed. Only conceptual demonstration is included in Figure 2. ", "title": "motivation and experiment are not convincing enough", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}