{"paper": {"title": "Cooperative Training of Descriptor and Generator Networks", "authors": ["Jianwen Xie", "Yang Lu", "Ruiqi Gao", "Song-Chun Zhu", "Ying Nian Wu"], "authorids": ["jianwen@ucla.edu", "yanglv@ucla.edu", "ruiqigao@ucla.edu", "sczhu@stat.ucla.edu", "ywu@stat.ucla.edu"], "summary": "Cooperative training of the descriptor and generator networks by coupling two maximum likelihood learning algorithms.", "abstract": "This paper studies the cooperative training of two probabilistic models of signals such as images. Both models are parametrized by convolutional neural networks (ConvNets). The first network is a descriptor network, which is an exponential family model or an energy-based model, whose feature statistics or energy function are defined by a bottom-up ConvNet, which maps the observed signal to the feature statistics. The second network is a generator network, which is a non-linear version of factor analysis. It is defined by a  top-down ConvNet, which maps the latent factors to the observed signal. The maximum likelihood training algorithms of both the descriptor net and the generator net are in the form of alternating back-propagation, and both algorithms involve Langevin sampling. We observe that the two training algorithms can cooperate with each other by jump-starting each other\u2019s Langevin sampling, and they can be seamlessly interwoven into a CoopNets algorithm that can train both nets simultaneously.", "keywords": ["Unsupervised Learning", "Deep learning"]}, "meta": {"decision": "Reject", "comment": "While the paper may have an interesting theoretical contribution, it seems to greatly suffer from problems in the presentation: the basic motivation is of the system is hardly mentioned in the introduction, and the conclusion does not explain much either. I think the paper should be rewritten, and, as some of the reviewers point out, the experiments strengthened before it can be accepted for publication.\n \n (I appreciate the last-minute revisions by the authors, but I think it really came too late, 14th/21st/23rd Jan, to be taken seriously into account in the review process.)"}, "review": {"H1m27Uevx": {"type": "rebuttal", "replyto": "B1E7Pwqgl", "comment": "We have added three new results: \n\n(1) Synthesis results by GAN for comparison. (Figure 6(b))\n(2) Synthesis results by algorithm G alone for comparison. (Figure 6(a))\n(3) Synthesis results on 224x224 resolution. (Figure 8)", "title": "Second Revision"}, "SJeGkQvUg": {"type": "rebuttal", "replyto": "B1E7Pwqgl", "comment": "Dear Reviewers, \n\nThank you for reviewing our paper and thank you for your comments! \n\nWe have uploaded a revision for your consideration. \n\nBecause the reviewers questioned the small training sizes in our experiments on textures and objects, we have opted to replace these experiments by a new experiment on 14 categories from standard datasets such as ImageNet and MIT place, where each training set consists of 1000 images randomly sampled from the category. Please see the experiment section as well as the appendix for the synthesis results. These are all we have got, without cheery picking. As can be seen, our method can generate meaningful and varied images. We haven\u2019t had time to tune the code. In fact, we had to recruit a new author (Ms. Ruiqi Gao) to help us run the code due to our various time constraints. With more careful tuning (including increasing image resolution), we expect to further improve the quality of synthesis. \n\nAbout the comparison with separate training method by either Algorithm D for descriptor or Algorithm G for generator individually, the separate training methods currently cannot produce synthesis results that are comparable to those produced by the cooperative training. This illustrates the advantage of cooperative training over separate training. In fact, the main motivation for this work is to overcome the difficulty with separate training by cooperative training.  \n\nWe have added a quantitative comparison with GAN for the face completion experiment, because our method is intended as an alternative to GAN.  Our original code was written in MatConvNet. We moved to TensorFlow in order to use existing code of GAN. We then rewrote Algorithm G in TensorFlow for image completion. GAN did not do well in this experiment. We are still checking and tuning our code to improve GAN performance. \n\nWe want to emphasize that we are treating the following two issues separately:\n\n(1) Train-test split and quantitative evaluation of generalizability. \n(2) Image synthesis judged qualitatively. \n\nWhile the face completion experiment is intended to address (1), the synthesis experiment is intended to address (2).  In fact, the generator network captures people\u2019s imagination mainly because of (2) (at least this is the case with ourselves), and some GAN papers are more qualitative than quantitative. \n\nWe will continue to work on experiments, to further address the questions raised by the reviewers and to continue to strengthen the quantitative side. \n\nWe have also made some minor changes to incorporate the reviewers\u2019 suggestions on wording and additional references. \n\nAs to the energy function, in particular, f(Y; W), for the descriptor, it is defined by a bottom-up ConvNet that maps the image Y to a score (very much like a discriminator), and we give the details of this ConvNet in the experiment section. We feel we made this clear in the original version. \n\nAs to equation (8), we have expanded the derivation. Equations (16) and (17) are about finite step Langevin dynamics. \n\nFinally please allow us to make some general comments regarding our paper. Our paper addresses the core issue of this conference, i.e., learning representations in the form of probabilistic generative models. There are two types of such papers: \n\n(1) Build on the successes of GAN. \n(2) Explore new connections and new routes. \n\nWe believe that papers in these two categories should be judged differently. Our paper belongs to category (2). It explores the connection between undirected model (descriptor) and directed model (generator). It also explores the connection between MCMC sampling (descriptor) and ancestral sampling (generator). Furthermore, it explores the new ground where two models interact with each other via synthesized data. We have also tried hard to gain a theoretical understanding of our method in appendix. \n\nThere have been a lot of papers in category (1) recently. We hope that the conference will be more open to the relatively fewer papers in category (2). In fact we are heartened that all three reviewers find our work interesting, and we can continue to improve our experiments. \n\nThanks for your consideration, and thanks for your comments that have helped us improve our work. \n", "title": "Reply to the Reviewers"}, "Hk5_yKIVl": {"type": "review", "replyto": "B1E7Pwqgl", "review": "-The authors proposes an interesting idea of connecting the energy-based model (descriptor) and \nthe generator network to help each other. The samples from the generator are used as the initialization \nof the descriptor inference. And the revised samples from the descriptor is in turn used to update\nthe generator as the target image. \n\nThe proposed idea is interesting. However, I think the main flaw is that the advantages of having that \narchitecture are not convincingly demonstrated in the experiments. For example, readers will expect \nquantative analysis on how initializing with the samples from the generator helps? Also, the only \nquantative experiment on the reconstruction is also compared to quite old models. Considering that \nthe model is quite close to the model of Kim & Bengio 2016, readers would also expect a comparison \nto that model. \n\n** Minor\n- I'm wondering if the analysis on the convergence is sound when considering the fact that samples \nfrom SGLD are biased samples (with fixed step size). \n- Can you explain a bit more on how you get Eqn 8? when p(x|y) is also dependent on W_G?\n", "title": "-", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJRnkerNl": {"type": "review", "replyto": "B1E7Pwqgl", "review": "-The authors proposes an interesting idea of connecting the energy-based model (descriptor) and \nthe generator network to help each other. The samples from the generator are used as the initialization \nof the descriptor inference. And the revised samples from the descriptor is in turn used to update\nthe generator as the target image. \n\nThe proposed idea is interesting. However, I think the main flaw is that the advantages of having that \narchitecture are not convincingly demonstrated in the experiments. For example, readers will expect \nquantative analysis on how initializing with the samples from the generator helps? Also, the only \nquantative experiment on the reconstruction is also compared to quite old models. Considering that \nthe model is quite close to the model of Kim & Bengio 2016, readers would also expect a comparison \nto that model. \n\n** Minor\n- I'm wondering if the analysis on the convergence is sound when considering the fact that samples \nfrom SGLD are biased samples (with fixed step size). \n- Can you explain a bit more on how you get Eqn 8? when p(x|y) is also dependent on W_G?\n", "title": "-", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJFPH0o7g": {"type": "rebuttal", "replyto": "H1AOHfOQx", "comment": "Dear Reviewer, \n\nThanks for reviewing our paper and thanks for your good question!\n\nYou are definitely correct that we may take more samples from each chain of Langevin dynamics. We take only one sample after running a finite number of Langevin steps within each learning iteration, mainly because of the auto-correlation between consecutive steps. During the learning algorithm, the parameters change gradually, so that the target distribution of the Langevin dynamics also changes gradually. We therefore only take one sample from the last Langevin step within each learning iteration as a matter of burn-in.  Of course this may be overly cautious, and we may instead average over the second half of the Langevin chain or take more samples as you suggested. We can do experiments to study this issue.  \n\nAbout using one Monte Carlo sample in each learning iteration versus averaging many samples, the learning algorithms for both the descriptor and the generator are stochastic gradient or stochastic approximation of Robbins-Monro, where the expectation is replaced by a single Monte Carlo sample in each iteration. The algorithm converges because it essentially accumulates the effects of the Monte Carlo samples over the iterations, so that the Monte Carlo variance is gradually eliminated. The basic idea is that instead of generating many Monte Carlo samples with fixed parameters, we may keep updating the parameters while generating these samples with the gradually changing parameters. Of course the contrast between one sample versus many samples is not mutually exclusive. We may use a small number of samples as a middle ground, and we can do experiments to study this issue. This is similar to the mini-batch training algorithm in supervised learning, which is also of Robbins-Monro type. \n\nThe cooperative training is beneficial to both the descriptor and generator. For the descriptor, each Langevin chain is initialized from a fresh independent sample provided by the generator, thus eliminating auto-correlation between samples of different learning iterations. Without the generator, we may have to run persistent chains, which may take a long time to explore the sample space due to auto-correlation between different learning iterations. For the generator, it learns from synthesized samples where the latent factors are known, so that the learning is effectively supervised. \n\nIt appears that we may insert the generator in the loop of any MCMC for any target distribution. The generator helps rejuvenate the MCMC by supplying fresh independent samples in each iteration, while the MCMC guides the generator towards the target distribution. In the end, we can generate independent samples from the target distribution using the learned generator directly without MCMC. \n\nThanks again for your question, which helps us clarify our presentation. \n", "title": "Reply to AnonReviewer3"}, "H1AOHfOQx": {"type": "review", "replyto": "B1E7Pwqgl", "review": "Hi, if I am reading the paper correctly, it seems from each Langevin dynamics, only one sample is used.  This appears to be case for the descriptor network where \\tilde{n} parallel dynamics are run and then expectation is computed using one sample each from those.  This is perhaps ok if \\tilde{n} is large enough, however why not take more samples from each dynamics?\nFor the case of generator network an expectation is needed for each training sample, and it appears that this is approximated by only one sample. \nI may be misreading things or there may be a lack in my understanding of how you are applying Langevin dynamics.  Would appreciate a clarification.This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks.  In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network.\n\nThis is an interesting approach for coupling the training of these two models.  The paper however is quite weak on the empirical studies.  In particular:\n- The training datasets are tiny, from sets of 1 image to 5-6.  What is the reason for not using larger sets?  I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach.\n- For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.  There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing.  Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks.\n\nAnother comment is that in the \u201crelated work\u201d section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included.\n\nDespite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR.  Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues.\n\n", "title": "Question about Langevin dynamics used to compute expectations", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyI6m6f4e": {"type": "review", "replyto": "B1E7Pwqgl", "review": "Hi, if I am reading the paper correctly, it seems from each Langevin dynamics, only one sample is used.  This appears to be case for the descriptor network where \\tilde{n} parallel dynamics are run and then expectation is computed using one sample each from those.  This is perhaps ok if \\tilde{n} is large enough, however why not take more samples from each dynamics?\nFor the case of generator network an expectation is needed for each training sample, and it appears that this is approximated by only one sample. \nI may be misreading things or there may be a lack in my understanding of how you are applying Langevin dynamics.  Would appreciate a clarification.This paper proposed a new joint training scheme for two probabilistic models of signals (e.g. images) which are both deep neural network based and are termed generator and descriptor networks.  In the new scheme, termed cooperative training, the two networks train together and assist each other: the generator network provides samples that work as initial samples for the descriptor network, and the descriptor network updates those samples to help guide training of the generator network.\n\nThis is an interesting approach for coupling the training of these two models.  The paper however is quite weak on the empirical studies.  In particular:\n- The training datasets are tiny, from sets of 1 image to 5-6.  What is the reason for not using larger sets?  I think the small datasets are leading to over training and are really masking the true value of the proposed cooperative training approach.\n- For most of the experiments presented in the paper it is hard to assess the specific value brought by the proposed cooperative training approach because baseline results are missing.  There are comparisons provided for face completion experiments - but even there comparisons with descriptor or generator network trained separately or with other deep auto-encoders are missing.  Thus it is hard to conclude if and how much gain is obtained by cooperative training over say individually training the descriptor and generator networks.\n\nAnother comment is that in the \u201crelated work\u201d section, I think relation with variational auto encoders (Kingma and Welling 2013) should be included.\n\nDespite limitations mentioned above, I think the ideas presented in the paper are intuitively appealing and worth discussing at ICLR.  Paper would be considerably strengthened by adding more relevant baselines and addressing the training data size issues.\n\n", "title": "Question about Langevin dynamics used to compute expectations", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SktJh1fXe": {"type": "rebuttal", "replyto": "SyKz5JJQl", "comment": "Dear Reviewer, \n\nThank you for spending time reviewing our paper!\n\nWe totally agree with you that we should pay attention to the issue of overfitting and generalizability in terms of train/test split. This is exactly the reason we have conducted the face completion experiment on the testing data. We can also conduct similar experiments on texture examples. Thank you for the reference. We will study it, and will study the issue of generalizability in texture experiments quantitatively. \n\nMeanwhile we do wish to point out that the main purpose of our texture experiments is to show qualitatively that our method is capable of generating realistic and varied high-resolution images (224x224). Because of the learned generator, the image generation is very efficient. \n\nAbout the image completion experiment and Table 1, as mentioned above, our goal is to test whether the learned model can generalize to the testing images. We have not implemented more recent methods on image completion. We can study them and implement them. Our quantitative experiment shows that the learned model can indeed generalize to the testing images. Another feature of our method is that even if the recovered image may be different from the original image (in terms of reconstruction error) when the occluding mask is big (e.g., a man\u2019s face is recovered into a woman\u2019s face in one of the examples shown in the paper), the recovered image is still perceptually plausible. \n\nThanks for your good questions. \n \n", "title": "Reply to AnonReviewer2"}, "SyKz5JJQl": {"type": "review", "replyto": "B1E7Pwqgl", "review": "Could the authors comment on the lack of train/test split for the results of Fig. 2, 3 ? Previous work on texture modeling has split individual textures in two (training and testing halves) and featured a numerical evaluation based on the Textual Similarity Score. This is particularly worrisome as there are clear signs of overfitting (not surprising given size of training set). As for Table 1, why did the authors not include more recent baselines such Variational Auto-Encoders ?\n\n[R1] Heess, N., Williams, C. K. I., and Hinton, G. E. (2009). Learning generative texture models with extended fields of-experts.This paper introduces CoopNets, an algorithm which trains a Deep-Energy Model (DEM, the \u201cdescriptor\u201d) with the help of an auxiliary directed bayes net, e.g. \u201cthe generator\u201d. The descriptor is trained via standard maximum likelihood, with Langevin MCMC for sampling. The generator is trained to generate likely samples under the DEM in a single, feed-forward ancestral sampling step. It can thus be used to shortcut expensive MCMC sampling, hence the reference to \u201ccooperative training\u201d.\n\nThe above idea is interesting and novel, but unfortunately is not sufficiently validated by the experimental results. First and foremost, two out of the three experiments do not feature a train /test split, and ignore standard training and evaluation protocols for texture generation (see [R1]). Datasets are also much too small. As such these experiments only seem to confirm the ability of the model to overfit. On the third in-painting tasks, baselines are almost non-existent: no VAEs, RBMs, DEM, etc which makes it difficult to evaluate the benefits of the proposed approach.\n\nIn a future revision, I would also encourage the authors to answer the following questions experimentally. What is the impact of the missing rejection step in Langevin MCMC (train with, without ?). What is the impact of the generator on the burn-in process of the Markov chain (show sample auto-correlation). How bad is approximation of training the generator from ({\\tilde{Y}, \\hat{X}) instead of ({\\tilde{Y}, \\tilde{X}) ? Run comparative experiments.\n\nThe paper would also greatly benefit from a rewrite focusing on clarity, instead of hyperbole (\u201cpioneering work\u201d in reference to closely related, but non-peer reviewed work) and prose (\u201ctale of two nets\u201d). For example, the authors fail to specify the exact form of the energy function: this seems like a glaring omission.\n\nPROS:\n+ Interesting and novel idea\nCONS:\n- Improper experimental protocols\n- Missing baselines\n- Missing diagnostic experiments\n\n[R1] Heess, N., Williams, C. K. I., and Hinton, G. E. (2009). Learning generative texture models with extended fields of-experts.", "title": "Train / test split", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJPZZKVEx": {"type": "review", "replyto": "B1E7Pwqgl", "review": "Could the authors comment on the lack of train/test split for the results of Fig. 2, 3 ? Previous work on texture modeling has split individual textures in two (training and testing halves) and featured a numerical evaluation based on the Textual Similarity Score. This is particularly worrisome as there are clear signs of overfitting (not surprising given size of training set). As for Table 1, why did the authors not include more recent baselines such Variational Auto-Encoders ?\n\n[R1] Heess, N., Williams, C. K. I., and Hinton, G. E. (2009). Learning generative texture models with extended fields of-experts.This paper introduces CoopNets, an algorithm which trains a Deep-Energy Model (DEM, the \u201cdescriptor\u201d) with the help of an auxiliary directed bayes net, e.g. \u201cthe generator\u201d. The descriptor is trained via standard maximum likelihood, with Langevin MCMC for sampling. The generator is trained to generate likely samples under the DEM in a single, feed-forward ancestral sampling step. It can thus be used to shortcut expensive MCMC sampling, hence the reference to \u201ccooperative training\u201d.\n\nThe above idea is interesting and novel, but unfortunately is not sufficiently validated by the experimental results. First and foremost, two out of the three experiments do not feature a train /test split, and ignore standard training and evaluation protocols for texture generation (see [R1]). Datasets are also much too small. As such these experiments only seem to confirm the ability of the model to overfit. On the third in-painting tasks, baselines are almost non-existent: no VAEs, RBMs, DEM, etc which makes it difficult to evaluate the benefits of the proposed approach.\n\nIn a future revision, I would also encourage the authors to answer the following questions experimentally. What is the impact of the missing rejection step in Langevin MCMC (train with, without ?). What is the impact of the generator on the burn-in process of the Markov chain (show sample auto-correlation). How bad is approximation of training the generator from ({\\tilde{Y}, \\hat{X}) instead of ({\\tilde{Y}, \\tilde{X}) ? Run comparative experiments.\n\nThe paper would also greatly benefit from a rewrite focusing on clarity, instead of hyperbole (\u201cpioneering work\u201d in reference to closely related, but non-peer reviewed work) and prose (\u201ctale of two nets\u201d). For example, the authors fail to specify the exact form of the energy function: this seems like a glaring omission.\n\nPROS:\n+ Interesting and novel idea\nCONS:\n- Improper experimental protocols\n- Missing baselines\n- Missing diagnostic experiments\n\n[R1] Heess, N., Williams, C. K. I., and Hinton, G. E. (2009). Learning generative texture models with extended fields of-experts.", "title": "Train / test split", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}