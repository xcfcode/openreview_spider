{"paper": {"title": "Parameter Space Noise for Exploration", "authors": ["Matthias Plappert", "Rein Houthooft", "Prafulla Dhariwal", "Szymon Sidor", "Richard Y. Chen", "Xi Chen", "Tamim Asfour", "Pieter Abbeel", "Marcin Andrychowicz"], "authorids": ["matthiasplappert@me.com", "rein.houthooft@openai.com", "prafulla@openai.com", "szymon@openai.com", "richardchen@openai.com", "peter@openai.com", "asfour@kit.edu", "pabbeel@cs.berkeley.edu", "marcin@openai.com"], "summary": "Parameter space noise allows reinforcement learning algorithms to explore by perturbing parameters instead of actions, often leading to significantly improved exploration performance.", "abstract": "Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks.", "keywords": ["reinforcement learning", "exploration", "parameter noise"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes adding noise to the parameters of a deep network when taking actions in deep reinforcement learning to encourage exploration.  The method is simple but the authors demonstrate its effectiveness through thorough empirical analysis across a variety of reinforcement learning tasks (i.e. DQN, DDPG, and TRPO).  Overall the paper is clear, well written and the reviewers enjoyed it.  However, a common trend among the reviews was that the authors overstated their claims and contributions.  The reviewers called out some statements in particular (e.g. the discussion of ES and RL) which the authors appear to have addressed when comparing their revisions (thank you).  Overall, a clear, well written paper conveying a simple but effective idea for exploration that often works across a variety of RL tasks.  The authors also released open-source code along with their paper for reproducibility (as evidenced by the reproducibility study below), which is appreciated.\n\nPros:\n- Clear and well written\n- Thorough experiments across deep RL domains\n- A simple strategy for exploration that is effective empirically\n\nCons:\n- Not a panacea for exploration (although nothing really is)\n- Claims are somewhat overstated\n- Lacks a strong justification for the method other than that it is empirically effective and intuitive"}, "review": {"By9jfdZkM": {"type": "review", "replyto": "ByBAl2eAZ", "review": "This paper explores the idea of adding parameter space noise in service of exploration. The paper is very well written and quite clear. It does a good job of contrasting parameter space noise to action space noise and evolutionary strategies.\n\nHowever, the results are weak. Parameter noise does better in some Atari + Mujoco domains, but shows little difference in most domains. The domains where parameter noise (as well as evolutionary strategies) does really well are Enduro and the Chain environment, in which a policy that repeatedly chooses a particular action will do very well. E-greedy approaches will always struggle to choose the same random action repeatedly. Chain is great as a pathological example to show the shortcomings of e-greedy, but few interesting domains exhibit such patterns. Similarly for the continuous control with sparse rewards environments \u2013 if you can construct an environment with sparse enough reward that action-space noise results in zero rewards, then clearly parameter space noise will have a better shot at learning. However, for complex domains with sparse reward (e.g. Montezuma\u2019s Revenge) parameter space noise is just not going to get you very far.\n\nOverall, I think parameter space noise is a worthy technique to have analyzed and this paper does a good job doing just that. However, I don\u2019t expect this technique to make a large splash in the Deep RL community, mainly because simply adding noise to the parameter space doesn\u2019t really gain you much more than policies that are biased towards particular actions. Parameter noise is not a very smart form of exploration, but it should be acknowledged as a valid alternative to action-space noise.\n\nA non-trivial amount of work has been done to find a sensible way of adding noise to parameter space of a deep network and defining the specific distance metrics and thresholds for (dual-headed) DQN, DDPG, and TRPO.\n", "title": "How far is parameter noise really going to get us?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1gBpq_gG": {"type": "review", "replyto": "ByBAl2eAZ", "review": "In recent years there have been many notable successes in deep reinforcement learning. However, in many tasks, particularly sparse reward tasks, exploration remains a difficult problem. For off-policy algorithms it is common to explore by adding noise to the policy action in action space, while on-policy algorithms are often regularized in the action space to encourage exploration. This work introduces a simple, computationally straightforward approach to exploring by perturbing the parameters (similar to exploration in some evolutionary algorithms) of policies parametrized with deep neural nets. This work argues this results in more consistent exploration and compares this approach empirically on a range of continuous and discrete tasks. By using layer norm and adaptive noise, they are able to generate robust parameter noise (it is often difficult to estimate the appropriate variance of parameter noise, as its less clear how this relates to the magnitude of variance in the action space).\n\nThis work is well-written and cites previous work appropriately. Exploration is an important topic, as it often appears to be the limiting factor of Deep RL algorithms. The authors provide a significant set of experiments using their method on several different RL algorithms in both continuous and discrete cases, and find it generally improves performance, particularly for sparse rewards.\n\nOne empirical baseline that would helpful to have would be a stochastic off-policy algorithm (both off-policy algorithms compared are deterministic), as this may better capture uncertainty about the value of actions (e.g. SVG(0) [3]).\n\nAs with any empirical results with RL, it is a challenging problem to construct comparable benchmarks due to minor variations in implementation, environment or hyper-parameters all acting as confounding variables [1]. It would be helpful if the authors are able to make their paper reproducible by releasing the code on publication. As one example, figure 4 of [1] seems to show DDPG performing much better than the DDPG baseline in this work on half-cheetah.\n\nMinor points:\n- The definition of a stochastic policy (section 2) is unusual (it is defined as an unnormalized distribution). Usually it would be defined as $\\mathcal{S} \\rightarrow \\mathcal{P}(\\mathcal{A})$\n\n- This work extends DQN to learn an explicitly parametrized policy (instead of the greedy policy) in order to useful perturb the parameters of this policy. Instead of using a single greedy target, you could consider use the relationship between the advantage function and an entropy-regularized policy [2] to construct a target.\n\n[1] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2017). Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560.\n\n[2] O'Donoghue, B., Munos, R., Kavukcuoglu, K., & Mnih, V. (2016). Combining policy gradient and Q-learning.\n\n[3] Heess, N., Wayne, G., Silver, D., Lillicrap, T., Erez, T., & Tassa, Y. (2015). Learning continuous control policies by stochastic value gradients. In Advances in Neural Information Processing Systems (pp. 2944-2952).", "title": "Nice exploration of parameter noise", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryVd3dFgf": {"type": "review", "replyto": "ByBAl2eAZ", "review": "This paper proposes a method for parameter space noise in exploration.\nRather than the \"baseline\" epsilon-greedy (that sometimes takes a single action at random)... this paper presents an method for perturbations to the policy.\nIn some domains this can be a much better approach and this is supported by experimentation.\n\nThere are several things to like about the paper:\n- Efficient exploration is a big problem for deep reinforcement learning (epsilon-greedy or Boltzmann is the de-facto baseline) and there are clearly some examples where this approach does much better.\n- The noise-scaling approach is (to my knowledge) novel, good and in my view the most valuable part of the paper.\n- This is clearly a very practical and extensible idea... the authors present good results on a whole suite of tasks.\n- The paper is clear and well written, it has a narrative and the plots/experiments tend to back this up.\n- I like the algorithm, it's pretty simple/clean and there's something obviously *right* about it (in SOME circumstances).\n\nHowever, there are also a few things to be cautious of... and some of them serious:\n- At many points in the paper the claims are quite overstated. Parameter noise on the policy won't necessarily get you efficient exploration... and in some cases it can even be *worse* than epsilon-greedy... if you just read this paper you might think that this was a truly general \"statistically efficient\" method for exploration (in the style of UCRL or even E^3/Rmax etc).\n- For instance, the example in 4.2 only works because the optimal solution is to go \"right\" in every timestep... if you had the network parameterized in a different way (or the actions left/right were relabelled) then this parameter noise approach would *not* work... By contrast, methods such as UCRL/PSRL and RLSVI https://arxiv.org/abs/1402.0635 *are* able to learn polynomially in this type of environment. I think the claim/motivation for this example in the bootstrapped DQN paper is more along the lines of \"deep exploration\" and you should be clear that your parameter noise does *not* address this issue.\n- That said I think that the example in 4.2 is *great* to include... you just need to be more upfront about how/why it works and  what you are banking on with the parameter-space exploration. Essentially you perform a local exploration rule in parameter space... and sometimes this is great - but you should be careful to distinguish this type of method from other approaches. This must be mentioned in section 4.2 \"does parameter space noise explore efficiently\" because the answer you seem to imply is \"yes\" ... when the answer is clearly NOT IN GENERAL... but it can still be good sometimes ;D\n- The demarcation of \"RL\" and \"evolutionary strategies\" suggests a pretty poor understanding of the literature and associated concepts. I can't really support the conclusion \"RL with parameter noise exploration learns more efficiently than both RL and evolutionary strategies individually\". This sort of sentence is clearly wrong and for many separate reasons:\n    - Parameter noise exploration is not a separate/new thing from RL... it's even been around for ages! It feels like you are talking about DQN/A3C/(whatever algorithm got good scores in Atari last year) as \"RL\" and that's just really not a good way to think about it.\n    - Parameter noise exploration can be *extremely* bad relative to efficient exploration methods (see section 2.4.3 https://searchworks.stanford.edu/view/11891201)\n\n\nOverall, I like the paper, I like the algorithm and I think it is a valuable contribution.\nI think the value in this paper comes from a practical/simple way to do policy randomization in deep RL.\nIn some (maybe even many of the ones you actually care about) settings this can be a really great approach, especially when compared to epsilon-greedy.\n\nHowever, I hope that you address some of the concerns I have raised in this review.\nYou shouldn't claim such a universal revolution to exploration / RL / evolution because I don't think that it's correct.\nFurther, I don't think that clarifying that this method is *not* universal/general really hurts the paper... you could just add a section in 4.2 pointing out that the \"chain\" example wouldn't work if you needed to do different actions at each timestep (this algorithm does *not* perform \"deep exploration\").\n\nI vote accept.", "title": "Random exploration at the policy level, rather than the action level - a good paper, but needs to be a bit more careful with the hype.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HJxWJ0eEG": {"type": "rebuttal", "replyto": "HyUtjoTmG", "comment": "I would like to thank you for conducting this study and evaluating the behavior of parameter space noise with duelling networks and prioritized replay. It is interesting that both seem to not help in this case.\n\nI would also like to note that the bug that you mention was only introduced while we refactored our code to be releasable (this was necessary due to the original code having a heavy dependance on our internal infrastructure). The experiments presented in our paper were not affected by it and the scaling was handled correctly.", "title": "Thank you for conducting this study"}, "HJjO3r97f": {"type": "rebuttal", "replyto": "ByBAl2eAZ", "comment": "Dear reviewers, a revised manuscript that takes your feedback into account has been uploaded.", "title": "Revised manuscript"}, "HJUU9kZzf": {"type": "rebuttal", "replyto": "By9jfdZkM", "comment": "We would like to thank the reviewer for the insightful comments and suggestions.\n\nWe do agree that parameter noise alone is not going to solve exploration in reinforcement learning. However, we do feel that it provides an interesting alternative to the still de-facto standard of exploration, which is action space noise like epsilon-greedy or additive Gaussian noise. We think that our paper demonstrates that parameter noise exhibits different behavior that can often result in superior exploration that such simple action space noise exploration methods while being conceptually similarly simple. Furthermore, many recent exploration strategies like intrinsic motivation or count-based approaches augment the reward function with a bonus to encourage exploration but still rely on action space noise for \u201clow-level\u201d exploration. We think that parameter noise could also be an interesting replacement for this low level exploration.\n\nThat being said, we do agree that the paper can often seem to overstate the exploration properties of our proposed method. We will carefully revise the manuscript to better present parameter space noise as an interesting alternative to action space noise while emphasizing that it by no means resolves the exploration problem universally.", "title": "Response to review"}, "ByiX9J-zf": {"type": "rebuttal", "replyto": "r1gBpq_gG", "comment": "We would like to thank the reviewer for the insightful comments and suggestions.\n\nWe agree that reproducibility is an important consideration. The code for DQN and DDPG has already been open-sourced. Unfortunately we cannot directly link to it in the paper due to the double-blind review process. The final version of the paper will include a link to the source code.\n\nWe further agree that a stochastic off-policy algorithm such as SVG(0) would be an interesting addition. However, we feel like DQN, DDPG, and TRPO already cover a significant spectrum and we would therefore leave the evaluation with other algorithms like SVG(0) and PPO to future work.\n\nWe will also revise the definition of a stochastic policy in the continuous case as suggested by the reviewer.", "title": "Response to review"}, "SkaJ5JWMG": {"type": "rebuttal", "replyto": "ryVd3dFgf", "comment": "We would like to thank the reviewer for the insightful comments and suggestions.\n\nWe will update section 4.2 to better reflect the limitations of our proposed method and to clarify that parameter noise is by no means a universally applicable strategy with guarantees. In particular, we will include a paragraph in the chain environment discussion to highlight that parameter noise works well here due to the simplicity of the optimal strategy. We will further clarify that this experiment was intended to highlight the difference in behavior between epsilon-greedy exploration and parameter noise and that it is clearly a toy problem that should not be interpreted as a claim that parameter noise exploration results in universally better exploration.\n\nWe will also revise the text that is concerned with the discussion of ES and RL. We do agree that parameter noise in general is by no means a novel concept and will revise accordingly. We will further clarify that the scope of the proposed approach is to make parameter noise work in the context of deep reinforcement learning and that our comparison is meant to highlight the advances in sample complexity compared to the method proposed by Salimans et al. (2017).\n\nGenerally speaking, we will revise our paper to better reflect what parameter noise really is: A conceptually simple replacement for simple exploration strategies like epsilon-greedy that often results in better exploration than these baselines. However, by no means is parameter noise a universally applicable method with guarantees like RLSVI or E^3. We will add language to clearly state this.", "title": "Response to review"}}}