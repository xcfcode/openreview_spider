{"paper": {"title": "A New Method of Region Embedding for Text Classification", "authors": ["chao qiao", "bo huang", "guocheng niu", "daren li", "daxiang dong", "wei he", "dianhai yu", "hua wu"], "authorids": ["chao.qiao@outlook.com", "bohuang0321@gmail.com", "niuguocheng@baidu.com", "lidaren@baidu.com", "dongdaxiang@baidu.com", "hewei06@baidu.com", "yudianhai@baidu.com", "wu_hua@baidu.com"], "summary": "", "abstract": "To represent a text as a bag of properly identified \u201cphrases\u201d and use the representation for processing the text is proved to be useful. The key question here is how to identify the phrases and represent them. The traditional method of utilizing n-grams can be regarded as an approximation of the approach. Such a method can suffer from data sparsity, however, particularly when the length of n-gram is large. In this paper, we propose a new method of learning and utilizing task-specific distributed representations of n-grams, referred to as \u201cregion embeddings\u201d. Without loss of generality we address text classification. We specifically propose two models for region embeddings. In our models, the representation of a word has two parts, the embedding of the word itself, and a weighting matrix to interact with the local context, referred to as local context unit. The region embeddings are learned and used in the classification task, as parameters of the neural network classifier. Experimental results show that our proposed method outperforms existing methods in text classification on several benchmark datasets. The results also indicate that our method can indeed capture the salient phrasal expressions in the texts.", "keywords": ["region embedding", "local context unit", "text classification"]}, "meta": {"decision": "Accept (Poster)", "comment": "despite not amazing scores, this is a solid paper.\nit created a lot of discussion and was found to be reproducible.\nwe should accept it to let the iclr community partake in the discussion and learn about this method of n-gram embeddings\n"}, "review": {"ryjxrEwlM": {"type": "review", "replyto": "BkSDMA36Z", "review": "The authors propose a mechanism for learning task-specific region embeddings for use in text classification. Specifically, this comprises a standard word embedding an accompanying local context embedding. \n\nThe key idea here is the introduction of a (h x c x v) tensor K, where h is the embedding dim (same as the word embedding size), c is a fixed window size around a target word, and v is the vocabulary size. Each word in v is then associated with an (h x c) matrix that is meant to encode how it affects nearby words, in particular this may be viewed as parameterizing a projection to be applied to surrounding word embeddings. The authors propose two specific variants of this approach, which combine the K matrix and constituent word embeddings (in a given region) in different ways. Region embeddings are then composed (summed) and fed through a standard model. \n\nStrong points\n---\n+ The proposed approach is simple and largely intuitive: essentially the context matrix allows word-specific contextualization. Further, the work is clearly presented.\n\n+ At the very least the model does seem comparable in performance to various recent methods (as per Table 2), however as noted below the gains are marginal and I have some questions on the setup.\n\n+ The authors perform ablation experiments, which are always nice to see. \n\nWeak points\n---\n- I have a critical question for clarification in the experiments. The authors write 'Optimal hyperparameters are tuned with 10% of the training set on Yelp Review Full dataset, and identical hyperparameters are applied to all datasets' -- is this true for *all* models, or only the proposed approach? \n\n- The gains here appear to be consistent, but they seem marginal. The biggest gain achieved over all datasets is apparently .7, and most of the time the model very narrowly performs better (.2-.4 range). Moreoever, it is not clear if these results are averaged over multiple runs of SGD or not (variation due to initialization and stochastic estimation can account for up to 1 point in variance -- see \"A sensitivity analysis of (and practitioners guide to) CNNs...\" Zhang and Wallace, 2015.)\n\n- The related work section seems light. For instance, there is no discussion at all of LSTMs and their application to text classificatio (e.g., Tang et al., EMNLP 2015) -- although it is noted that the authors do compare against D-LSTM,  or char-level CNNs for the same (see Zhang et al., NIPs 2015). Other relevant work not discussed includes Iyyer et al. (ACL 2015). In their respective ways, these papers address some of the same issues the authors consider here. \n\n- The two approaches to inducing the final region embedding (word-context and then context-word in sections 3.2 and 3.3, respectively) feel a bit ad-hoc. I would have appreciated more intuition behind these approaches. \n\nSmall comments\n---\nThere is a typo in Figure 4 -- \"Howerver\" should be \"However\"\n\n*** Update after author response ***\n\nThanks to the authors for their responses. My score is unchanged.", "title": "Review of \"Bag of region embeddings ...\"", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r1sXToOgf": {"type": "review", "replyto": "BkSDMA36Z", "review": "The authors present a model for text classification. The parameters of the model are an embedding for each word and a local context unit. The local context unit can be seen as a filter for a convolutional layer, but which filter is used at location i depends on the word at location i (i.e. there is one filter per vocabulary word). After the filter is applied to the embeddings and after max pooling, the word-context region embeddings are summed and fed into a neural network for the classification task. The embeddings, the context units and the neural net parameters are trained jointly on a supervised text classification task. The authors also offer an alternative model, which changes the role of the embedding an the context unit, and results in context-word region embeddings. Here the embedding of word i is combined with the elements of the context units of words in the context. To get the region embeddings both model (word-context and context-word) combine attributes of the words (embeddings) with how their attributes should be emphasized or deemphasized based on nearby words (local context units and max pooling) while taking into account the relative position of the words in the context (columns of the context units). \n\nThe method beats existing methods for text classification including d-LSTMs , BoWs, and ngram TFIDFs on held out classification accuracy. the choice of baselines is convincing. What is the performance of the proposed method if the embeddings are initialized to pretrained word embeddings and a) trained for the classification task together with randomly initialized context units b) frozen to pretrained embeddings and only the context units are trained for the classification task?\n\nThe introduction was fine. Until page 3 the authors refer to the context units a couple of times without giving some simple explanation of what it could be. A simple explanation in the introduction would improve the writing.\nThe related work section only makes sense *after* there is at least a minimal explanation of what the local context units do. A simple explanation of the method, for example in the introduction, would then make the connections to CNNs more clear. Also, in the related work, the authors could include more citations (e.g. the d-LSTM and the CNN based methods from Table 2) and explain the qualitative differences between their method and existing ones.\n\nThe authors should consider adding equation numbers. The equation on the bottom of page 3 is fine, but the expressions in 3.2 and 3.3 are weird. A more concise explanation of the context-word region embeddings and the word-context region embeddings would be to instead give the equation for r_{i,c}.  \n\nThe included baselines are extensive and the proposed method outperforms existing methods on most datasets. In section 4.5 the authors analyze region and embedding size, which are good analyses to include in the paper. Figure 2 and 3 could be next to each other to save space. \nI found the idea of multi region sizes interesting, but no description is given on how exactly they are combined. Since it works so well, maybe it could be promoted into the method section? Also, for each data set, which region size worked best?\n\nQualitative analysis: It would have been nice to see some analysis of whether the learned embeddings capture semantic similarities, both at the embedding level and at the region level. It would also be interesting to investigate the columns of the context units, with different columns somehow capturing the importance of relative position. Are there some words for which all columns are similar meaning that their position is less relevant in how they affect nearby words? And then for other words with variation along the columns of the context units, do their context units modulate the embedding more when they are closer or further away? \n\nPros:\n + simple model\n + strong quantitative results\n\nCons:\n - notation (i.e. precise definition of r_{i,c})\n - qualitative analysis could be extended\n - writing could be improved  ", "title": "New model for text classification", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "Sy6ClHqef": {"type": "review", "replyto": "BkSDMA36Z", "review": "() Summary\nIn this paper, the authors introduced a new simple model for text classification, which obtains state of the art results on several benchmark. The main contribution of the paper is to propose a new technique to learn vector representation of fixed-size text regions of up to a few words. In addition to learning a vector for each word of the vocabulary, the authors propose to also learn a \"context unit\" of size d x K, where d is the embedding size and K the region size. Thus, the model also have a vector representation for pair of word and position in the region. Then, given a region of K words, its vector representation is obtained by taking the elementwise product of the \"context unit\" of the middle word and the matrix obtained by concatenating the K vectors of words appearing in the region (the authors also propose a second model where the role of word vectors and \"context\" vectors are exchanged). The max-pooling operation is then used to obtain a vector representation of size d. Then a linear classifier is applied on top of the sum of the region embeddings. The authors then compare their approach to previous work on the 8 datasets introduced by Zhang et al. (2015). They obtain state of the art results on most of the datasets. They also perform some analysis of their models, such as the influence of the region size, embedding size, or replacing the \"context units\" vector by a scalar. The authors also provide some visualisation of the parameters of their model.\n\n() Discussion\nOverall, I think that the proposed method is sound and well justified. The empirical evaluations, analysis and comparisons to existing methods are well executed. I liked the fact that the proposed model is very simple, yet very competitive compared to the state-of-the-art. I suspect that the model is also computationally efficient: can the authors report training time for different datasets? I think that it would make the paper stronger. One of the main limitations of the model, as stated by the authors, is its number of parameters. Could the authors also report these?\n\nWhile the paper is fairly easy to read (because the method is simple and Figure 1 helps understanding the model), I think that copy editing is needed. Indeed, the papers contains many typos (I have listed a few), as well as ungrammatical sentences. I also think that a discussion of the \"attention is all you need\" paper by Vaswani et al. is needed, as both articles seem strongly related.\n\nAs a minor comment, I advise the authors to use a different letter for \"word embeddings\" and the \"projected word embeddings\" (equation at the bottom of page 3). It would also make the paper more clear.\n\n() Pros / Cons:\n+ simple yet powerful method for text classification\n+ strong experimental results\n+ ablation study / analysis of influence of parameters\n- writing of the paper\n- missing discussion to the \"attention is all you need paper\", which seems highly relevant\n\n() Typos:\nPage 1\n\"a support vectors machineS\" -> \"a support vector machine\"\n\"performs good\" -> \"performs well\"\n\"the n-grams was widely\" -> \"n-grams were widely\"\n\"to apply large region size\" -> \"to apply to large region size\"\n\"are trained separately\" -> \"do not share parameters\"\n\nPage 2\n\"convolutional neural networks(CNN)\" -> \"convolutional neural networks (CNN)\"\n\"related works\" -> \"related work\"\n\"effective in Wang and Manning\" -> \"effective by Wang and Manning\"\n\"applied on text classification\" -> \"applied to text classification\"\n\"shard(word independent)\" -> \"shard (word independent)\"\n\nPage 3\n\"can be treat\" -> \"can be treated\"\n\"fixed length continues subsequence\" -> \"fixed length contiguous subsequence\"\n\"w_i stands for the\" -> \"w_i standing for the\"\n\"which both the unit\" -> \"where both the unit\"\n\"in vocabulary\" -> \"in the vocabulary\"\n\netc...", "title": "review", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B1b7od97z": {"type": "rebuttal", "replyto": "r1sXToOgf", "comment": "Thank for your suggestions. We will explain your concerns point by point.\n\n1) \"What is the performance of the proposed method if the embeddings are initialized to pretrained word embeddings and a) trained for the classification task together with randomly initialized context units b) frozen to pretrained embeddings and only the context units are trained for the classification task?\"\nWe evaluated the experiments with pretrained word embeddings on Yelp.F and Yelp.P dataset:\n\n*Datasets*        *Method*        Best epoch(start from 0)*        *Accuracy*\nYelp F.        Random        1        0.649500\nYelp F.        Finetune        1        0.638580\nYelp F.        Frozen        2        0.633060\nYelp P.        Random        2        0.963895\nYelp P.        Finetune        1        0.962500\nYelp P.        Frozen        2        0.960842\n\nThe word embeddings are pre-trained by Wikipedia+Gigaword 5 glove with 200 dimensions, region size is 7.  The result of a) is similar with randomly initialized word embeddings, and result of b) is slightly worse.\nIntuitively, pre-trained word embeddings should have a role, but maybe should not be applied directly. In fact, we will explore the way to apply local context unit to semi-supervised and unsupervised learning in our future work.\n\n2) \"Until page 3 the authors refer to the context units a couple of times without giving some simple explanation of what it could be. A simple explanation in the introduction would improve the writing.\" && \"the authors could include more citations (e.g. the d-LSTM and the CNN based methods from Table 2) and explain the qualitative differences between their method and existing ones.\"\n\nWe have deeply rewrited the introduction section and added citations related to ours.\n\n3)\"The authors should consider adding equation numbers. The equation on the bottom of page 3 is fine, but the expressions in 3.2 and 3.3 are weird. A more concise explanation of the context-word region embeddings and the word-context region embeddings would be to instead give the equation for r_{i,c}. \"\n\nThanks for your suggestions, equation numbers have been added, and expressions in 3.2 and 3.3 have been updated. We also add explanations and discussions in 3.2 to make this paper clearer.\n \n4)\"I found the idea of multi region sizes interesting, but no description is given on how exactly they are combined. Since it works so well, maybe it could be promoted into the method section? Also, for each data set, which region size worked best?\"\n\nDetailed information about multi region sizes has been added at section 4.5.1. The gains of the multi region sizes method are not so large. As a natural extend for our core idea, we prefer to discuss it in the exploratory experiments sections. Performances for each data set with different region size have been reported in Appendix.A now.\n\n5)\"Are there some words for which all columns are similar meaning that their position is less relevant in how they affect nearby words? And then for other words with variation along the columns of the context units, do their context units modulate the embedding more when they are closer or further away? \"\n\nWe have discussed this issue at section 4.5.4 and added words whose positions are less relevant in how they affect nearby words, which is consistent with our previous hypothesis. As for the second question, we have evaluated the entire vocabulary from the perspective of statistics and no obvious differential distribution built on different columns. In fact, it seems size of half of region characters expression patterns more, instead of strong distance distinction.\n\n", "title": "Response to ICLR 2018 Conference Paper65 AnonReviewer3"}, "B1_2OO9QG": {"type": "rebuttal", "replyto": "Sy6ClHqef", "comment": "Thank you very much for suggestions and meticulous corrections for this paper. Your description of our work is accurate. We have addressed each of your comments:\n\n1) Didn't report training time.\nWe have reported the training time for each dataset with different region sizes in appendix A.\n\n2) Didn't report number of parameters.\nParameters have been discussed in 4.5.1, and reported in appendix A for different settings.\n\n3) Typos and ungrammatical sentences in the paper.\nWe have greatly improved the writing of this paper, including all the typos you pointed out and others textual errors. We will continue to improve the writing quality before the camera ready version.\n\n4) The lack of discussion of the \"attention is all you need\" paper.\nThis work has been discussed now in the related work section.\n\n5) Should use different letters for \"word embeddings\" and the \"projected word embeddings\"\nWe have improved the notations to make the paper clearer.\n", "title": "Response to ICLR 2018 Conference Paper65 AnonReviewer2"}, "rJGB_OqXz": {"type": "rebuttal", "replyto": "ryjxrEwlM", "comment": "Thanks for your valuable comments. We will explain your concerns point by point:\n\n1)\"The authors write 'Optimal hyperparameters are tuned with 10% of the training set on Yelp Review Full dataset, and identical hyperparameters are applied to all datasets' -- is this true for *all* models, or only the proposed approach? \"\n\nThis is true  only for the proposed approach. Results of the previous methods are their best results reported in corresponding previous papers, in which the hyperparameters are not identical for each datasets. (reference section 3.3 in D-LSTM, Table 5 in VDCNN, Table 1 and second paragraph of section 3.1 in FastText, section 4.2 in char-CRNN).\n\n\n2) \"The gains here appear to be consistent, but they seem marginal. The biggest gain achieved over all datasets is apparently .7, and most of the time the model very narrowly performs better (.2-.4 range). Moreover, it is not clear if these results are averaged over multiple runs of SGD or not.\"\n\nThe result are averaged over multiple runs. We have experimented the performance variance in independent tries on yelp datasets, the results are reported in Appendix. A.\n\nIn this paper, we want to show that, with the ability of word-specific contextualization given by our proposed local context unit, our simple model can consistently beats or achieves the state-of-the-art results on almost all text classification tasks against to previous methods (traditional and deep models). This gives us an insight to represent and understand natural language by word specific context units in our future work. Therefore, we didn't use any trival tricks (e.g. multi-region-size which have been proved can improve the performance) and extra regularization methods. In fact, the gains are not so marginal since the best previous method's gains are similar and even less than ours on some datasets.\n\n3)The related work section seems light.\n\nWe have improved and completed related work section now.\n\n4)The two approaches to inducing the final region embedding (word-context and then context-word in sections 3.2 and 3.3, respectively) feel a bit ad-hoc. I would have appreciated more intuition behind these approaches. \n\nThe main intuition we compose the region embeddings in two approaches is the flowing: We consider the semantic of a given region is derived from the mutual influences of the words in this region. Since the regions can be regarded as snapshots of a window sliding on a document, whose middle words are contiguous,  we can just focus on the middle word influences on the context words, or the context words' influences on the middle word. According to the property of local context unit introduced in section 3.1, embeddings and units are used in two ways to address these influences, respectively. Finally, to extract the most predictive information and then produce a fixed length vector representation, a max pooling operation is used.\n\nWe also revised sections 3.2 and 3.3 of this paper.", "title": "Response to ICLR 2018 Conference Paper65 AnonReviewer1"}, "Sy29H_5Qz": {"type": "rebuttal", "replyto": "BkSDMA36Z", "comment": "Thank you very much for reviewing our submission and making so many valuable comments. We also thank people for their attention to our work and their experiments in reproducing our results.\n\nWe have addressed all the issues pointed out by you and now the submission is of quite high quality.  Could you please review our submission again?  We hope that our submission will be accepted.\n\nWith the help from our colleagues, we have significantly improved the writing of the paper.  The title has been modified to better describe the main contribution of the work. The abstract and body have also been significantly revised accordingly. \n\nWe further plan to have a native speaker to conduct proof reading on our paper, if it is accepted. \n\nBelow is a summary of the major changes.\n\n1. Title has been changed to \u201cA new method of region embedding for text classification\u201d.\n\n2. Abstract and introduction have been deeply revised. The expression has been improved, and a more clear explanation about the local context unit has been added in the introduction.\n\n3. In the related work section, discussion about \"Attention is all you need\" paper and citations including xx have been added. \n\n4. In method, we have improved the notations. including the notations of projected embedding (e_{w_i}^j - > p_w{w_i}^j), region size(c -> 2 * c +1).  We also added equation numbers and refine the equations in 3.2 and 3.3.  More explanations and discussions about the intuition behind the approaches we produce the region embeddings have been added in 3.2.\n\n5. We have added information about the datasets(average document lengths), implement details(multi-region-sizes mode), more cases about context units visualization, experimental results(training time and parameters numbers, best region sizes) in experiments section and appendix A. Figure 2 and Figure 3 are put next to each other to save space.\n", "title": "Submission Update 2017-01-03: Summary of Changes"}, "HyE1mxEGf": {"type": "rebuttal", "replyto": "B1QjRyMGG", "comment": "Thank you very much for the reproducing experiments and suggestions about this paper. We have updated our code and discussed the common issues about the reproducibility. \nIn summary, within 1% variance gap can be explained by the 90% training data in the published configure as default, while we use 100% in the paper results. \nPlease see our latest comments to get more detailed information.\n\nThank you for pointing out, we have fix the stop condition issue in train.py and refined the code.\n", "title": "Reply"}, "SJphme4Mz": {"type": "rebuttal", "replyto": "Sy-OR3-fM", "comment": "Thank you very much for the reproducing experiments and suggestions about this paper. We have updated our code and discussed the common issues about the reproducibility. \nIn summary, within 1% variance gap can be explained by the 90% training data in the published configure as default, while we use 100% in the paper results. \nThe problem of slow convergence may be caused by the learning rate. In the example configure it was 1e-5 which declared 1e-4 in the paper.\nThe significant different reproducing results on DBPedia and AG News can be explained by the preprocess bug in our published code, and we have fixed it.\nPlease see our latest comments to get more detailed information.\n\nWe have add scalar mode context unit in our code, which should not take 2 days long to train the model until convergence, could you refer our implementation or share yours so we can find out the problem?\n", "title": "Reply"}, "BkQabg4MM": {"type": "rebuttal", "replyto": "Hk992WfGz", "comment": "Thank you very much for the reproducing experiments and suggestions about this paper. We have updated our code and discussed the common issues about the reproducibility. \nIn summary, within 1% variance gap can be explained by the 90% training data in the published configure as default, while we use 100% in the paper results. \nThe problem of slow convergence may be caused by the learning rate. In the example configure it was 1e-5 which declared 1e-4 in the paper.\nThe significant different reproducing results on DBPedia and AG News can be explained by the preprocess bug in our published code, and we have fixed it.\nPlease see our latest comments to get more detailed information.\n\nCould you please share which datasets you applied on FastText Uni. & Bigram with different embedding sizes? Since some datasets like DBPedia were preprocessed incorrectly, experiments on these datasets may lead different conclusion. \n\nInterestingly\uff0cwe found there is a hidden part in the .tex file (%experiment notes part)of the original FastText paper(https://arxiv.org/abs/1607.01759, click other formats link). Embedding size 10 is better than 100 for both unigram&bigram in Fasttext.\n\nModel && AG & Sogou & DBP & Yelp P. & Yelp F. & Yah. A. & Amz. F. & Amz. P. \\\\\n%Ours, $h=100$                         && 91.0 & 92.6 & 98.2 & 92.9 & 59.6 & 70.7 & 55.3 & 90.9 \\\\\n%Ours, $h=100$, bigram                 && 92.4 & 96.4 & 98.5 & 95.7 & 63.7 & 71.9 & 59.2 & 94.5 \\\\\n\\texttt{fastText}, $h=10$             && 91.5 & 93.9 & 98.1 & 93.8 & 60.4 & 72.0 & 55.8 & 91.2 \\\\\n\\texttt{fastText}, $h=10$, bigram     && 92.5 & 96.8 & 98.6 & 95.7 & 63.9 & 72.3 & 60.2 & 94.6 \\\\\n\nFrom this result we can make the consistent conclusion with our paper.\nWe have add multi-region size mode in our code and we will add more implement details in our paper, thank you for your suggestion! \nIs there a typo of the results you reported on Yahoo! Answers and Yelp Review? the numbers seems not similar with the results reported in this paper.\n", "title": "Reply"}, "HJvnFyVMG": {"type": "rebuttal", "replyto": "SyUEJzzfM", "comment": "Thank you very much for the reproducing experiments. We have updated our code and discussed the common issues about the reproducibility. \nIn summary, within 1% variance gap can be explained by the 90% training data in the published configure as default, while we use 100% in the paper results. \nThe problem of slow convergence may be caused by the learning rate. In the example configure it was 1e-5 which declared 1e-4 in the paper.\nPlease see our latest comments to get more detailed information.\nThank you for your suggestions, we have refined the code with more guild-lines and comments.", "title": "Reply"}, "ryfTd1Nfz": {"type": "rebuttal", "replyto": "SklnG7Mff", "comment": "Thank you very much for the reproducing experiments. We have updated our code and discussed the common issues about the reproducibility. \n\nIn summary, within 1% variance gap can be explained by the 90% training data in the published configure as default, while we use 100% in the paper results. \n\nThe problem of slow convergence may be caused by the learning rate. In the example configure it was 1e-5 which declared 1e-4 in the paper.\n\nPlease see our latest comments to get more detailed information.\n\nThank you for your suggestions about participating in some competitions, we will consider it!", "title": "Reply"}, "BkA08yEGG": {"type": "rebuttal", "replyto": "BkSDMA36Z", "comment": "Thank you very much for the effort on reproducing experiments and suggestions for this paper. \n\nAlthough results on most datasets were reported reproducible, we have updated our code to reproduce more consistent experimental results straightly(including the exploratory experiments). Due to the time limit, the previous version of the shared code is not complete clear enough, we have update the code: 1)fixed a bug in the preprocess code which leads significant difference on DBPedia and AG News; 2)added exploratory experiments module, 3)published training configures for each dataset 4)added guild-lines and comments for the code. The latest code can be pulled from the same repository. We will also update this paper in a few days.\n \nWe reply issues about reproducibility here together:\n1. Significant difference of reproducing results on DBPedia and AG News:\nWe find a bug in the public version of prepare.py that we treat the raw csv input files as two-columns for all datasets, while some of them are not. \nThis bug is caused by our negligence during migrate the code from internal version(which worked as expected) to public version. Unfortunately, we only verified the public version code on Yelp datasets which are two-columns files. This bug may lead the significant different reproducing results on DBPedia and AG News. We are very sorry for this bug in the shared code and now it has been fixed and verified reproducible.\n\n2. 1% variance on most reproducing result:\nAlthough similar results(variance within 1% on accuracy) have been reported on most datasets, the training data were defaultly set to 90% training data to tune the hyperparameters  in the example configure. Since we only tuned the hyperparameters on Yelp F and applied these hyperparameters on all datasets,  the results reported in this paper are trained by 100% training data, this can explain the variance on accuracy in reproducing results. And we have set 100% training data as default value in the new version of config.\n\n3. Training time:\nWe have listed the training time and best epoch with different region sizes of each dataset in our paper(will be upload in a few days). In our experiments, it usually converges at 2 or 3  epoch with learning rate 1e-4 instead of more than 20 epochs, we are not sure whether it misled people that the initial learning rate in the example configure was 1e-5 which declared 1e-4 in the paper. Ignored the extra look up operation, the computational complexities of the proposed methods are basically the same magnitude with CNN. However, the shared code was not well optimized which may lead somehow slower in practice.\n\n4. Hyperparameters:\nWe have tuned hyperparameters on Yelp. F. and applied them on all datasets, so there may be better hyperparameters for a given dataset. We chose the region size as 7 since we found that the performances with region size 7 and 9 were almost the same and 7 needed less model parameters, and similar with the embedding size.\n\n5. Reproduction for baseline methods:\nWe have implemented some baseline models and achieved similar results with small fluctuations, considering the consistency of the comparison and lack of implementation details of some models, we reported the best results from previous works instead of reproducing all the baseline models.\n", "title": "Reproducibility official explanation"}}}