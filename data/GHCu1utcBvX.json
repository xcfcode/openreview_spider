{"paper": {"title": "Transferability of Compositionality", "authors": ["Yuanpeng Li", "Liang Zhao", "Joel Hestness", "Ka Yee Lun", "Kenneth Church", "Mohamed Elhoseiny"], "authorids": ["~Yuanpeng_Li2", "~Liang_Zhao2", "~Joel_Hestness2", "kayeelun@gmail.com", "~Kenneth_Church1", "~Mohamed_Elhoseiny1"], "summary": "", "abstract": "Compositional generalization is the algebraic capacity to understand and produce large amount of novel combinations from known components. It is a key element of human intelligence for out-of-distribution generalization. To equip neural networks with such ability, many algorithms have been proposed to extract compositional representations from the training distribution. However, it has not been discussed whether the trained model can still extract such representations in the test distribution. In this paper, we argue that the extraction ability does not transfer naturally, because the extraction network suffers from the divergence of distributions. To address this problem, we propose to use an auxiliary reconstruction network with regularized hidden representations as input, and optimize the representations during inference. The proposed approach significantly improves accuracy, showing more than a 20% absolute increase in various experiments compared with baselines. To our best knowledge, this is the first work to focus on the transferability of compositionality, and it is orthogonal to existing efforts of learning compositional representations in training distribution. We hope this work will help to advance compositional generalization and artificial intelligence research.", "keywords": ["Compositionality"]}, "meta": {"decision": "Reject", "comment": "This work considers an apparent problem with current approaches to compositional generalisation (CG) in neural networks. The problem seems to be roughly:\n1. prior work in CG aims to extract 'compositional representations' from the training distribution\n2. work on CG, the training set and the test set are drawn from different distributions\ntherefore\n3. we don't know whether these models can also extract compositional representations from the test distribution\n\nAll four expert reviewers were, to differing degrees, confused by this problem framing, largely because they consider the premise (1) to be false. \n\nI am also aware of a large body of recent work on CG in neural networks (see those papers listed by R2) and, as far as i know, none of it involves extracting 'compositional representations' from the training set. Rather, it involves learning something (from the training set) that enables strong performance on a test set that differs from the training set in a way that is informed by ideas of compositionaity. \n\nAs far as I know, there are very few  studies that try to identify compositionality by considering the internal representations of neural networks, so it feels incorrect to claim this is standard practice. Any work that goes down this route ought to have a very thorough treatement of the various thorny philosophical and theoretical treatments of compositionality in the literature. As pointed out by R4, the work in its current form does not do this. \n\nIn summary, this work attempts to solve a problem that none of the four expert reviewers consider to be in need of a solution. "}, "review": {"7_3ehygfkjK": {"type": "review", "replyto": "GHCu1utcBvX", "review": "*Summary*\n\nThis paper proposes an architecture that addresses transferability of compositionality. The proposed architecture consists of three components: a network that transforms the input X into a series of hidden representations {H_1, H_2, ... H_K}, a network that reconstructs the input X from this series of hidden representations, and a prediction network that generates a prediction from the hidden representations. The authors propose several datasets meant to address transferability of compositional generalisation, and show that their architecture significantly improves standard DNN architectures as well as humans on these datasets.\n\n*Motivation for score*\n\nCompositional generalization in neural networks is a relevant and hot topic, with still many open questions. This paper aims to contribute on this topic and proposes some interesting datasets. However, However, despite citing several papers addressing compositionality in neural networks in the related work section, I am not convinced that the authors have properly understood the questions that are asked in this domain and were able to address them properly. Below, I outline my concerns.\n\n1. Definition of compositionality\n\nI do not find the definition of compositionality that the authors propose well motivated.\n- None of the three papers cited in the introduction to motivate the work actually has the word \"compositionality\" in the paper\n- The authors claim that previous work has focused just on whether models can extract compositional representations in the training distribution while ignoring the test distribution, while actually most recent papers they cite in related work test compositionality by considering very specific train/test splits\n- The author's definition of compositional generalisation does not seem to take into account that compositionality is traditionally a property of mapping between input and output, not of a model itself. In addition to that, whether the mapping between input and output is compositional does not depend on what is in the train and what is in the test distribution. A model can understand the compositional structure of a dataset also if it has been trained on *all* examples of the dataset, only it will be impossible to behaviourally evaluate if it has. For this reason, much previous work on compositionality in neural networks has created datasets where the training and testing data were distributionally different (as also the authors of this paper do).\nI would recommend the authors to have a look at the paper _Compositionality decomposed: how do neural networks generalise?_ (Hupkes et al.; 2020), for a detailed account of compositionality in the context of neural networks. In particular, their section on _localism_ is particularly important for the author's definition of compositionality.\n\n2. Architecture\n\nI find the proposed architecture interesting, but it is not completely clear to me how it differs from an auto-encoder setup where the encoding is larger than the input instead of smaller (it is very well possible I misunderstood). Nevertheless, it can be interesting to see if auto-encoding based architectures behave better on datasets proposed to evaluate compositionality. One thing that is not clear to me is how the number of components _K_ is determined.\n\n3. Data\n\nI appreciate the effort of the authors to design new datasets that test out-of-distribution generalisation. I do have a few comments/questions:\n- If the main motivation for wanting compositional generalisation is that this is an important capacity of humans, isn't it a problem that humans perform very poorly on the dataset (much worse than the best deep neural network)?\n- What is the motivation for using a new dataset, rather than one of the previously proposed datasets for out-of-distribution generalisation?\n- It is nice that the authors try to include tests from different domains, but I think that calling a dataset mapping inputs like \"januarymarch\" to (0, 2) cannot really be called \"natural language processing\"\n\nOverall, I do not believe that this paper should be accepted for the conference.\n\n_**Update after author response:** I have read the author response, but do not find that the answers really address my concerns.  I have also not really seen any improvements in the paper itself._ ", "title": "Paper about a relevant topic, but with insufficient motivation and grounding in previous work", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "XrAE1BQ--0G": {"type": "rebuttal", "replyto": "LwA2Gc3fsA5", "comment": "Thank you for the question.\n\nA: Here, we just show there exist transferring problems in some compositional generalization tasks, but the problems may not appear in other settings. Also, in previous work with downstream tasks, the focus is on disentangled representation learning. It is likely that the training dataset has marginal independence between underlying factors. So the settings are quite different.\n", "title": "Reply to Reviewer 3"}, "Rc1e1wY1Ntq": {"type": "rebuttal", "replyto": "7_3ehygfkjK", "comment": "Thank you for helping us to improve the paper.\n\nQ1: I do not find the definition of compositionality that the authors propose well motivated.\nNone of the three papers cited in the introduction to motivate the work actually has the word \"compositionality\" in the paper\n\nA1: It is mainly motivated from Lake et. al. 2017.\n\nQ2: The authors claim that previous work has focused just on whether models can extract compositional representations in the training distribution while ignoring the test distribution, while actually most recent papers they cite in related work test compositionality by considering very specific train/test splits\n\nA2: The previous work either uses language examples where the transferring problem is not prominent, or does not measure compositional generalization.\n\nQ3: The author's definition of compositional generalisation does not seem to take into account that compositionality is traditionally a property of mapping between input and output, not of a model itself.\n\nA3: We consider the input to output compositional generalization approach with compositional hidden representations.\n\nQ4: I would recommend the authors to have a look at the paper Compositionality decomposed: how do neural networks generalise? (Hupkes et al.; 2020)\n\nA4: Thank you. We will look into it.\n\nQ5: I find the proposed architecture interesting, but it is not completely clear to me how it differs from an auto-encoder setup where the encoding is larger than the input instead of smaller (it is very well possible I misunderstood). Nevertheless, it can be interesting to see if auto-encoding based architectures behave better on datasets proposed to evaluate compositionality.\n\nA5: Auto-encoding is for unsupervised problems, and we consider general supervised learning with different input and output. We will investigate how to compare with auto-encoding methods.\n\nQ6: One thing that is not clear to me is how the number of components K is determined.\n\nA6: K is given as prior knowledge in this case, and we consider this as a part of the training problem. We focus on the transfer problem in inference.\n\nQ7: If the main motivation for wanting compositional generalisation is that this is an important capacity of humans, isn't it a problem that humans perform very poorly on the dataset (much worse than the best deep neural network)?\n\nA7: The main motivation is that capability of compositional generalization itself. The human performance shows the task is a difficult one.\n\nQ8: What is the motivation for using a new dataset, rather than one of the previously proposed datasets for out-of-distribution generalisation?\n\nA8: Previous dataset for out-of-distribution generalization, such as SCAN dataset, have word unit based language tasks where the transferring problem is not prominent.\n\nQ9: It is nice that the authors try to include tests from different domains, but I think that calling a dataset mapping inputs like \"januarymarch\" to (0, 2) cannot really be called \"natural language processing\"\n\nA9: We would modify this as a language problem.\n", "title": "Reply to Reviewer 4"}, "jv1P4mt8Nc0": {"type": "rebuttal", "replyto": "xGHcvStyzNG", "comment": "Thank you for the constructive suggestions.\n\nQ1: Positioning with the respect to the prior work. The literature on learning object-oriented representations is not cited. The work on disentangled representation is cited, but still new setups are created from scratch.\n\nA1: We will cite the object-oriented representation work. The disentangled representation setup does not consider compositional generalization.\n\nQ2: Related to the previous point, the use of full supervision (in the form of labels) in the proposed tasks strikes me as a deviation from most previously used setups.\n\nA2:  We use supervision to focus on the transfer problem during inference.\n\nQ3: Viewing the work as an ML paper, one thing that stands out is the lack of connections to any applied ML problem.\n\nA3: This paper focuses on discussing the problem of transferability and finding fundamental mechanisms to address it. We do not extend to applications with compositional generalization here.\n\nQ4: I think the negative results in the paper would look stronger if pretrained image- and language- processing models were used in all experiments (e.g. Contrastive Predicting Coding & BERT) The proposed method seems appropriate for the tasks that the paper considers.\n\nA4: Thank you for the suggestion. We will look into it.\n\nQ5: My main concerns are thus focused on the motivation of the proposed tasks themselves and the positioning with respect to their prior work. I think a great direction to improve the paper would be to add experiments without supervised learning and using 3D-rendered images with multiple objects as it is done e.g. in [1] and [2].\n\nA5: We will investigate it.\n\nQ6: Algorithm 1 is very confusing because sample-level steps 1-4 are mixed with dataset-level steps 5 and 6.\n\nA6: Thank you for mentioning. We will clarify.\n\nQ7: A confusing sentence in the intro: \u201cFor a test sample, we regularize each hidden representation in its training manifold, and optimize them to recover the original input\u201d\n\nA7: This sentence is the motivation of the approach, addressing the previous sentence \u201ceach extracted representation shifts away from the corresponding one in training\u201d.\n\nQ8: for colored digits experiments you might want to compare to and cite [3]\n\nA8: Thank you, we will find more details.\n", "title": "Reply to Reviewer 1"}, "qHKKNMieYrN": {"type": "rebuttal", "replyto": "kJbEvyHG9U", "comment": "Thank you for the comments.\n\nQ1: The authors frequently say that compositionality may not transfer to test distribution but I have a hard time understanding exactly what they mean by this. As I understand it, \"compositionality\" is a property of a representation. Do authors mean that, on the test data, the representation of an input is able to separate multiple components, yet the same network does not separate the components on the test data? It may be true for the models they trained here, but I would have appreciated a comparison with other methods. As such, I find the claims of this paper difficult to evaluate with respect to previous work. They claim theirs is the first work for the transferability problem of compositionality which I find really hard to believe. I would have appreciated a thorough study of the \"compositionaly\" limitations of previous techniques.\n\nA1: Your understanding of compositionality is correct. We focus on the case to use compositional representation to achieve compositional generalization, and this is the first work to focus on the transferability in such cases.\n\nQ2: I found section 4 particularly hard to understand. A lot of symbols, equations and nomenclatures seem to be used with too little introduction. As a result, I cannot vouch for the correctness of this section.\n\nA2: We will make them more clear.\n\nQ3: Given their claim that this is the first work for the transferability problem of compositionality the experiments presented on section 5 are on a new dataset and are not compared to previous work. Moreover, the proposed experiments seem relatively simple (two overlapped MNIST digits, colored MNIST digits, concatenated month names) and their baseline seem trivial (we use a standard neural network with two sub networks, each for an output)\n\nA3: We focus on the compositional generalization with compositional representation, where the works are still in the stage of using illustrative examples to find the fundamental mechanisms.\n\nQ4: Typos. I find the text difficult to read, it would benefit from a thorough revision.\n\nA4: We will make them more clear.\n", "title": "Reply to Reviewer 3"}, "w7yzYJh846L": {"type": "rebuttal", "replyto": "2oUn545DV9i", "comment": "Thank you for your feedback.\n\nQ1. The submission claims that other works that investigation compositionality in representation learning do not actually test compositional generalization (\"because all combinations have positive joint probabilities in training\"). However, I disagree that this is the case in prior work.\n\nA1: Existing methods tend to either measure compositionality but do not measure compositional generalization, or do not use compositional representation. We focus on the case to use compositional representation to achieve compositional generalization.\n\nQ2: The algorithmic components in Section 4 are not adequately motivated, and the relationship of the algorithm to prior work in compositional representation learning is not discussed.\n\nA2: The motivation of the algorithm is in the last two paragraphs of the Introduction section.\n\nQ3: The evaluation tasks are extremely simple.\n\nA3: The compositional generalization works with compositional representation are still in the stage of using simple examples to find the fundamental mechanisms.\n\nQ4: \"The main approach for compositional generalization is to learn compositional representations\" Is this really the \"main approach\"?\n\nA4: We will change that this is one approach.\n\nQ5: \"We find that the extraction ability does not transfer naturally, because the extraction network suffers from the divergence of distributions\" Why is it assumed here that there is an extraction network? The \"extraction network\" is referred to several times in the introduction and methods section prior to its introduction/explanation.\n\nA5: We focus on the approach with compositional representation, so there is an extraction network.\n\nQ6: \"compositional generalization is a type of out-of-distribution (o.o.d.) transferring or generalization, which is also called domain adaptation\" This is inconsistent with the previously discussed definition of compositional generalization i.e., that it is not just domain adaptation.\n\nA6: The next sentence after the cited sentence explains the distinction between OOD generalization and compositional generalization: \u201cA sample in such a setting (compositional generalization) is a combination of several components, and the generalization is enabled by recombining the seen components of the unseen combination during inference.\u201d\n\nQ7: \"We propose to obtain compositional representations not from the extractor but reversely from an auxiliary network.\" At this point, neither the \"extractor\" nor the \"auxiliary network\" are defined.\n\nA7: We will make them clear.\n\nQ8: \"These networks can be some existing networks for compositionality learning\" If so, what are examples of \"existing networks for compositionality learning\"?\n\nA8: Russin et al. 2019 and Li et al. 2019 can be examples, but they are designed for word unit based language tasks as mentioned in the paper.\n", "title": "Reply to Reviewer 2"}, "xGHcvStyzNG": {"type": "review", "replyto": "GHCu1utcBvX", "review": "The paper introduces a \u201ctransferability of compositionality\u201d problem and proposes an approach to alleviate it. The said problem may arise when one trains neural models to produce \u201ccompositional\u201d representations of the input. In the paper \u201ccompositional representations\u201d consist of multiple vectors which are supposed to correspond to semantically meaningful aspects of the input, for example different objects in the case of images or different parts of compound words in the case of linguistic inputs. The transferability problem arises when there is a difference between training and test distributions, namely when certain combinations of objects have different probabilities in training & testing. The proposed solution at inference time is to project object representations to the manifold of individual object representations. The manifold is estimated by saving representations of individual object representations from the training time. \n\nThe problem that the paper considers is an interesting one. There have been a lot of papers on learning object-oriented representations recently [1, 2], and an implicit assumption in all these works is that there is no statistical dependency between which objects that occur in the scenes. There is also the literature on disentangled representations that the paper extensively cites, where the independence assumption is also common. \n\nMy concerns regarding the paper are as follows: \n- Positioning with the respect to the prior work. The literature on learning object-oriented representations is not cited. The work on disentangled representation is cited, but still new setups are created from scratch. \n- Related to the previous point, the use of full supervision (in the form of labels) in the proposed tasks strikes me as a deviation from most previously used setups. Previous work aimed to learn compositional representations without supervision, often positioning their efforts as a cog-sci-style inquiry in building human-like models. The use of labels makes this look less like a cog-sci and more like a machine learning paper. Viewing the work as an ML paper, one thing that stands out is the lack of connections to any applied ML problem. \n- I think the negative results in the paper would look stronger if pretrained image- and language- processing models were used in all experiments (e.g. Contrastive Predicting Coding & BERT) \n\nThe proposed method seems appropriate for the tasks that the paper considers. The experiments appear to be technically sound. My main concerns are thus focused on the motivation of the proposed tasks themselves and the positioning with respect to their prior work. I think a great direction to improve the paper would be to add experiments without supervised learning and using 3D-rendered images with multiple objects as it is done e.g. in [1] and [2]. \n\nFew comments on writing: \n- Algorithm 1 is very confusing because sample-level steps 1-4 are mixed with dataset-level steps 5 and 6. \n- A confusing sentence in the intro: \u201cFor a test sample, we regularize each hidden representation in its training manifold, and optimize them to recover the original input\u201d\n- for colored digits experiments you might want to compare to and cite [3] \n\n- [1] \u201cMulti-Object Representation Learning with Iterative Variational Inference\u201d by Greff et al, 2020\n- [2] \u201cMONet: Unsupervised Scene Decomposition and Representation\u201d by Burgess et al, 2019\n- [3] \u201cInvariant Risk Minimization\u201d by Arjovsky et al, 2019", "title": "the paper considers an interesting general problem, but the concrete supervised learning instantiation is problematic", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "kJbEvyHG9U": {"type": "review", "replyto": "GHCu1utcBvX", "review": "## Summary\n\nThis paper studies \"compositionality\" and in particular the way in which it \"transfers\" on test data. They run simple baselines on three experiments (overlapped MNIST, colored MNIST and concatenated month names) and find that the baselines do not learn compositional representation. They proposed the use of an *auxiliary reconstruction network and a regularized optimization* which improves on these baselines. \n\n## Analysis\n\nThe authors frequently say that *compositionality may not transfer to test distribution* but I have a hard time understanding exactly what they mean by this. As I understand it, \"compositionality\" is a property of a representation. Do authors mean that, on the test data, the representation of an input is able to separate multiple components, yet the same network does not separate the components on the test data? It may be true for the models they trained here, but I would have appreciated a comparison with other methods. As such, I find the claims of this paper difficult to evaluate with respect to previous work. They claim theirs is the *first work for the transferability problem of compositionality* which I find really hard to believe. I would have appreciated a thorough study of the \"compositionaly\" limitations of previous techniques.\n\nI found section 4 particularly hard to understand. A lot of symbols, equations and nomenclatures seem to be used with too little introduction. As a result, I cannot vouch for the correctness of this section.\n\nGiven their claim that this is the *first work for the transferability problem of compositionality* the experiments presented on section 5 are on a new dataset and are not compared to previous work. Moreover, the proposed experiments seem relatively simple (two overlapped MNIST digits, colored MNIST digits, concatenated month names) and their baseline seem trivial (*we use a standard neural network with two sub networks, each for an output*) \n\n## Conclusion\n\nOverall, I find the claim of this paper substantial, while the experiments are relatively simple with trivial baselines and an absence of comparison to related work.\n\n## Typos\n\nI find the text difficult to read, it would benefit from a thorough revision. Ex: *This work is orthogonal to many efforts of learning compositionality in training distribution.*\n", "title": "Review of Transferability of Compositionality", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "2oUn545DV9i": {"type": "review", "replyto": "GHCu1utcBvX", "review": "##### Impact:\n\nThe submission claims that other works that investigation compositionality in representation learning do not actually test compositional generalization (\"because all combinations have positive joint probabilities in training\"). However, I disagree that this is the case in prior work; here are some examples of prior works that correctly hold out novel combinations (of underlying components) for test time:\n- https://openreview.net/forum?id=HJz05o0qK7\n- http://papers.nips.cc/paper/8825-learning-by-abstraction-the-neural-state-machine\n- https://arxiv.org/abs/1910.09113\n- https://arxiv.org/abs/1912.09713\n- https://arxiv.org/abs/1912.12179\n\nThere are many such examples; they are too numerous to list here.\n\n##### Quality: \n\nThe algorithmic components in Section 4 are not adequately motivated, and the relationship of the algorithm to prior work in compositional representation learning is not discussed.\n\n  The evaluation tasks are extremely simple (overlayed MNIST digits and conjoined word token) and are, as such, far from the complexity of existing work on compositionality (which can deal with, for example, naturalistic image data; see the references above for examples of such works).\n\n##### Clarity:\nThere are many points of ill clarity / inconsistencies; for example:\n- \"The main approach for compositional generalization is to learn compositional representations\" Is this really the \"main approach\"? Compositional generalization has been studied in many contexts outside of representation learning (e.g., see https://semanticsarchive.net/Archive/jcyZDc1Y/Goldberg.Compositionality.RoutledgeHandbook.pdf)\n\n- \"We find that the extraction ability does not transfer naturally, because the extraction network suffers from the divergence of distributions\" Why is it assumed here that there is an extraction network? The \"extraction network\" is referred to several times in the introduction and methods section prior to its introduction/explanation.\n\n- \"compositional generalization is a type of out-of-distribution (o.o.d.) transferring or generalization, which is also called domain adaptation\" This is inconsistent with the previously discussed definition of compositional generalization i.e., that it is not just domain adaptation. I think the submission could do with a better job of dealing with the distinctions between OOD generalization / domain adaptation / concept drift.\n\n- \"We propose to obtain compositional representations not from the extractor but reversely from an auxiliary network.\" At this point, neither the \"extractor\" nor the \"auxiliary network\" are defined.\n\n- \"These networks can be some existing networks for compositionality learning\" If so, what are examples of \"existing networks for compositionality learning\"?", "title": "The submission claims to be the first to truly test compositional generalization (ignoring all prior work that does) and fails to motivate an algorithm (whose relation to prior methods is not discussed) applied to toyish datasets (whose relations to existing evaluations that investigate compositionality are not discussed).", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}