{"paper": {"title": "FLAT MANIFOLD VAES", "authors": ["Nutan Chen", "Alexej Klushyn", "Francesco Ferroni", "Justin Bayer", "Patrick van der Smagt"], "authorids": ["nutan.chen@gmail.com", "a.klushyn@gmail.com", "francescoferroni1@gmail.com", "bayer.justin@googlemail.com", "smagt@argmax.ai"], "summary": "", "abstract": "Latent-variable models represent observed data by mapping a prior distribution over some latent space to an observed space.  Often, the prior distribution is specified by the user to be very simple, effectively shifting the burden of a learning algorithm to the estimation of a highly non-linear likelihood function. This poses a problem for the calculation of a popular distance function, the geodesic between data points in the latent space, as this is often solved iteratively via numerical methods. These are less effective if the problem at hand is not well captured by first or second-order approximations. In this work, we propose less complex likelihood functions by allowing complex distributions and explicitly penalising the curvature of the decoder. This results in geodesics which are approximated well by the Euclidean distance in latent space, decreasing the runtime by a factor of 1,000 with little loss in accuracy. \n", "keywords": []}, "meta": {"decision": "Reject", "comment": "The paper proposes to regularize the decoder of the VAE to have a flat pull-back metric, with the goal of making Euclidean distances in the latent space correspond to geodesic distances. This, in turn, results in faster geodesic distance computation. I share the concern of R2 that this regularization towards a flat metric could result in \"biased\" geodesic distances in regions where data is scarce. I suggest the authors discuss in the next version of the paper if there are situations where this regularization might have drawbacks and if possible, conduct experiments (perhaps on toy data) to either rule out or highlight these points, particularly about scarce data regions. "}, "review": {"rJlmU9O3iB": {"type": "rebuttal", "replyto": "rkekuz-3sB", "comment": "The reviewer\u2019s arguments are mainly based on the above mentioned paper \u201conly Bayes should learn a manifold\u201d (which is not peer reviewed). We have doubts about some points in that paper. There are alternative solutions (not \u201conly\u201d Bayes), as we mentioned above, to learn a manifold for a deterministic decoder Jacobian.\n\nWe have argued that our contribution is a fast, geodesics-inspired distance function based on generative models (we declared in the paper that a bit of geodesic accuracy is sacrificed). It is also demonstrated that it works empirically in relevant settings. We regret that this is not enough reason to refrain from a \u201cclear reject\u201d of the paper in your eyes.", "title": "Thanks for reviewing"}, "HJlXO5WwYr": {"type": "review", "replyto": "SkgWIxSFvr", "review": "Summary of paper:\nThe paper is concerned with the geometry of latent spaces in VAEs. In particular, it is argued that since geodesics (shortest paths) in the Riemannian interpretation of latent spaces are expensive to compute, then it might be beneficial to regularize the decoder (generator) to be flat, such that geodesics are straight lines. One such regularization is proposed.\n\nReview:\nI have several concerns with the paper:\n\n1) Geodesics are never motivated:\nThe paper provides no motivation for why geodesics are interesting objects in the first place, so it is not clear to me what the authors are even trying to approximate.\n\n2) Under the usual motivation, the work is flawed:\nThe usual motivation for geodesics is that they should follow the trend of the data (e.g. go through regions of high density). Since no other motivation is provided, I will assume this to be the motivation of the paper as well. The paper propose to use a flexible prior and then approximate geodesics by straight lines. Beyond the most simple linear models, then this cannot work. If the prior is flexible, then straight lines will hardly ever constitute paths through regions of high density. The core idea of the work, thus, seem to be in conflict with itself.\n\n3) A substantial bias is ignored:\nThe paper consider the Riemannian metric associated with the *mean* decoder. Due to regularization, holes in the data manifold will be smoothly interpolated by the mean decoder, such that geodesics under the associated metric will systematically be attracted to holes in the data manifold. Hauberg discuss this issue in great length here:\n\n  https://arxiv.org/abs/1806.04994\n\nHere it is also demonstrated that geodesics under the mean decoder tend to be straight lines (which is also what the authors observe). Taking the stochasticity of the VAE decoder into account drastically change the behavior of geodesics to naturally follow the trend of the data.\n\n4) Related work is mischaracterized:\nPrevious work on the geometry of latent spaces largely fall into two categories: those that treat the decoder as deterministic and those that treat it as being stochastic. In the cited papers Arvanitidis et al and Tosi et al consider stochastic decoders, while the other consider deterministic decoders. Given that geodesics have significantly different behavior in the two cases, it is odd that the difference is never discussed in the paper.\n\n5) It is not clear to me what the experiments actually show:\n\n-- I did not understand the sentence (page 5): \"The model is more invariant if the condition number is smaller...\" What does it mean to be \"more invariant\" ? And how is invariance (to what) related to the condition number of the metric?\n\n-- Figure 3 show example geodesics, but only geodesics going between clusters (I have no idea how such geodesics should look). If I look at the yellow cluster of Fig3a, then it seems clear  to me that geodesics really should be circular arcs, yet this is being approximated with straight lines. Are the ground truth geodesics circular? At the end, it seems like the shown examples are the least informative ones, and that intra-cluster geodesics would carry much more meaning.\n\n-- What am I supposed to learn from the \"Smoothness\" experiment (page 7) ? My only take-away is currently that the proposed regularization does what it is asked to do. It is not clear to me if what it aims to do is desirable? Does the experiment shed light on the desirability of the regularizer or is it more of a \"unit test\" that show that the regularizer is correctly implemented?\n\n-- In the \"Geodesic\" experiment (page 7) I don't agree with the choice of baseline. If I understand correctly, the baseline approximate geodesics with shortest paths over the neighbor graph (akin to Isomap). However, there is no reason to believe that the resulting paths bare any resemblance to geodesics under the studied Riemannian metric. The above-mentioned paper by Hauberg provide significant evidence that these baseline geodesics are not at all related to the actual geodesics of the studied metric. The only sensible baseline I can think of is the expensive optimization-based geodesics.\n\n== rebuttal ==\nI have read the rebuttal and discussed with the authors, and I retain my original score.", "title": "Official Blind Review #2", "rating": "1: Reject", "confidence": 4}, "HkeKVmyjiS": {"type": "rebuttal", "replyto": "rJg9E72qoH", "comment": "We expect that short (approximate) geodesics under the learned model indicate similarity of data points in question. We will add this to the last paragraph of the introduction.", "title": "Response about geodesics"}, "rkekX8OYiS": {"type": "rebuttal", "replyto": "SJeN-ZNl5r", "comment": "We appreciate all reviewers\u2019 opinions and suggestions very much and it enabled us to substantially improve our manuscript. \n\n2. Answer: If we understand correctly, the question is why we interpolate to extend the data in the entire latent space and input the data into the Jacobian regularisation. When we measure the distance between two points in the latent space, the region (latent space) in between of these points has to be regularised/smooth. To obtain such a regularised latent space, the regulariser needs data at these regions---otherwise the latent space might be folded/unsmooth. However, data is not always available (e.g., data between two clusters). Therefore, we augment the data for the entire latent space using mixup (a powerful and simple method based on interpolation). To constrain the latent space between any two points to be smooth (approximate constant MF), we use interpolated data as input for the Jacobian regularisation term in Eq (9).\n\n3. Answer: We do not put the Lipschitz continuity constraint on the decoder, but we prove that our decoder satisfies Lipschitz continuity. We have clarified it in the updated version.\n\n4. Answer: Thanks for pointing this out. We have modified the title to \u201cLearning flat latent manifolds with VAEs\u201d.\n\n5. Answer: We agree that this would be interesting but consider it beyond the scope of this work.  \n\n6. Answer: This experiment shows how our method is applied to a state-of-the-art algorithm in terms of measuring the distance. We have revised the tracking experiment to improve readability. \n\n7. Answer: The model can be relatively smooth without Jacobian normalisation, but the distance in the latent space cannot reflect the truth distance. We take Jogging and walking as an example. Jogging is a larger movement than walking in terms of the joints. Without the Jacobian normalisation,  the distribution of walking in the latent space is still larger than that of jogging, which is in conflict with the true distance.", "title": "Replies to Reviewer #3"}, "HJxzlWLFjS": {"type": "rebuttal", "replyto": "HJlXO5WwYr", "comment": "5)\n5.1 Question: \u2026 What does it mean to be \"more invariant\" ? And how is invariance (to what) related to the condition number of the metric?\n\nAnswer: \u201cMore invariant\u201d means the equidistance lines are more similar along different directions and more similar at different centres. As described in section 4 paragraph 2, the conditional number is computed by means of the metric tensor as well as the equidistance lines. We have clarified it in the paper.\n\n\n5.2 Question: Figure 3 shows example geodesics, but only geodesics going between clusters...\n\nAnswer: \nWe agree that it does not make sense to interpolate between clusters using the previous models. However, in our task, it is important to have a correct distance in the latent space between clusters, but the interpolation is just a tool to show show the results. Our proposed model corrects the metric between clusters, which makes it possible for interpolation between classes (see Fig. 6 for intuitive results). It is more difficult to interpret interpolation between clusters using image dataset (where there are clear separations between clusters), but more intuitive using MoCap data (where there is a path from each data point to each other data point). \n\nFor better observation, the data points (e.g. the yellow dots) are the mean output of the encoder. For the Geodesic without jacobian normalisation (Fig. 3b), it does not necessarily follow the mean of the trajectory. In addition, our method regularises to flatten the latent space, but it cannot completely flatten special data structure such as a circle. However, it already reduces a lot of unsafe (large MF) area. See the circle example in Fig. 1. There is still a tiny high MF area in the center.\n\n\n5.3 Question: What am I supposed to learn from the \"Smoothness\" experiment (page 7) ? ...\n\nAnswer: \nIf it is smooth, a continuous trajectory in the latent space is corresponding to a continuous trajectory in the observation space. In this case (e.g., two clusters would not squeeze together), we can safely measure the distance in the latent space.\n\n\n5.4 Question: In the \"Geodesic\" experiment (page 7) I don't agree with the choice of baseline...\n\nAnswer: \nAs shown in the paper [Chen et al., 2018a; Arvanitidis et al., 2018], the expensive geodesic method has not been developed for latent spaces with more than two dimensions. Therefore, we select the graph-based method for comparison. In addition, the number of graph nodes and neighbours influence the accuracy of the approximated geodesic. Similarly, the accuracy of ODE- and NN-based geodesic approximations depends on the step length as well.\n", "title": "Replies to Reviewer #2. Part 2"}, "ryl1AlLKiS": {"type": "rebuttal", "replyto": "HJlXO5WwYr", "comment": "We thank the reviewer for the valuable comments and suggestions. \n\n1) Answer: Thanks for pointing this out. We have added a clearer motivation. The main aim of the paper is approximating the geodesic sufficiently (sacrificing a bit of accuracy) but maintain high computational speed to enable useful applications. We use the Riemannian distance (with the geodesic as the shortest path) as an inspiration to develop a distance function which can be computed rapidly (1000 times faster than previous methods, see Sec. 4.2). This is of crucial importance in certain scenarios such as autonomous driving.\n\n2) Answer: We agree that the geodesic should follow the trend of the data and the variance of the decoder can improve the results. However, our goal is to achieve a homogeneous MF, and consequently Euclidean distance approximates to Riemannian distance. We use data augmentation (mixup) to fill the data into the missing data regions, so that there is no low density regions. While this certainly is a further inductive bias on the data distribution, e.g. it makes low density regions less likely to emerge during training, it is a heuristic that helps the experimental results.\n\n3) Answer: We agree that moving towards a fully stochastic decoder and using the appropriate geodesics framework is a promising direction. However, we are mainly interested in a distance function. Further, all experiments are done with a Gaussian likelihood with homoscedastic variance. Hence, the stochasticity of the decoder in this work is only used to explain the noise of the data, which certainly is not an interesting thing to reflect for estimating distances.  We have cited the paper mentioned above [Hauberg, 2019] and extend the paper regarding this direction. \n\nAs shown in Fig. 3b (green lines), the geodesics do NOT tend to be straight lines. This is only the case because of our contribution (see experiments in Fig. 3a). Therefore, our results are different to [Hauberg, 2019]. Regions in the latent space, which have a high MF are \u201cstretched\u201d by the Jacobian regularisation, and the distance between points is thus established in a different way. \n\nIn [Hauberg, 2019], the variance of the Jacobian is used to constrain the geodesic to follow the data manifold. However, there are alternative solutions for deterministic approaches like e.g. regularising the singular term of the SVD decomposition [Chen et al., 2018a].\n\n\n4) Answer:  We have added the difference in the related work. The main difference is that the stochastic methods work for the regions without data, because the RBF layer generates high MF for those. However, it is less general and post hoc. The uncertainty does not emerge from a principled way (such as in a Bayesian model) but is instead driven by certain assumptions. The deterministic method requires other strategies to guarantee that the geodesic is within the data manifold. In our proposed method, we regularise the latent space to have the same metric, so that we do not need to consider the region without data.\n", "title": "Title: Replies to Reviewer #2. Part 1"}, "r1xw1pSKjS": {"type": "rebuttal", "replyto": "rJxI645i_r", "comment": "We would like to thank the reviewer for the thorough and helpful reviews.\n\n\n  -This paper extends VHP-VAE with jacobian regularization, which is approximated (the paper doesn't say so but I think it's a first order taylor expansion).  \n\nAnswer: Thank you very much for pointing that out. We have added this to the manuscript.\n\n\n  -In figure 2, the background color indicates the degree of magnification (so the VAE-VHP has greater variability in distances?)  I found this figure a bit hard to itnerpret.  \n\nAnswer: We assume that the reviewer means Fig. 3 instead of Fig. 2. It is more intuitive to interpret the MF by means of the equidistance lines. The shape and size of the equidistance lines are more irregular if the variability distance is larger.\n\n\n  -This paper cites Mixup but there are two more papers to consider here: Manifold Mixup (ICML 2019) and Adversarial Mixup Resynthesis (Neurips 2019) which both considered mixing in a latent space.  AMR considered in an autoencoder, and Manifold Mixup is also relevant because its theoretical analysis explicitly considers flattening although in a somewhat difference sense (and both are different from what's done here).  \n\nAnswer: Thanks for the suggestion. We have cited these two papers in the related work.\n\n\n  -The object tracking experiments don't seem very convincing to me (just looking at table 2 at least).  \n\nAnswer: Table 2 shows both supervised learning and unsupervised learning methods. Usually supervised learning methods require labeled data, which is often not possible. Tasks such as autonomous driving are very data hungry and it is expensive to label the required data. Our unsupervised method (no labels required) is close to the supervised learning method (DeepSORT) and outperforms other unsupervised learning models. In four out of 16 metrics, our method outperforms the supervised model; In 11 out of 16 metrics, our model outperforms other non-supervised learning models.  We have revised Table 2 to highlight the results. \n", "title": "Replies to Reviewer #1"}, "rJxI645i_r": {"type": "review", "replyto": "SkgWIxSFvr", "review": "Notes: \n\n  -This paper suggests the use of VAEs with stronger priors along with more powerful regularization of the decoder (especially its curvature).  This lowering of curvature is seen as \"flattening\".  \n\n  -Paper uses the normal VAE ELBO.  \n\n  -The normal prior over-regularizes the approximate posterior.  One proposal is to use a \"hierarchical prior\": integral(p(z|zeta)*p(zeta), zeta) where zeta is a normal distribution.  So basically a function can transform the prior.  \n\n  -Importance weighting with q(z|x) has been proposed as a way to define a valid learning objective for this setting.  \n\n  -Another objective using lagrangian is called \"VHP-VAE\".  \n\n  -This paper extends VHP-VAE with jacobian regularization, which is approximated (the paper doesn't say so but I think it's a first order taylor expansion).  \n\n  -Paper also uses mixup in the latent space to provide regularization at points farther from the data.  \n\n  -With this mixup objective the mixing is also done to consider extrapolations in addition to interpolations.  \n\n  -The resulting latent space does indeed look much better (Figure 1).  \n\n  -The condition number is also way better (2a, 2b).  \n\n  -In figure 2, the background color indicates the degree of magnification (so the VAE-VHP has greater variability in distances?)  I found this figure a bit hard to itnerpret.  \n\nComments: \n\n  -This paper cites Mixup but there are two more papers to consider here: Manifold Mixup (ICML 2019) and Adversarial Mixup Resynthesis (Neurips 2019) which both considered mixing in a latent space.  AMR considered in an autoencoder, and Manifold Mixup is also relevant because its theoretical analysis explicitly considers flattening although in a somewhat difference sense (and both are different from what's done here).  \n\n  -The object tracking experiments don't seem very convincing to me (just looking at table 2 at least).  \n\nReview: \n\n  This paper considers augmenting the hierarchical VHP-VAE with a criteria in which the jacobian is approximately regularized at interpolations and extrapolations between different points in z space.  The experiments suggest this is an important problem with VHP-VAE and also that it's successfully addressed.  ", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "SJeN-ZNl5r": {"type": "review", "replyto": "SkgWIxSFvr", "review": "1.\tThe idea of explicitly forcing the encoding space to be flat by putting constraint on metric tensor is simple but neat.\n2.\tThe use of Jacobi regularization in Eq. (9) is effective but the choice of using interpolation to extend this in the entire decoding space is kind of adhoc. Can authors please justify?\n3.\tNot sure how authors put the Lipschitz continuity constraint on f. Please explain. \n4.\tThe title of \u201cFlat manifold VAEs\u201d is misleading as it potentially means VAEs for flat manifold \n5.\tI wonder what will happen if you put an unfolding constraint in the encoding space like LLE, ISOMAP etc.. The loss function is data driven so this should give atleast similar behavior.\n6.\tOverall I like the experimental setup, but the tracking experiment is kind of distracting. The authors may want to remove this experiment.\n7.\tIn Fig. 7, the authors have shown with and without  Jacobi normalization which I am really not convinced with, need better explanation.  \n", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}}}