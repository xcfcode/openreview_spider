{"paper": {"title": "Linear Symmetric Quantization of Neural Networks for Low-precision Integer Hardware", "authors": ["Xiandong Zhao", "Ying Wang", "Xuyi Cai", "Cheng Liu", "Lei Zhang"], "authorids": ["zhaoxiandong@ict.ac.cn", "wangying2009@ict.ac.cn", "caixuyi18s@ict.ac.cn", "liucheng@ict.ac.cn", "zlei@ict.ac.cn"], "summary": "We introduce an efficient quantization process that allows for performance acceleration on specialized integer-only neural network accelerator.", "abstract": "With the proliferation of specialized neural network processors that operate on low-precision integers, the performance of Deep Neural Network inference becomes increasingly dependent on the result of quantization. Despite plenty of prior work on the quantization of weights or activations for neural networks, there is still a wide gap between the software quantizers and the low-precision accelerator implementation, which degrades either the efficiency of networks or that of the hardware for the lack of software and hardware coordination at design-phase. In this paper, we propose a learned linear symmetric quantizer for integer neural network processors, which not only quantizes neural parameters and activations to low-bit integer but also accelerates hardware inference by using batch normalization fusion and low-precision accumulators (e.g., 16-bit) and multipliers (e.g., 4-bit). We use a unified way to quantize weights and activations, and the results outperform many previous approaches for various networks such as AlexNet, ResNet, and lightweight models like MobileNet while keeping friendly to the accelerator architecture. Additional, we also apply the method to object detection models and witness high performance and accuracy in YOLO-v2. Finally, we deploy the quantized models on our specialized integer-arithmetic-only DNN accelerator to show the effectiveness of the proposed quantizer. We show that even with linear symmetric quantization, the results can be better than asymmetric or non-linear methods in 4-bit networks. In evaluation, the proposed quantizer induces less than 0.4\\% accuracy drop in ResNet18, ResNet34, and AlexNet when quantizing the whole network as required by the integer processors.", "keywords": ["quantization", "integer-arithmetic-only DNN accelerator", "acceleration"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper considers the question of how to quantize deep neural networks, for processors operating on low-precision integers.  The authors propose a methodology and have evaluated it thoroughly. The reviewers all agree that this question is important in practice, though there was disagreement about how novel a contribution this paper is specifically, and on its clarity. The clarity questions were resolved on rebuttal, so I lean to accepting the paper."}, "review": {"H1eYUpB8oH": {"type": "rebuttal", "replyto": "H1lBj2VFPS", "comment": "New supplemental materials have been added to the appendix of the paper, and we will continue to update the paper before the rebuttal deadline.\n\nMainly update:\n1. The pseudo-code;\n2. The training time of LLSQ;\n3. The simulated gradient formulation of the scaling factors and the bit-shift quantization formulation.", "title": "New supplemental materials"}, "rJgfbir8jr": {"type": "rebuttal", "replyto": "ByxRzN_RKH", "comment": "We thank the reviewer for their time in looking at our paper, and we appreciate their feedback.\n\n\u201c1. The motivation for choosing simulated gradient method\u201d\nR: As for the scaling factors, we hope to update them fast in the early stages of training and gradually stabilize them in the later stages of training to achieve better performance (e.g., \u201caccuracy\u201d for classification tasks). The EMA method takes advantage of the old values but does not change the update speed during training. However, we can use the learning rate to control the updating speed of scaling factors if the simulated gradients are given. This is the motivation for choosing the SG method.\n\n\u201c2. The scaling factors and weights are updated at the same time\u201d\nR: Although we use pre-trained full-precision parameters, we re-train the parameters and the scaling factors in the training phase. This paper focuses on quantization-aware training [1,2,3] while other works [4] that only train the quantizers are post-training quantization. In general, the accuracy degradation of 4-bit post-training quantization is significant.\n\n\u201cThe training time for different bit-width settings\u201d\nR: The results of the training time will be included in the paper as soon as possible.\nR2: The results of the training time have been updated in the paper.\n\n\u201cThe reason for leaving the first and last layers in full precision\u201d\nR: Leaving the first and last layers in full precision is a common feature of prior work on network quantization, which cannot be run on popular integer machine processors and is also why we need a new quantizer like LLSQ in this work to make it work. For a fair comparison with previous state-of-the-art quantization methods [2], which leave the first and last layers in full-precision, we adopt the same settings and show the results of LLSQ with full-precision first and last layers. However, we quantize all of the parameters and scaling factors in the networks when deploying models to the specialized integer hardware. Showing results of baselines and LLSQ with full-precision first and last layers, as well as that of the hardware-friendly LLSQ with all quantized layers, is believed to help the readers to better understand the effects of the proposed solution.\n\n[1] Benoit Jacob, et al. Quantization and Training of Neural Networks for Ef\ufb01cient Integer-Arithmetic-Only Inference. CVPR2018\n[2] Dongqing Zhang, et al. LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks. ECCV2018\n[3] Christos Louizos, et al. Relaxed quantization for Discretized Neural Networks. ICLR2019\n[4] Markus Nagel, et al. Data-Free Quantization through Weight Equalization and Bias Correction.", "title": "Reply to Official Blind Review #2"}, "ryxfBzICtS": {"type": "review", "replyto": "H1lBj2VFPS", "review": "The submission proposes to train a linear symmetric quantizing function for integer processors.\n\nThe proposed idea makes sense at a high level, and the empirical results look somewhat compelling, but the write-up is not particularly clear (see clarity-related comments). I found it hard to extract a complete picture of how the proposed approach operates: from the leftmost diagram in Figure 1 I can infer what high-level steps are involved, but I wouldn\u2019t know how to re-implement the approach from the textual description itself.\n\nClarity-related comments:\n\n- The submission never explicitly states what x^r and p(x^r) correspond to in the network. My understanding from the context is that x^r is the scalar value of a model parameter or activation, and that p(x^r) is the empirical distribution over all model parameters and activations. Can the authors clarify?\n- Some kind of pseudo-code for the re-training of the weights and activations (Section 3.3) would help clear up reader confusion. At this point, are only the weights and activations quantized in the network? How is backpropagation handled (and if the straight-through estimator is used, why is it only mentioned in Section 3.5)? How do parameter and alpha updates interleave?\n- How many alpha values are there? Section 3.3 gives the impression that there is a single quantization parameter shared by all parameters and activations, but Equation 3 uses different alpha values for the weights and activations.\n- Section 3.4 is difficult to parse due to bad notation. If the output of the convolution layer is fed into the batch normalization layer, it would be clearer to reuse symbols (i.e. change \u201cx\u201d to \u201co\u201d in Equation 4).\n\nAdditional comments:\n\n- Organization of the different sections could be improved. Related work discussion is scattered throughout three different sections, namely Introduction, Motivation, and Related Work.\n- Writing quality could be improved  (e.g., \u201c*Except that*, some designs rely [...]\u201d, \u201cEdge or embedded neural network accelerators *are generally having* three primary design goals [...]\u201d) but has a relatively small negative impact on readability.", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}, "S1xjlcrUjB": {"type": "rebuttal", "replyto": "ryxfBzICtS", "comment": "We thank the reviewer for their time in looking at our paper, and we appreciate their feedback.\n\n\u201cI wouldn\u2019t know how to re-implement the approach from the textual description itself\u201d\nR: The pseudo-code for the re-training of the weights, activations, bias, and scaling factors is summarized in the paper. And the training code for LLSQ is also available at \nhttps://anonymous.4open.science/r/c05a5b6a-1d0c-4201-926f-e7b52034f7a5/\nThe parameters and alpha values update strategy is also summarized in pseudo-code. \n\n\u201c$x^r$ and $p(x^r)$\u201d\nR: $x^r$ is indeed the model parameters or activations to be quantized, and $p(x^r)$ is the probability density distributions of weights or activations. Thanks for the reviewer points it out, and we have updated it in the submission.\n\n\u201care only the weights and activations quantized in the network\u201d\nR: The experimental results are summarized in Table 3 and Table 7 in the paper. And the bit-width of weights (w), activations (a), bias(b), and scaling factor (alpha) are given. \u201c4/4\u201d donates that we only quantize the weights and activations into 4-bit integers while \u201c4/4/8/8\u201d indicates that we not only quantize the weights and activations into 4-bits but also quantize bias and scaling factor into 8-bit. \n\n\u201cHow many alpha values are there?\u201d\nR: We use channel-wise quantization for convolutional layers and layer-wise quantization for fully-connected layers and activations. For example, suppose there are 32 output-channels for one convolutional layer, there are 32 kernels(filters) of weights and 32 alpha values for each kernel. And For activations, each layer has only one alpha value. Please note that the alpha values are not shared across layers because the weights and activations distributions of different layers are different. The equations in Section 3.3 correspond to a weight kernel or a layer of activations. Due to the quantizer optimization algorithm is the same for each kernel of weights or each layer of activations, we present a general formalization in the paper only.  However, Section 3.3 is indeed somewhat misleading. We have revised the corresponding content in Section 3.3.\n\n\u201cSection 3.4 is difficult to parse due to bad notation.\u201d And \u201cAdditional comments\u201d\nR: Thanks for the reviewer\u2019s suggestion. We modified the paper so that the reader can better understand.\n", "title": "Pseudo-code and Code are available"}, "H1e9joSUiS": {"type": "rebuttal", "replyto": "HJezXLYC5B", "comment": "We thank the reviewer for their patience in providing value feedbacks.\n\nIn contrast to previous quantization work, this work addresses a not-solved-yet problem with a simple but effective method. The motivation of this work is to deploy the quantized models to the integer accelerators, which are popular low-power hardware solutions to neural networks in the market. We not only achieve BETTER results than previous quantization work, and also succeed to eliminate the complicated operators like first and last layer treatment and the scaling layers, which cannot be handled by off-the-shelf integer accelerators. Thus, using simple methods and operators is not just to show another viable technique of quantization, but is MANDATARY for smooth running on the integer accelerators, which cannot be satisfied by previous quantizers. During network quantization, there are many other constraints in such hardware to meet, which is the essential motivation of this work. For example, Non-linear quantization needs LUT operations rather than low-precision multipliers, while asymmetric quantization requires additional subtraction [1] before multiplication. As a result, we adopt the symmetric linear quantization. To our best knowledge, eliminating the Batch-Norm layers can bring performance improvements to accelerators, while many other low bit-width (4-bit or lower) quantization methods didn\u2019t consider or solve this problem. In contrast, LLSQ can easily adopt BN fusion and achieve state-of-the-art accuracy. In addition, the remanding values in the neural networks are also quantized to low bit-width integers. Finally, we are able to deploy the quantized models to the low-bit integer accelerators. \n\n[1] Benoit Jacob, et al. Quantization and Training of Neural Networks for Ef\ufb01cient Integer-Arithmetic-Only Inference. CVPR2018", "title": "Reply to Official Blind Review #1"}, "ByxRzN_RKH": {"type": "review", "replyto": "H1lBj2VFPS", "review": "This paper proposes a linear symmetric quantizer for integer accelerators called LLSQ, which learns the quantization scaling factor using simulated gradient as update policy. Their main contribution is enabling inference on integer-only hardware by covering all parameters of all operators in convolutional networks, including weight, bias, activation and scaling factor. To address the quantization noise issue in bias parameters, they adopt Straight-Through Estimator and fine-tune the parameters after quantization. To improve inference efficiency, they apply BN layer fusion. They conduct experiments on public datasets for image classification and object detection to conclude that LLSQ achieves lower accuracy degradation compared to previous work. Finally, they test the quantized model on a specialized integer accelerator, showing the feasibility of the quantization on real hardware.\n\nIn conclusion, this paper generalizes linear symmetric quantization to all parameters in order to deploy the network on specialized integer neural network processors for efficient inference. In terms of algorithmic contribution, this paper introduces the scaling factor as a learnable parameter of the quantizer, but lacks enough theoretical justification. Therefore, I would consider weakly accepting the paper.\n\nFor the algorithm, the following should be addressed.\n1.\tIn scaling factor updating policy, the simulated gradient performs better compared to EMA. However, the motivation for choosing this policy is unclear, and there is no mathematical derivation of the gradient value.\n2.\tIn this study, pretrained network weights in full precision are used, thus the quantization procedure should take fixed weights as inputs. However, in section 3.3, the quantization scaling factor is updated in the training phase of the network, with weights being updated at the same time This seems to be a contradiction, and the experiment results where quantizer is trained with network weights fixed should also be presented.\n\nFor the experiment, the following should be addressed.\n1.\tThe training time for different bit-width settings should be included in the results.\n2.\tThe reason for leaving the first and last layers in full precision is unclear, and doing so may go against the objective of deploying the quantized model on specialized integer hardware according to the paper.\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 1}, "HJezXLYC5B": {"type": "review", "replyto": "H1lBj2VFPS", "review": "This paper focuses on the quantization of ConvNets. This paper proposes a learned linear symmetric quantizer to reduce the precision of weight, bias, and activation. The proposed approach works as the following: for a pre-trained neural network, it computes the new weight and activation as a product of a quantized value with a scaling factor. The quantization is based on a simple linear, symmetric function as in equation (1). The value of the scaling factor is searched by \"simulated gradient\" or exponential moving average during re-training. Next, batch normalization is fused into convolution, and the scaling factor and biases are re-calculated. Last, the scaling factor on the convolution is merged with bias terms, to remove the need for multiplication in hardware implementation. Since bias terms usually have a much larger dynamic range, higher precision is used to represent biases. Experiments show that the method achieves competitive results compared with previous quantization methods, and the quantized models can be deployed on hardware more easily. \n\nThe contribution of the paper, in my opinion, is to show that using simple methods without many bells and whistles, we can achieve competitive quantization performance.  And when performing quantization, it is important to consider the hardware implementation. Details on how to deal with scaling factors, how to deal with biases, and so on, can have significant influences on the overall performance. \n\nHowever, the main concern of the paper is that the methods adopted in the paper are too plain. The paper successfully integrated previous methods but did not propose new ideas that inspire future research. As a result, I would not recommend acceptance for publication. ", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}}}