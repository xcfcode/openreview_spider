{"paper": {"title": "A Convolutional Encoder Model for Neural Machine Translation", "authors": ["Jonas Gehring", "Michael Auli", "David Grangier", "Yann N. Dauphin"], "authorids": ["jgehring@fb.com", "michaelauli@fb.com", "grangier@fb.com", "ynd@fb.com"], "summary": "Investigate encoder models for translation and demonstrate that convolutions can outperform LSTMs as encoders.", "abstract": "The prevalent approach to neural machine translation relies on bi-directional LSTMs to encode the source sentence.\nIn this paper we present a faster and simpler architecture based on a succession of convolutional layers. \nThis allows to encode the entire source sentence simultaneously compared to recurrent networks for which computation is constrained by temporal dependencies.\nOn WMT'16 English-Romanian translation we achieve competitive accuracy to the state-of-the-art and we outperform several recently published results on the WMT'15 English-German task. \nOur models obtain almost the same accuracy as a very deep LSTM setup on WMT'14 English-French translation.\nOur convolutional encoder speeds up CPU decoding by more than two times at the same or higher accuracy as a strong bi-directional LSTM baseline. ", "keywords": []}, "meta": {"decision": "Reject", "comment": "This work demonstrates architectural choices to make conv nets work for NMT. In general the reviewers liked the work and were convinced by the results but found the main contributions to be \"incremental\". \n \n Pros:\n - Clarity: The work was clearly presented, and besides for minor comments (diagrams) the reviewers understood the work\n - Quality: The experimental results were thorough, \"very extensive and leaves no doubt that the proposed approach works well\".\n \n Mixed:\n - Novelty: There is appreciation that the work is novel. However as the work is somewhat \"application-specific\" the reviewers felt the technical contribution was not an overwhelming contribution.\n - Impact: While some speed ups were shown, not all reviewers were convinced that the benefit was sufficient, or \"main speed-up factor(s)\" were. \n \n This work is clearly worthwhile, but the reviews place it slightly below the top papers in this area."}, "review": {"Skg7pTPvl": {"type": "rebuttal", "replyto": "rJFd3PUDe", "comment": "PixelCNN was applied to vision and CNNs have been known to work on vision tasks for quite some time. The same is not true for language. Our paper is the first work that successfully demonstrates that CNNs can be very competitive to LSTMs on language and in particular translation. It demonstrates that CNNs can model long-range dependencies for language.", "title": "Re: acknowledge"}, "ryiuoXhUl": {"type": "rebuttal", "replyto": "rJC_2IqUl", "comment": "We just updated the paper according to our reply to the reviewers on January 9th. \n\nWe respectfully disagree that requiring two stacks is the main finding of the paper. Our goal is to show that convolutional encoders with finite contexts can compare favorably to RNNs. This claim is supported with extensive experiments.\n\nWe also edited the discussion around the two-stack architecture. In particular, Figure 1a already presents extensive experimentation around the depth of the two stacks. At this point we do not see how to extend our analysis further.", "title": "Re: ICLR 2017 conference paper104 AnonReviewer1"}, "B1-h4nZ8l": {"type": "rebuttal", "replyto": "Hytb1xzVe", "comment": "Paper is application specific: we chose to focus on machine translation which is a challenging task with strong competition systems, dedicated product teams interested in improvements, and MT has been used in the past to introduce sequence to sequence models (e.g. Sutskever et al, Bahdanau et al). It is very likely that tasks relying on architectures similar to attention-based translation will benefit by our architecture as well, e.g. summarization, constituency parsing, or dialog modeling. In future work, we plan to evaluate on other sequence to sequence problems as well.\n\nTwo stacks: our experimentation with the two stacks suggests that the attention weights (CNN-a) need to integrate information from a wide context which can be done with a deep stack; at the same time, the vectors which are averaged (CNN-c) seem to benefit from a shallower, more local representation closer to the input words. Two stacks is an easy way to achieve these contradicting requirements. We will add this discussion to the text to make the point clearer. Inspired by LSTMs, we are currently exploring non-linearities with gating that could potentially derive a single representation satisfying both requirements.\n\nFaster evaluation: we will clarify this in the next version of the paper. Please refer to our previous answer to this question in the thread below.", "title": "Re: ICLR 2017 conference paper104 AnonReviewer1"}, "r1xxEhZ8e": {"type": "rebuttal", "replyto": "SJK13DGEg", "comment": "We will add figures illustrating our model architecture better; our baseline biLSTM architecture closely follows [Zhou et al, TACL 2016] https://arxiv.org/pdf/1606.04199v3.pdf who have detailed figures illustrating the model. \nPosition embeddings are important. Without them our encoder has no notion of position. We investigated position embeddings numbered from the left to right and right to left but found that one direction was enough. \n\nRegarding conference fit: we show that RNNs are not necessary to perform encoding in a complex sequence to sequence task and that CNNs are comparable or better. Of course, this is of interest to the NLP community but also to the ML community. In term of representation learning, the two stack CNN highlight that attention weights and the input representation might benefit from different depth, which can impact other tasks, and other models. Our work also highlights  that the success of RNNs in MT cannot be attributed to their unique ability to model long term dependencies. We believe that those findings are of great interest to the ML community.", "title": "Re: ICLR 2017 conference paper104 AnonReviewer3"}, "HyatmhWIg": {"type": "rebuttal", "replyto": "BJ-bx1I4x", "comment": "We will add figures illustrating the architecture. Please see our response regarding conference fit in the replies to the other reviewers below.", "title": "Re: ICLR 2017 conference paper104 AnonReviewer4"}, "SJbwh2RSl": {"type": "rebuttal", "replyto": "BJAA4wKxg", "comment": "Authors, It would be great to have a rebuttal for this paper as reviewers will be discussing over the next week. Thanks.", "title": "Authors: Please post a rebuttal"}, "Hk2T90nzx": {"type": "rebuttal", "replyto": "ryUr3c_fx", "comment": "**Why is the convnet encoder faster at decoding?**\nOur architecture works very well with smaller embedding sizes which allows us to match or outperform the accuracy of the LSTM baseline at lower capacity. In the particular experiment you are referring to, we use a smaller embedding size and encoder output dimensionality which makes computation faster despite having more layers. Also, the 2-layer bi-directional LSTM encoder baseline contains 4 LSTMs, because it is bi-directional. The accuracy of the baseline (Table 3b) has been tuned and the results are very competitive to other published work (cf. Table 2 WMT'15 English-German).\n\nThe smaller embedding size is not the only reason for the speed-up. In Table 3 (a), we compare a Conv 6/3 encoder and a BiLSTM with equal embedding sizes. The convolutional encoder is still 1.34x faster (at 0.7 higher BLEU) although it requires roughly 1.6x as many FLOPs. We believe that this is likely due to better cache locality for convolutional layers on CPUs: a LSTM with fused gates requires two big matrix multiplications with different weights as well as additions, multiplications and non-linearities  for each word in the input sentence, while each convolutional layer can be computed in a tight loop with the same weight matrix. Both architectures might benefit from a further optimized multi-core implementation, but in comparison to other NMT implementations our setup is quite fast already.\n\nOur bi-directional LSTM implementation is based on torch rnnlib which uses fused LSTM gates ( https://github.com/facebookresearch/torch-rnnlib/blob/master/rnnlib/cell.lua) and which we consider an efficient implementation. \n\n**Why are there two convolutional stacks in the encoder? **\nUsing only a single stack of convolutions was indeed much worse (22.9 BLEU vs 28.5 BLEU). The encoder output is used for two important tasks: first, it is matched to the decoder state to identify which parts of the input are relevant to generate the next token. Second, it actually needs to encode the source sentence to provide useful inputs to the decoder. Tackling both these tasks with a simple deep convolutional network is much harder than for a bi-directional LSTM which has a gating mechanism.  Our goal was to use very simple conv-tanh blocks but there may be a more complex setting that enables a single encoder stack.\n\n**Why do we claim our encoder model to be conceptually simpler? **\nWe acknowledge that this statement is subjective. We removed it and highlighted the differences between RNNs and convnets to support this statement, namely: (i) bi-directional LSTMs process past and future words in separate stacks which then need to be combined, whereas convnets process the surrounding context at every step, (ii) convnets perform the same number of non-linear operations for each word, whereas RNNs  apply a very large number of non-linearities to words far away, (iii) parallelization is easier with convnets.", "title": "Re: ICLR 2017 conference paper104 AnonReviewer1"}, "ryUr3c_fx": {"type": "review", "replyto": "BJAA4wKxg", "review": "Hi, I have a few questions about your submission.\n\nFirst, can you please explain in more detail why your convnet model is faster for decoding? Is it because less FLOPs are required? Or is it because you were able to use less hidden units? Are both implementations efficient? It's not obvious to me why 15+5 layers of convolution are two times faster than 2 layers of LSTM when a single CPU thread is used.\n\nSecond, a very interesting finding of the paper is that two convolutional network are needed for good performance, CNN-a and CNN-c. Can you provide a qualitative explanation for this phenomenon?\n\nFinally, I think that the wording \"conceptually simpler encoder\" that you used in the conclusion is a very subjective judgement. Especially given that you have to use 2 different convnets.The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation. \n\nApart from the known architectural elements, such as convolution, pooling, residual connections, position embeddings, the paper features one unexpected architectural twist: two stacks of convolutions, one for computing alignment and another for computing the representations.\nThe empirical evidence that this was necessary is provided, however the question of *why* it is necessary remains open. \n\nThe experimental evaluation is very extensive and leaves no doubt that the proposed approach works well. The convnet-based model was faster at evaluation, but it is not very clear what is the main speed-up factor. It\u2019s however hard to argue against the fact that the speed advantage of convnets is likely to increase if a more parallel implementation is considered. \n\nMy main concern is whether or not the paper is appropriate for ICLR, because the contribution is quite incremental and rather application-specific. ACL, EMNLP and other NLP conferences would be a better venue, I think. ", "title": "A few questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hytb1xzVe": {"type": "review", "replyto": "BJAA4wKxg", "review": "Hi, I have a few questions about your submission.\n\nFirst, can you please explain in more detail why your convnet model is faster for decoding? Is it because less FLOPs are required? Or is it because you were able to use less hidden units? Are both implementations efficient? It's not obvious to me why 15+5 layers of convolution are two times faster than 2 layers of LSTM when a single CPU thread is used.\n\nSecond, a very interesting finding of the paper is that two convolutional network are needed for good performance, CNN-a and CNN-c. Can you provide a qualitative explanation for this phenomenon?\n\nFinally, I think that the wording \"conceptually simpler encoder\" that you used in the conclusion is a very subjective judgement. Especially given that you have to use 2 different convnets.The paper reports a very clear and easy to understand result that a convolutional network can be used instead of the recurrent encoder for neural machine translation. \n\nApart from the known architectural elements, such as convolution, pooling, residual connections, position embeddings, the paper features one unexpected architectural twist: two stacks of convolutions, one for computing alignment and another for computing the representations.\nThe empirical evidence that this was necessary is provided, however the question of *why* it is necessary remains open. \n\nThe experimental evaluation is very extensive and leaves no doubt that the proposed approach works well. The convnet-based model was faster at evaluation, but it is not very clear what is the main speed-up factor. It\u2019s however hard to argue against the fact that the speed advantage of convnets is likely to increase if a more parallel implementation is considered. \n\nMy main concern is whether or not the paper is appropriate for ICLR, because the contribution is quite incremental and rather application-specific. ACL, EMNLP and other NLP conferences would be a better venue, I think. ", "title": "A few questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyRsrljWx": {"type": "rebuttal", "replyto": "BJAA4wKxg", "comment": "Hi,\n\nThe conditional input is denoted by both c_i and c_t in sections 2 and 3.1 respectively. c_i is reused in section 3.2\n", "title": "Small typo"}, "HkTfjK5Zx": {"type": "rebuttal", "replyto": "BkqbKptbl", "comment": "You are right. We fixed this in the latest version and now report detokenized BLEU for WMT'16 English-Romanian. Our conv encoder is 0.3 BLEU short of your result.", "title": "Re: multi-bleu.perl"}, "BkqbKptbl": {"type": "rebuttal", "replyto": "BJAA4wKxg", "comment": "I want to draw attention to the fact that you compare tokenized BLEU results (with multi-bleu.perl) and detokenized BLEU results (with mteval-v13a.pl). The two should *never* be mixed in the same table, as the tokenization will have a big effect on results. Even when comparing systems that all have tokenized BLEU, and all use the Moses tokenizer, using different parameters (such as the \"-a\" option for aggressive hyphen splitting) will skew the results.\n\nDetokenized BLEU is standard for WMT, and reported by Sennrich et al. (2016a,b).\n\nI re-ran BLEU on our EN-RO system for comparison:\n\ndetokenized BLEU, mteval-v13a.pl: 28.1 BLEU\ntokenized BLEU, multi-bleu.perl: 29.4 BLEU\n\nyour reported result (multi-bleu.perl): 28.5", "title": "multi-bleu.perl"}}}