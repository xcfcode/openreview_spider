{"paper": {"title": "Neural Functional Programming", "authors": ["John K. Feser", "Marc Brockschmidt", "Alexander L. Gaunt", "Daniel Tarlow"], "authorids": ["feser@csail.mit.edu", "mabrocks@microsoft.com", "t-algaun@microsoft.com", "dtarlow@microsoft.com"], "summary": "A differentiable functional programming language for learning programs from input-output examples.", "abstract": "We discuss a range of modeling choices that arise when constructing an end-to-end differentiable programming language suitable for learning programs from input-output examples. Taking cues from programming languages research, we study the effect of memory allocation schemes, immutable data, type systems, and built-in control-flow structures on the success rate of learning algorithms. We build a range of models leading up to a simple differentiable functional programming language. Our empirical evaluation shows that this language allows to learn far more programs than existing baselines.", "keywords": ["Supervised Learning"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "Quality, Clarity: There is no consensus on this, with the readers having varying backgrounds, and one reviewer commenting that they found it to be unreadable. \n \n Originality, Significance:\n  The reviews are mixed on this, with the high score (7) acknowledging a lack of expertise on program induction.\n The paper is based on the published TerpreT system, and some think that it marginal and contradictory with respect to the TerpreT paper. In the rebuttal, point (3) from the authors points to the need to better understand gradient-based program search, even if it is not always better. This leaves me torn about a decision on this paper, although currently it does not have strong support from the most knowledgeable reviewers.\n That said, due to the originality of this work, the PCs are inclined to invite this work to be presented as a workshop contribution."}, "review": {"r1kmXFj8e": {"type": "rebuttal", "replyto": "HkpLeH9el", "comment": "Thank you to all the reviewers for their comments. We believe that these\nare the primary points raised by the reviewers:\n\n1. A comparison with a neural programming technique which does not\n   generate code would be a valuable addition to our experiments.\n\n   We did not include a comparison to a network which does not\n   generate source code because because they usually require\n   substantially more training data than the 5 input-output examples\n   we provide to have any success at generalization.\n   While it would be interesting to show this effect in experiments,\n   the wide variety of different models and training strategies,\n   the custom structure of list data and list-aware objective function in our work,\n   together with the lack of released standard implementations,\n   makes it unclear what neural programming baseline (and with what\n   training regime) would be appropriate.\n\n2. The tasks our network can learn are simple. In particular, we do\n   not consider sorting or merging problems.\n\n   Although the tasks that we considered are simple, they are more complex\n   than the tasks which many other neural programming approaches can handle,\n   particularly as we test for perfect generalization.\n\n   The approaches to learning to sort do so using program traces,\n   which provide much stronger supervision than the input-output examples\n   that we use [1], or use specialized memory representations that simplify\n   sorting [2]. \n\n3. This paper does not conclusively show that gradient-based\n   evaluators are appropriate for program induction.\n\n   This paper certainly does not contradict the findings of the\n   original TerpreT paper that discrete solvers are good backends for\n   program induction. Our motivation in this paper is to improve gradient-based\n   program search, and to understand the effect of different design choices\n   that arise when building differentiable interpreters. Even if gradient descent\n   isn't currently the best method for program induction, we believe it merits\n   further study. It is a very new idea, and it's feasible to us that seemingly\n   subtle design decisions could make a big difference in its performance (indeed,\n   this is one take-away from our experiments). Further, since gradient descent\n   is very different from alternatives, improving its performance may enable \n   new uses of program synthesis such as jointly inducing programs and training\n   neural network subcomponents as in [3], using SGD to scale up to large data\n   sets, or giving new ways of thinking about noisy data in program synthesis.\n\n   Finally, the recommendations for the design of such evaluators\n   discussed in this paper are not necessarily restricted to TerpreT-based models,\n   and we believe that our design recommendations apply to related\n   neural architectures that try to learn algorithmic patterns.\n\n4. How will this model generalize to programs that can't be solved\n   using the prefix-loop-suffix structure?\n\n   Defining new program structures is simple, and our current\n   implementation allows the optimizer to choose between several\n   program structures. More loops could be added if desired, and more\n   looping schemes could be added to extend the class of programs\n   which can be learned.\n\n5. TerpreT is not yet publicly available.\n\n   TerpreT is nearly ready for public release. Approval for open-sourcing\n   under the MIT license has been obtained, and we are currently in the\n   process of documenting the source code to publish it (with all models\n   used in this paper).\n\n\nReferences:\n[1] Scott Reed and Nando de Freitas. Neural Programmer-Interpreters.\n    In ICLR 2016.\n[2] Marcin Andrychowicz, Karol Kurach. Learning Efficient Algorithms\n    with Hierarchical Attentive Memory. https://arxiv.org/abs/1602.03218\n[3] Alexander L. Gaunt, Marc Brockschmidt, Nate Kushman, Daniel Tarlow.\n    Lifelong Perceptual Programming by Example. https://arxiv.org/abs/1611.02109", "title": "General response"}, "ByuO8PeVe": {"type": "rebuttal", "replyto": "B1nNqBjXl", "comment": "Thanks for the comment. We agree with the commenter that the above statement is incomplete. What we should have said is that dealing with noisy data is a more difficult problem and requires non-trivial changes to standard synthesis techniques (e.g., demonstrated by requiring dedicated papers). It may be the case that gradient-based optimization over program space is relatively more effective in this case, but to our knowledge this direction has not been explored yet. The point is just that having good ways of searching over program space as a gradient-based optimization problem may enable new ways of handling noisy data, as it also enables new models that mix program structure and neural network structure (like in the \"Lifelong Perceptual Programming by Example\" paper mentioned above). Thus, we believe it valuable to continue studying how to make gradient-based program search work better, even if it's currently not the best technique on benchmarks.\n\nAs discussed in the paper, a Lambda2 failure is reported as success rate \"0\". In all cases, failure is due to exceeding a time bound of 600s. This detail was not in the paper, but we will make it clear in the next revision.\nWe have now also run Lambda2 again with significantly longer timeouts. \"getIdx\" and \"findLastIdx\" are eventually solved (after 4166s resp. 5896s), but all other examples reported with a \"0\" success ratio (\"last2\" and the bigger get$N/dup$N examples) run into a timeout of 6000s.", "title": "Re: Dealing with noise"}, "BkSHjh0Qe": {"type": "rebuttal", "replyto": "ryMuLaLmx", "comment": "While we agree it would be nice to compare to a model like Neural Random Access Machines,  we have not done this yet for two reasons. Firstly, our problem setting is somewhat different, as we only consider very few (5) input-output examples, which might be problematic when training an RNN-based controller due to overfitting. Secondly, the NRAM paper notes that a range of tricks (most notably, curriculum learning and extensive hyperparameter searches) were required for good results. As we are not aware of a well-tested open source implementation of the NRAM model, doing these experiments would entail development, debugging and significant computational effort for not necessarily comparable results (due to implementation details), so we feel that the amount of work that it would take to provide these baseline experiments outweighs the benefit.", "title": "Re: Comparison with neural network models"}, "B1nNqBjXl": {"type": "rebuttal", "replyto": "ByjI1Qm7l", "comment": "The claim in point (1) seems somewhat incorrect: some end-user synthesizers in PL have a way to deal with noisy data (a standard problem as a user can mistype an example, etc). \n\nSee for instance:\nTransforming spreadsheet data types using examples. POPL 2016: 343-356\n\nThere are also entire papers dedicated to dealing with noise, e.g.:\nLearning programs from noisy data. POPL 2016: 761-774\n\nFor point 2, what is going on with lambda 2 for the 3 programs where it is stated to be 0.00? The other models also do not do well there.\n", "title": "Dealing with noise"}, "ryMuLaLmx": {"type": "review", "replyto": "HkpLeH9el", "review": "It would be interesting to know how well one of the neural network models does on the evaluated tasks, as additional baseline besides lambda squared. While they do not produce source code they can still probably solve the tasks, but to which extent ?The paper proposes a set of recommendations for the design of differentiable programming languages, based on what made gradient descent more successful in experiments.\n\nI must say i\u2019m no expert in program induction. While i understand there is value in exploring what the paper set out to explore -- making program learning easier -- i did not find the paper too engaging. First everything is built on top of Terpret, which isn\u2019t yet publicly available. Also most of the discussion is very detailed on the programming language side and less so on the learning side. It is conceivable that it would be best received on a programming language conference. A comparison with alternatives not generating code would be valuable in my opinion, to motivate for the overall setup.\n\nPros: \nUseful, well executed, novel study.\nCons:\nLow on learning-specific contributions, more into domain-related constraints. Not sure a great fit to ICLR.\n", "title": "Comparison with neural network models", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "BksdhxGVg": {"type": "review", "replyto": "HkpLeH9el", "review": "It would be interesting to know how well one of the neural network models does on the evaluated tasks, as additional baseline besides lambda squared. While they do not produce source code they can still probably solve the tasks, but to which extent ?The paper proposes a set of recommendations for the design of differentiable programming languages, based on what made gradient descent more successful in experiments.\n\nI must say i\u2019m no expert in program induction. While i understand there is value in exploring what the paper set out to explore -- making program learning easier -- i did not find the paper too engaging. First everything is built on top of Terpret, which isn\u2019t yet publicly available. Also most of the discussion is very detailed on the programming language side and less so on the learning side. It is conceivable that it would be best received on a programming language conference. A comparison with alternatives not generating code would be valuable in my opinion, to motivate for the overall setup.\n\nPros: \nUseful, well executed, novel study.\nCons:\nLow on learning-specific contributions, more into domain-related constraints. Not sure a great fit to ICLR.\n", "title": "Comparison with neural network models", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "ByjI1Qm7l": {"type": "rebuttal", "replyto": "B1BCBBJ7e", "comment": "We've briefly touched on this question in the Introduction (2nd paragraph) and Conclusion (last paragraph). Fundamentally, we see two important points in reply to your question:\n\n(1) Synthesis techniques from the programming languages community are doing well on simple data types such as integers or lists of integers. They offer no natural extension points to work with noisy (i.e., sometimes incorrect) or perceptual data, which can easily be integrated in the gradient descent-based setting. We are excited about synthesizing systems that are part interpretable, but differentiable program and part traditional neural network. As example, our concurrent ICLR submission \"Lifelong Perceptual Programming By Example\" handles inputs in the form of pictures, and indeed, when developing the models for that paper, our modelling recommendations from this paper proved crucial to achieve meaningful results.\n\n(2) The machine learning community has only very recently started to investigate differentiable interpreters and gradient descent as methods to synthesize programs. They currently do not work as well as more mature, optimized search techniques, but as the experimental results in Fig. 1 and Tab. 2 show, there are examples on which our gradient descent-based methods do better than a specialized PL technique. The results also show that modelling choices can have dramatic impact on the performance of gradient descent-based methods. Overall, our conclusion in this paper is that such methods can be improved substantially, and that further investigation is needed to understand if (and in what cases) they can outperform search-based baselines.\n", "title": "Re: Just an application of TerpreT?"}, "ByzfEukXl": {"type": "rebuttal", "replyto": "HJSpcihMe", "comment": "The main differences between our memory model and Graves et. al. are in how memory is addressed and how it is allocated and freed.\n \nFor addressing, Graves et al. use content-based addressing and a \"link matrix\" to make it easier to iterate over memory in the order that it was written. These appear to be useful constructs but are outside of what we considered because we are focused on inducing source code rather than a neural network controller. It's not clear that these addressing schemes, particularly the content-based addressing, have a simple natural representation as source code. However, it would be interesting in future work to think about programming language constructs that could produce similar behavior.\n \nThe method of allocating and freeing memory in Graves et al. is interesting, but our argument is that re-using the same memory for multiple purposes is not generally desirable in differentiable memory models. If memory is mutable, then during learning we must store the state of each memory location at each timestep, which can quickly become expensive. If memory is immutable, then we only need to store one copy of the memory's value. Thus, while it may seem wasteful to just continue to allocate new memory, it's actually cheaper in terms of memory usage. We note that the recent paper \"Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes\" (Rae et al, 2016) leverages sparsity to address the same issue, at the cost of some additional complexity in the implementation.", "title": "Comparison with Graves et al 16"}, "B1BCBBJ7e": {"type": "review", "replyto": "HkpLeH9el", "review": "Having just been introduced to TerpreT by this paper, it appears to me as if this paper might be pandering to the ICLR community just a bit by failing to mention your earlier claims (in \"TerpreT: A Probabilistic Programming Language for Program Induction\") that \"the overwhelming trend in the experiments is that the techniques from the programming languages community outperform the machine learning approaches by a significant margin,\" where, specifically, the gradient-based optimization in this paper is precisely this disparaged machine learning approach.  So, what's actually new here (\"we developed our models in TerpreT which hides [] technicalities\") and which is the reason you're pushing gradient descent learning here: a) because it's ICLR or b) because by taking into account the nuggets of advice given in the paper you actually find that the gradient methods now outperform the programming languages techniques previously favored?The authors talk about design choice recommendations for performing program induction via gradient descent, basically advocating reasonable programming language practice (immutable data, higher-order language constructs, etc.).  \n\nAs mentioned in the comments I feel fairly strongly that this is a marginal at best contribution beyond TerpreT, an already published system with extensive experimentation and theoretical grounding.  To be clear I think the TerpreT paper deserves a large amount of attention.  It is truly inspiring.\n\nThis paper contradicts one of the key findings in the original paper but doesn't provide convincing evidence that gradient-based evaluators for TerpreT are superior or even, frankly, appropriate for program induction.   This is uncomfortable for me and makes me wonder why gradient-based methods weren't more carefully vetted in the first place or why more extensive comparisons to already implemented alternatives weren't included in this paper.  \n\nMy opinion: if we want to give the original TerpreT paper more attention, which I think it deserves, then this paper is above threshold.  On the other hand it's basically unreadable, actually contradicts its mother-paper in not well-defended ways, and is irreproducible without the same so I think, unfortunately, it's below threshold.  ", "title": "Just an application of TerpreT?", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BkuzfSIVx": {"type": "review", "replyto": "HkpLeH9el", "review": "Having just been introduced to TerpreT by this paper, it appears to me as if this paper might be pandering to the ICLR community just a bit by failing to mention your earlier claims (in \"TerpreT: A Probabilistic Programming Language for Program Induction\") that \"the overwhelming trend in the experiments is that the techniques from the programming languages community outperform the machine learning approaches by a significant margin,\" where, specifically, the gradient-based optimization in this paper is precisely this disparaged machine learning approach.  So, what's actually new here (\"we developed our models in TerpreT which hides [] technicalities\") and which is the reason you're pushing gradient descent learning here: a) because it's ICLR or b) because by taking into account the nuggets of advice given in the paper you actually find that the gradient methods now outperform the programming languages techniques previously favored?The authors talk about design choice recommendations for performing program induction via gradient descent, basically advocating reasonable programming language practice (immutable data, higher-order language constructs, etc.).  \n\nAs mentioned in the comments I feel fairly strongly that this is a marginal at best contribution beyond TerpreT, an already published system with extensive experimentation and theoretical grounding.  To be clear I think the TerpreT paper deserves a large amount of attention.  It is truly inspiring.\n\nThis paper contradicts one of the key findings in the original paper but doesn't provide convincing evidence that gradient-based evaluators for TerpreT are superior or even, frankly, appropriate for program induction.   This is uncomfortable for me and makes me wonder why gradient-based methods weren't more carefully vetted in the first place or why more extensive comparisons to already implemented alternatives weren't included in this paper.  \n\nMy opinion: if we want to give the original TerpreT paper more attention, which I think it deserves, then this paper is above threshold.  On the other hand it's basically unreadable, actually contradicts its mother-paper in not well-defended ways, and is irreproducible without the same so I think, unfortunately, it's below threshold.  ", "title": "Just an application of TerpreT?", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJSpcihMe": {"type": "review", "replyto": "HkpLeH9el", "review": "It'd be interesting to see a comparison between the memory model proposed in this paper and the one used in \"Hybrid computing using a neural network with dynamic external memory\". More specifically, the way allocation and releasing works in Graves et al 16 seems more robust than what is done here.This paper presents small but important modifications which can be made to differentiable programs to improve learning on them. Overall these modifications seem to substantially improve convergence of the optimization problems involved in learning programs by gradient descent. That said, the set of programs which can be learned is still small, and unlikely to be directly useful. ", "title": "Comparison with the memory model of Graves et al 16", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Byr8WPl4e": {"type": "review", "replyto": "HkpLeH9el", "review": "It'd be interesting to see a comparison between the memory model proposed in this paper and the one used in \"Hybrid computing using a neural network with dynamic external memory\". More specifically, the way allocation and releasing works in Graves et al 16 seems more robust than what is done here.This paper presents small but important modifications which can be made to differentiable programs to improve learning on them. Overall these modifications seem to substantially improve convergence of the optimization problems involved in learning programs by gradient descent. That said, the set of programs which can be learned is still small, and unlikely to be directly useful. ", "title": "Comparison with the memory model of Graves et al 16", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}