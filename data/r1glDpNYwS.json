{"paper": {"title": "LabelFool: A Trick in the Label Space", "authors": ["Yujia Liu", "Tingting Jiang", "Ming Jiang"], "authorids": ["yujia_liu@pku.edu.cn", "ttjiang@pku.edu.cn", "ming-jiang@pku.edu.cn"], "summary": "A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers", "abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "keywords": ["Adversarial attack", "LabelFool", "Imperceptibility", "Label space"]}, "meta": {"decision": "Reject", "comment": "Thanks for the discussion with reviewers, which improved our understanding of your paper significantly.\nHowever, we concluded that this paper is still premature to be accepted to ICLR2020. We hope that the detailed comments by the reviewers help improve your paper for potential future submission."}, "review": {"r1g-9ZHlsB": {"type": "review", "replyto": "r1glDpNYwS", "review": "This paper proposes a method to create adversarial perturbations whose target labels are similar to their ground truth. The target labels are selected using an existing perceptual similarity measure for images.  Perturbations are generated using a DeepFool-like algorithm. Human evaluation supports that the pair of the generated images and target labels are more natural to humans than prior attack algorithms.\n\nThis paper should be rejected due to the lack of motivation to create adversarial examples less detectable by humans automatically. Attackers can manually select target labels and apply targeted attacks. In the target label selection, attackers can choose less detectable labels if necessary. It is encouraged to provide some applications where attackers want to create less detectable adversarial examples in label space without manually assigning target labels.\n\n==========\nUpdate:\n\nAfter reading the authors' responses, the motivation of the paper became clearer. I will not get surprised if this paper is accepted. However, all reviewers still share concerns about the importance of the problem tackled. I think the paper needs to suggest more applications and emphasize the value of the goal in the main paper before being published.", "title": "Official Blind Review #4", "rating": "3: Weak Reject", "confidence": 3}, "S1lFBoocjH": {"type": "rebuttal", "replyto": "HJxDPHLcjS", "comment": "Let\u2019s clarify our motivation again. We think it is important for attackers to design less detectable adversarial examples. Are there any attackers who would like their attack to be easily detected by users? And if adversarial attacks are less likely to be \u201cinspected\u201d by humans, why many previous works generate adversarial examples \u201cimperceptible to human eyes\u201d in the image space? Therefore, we think it is common and natural for attackers to have this requirement where they would like to generate less detectable adversarial examples. In our opinion, subtle mistakes can last for a long time so that the attack can bring huge potential risks a human can\u2019t imagine, while obvious mistakes will be corrected in time so that the attack doesn\u2019t have time to cause damage to the user.", "title": "Response to Reviewer #3"}, "r1ggfNdqor": {"type": "rebuttal", "replyto": "BklW6d6tsB", "comment": "We thank the reviewer for increasing the score to borderline, but we could not see the rating changed in the system. We would like to answer the reviewer\u2019s questions as follows:\n\nWe are a little confused about \u201cshow that the label selection algorithm and targeted attack algorithm used in LabelFool performs better than other possible candidates\u201d. Do you mean we should use our label selection algorithm to choose target label for existing targeted attacks such as CW-target? If so, we don\u2019t think this experiment is meaningful. In this case, it is meaningless to do subjective experiments because the label for adversarial examples generated by different methods are all the same. It is also meaningless to calculate the attack rate in this case, because the attack rate doesn\u2019t matter to our label selection algorithm, it only matters to the existing targeted attack method itself.\n\nOr do you mean we should compare our label selection algorithm with some other na\u00efve selections such as random selection? If so, we didn\u2019t compare our selection with random selections because we think it is obvious that our carefully designed label selection algorithm is better than randomly chosen label. We also considered about using the second-highest label as the target label, but we found some drawbacks in doing so. On the one hand, when the network\u2019s confidence score for the highest class is higher than $\\delta_1$ (we use 0.8 in this paper), there is no significant difference in confidence scores for other classes, so choosing the second-highest label is not appropriate. On the other hand, when the network\u2019s confidence score for the highest class is lower than $\\delta_1$, the second-highest label is usually the ground truth label (In Figure 8 in Appendix D). It will reduce the attack rate if choosing the second-highest label as the target label. So before we read the reviews, we think it is obvious that our carefully designed label selection algorithm is better than second-highest label, too. However, it may be not so obvious. We will add this experiment in our future work.\n\nWhatever, we appreciate your suggestions, it may be more convincing if we show experimental results to illustrate LabelFool does better than na\u00efve label selections. If you still have questions, don\u2019t hesitate to let us know.\n", "title": "Response to Reviewer #4"}, "r1gvWsUYsB": {"type": "rebuttal", "replyto": "Ske39HWtiB", "comment": "We thank the reviewer for the comments, and would like to answer the reviewer\u2019s questions as follows:\n\nFirst, as last comment said, please read the example in Figure 9 in Appendix E in our new version. And we have illustrated a real life case where LabelFool is needed in Appendix E. We just give an example in Figure 1 to help the readers understand what LabelFool actually does in theory. We can change the examples in Figure 1 into the examples in Figure 9 if you think examples in Figure 9 are more helpful for readers to understand our motivation.\n\nSecond, we have considered about using the second-highest label as the target label, but we found some drawbacks in doing so. On the one hand, when the network\u2019s confidence score for the highest class is higher than $\\delta_1$ (we use 0.8 in this paper), there is no significant difference in confidence scores for other classes, so choosing the second-highest label is not appropriate. On the other hand, when the network\u2019s confidence score for the highest class is lower than $\\delta_1$, the second-highest label is usually the ground truth label (In Figure 8 in Appendix D). It will reduce the attack rate if we choose the second-highest label as the target label. So choosing the second-highest label is not appropriate in this case either. Therefore, we don\u2019t use the second-highest label as the target label, but design an explicit method instead.\n\nIf you still have questions about our motivation, please don\u2019t hesitate to let us know. We are here to answer your questions all the time.\n", "title": "Response to Reviewer #3"}, "r1e0X66zsr": {"type": "rebuttal", "replyto": "B1g1QouqYB", "comment": "We thank the reviewer for the time and some good suggestions, and would like to answer the reviewer\u2019s questions as follows:\n\nFirst, there is some misunderstanding about \u201cadversarial examples\u201d. This concept was first proposed in 2014 [1]. It is defined as \u201cWe find that applying an imperceptible non-random perturbation to a test image, it is possible to arbitrarily Change the Network\u2019s Prediction. \u2026 We term the so perturbed examples \u2018adversarial examples\u2019.\u201d in work [1]. Many other works [2,3] follow this definition that adversarial examples are the ones misclassified by the network \uff08by adding imperceptible perturbations\uff09. Meanwhile, to the best of our knowledge, all works in adversarial attack community report the \u201csuccess rate of attack\u201d as the misclassification rate of their target model without checking whether the generated examples are reasonable to humans or not. As long as the prediction is different from the ground truth, the perturbed example is \u201cadversarial\u201d no matter it is reasonable to human or not.\n\nSecond, we think there are two evaluations of \u201cthe impact of the adversarial attack\u201d. One is whether the error happens, it is evaluated by the success rate of attack (misclassification rate of the target model). The other is how long the error lasts, it is evaluated by the time that a human user detects the attack. LabelFool does as well as other attacks on the first evaluation, but does better on the second evaluation because it is less detectable by a human user.\n\nThird, we only conduct experiments on the image classification task and you think this task is less likely to be \u201cinspected\u201d by humans. But the fact is, this task is widely used in many fields in our life. Please see Figure 9 in Appendix E in the new version. Taking face recognition system for entrance as an example, there will usually be a guard as \u201ca human inspector\u201d to check whether the man A who is entering the gate is the same as the system shows. In this case, if a model just misclassifies A and C, the person who looks totally different from A, the attack will be detected easily. But LabelFool aims to let the model misclassify A and B who looks like A as shown in Figure 9. If the guard doesn\u2019t identify carefully, he will let a fake B in and this error brings great potential security risks. So as an attacker, it is necessary to generate imperceptible examples in the label space for this task. \n\nFourth, we appreciate your suggestion about applying this method to other areas of ML security, we will do this in our future work, but now, we just introduce this idea, provide a tool and conduct experiments on simple tasks to prove the feasibility of this idea.\n\nIf we misunderstood you in this rebuttal or you still have questions about our motivation, please don\u2019t hesitate to let us know. We are here to answer your questions all the time.\n\nReferences:\n[1] Szegedy et al. Intriguing properties of neural networks. (ICLR 2014)\n[2] Kurakin et al. Adversarial examples in the physical world[J]. arXiv preprint arXiv:1607.02533, 2016.\n[3] Goodfellow et al. Explaining and harnessing adversarial examples. (ICLR 2015)", "title": "Response to Reviewer #3"}, "r1xM_96for": {"type": "rebuttal", "replyto": "HkgFLke2tr", "comment": "We thank the reviewer for the time and concern, and would like to answer the reviewer\u2019s questions as follows:\n\nFirst, let\u2019s clarify the concept of adversarial examples. The concept of \u201cadversarial examples\u201d was first proposed in 2014 [1] and defined as \u201cWe find that applying an imperceptible non-random perturbation to a test image, it is possible to arbitrarily Change the Network\u2019s Prediction. \u2026 We term the so perturbed examples \u2018adversarial examples\u2019.\u201d That is to say, adversarial examples are the ones misclassified by the network (by adding imperceptible perturbations). As long as the prediction is changed by the perturbation, the perturbed example is called \u201cadversarial example\u201d no matter how the prediction is far from the ground truth. Therefore, we think that each adversarial example should be treated equally. There is no \u201cgood\u201d or \u201cbad\u201d attack. There is only \u201csuccessful\u201d and \u201cfailed\u201d attack.\n\nSecond, to the best of our knowledge, most papers in adversarial attack community focus on untargeted attacks. Unfortunately, exiting untargeted attacks just focus on getting the networks failed (with imperceptible perturbations in the image space). They don\u2019t care about the misclassified-label in the label space so that the misclassified-label may be very unreasonable. However, many effective defense methods [2,3] have been published. Unreasonable labels will cause the users detect the attack and take defensive measures to let the attack fail (just as introduced in Section 1). So \u201ccreating an attack without regards to the target label\u201d is adverse for an attacker. Therefore, our method is applicable when an attacker conducts untargeted attack and hope it not to be detected. We recommend to create carefully automatically designed target label when doing untargeted attacks.\n\nThird, in Figure 7 in Appendix B of the original version, there are some figures as you suggested. In Appendix E of our new version, we add more examples in Figure 9. We also add true/target labels in Figure 5 as you suggested.\n\nIf you still have questions about our motivation, please don\u2019t hesitate to let us know. We are here to answer your questions all the time.\n\nReferences:\n[1] Szegedy et al. Intriguing properties of neural networks. (ICLR 2014)\n[2] Jia X, et al. ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples. (CVPR 2019)\n[3] Dubey, Abhimanyu, et al. Defense against adversarial images using web-scale nearest-neighbor search. (CVPR 2019)\n\n", "title": "Response to Reviewer #2"}, "SJgk3O6GoS": {"type": "rebuttal", "replyto": "r1g-9ZHlsB", "comment": "We thank the reviewer for the time , and would like to answer the reviewer\u2019s questions as follows:\n\nFirst, we want to explain our motivation for this work. Our method is an untargeted attack, not a targeted attack. To the best of our knowledge, most papers in adversarial attack community focus on untargeted attacks. But exiting untargeted attacks don\u2019t care about the misclassified-label in the label space so that the misclassified-label may be very unreasonable. However, many effective defense methods [1,2] have been published. Unreasonable labels will cause the users detect the attack and take defensive measures to let the attack fail (just as introduced in Section 1). So creating an attack without regards to the target label is adverse for an attacker. Therefore, we propose this LabelFool method in order to help an untargeted attack not to be defensed. We recommend to create carefully automatically designed target label when doing untargeted attacks\n\nSecond, there are many applications where \u201cattackers want to create less detectable adversarial examples in label space without manually assigning target labels\u201d. Please see Figure 9 in Appendix E in the new version. Taking face recognition system for entrance as an example, there will usually be a guard as \u201ca human inspector\u201d to check whether the man A who is entering the gate is the same as the system shows. In this case, if the model just misclassifies A and C, the person who looks totally different from A, the attack will be detected easily. But LabelFool aims to let the model misclassify A and B who looks like A as shown in Figure 9. If the guard doesn\u2019t identify carefully, he will let a fake B in and this error brings great potential security risks. So as an attacker, it is necessary to generate imperceptible examples in the label space for this task. But how does the attacker know who is the one looks like A in such a huge face database? LabelFool can help him much in this application.\n\nThird, we appreciate your suggestion about applying this method to other applications, we will do this in our future work, but now, we just introduce this idea, provide a tool and conduct experiments on simple tasks to prove the feasibility of this idea.\n\nIf you still have questions about our motivation, please don\u2019t hesitate to let us know. We are here to answer your questions all the time.\n\nReferences:\n[1] Jia X, et al. ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples. (CVPR 2019)\n[2] Dubey, Abhimanyu, et al. Defense against adversarial images using web-scale nearest-neighbor search. (CVPR 2019)\n\n", "title": "Response to Reviewer #4"}, "HkgFLke2tr": {"type": "review", "replyto": "r1glDpNYwS", "review": "This paper describes a technique for creating adversarial images where the added perturbations are not only imperceptible to machines, but also to human observers. The authors describe why this might be beneficial. The method works by finding labels that are not too far from the source image's ground-truth labels, and moving the source image in that direction. To find the target label, the authors use a threshold on the confidence of predicted ground-truth labels. The authors test their algorithm using a newly proposed metric of how much a method allows imperceptibility for a human observer. They show that their method creates images whose perturbations are more impercetible to humans, compared to other methods, but are also imperceptible to machines.\n\nMy concern is as follows: If the misclassification is between A and B, and they are related classes, is the attack so bad? And what are the scenarios in practice when a user simply wants to create an attack, without regards to the target label chosen? I imagine normally the attacker has a target label in mind, so the part of the paper that chooses a target label is not very useful; and this is the main element of novelty, since the rest of the method is from DeepFool, as the authors explain. Some specific use cases of this methods should be discussed. \n\nMinor suggestion: It would be useful to see examples like in Fig. 5 but with the classes (true/target) listed.", "title": "Official Blind Review #2", "rating": "3: Weak Reject", "confidence": 1}, "B1g1QouqYB": {"type": "review", "replyto": "r1glDpNYwS", "review": "This paper proposes a method for constructing adversarial attacks that are less detectable by humans, by changing the target class to be a class similar to the original class of the image. The resulting attack methodology is then studied in terms of its imperceptibility in label space, and shown to be less perceptible in label space to human observers, while not coming at a cost in image space.\n\nThe paper presents compelling evaluation of the method and does seem to succeed in proving that their proposed attack satisfies the stated goal. However, it appears as though this goal is somewhat counter to the main point of adversarial examples---indeed, if the label is reasonable to a human, then what makes the adversarial example adversarial? The main threat in adversarial examples research seems to be that it is possible to induce predictions that are arbitrarily different from humans' on natural-looking in puts. Thus, changing the label to something that a human actually agrees with would actually reduce the impact of the adversarial attack.\n\nIn order to improve the paper, I would suggest applying the same (or similar) methodologies to other areas of ML security where imperceptibility in label space is commonly desired---for example, in data poisoning attacks or backdoor attacks. In general, such attacks are much more likely to be \"inspected\" by humans, and so imperceptibility in both label and image space is very desirable. However, I suspect that this would require significant effort and changes to the paper, and so for now I recommend rejection.", "title": "Official Blind Review #3", "rating": "1: Reject", "confidence": 4}}}