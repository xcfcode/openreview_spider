{"paper": {"title": "DPSNet: End-to-end Deep Plane Sweep Stereo", "authors": ["Sunghoon Im", "Hae-Gon Jeon", "Stephen Lin", "In So Kweon"], "authorids": ["dlarl8927@kaist.ac.kr", "haegonj@andrew.cmu.edu", "stevelin@microsoft.com", "iskweon77@kaist.ac.kr"], "summary": "A convolution neural network for multi-view stereo matching whose design is inspired by best practices of traditional geometry-based approaches", "abstract": "Multiview stereo aims to reconstruct scene depth from images acquired by a camera under arbitrary motion. Recent methods address this problem through deep learning, which can utilize semantic cues to deal with challenges such as textureless and reflective regions. In this paper, we present a convolutional neural network called DPSNet (Deep Plane Sweep Network) whose design is inspired by best practices of traditional geometry-based approaches. Rather than directly estimating depth and/or optical flow correspondence from image pairs as done in many previous deep learning methods, DPSNet takes a plane sweep approach that involves building a cost volume from deep features using the plane sweep algorithm, regularizing the cost volume via a context-aware cost aggregation, and regressing the depth map from the cost volume. The cost volume is constructed using a differentiable warping process that allows for end-to-end training of the network. Through the effective incorporation of conventional multiview stereo concepts within a deep learning framework, DPSNet achieves state-of-the-art reconstruction results on a variety of challenging datasets.", "keywords": ["Deep Learning", "Stereo", "Depth", "Geometry"]}, "meta": {"decision": "Accept (Poster)", "comment": "A deep neural network pipeline for multiview stereo is presented. After rebuttal and discussion, all reviewers learn toward accepting the paper. Reviewer3 points to good results, but is concerned that the technical aspects are somewhat straightforward, and thus the contribution in this area is limited. The AC concurs with the reviewers."}, "review": {"BklzKibF27": {"type": "review", "replyto": "ryeYHi0ctQ", "review": "Summary\nThis paper proposes an end-to-end learnable  multiview stereo depth estimation network, which is basically very similar to the GCNet (Kendall et.al 2017) or PSMNet (Chang et.al 2018) for stereo estimation. The differences are using SPN to warp feature w.r.t RT, adding a multi view averaging cost and a cost aggregation component for final depth regression, which transform the original network to support multi-view stereo, yielding performance boost over other baselines.\n\nTechnically, I believe it is sound  because cost volume from stereo matching has already been demonstrate very effective in boosting performance because it use underlining geometry constraint.  My major concern lies in three aspects. \n\n1) Another most recent SOTA algorithm is  MVSNet (Yao et.al ECCV 2018), the paper should be considered for comparison. In addition, the structure is even more similar with the proposed network architecture.  \n\n2) The evaluation metrics are mostly use for single view depths, it is not consistent with paper of DeepMVS (Tab. 1) or that from MVSNet. Therefore, it might be hard to actual understand whether the numbers are  exactly comparable. \n\n3) Since the method largely improved over their baseline algorithms, and the number between different papers are hard to compare. In my opinion, to better show the results,  I suggest submitting results to an online benchmark with test data for verifying the results. such as ETH3D multi view benchmark, where everything is standardized. \n\nI hope the author can make strong feedback for validating the results. \n\n####### . After rebuttal\n\nThe author makes more clear indication of the performance contribution of the completeness of recovery.", "title": "Practically good but technically weak.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "S1gw0HrzRQ": {"type": "rebuttal", "replyto": "BklzKibF27", "comment": "3) Since the method largely improved over their baseline algorithms, and the number between different papers are hard to compare. In my opinion, to better show the results, I suggest submitting results to an online benchmark with test data for verifying the results. such as ETH3D multi view benchmark, where everything is standardized.\n\nDifferent from MVSNet which estimates the full 3D of objects, our DPSNet is inspired by the plane sweeping algorithm which is originally devised for dense depth reconstruction. That is, DPSNet focuses on estimating the dense depth map. By your suggestion, we have submitted depth maps from DPSNet to the ETH3D benchmark, but did not receive satisfactory results, with lower ranks on the \u2018F1 score\u2019 metric. \nMethod\n(20cm)\tlow-res many-view\tindoor\toutdoor\tlakeside\tsand box\tstorage room\tstorage room 2\ttunnel\nDPSNet\t59.89\t54.16\t63.70\t71.39\t70.87\t52.09\t56.24\t48.85\nMVSNet\t63.58\t56.25\t68.47\t66\t71.12\t48.36\t64.13\t68.29\n\nIn order to obtain a high score on the accuracy metric, the depth corresponding to unobserved pixels should be removed. MVSNet performs a depth map filtering to remove outliers, and its multiple observed 3D points merged well to select a correct depth value from multiple observations for a pixel. However, the outlier rejection and the 3D point merging proces are out of the scope of this paper. Instead, we demonstrate that DPSNet produces dense depth maps well by achieving higher scores on the \u2018Completeness\u2019 metric than that of MVSNet on the ETH3D benchmark.\nMethod\n(20cm)\tlow-res many-view\tIndoor\toutdoor\tlakeside\tsand box\tstorage room\tstorage room 2\ttunnel\nDPSNet\t58.64\t47.21\t66.25\t72.10\t77.08\t48.12\t46.31\t49.58\nMVSNet\t47.72\t40.64\t52.43\t49.39\t55.25\t33.58\t47.7 \t52.66\n\nIn conclusion, DPSNet and MVSNet have different goals, and thus different strengths. In the revised paper, we clarify that our DPSNet aims to infer dense depth maps, to highlight its advantages.\n\n", "title": "Response to Reviewer2 (c)"}, "rJeH6BHMC7": {"type": "rebuttal", "replyto": "BklzKibF27", "comment": "2) The evaluation metrics are mostly use for single view depths, it is not consistent with paper of DeepMVS (Tab. 1) or that from MVSNet. Therefore, it might be hard to actual understand whether the numbers are exactly comparable. \nThe error metrics used in this paper are for dense depth measurements on the KITTI benchmark. Since the main goal of our work is to reconstruct a dense depth map from multiple images, we think the error metrics are suitable for performance evaluation of our DPSNet and the state-of-the-art methods. However, we agree with your view and report a quantitative evaluation using the three error metrics used in DeepMVS: (1) Completeness, which is the percentage of pixels whose errors are below a certain threshold. (2) Geometry error, taking the L1 distance between the estimated disparity and the ground truth. (3) Photometry error, which is the L1 distance between the reference image and warped image using the estimated disparity map. \n\nWe evaluated depth maps from our DPSNet and all comparison methods including MVSNet using those measures. For fair comparison, we use the ETH3D dataset on which all methods are not trained. As shown in Table 2 of the revised paper, our DPSNet shows promising results on these measures, compared to the state-of-the-art methods. In particular, DPSNet outperforms MVSNet in all metrics, except for the photometric error of filtered MVSNet. Since MVSNet is not designed for dense depth reconstruction, this evaluation protocol is highly disadvantageous to MVSNet. \nIn the next response, we will discuss error measures in the ETH3D benchmark, which are also used for MVSNet evaluation.\n", "title": "Response to Reviewer2 (b) "}, "S1lRoHBM0X": {"type": "rebuttal", "replyto": "BklzKibF27", "comment": "1) Another most recent SOTA algorithm is MVSNet (Yao et.al ECCV 2018), the paper should be considered for comparison. In addition, the structure is even more similar with the proposed network architecture.  \nThanks for your suggestion to improve our experiments. Following your comment, we have added a description of MVSNet at the end of Sec. 2 and mentioned the differences from our DPSNet. Traditional stereo matching and multiview stereo consist of four steps: initial matching, cost aggregation, multi-label optimization and refinement. In our opinion, MVSNet mainly focuses on computing initial matching and refinement. Our contributions are mainly on initial matching and cost aggregation. Even though the main scheme for warping is similar, the cost volume generation and aggregation approaches are different. \n\nMVSNet was developed concurrently to our work, and there are differences in contributions. In particular, (1) our DPSNet concatenates the reference image features and warped pair image features, then builds a cost volume. If multiple views are available (more than 3 views), we iteratively compute the cost volume and average them. This cost volume generation strategy can produce a more confident cost volume as more views are matched. Moreover, we show that our DPSNet can be generalized to binocular stereo, while MVSNet builds a cost volume based on the variance of features, which fails to estimate accurate depth values when only using two views. (2) Our novel cost aggregation network shows significant performance improvements on textureless regions over MVSNet\u2019s refinement with a reference image feature. (3) Both the minimum and maximum depth range are user-defined parameters and scene-dependent. As an alternative, our DPSNet uniformly samples matching planes in inverse-depth scale, which is beneficial for alleviating depth quantization error and reconstructing scenes with large depth ranges. \nWe cannot say with certainty which method performs better. What we can say clearly is that both methods have shown great performance improvements by applying traditional techniques used in multiview stereo into deep learning architectures. In other words, both studies have independent academic contributions.\nFor a comparison to MVSNet, please refer to the next response.\n", "title": "Response to Reviewer2 (a)"}, "rJeqHUBf0X": {"type": "rebuttal", "replyto": "rJeI9Oz5hQ", "comment": "3) I do believe the paper would significantly benefit from more discussion of DeepMVS since it's clearly the closest to this method (also solves MVS by deep networks + plane sweep). DeepMVS also learns the matching cost for cost volume generation, and the major difference seems to be that this method is learned end-to-end. It would be better to have a more detailed discussion of the differences (the current discussion at the end of Sec 2 is a little short on details)---architectures, super-vision at the end of the cost-volume vs end-to-end, etc.\n\nDeepMVS shows good performance on various datasets, but it is not an end-to-end system. DeepMVS computes input volumes for their network using the traditional warping process, different from our network which employs a differential warping process. In addition, even though they take feature aggregations (intra-, inter-feature aggregation), the final depth map is obtained from a conventional CRF which is sensitive to over-smoothing artifacts in textureless regions (see the artifacts in Fig. 3 and Fig. 4 of our paper). Following your comment, we have further explained the difference between DeepMVS and our work at end of Sec. 2. ", "title": "Response to Reviewer1 (c)"}, "HkeF3NBMRQ": {"type": "rebuttal", "replyto": "rJeI9Oz5hQ", "comment": "1) While the proposed network is complex, I do believe the description of the architecture could be a little better. It would be good to clarify that i indexes view (and N is the total number of views), and provide a few more definitions for the terms in equation (2): namely, are R and t the extrinsics of the reference camera or the i^th camera, etc. The overall approach is clear (for each plane, the method maps features from the paired camera to the reference camera assuming all points in the the world lie on that plane), but it would be good to clarify the specifics. It might also be useful to emphasize that the cost-volume generation is per-pair (perhaps change the title of Sec 3.2) and that these volumes are averaged for all pairs.\n\nThanks for your suggestions to improve our manuscript. Following your comments, we have clarified these parts by adding descriptions in Sec 3.2. In the first paragraph of Sec. 3.2, we explain that our cost volume generation is based on unstructured two-view images and can be extended to multiview matching by averaging other cost volumes to reduce a negative effect of image noise. In particular, we have changed the title of Sec 3.2 from \u201cCost Volume Generation\u201d to \u201cCost Volume Generation using Unstructured Two-View Images\u201d to highlight that the cost volume is computed from an image pair. We also added explanations for the notations i, R and t in the third paragraph of Sec. 3.2.\n", "title": "Response to Reviewer1 (a)"}, "SyxNEIBfCQ": {"type": "rebuttal", "replyto": "rJeI9Oz5hQ", "comment": "2) It might also be useful to apply the algorithm to the rectified binocular stereo case (where the warping and definition of planes by disparity are much simpler), and show comparisons to the many stereo algorithms on datasets like KITTI. At some level, the proposed algorithm can be thought of taking approaches proved to be successful for rectified binocular stereo and generalizing them (by generic warping + plane sweep) to the multi-view case. Hence, such comparisons could be illuminating. (Note: the method doesn't need to outperform the state-of-the-art there, but the results would be informative).\n\nWe applied our algorithm to rectified stereo matching, particularly on the KITTI2015 test set, following the comment of reviewer1. We have obtained reasonable results using the model trained on SUN3D, RGBD, and Scenes11 as shown in Fig. 7(b) and Table 5. We also report the results with the KITTI finetuning in Fig. 7(c) and Table 5. \nOur DPSNet achieves performance compatible to DispNet, but not beyond the performance of recent CNN-based stereo matching algorithms (GC-Net and PSMNet). Based on the rectified stereo experiments, we also agree with reviewer 1\u2019s comments that our DPSNet can be considered a generalized version of stereo matching. We mention that our DPSNet can be directly applied to both rectified stereo and unrectified multiview stereo, and add the experiment results in Sec. 4.3.\n\n", "title": "Response to Reviewer1 (b)"}, "BJlme8SzAm": {"type": "rebuttal", "replyto": "SkxJToirn7", "comment": "More details would be welcome for Section 3.2\nThanks for your positive comments on our paper. Since your comment is the same as that of Reviewer 1, please refer to the response 1 to Reviewer 1\u2019s question and check our revised manuscript.\n", "title": "Response to Reviewer3"}, "rJeI9Oz5hQ": {"type": "review", "replyto": "ryeYHi0ctQ", "review": "The paper describes a method for learning a deep neural network for multi-view stereo. The overall network includes feature-extraction layers applied to all images, followed by a spatial-transformer network (which is differentiable, but with no learnable parameters) that is applied to warp these features from every matching image to the reference image's co-ordinate frame for a series of candidate depth planes, followed by concatenation of the reference and match image features and 3D convolution layers to form a cost volume. The cost volumes of different pairs are averaged, and additional layers are used to refine this cost volume while relying on the reference image's RGB features, followed by soft-max and an expectation over depth values to output the final depth at each pixel. The entire network is trained end-to-end and experiments show that it outperforms state-of-the-art methods for MVS by a significant margin on a number of datasets.\n\nOverall, I have a positive view of the paper and believe it should be accepted to ICLR. However, I would like the authors to address the following issues:\n\n- While the proposed network is complex, I do believe the description of the architecture could be a little better. It would be good to clarify that i indexes view (and N is the total number of views), and provide a few more definitions for the terms in equation (2): namely, are R and t the extrinsics of the reference camera or the i^th camera, etc. The overall approach is clear (for each plane, the method maps features from the paired camera to the reference camera  assuming all points in the the world lie on that plane), but it would be good to clarify the specifics. It might also be useful to emphasize that the cost-volume generation is per-pair (perhaps change the title of Sec 3.2) and that these volumes are averaged for all pairs.\n\n- It might also be useful to apply the algorithm to the rectified binocular stereo case (where the warping and definition of planes by disparity are much simpler), and show comparisons to the many stereo algorithms on datasets like KITTI. At some level, the proposed algorithm can be thought of taking approaches proved to be successful for rectified binocular stereo and generalizing them (by generic warping + plane sweep) to the multi-view case. Hence, such comparisons could be illuminating. (Note: the method doesn't need to outperform the state-of-the-art there, but the results would be informative).\n\n- I do believe the paper would significantly benefit from more discussion of DeepMVS since it's clearly the closest to this method (also solves MVS by deep networks + plane sweep). DeepMVS also learns the matching cost for cost volume generation, and the major difference seems to be that this method is learned end-to-end. It would be better to have a more detailed discussion of the differences (the current discussion at the end of Sec 2 is a little short on details)---architectures, super-vision at the end of the cost-volume vs end-to-end, etc.\n", "title": "Likely Accept: but requires some comments to be addressed", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SkxJToirn7": {"type": "review", "replyto": "ryeYHi0ctQ", "review": "This paper proposes a method for stereo reconstruction using Deep Learning. Like some previous methods, a 'cost volume' is first computed by plane sweeping, in other words the cost volume is indexed by the 2D locations in the image plane, and the disparities for 3D planes parallel to the image plane. A network then predicts the disparities for each image location from this cost volume.\n\nThe contributions with respect to the state-of-the-art are:\n\n- the cost volume is computed using differential warps, thus the network can be trained end-to-end;\n\n- a better cost volume is computed from the original cost volume and the reference image.\n\nThe results look good, both quantitatively and qualitatively. The paper reads well, and related work is correctly referenced.\n\nThere is nothing wrong with the proposed method, it makes sense and I am convinced it works well. However, I found the contributions quite straightforward, and it is difficult to get excited about the paper.\n\nMore details would be welcome for Section 3.2", "title": "Works well, maybe too straightforward for ICLR", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJgWT3KSq7": {"type": "rebuttal", "replyto": "HygmvWIH5Q", "comment": "Hello, thank you for your feedback and your interest in our work. Regarding your comments:\n\n\n1. We will release our PyTorch implementation for DPSNet upon the acceptance of this paper.\n\n2. The camera rotation R (3x3 matrix) and transform T (3x1 vector) described in our paper are defined in 3D space. Camera extrinsic [R | T] and the intrinsic parameter K are used to obtain the projected image coordinates (\\hat{u}_l) described in equation (2). Using the projected image coordinates (\\hat{u}_l), we calculate the pixel-wise 2x1 translation vector {T_stn} defined in the STN (similar to the flow field). Because we warp the image based on the projected coordinates, we set the 2x2 rotation matrix {R_stn} defined in the STN as the identity matrix. ", "title": "Re: Access to Code"}}}