{"paper": {"title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "authors": ["Kevin Clark", "Minh-Thang Luong", "Quoc V. Le", "Christopher D. Manning"], "authorids": ["kevclark@cs.stanford.edu", "thangluong@google.com", "qvl@google.com", "manning@cs.stanford.edu"], "summary": "A text encoder trained to distinguish real input tokens from plausible fakes efficiently learns effective language representations.", "abstract": "Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.\n", "keywords": ["Natural Language Processing", "Representation Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper investigates the tasks used to pretrain language models. The paper proposes not using a generative tasks ('filling in' masked tokens), but instead a discriminative tasked (recognising corrupted tokens). The authors empirically show that the proposed method leads to improved performance, especially in the \"limited compute\" regime. \n\nInitially, the reviewers had quite split opinions on the paper, but after the rebuttal and discussion phases all reviewers agreed on an \"accept\" recommendation. I am happy to agree with this recommendation based on the following observations:\n- The authors provide strong empirical results including relevant ablations. Reviews initially suggested a limitation to classification tasks and a lack of empirical analysis, but those issues have been addressed in the updated version. \n- The problem of pre-training language model is relevant for the ML and NLP communities, and it should be especially relevant for ICLR. The resulting method significantly outperforms existing methods, especially in the low compute regime. \n- The idea is quite simple, but at the same time it seems to be a quite novel idea. "}, "review": {"BkxelNPJqB": {"type": "review", "replyto": "r1xMH1BtvB", "review": "The authors propose replaced token detection, a novel self-supervised task, for learning text representations.\n\nThe principle advantage of the approach is that, in contrast with the standard masked language model (MLM) objective used by BERT and derivatives, there is a training signal for all tokens of the input (rather than a small fraction, when 10-20% of the input tokens are masked and then reconstructed under the MLM objective).\n\nA smaller MLE-trained BERT-style generator is used to replace masked words with plausible alternatives, which the ELECTRA discriminator (the part that is retained and finetuned on downstream tasks) must detect (unmodified word slots are also in the objective, and must be detected as such).\n\nIn general the paper reads well, and the authors present ablations to reveal the source of gains. ELECTRA matches the performance of RobBERTa on the popular GLUE NLP task, with just 1/4 of the training compute.\n\nStrengths:\n-Simple but novel self-supervised task for learning text representations, strong results, adequate ablation.\n\nLimitations:\n-The authors limit their investigation of downstream performance to the GLUE set of tasks, which are classification tasks. This is a significant limitation of the current version of the paper, as it may be that replaced token detection is more suitable for these tasks, but inferior to MLM (a higher precision self-supervised task) for more involved tasks like question answering. The latter is arguably of much higher importance to the NLP research community at this point, and some consider the GLUE task to be essentially solved for all practical purposes, given inherent noise levels.\n-In contrast with BERT, there is no mention of any plan to release ELECTRA (big or small versions), which is a disappointment, lowers the significance of the work\n\nOverall:\nAn okay paper. Results on SQUAD or another more elaborate NLP task and/or the release of the ELECTRA models would make the paper much stronger.", "title": "Official Blind Review #1", "rating": "8: Accept", "confidence": 2}, "SJgKAmShKH": {"type": "review", "replyto": "r1xMH1BtvB", "review": "Summary: Authors offer an alternative for masked LM pretraining that's more sample-efficient called replaced token detection. Their method basically replaces certain input tokens with alternatives which are sampled from a generator and train a discriminative model to determine whether its generated or real. The work shows empirical success getting better results than GPT with a fraction of the compute on GLUE and others.\n\nPositives: Idea is simple and makes sense intuitively, but not something one would think immediately would work better with a such a small fraction of the compute. I think the formulations of the experiments and ideas to develop this are adequate.\n\nConcerns & Questions: I'd like to see a little more investigation into Table 3. I don't have intuition over why these results are the way that they are and the text nor the experimentation really gives me an indication. How well does this model work with very very little compute; lets say you have only a couple of gpu hours. Whats the degradation in performance?\n\nOverall I'd like to see more clarity in the overall analysis because I'm still unsure how to interpret your results on the why certain choices/experimental groups get the performance numbers they get.\n\n------------------------------------------------------------------------------------------------------------------------\n\nAfter the author response, I have changed my score to a 6. I think the paper merits acceptance.", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 3}, "SkxNfzO9or": {"type": "rebuttal", "replyto": "r1xMH1BtvB", "comment": "We want to thank the reviewers again for their suggestions! We have updated the paper with the following changes: \n  - Addressing Reviewer 1\u2019s concern, we added results for SQuAD (both 1.1 and 2.0). ELECTRA-Large matches RoBERTa on SQuAD (getting 0.2 exact-match points worse on 1.1 but 0.4 points better on 2.0) despite using less than \u00bc of the pre-training compute. ELECTRA-Base scores much higher than BERT-Base/XLNet-Base and even outperforms BERT-Large.\n  - Addressing Reviewer 2\u2019s question, we added some discussion and empirical analysis of the problems with adversarial ELECTRA.\n  - Addressing Reviewer 3\u2019s question, we added results for ELECTRA-Small with even fewer GPU-hours in Table 1. ELECTRA performs well even when using as little as 6 GPU-hours of pre-training. \n  - We also updated the appendix with descriptions of the GLUE dataset and test-set results of the Base/Small ELECTRA models. A new finding is that ELECTRA-Small outperforms TinyBERT, a model of comparable size that uses a complicated distillation procedure to learn from BERT-Base during both pre-training and fine-tuning, as well as data augmentation (whereas ELECTRA-Small uses no distillation or data augmentation).\n\nLastly, as suggested by Reviewer 2, we are in the process of training ELECTRA for longer (with compute comparable to RoBERTa), although it is still early along in training. ", "title": "Paper Updates"}, "SkeFTOXIsS": {"type": "rebuttal", "replyto": "SJgKAmShKH", "comment": "Thank you for the comments! We address some of the concerns and questions below:\n\n>\u201cI'd like to see a little more investigation into Table 3. I don't have intuition over why these results are the way that they are and the text nor the experimentation really gives me an indication.\u201d\n>\u201cOverall I'd like to see more clarity in the overall analysis because I'm still unsure how to interpret your results...\u201d\n\nThe ablations in Table 3 are a series of \u201cstepping stones\u201d between BERT and ELECTRA designed to show where ELECTRA\u2019s improvements come from. To reiterate the discussion in the paper:\n  - \u201cReplace MLM\u201d slightly outperforming BERT suggests that a small amount of the gains can be attributed to solving BERT\u2019s pre-train/fine-tune mismatch due to [MASK] tokens, as \u201cReplace MLM\u201d is essentially BERT with this mismatch fixed.\n  - \u201cELECTRA 15%\u201d matching \u201cReplace MLM\u201d suggests that ELECTRA is benefitting a lot from learning from all input tokens. ELECTRA 15% is essentially ELECTRA with this advantage over BERT removed and indeed the gains over BERT mostly go away. \n  - \u201cAll-Tokens MLM\u201d outperforming \u201cReplace MLM\u201d further demonstrates the benefit of learning from all input tokens, as we substantially improve BERT\u2019s masked language model objective when incorporating this idea into BERT.\n  - Lastly, ELECTRA outperforming \u201cAll-Tokens MLM\u201d shows the additional value of our discriminative second stage classifier rather than replacing it with a BERT-style generative model. \n\nPlease let us know if there are other results you find unclear or if you have suggestions for further experiments/discussion that would help clarify the results. \n\n\n>\u201cHow well does this model work with very very little compute; lets say you have only a couple of gpu hours. Whats the degradation in performance?\u201d\n\nHere are GLUE scores for ELECTRA-Small for various training times, which we will add to the paper. \n4 days: 79.9 (slight improvement over the number in the submission due to some additional hyperparameter tuning)\n2 days: 79.0\n1 day: 77.7\n12 hours: 76.0\n6 hours: 74.1\nWe note that even the full 4 days is already a tiny fraction (~1/50th) of the compute used to train BERT, but these results show that an effective model can be built with even more limited resources. \n\n", "title": "Reply to Reviewer #3"}, "B1eYvDmUiH": {"type": "rebuttal", "replyto": "r1xUDhfRtB", "comment": "Thank you for the comments! We address some of the concerns and questions below:\n\n>\u201cIt will be helpful if the authors provide more empirical analysis why the adversarial ELECTRA perform worse or failed.\u201d\n\nYes, that is a good question! We did not have too much discussion on this in the submission because it is a negative result and we were limited for space. We found two problems with the adversarially trained generator. The main one is that the adversarial generator is simply worse at masked language modeling. For example, a size-256 adversarial generator after 500k training steps achieves 58% accuracy at masked language modeling compared to 65% accuracy for an MLE-trained one. We believe the worse accuracy is mainly due to the poor sample efficiency of reinforcement learning when working in the large action space of generating text. As evidence for this, the adversarial generator's MLM accuracy was still increasing towards the end of training while the MLE generator\u2018s accuracy vs train step curve had mostly flattened out. The second problem is that the adversarially trained generator produces a \u201cpeaky\u201d low-entropy output distribution where most of the probability mass is on a single token, which means there is not much diversity in the generator samples. Both of these problems have been observed in GANs for text in prior work (see \u201cLanguage GANs Falling Short\u201d from Caccia et al., 2018 and \u201cEvaluating Text GANs as Language Models\u201d from Tevet et al., 2019). We will add this additional discussion to the paper.\n\n\n>\u201cBut, it will be a big plus if the authors can show ELECTRA can outperform RoBERTa with the same amount of training time.\u201d\n\nWe are working on training ELECTRA for longer (it just takes lots of compute!). We want to emphasize that our focus is on compute efficiency. However, given the trend of the accuracy vs compute curve in Figure 1, we are confident that ELECTRA will continue to improve and therefore outperform RoBERTa when given the same training time. ", "title": "Reply to Reviewer #2"}, "r1ebpQmLoS": {"type": "rebuttal", "replyto": "BkxelNPJqB", "comment": "Thank you for the comments! We address some of the concerns and questions below:\n\n>\u201cThe authors limit their investigation of downstream performance to the GLUE set of tasks, which are classification tasks. This is a significant limitation of the current version of the paper\u2026\u201d\n\nWe definitely agree with the reviewer\u2019s point that evaluating on diverse tasks is useful! We initially focused on GLUE because it contains a variety of tasks and has been the main benchmark for evaluating pre-trained representations from GPT onwards. However, we have since run experiments on SQuAD (both 1.1 and 2.0) and will add them to the paper. Results are consistent with the GLUE ones (e.g., ELECTRA-Base outperforms BERT-Large and ELECTRA-Large matches RoBERTa). ELECTRA appears to be slightly better at SQuAD 2.0 (where it outperforms RoBERTa by 0.4 exact-match points) than 1.1 (where it slightly underperforms RoBERTa by 0.2 exact-match points). We think the best way of evaluating pre-trained encoders is still an open question - more challenging tasks might better distinguish models, but require more sophisticated classifiers on top of the transformer that can complicate the analysis. \n\n\n>\u201cIn contrast with BERT, there is no mention of any plan to release ELECTRA (big or small versions), which is a disappointment, lowers the significance of the work.\u201d\n\nWe absolutely will release the code and pre-trained weights (for all model sizes)! We apologize for not making that clear in the submission. ", "title": "Reply to Reviewer #1"}, "rJxvxzXIiS": {"type": "rebuttal", "replyto": "Skg95kdCcS", "comment": "You're welcome! \n\n1. The generator is a small network only used to help train the discriminator. In fact, it could be as simple as a unigram LM, in which case there would be no way to fine-tune it for downstream tasks. At any rate, it\u2019s performance for downstream use is usually much below that of the discriminator.\n\n2. Could you please rephrase the question? We don\u2019t quite understand what you are asking.", "title": "Answers to the questions"}, "r1xUDhfRtB": {"type": "review", "replyto": "r1xMH1BtvB", "review": "The paper proposed a novel sample-efficient pretraining task. One inefficiency of BERT is that only 15% tokens are used for training in each example. The paper introduced a generator+discriminator framework to optimize the utility of training examples. The generator task is the MLM which predicts the masked word. The author adds a discriminator to further learn from the example by classifying each word to be either generated or original. In this way, more words can be used. This method looks as only adding the discrimination task after BERT pretraining task. But, the authors later show that the best GLUE scores can be obtained only when both generator and discriminator are co-trained. Moreover, the adversarial ELECTRA perform worse. All these observations are interesting. It will be helpful if the authors provide more empirical analysis why the adversarial ELECTRA perform worse or failed. Is it because the GAN is hard to train or the adversarial task doesn't fit the pretraining? \n\nOverall, I think this is a good paper. The studied problem is important, the idea is new and the experimental results are positive. Specifically, it shows that ELECTRA can outperforms BERT and match RoBERTa with less training time. But, it will be a big plus if the authors can show ELECTRA can outperform RoBERTa with the same amount of training time. Analysis are also provided to give audience insights in this method.", "title": "Official Blind Review #2", "rating": "8: Accept", "confidence": 2}, "HJe2AfI4dB": {"type": "rebuttal", "replyto": "HJlI-6LZdr", "comment": "Hi! We considered a token \"hard to predict\" if it had low probability under BERT's output distribution when it was masked out. The model learning which tokens were hard to predict was a small transformer network taking the unmasked text as input. For each token, it predicted (using a sigmoid output layer) how much probability BERT would assign that token if it was masked. We trained it by running BERT as normal on the input and then minimizing the sigmoid cross-entropy loss between the model\u2019s output and the probability BERT gave to the correct token for the 15% of tokens that were masked. It\u2019s hard to come up with an easily interpretable evaluation metric for this model, but the per-token KL divergences were ~0.25 for our model versus ~0.5 for the baseline of saying all tokens are equally hard to predict.", "title": "Strategic Masking Model"}}}