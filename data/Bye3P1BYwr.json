{"paper": {"title": "Deep End-to-end Unsupervised Anomaly Detection ", "authors": ["Li Tangqing", "Wang Zheng", "Liu Siying", "Daniel Lin Wen-Yan"], "authorids": ["li_tangqing@u.nus.edu", "sliu50@illinois.edu", "zhwang@i2r.a-star.edu.sg", "daniellin@smu.edu.sg"], "summary": "", "abstract": "This paper proposes a novel method to detect anomalies in large datasets under a fully unsupervised setting. The key idea behind our algorithm is to learn the representation underlying normal data. To this end, we leverage the latest clustering\ntechnique suitable for handling high dimensional data. This hypothesis provides a reliable starting point for normal data selection. We train an autoencoder from the normal data subset, and iterate between hypothesizing normal candidate subset\nbased on clustering and representation learning. The reconstruction error from the learned autoencoder serves as a scoring function to assess the normality of the data. Experimental results on several public benchmark datasets show that the proposed method outperforms state-of-the-art unsupervised techniques and is comparable to semi-supervised techniques in most cases.", "keywords": []}, "meta": {"decision": "Reject", "comment": "The authors propose an approach for anomaly detection in the setting where the training data includes both normal and anomalous data.  Their approach is a fairly straightforward extension of existing ideas, in which they iterate between clustering the data into normal vs. anomalous and learning an autoencoder representation of normal data that is then used to score normality of new data.  The results are promising, but the experiments are fairly limited.  The authors argue that their experimental settings follow those of prior work, but I think that for such an incremental contribution, more empirical work should be done, regardless of the limitations of particular prior work."}, "review": {"SklXH3l2jB": {"type": "rebuttal", "replyto": "Bye3P1BYwr", "comment": "We would like to thank the reviewers for their valuable time and comments.  Our reviews are mixed, with R1=Weak Accept,  R2=Reject, and R3=Weak Reject.   We believe we can address the concerns raised by the reviewers that have led to the lower scores, especially R2 who has raised concerns on the novelty of the proposed method.  We have re-run our KDD experiments on the suggested algorithms and added the AUPRC evaluation metric. In this regard, our method demonstrates the best performance among unsupervised methods, and comparable performance to semi-supervised counterparts.  We would like to emphasize that our proposed method is an UNSUPERVISED method which did not use any label information, even in the training phase. Some methods, although claimed as unsupervised, require label information, because only normal data is used in the training. For example, in the GAN-based methods, normal data is fed into the discriminator during training. Hence, methods such as IGMM-GAN or GPND (mentioned by R3) should be classified under semi-supervised. It should be noted that while comparing with semi-supervised methods can put our method into perspective, it is UNFAIR to compare our method directly with semi-supervised methods.\n\nMoreover, we would like to highlight that our model works with the least assumption on the data itself. Our ONLY assumption is that the anomalies are not statistically dominant in the entire dataset (and for this exact reason they are anomalies by nature). We do not make assumptions on how many classes are present in the normal data, unlike the semi-supervised methods, which require pre-isolation/identification of classes of normal data. The superiority of our method is more evident in finding undefined anomaly in video data, as demonstrated in section 4.3 of the paper.\n\nWe sincerely hope the reviewers to reconsider their scores after our rebuttal, we believe we can address all concerns in the final revision. \n\nWe have incorporated the comments in our revision (please refer to revised.pdf). The changes are highlighted in red.\n", "title": "General Response to All Reviewers"}, "rylzfReniH": {"type": "rebuttal", "replyto": "SJlQMrZjtS", "comment": "Thank you for your valuable time and comments\n\n1. Comments on formatting and typographical errors.\n\nThank you for your careful reading. We have revised our paper per your suggestions. \n\n2. Is the DAGMM method SOTA in anomaly detection with deep autoencoder? \n\nTo our best knowledge, it is the SOTA for UNSUPERVISED anomaly detection.  We have compared against GAN methods as recommended. However, available GAN methods, such as IGMM-GAN and GPND mentioned by the reviewer, require labels of normal data for training discriminator. Our method, despite being fully UNSUPERVISED, performed competitively. \n\nFor IGMM-GAN, unfortunately we did not find publicly available code, so we cannot compare the performance. \n\n For GPND, when rerun on MNIST dataset with \u20184\u2019 as normal data, we found out that the source code uses training label information to search for the optimal threshold for F1/AUROC calculation, so they can report very good result. This trick gives the method unfair advantage. For 20% of anomaly, they achieved F1 score of 0.966 and AUROC score of 0.9737. For 50% of anomaly, they achieve even higher AUROC of 0.9748. This makes the GPND results dubious. \n\n We also evaluate another semi-supervised GAN method done by Zaneti2018*. This paper, is claimed to be the improved version of \u201cEfficient GAN-based anomaly detection(https://arxiv.org/abs/1802.06222), mentioned by the reviewer. Please note that it is still semi-supervised because it requires training with normal data only. Their code is publicly available. The re-run on KDD dataset shows that the best AUC-ROC and AUC-PRC achievable is 0.9925 and 0.9325; whereas for our method, the AU-ROC on KDD dataset is 0.935. Despite being unsupervised, our method performs competitively.\n\n*Adversarially Learned Anomaly Detection(Zaneti, ICDM2018) (https://arxiv.org/abs/1812.02288)    \n\n3. Why was the only selected digit for analysis is 4?  \n\nAs 2 of published works*, that  we are comparing with, only use digit 4 (as normal data) for analysis, we believe this is an established benchmark. We therefore follow the protocol specified in Zhou2017 and Chalapathy2018. We tested our method by replacing digit 4 with other digits and it achieved similar results.\n              \n We note that in some literatures, such as \u201cEfficient GAN-based anomaly detection\u201d(https://arxiv.org/abs/1802.06222) and \u201cA survey on GANs for anomaly detection\u201d(https://arxiv.org/abs/1906.11632), each figure in MNIST dataset is treated as anomaly data, while the rest of the digits are treated as normal ones. We did not adopt this setup, because it is more suitable for semi-supervised methods who are given the labels of normal classes, such that discriminators from normal data can be learned. In this setup, when there\u2019s a subset in the test data the network never \u201csees\u201d, it is deemed \u2018anomaly\u2019. We do not think it is truly unsupervised.\n   \nTherefore, in our evaluation we follow the methodology of the 2 afore-mentioned UNSUPERVISED works, Zhou2017 and Chalapathy2018, for a fairer comparison on the MNIST performance.\n\n*Anomaly Detection with Robust Deep Autoencoders(Zhou, KDD2017)\n(https://www.eecs.yorku.ca/course_archive/2017-18/F/6412/reading/kdd17p665.pdf)\n*Anomaly Detection using One-Class Neural Networks(Chalapathy, KDD2018)(https://arxiv.org/pdf/1802.06360v1.pdf)\n\n\n4. Other metrics like AUPRC, F1-scores will better reflect the work of the algorithms in comparison.\n\nThank you for the advice. We have added evaluation based AUPRC metric in our revision. Please refer to Table 3 in the revision.\n", "title": "Response to Reviewer #3"}, "rylFrTg3sH": {"type": "rebuttal", "replyto": "B1g4zUaotB", "comment": "We appreciate your constructive comments.  \n\n1. How is performance affected by very low ratio of anomalies? Results on 2%, 3% anomalies?\n\nWe assessed our model on datasets with smaller anomaly ratios: 1%, 2%, 3%, 4% and 5%. Both averaged AUROC and AUPRC scores indicate consistent and robust performance.  \n\n| Anomaly ratio | AUROC       | AUPRC      |\n| ------------------- | ---------------|--------------- |\n| 1                        | 98.2 +- 0.1 | 90.0 +- 1.0 |\n| 2                        | 98.1 +- 0.1 | 91.8 +- 0.2 |\n| 3                        | 98.4 +- 0.2 | 92.4 +- 0.1 |\n| 4                        | 98.2 +- 0.1 | 91.3 +- 0.9 |\n| 5                        | 99.0 +- 0.3 | 94.5 +- 0.1 |\n\n\n2. Comparison with Zhou 2017 paper?\n\nSimilar to Zhou\u2019s paper, our method does not require any noise-free training data for model training. Both Zhou\u2019s method and our method does not need any data label. However, Zhou\u2019s paper use L1 or L2,1 as penalty terms, while our method combined reconstruction error and the learned low-dimensional representation (bottleneck feature from AE) in the training process. In Zhou\u2019s paper, it has been reported that the  best F1-Score (based on threshold that gave the best trade-off between precision and recall) for MNIST dataset where \u201c4\u201d is treated as normal data (remaining digits are anomaly) is 64%(Figure 5). In our paper, the ROC figure reported is (70.5+/-2.1)%, which validates the performance of our method. Unfortunately, Zhou\u2019s paper does not evaluate on datasets other than MNIST.\n", "title": "Response to Reviewer #1"}, "rkxuJ6x3sS": {"type": "rebuttal", "replyto": "SJx1fc7aKr", "comment": "We thank the reviewer for the comments. We feel there might be misunderstanding that needs to be clarified. \n\n1. \u201cNovelty of this paper is in doubt. .. reconstruction error of AE has been explored thoroughly. Extension seems to be straightforward. \u201c\n\nThe novelty of this paper lies in the fusion of data selection (based on clustering) and representation learning for anomaly detection under a fully unsupervised setting. By incorporating clustering to provide supervisory signals, we iterate between hypothesizing normal candidate subset and representation learning. This framework is carefully engineered to distill out anomalous data and improve the learned representation of normal data. \n\nWe would like to clarify that our method did not use reconstruction error alone. Instead, it combined reconstruction error and the learned low-dimensional representation (bottleneck feature from AE) in the training process. Using reconstruction error alone is insufficient for classifying normal/anomalous samples in the training process. The reasons are two-fold: \n\nFirst, in the early iterations, as our algorithm adopts a tight cut-off to guess the normal candidates for auto-encoder training, some excluded but true normal samples will see large reconstruction errors. Second, during the intermediate steps, it is hard to draw a decision boundary based on the reconstruction errors (scalar values) alone. Empirically, we have tested using only the reconstruction error for all our experiments and it was confirmed to perform poorly. When combining cosine error and bottleneck feature, we re-normalized the vector after concatenation. We found such combined feature more discriminative for training set selection. The bottleneck feature, working as a low-dimensional embedding, improves the similarity measure in a clustering process. We will add this discussion to the revision.\n\n2. Dependency on initialization\n\nWe discussed this issue in the submission. Our method starts with a tight variance cut-off to ensure the initial training set is as pure as possible. This means some of the true normal samples could be discarded in this initialization. Our assumption is that given partial normal data, the autoencoder would be able to generalize well on the \u201cunseen\u201d normal data that was discarded, and progressively recover them as iterations go on. We also presented in Fig. 4, Appendix section that empirically, low-variance clusters correspond well to normal samples, while higher cluster variances suggest the presence of anomalous samples. In Fig. 5, we showed the convergence behavior, with initial threshold p_0 (assumed anomaly percentage) set to 30%, while ground truth is 20%. \n\n3. Problematic refining process\n\nWe would like to address that the re-clustering of data in each iteration, is not solely based on reconstruction error. Our error handling mechanism which combines reconstruction error and the learned low-dimensional representation (bottleneck feature from AE), improves the performance of method from pure clustering, as validated in Tab 3, right-most column.\n\n4. Poor result on MNIST dataset \n\nWe explained in our submission that this is due to the MNIST data being innately not high-dimensional. The NetVlad feature may have been an overkill. We re-run our experiment using raw images, and it achieved AUROC 0.824 and F1-score 0.97 (based on threshold that gave the best trade-off between precision and recall). In terms of AUROC, this result is ~30% better than state-of-the-art unsupervised method, DAGMM. Also, we would like to bring to your attention that our results on more complex dataset are significantly better compared to other UNSUPERVISED methods, and comparable with semi-supervised methods. As far as we know, we are the first paper to obtain such a good performance on unsupervised anomaly detection on video data.", "title": "Response to Reviewer #2"}, "SJlQMrZjtS": {"type": "review", "replyto": "Bye3P1BYwr", "review": "In the article, the authors solve the problem of anomaly detection using a fully unsupervised approach. They try to deal with the main challenge of anomaly detection: a lack of certainty on what defines normal data and anomaly one. For this purpose, the authors iteratively use: 1) autoencoders to learn the representation of the data; 2) applying in the latent space clustering to get a new training set and retrain autoencoders. The experimental results show that the author\u2019s method performed better results than such a baseline model as one-class SVM and one-class NN. \n \nThe proposed algorithm looks robust and well-motivated, but the text of the article and the experiments can be improved. As the proposed approach is a heuristic, the experiments should be done more persuasively, including more metrics used and more alternative algorithms considered. \n\nThe key comments are the following:\n1. The formatting of the article needs to be improved e.g.:\n2. there is no comma between rows in the equation (1) ;\n<<to be accepted into the \u201ctraining\u201d set, .>> - there is an extra comma;\n3. round brackets in the equation (6) should be bigger;\n4. Table 3 is bigger than the page sizes.\n5. The quality of the pictures should be improved:\n- Increase the captions font size in Figure 2;\n- The captions and the legends in Figure 3 are practically not visible;\n6.  Is the DAGMM method SOTA in anomaly detection with deep autoencoder? There are many other methods with similar ideas. We expect that we should provide a comparison with other methods: \nhttps://arxiv.org/pdf/1809.02728.pdf - IGMM-GAN\nhttps://papers.nips.cc/paper/7915-generative-probabilistic-novelty-detection-with-adversarial-autoencoders.pdf - GPND AE\n6. Also, DAGMM works badly according to the experiments in the article with max AUROC in Table 1 only 50.3 (so it seems that it is no better than the coin-flipping)\n7. Why was the only selected digit for analysis 4? Usual for comparison anomaly detection on MNIST dataset apply the following procedure: for each figure in dataset consider corresponded class as anomaly data, and the rest of the digits are used as normal data, e.g.:\nhttps://arxiv.org/pdf/1802.06222.pdf\nhttps://arxiv.org/pdf/1906.11632.pdf \n8.  Class imbalances can affect the value of the AUROC metric. Possibly, the other metrics like AUPRC, F1-scores will better reflect the work of the algorithms for comparison. Also, AUROC is not representative when it comes to the selection of the threshold for anomaly detection. Precision and Recall can help to get more insights.\n9. In Table 3, the result of applying the proposed algorithm presented with standard deviation, but other methods are represented by one metric value. Why? The explanation is required.", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 3}, "B1g4zUaotB": {"type": "review", "replyto": "Bye3P1BYwr", "review": "In this paper the authors propose a framework for anomaly detection. The method is based on autoencoders and reconstruction error, but instead of training the autoencoder using all the data-points, the method iteratively uses some form of clustering to determine the points which presumably belong to the normal set, and uses them for training the autoencoder. This helps make the method robust when the portion of anomalous data-points is high.\n\nThe paper is well-written and clear and the proposed architecture is novel to my knowledge. Nevertheless, I have the following concerns about the paper. Given clarifications in the author response, I would be willing to increase the score. \n\n- In terms of novelty, separating the dataset into normal + outliers/noise is not novel (Zhou 2017 cited in the paper). The novel part here is perhaps using variance and clustering for making the separation. However, using variance is not well motivated and it is referred to Figure 4, in which the argument is not clear. Similarly, why the reconstruction error is included in the latent representation (eq 5) is not clear. \n\n- Given the similarity of the idea to Zhou 2017, the comparison seems important (if the code is available), and also proper discussion of such similar works is required, which currently is not presented.\n\n- How is the performance affected by very low ratio of anomalies? This can be shown by including %2,%3 of anomalies in Table 2. \n\n- The sensitivity of the results to the choice of hyper-parameters: p0, p, and r is not clear, and how these parameters are chosen is not discussed. It would be interesting to see how the performance is affected by different choices of the hyper-parameters.", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 3}, "SJx1fc7aKr": {"type": "review", "replyto": "Bye3P1BYwr", "review": "The paper proposed an unsupervised anomaly detection method for the scenarios where the training data not only includes normal data but also a lot of anomaly data. The basic idea of this paper is to iteratively refine the normal data subset, selected from the whole training data set. Specifically, the paper first train an auto-encoder (AE) and determine which are the normal data samples according to the reconstruction errors. Then, using the normal data to retrain the AE again, and re-select the normal samples. Repeat the above two steps until convergence. \n\nOverall, the novelty of this paper is in doubt. Detecting anomaly by reconstruction error of AE has been explored thoroughly, and this paper only extends it to iteratively select the normal samples. The extension seems to be very straightforward. \n\nAlso, the refining process is also problematic. It will highly depend on the initial selection, and the error will be propagated to subsequent detections. How to determine which data samples are anomalous is a key to the success of the model, but the proposed method based on the variance assumption is too intuitive and not convincing.\n\nIn addition, the experimental results on the very simple MNIST task is very poor, putting the effectiveness of the proposed model in doubt.\n", "title": "Official Blind Review #2", "rating": "1: Reject", "confidence": 2}}}