{"paper": {"title": "Third Person Imitation Learning", "authors": ["Bradly C Stadie", "Pieter Abbeel", "Ilya Sutskever"], "authorids": ["bstadie@openai.com", "pieter@openai.com", "ilyasu@openai.com"], "summary": "Agent watches another agent at a different camera angle completing the task and learns via raw pixels how to imitate. ", "abstract": "Reinforcement learning (RL) makes it possible to train agents capable of achieving\nsophisticated goals in complex and uncertain environments. A key difficulty in\nreinforcement learning is specifying a reward function for the agent to optimize.\nTraditionally, imitation learning in RL has been used to overcome this problem.\nUnfortunately, hitherto imitation learning methods tend to require that demonstrations\nare supplied in the first-person: the agent is provided with a sequence of\nstates and a specification of the actions that it should have taken. While powerful,\nthis kind of imitation learning is limited by the relatively hard problem of collecting\nfirst-person demonstrations. Humans address this problem by learning from\nthird-person demonstrations: they observe other humans perform tasks, infer the\ntask, and accomplish the same task themselves.\nIn this paper, we present a method for unsupervised third-person imitation learning.\nHere third-person refers to training an agent to correctly achieve a simple\ngoal in a simple environment when it is provided a demonstration of a teacher\nachieving the same goal but from a different viewpoint; and unsupervised refers\nto the fact that the agent receives only these third-person demonstrations, and is\nnot provided a correspondence between teacher states and student states. Our\nmethods primary insight is that recent advances from domain confusion can be\nutilized to yield domain agnostic features which are crucial during the training\nprocess. To validate our approach, we report successful experiments on learning\nfrom third-person demonstrations in a pointmass domain, a reacher domain, and\ninverted pendulum.", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "pros:\n - new problem\n - huge number of experimental evaluations, based in part on open-review comments\n \n cons:\n - the main critiques related to not enough experiments being run; this has been addressed in the revised version\n \n The current reviewer scores do not yet reflect the many updates provided by the authors.\n I therefore currently learn in favour of seeing this paper accepted."}, "review": {"B1N4238Le": {"type": "rebuttal", "replyto": "SJezwxzEg", "comment": "We thank you for your constructive feedback, which significantly improved the quality of this paper.  We were able to incorporate 16 new experiments and other improvements.  We believe this has further strengthened the work and the paper (beyond its state at initial submission) and hope that you will potentially consider updating your score with this in mind. ", "title": "significantly improved experiments"}, "Hyv9khVLx": {"type": "rebuttal", "replyto": "B1uj8o-Ee", "comment": "\nQuestion: \nI would have expected to see comparison to the following methods added to Figure 3:\n1)  Standard 1st person imitation learning using agent A data, and apply the policy on agent A.  This is an upper-bound on how well you can expect to do, since you have the correct perspective.\n\n2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.  Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance.\n\n\n3)  Reinforcement learning using agent A data, and apply the policy on agent A.  I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents). I understand this is how the expert data is collected for the demonstrator, but I don\u2019t see the performance results from just using this procedure on the learner (to compare to Fig.3 results).\n\nAnswer: \nThank you for suggesting these experiment. We have added all of them to Appendix A. We feel like this suggestion significantly improved the quality of the paper. Please see earlier responses/the paper itself for a discussion of these experiments. \n", "title": "Added these three experiments in Appendix A "}, "SywPy24Ie": {"type": "rebuttal", "replyto": "HJsxV1GVx", "comment": "Question: \n- The paper feels at times to be a bit hurriedly written (this also mainly manifests itself in the experiments, see comment below) and makes a few fairly strong claims in the introduction that in my opinion are not backed up by their approach. For example: \"Advancements in this class of algorithms would significantly improve the state of robotics, because it will enable anyone to easily teach robots new skills\"; given that the current method to my understanding has the same issues that come with standard GAN training (e.g. instability etc.) and requires a very accurate simulator to work well (since TRPO will require a large number of simulated trajectories in each step) this seems like an overstatement.\n\nAnswer: \nThe claim is that advancements in this class of algorithms [i.e., third person imitation learning algorithms] would significantly improve the state of robotics. Ergo, it is a problem worthy of considering, stating formally, and offering a first solution attempt. We do not claim that our specific solution significantly advances the state of robotics.  This paper is an attempt to draw attention to the problem, not to conclusively solve it, but rather providing a contribution that can be built on. \n\nQuestion: \nThere are some sentences that are ungrammatical or switch tense in the middle of the sentence making the paper harder to read than necessary, e.g. Page 2: \"we find that this simple approach has been able to solve the problems\" \n\nAnswer: \nWe will extra carefully edit the final version of the paper (up to and including hiring outside grammar and style police to ensure the highest possible quality).\n\nQuestion: \nThe general idea of third person imitation learning is nice, clear and (at least to my understanding) also novel. However, instead of exploring how to generally adapt current IRL algorithms to this setting the authors pick a specific approach that they find promising (using GANs for IRL) and extend it. A significant amount of time is then spent on explaining why current IRL algorithms will fail in the third-person setting. I fail to see why the situation for the GAN based approach is any different than that of any other existing IRL algorithm. To be more clear: I see no reason why e.g. behavioral cloning could not be extended with a domain confusion loss in exactly the same way as the approach presented. \nAnswer: \nThank you, and this is an excellent suggestion (which we hope to pursue in future work), but beyond the scope of this current paper, which already studies many factors even while staying within the IRL setting. Indeed, to fully flesh out everything you suggest here could easily fill a 30 page paper if done thoroughly, making it far too large for the venue! At some point, a narrowing of focus is required. And we felt IRL was the appropriate scope for this work. As a compromise, we would be willing to rename the paper \u201cThird Person Inverse Reinforcement Learning\u201d if the reviewer feels this distinction is essential to maintain. \n\nIt is worth noting that behavioral cloning requires access to the raw actions taken by both agents.  Hence the requirements for behavioral cloning are significantly more constraining in terms of how data is collected.  In this work, we are considering the setting where there is only access to a video of the behavior we are trying to imitate.   We will discuss this difference more explicitly in the next revision of the paper.\n\nQuestion: \nTo this end it would have been nice to rather discuss which algorithms can be adapted in the same way (and also test them) and which ones cannot. One straightforward approach to apply any IRL algorithm would for example be to train two autoencoders for both domains that share higher layers + a domain confusion loss on the highest layer, should that not result in features that are directly usable? If not, why?\n\nAnswer: \nWe actually tried this in our initial investigations for 3rd person imitation learning.  Our findings suggested it wasn\u2019t as promising a direction, hence we proceeded along the reported line of research.   We did not mention these attempts in the paper as we discarded it for our own purposes as not as promising, but don\u2019t want to necessarily discourage others from pushing this harder (than we believe is warranted for our own time).  \n\nQuestion: \n- While the general argument that existing IRL algorithms will fail in the proposed setting seems reasonable it is still unfortunate that no attempts have been made to validate this empirically. No comparison is made regarding what happens when one e.g. performs supervised learning (behavioral cloning) \n\nAnswer: \nWrt behavioral cloning, see above.  \nWrt existing IRL algorithms -- as discussed in the paper, generally speaking there are two categories: (i) using hand-engineering/crafted features of which the reward is a linear combination, (ii) using arbitrary reward functions (as could be represented by a generic neural net, or in earlier work, by a generic Gaussian process).   For (i), as discussed in the paper, if carefully choosing these features, one might be able to choose them to be domain invariant in some situations, but this will require extensive engineering effort in every new setting.  We are pursuing (ii), as has recent work by Ho et al (nips16), and Finn et al (icml16), but their work assumed demonstrator and agent act in the same view / environment, and their work would fail -- indeed, when we remove the multi-time frame input and the domain confusion, ours corresponds to Ho et al, and performance drops significantly.\n\n\n\nQuestion: \nusing the expert observations and then transfers to the changed domain. How well would this work in practice ? Also, how fast can different IRL algorithms solve the target task in general (assuming a first person perspective) ?\n\nAnswer: \nThank you for this suggestion. We have added further comparisons accordingly in appendix A. We see that first person IRL and RL both solve the task moderately faster (requiring for instance 7 epochs of training to reach the final performance attained by third person imitation after 20 epochs). However, we see that the final performance of our algorithm is still comparable. Further, we see that attempting to apply the policy learned from the first person cost on the third person agent utterly fails for all three environments, suggesting that something like our algorithm is necessary in the case of third-person imitation learning. \n\n\nQuestion: \n- Although I like the idea of presenting the experiments as being directed towards answering a specific set of questions I feel like the posed questions somewhat distract from the main theme of the paper. Question 2 suddenly makes the use of additional velocity information to be a main point of importance and the experiments regarding Question 3 in the end only contain evaluations regarding two hyperparameters (ignoring all other parameters such as the parameters for TRPO, the number of rollouts per iteration, the number of presented expert episodes and  the design choices for the GAN). I understand that not all of these can be evaluated thoroughly in a conference paper but I feel like some more experiments or at least some discussion would have helped here.\n\nAnswer: \nSomewhat happily, there was no hyper-parameter tuning required for TRPO. The hyper-parameters that are supplied in RLLab by default worked out of the box. While we understand that this is sometimes a potential warning sign, please note that the TRPO hyper-parameters have been tuned extensively in past work. The GAN discriminator neural network architecture choices and hyper-parameters were taken from Levine et al, JMLR 2016, who extensively investigated these choices to deal with similar (robotics) tasks in the past.  We will clarify this in the paper.\n\nQuestion: \n- The presented experimental evaluation somewhat hides the cost of TRPO training with the obtained reward function. How many roll-outs are necessary in each step?\n\nAnswer: \nBetween 10 and 30.   (This has been clarified in the paper.)\n\n\nQuestion: \n- The experiments lack some details: How are the expert trajectories obtained? \n\nAnswer: \nVia TRPO with full state information. (This has been clarified in the paper.)\n\nQuestion: \nThe domains for the pendulum experiment seem identical except for coloring of the pole, is that correct (I am surprised this small change seems to have such a detrimental effect)? \n\nAnswer: \nIndeed, we chose this domain to investigate how the discriminator and reward function would behave with such clearly distinguishing visual appearance.  If not using domain confusion loss, the reward function immediately uses this bit of information to distinguish between expert vs non-expert trajectories (since all of the expert trajectories have a blue pole and the non-experts have a purple pole).   Consequently, without the domain confusion loss, the learned reward function is meaningless for the test domain.  We will further clarify this in the paper.\n\nQuestion: \nWere you able to re-use the hyperparameters across all experiments?\n\nAnswer: \nYes, aside from the number of expert sample rollouts utilized. This number varied between 10 and 30. \n\nWe thank you for this very thorough and comprehensive review! Carefully considering your comments has allowed us to improve the quality of our paper. \n", "title": "Added more experiments to address your concerns "}, "ryctCj4Ux": {"type": "rebuttal", "replyto": "SJezwxzEg", "comment": "Question: \nWhile the idea is quite elegant and novel (I enjoy reading it), more experiments are needed to justify the approach. Probably the most important issue is that there is no baseline, e.g., what if we train the model with the image from the same viewpoint? \n\nAnswer: \nWe have added new experiments, including training the model with the image from the same viewpoint.   We have also added RL in the test domain and standard first person imitation (i.e. our approach, but without the domain confusion).  All discussed in Appendix A. As we see from these graphs, simply using the cost/policy recovered from first person imitation on the third person agent is not sufficient to learn the task presented. We see that generally RL and first person imitation learning do perform better on these tasks (in terms of sample efficiency and overall performance). However, we feel that third person imitation performs comparably, and this fact is significant enough to warrant strong consideration for accepting this paper.  \n\nQuestion: \nHow the performance changes when we gradually change the viewpoint from third-person to first-person? \nAnswer: \nWe have added this experiment to appendix A. We see that for the point env, performance linearly declines with the difference in camera angle between the expert and the novice. For reacher, the story is more complex and the behavior is more step like. Thank you for suggesting this experiment! \n\nQuestion: \nAnother important question is that maybe the network just blindly remembers the policy, in this case, the extracted feature could be artifacts of the input image that implicitly counts the time tick in some way (and thus domain-agonistic), but can still perform reasonable policy. Since the experiments are conduct in a synthetic environment, this might happen. An easy check is to run the algorithm on multiple viewpoint and/or with blurred/differently rendered images, and/or with random initial conditions.\n\nAnswer: \nWe sample different initial conditions as in the underlying RL Lab environments. We see that the controls generated by different initial conditions are quantitatively different. Further, when we cross-apply the controls generated from one set of initial conditions to another set of initial conditions, we see that performance is generally poor. We are hesitant to add these graphs to the paper, as there are already 16 additional graphs as a result of this rebuttal (on top of the 15 graphs in the original paper for a total of 31). \n\n\nQuestion: \nOther ablation analysis is also needed. For example, I am not fully convinced by the gradient flipping trick used in Eqn. 5, and in the experiments there is no ablation analysis for that (GAN/EM style training versus gradient flipping trick). \n\nAnswer: \nFigure 5 contains an ablation analysis for the gradient trick, concretely, it compares what happen with and without domain confusion, and with and without the velocity information.  The experiments show that having both domain confusion (i.e. gradient flipping trick) and velocity information outperforms ablated variants.\n  \nFigure 6 analyzes performance as a function of the parameter lambda, which weighs the domain confusion loss (against the other losses).  It shows the approach is robust to choices of lambda, however, extreme choices don\u2019t perform well: lambda too small results in domain confusion loss largely ignored and poor performance; lambda too large results in domain confusion loss dominating too much (at the expense of the other losses).\n\nQuestion: \nFor the experiments, Fig. 4,5,6 does not have error bars and is not very convincing.\n\nAnswer: \nWe ran these experiments multiple times, and results were consistent, as in the reported graphs.  We felt that the error bars were distracting for these experiments because they cluttered the graphs. For the final, we will add error bars, and increase the size of the graphs to maintain readability.  \nOur additional experiments (added in rebuttal phase into Appendix A) have error bars, which further suggest the robustness of the algorithm.\n\nNote that there are now 31 experiments detailed in this paper. We feel that adding any more may cause it to burst at the seams! :) \n", "title": "Added more experiments "}, "SkpPEJzEx": {"type": "review", "replyto": "B16dGcqlx", "review": "I apologize for missing the opportunity to ask a pre-review questions. Please see the main review for my questions.The paper presents an interesting new problem setup for imitation learning: an agent tries to imitate a trajectory demonstrated by an expert but said trajectory is demonstrated in a different state or observation space than the one accessible by the agent (although the dynamics of the underlying MDP are shared). The paper proposes a solution strategy that combines recent work on domain confusion losses with a recent IRL method based on generative adversarial networks.\n\nI believe the general problem to be relevant and agree with the authors that it results in a more natural formulation for imitation learning that might be more widely applicable.\nThere are however a few issues with the paper in its current state that make the paper fall short of being a great exploration of a novel idea. I will list these concerns in the following (in arbitrary order)\n- The paper feels at times to be a bit hurriedly written (this also mainly manifests itself in the experiments, see comment below) and makes a few fairly strong claims in the introduction that in my opinion are not backed up by their approach. For example: \"Advancements in this class of algorithms would significantly improve the state of robotics, because it will enable anyone to easily teach robots new skills\"; given that the current method to my understanding has the same issues that come with standard GAN training (e.g. instability etc.) and requires a very accurate simulator to work well (since TRPO will require a large number of simulated trajectories in each step) this seems like an overstatement.\n  There are some sentences that are ungrammatical or switch tense in the middle of the sentence making the paper harder to read than necessary, e.g. Page 2: \"we find that this simple approach has been able to solve the problems\"\n- The general idea of third person imitation learning is nice, clear and (at least to my understanding) also novel. However, instead of exploring how to generally adapt current IRL algorithms to this setting the authors pick a specific approach that they find promising (using GANs for IRL) and extend it. A significant amount of time is then spent on explaining why current IRL algorithms will fail in the third-person setting. I fail to see why the situation for the GAN based approach is any different than that of any other existing IRL algorithm. To be more clear: I see no reason why e.g. behavioral cloning could not be extended with a domain confusion loss in exactly the same way as the approach presented. To this end it would have been nice to rather discuss which algorithms can be adapted in the same way (and also test them) and which ones cannot. One straightforward approach to apply any IRL algorithm would for example be to train two autoencoders for both domains that share higher layers + a domain confusion loss on the highest layer, should that not result in features that are directly usable? If not, why?\n- While the general argument that existing IRL algorithms will fail in the proposed setting seems reasonable it is still unfortunate that no attempts have been made to validate this empirically. No comparison is made regarding what happens when one e.g. performs supervised learning (behavioral cloning) using the expert observations and then transfers to the changed domain. How well would this work in practice ? Also, how fast can different IRL algorithms solve the target task in general (assuming a first person perspective) ?\n- Although I like the idea of presenting the experiments as being directed towards answering a specific set of questions I feel like the posed questions somewhat distract from the main theme of the paper. Question 2 suddenly makes the use of additional velocity information to be a main point of importance and the experiments regarding Question 3 in the end only contain evaluations regarding two hyperparameters (ignoring all other parameters such as the parameters for TRPO, the number of rollouts per iteration, the number of presented expert episodes and  the design choices for the GAN). I understand that not all of these can be evaluated thoroughly in a conference paper but I feel like some more experiments or at least some discussion would have helped here.\n- The presented experimental evaluation somewhat hides the cost of TRPO training with the obtained reward function. How many roll-outs are necessary in each step?\n- The experiments lack some details: How are the expert trajectories obtained? The domains for the pendulum experiment seem identical except for coloring of the pole, is that correct (I am surprised this small change seems to have such a detrimental effect)? Figure 3 shows average performance over 5 trials, what about Figure 5 (if this is also average performance, what is the variance here)? Given that GANs are not easy to train, how often does the training fail/were you able to re-use the hyperparameters across all experiments?\n\nUPDATE:\nI updated the score. Please see my response to the rebuttal below.\n", "title": "Missing", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJsxV1GVx": {"type": "review", "replyto": "B16dGcqlx", "review": "I apologize for missing the opportunity to ask a pre-review questions. Please see the main review for my questions.The paper presents an interesting new problem setup for imitation learning: an agent tries to imitate a trajectory demonstrated by an expert but said trajectory is demonstrated in a different state or observation space than the one accessible by the agent (although the dynamics of the underlying MDP are shared). The paper proposes a solution strategy that combines recent work on domain confusion losses with a recent IRL method based on generative adversarial networks.\n\nI believe the general problem to be relevant and agree with the authors that it results in a more natural formulation for imitation learning that might be more widely applicable.\nThere are however a few issues with the paper in its current state that make the paper fall short of being a great exploration of a novel idea. I will list these concerns in the following (in arbitrary order)\n- The paper feels at times to be a bit hurriedly written (this also mainly manifests itself in the experiments, see comment below) and makes a few fairly strong claims in the introduction that in my opinion are not backed up by their approach. For example: \"Advancements in this class of algorithms would significantly improve the state of robotics, because it will enable anyone to easily teach robots new skills\"; given that the current method to my understanding has the same issues that come with standard GAN training (e.g. instability etc.) and requires a very accurate simulator to work well (since TRPO will require a large number of simulated trajectories in each step) this seems like an overstatement.\n  There are some sentences that are ungrammatical or switch tense in the middle of the sentence making the paper harder to read than necessary, e.g. Page 2: \"we find that this simple approach has been able to solve the problems\"\n- The general idea of third person imitation learning is nice, clear and (at least to my understanding) also novel. However, instead of exploring how to generally adapt current IRL algorithms to this setting the authors pick a specific approach that they find promising (using GANs for IRL) and extend it. A significant amount of time is then spent on explaining why current IRL algorithms will fail in the third-person setting. I fail to see why the situation for the GAN based approach is any different than that of any other existing IRL algorithm. To be more clear: I see no reason why e.g. behavioral cloning could not be extended with a domain confusion loss in exactly the same way as the approach presented. To this end it would have been nice to rather discuss which algorithms can be adapted in the same way (and also test them) and which ones cannot. One straightforward approach to apply any IRL algorithm would for example be to train two autoencoders for both domains that share higher layers + a domain confusion loss on the highest layer, should that not result in features that are directly usable? If not, why?\n- While the general argument that existing IRL algorithms will fail in the proposed setting seems reasonable it is still unfortunate that no attempts have been made to validate this empirically. No comparison is made regarding what happens when one e.g. performs supervised learning (behavioral cloning) using the expert observations and then transfers to the changed domain. How well would this work in practice ? Also, how fast can different IRL algorithms solve the target task in general (assuming a first person perspective) ?\n- Although I like the idea of presenting the experiments as being directed towards answering a specific set of questions I feel like the posed questions somewhat distract from the main theme of the paper. Question 2 suddenly makes the use of additional velocity information to be a main point of importance and the experiments regarding Question 3 in the end only contain evaluations regarding two hyperparameters (ignoring all other parameters such as the parameters for TRPO, the number of rollouts per iteration, the number of presented expert episodes and  the design choices for the GAN). I understand that not all of these can be evaluated thoroughly in a conference paper but I feel like some more experiments or at least some discussion would have helped here.\n- The presented experimental evaluation somewhat hides the cost of TRPO training with the obtained reward function. How many roll-outs are necessary in each step?\n- The experiments lack some details: How are the expert trajectories obtained? The domains for the pendulum experiment seem identical except for coloring of the pole, is that correct (I am surprised this small change seems to have such a detrimental effect)? Figure 3 shows average performance over 5 trials, what about Figure 5 (if this is also average performance, what is the variance here)? Given that GANs are not easy to train, how often does the training fail/were you able to re-use the hyperparameters across all experiments?\n\nUPDATE:\nI updated the score. Please see my response to the rebuttal below.\n", "title": "Missing", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJu-_seVe": {"type": "rebuttal", "replyto": "HyfOgMJmg", "comment": "\n\nQuestion: How is the expert data generated? \n\nAnswer: The expert data is generated via an RL algorithm (TRPO) that has access to full state information and reward signal. In fact, this RL algorithm is just the out of the box TRPO from RLLab being used to solve the reacher, point, and pendulum environments from RLLab. \n\nQuestion: Also, there is no detailed specification of network architecture (e.g., what layers did you use? How complicated is D_F, D_R and D_D) in the paper besides Fig. 2. Please elaborate.\n\nAnswer: The joint feature extractor is 2 convolutional layers with 5 filters of size 3 each. Each layer is followed by a max-polling layer with size 2. The input images are size 50 x 50 with 3 channels, RGB. \nBoth the domain classifier and the class discriminator take as input the domain-agnostic image features and pass them through 2 MLP layers of size 128 before going through the final MLP layer of size 2 and a softmax layer. We have clarified this in the updated paper in Appendix A.\n\nIn addition, if any further architecture questions should arise, the discriminator and generator code is available here: \nhttps://github.com/bstadie/third_person_im/blob/master/sandbox/bradly/third_person/discriminators/discriminator.py\n\n\n", "title": "Answer to questions"}, "SJ1rDje4g": {"type": "rebuttal", "replyto": "Hku195k7l", "comment": "\n\nQuestion: On p.5 you choose to instantiate the mutual information by introducing another classifier.  Is this common? Can you add a reference?  Or is it novel?  What are the impacts of this choice? \n\nAnswer: This is not novel. A similar idea is proposed in for instance \nJ. S. Bridle, A. J. Heading, and D. J. MacKay, \u201cUnsupervised classifiers, mutual information and \u2019phantom targets\u2019,\u201d in NIPS, 1992.\nD. Barber and F. V. Agakov, \u201cKernelized infomax clustering,\u201d in NIPS, 2005, pp. 17\u201324.\nA. Krause, P. Perona, and R. G. Gomes, \u201cDiscriminative clustering by regularized information maximization,\u201d in NIPS, 2010, pp. 775\u2013783.\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. CoRR, abs/1606.03657, 2016.\n\nWe have updated the paper to explicitly mention these references in section 5.1.\n\nQuestion: If I understand correctly, the imitation task is reduced to predicting whether the demonstrations come from Expert or Non-expert.  Then the label is associated with a (known/stored) reward function, and that reward function is used in TRPO.  Is that correct?  It seems that is a lot of information, compared to the standard imitation learning setup, where the policy must be fully learned from demonstrations.  Am I misunderstanding something?\n\nAnswer: This is incorrect. TRPO receives as a reward the (learned) log probability that a set of demonstration frames belongs to the expert class, and attempts to maximize this number. In doing so, it is trying to learn a policy that generates more frames that look like they belong to the expert class. TRPO never has access to the raw reward of the system. This problem setup is identical to that presented by Ho in Generative Adversarial Imitation Learning. Line 29 of Algorithm 1 makes this explicit. It is also shown and specified in figure 2. \n\nQuestion: With so many terms being simultaneously optimized, it is not so surprising that it is difficult to get stable learning. How do you optimize hyper-parameters in this setting?   Presumably you cannot use a hold-out set.  Fig.6 & 7 show some sensitivity analysis, but nothing is shown about the learning rates.\n\nAnswer: Hyperparameters were optimized with a standard hold-out procedure, where the same hyperparameter choices were required across all problem instances.  For the discriminator, we use ADAM and a learning rate of 0.001. This worked out of the box. For the RL generator, we used the standard TRPO implementation from RLLab and made no changes. \n\nQuestion: I would have expected to see comparison to the following methods added to Figure 3:\n1)  Standard 1st person imitation learning using agent A data, and apply the policy on agent A.  This is an upper-bound on how well you can expect to do, since you have the correct perspective.\n2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.  Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance.\n3)  Reinforcement learning using agent A data, and apply the policy on agent A.  I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents). It would be nice to show this.\n\nAnswer: 1) and 2) are great suggestions for further calibrating the results, thank you, we plan to incorporate them in the next revision of the paper.\n3) is actually already how the expert demonstration data is generated.\n\n\n", "title": "Reply to reviewer comments "}, "Hku195k7l": {"type": "review", "replyto": "B16dGcqlx", "review": "On p.5 you choose to instantiate the mutual information by introducing another classifier.  Is this common? Can you add a reference?  Or is it novel?  What are the impacts of this choice?\n\nIf I understand correctly, the imitation task is reduced to predicting whether the demonstrations come from Expert or Non-expert.  Then the label is associated with a (known/stored) reward function, and that reward function is used in TRPO.  Is that correct?  It seems that is a lot of information, compared to the standard imitation learning setup, where the policy must be fully learned from demonstrations.  Am I misunderstanding something?\n\nWith so many terms being simultaneously optimized, it is not so surprising that it is difficult to get stable learning. How do you optimize hyper-parameters in this setting?   Presumably you cannot use a hold-out set.  Fig.6 & 7 show some sensitivity analysis, but nothing is shown about the learning rates.\n\nI would have expected to see comparison to the following methods added to Figure 3:\n1)  Standard 1st person imitation learning using agent A data, and apply the policy on agent A.  This is an upper-bound on how well you can expect to do, since you have the correct perspective.\n2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.  Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance.\n3)  Reinforcement learning using agent A data, and apply the policy on agent A.  I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents). It would be nice to show this.The paper extends the imitation learning paradigm to the case where the demonstrator and learner have different points of view. This is an important contribution, with several good applications.  The main insight is to use adversarial training to learn a policy that is robust to this difference in perspective.  This problem formulation is quite novel compared to the standard imitation learning literature (usually first-order perspective), though has close links to the literature on transfer learning (as explained in Sec.2).\n\nThe basic approach is clearly explained, and follows quite readily from recent literature on imitation learning and adversarial training.\n\nI would have expected to see comparison to the following methods added to Figure 3:\n1)  Standard 1st person imitation learning using agent A data, and apply the policy on agent A.  This is an upper-bound on how well you can expect to do, since you have the correct perspective.\n2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.  Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance.\n3)  Reinforcement learning using agent A data, and apply the policy on agent A.  I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents). I understand this is how the expert data is collected for the demonstrator, but I don\u2019t see the performance results from just using this procedure on the learner (to compare to Fig.3 results).\n\nIncluding these results would in my view significantly enhance the impact of the paper.", "title": "Questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1uj8o-Ee": {"type": "review", "replyto": "B16dGcqlx", "review": "On p.5 you choose to instantiate the mutual information by introducing another classifier.  Is this common? Can you add a reference?  Or is it novel?  What are the impacts of this choice?\n\nIf I understand correctly, the imitation task is reduced to predicting whether the demonstrations come from Expert or Non-expert.  Then the label is associated with a (known/stored) reward function, and that reward function is used in TRPO.  Is that correct?  It seems that is a lot of information, compared to the standard imitation learning setup, where the policy must be fully learned from demonstrations.  Am I misunderstanding something?\n\nWith so many terms being simultaneously optimized, it is not so surprising that it is difficult to get stable learning. How do you optimize hyper-parameters in this setting?   Presumably you cannot use a hold-out set.  Fig.6 & 7 show some sensitivity analysis, but nothing is shown about the learning rates.\n\nI would have expected to see comparison to the following methods added to Figure 3:\n1)  Standard 1st person imitation learning using agent A data, and apply the policy on agent A.  This is an upper-bound on how well you can expect to do, since you have the correct perspective.\n2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.  Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance.\n3)  Reinforcement learning using agent A data, and apply the policy on agent A.  I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents). It would be nice to show this.The paper extends the imitation learning paradigm to the case where the demonstrator and learner have different points of view. This is an important contribution, with several good applications.  The main insight is to use adversarial training to learn a policy that is robust to this difference in perspective.  This problem formulation is quite novel compared to the standard imitation learning literature (usually first-order perspective), though has close links to the literature on transfer learning (as explained in Sec.2).\n\nThe basic approach is clearly explained, and follows quite readily from recent literature on imitation learning and adversarial training.\n\nI would have expected to see comparison to the following methods added to Figure 3:\n1)  Standard 1st person imitation learning using agent A data, and apply the policy on agent A.  This is an upper-bound on how well you can expect to do, since you have the correct perspective.\n2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.  Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance.\n3)  Reinforcement learning using agent A data, and apply the policy on agent A.  I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents). I understand this is how the expert data is collected for the demonstrator, but I don\u2019t see the performance results from just using this procedure on the learner (to compare to Fig.3 results).\n\nIncluding these results would in my view significantly enhance the impact of the paper.", "title": "Questions", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HyfOgMJmg": {"type": "review", "replyto": "B16dGcqlx", "review": "In this paper, I didn't see how the expert (teacher) data is acquired. Is this demonstrated from human or optimized (given the known physical parameters)? \n\nAlso, there is no detailed specification of network architecture (e.g., what layers did you use? How complicated is D_F, D_R and D_D) in the paper besides Fig. 2. Please elaborate.This paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view. Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different view point.\n\nWhile the idea is quite elegant and novel (I enjoy reading it), more experiments are needed to justify the approach. Probably the most important issue is that there is no baseline, e.g., what if we train the model with the image from the same viewpoint? It should be better than the proposed approach but how close are they? How the performance changes when we gradually change the viewpoint from third-person to first-person? Another important question is that maybe the network just blindly remembers the policy, in this case, the extracted feature could be artifacts of the input image that implicitly counts the time tick in some way (and thus domain-agonistic), but can still perform reasonable policy. Since the experiments are conduct in a synthetic environment, this might happen. An easy check is to run the algorithm on multiple viewpoint and/or with blurred/differently rendered images, and/or with random initial conditions.\n\nOther ablation analysis is also needed. For example, I am not fully convinced by the gradient flipping trick used in Eqn. 5, and in the experiments there is no ablation analysis for that (GAN/EM style training versus gradient flipping trick). For the experiments, Fig. 4,5,6 does not have error bars and is not very convincing.", "title": "Some questions.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SJezwxzEg": {"type": "review", "replyto": "B16dGcqlx", "review": "In this paper, I didn't see how the expert (teacher) data is acquired. Is this demonstrated from human or optimized (given the known physical parameters)? \n\nAlso, there is no detailed specification of network architecture (e.g., what layers did you use? How complicated is D_F, D_R and D_D) in the paper besides Fig. 2. Please elaborate.This paper proposed a novel adversarial framework to train a model from demonstrations in a third-person perspective, to perform the task in the first-person view. Here the adversarial training is used to extract a novice-expert (or third-person/first-person) independent feature so that the agent can use to perform the same policy in a different view point.\n\nWhile the idea is quite elegant and novel (I enjoy reading it), more experiments are needed to justify the approach. Probably the most important issue is that there is no baseline, e.g., what if we train the model with the image from the same viewpoint? It should be better than the proposed approach but how close are they? How the performance changes when we gradually change the viewpoint from third-person to first-person? Another important question is that maybe the network just blindly remembers the policy, in this case, the extracted feature could be artifacts of the input image that implicitly counts the time tick in some way (and thus domain-agonistic), but can still perform reasonable policy. Since the experiments are conduct in a synthetic environment, this might happen. An easy check is to run the algorithm on multiple viewpoint and/or with blurred/differently rendered images, and/or with random initial conditions.\n\nOther ablation analysis is also needed. For example, I am not fully convinced by the gradient flipping trick used in Eqn. 5, and in the experiments there is no ablation analysis for that (GAN/EM style training versus gradient flipping trick). For the experiments, Fig. 4,5,6 does not have error bars and is not very convincing.", "title": "Some questions.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}