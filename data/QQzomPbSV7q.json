{"paper": {"title": "Reducing Class Collapse in Metric Learning with Easy Positive Sampling", "authors": ["Elad Levi", "Tete Xiao", "Xiaolong Wang", "trevor darrell"], "authorids": ["~Elad_Levi1", "~Tete_Xiao1", "~Xiaolong_Wang3", "~trevor_darrell1"], "summary": "", "abstract": "Metric learning seeks perceptual embeddings where visually similar instances are close and dissimilar instances are apart, but learned representation can be sub-optimal when the distribution of intra-class samples is diverse and distinct sub-clusters are present. We theoretically prove and empirically show that under reasonable noise assumptions, prevalent embedding losses in metric learning, e.g., triplet loss, tend to project all samples of a class with various modes onto a single point in the embedding space, resulting in a class collapse that usually renders the space ill-sorted for classification or retrieval. To address this problem, we propose a simple modification to the embedding losses such that each sample selects its nearest same-class counterpart in a batch as the positive element in the tuple/triplet. This allows for the presence of multiple sub-clusters within each class. The adaptation can be integrated into a wide range of metric learning losses. Our method demonstrates clear benefits on various fine-grained image retrieval datasets over a variety of existing losses; qualitative retrieval results show that samples with similar visual patterns are indeed closer in the embedding space.\n", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper is truly borderline. On one hand, the theoretical contribution seems novel and interesting, however, there appears to be somewhat of a gap between theory and practice. \n\nThere is unfortunately another problem. According to the authors, the main contribution of this publication is arguably the introduction of the nearest neighbor as the positive example in the triplet loss. However, the authors seem to be unaware of the history of the triplet loss. It was originally introduced by Schultz & Joachims 2004 as a loss over all triplets.  Weinberger et al. 2005 changed it and use the nearest neighbor as \"target neighbor\", which is called \"easy positives\" here, as the objective of LMNN. In 2009 Chechik et al. subsequently relaxed this positive neighbor formulation to any similarly labeled sample (going back to the Schultz & Joachims formulation) but sampling triplets. The re-introduction of the nearest neighbor as \"easy positive\" was then covered by Xuan et al. 2020.  \n\nUnfortunately all of this diminishes the novelty significantly and it is clear that the paper in its current form does not have a strong enough contribution. I do encourage the authors to take a close look at the original LMNN publication and Xuan et al and write an improved re-submission for the next conference that maybe focuses more on the theoretical contribution. \nGood luck,\n\nAC"}, "review": {"gN1R94iJ9jP": {"type": "review", "replyto": "QQzomPbSV7q", "review": "Post-rebuttal: The rebuttal partly addresses my concerns, so I would like to change my score to 4.\n------------------------------------------------------\nThis paper proposes an easy positive sampling method for deep metric learning which aims to reduce the class collapse problem which is found to harm the performance of existing DML methods.\n\nPros: \n1. This paper is well-written and easy to follow. \n2. The idea is simple but makes good sense. The author also provide solid theoretical analysis of the flaw of existing methods and the advantage of the proposed easy positive sampling strategy. \n\nCons:\n1. The idea of sampling easy positive for deep metric learning is actually not new. [1] already proposed an easy positive sampling method and the motivation is quite similar (to relax the constraints of intra-class variations). [1] should be cited in this paper.\n2. The authors only provide theoretical analysis on the binary case and claims it can be easily extended to the multi-label case, which I find not trivial.\n3. A concern is the limited batch size, which might cause the easy positive sample of one particular sample at different iterations to be different (and possibly from different subcluster). This might lead to inconsistent effect of pushing the same sample to different subclusters. \n4. Similar to the last one, a more general problem is the theoretical analysis only consider the optimal situation but neglects the nature of batch-based training, which might bring unexpected problems.\n5. For the experiments, the performance improvement using the proposed easy positive sampling is not strong. Specifically, the best performance on the Cars196 and CUB200 dataset is achieved with EPS + margin, but the authors did not report the performance of margin loss  with distance-weighted sampling. Comparisons with other sampling methods on the same loss should be provided.\n6. The authors should design an experiment to better demonstrate the class collapsing problem on a regular dataset like CUB. The toy experiment on the MNIST dataset is not convincing.\n\nIn summary, I think this paper is solid and well-motivated, but I find the idea not new and the experiments not satisfying. The latter weighs more in my decision. \n\n[1] Xuan H, Stylianou A, Pless R. Improved embeddings with easy positive triplet mining[C]//The IEEE Winter Conference on Applications of Computer Vision. 2020: 2474-2482.\n", "title": "REDUCING CLASS C OLLAPSE IN M ETRIC L EARNING WITH EASY POSITIVE S AMPLING", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "9j3TIx8mD02": {"type": "rebuttal", "replyto": "gN1R94iJ9jP", "comment": "We thank the reviewer for the review and comments. We provide answers to the comments below.\n1) In our work, in general, we had a theoretical analysis on two types of losses: (1) The losses that trivially (always) lead to class collapse properties, and (2) The losses that are theoretically robust to class collapse when certain conditions (non-noisy setting) are met. For the former ones, we only briefly discussed them in the paper as from a theoretic perspective it is clear that EPS should resolve class collapsing; for the latter ones, it turns out that noisy environment modelling batter fit the real-world data, thus as our theoretical and extensive experimental analysis demonstrate, this family of losses also need EPS to resolve the class collapsing, and indeed EPS yields improvement with respect to different datasets, losses and negative sampling methods. The analysis and remedy on loss type 2) is the core contribution of our work.\n\tThe authors in [1] strived to improve the (N+1)-tuplet loss [2] (eq. 3)  and a few of its variations. The (N+1)-tuplet loss (as the contrastive loss which was discussed in section 3), belongs to loss type (1). Note that these type (1) kinds of losses result in a class collapse in both non-noisy and noisy settings. Therefore we do not believe that the reference interfered with the contribution of our work.\nRegardless, we thank the reviewer for bringing up the relevant reference, we have added it to the reference list and modified the related work section to reflect the discussion.\n2) We changed the definitions in subsection 4.2 to the multi-class case and modified the theorems and proofs accordingly. This required only small modifications (mainly in the proof of theorem 1), please see the new version. \n3) The concern on the batch size is addressed in subsection 5.2.3. As we demonstrate on the Omniglot dataset, if the number of inner-class modalities is extremely high (compared to the batch size), then with low probability there will be two samples from the same modality in the mini-batch, and the EPS becomes less effective. However, as can be seen in Figure 3(b), even when the probability is low (less than 10%), there is still a significant improvement when using EPS.\n4) The triplet and margin loss functions are convex with respect to the distances in the embedding between every two samples. We proved (Theorem 1+2)  that they had a unique global solution (as a function of the distances) in the noisy-environment setting. Therefore if the optimization is done with respect to the distances, then with sufficiently small updates the process should converge to the class-collapsing solution. It is true that in practice the model hypothesis space is restricted, and we are not directly optimizing the distances relations but rather the network parameters. However, These assumptions and theoretical simplification is common, see for example [3] (sec 4.2). \nPlease note that in our work, we also have the experiments section, as discussed in the paper, the empirical evidence supporting the theoretical findings. \n5) It is important to us to clarify that the comparison in all tested experiments with/without EPS was done in the exact same setting (including hyper-parameters like batch-size and even random seeds), the only difference is the positive sampling method. The same applies to the distance-margin experiment. In both cases distance sampling was used on the negative part. We think that the reviewer might be confused about the experiment abbreviation in the tables. To clarify it we changed the abbreviation name in the new draft.\n6) We add in Appendix C a small subsection describing the embedding behaviour on the training sets. This includes also a t-SNE visualization of Cars-196 which indicates on class collapsing when training without EPS. Another important quantitative indication to the embedding behaviour is the change in performances of the NMI score when increasing the number of clusters. This indicates that training with EPS results in more homogeneous small clusters, which are more blended in the embedding space (compared to training without EPS). \n\n[1] Xuan H, Stylianou A, Pless R. Improved embeddings with easy positive triplet mining[C]//The IEEE Winter Conference on Applications of Computer Vision. 2020: 2474-2482.\n[2] Kihyuk Sohn, \u201cImproved Deep Metric Learning with Multi-class N-pair Loss Objective\u201d\n[3]  Goodfellow et al., \u201cGenerative Adversarial Nets\u201d\n", "title": "Response to Reviewer3"}, "f87kq_8pfAC": {"type": "rebuttal", "replyto": "Lamsa4rf62j", "comment": "We thank the reviewer for the review, comments, and constructive feedback. We provide answers to the comments below.\n1) We believe that [1] fundamentally differs with our work in terms of the motivation and problem setting. In [1] the authors collected a noisy dataset, in which every image had: 1) a negative set containing samples of different categories, and 2) a potentially positive set containing at least one sample of the same category (Sec.4, 3rd paragraph). Therefore, they formulate the task in a multiple instance learning setting (Sec.4, second to the last paragraph) and propose an algorithm to perform weakly-supervised metric learning on a noisy positive set. On the other hand, our motivation derived from the problem of class collapse due to intra-class multi-modality in fully-supervised metric learning. In our problem setting samples in the positive set are universally positive samples in the positive set are.\nIt can also be inferred that the aim of NetVLAD is not about learning intra-class sub-clusters; on the contrary, the authors in [1] argued that to achieve good retrieval performance a model needed to be invariant w.r.t. perspectives, seasons and lighting, etc. (e.g., Fig. 4 and Sec.4 in [1]). That is to say, NetVLAD aimed to blend all intra-class invariances while we strived to preserve them. Fig. 11 in [1] also strongly supports our argument, in which nighttime images were provided and the authors believed that their method outperformed the baseline by retrieving daytime images of the same category, whereas our method would have retrieved nighttime images of the same category. We believe that the algorithms behave significantly differently because their dataset lacks clear distinct class-modalities due to the incompleteness of the data and a large amount of noise (Sec.4, 1st paragraph).\n2)  We thank the reviewer for this suggestion, we changed the definitions in subsection 4.2 to the multi-class case and modified the theorems and proofs accordingly. This required small modifications (mainly in the proof of theorem 1), see the new draft. \n3) The paper addresses the issue of class collapsing and the EPS method was suggested to resolve this issue. In cases where class collapsing is not a concern, for example in Stanford Online Products and In-shop datasets, where the numbers of elements per class is very small, there is no benefit in using EPS. In these cases (as expected) there was no statistical significance when using and not using EPS. However, the scenario of inner-class multi-modalities is broad and happens naturally in many use cases. This is especially true in real-world datasets, in which fine-grained annotations are costly. It is also important to note that this issue was also addressed by others, for example, see [2] in the classification context, which strengthens our belief that this is indeed an important problem by its own.\n\nQ: in [3] This easiest positive sampling scheme falls short in performance.\n\nIn [3] they got better results compared to [1]. However, they used external information, i.e., the camera position and the 3D-model  (Sec.4.1, 2nd paragraph), in order to relax the loss constraints on the positive samples relations. With EPS the relaxation is done in an unsupervised way without any external information. It is expected to get a boost in performances when adding labels and doing the sampling in a supervised way, but this is a different problem setting.\n\n\n[1] Arandjelovic et al., \u201cNetVLAD: CNN architecture for weakly supervised place recognition\n[2] Qian et al., \u201cSoftTriple Loss: Deep Metric Learning Without Triplet Sampling\u201d\n[3] Radenovi\u0107, Filip, Giorgos Tolias, and Ond\u0159ej Chum. \"CNN image retrieval learns from BoW: Unsupervised fine-tuning with hard examples.\" European conference on computer vision. Springer, Cham, 2016.\n", "title": "Response to Reviewer1"}, "LHQEwE7IhF": {"type": "rebuttal", "replyto": "otggLWTgbPt", "comment": "We thank the reviewer for the review, comments, and constructive feedback. We provide answers to the comments below.\n\nThe two theoretical concerns that were raised by the reviewer are strongly related.\nThe noisy environment setting, not only described the aleatoric uncertainty (which is indeed less relevant in the case of clean datasets), but it can also describe the approximation uncertainty which is a result of the model incapacity to perfectly overfit the data. In this case, the probability of the labels can be taken as the results of the Bayes optimal model within the hypothesis space. \n\nRegarding the approximation concern of the reviewer (comment 2);  the approximation uncertainty in deep neural networks is considered to be negligible [1],[2].  However, as we prove in our work (Theorem 1+2), as long as the probability is not exactly 0/1, even a small amount of noise causes the family of optimal solutions to degenerate to only class-collapsing embeddings.\n\nQ: \"In addition, I wonder how different architectures can affect the difference between previous methods and EPS.\"\n\nIn appendix C we provide more results on the MS-loss (which is considered to have the best performances among tested losses [3]), with different architecture; An inception backbone with different embedding size (512). This was the architecture that was used in the original paper [4] and achieved the best result.  We reproduce the result from [4] (without EPS) and demonstrate that also in this case there is a significant improvement when using EPS. \n\n\n[1] Cybenko ,  \u201cApproximation by superpositions of a sigmoidal function\u201d\n[2] Tagasovska et al., \u201cSingle-Model Uncertainties for Deep Learning\u201d\n[3] Musgrave et al., \u201cA Metric Learning Reality Check\u201d\n[4]  Wang et al, \u201cMulti-Similarity Loss with General Pair Weighting for Deep Metric Learning\u201d\n", "title": "Response to Reviewer2"}, "prj6AOxIkV_": {"type": "rebuttal", "replyto": "38RwnwiLlqQ", "comment": "We thank the reviewer for the review, comments, and constructive feedback. We provide answers to the comments below.\n1) As discussed in the first paragraph of section 3, the standard evaluation and prediction method for image retrieval tasks are typically based on the properties of K-nearest neighbours in the embedding space. In this case, the class-collapsing property is a side-effect which harms the performance, as was demonstrated in the experiments. However, we do acknowledge that under certain circumstances, class-collapsing of the embedding might be a desired property. In the experiment section (1st paragraph) we provide one such metric: the NMI score where the number of clusters equals the number of classes. As discussed in the paper, with respect to this specific metric, splitting the class cluster to separate smaller homogeneous sub-clusters in the embedding space is not desired property, and indeed as was shown in Table 2, using EPS reduced the NMI score with respect to all tested losses and datasets. \nWe note that forcefully mapping samples of various modalities into a single cluster (class collapsing) requires memorization of the intra-class relations, which will ultimately hurt generalization.\n2) We changed the definitions in subsection 4.2 to the multi-class case and modified the theorems and proofs accordingly. This required small modifications (mainly in the proof of theorem 1), please see the revision. \n3)  In order to batter illustrate the effect of the method, we add in Appendix C a subsection describing the embedding behaviour on the training sets. This includes a visualization on Cars-196 training embedding and results on the Omniglot fine-tuned task, both of them demonstrate the effect on the class-collapsing when training with/without EPS\n", "title": "Response to Reviewer4 comments"}, "Lamsa4rf62j": {"type": "review", "replyto": "QQzomPbSV7q", "review": "This paper proposes/adopts a simple positive sampling scheme in metric learning: only sampling the easiest positive for each anchor. Authors give a theoretical analysis of how the proposed sampling scheme can reduce class collapse. Experiments on fine-grain retrieval datasets show the effectiveness of the sampling scheme. Using the sampled easiest positive, nearly all current metric learning methods got improved performance.\n\nPros:\n\n1. propose/adopt a simple easiest positive sampling scheme, and show its usefulness in fine-grain retrieval task;\n2. Extensive theoretical analysis of how the proposed sampling scheme can reduce class collapse.\n\nCons:\n\n1. I don't think this easiest positive sampling scheme is a contribution, though authors give a theoretical analysis of why this scheme can reduce class collapse. Specifically,  Arandjelovic et al. (2016) used exactly the same easiest positive sampling scheme. Though authors explicitly show the difference between Arandjelovic et al. (2016) and the proposed method (Equation in section 4.3), I found no difference.\n\nIn the paper of Arandjelovic et al. (2016), they don't clean the positive set (only minor negatives could be included) for efficient training. This is good for practical usage.\n\n2. For section 4, it is good to analyze the Class-collapsing property. However, I would suggest using three classes to derive theorems, rather than using two classes. As a metric-learning problem usually has multiple classes, having two or three classes are usually different stories.\n\n3. While I trust the effectiveness of the easiest positive sampling scheme in the fine-grain image retrieval datasets, I strongly suspect its effectiveness in a broad image retrieval task.\n\nFor example, in the following paper:\n\nRadenovi\u0107, Filip, Giorgos Tolias, and Ond\u0159ej Chum. \"CNN image retrieval learns from BoW: Unsupervised fine-tuning with hard examples.\" European conference on computer vision. Springer, Cham, 2016.\n\nThis easiest positive sampling scheme falls short in performance.\n\nCombining the conclusions from the above paper and the paper under review, I would say this easiest positive sampling scheme has limited contribution, as it is not broadly applicable.\n\n\n\n", "title": "A simple and effective sampling method, but is used before and its effectiveness is yet to be validated", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "otggLWTgbPt": {"type": "review", "replyto": "QQzomPbSV7q", "review": "Motivated by the theoretical results on class collapse problems, this paper proposes a simple positive sampling mechanism called EPS for metric learning. The method is simple -- each sample selects its nearest same-class counterpart in a batch as the positive element. The authors provide both theoretical motivations and empirical studies on the proposed method. \n\nStrengths: \n+ Theoretical analysis on the existing class collapse problem for triplet and margin loss\n+ well-motivated and simple solutions that are proven to be effective in theory\n\nWeaknesses:\n- there is a gap between theoretical analysis and empirical studies. In the analysis, the paper shows in the noisy label setting, margin and triplet loss also induce the class collapse problem but in the empirical study, the paper only conducted analysis on dataset with clean labels. \n- the theoretical analysis based on the assumption that the function f can approximate any functions. However for any fixed deep nets, it does not satisfy the requirement \n\nOverall, the EPS method is simple and supported by theoretical analysis. I would find it more convincing if the paper can provide empirical analysis on noisy labeled data. In addition, I wonder how different architectures can affect the difference between previous methods and EPS.", "title": "Gap between theory and empirical", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "38RwnwiLlqQ": {"type": "review", "replyto": "QQzomPbSV7q", "review": "The authors find that the popular triplet loss will force all same-class instances to a single center in a noisy scenario, which is not optimal to deal with the diverse and distinct sub-classes. After some analyses, the authors propose a simple sampling strategy, EPS, where anchors only pull the most similar instances. The method achieves good visualization results on MNIST and gets promising performance on benchmarks.\n\nAvoiding class collapse is meaningful and important in metric learning when dealing with some tasks. The analyses in the paper provide insights. Here are some possible issues of this paper.\n1. The authors should discuss when we need to avoid such class collapse. Maybe in some cases, pulling all similar instances to a single point leads to more discriminative embeddings. Even some methods are designed following that consideration. Some examples and demonstrations are required.\n2. It's better to write a sketch of the analysis on how to extend it to multi-class cases and analyze will the definition of the noise influence the final results.\n3. Maybe the authors need to find another real-world dataset with multiple meanings in one class and show the advantage of the proposed method. We can find the improvement of performance on the benchmarks, but the numbers are hard to illustrate the effect of the method.", "title": "A simple sampling manner for diverse and distinct sub-classes in metric learning", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}}}