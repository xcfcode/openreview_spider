{"paper": {"title": "Convolutional Sequence Modeling Revisited", "authors": ["Shaojie Bai", "J. Zico Kolter", "Vladlen Koltun"], "authorids": ["shaojieb@cs.cmu.edu", "zkolter@cs.cmu.edu", "vkoltun@gmail.com"], "summary": "We argue that convolutional networks should be considered the default starting point for sequence modeling tasks.", "abstract": "This paper revisits the problem of sequence modeling using convolutional \narchitectures.  Although both convolutional and recurrent architectures have a\nlong history in sequence prediction, the current \"default\" mindset in much of\nthe deep learning community is that generic sequence modeling is best handled\nusing recurrent networks.  The goal of this paper is to question this assumption. \nSpecifically, we consider a simple generic temporal convolution network (TCN),\nwhich adopts features from modern ConvNet architectures such as a dilations and \nresidual connections.  We show that on a variety of sequence modeling tasks,\nincluding many frequently used as benchmarks for evaluating recurrent networks,\nthe TCN outperforms baseline RNN methods (LSTMs, GRUs, and vanilla RNNs) and\nsometimes even highly specialized approaches.  We further show that the\npotential \"infinite memory\" advantage that RNNs have over TCNs is largely\nabsent in practice: TCNs indeed exhibit longer effective history sizes than their \nrecurrent counterparts.   As a whole, we argue that it may be time to (re)consider \nConvNets as the default \"go to\" architecture for sequence modeling.", "keywords": ["Temporal Convolutional Network", "Sequence Modeling", "Deep Learning"]}, "meta": {"decision": "Invite to Workshop Track", "comment": "meta score: 5\n\nThis paper gives a thorough experimental comparison of convolutional vs recurrent networks for a variety of sequence modelling tasks.  The experimentation is thorough, but the main point of the paper,  that convolutional networks are unjustly ignored for sequence modelling, is overstated as there are several areas where convolutional networks are well explored.\nPros:\n clear and well-written\n thorough set of experiments\nCons\n original contribution is not strong\n it is not as radical to consider convolutional networks for sequence modeling as the authors seem to suggest\n"}, "review": {"SJmrw95TM": {"type": "rebuttal", "replyto": "SJ9bHR8hf", "comment": "Thanks for your comment.  However, we do strongly disagree that this should be the main lesson from the Melis et al. paper, and it's really orthogonal to the main point we are trying to make. The takeaway from the Melis et al. paper should absolutely not be that \"ordinary LSTMs get 60 perplexity on PTB\", but rather that extensive hyperparameter tuning _can_ improve LSTM results on any given task, effectively overfitting to the test set. And therefore, it's difficult to confirm whether much of the follow-on work in LSTM models (the subset of works that look at only a few relatively small datasets) is really improving the underlying model or just essentially working by tuning hyperparameters. The Melis et al. paper exactly points to the need to evaluate on a wider and more diverse set of benchmarks, where extensive hyperparameter optimization cannot simply \"overfit\" the data.  The authors are quite explicit on this point, even mentioning in one of their review rebuttals: \"The main criticism seems to center on evaluating models on datasets that are too small which increases evaluation variance, and the results are thus not trustworthy. That is a very good summary of the main message of the paper!\"\n\nWe also feel such large-scale hyperparameter search for _one_ dataset is not particularly informative (and most importantly, not representative of what practitioners would typically encounter). As the authors did not release the set of hyperparameters they obtained, we reproduced the LSTM result using the best hyperparameter set that we found in prior works and open source codes (without advanced regularizations). Moreover, given the message we want to convey in our paper, we believe it is much more important to evaluate a model's performance across tasks and datasets, instead of doing extensive hyperparameter search(es) on a single dataset.  The results for both the LSTM and TCN models use minimal tuning, and thus we feel are a good illustration of initial \"expected\" performance.", "title": "Response to Comment"}, "rJesaXLhM": {"type": "rebuttal", "replyto": "rksxfh73z", "comment": "Thank you for your comment. We refer you to the version of our work on arXiv [1], where we have provided a table with updated results for both baselines and the TCNs. \n\nNote that for the PTB perplexity you mentioned, to reach a level of 58.3 you still need a lot more (advanced) recurrent optimizations and regularizations [2], which is orthogonal to what we try to accomplish here. For an LSTM with standard regularizations and no more than 13M parameters, we got 78.93 perplexity, which is consistent with prior works and open source codes. For other tasks, for example on LAMBADA, the prior results can be seen at [3], with an LSTM perplexity at 5357. Graves et al. also tested on this dataset and Wikitext-103 [4], getting similar result as ours. \n\nAlso note that we explicitly emphasize that state-of-the-art architectures do attain much lower errors than the generic TCN _and_ LSTM architectures we consider, which we highlight in the appendix.\n\n[1] https://arxiv.org/abs/1803.01271\n[2] https://github.com/salesforce/awd-lstm-lm\n[3] https://arxiv.org/pdf/1606.06031\n[4] https://arxiv.org/pdf/1612.04426", "title": "Response to Comment"}, "SkdHpQDez": {"type": "review", "replyto": "rk8wKk-R-", "review": "In this paper, the authors argue for the use of convolutional architectures as a general purpose tool for sequence modeling. They start by proposing a generic temporal convolution sequence model which leverages recent advances in the field, discuss the respective advantages of convolutional and recurrent networks, and benchmark their architecture on a number of different tasks.\n\nThe paper is clearly written and easy to follow, does a good job of presenting both the advantages and disadvantages of the proposed method, and convincingly makes the point that convolutional architectures should at least be considered for any sequence modeling task; they are indeed still often overlooked, in spite of some strong performances in language modeling and translation in recent works.\n\nThe only part which is slightly less convincing is the section about effective memory size. While it is true that learning longer term dependencies can be difficult in standard RNN architectures, it is interesting to notice that the SoTA results presented in appendix B.3 for language modeling on larger data sets are architectures which focus on remedying this difficulty (cache model and hierarchical LSTM). It would also be interesting to see how TCN works on word prediction tasks which are devised explicitly to test for longer memory, such as Lambada (1) or Children Books Test (2).\n\nAs a minor point, adding a measure of complexity in terms of number of operations could be a useful hardware-independent indication of the computational cost of the architecture.\n\nPros:\n- Clearly written, well executed paper\n- Makes a strong point for the use of convolutional architecture for sequences\n- Provides useful benchmarks for the community\n\nCons:\n- The claims on effective memory size need more context and justification\n\n1: The LAMBADA dataset: Word prediction requiring a broad discourse context, Paperno et al. 2016\n2: The Goldilocks principle: reading children's books with explicit memory representation, Hill et al. 2016", "title": "The authors benchmark a general-purpose convolutional architecture on several sequence modeling tasks across a variety of domains. The results will be of broad use to the community, although some of the claims in the paper could do with more justification.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkUwN_Ylf": {"type": "review", "replyto": "rk8wKk-R-", "review": "The authors claim that convolutional networks should be considered as possible replacements of recurrent neural networks as the default choice for solving sequential modelling problems. The paper describes an architecture similar to wavenet with residual connections. Empirical results are presented on a large number of tasks where the convolutional network often outperforms modern recurrent baselines or reaches similar performance.\n\nThe biggest strength of the paper is the large number of tasks on which the models are evaluated. The experiments seem sound and the information in both the paper and the appendix seem to allow for replication. That said, I don\u2019t think that all the tasks are very relevant for comparing convolutional and recurrent architectures. While the time windows that RNNs can deal with are infinite in principle, it is common knowledge that the effective length of the dependencies RNNs can model is quite limited in practise. Many of the artificial task like the adding problem and sequential MNIST have been designed to highlight this weakness of RNNs. I don\u2019t find it very surprising that these tasks are easy to solve with a feedforward architecture with a large enough context window. The more impressive results are in my opinion those on the language modelling tasks where one would indeed expect RNNs to be more suitable for capturing dependencies that require stack-like memory functionality. \n\nWhile the related work is quite comprehensive, it downplays the popularity of convolutional architectures throughout history a bit. Especially in speech recognition, RNNs have only recently started to gain popularity while deep feedforward networks applied to overlapping time windows (i.e., 1D convolutions) have been the state-of-the-art for years. Of course the recent successes of dilated convolutions are likely to change the landscape in this application domain yet again.\n\nThe paper is well-structured and written. If anything, it is perhaps a little bit wordy at times but I prefer that over obscurity due to brevity.\n\nThe ideas in the paper are not novel and neither do the authors claim that they are. Unfortunately, I also think that the impact of the work is also somewhat limited due to the enormous success of the wavenet architecture. I do think that the results on the real-world tasks are valuable and worthy of publication. However, I feel that the authors exaggerate the extent to which researchers in this field still consider RNNs superior models for sequences. \n\n+ Many experiments and tasks.\n+ Well-written and clear.\n+ Good results\n- Somewhat exaggerated claims about the extent to which RNNs are still being considered more suitable sequence models\n than dilated convolutions. Especially in light of the success of Wavenet.\n- Not much novelty/originality.\n", "title": "Nice results but not much novelty and I don't think that the views are as contrarian as the paper claims.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkTNLM5gM": {"type": "review", "replyto": "rk8wKk-R-", "review": "This paper argues that convolutional networks should be the default\napproach for sequence modeling.\n\nThe paper is nicely done and rather easy to understand. Nevertheless, I find\nit difficult to assess its significance. In order to support the original hypothesis,\nI think that a much larger and more diverse set of experiments should have\nbeen considered. As pointed out by another reviewer please add  https://arxiv.org/abs/1703.04691\nto your references.", "title": "Convolutional networks are good for solving sequence tasks", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rkR0McB7M": {"type": "rebuttal", "replyto": "rk8wKk-R-", "comment": "We thank the reviewers and other discussants for their comments. In order to address points discussed in OpenReview reviews, comments, and our responses, we have updated our paper. The key changes are as follows:\n\n1. We\u2019ve added content to the Related Work section. This content elaborates on the relationship to prior work (e.g., non-dilated gated ConvNets, convolutional models for sequence to sequence prediction, etc.), in accordance with our responses to OpenReview reviews and comments. As highlighted in the revision, the TCN model we focus on avoids much of the specialized machinery present in prior work and is evaluated on an extremely diverse set of tasks rather than a specific domain or application.\n\n2. We have added experiments on the LAMBADA dataset, as suggested by Reviewer 3, which in fact show very strong performance for the TCN models.  LAMBADA is an especially challenging task where each data sample consists of a long context segment (4.6 sentences on average) and a target sentence, the last word of which needs to be predicted. In this setting, a human can perfectly predict the last word when given the context, but most of the existing models (e.g., LSTM, vanilla RNN) fail to do so. As shown in Table 1 of Section 4 in the revision, without much tuning (due to limited rebuttal time), TCN can achieve a perplexity of < 1300 on LAMBADA, substantially outperforming LSTMs (~4000 ppl) and vanilla RNNs (~15000 ppl), as listed in prior works. This is a strong result that suggests that TCNs are able to recall from a much larger context than recurrent networks, and thus may be more suitable for tasks where long dependencies are required.\n\n3. The appendix now includes a new section that compares the baseline TCN to a TCN that uses a gating mechanism.  This mainly serves as a comparison point to the Dauphin et al. paper, which one reviewer pointed out was not sufficiently addressed in our original draft.  Our experiments show that a gating mechanism can indeed be useful on certain language modeling tasks, but such benefits may not generalize well to other tasks (e.g., polyphonic music and other benchmark tasks).  Thus, while we do absolutely agree with the relevance of the Dauphin et al. paper, and stress this more in the update, we also feel that much the same considerations apply here as to e.g., the WaveNet paper, where the focus of the previous work was really on a single domain, whereas our paper stresses the generality of convolutional sequence models.\n\n4. The revision includes the latest results on certain large experiments (e.g., Wikitext-103).  Specifically, as mentioned in our responses, the TCN achieves a perplexity of 45.2 on this dataset (the only change from our original result was simple optimizing the model for longer), compared to an LSTM that achieves 48.4 perplexity.\n", "title": "Revision posted"}, "SyYmNNQzf": {"type": "rebuttal", "replyto": "BJ7tE5OCb", "comment": "Thanks for your note. \u00a0We will certainly update the paper to include this arXiv report. \u00a0However, we also believe that the precise conclusions of this report are somewhat orthogonal as it applies an architecture virtually identical to WaveNet to one particular time series prediction task; thus, from an architectural standpoint, we think that the WaveNet paper is the more relevant prior work, which of course we do cite and discuss. \u00a0In contrast, the goal of our current work is to highlight a simpler architecture and empirically study it across a wide range of sequence modeling tasks. \u00a0But as mentioned, we're happy to include the reference and explain this connection.\n", "title": "Response to Comment"}, "S1FZN4mzf": {"type": "rebuttal", "replyto": "SkdHpQDez", "comment": "Thank you very much for the review, we agree with virtually all your points. \u00a0\u00a0As per your suggestion, we are currently integrating experiments on the LAMBADA dataset into the paper, and will post a revision with these results shortly.\n", "title": "Response to AnonReviewer1"}, "SyxkEEQfM": {"type": "rebuttal", "replyto": "HkUwN_Ylf", "comment": "Thank you very much for this review. \u00a0We agree on most points, except in the ultimate conclusions and assessment of the current \"default\" mindset of temporal modeling in RNNs.\n\nFirst, we agree that speech data in particular (or perhaps audio data more broadly), is indeed one instance where CNNs do appear to have a historical edge over recurrent models, and we can emphasize this in the background section. \u00a0Indeed, as you mention, the success of WaveNet has certainly made clear the power of CNNs in this application domain.\n\nThe question, then, is to what extent the community already feels that the success of WaveNet in the speech setting is sufficient to \"standardize\" the use of CNNs across all sequence prediction tasks. \u00a0And our genuine impression here is that these ideas have yet to permeate the mindset of the community for generic sequence prediction. \u00a0Numerous resources (e.g., Goodfellow et al.'s deep learning book, with its chapter \"Sequence Modeling: Recurrent and Recursive Nets\", plus virtually all current papers on recurrent networks), still highlight LSTMs and other similar architectures as the \"standard\" for sequence modeling. \u00a0The precise goal of our work is to highlight the fact that WaveNet-like architectures (though substantially simplified too, as we describe below) can indeed work well across the many other settings we consider. \u00a0And we feel that this is an important point to make empirically, even if the results or conclusion may seem \"unsurprising\" to people who are very familiar with CNN architectures.\n\nThe second point, also, is that the architecture we consider is indeed simpler than WaveNet in many respects: e.g. no gated activation but just ReLUs (which, as we highlighted in our response to a previous reviewer, we will include more experimentation on in a forthcoming update), no context stacks, etc; and residual units and dilation structure that more directly mirror the corresponding \"standard\" architectures in convolutional image networks. \u00a0Thus, a practitioner wishing to apply WaveNet-style architectures to some new sequence prediction task may be unclear about which elements of the architecture are really necessary, and we attempt to distill this as much as possible in our current paper.\n\nOverall, therefore, we agree that the significance of our current work is largely making the empirical point that TCN architectures are not just for audio, but really for any sequence modeling problem. \u00a0But we do feel that this is an important point to make and thoroughly substantiate, even given the success of WaveNet.\n", "title": "Response to AnonReviewer3"}, "rkehXEQff": {"type": "rebuttal", "replyto": "HkTNLM5gM", "comment": "Thanks for your note, though we honestly found it a bit surprising. \u00a0The entire point of our paper _is_ to evaluate the improved TCN performance over a large and diverse set of experiments, and on this point it is by far the single _most diverse_ study of CNN vs. RNN performance that we are aware of. \u00a0And while many of the particular benchmarks are indeed \"small-sized\" in and of themselves, they are standard benchmarks for evaluating the performance of recurrent networks (see appendix A for some references to papers that used these benchmark tests); and we include experiments on domains such as Wikitext-103, which is certainly not a small dataset.\n\nRegarding arXiv:1703.04691, see our comments in the response to the discussant who originally brought this up.\n", "title": "Response to AnonReviewer2"}, "HkDIQN7ff": {"type": "rebuttal", "replyto": "Bk2tcOm-f", "comment": "Thanks for the note. \u00a0We believe this note is addressing the same points as the note above (with a few additional follow-on points), so we refer to our comment above.", "title": "Response to Comment"}, "ByC7mNQGf": {"type": "rebuttal", "replyto": "B1lh3uXbf", "comment": "Thanks very much for your note. \u00a0We absolutely agree with your general comments about the related work. We respond to two different points here, because in our mind there are two different categories in the papers you mention.\n\nFirst, the Kalchbrenner et al., and Gehring et al., papers both relate to convolutional sequence to sequence models. While we absolutely agree that this work is related to our topic, we made the explicit choice not to consider seq2seq models in this paper. \u00a0The rationale for us is that these models differ in substantial ways from \"pure\" temporal convolutional models. \u00a0Since the input to the model is the entire input sentence (captured by non-causal convolutions), and only the autoregressive output network needs to follow causal generation, the task itself is quite different from pure temporal sequence modeling, even if it may be an extension. \u00a0Specifically, the two-stage encoder/decoder architecture (first to encode the entire input sentence, then to autoregressively generate the translation) of typical seq2seq models seems so fundamental to these approaches that we felt it was substantially more specialized than the generic temporal modeling problem.\n\nHowever, we also of course concede that the work is related, especially given the machine translation community's departure from pure recurrent networks to convolutional (or even pure attention-based) models. \u00a0Thus we will edit the paper to cite these works and address these points (we'll be posting a revised version within a week or so).\n\nSecond, there is the work of Dauphin et al., which more directly relates to a language modeling task. \u00a0And while we _do_ cite this work, we believe your point combined with the point in the comment below is more that we don't devote sufficient attention to this previous work. \u00a0We agree that the relationship is not clarified enough in the paper and are currently revising to fix this, but let us briefly mention here the connections and how we see this relationship.\n\nFirst, we should mention that while we did include the 48.9 PPL figure on one GPU, running the TCN model for more epochs (still on one GPU) actually achieves a PPL of 45.2, which isn't far off from Dauphin\u2019s 44.9. (Note that we use a network approximately half the size of Dauphin et al.\u2019s, and little tuning.) We'll naturally update the paper on this point. \u00a0Second, the main technical contribution of the paper of Dauphin et al. is the combination of (non-dilated) convolutional networks with a gating mechanism. \u00a0We experimented quite extensively with this gating mechanism combined with our generic TCN architecture, but didn\u2019t see significant overall performance improvements due to the gating mechanism. \u00a0We can include these results in an appendix. \u00a0Indeed, a main characteristic of our work is simply the claim that the generic TCN architecture (which is quite simple in nature, as we highlight) is _sufficient_ to achieve most of the benefits proposed by more complex convolutional architectures, without the need for attention, gating mechanisms, and other architectural elaborations. \u00a0We believe that the comparison to the Dauphin et al. work actually supports this conclusion, and we will update the paper accordingly (we will post a follow-up note here once the paper has been updated).\n", "title": "Response to Comment"}}}