{"paper": {"title": "Nonparametrically Learning Activation Functions in Deep Neural Nets", "authors": ["Carson Eisenach", "Zhaoran Wang", "Han Liu"], "authorids": ["eisenach@princeton.edu", "zhaoran@princeton.edu", "hanliu@princeton.edu"], "summary": "A new class of nonparametric activation functions for deep learning with theoretical guarantees for generalization error.", "abstract": "We provide a principled framework for nonparametrically learning activation functions in deep neural networks. Currently, state-of-the-art deep networks treat choice of activation function as a hyper-parameter before training. By allowing activation functions to be estimated as part of the training procedure, we expand the class of functions that each node in the network can learn. We also provide a theoretical justification for our choice of nonparametric activation functions and demonstrate that networks with our nonparametric activation functions generalize well. To demonstrate the power of our novel techniques, we test them on image recognition datasets and achieve up to a 15% relative increase in test performance compared to the baseline.", "keywords": []}, "meta": {"decision": "Invite to Workshop Track", "comment": "The authors propose a nonparametric regression approach to learn the activation functions in deep neural networks. The proposed theoretical analysis, based on stability arguments, is quite interesting. Experiments on MNIST and CIFAR-10 illustrate the potential of the approach. \n \n Reviewers were somewhat positive, but preliminary empirical evidence on small datasets makes this contribution better suited for the workshop track."}, "review": {"rJbCd3IIl": {"type": "rebuttal", "replyto": "SJRT6dgEl", "comment": "The non-linearities learned for convolution layers are shared across all nodes in that filter. Otherwise, they are not shared.\n\nWe did look at learning one non-linearity for the entire network to answer just that question -- is there an optimal non-linearity? But we found that to not be any more successful than using a fixed activation function. It is possible looking at other training procedures might lead to such a method being successful.\n\nWe did not look at the effects of batch normalization.", "title": "Review Response"}, "rkh8W3UVx": {"type": "rebuttal", "replyto": "Skjj5W8Ee", "comment": "Hi,\n\nWe could extend Theorem 4.7 for any algorithm, as long as we can control expansiveness. Ensuring that the updates are not too expansive is the main property we need to obtain the stability bound.", "title": "Response: Theorem 4.7"}, "SJlig2U4x": {"type": "rebuttal", "replyto": "SJeWe3_7g", "comment": "Hi,\n\nApologies for the slow response -- yes, I agree that learning a non-linearity as a linear combination of a set of non-linear basis functions is equivalent to a network with nodes that have those basis functions as non-linearities and constraints on the weight matrices. However, I think the view of learning non-linearities is more intuitive. It also is a view others have taken (such as Agostinelli, et al (2014)) and so by treating what we are doing as learning activation functions for a particular node, we can more easily compare to other approaches.\n\nThis sort of equivalence also arises in the case of convolution nets. We could of course view the convolution layers as a standard feedforward neural network with more nodes and some constraints on the weight matrices, but the view that we learn a stack of filters in each convolution layer more accurately captures the intuition behind why convolution nets are a good idea and what actually happens after fitting the model.\n\nTo answer the second question, I do not think that the gains are purely from increasing the capacity. If we increased capacity and fixed a single non-linearity for the entire network, I do not think the results would be as good. But, this is something we could check to provide evidence that the gains are not purely due to increased capacity.\n", "title": "Response to: learn non-linearities vs network capacity"}, "Skjj5W8Ee": {"type": "review", "replyto": "H1wgawqxl", "review": "Can you extend Theorem 4.7 for settings other than SGD with a max-norm constraint ?This paper provides a principled framework for nonparametrically learning activation\nfunctions in deep neural networks. A theoretical justification for authors' choice of \nnonparametric activation functions is given. \nTheoretical results are satisfactory but I particularly like the experimental setup\nwhere their methods are tested on image\nrecognition datasets and achieve up to a 15% relative increase in test performance\ncompared to the baseline.\nWell-written paper and novel theoretical techniques. \nThe intuition behind the proof of Theorem 4.7 can be given in a little bit more clear way in the main body of the paper, but the\nAppendix clarifies everything.\n", "title": "Theorem 4.7", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SkL_c-IEx": {"type": "review", "replyto": "H1wgawqxl", "review": "Can you extend Theorem 4.7 for settings other than SGD with a max-norm constraint ?This paper provides a principled framework for nonparametrically learning activation\nfunctions in deep neural networks. A theoretical justification for authors' choice of \nnonparametric activation functions is given. \nTheoretical results are satisfactory but I particularly like the experimental setup\nwhere their methods are tested on image\nrecognition datasets and achieve up to a 15% relative increase in test performance\ncompared to the baseline.\nWell-written paper and novel theoretical techniques. \nThe intuition behind the proof of Theorem 4.7 can be given in a little bit more clear way in the main body of the paper, but the\nAppendix clarifies everything.\n", "title": "Theorem 4.7", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SJeWe3_7g": {"type": "review", "replyto": "H1wgawqxl", "review": "This is perhaps more of a discussion than a question.  It seems like writing a non-linearity as a linear or affine combination of a set of non-linear basis functions would be equivalent to a network of larger capacity with nodes with those basis functions as non-linearities and constraints on the weight matrices.  Thus could one argue that the gains observed in the paper are perhaps coming from this capacity increase?  And if that\u2019s the case, this may be more of a choosing the appropriate capacity question (with any fixed non-linearity) and less so of learning non-linearities?  Would greatly appreciate your thoughts on this.This paper describes an approach to learning the non-linear activation function in deep neural nets.  This is achieved by representing the activation function in a basis of non-linear functions and learning the coefficients.  Authors use Fourier basis in the paper.  A theoretical analysis of the proposed approach is also presented, using algorithmic stability arguments, to demonstrate good generalization behavior (vanishing generalization error with large data sets) of networks with learnt non-linearities.\n\nThe main question I have about this paper is that writing a non-linear activation function as a linear or affine combination of other non-linear basis functions is equivalent to making a larger network whose nodes have the basis functions as non-linearities and whose weights have certain constraints on them.  Thus, the value of the proposed approach of learning non-linearities over optimizing network capacity for a given task (with fixed non-linearities) is not clear to me.  Or could it be argued that the constrained implied by learnt non-linearity approach are somehow good thing to do?\n\nAnother question - In the two stage training process for CNNs, when ReLU activation is replaced by NPFC(L,T), is the NPFC(L,T) activation initialized to approximate ReLU, or is it initialized using random coefficients?\n\nFew minor corrections/questions:\n- Pg 2. \u201c \u2026 the interval [-L+T, L+T] \u2026\u201d should be \u201c \u2026 the interval [-L+T, L-T] \u2026 \u201c ?\n- Pg 2., Equation for f(x), should it be \u201c (-L+T) i \\pi x / L \u201c in both sin and cos terms, or without \u201c x \u201c ?\n- Theorem 4.2 \u201c \u2026 some algorithm \\eps-uniformly stable \u2026\u201d remove the word \u201calgorithm\u201d\n- Theorem 4.5.  SGM undefined", "title": "learnt non-linearities vs network capacity", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJ4DJdxVg": {"type": "review", "replyto": "H1wgawqxl", "review": "This is perhaps more of a discussion than a question.  It seems like writing a non-linearity as a linear or affine combination of a set of non-linear basis functions would be equivalent to a network of larger capacity with nodes with those basis functions as non-linearities and constraints on the weight matrices.  Thus could one argue that the gains observed in the paper are perhaps coming from this capacity increase?  And if that\u2019s the case, this may be more of a choosing the appropriate capacity question (with any fixed non-linearity) and less so of learning non-linearities?  Would greatly appreciate your thoughts on this.This paper describes an approach to learning the non-linear activation function in deep neural nets.  This is achieved by representing the activation function in a basis of non-linear functions and learning the coefficients.  Authors use Fourier basis in the paper.  A theoretical analysis of the proposed approach is also presented, using algorithmic stability arguments, to demonstrate good generalization behavior (vanishing generalization error with large data sets) of networks with learnt non-linearities.\n\nThe main question I have about this paper is that writing a non-linear activation function as a linear or affine combination of other non-linear basis functions is equivalent to making a larger network whose nodes have the basis functions as non-linearities and whose weights have certain constraints on them.  Thus, the value of the proposed approach of learning non-linearities over optimizing network capacity for a given task (with fixed non-linearities) is not clear to me.  Or could it be argued that the constrained implied by learnt non-linearity approach are somehow good thing to do?\n\nAnother question - In the two stage training process for CNNs, when ReLU activation is replaced by NPFC(L,T), is the NPFC(L,T) activation initialized to approximate ReLU, or is it initialized using random coefficients?\n\nFew minor corrections/questions:\n- Pg 2. \u201c \u2026 the interval [-L+T, L+T] \u2026\u201d should be \u201c \u2026 the interval [-L+T, L-T] \u2026 \u201c ?\n- Pg 2., Equation for f(x), should it be \u201c (-L+T) i \\pi x / L \u201c in both sin and cos terms, or without \u201c x \u201c ?\n- Theorem 4.2 \u201c \u2026 some algorithm \\eps-uniformly stable \u2026\u201d remove the word \u201calgorithm\u201d\n- Theorem 4.5.  SGM undefined", "title": "learnt non-linearities vs network capacity", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Hkwusiemx": {"type": "rebuttal", "replyto": "r1K_0NCfl", "comment": "Hi,\n\nThank you for the questions / suggestions. To respond:\n\n- We did look at training with tanh activation functions in the first stage. The results we got were very similar to when we used ReLU activations in the stage I/pre-training stage. Because performing the stage I training with ReLus requires fewer weight updates, we chose to use that version in the paper. We can add a comment regarding this, because using the tanh function does seem to be the intuitive choice.\n\n- That sounds like a good idea and it could provide some insight into the effects of learning nonparametric activation functions. We will add such a plot in the next revision. Thank you for the suggestion.\n\n- This sounds like an interesting suggestion as well. We did not try that, but it may be something to consider going forwards.", "title": "Response to: Effect of non linearity and basis expansion choice"}, "r1K_0NCfl": {"type": "review", "replyto": "H1wgawqxl", "review": "The paper suggests a two stage learning of weights and non linearities in neural networks.\n\n- the network is trained with RELU , then it is initialized with tanh Fourier series coefficients. this seems inconsistent. why not train with tanh first? \n\n- It is not intuitively clear what is the effect of learning the non linearity. I suggest to do a 2D data of 2 classes (a ring data for instance ) and to train a a 2 layer network (Linear -> Relu -> linear-> softmax), and then one with a learned non linearity in the fourier basis , and another with tanh ,and plot the decision function surface  to see how it changes with the non linearity choice. \n\n \n-   Seems the non linearity is not moving too much from a tanh. maybe there is a capacity problem in the Fourier expansion. Have you tried using an over-complete basis expansion and regularize with l_1 the coefficient a_i? Maybe this will help also with other basis such polynomials that did not work?\n\nSummary:\n\nThe paper introduces a parametric class for non linearities used in neural networks. The paper suggests two stage optimization to learn the weights of the network, and the non linearity weights.\n\nsignificance:\n\nThe paper introduces a nice idea, and present nice experimental results. however  I find the theoretical analysis not very informative, and  distractive from the main central idea of the paper. \n \nA more thorough experimentation with the idea using different basis and comparing it to wider networks (equivalent to the number of cosine basis used in the leaned one ) would help more supporting results in the paper. \n\n\nComments: \n\n- Are the weights of the non -linearity learned shared across all units in all layers ? or each unit has it is own non linearity?\n\n- If all weights are tied across units and layers. One question that would be interesting to study , if there is an optimal non linearity. \n\n- How different is the non linearity learned if the hidden units are normalized or un-normalized.  In other words how does the non linearity change if you use or don't use batch normalization? \n\n- Does normalization affect  the conclusion that polynomial basis fail? \n\n\n\n\n\n", "title": "Effect of non linearity and basis expansion choice", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJRT6dgEl": {"type": "review", "replyto": "H1wgawqxl", "review": "The paper suggests a two stage learning of weights and non linearities in neural networks.\n\n- the network is trained with RELU , then it is initialized with tanh Fourier series coefficients. this seems inconsistent. why not train with tanh first? \n\n- It is not intuitively clear what is the effect of learning the non linearity. I suggest to do a 2D data of 2 classes (a ring data for instance ) and to train a a 2 layer network (Linear -> Relu -> linear-> softmax), and then one with a learned non linearity in the fourier basis , and another with tanh ,and plot the decision function surface  to see how it changes with the non linearity choice. \n\n \n-   Seems the non linearity is not moving too much from a tanh. maybe there is a capacity problem in the Fourier expansion. Have you tried using an over-complete basis expansion and regularize with l_1 the coefficient a_i? Maybe this will help also with other basis such polynomials that did not work?\n\nSummary:\n\nThe paper introduces a parametric class for non linearities used in neural networks. The paper suggests two stage optimization to learn the weights of the network, and the non linearity weights.\n\nsignificance:\n\nThe paper introduces a nice idea, and present nice experimental results. however  I find the theoretical analysis not very informative, and  distractive from the main central idea of the paper. \n \nA more thorough experimentation with the idea using different basis and comparing it to wider networks (equivalent to the number of cosine basis used in the leaned one ) would help more supporting results in the paper. \n\n\nComments: \n\n- Are the weights of the non -linearity learned shared across all units in all layers ? or each unit has it is own non linearity?\n\n- If all weights are tied across units and layers. One question that would be interesting to study , if there is an optimal non linearity. \n\n- How different is the non linearity learned if the hidden units are normalized or un-normalized.  In other words how does the non linearity change if you use or don't use batch normalization? \n\n- Does normalization affect  the conclusion that polynomial basis fail? \n\n\n\n\n\n", "title": "Effect of non linearity and basis expansion choice", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}