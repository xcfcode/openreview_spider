{"paper": {"title": "Which Tasks Should Be Learned Together in Multi-task Learning?", "authors": ["Trevor Standley", "Amir R. Zamir", "Dawn Chen", "Leonidas Guibas", "Jitendra Malik", "Silvio Savarese"], "authorids": ["tstand@cs.stanford.edu", "zamir@cs.stanford.edu", "sdawnchen@gmail.com", "guibas@cs.stanford.edu", "malik@eecs.berkeley.edu", "ssilvio@stanford.edu"], "summary": "We analyze what tasks are best learned together in one network, and which are best to learn separately. ", "abstract": "Many computer vision applications require solving multiple tasks in real-time. A neural network can be trained to solve multiple tasks simultaneously using 'multi-task learning'. This saves computation at inference time as only a single network needs to be evaluated. Unfortunately, this often leads to inferior overall performance as task objectives compete, which consequently poses the question: which tasks should and should not be learned together in one network when employing multi-task learning? We systematically study task cooperation and competition and propose a framework for assigning tasks to a few neural networks such that cooperating tasks are computed by the same neural network, while competing tasks are computed by different networks. Our framework offers a time-accuracy trade-off and can produce better accuracy using less inference time than not only a single large multi-task neural network but also many single-task networks.\n", "keywords": ["multi-task learning", "Computer Vision"]}, "meta": {"decision": "Reject", "comment": "An approach to make multi-task learning is presented, based on the idea of assigning tasks through the concepts of cooperation and competition. \n\nThe main idea is well-motivated and explained well. The experiments demonstrate that the method is promising. However, there are a few  concerns regarding fundamental aspects, such as: how are the decisions affected by the number of parameters? Could ad-hoc algorithms with human in the loop provide the same benefit, when the task-set is small? More importantly, identifying task groups for multi-task learning is an idea presented in prior work, e.g. [1,2,3]. This important body of prior work is not discussed at all in this paper.\n\n[1] Han and Zhang. \"Learning multi-level task groups in multi-task learning\"\n[2] Bonilla et al. \"Multi-task Gaussian process prediction\"\n[3] Zhang and Yang. \"A Survey on Multi-Task Learning\"\n"}, "review": {"HJgJ83u0KS": {"type": "review", "replyto": "HJlTpCEKvS", "review": "This paper focuses on how to partition a bunch of tasks in several groups and then it use multi-task learning to improve the performance.  The paper makes an observation that multi-task relationships are not entirely correlated to transfer relationships and proposes a computational framework to optimize the assignment of tasks to network under a given computational budget constraint. It experiments on different combinations of the tasks and uses two heuristics to reduce the training overheads, early stopping approximation and higher order approximation. \n\nPlease see the detained comments as follows:\n1. The experiments are based on the assumptions that the network structures (how parameters are shared across tasks) are fixed. From my perspective, understanding how to optimize the parameters sharing across two tasks should be the first step to study how to optimally combine the training of tasks. Otherwise, different parameter sharing structures across tasks may lead to different conclusions.\n\n2. It requires optimization of the article structure. E.g., algorithm 1 is important and should be in the main context.\n\n3. It is also related to neural architecture search and it requires some discussions.\n\n4. The paper is over-length. \n\n5. A lot of typos. \n\nNits:\nPage 1: vide versa -> vice versa\n\nPage 3: two networsk -> two networks\n\nPage 6: budge ?? 1.5\n\nPage 7: overlap between lines (under figure 3)\n\nPage 8: half segmented sentence in section 6.\n", "title": "Official Blind Review #1", "rating": "6: Weak Accept", "confidence": 4}, "S1lIQQ92jB": {"type": "rebuttal", "replyto": "HkxHDaSaKS", "comment": "Thank you for your review and your appreciation of the section comparing transfer learning and multi-task learning. We value your comments and will be updating the final version according to the discussion below.\n\nThe data in Figure 3 shows the performance of all-in-one networks and individual networks as the number of parameters grows. The trend seems to be that as the number of parameters increases, training individually actually beats all-in-one networks by wider and wider margins. Thus, competition actually seems to increase as the number of parameters grows. Nevertheless, a distillation approach seems interesting and MTL performance might be improved by distilling from very large networks. We are not aware of any work exploring this technique, but it might be an interesting topic for a separate study of its own.\n\nIf distillation does work for MTL, it could be combined with our technique, possibly leading to even better performance.\n\nWe've edited the paper to exactly 8 pages by removing unnecessary section headings and making minor wording changes.\n", "title": "Response to Review #2"}, "BkeoAM9hor": {"type": "rebuttal", "replyto": "ByeKMG-W5H", "comment": "Thank you for your review and finding the submission \u201ca good paper to open interesting research direction with solid baselines\u201d. We appreciate your comments and will be updating the final version according to the discussion below.\n\nRegarding manually designed groupings, Figure 5 shows that there seems to be a negative correlation between multi-task affinity and the more intuitive transfer learning affinity of Taskonomy. This makes it seem unlikely that a human would be able to do a good job of engineering task groupings without extensive experience. Furthermore, humans are unlikely to leverage the full potential of auxiliary tasks (i.e., tasks that help other tasks when trained with them but have better performance when trained separately).\n\nAs an example, using the data in Table 9, we can see how well the grouping you suggest performs. We can also evaluate an alternative grouping supported by intuition (e.g., group the two 3D tasks together and the two 2D tasks together). The table below shows how each of those groupings perform versus the computationally found optimal grouping:\n\n        Grouping                                           Total Loss (lower is better) \noptimal 3 grouping (SDn,N,nKE)                       0.44235\t\nYour suggested grouping (S, NE, DK)              0.45866\t\n2D/3D grouping (S, DN, KE)                               0.46368\t\n\nWe see that the two human designed groupings underperform the optimal grouping found computationally. In addition, the optimal task grouping is likely architecture/data-dependent at least to a certain extent, which supports developing computational methods for finding them, in contrast to fixed human intuitions.\n\nAs for the comparison to Sener and Koltun (and single traditional network), the reason we chose to make the network wider rather than deeper is that wider networks can emulate multiple networks, but it is not clear that deeper ones can. Nevertheless it would be interesting to see how well deeper (or some combination of deeper and wider) networks perform.  We will have this comparison for the camera ready, but we expect they will perform similarly to the wider networks we use.\n", "title": "Response to Review #3"}, "S1gehxK3sB": {"type": "rebuttal", "replyto": "HJgJ83u0KS", "comment": "Thank you for your review. We appreciate your comments. \n\n1. How to best share parameters:\nWe agree that finding the best way to share parameters is an open problem and a promising direction for future work. However, we believe that a good technique for determining which parameters to share could be used in conjunction with our techniques to yield further performance improvements.\n\nIn our work, we chose the [encoder->multiple decoder] structure because a number of recent and high profile MTL works use it (Kokkinos 2016 (Ubernet), Chen et al 2018 (Gradnorm), Sener et al 2018, Kendall et al. (2018) and many more). Furthermore, we considered two additional sharing schemes, [U-Net with multiple output channels] and [encoder->single decoder with multiple output channels]. Both were found to be inferior to [encoder->multiple decoder]. Experimental numbers below for 1-SNT networks trained on all tasks jointly:\n\n                                                          total_loss (lower is better)\nU-Net                                                               0.414                               14.05% Worse\nencoder->single decoder                             0.377                               3.85% Worse\nencoder->multiple decoders (ours)           0.363\n*note that these numbers cannot be compared with those in the paper due to differences in the loss function.\n\nThis is far from a comprehensive search of parameter sharing possibilities, but out of the techniques commonly used in the literature, the one we chose performs the best.\n\n2. Algorithm 1.\nThanks for the suggestion. We believe that the particular algorithm chosen for our optimization problem is a detail because many algorithms would work equally well, but we can move Algorithm 1 to the main text if other reviewers agree.\n\n3. Neural Architecture Search section.\nWe've added a neural architecture search section in the related work.\n\n4. Paper length. \nWe've edited the paper to exactly 8 pages by removing unnecessary section headings and making minor wording changes.\n\n5. Typos. \nThank you for these edits. We've gone through the entire paper and fixed typos and grammatical errors. \n", "title": "Response to Review #1"}, "r1e4KROnjH": {"type": "rebuttal", "replyto": "HJlTpCEKvS", "comment": "We thank the reviewers and meta-reviewers for their consideration. \n\nWe've edited the paper to exactly 8 pages by removing unnecessary section headings and making minor wording changes.\n", "title": "Thanks"}, "HkxHDaSaKS": {"type": "review", "replyto": "HJlTpCEKvS", "review": "This submission studies how to group tasks to train together to find a better runtime-accuracy trade-off with a single but large multi-task neural model. The authors perform an extensive empirical study in the Taskonomy dataset. I like Section 6.1 in particular, which shows the difference between multi-task learning and transfer learning. \n\nMy main concern is that the competition between different tasks may stem from the limited capacity of the model during training. It might be possible that with enough parameters, the competing tasks become compatible. If I were the authors, I would first train a bigger model on multiple tasks and then distill it into a smaller one, which does not increase inference time. \n\nFurthermore, the paper has more than 8 pages. ", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 2}, "ByeKMG-W5H": {"type": "review", "replyto": "HJlTpCEKvS", "review": "This paper works on the problem if training a set of networks to solve a set of tasks. The authors try to discover an optimal task split into the networks so that the test performances are maximized given a fixed testing resource budget. By default, this requires searching over the entire task combination space and is too slow. The authors propose two strategies for fast approximating the enumerative search. Experiments show their searched combinations give better performance in the fixed-budget testing setting than several alternatives.\n\n+ This paper works on a new and interesting problem. Training more than one networks for a few tasks is definitely a valid idea in real applications and related to broad research fields.\n+ The baseline setup is comprehensive. The difference between optimal, random, and worst clearly shows this problem worths effort for research.\n- I believe this problem setup requires a larger task set. In the paper the authors manually picked 5 tasks. It seems straightforward for a human to manually group them together: segmentation and edge/ surface + depth/ keypoint for 3 networks. It is unclear how better a network can do than a human in one minute or should we expect learning the task split is better than manual design.\n- Both technical contributions in Section 3.3 look straightforward. Given the good performance in Figure3, it is fine.\n- I am confused by the comparison to Sener and Koltun. How do you change the inference budget for them? If it is changing the number of channels for a single network, I believe it can be improved more.\n\nOverall I believe this is a good paper to open interesting research direction with solid baselines. I am happy to accept this paper and see more exciting future works in this direction.", "title": "Official Blind Review #3", "rating": "6: Weak Accept", "confidence": 2}}}