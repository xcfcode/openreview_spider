{"paper": {"title": "Wav2Letter: an End-to-End ConvNet-based Speech Recognition System", "authors": ["Ronan Collobert", "Christian Puhrsch", "Gabriel Synnaeve"], "authorids": ["locronan@fb.com", "cpuhrsch@fb.com", "gab@fb.com"], "summary": "We propose convnet models and new sequence criterions for training end-to-end letter-based speech systems.", "abstract": "This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC (Graves et al., 2006) while being simpler. We show competitive results in word error rate on the Librispeech corpus (Panayotov et al., 2015) with MFCC features, and promising results from raw waveform.", "keywords": ["Deep learning", "Speech", "Structured prediction"]}, "meta": {"decision": "Reject", "comment": "Without revisions to this paper or a rebuttal from the authors, it is hard to accept this paper. The main contribution of the paper is removing the blank from CTC to create a somewhat different criterion, but this is not particularly novel (see, for example, http://www.isca-speech.org/archive/Interspeech_2016/pdfs/1446.PDF). None of the reviewers were willing to argue for acceptance in the deliberation phase, so unfortunately the recommendation must be to reject this paper."}, "review": {"B19f_3dQx": {"type": "rebuttal", "replyto": "BkUDvt5gg", "comment": "Dear authors,\n\nHere are some missing relevant citations.\n\nYou should definitely cite the original paper that used CTC with characters.\nGraves et al., \"Towards End-to-End Speech Recognition with Recurrent Neural Networks\", in ICML 2014.\n\nYou should probably also cite and have a related work section with attention-based models such as:\nChan et al., \"Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition\", in ICASSP 2016.\nBahdanau et al., \"End-to-End Attention-based Large Vocabulary Speech Recognition\", in ICASSP 2016.\n\nboth of which are highly relevant to end-to-end ASR.\n\nQuestion:\nWhy did you use Librispeech as opposed to WSJ and/or SWBD. Most end-to-end ASR papers publish on WSJ, especially since there is an established benchmark for comparison (i.e., Graves et al., 2014, Bahdanau et al., 2016, Chan et al., 2016). SWBD also has much stronger benchmarks from the general speech community, and even for end-to-end ASR (i.e., see MSR's CTC paper by Zweig et., \"Advances in All-Neural Speech Recognition\", 2016). You should also definitely comment and compare to Zweig's paper, since they used a similar encoding mechanism.\n\nQuestion:\nIs \"Letter Error Rate\" (LER) the common terminology? From Alex Graves papers and others I see \"Character Error Rate\" (CER). What is the difference?\n\nQuestion:\nVery cool that you can combine wav+cnns+ctc->ASR, but still a little bit disappointed that handcrafted features perform better. Do you expect this to change with more data?", "title": "citations missing"}, "rJk124PQx": {"type": "rebuttal", "replyto": "ryT2_zDQe", "comment": "Yes, in theory when comparing scores of same-length paths normalized or un-normalized acoustic model scores will not matter, but do matter for transitions. My answer was addressing the acoustic model side, as asked by Reviewer 3. Note that we find training with normalized transition scores much harder and more unstable than the un-normalized version. We can try to squeeze in a comment about this.", "title": "More on normalization."}, "BJsDsND7x": {"type": "rebuttal", "replyto": "rymoaGsGx", "comment": "Thanks a lot for mentioning these references, we should have added more context about our method. [Bengio, 1993] is a survey and mentions several techniques, including conditional maximum likelihood (CML) approaches like MMI. [Johansen et al, 1996] and [Povey et al, 2016] both refer to MMI (Compared to CML, MMI algorithm updates only the acoustic model (not the language model), but common usage often mix up the two names). MMI maximizes the logarithm of the ratio of two likelihoods (constrained model on the numerator, unconstrained on the denominator). In contrast, our approach considers only un-normalized scores, both for the acoustic model output scores and transition scores, which follows a line of research running parallel to hybrid approaches and MMI-like systems. More precisely, in [Bottou, 1991] it was first mentioned concerns about using normalized probabilities in discriminative sequence models. In the very last section of [Bengio, 1993],  a research direction to address the issue is mentioned, but no solution is proposed. The first solution came with [Denker and Burges, 1994], then a more general framework (GTNs) was proposed in [Bottou and LeCun, 1997] and [LeCun et al, 1998]. CRFs also follow this line of research, with [Lafferty et al, 2001]. While we should have added more context about our approach (we will fix this), we found relevant to relate it to CTC, given that many people using sequence-level end-to-end-training now rely on CTC.\n\nNote that in [Povey et al, 2016], un-normalized probabilities are mentioned (\"We interpret the neural net output as the log of a pseudo-likelihood\") concerning the acoustic model. A 4-gram (phone) language model is used in the normalization, but seemed to be trained generatively beforehand. In contrast, in Figure 3c we have a 2-gram (letter) language model, trained discriminatively, jointly with the rest of the architecture. (Extension to a 4-gram letter language model could be possible (say via sampling for efficiency) and is left to future work). It is hard to conclude on the actual other differences, as in [Povey et al, 2016] no equation is provided, a reference to the MMI research lineage being instead provided, and no links with the GTNs/CRFs research lineage was established (\"We don\u2019t give any equations here, because MMI training is well known\"). Finally, we did not cite [Povey et al, 2016], as it was published after the writing of our paper. We will add it (ASAP) with more context around our approach in a future revision.\n\n[Bottou, 1991] Une Approche th\u00e9orique de l'Apprentissage Connexionniste: Applications \u00e0 la Reconnaissance de la Parole.\n[Denker and Burges, 1994] Image Segmentation and Recognition.\n[Bottou and LeCun, 1997] Global Training of Document Processing Systems using Graph Transformer Networks.\n[LeCun et al, 1998] Gradient Based Learning Applied to Document Recognition.\n[Lafferty et al, 2001] Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.\n", "title": "Missing context around our approach."}, "BkoOqVD7l": {"type": "rebuttal", "replyto": "BJDsh7jGl", "comment": "Our system could be seen as from the CRF family, not from the HMM family. CRFs are discriminative models, compared to HMMs which are generative. All our scores (transitions or observations) are not normalized, the normalization being achieved at the sequence level. Compared to standard CRFs, our model is non-linear, and trained in a end-to-end manner. See [Lafferty et al, 2001]. We are not sure what the comment \"HMM-based system that people are using now\" is referring to; standard hybrid NN/HMM systems also use normalized probabilities, often skipping details when converting conditional probabilities from the neural net into emission probabilities. You can also read the answer to Reviewer 1 below for more context around our approach.\n\nConcerning d_y, it is the output size of a convolutional layer. In the instance of the last layer d_y is also the size of the letters alphabet (|L|).\n\n[Lafferty et al, 2001] Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.\n", "title": "HMMs and CRFs."}, "rk3ojnrQg": {"type": "rebuttal", "replyto": "HJFR8A0zx", "comment": "Thanks for pointing this out; there is a typo, indeed. We will fix it.", "title": "logadd definition"}, "BJeX9nrQl": {"type": "rebuttal", "replyto": "Sy9BdCRMx", "comment": "The t notation was used to represent time; it is true there is a different time scale between (1) and x_t; we will update with t and t', as you suggest this might be clearer.", "title": "t notation"}, "HkFGFnHmx": {"type": "rebuttal", "replyto": "rJrwyyyml", "comment": "That is a very good question, thanks. Yes, we actually tried with and without normalization; performance (both speed and WER) was not affected, as long as the scaling factor was properly tuned for each use case. In practice, the beam search threshold was robust to different sentences and conditions on our dataset.", "title": "acoustic score normalization"}, "rJ8ULhSQg": {"type": "rebuttal", "replyto": "rJNG6CAfg", "comment": "In many standard datasets (as LibriSpeech that we use here), numbers are spelled in plain letters. That is what we use and so we do not encounter a problem with this.", "title": "handling digits"}, "HJFR8A0zx": {"type": "review", "replyto": "BkUDvt5gg", "review": "I suppose that your definition of the logadd function is wrong, correct would be log(e^a+e^b) if a and b are log-probabilities (with your definition you would end up with a times b). Please confirm.This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. \n\nThe approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.\n\nYou are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.\n\nThe submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.\n\nPrior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.\n\nWhat do you mean by transition \"scalars\"?\n\nI do not repeat further comments here, which were already given in the pre-review period.\n\nMinor comments:\n - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly\n   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)\n - Sec. 2.3: Bayse -> Bayes\n - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).\n - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)\n - Sec. 2.4, first line: threholding -> thresholding (spell check..)\n - Figure 4: mention the corpus used here - dev?\n", "title": "logadd?", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJrUKI_7e": {"type": "review", "replyto": "BkUDvt5gg", "review": "I suppose that your definition of the logadd function is wrong, correct would be log(e^a+e^b) if a and b are log-probabilities (with your definition you would end up with a times b). Please confirm.This submission proposes a letter-level decoder with a variation of the CTC approach they call ASG, where the blank symbol is dropped and replaced by letter repetition symbols, and where explicit normalization is dropped. Both the description of a letter-level model (though not novel), as well as the CTC-variant are interesting. \n\nThe approach is evaluated on the LibriSpeech task. The authors claim that their approach is competitive. They compare their modelling variant ASG to CTC, but a comparison of the letter-level approach to available word-level results are missing. Compared to the results obtained in Panayotov et al. 2015, the performance obtained here seems only comparable to word-level GMM/HMM models, but worse than word-level hybrid DNN/HMM models, though Panayotov et al. also appled speaker adaptation, which was not done, as far as I can see. I suggest to add a comparison to Panyotov's results (in addition to mentioning Baidu's results on Librispeech, which are not comparable due to much larger amounts of training data), to allow readers to get a quantitative idea. As pointed out by the authors in the text, Baidu's GPU implementation for CTC is more aimed at larger vocabularies, therefore the comparison to GPU in Tables 1a-c do not seem to be helpful for this work, without further discussing the implementations.\n\nYou are using quite a huge analysis window (nearly 2s). Even though other authors also use windows up to 0.5s to 1s (e.g. MRASTA features), some comments on how you arrive at such a large window, and what advantages you observe for it, would be interesting.\n\nThe submission is well written, though more details on the experiences with using non-normalized (transition) scores and beam pruning would be desirable. Table 1 would be better readable if the units of the numbers shown in a/b/c would be shown within the tables, and not only in the caption.\n\nPrior (partial) publications of this work (your NIPS end-to-end workshop paper) should clearly be mentioned/referenced.\n\nWhat do you mean by transition \"scalars\"?\n\nI do not repeat further comments here, which were already given in the pre-review period.\n\nMinor comments:\n - Sec. 2.3, end of 2nd sentence: train properly the model -> train the model properly\n   End of same paragraph: boostrap -> bootstrap (such errors should be avoided by performing an automatic spell check)\n - Sec. 2.3: Bayse -> Bayes\n - definition of logadd is wrong (see comment) - (applies also for your NIPS end-to-end workshop paper).\n - line before Eq. (3): all possible sequence of letters -> all possible sequences of letters (plural)\n - Sec. 2.4, first line: threholding -> thresholding (spell check..)\n - Figure 4: mention the corpus used here - dev?\n", "title": "logadd?", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "BJDsh7jGl": {"type": "review", "replyto": "BkUDvt5gg", "review": "Fig 3(a) looks like a monophone HMM with self-loop and using a character-level lexicon, and eq(3) is very similar to sequence-training of HMMs. So the question is how it is different from the HMM based system that people are using now?\n\nd_y in section 2.2 is reused, see eq(1) and last paragraph of section 2.2.\u200bThere have been numerous works \u200bon learning from raw waveforms and training letter-based CTC networks for speech recognition, however, there are very few works on combining both of them with purely ConvNet as it is done in this paper. It is interesting to see results on a large scale corpus such as Librispeech that is used in this paper, though some baseline results from hybrid NN/HMM systems should be provided. To readers, it is unclear how this system is close to state-of-the-art only from Table 2.\n\nThe key contribution of this paper may be the end-to-end sequence training criterion for their CTC variant (where the blank symbol is dropped), which may be viewed as sequence training of CTC as H. Sak, et al. \"Learning acoustic frame labeling for speech recognition with recurrent neural networks\", 2015. However, instead of generating the denominator lattices using a frame-level trained CTC model first, this paper directly compute the sequence-level loss by considering all the competing hypothesis in the normalizer. Therefore, the model is trained end-to-end. From this perspective, it is closely related to D. Povey's LF-MMI for sequence-training of HMMs. As another reviewer has pointed out, references and discussions on that should be provided. \n\nThis approach should be more expensive than frame-level training of CTCs, however, from Table 1, the authors' implementation is much faster. Did the systems there use the same sampling rate? You said at the end of 2.2 that the step size for your model is 20ms. Is it also the same for Baidu's CTC system. Also, have you tried increasing the step size, e.g. to 30ms or 40ms, as people have found that it may work (equally) better, while significantly cut down the computational cost.", "title": "How is this model different from conventional HMM based model?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJgU53uXl": {"type": "review", "replyto": "BkUDvt5gg", "review": "Fig 3(a) looks like a monophone HMM with self-loop and using a character-level lexicon, and eq(3) is very similar to sequence-training of HMMs. So the question is how it is different from the HMM based system that people are using now?\n\nd_y in section 2.2 is reused, see eq(1) and last paragraph of section 2.2.\u200bThere have been numerous works \u200bon learning from raw waveforms and training letter-based CTC networks for speech recognition, however, there are very few works on combining both of them with purely ConvNet as it is done in this paper. It is interesting to see results on a large scale corpus such as Librispeech that is used in this paper, though some baseline results from hybrid NN/HMM systems should be provided. To readers, it is unclear how this system is close to state-of-the-art only from Table 2.\n\nThe key contribution of this paper may be the end-to-end sequence training criterion for their CTC variant (where the blank symbol is dropped), which may be viewed as sequence training of CTC as H. Sak, et al. \"Learning acoustic frame labeling for speech recognition with recurrent neural networks\", 2015. However, instead of generating the denominator lattices using a frame-level trained CTC model first, this paper directly compute the sequence-level loss by considering all the competing hypothesis in the normalizer. Therefore, the model is trained end-to-end. From this perspective, it is closely related to D. Povey's LF-MMI for sequence-training of HMMs. As another reviewer has pointed out, references and discussions on that should be provided. \n\nThis approach should be more expensive than frame-level training of CTCs, however, from Table 1, the authors' implementation is much faster. Did the systems there use the same sampling rate? You said at the end of 2.2 that the step size for your model is 20ms. Is it also the same for Baidu's CTC system. Also, have you tried increasing the step size, e.g. to 30ms or 40ms, as people have found that it may work (equally) better, while significantly cut down the computational cost.", "title": "How is this model different from conventional HMM based model?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}