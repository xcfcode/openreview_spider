{"paper": {"title": "Revisiting Denoising Auto-Encoders", "authors": ["Luis Gonzalo Sanchez Giraldo"], "authorids": ["lgsanchez@cs.miami.edu"], "summary": "Modified objective for denoising autoencoders with explicit robustness in the encoding.", "abstract": "Denoising auto-encoders (DAE)s were proposed as a simple yet powerful way to obtain representations in an unsupervised manner by learning a map that approximates the clean inputs from their corrupted versions. However, the original objective function proposed for DAEs does not guarantee that denoising happens only at the encoding stages. We argue that a better representation can be obtained if the encoder is forced to carry out most of the denoising effort. Here, we propose a simple modification to the DAE's objective function that accomplishes the above goal. ", "keywords": []}, "meta": {"decision": "Reject", "comment": "The proposed modification of the denoising autoencoder objective is interesting. Shifting some of the burden to the encoder has potential, but the authors need to to show that this burden cannot be transferred to the decoder; they propose some ways of doing this in their response, but these should be explored. And more convincing, larger-scale results are needed."}, "review": {"Hy0373JVl": {"type": "rebuttal", "replyto": "B1ZXuTolx", "comment": "An updated version of the paper taking into consideration the reviewers comments has been uploaded", "title": "paper update"}, "B1A873yVg": {"type": "rebuttal", "replyto": "B1y4rQbml", "comment": "Thanks for the observation.\nIt is indeed possible to obtain a trivial behaviour of the modified denoising auto-encoder where the encoding term is compensated by shrinkage of the encoding weights. The most straightforward way to avoid such collapse is by using tied weights or by adding a batch normalization to the top layer of the encoder. \nAnother way is through the choice of the encoding loss. A normalized distance for example the squared Euclidean distance divided by the total variance of the encoded data can also avoid such degenerate cases.", "title": "details on auto encoder architecture and objective function "}, "HJGEWn1Vl": {"type": "rebuttal", "replyto": "Bk7n70vXe", "comment": "Although the two approaches are indeed very similar in spirit they differ in two main aspects:\n-- The role and the choice of corruption process in the denoising auto-encoder.\n-- The choice of loss (distance) for the encoder and decoder terms.\n\nIt was argued that for very small additive Gaussian corruption and Euclidean norm for the reconstruction penalty, the objective of the denoising autoencoder could be approximated by the same reconstruction term of the original auto-encoder objective and a second term containing the norm of the Jacobian of the encoder-decoder function. Under similar conditions, the encoding loss in the modified denoising auto-encoder objective could be approximated by considering a term containing the norm of the Jacobian of the encoder, as well. Therefore, different corruption processes and different distances do not necessarily yield the same interpretation.\n\n", "title": "relation to contractive autoencoders"}, "Bk7n70vXe": {"type": "review", "replyto": "B1ZXuTolx", "review": "How different is the proposed regularizer differ from the one introduced in Contractive autoencoder, which also tries to enforce the encoding to be robust to perturbation in the input?The work introduced a new form of regularization for denoising autoencoders, which explicitly enforces robustness in the encoding phrase w.r.t. input perturbation. The author motivates the regularization term as minimizing the conditional entropy of the encoding given the input. The modifier denoising autoencoders is evaluated on some synthetic datasets as well as MNIST, along with regular auto-encoders and denoising autoencoders. The work is fairly similar to several existing extensions to auto-encoders, e.g., contractive auto encoders, which the author did not include in the comparison.   The experiment section needs more polishing. More details should be provided to help understand the figures in the section. ", "title": "Contractive autoencoders", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BJMGCuMVe": {"type": "review", "replyto": "B1ZXuTolx", "review": "How different is the proposed regularizer differ from the one introduced in Contractive autoencoder, which also tries to enforce the encoding to be robust to perturbation in the input?The work introduced a new form of regularization for denoising autoencoders, which explicitly enforces robustness in the encoding phrase w.r.t. input perturbation. The author motivates the regularization term as minimizing the conditional entropy of the encoding given the input. The modifier denoising autoencoders is evaluated on some synthetic datasets as well as MNIST, along with regular auto-encoders and denoising autoencoders. The work is fairly similar to several existing extensions to auto-encoders, e.g., contractive auto encoders, which the author did not include in the comparison.   The experiment section needs more polishing. More details should be provided to help understand the figures in the section. ", "title": "Contractive autoencoders", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1y4rQbml": {"type": "review", "replyto": "B1ZXuTolx", "review": "The modified DAE objective could lead to a model where the encoding weights are very small in order to render the second regularization term negligible and then the denoising could take place in the decoder. So this modified DAE does not guarantee that the encoder performs the denoising. Can you comment on this and any measures you took to avoid such a collapse?\n\nCan you explain in detail the specific  architecture of encoder and decoder (how many hidden layers, tied or untied weights between encoder and decoder)?The paper proposes a modified DAE objective where it is the mapped representation of the corrupted input that is pushed closer to the representation of the uncorrupted input. This thus borrows from both denoising (DAE) for the stochasticity and from the contractive (CAE) auto-encoders objectives (which the paper doesn\u2019t compare to) for the representational closeness, and as such appears rather incremental. In common with the CAE, a collapse of the representation can only be avoided by additional external constraints, such as tied weights, batch normalization or other normalization heuristics. While I appreciates that the authors added a paragraph discussing this point and the usual remediations after I had raised it in an earlier question, I think it would deserve a proper formal treatment. Note that such external constraints do not seem to arise from the information-theoretic formalism as articulated by the authors. This casts doubt regarding the validity or completeness of the proposed formal motivation as currently exposed.  What the extra regularization does from an information-theoretic perspective remains unclearly articulated (e.g. interpretation of lambda strength?).\n\nOn the experimental front, empirical support for the approach is very weak: few experiments on synthetic and small scale data. The modified DAE's test errors on MNIST are larger than those of Original DAE all the time expect for one precise setting of lambda, and then the original DAE performance is still within the displayed error-bar of the modified DAE. So, it is unclear whether the improvement is actually statistically significant. \n", "title": "Details on autoencoder architecture", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "BJGxx7DVl": {"type": "review", "replyto": "B1ZXuTolx", "review": "The modified DAE objective could lead to a model where the encoding weights are very small in order to render the second regularization term negligible and then the denoising could take place in the decoder. So this modified DAE does not guarantee that the encoder performs the denoising. Can you comment on this and any measures you took to avoid such a collapse?\n\nCan you explain in detail the specific  architecture of encoder and decoder (how many hidden layers, tied or untied weights between encoder and decoder)?The paper proposes a modified DAE objective where it is the mapped representation of the corrupted input that is pushed closer to the representation of the uncorrupted input. This thus borrows from both denoising (DAE) for the stochasticity and from the contractive (CAE) auto-encoders objectives (which the paper doesn\u2019t compare to) for the representational closeness, and as such appears rather incremental. In common with the CAE, a collapse of the representation can only be avoided by additional external constraints, such as tied weights, batch normalization or other normalization heuristics. While I appreciates that the authors added a paragraph discussing this point and the usual remediations after I had raised it in an earlier question, I think it would deserve a proper formal treatment. Note that such external constraints do not seem to arise from the information-theoretic formalism as articulated by the authors. This casts doubt regarding the validity or completeness of the proposed formal motivation as currently exposed.  What the extra regularization does from an information-theoretic perspective remains unclearly articulated (e.g. interpretation of lambda strength?).\n\nOn the experimental front, empirical support for the approach is very weak: few experiments on synthetic and small scale data. The modified DAE's test errors on MNIST are larger than those of Original DAE all the time expect for one precise setting of lambda, and then the original DAE performance is still within the displayed error-bar of the modified DAE. So, it is unclear whether the improvement is actually statistically significant. \n", "title": "Details on autoencoder architecture", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJokE01me": {"type": "rebuttal", "replyto": "SkjFp7yQx", "comment": "The constraint of having both the encoding of the corrupted input and the clean input to lie close in the code space has as a main purpose to remove any contraction that may be occurring at the decoder which may be contributing to the denoising objective of the auto-encoder. I think the interpretation suggested by the reviewer can be valid, but we have to mention that the denoising objective is still important. Adding the encoding  constraint to the regular auto-encoder objective does not lead to the same result from learning. However, the constraint in the encoder could be given this same interpretation (clean and noisy input must be \"recognized\" as being the same by the encoder) as with the denoising objective. ", "title": "Response to reviewers about interpretation"}, "SkjFp7yQx": {"type": "review", "replyto": "B1ZXuTolx", "review": "Can the new regularizer be interpreted as enforcing the constraint that the \nclean and noisy input must be \"recognized\" (close in code space) as being the same by the encoder? The paper proposes to add an additional term to the denoising-autoencoder objective. The new term is well motivated, it introduces an asymmetry between the encoder and decoder, forcing the encoder to represent a compressed, denoised version of the input. The authors propose to avoid the trivial solution introduced by the new term by using tied weights or normalized Euclidean distance error (the trivial solution occurs by scaling the magnitude of the code down in the encoder, and back up in the decoder). The proposed auto-encoder scheme is very similar to a host of other auto-encoders that have been out in the literature for some time. The authors evaluate the proposed scheme on toy-data distributions in 2D as well as MNIST. Although the work is well motivated, it certainly seems like an empirically unproven and incremental improvement to an old idea. \n", "title": "interpretation", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "r14dicb4l": {"type": "review", "replyto": "B1ZXuTolx", "review": "Can the new regularizer be interpreted as enforcing the constraint that the \nclean and noisy input must be \"recognized\" (close in code space) as being the same by the encoder? The paper proposes to add an additional term to the denoising-autoencoder objective. The new term is well motivated, it introduces an asymmetry between the encoder and decoder, forcing the encoder to represent a compressed, denoised version of the input. The authors propose to avoid the trivial solution introduced by the new term by using tied weights or normalized Euclidean distance error (the trivial solution occurs by scaling the magnitude of the code down in the encoder, and back up in the decoder). The proposed auto-encoder scheme is very similar to a host of other auto-encoders that have been out in the literature for some time. The authors evaluate the proposed scheme on toy-data distributions in 2D as well as MNIST. Although the work is well motivated, it certainly seems like an empirically unproven and incremental improvement to an old idea. \n", "title": "interpretation", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}