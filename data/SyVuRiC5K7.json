{"paper": {"title": "LEARNING TO PROPAGATE LABELS: TRANSDUCTIVE PROPAGATION NETWORK FOR FEW-SHOT LEARNING", "authors": ["Yanbin Liu", "Juho Lee", "Minseop Park", "Saehoon Kim", "Eunho Yang", "Sung Ju Hwang", "Yi Yang"], "authorids": ["csyanbin@gmail.com", "juho.lee@stats.ox.ac.uk", "mike_seop@aitrics.com", "shkim@aitrics.com", "eunhoy@kaist.ac.kr", "sjhwang82@kaist.ac.kr", "yi.yang@uts.edu.au"], "summary": "We propose a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem.", "abstract": "The goal of few-shot learning is to learn a classifier that generalizes well even when trained with a limited number of training instances per class. The recently introduced meta-learning approaches tackle this problem by learning a generic classifier across a large number of multiclass classification tasks and generalizing the model to a new task. Yet, even with such meta-learning, the low-data problem in the novel classification task still remains. In this paper, we propose Transductive Propagation Network (TPN), a novel meta-learning framework for transductive inference that classifies the entire test set at once to alleviate the low-data problem. Specifically, we propose to learn to propagate labels from labeled instances to unlabeled test instances, by learning a graph construction module that exploits the manifold structure in the data. TPN jointly learns both the parameters of feature embedding and the graph construction in an end-to-end manner.  We validate TPN on multiple benchmark datasets, on which it largely outperforms existing few-shot learning approaches and achieves the state-of-the-art results. ", "keywords": ["few-shot learning", "meta-learning", "label propagation", "manifold learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "As far as I know, this is the first paper to combine transductive learning with few-shot classification. The proposed algorithm, TPN, combines label propagation with episodic training, as well as learning an adaptive kernel bandwidth in order to determine the label propagation graph. The reviewers liked the idea, however there were concerns of novelty and clarity. I think the contributions of the paper and the strong empirical results are sufficient to merit acceptance, however the paper has not undergone a revision since September. It is therefore recommended that the authors improve the clarity based on the reviewer feedback. In particular, clarifying the details around learning \\sigma_i and graph construction. It would also be useful to include the discussion of timing complexity in the final draft."}, "review": {"ryek8HjfyN": {"type": "rebuttal", "replyto": "HkgdqB_y1V", "comment": "Thanks for the feedback. \n\nWe are going to revise the paper according to the useful suggestions regarding ce-loss and Figure4.\n\n", "title": "Thanks for the feedback. New version will be revised."}, "r1lRhOusR7": {"type": "rebuttal", "replyto": "rkg_emdoA7", "comment": "Thanks for the clarification of reproduction.\n\nFor the pre-processing step you mentioned, here we have two pieces of advices:\n1. We have the dataset flag '--pkl' which controls the usage of pkl data or original image data (our own preprocessing). We tested with our own preprocessing, the performance is similar to pkl data.\n2. For Mengye Ren's code, I think you can refer to https://github.com/renmengye/few-shot-ssl-public/blob/master/fewshot/data/mini_imagenet.py for more details.\n\nFor other code issues, we are glad to offer help in github issue.", "title": "Thanks for the reproduction. Pre-processing details."}, "rkxcWJf_Am": {"type": "rebuttal", "replyto": "rkxfh9RTTQ", "comment": "Thanks for the feedback.\n\nWe got the approval from program chairs to release an anonymous code link, as follow:\nhttps://github.com/anonymisedsupplemental/TPN\n\nWe would like to answer the related questions about our paper and code.", "title": "Anonymous Code Link"}, "r1xOHPbuAm": {"type": "rebuttal", "replyto": "HJx6UQbfhX", "comment": "Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers. Here we respond to your specific comments.\n\n\"What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly. \"\n\n>>> In few-shot learning, episodic paradigm proposed by Matching Networks [1] is widely adopted by current researchers (we follow the same setting to make a fair comparison). In each episode, a small subset of N-way K-shot Q-query examples is sampled from the training set. Typically, for 1-shot experiments, N=5, K=1, Q=15 and for 5-shot experiments, N=5, K=5, Q=15. Thus, the number of training examples are Nx(K+Q) (80 for 1-shot and 100 for 5-shot). Constructing label propagation matrix W involves both support and query examples (80 or 100). So the dimension of W is either 80x80 or 100x100. Running label propagation on such small matrix is quite efficient.\n\n\"It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)\"\n\n>>> In Figure 4 of appendix A, we describe the detailed structure of the graph construction module. After we get the per-example feature representation f_{\\varphi}(x_i) for x_i, we feed it into the graph construction module g_{\\phi}. The output of this module is a one-dimensional scalar. f and g are learned in an end-to-end way in our approach.\n\n\"solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization \"\n\n>>> We want to answer this question from two aspects. On one hand, few-shot learning assumes that training examples in each class are quite small (only 1 or 5). In this situation, Eq (3) and the closed-form version can be efficiently solved, since the dimension of S is only 80x80 or 100x100. On the other hand, there is plenty of prior work on the scalability and efficiency of label propagation, such as [2], [3], [4], which can extend our work to large-scale data. \nOn miniImagenet, we performed iterative optimization and got 53.05/68.75 for 1-shot/5-shot experiments with only 10 steps. This is slightly worse than closed-form version (53.75/69.43), because of the inaccurate computation and unstable gradients caused by multiple step iterations.\n\n\n[1] Vinyals, Oriol, et al. \"Matching networks for one shot learning.\" NIPS. 2016.\n[2] Liang, De-Ming, and Yu-Feng Li. \"Lightweight Label Propagation for Large-Scale Network Data.\" IJCAI. 2018.\n[3] Fujiwara, Yasuhiro, and Go Irie. \"Efficient label propagation.\" ICML. 2014.\n[4] Weston, Jason. \"Large-Scale Semi-Supervised Learning.\"", "title": "Response to AnonReviewer1"}, "Hye0RIZ_RX": {"type": "rebuttal", "replyto": "S1x4ca-chQ", "comment": "Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers. Here we respond to your specific comments.\n\n\"Some technical details are missing. In Section 3.2.2, the authors only explain how they learn example-based \\sigma, but details on how to make graph construction end-to-end trainable are missing. Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?\"\n\n>>> Thanks for pointing out the details. We want to clarify the few-shot setting. We follow the widely-used episodic paradigm proposed by Matching Networks [1]. In each episode (training batch), our algorithm solves a small classification problem which contains N classes each having K support and Q query examples (e.g., N=5, K=1, Q=15, totally 80 examples). The weight matrix is constructed on the support and query examples in each episode rather than the whole dataset. This is very fast and efficient. \nIn deep neural networks, there is a common trick in computing the gradient of operations non-differentiable at some points, but differentiable elsewhere, such as Max-Pooling (top-1) and top-k. In forward computation pass, the index position of the max (or top-k) values are stored. While in the back propagation pass, the gradient is computed only with respect to these saved positions. This trick is implemented in modern deep learning frameworks such as tensorflow and pytorch. In our paper, we use the tensorflow function tf.nn.top_k() to compute k-nearest neighbor operation.\n\n\"Does episode training help label propagation? How about the results of label propagation without the episode training? \"\n\n>>> In our paper, the length scale parameter \\sigma is trained in an example-wise and episodic-wise way, as described in section 3.2.2 and Figure 4 of Appendix A. In order to investigate the benefit of episodic training, we combine the heuristic-based label propagation methods [2] with meta-learning to serve as a transductive baseline. Please refer to Table 1 and Table 2 line \"Label Propagation\". It can be seen that TPN outperforms naive label propagation with a large margin, thus verifying the effectiveness of episode training.\n\n\n[1] Vinyals, Oriol et al. \"Matching networks for one shot learning.\" NIPS. 2016.\n[2] Zhou, Denny et al. \"Learning with local and global consistency.\" NIPS. 2004.", "title": "Response to AnonReviewer2"}, "r1xNOL-uCX": {"type": "rebuttal", "replyto": "Skx7vDii3X", "comment": "Please refer to our main response in an above comment that addresses the primary and common questions amongst all reviewers. Here we respond to your specific comments.\n\n\"(1) There is not much technical contribution. It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning. Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.\"\n\n>>> As mentioned in the main response, the proposed TPN is not a mere combination of CNN representation learning and label propagation. The original label propagation constructs a fixed graph (Eq (1)) to explore the correlation between examples. While in our work, we adaptively construct the graph structure for each episode (training task) with a learnable graph construction module (Figure 4, Appendix A). This leads to better generalization ability for test tasks. \nIn Table 1 and Table 2, the proposed TPN achieved much higher accuracy than the mere combination model (referred to as \"Label Propagation\"). \n\n\"(2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation.  Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature. For example,  on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work. This is a major concern.\"\n\n>>> At first, we want to clarify the few-shot network architecture setting. Currently, there are two common network architectures: 4-layer ConvNets (e.g., [1][2][3]) and 12-layer ResNet (e.g., [4][5][6][7]). Our method belongs to the first one, which contains much fewer layers than the ResNet setting. Thus, it is more reasonable to compare TADAM with ResNet version of our method. To better relieve the reviewer's concern, we implemented our algorithm with ResNet architecture on miniImagenet dataset and show the results as follow:\n\nMethod                                  1-shot    5-shot\nSNAIL [4]                                 55.71     68.88\nadaResNet [5]                        56.88     71.94\nDiscriminative k-shot [6]     56.30     73.90\nTADAM [7]                              58.50     76.70\n--------------------------------------------------------\nOurs                                        59.46     75.65\n--------------------------------------------------------\n\nIt can be seen that we beat TADAM for 1-shot setting. For 5-shot, we outperform all other recent high-performance methods except for TADAM.\n\n>>> We want to clarify that \"Label Propagation\" in Table 1 and Table 2 is a strong baseline. It combines label propagation method [8] with episodic meta-learning. The usage of transductive inference makes this baseline outperform most published state-of-the-art methods. Moreover, the performance of TPN over label propagation is not very small. For example, in miniImagenet, TPN outperforms label propagation with 1.44% and 1.25% for 1-shot and 5-shot respectively, but this advantage grows to 3.20% and 1.68% with \"Higher Shot\" training. The improvements are even larger for tieredImagenet with 4.68% and 2.87%. We believe in few-shot learning, this is a large improvement.\n\n\n[1] Finn, Chelsea, Pieter Abbeel, and Sergey Levine. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" ICML. 2017.\n[2] Snell, Jake, Kevin Swersky, and Richard Zemel. \"Prototypical networks for few-shot learning.\" NIPS. 2017.\n[3] Yang, Flood Sung Yongxin et al. \"Learning to compare: Relation network for few-shot learning.\" CVPR. 2018.\n[4] Mishra, Nikhil et al. \"A simple neural attentive meta-learner.\" ICLR. 2018.\n[5] Munkhdalai, Tsendsuren et al. \"Rapid adaptation with conditionally shifted neurons.\" ICML. 2018.\n[6] Bauer, Matthias et al. \"Discriminative k-shot learning using probabilistic models.\" arXiv. 2017.\n[7] Oreshkin, B.N., Lacoste, A. and Rodriguez, P., 2018. \"TADAM: Task dependent adaptive metric for improved few-shot learning.\" NIPS. 2018.\n[8] Zhou, Denny, et al. \"Learning with local and global consistency.\" NIPS. 2004.", "title": "Response to AnonReviewer3"}, "S1lqdSZuRX": {"type": "rebuttal", "replyto": "SyVuRiC5K7", "comment": "We wish to thank the reviewers for their enlightening feedback! We would like to highlight the novelty and contribution of the proposed method. \n\n(1) The proposed TPN is not a direct combination of the original label propagation and few-shot learning, but a novel transductive meta-learning method when facing unevenly distributed data.\n\nThe main contribution of the proposed TPN is to propose a novel transductive meta-learning method when facing an uneven data distribution. Most of previous (label) propagation algorithms usually assume samples are distributed evenly in the data space. Unfortunately, in low-shot learning, the data is limited and unevenly distributed, which makes most of existing label propagation algorithms inapplicable. To clearly model the data distribution in low-shot learning settings, previous transductive methods adopt a fixed scheme to explore the correlations between data, i.e., compute the weights with a fixed \\sigma as shown in Eq (1). However, as pointed out in previous work [2][3] and in our experimental results, the performance of a transductive method is quite sensitive to the parameter \\sigma, and a fixed \\sigma will lead to suboptimal results. Our proposed TPN adaptively learns data correlation by calculating an optimal \\sigma on a per data basis. As shown in Figure 4 (Appendix A), the correlation among data pairs is optimized and updated in each episode according to data distribution of the neighborhood. In this way, a different model is specifically learned to uncover the correlation of each data pair, thereby largely ameliorating the uneven data distribution problem. \nExperimentally, TPN shows a big advantage over the direct combination (referred to as \"Label Propagation\" in Table 1 and Table 2).\n\n(2) To the best of our knowledge, we are the first to model transductive inference explicitly in the few-shot meta-learning. This transductive setting paves a new way to solve the limited data problem in few-shot learning. As shown in this paper, if one has the test data in whole or a batch manner, transductive inference significantly improves the performance without additional human annotations.\n\n(3) We advanced the state-of-the-art performance on the two most commonly-used benchmark datasets with large margins using the standard 4-layer ConvNets architecture.\n\n\n\n[1] Zhou, Denny, et al. \"Learning with local and global consistency.\" NIPS. 2004.\n[2] Wang, Fei, and Changshui Zhang. \"Label propagation through linear neighborhoods.\" TKDE. 2008.\n[3] Xiaojin Z, Zoubin G. \"Learning from labeled and unlabeled data with label propagation.\" Technical Report. 2002.", "title": "Main response to reviewers"}, "rJxOdw3sTX": {"type": "rebuttal", "replyto": "BJgohCDj6X", "comment": "Thanks for the comment and interest about our paper. \nAccording to the blind review policy, we can not release the code at this moment. We will release our code and the trained model as soon as the review process ends. Meanwhile, we have sent an email to the program chairs to check if it is allowed to release the code anonymously.  We will share the code upon approval.\n\nWe are sure about the reproducibility of the results shown in our paper. And in order to ensure the reproducibility, we ran the test procedure 10 times (each with 600 randomly generated episodes) and reported the average results to avoid accidentally high results. We are not sure if you have reproduced the result as outlined in [1]. If not, we sincerely hope you first try to reproduce the baseline method [1], so you may be closer to the right implementation. It took us quite a while to reproduce [1] even the code has been released. \nNevertheless, we would like to provide more details below which could be useful for you to reproduce the results of our paper.\n\n(1) Our implementation is based on Tensorflow 1.3+, and we also tested on Pytorch 0.4.0. There is only a slight accuracy difference.\n(2) The reason why your results only achieved 25% could be caused by value issues such as divided by zero. Sincerely hope you could double check your code and please make sure you have added an epsilon whenever you call a divide operation. \n(3) Our model is learned end-to-end from scratch, and no pretrain is needed. We did not see your code, but we reckon you did not use the validation set to decide the early stopping iteration, which is commonly used in few-shot learning, such as Prototypical networks. Please use this practice if it is the case.\n(4) The detailed hyperparameters are: alpha=0.99, k=20, query=15, lr=0.001 and halved every 10,000 episodes for at most 100,000 episodes. \n(5) Network architecture details: feature extraction module is exactly the same as Prototypical networks [1], graph construction module is described in Figure 4 of Appendix A. Note that BatchNorm is applied only in Conv layers. In Figure 4, there is no Relu activation after FC layer2. More training details: we use Tensorflow default initialization, BatchNorm with default parameters: decay=0.999 and epsilon=0.001. \n(6) As to preprocessing, for miniImagenet, we follow Prototypical networks [1] while for tieredImagnet we follow Ren et al. [2].\n\nWe have endeavored our best to 'guess' what mistakes you may have made, but there could be other issues that we are unable to enumerate. \nWe highly suggest that a basic starting point is to reproduce the results of Prototypical networks.  Below we provide a few good implementation codes of some related papers. \nPrototypical networks: \n\thttps://github.com/jakesnell/prototypical-networks\n\thttps://github.com/cyvius96/prototypical-network-pytorch\ntieredImagenet: \n\thttps://github.com/renmengye/few-shot-ssl-public\n\n\n[1] Snell, Jake, Kevin Swersky, and Richard Zemel. \"Prototypical networks for few-shot learning.\" NIPS. 2017.\n[2] Ren, Mengye, et al. \"Meta-learning for semi-supervised few-shot classification.\" ICLR. 2018.\n", "title": "Implementation details. Code will be released soon."}, "Skx7vDii3X": {"type": "review", "replyto": "SyVuRiC5K7", "review": "This paper proposes to address few-shot learning in a transductive way by learning a label propagation model in an end-to-end manner.  Semi-supervised few-shot learning is important considering the limitation of the very few labeled instances. This is an interesting work. \n\nThe merits of this paper lie in the following aspects: (1) It is the first to learn label propagation for transductive few-shot learning. (2) The proposed approach produced effective empirical results.\n\nThe drawbacks  of the work include the following: (1) There is not much technical contribution. It merely just puts the CNN representation learning and the label propagation together to perform end-to-end learning. Considering the optimization problem involved in the learning process, it is hard to judge whether the effect of such a procedure from the optimization perspective.  (2) Empirically, it seems TPN achieved very small improvements over the very baseline label propagation.  Moreover, the performance reported in this paper seems to be much inferior to the state-of-the-art results reported in the literature. For example,  on miniImageNet, TADAM(Oreshkin et al, 2018) reported 58.5 (1-shot) and 76.7(5-shot), which are way better than the results reported in this work. This is a major concern.\n", "title": "interesting empirically", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "S1x4ca-chQ": {"type": "review", "replyto": "SyVuRiC5K7", "review": "Summary\nThis paper proposes a meta-learning framework that leverages unlabeled data by learning the graph-based label propogation in an end-to-end manner.  The proposed approaches are evaluated on two few-shot datasets and achieves the state-of-the-art results. \n\nPros. \n-This paper is well-motivated. Studying label propagation in the meta-learning setting is interesting and novel. Intuitively, transductive label propagation should improve supervised learning when the number of labeled instances is low. \n-The empirical results show improvement over the baselines, which are expected. \n\nCons.\n-Some technical details  are missing. In Section 3.2.2, the authors only explain how they learn example-based \\sigma, but details on how to make graph construction end-to-end trainable are missing. Constructing the full weight matrix requires the whole dataset as input and selecting k-nearest neighbor is a non-differentiable operation. Can you give more explanations?\n-Does episode training help label propagation? How about the results of label propagation without the episode training? \n", "title": "Novel idea, but important details and deeper analysis are missing", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJx6UQbfhX": {"type": "review", "replyto": "SyVuRiC5K7", "review": "The paper studies few-host learning in a transductive setting: using meta learning to learn to propagate labels from training samples to test samples. \n\nThere is nothing strikingly novel in this work, using unlabeled test samples in a transductive way seem to help slightly. However, the paper does cover a setup that I am not aware that was studied before. The paper is written clearly, and the experiments seem solid. \n\nComments: \n-- What can be said about how computationally demanding the procedure is? running label propagation within meta learning might be too costly. \n-- It is not clear how the  per-example scalar sigma-i is learned. (for Eq 2)\n-- solving Eq 3 by matrix inversion does not scale. Would be best to also show results using iterative optimization \n\n\n\n\n", "title": "Transductive few-shot by meta-learning to propagate labels for . Solid work.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "H1l4b54ijm": {"type": "rebuttal", "replyto": "SklAweC0qX", "comment": "Thanks for the comments.\n\nFor each episode, we first utilize both the support set and query set to construct the graph structure. Then, label propagation is performed according to the graph information to get all query set labels. The performance gain comes from the fact that we share information among all query examples and learn to propagate labels. In contrast, inductive methods predict query examples one by one, which does not enjoy this benefit.\n\nAs to query set number experiments, please refer to Appendix B.2 for detailed information.\n\nFor distractor classes, this is not the main focus of our paper. However, in order to explore the extent of our method, we performed experiments in the presence of distractor classes (same setting as [1]). The results are shown below:\nModel                                  mini-5way1shot     mini-5way5shot    tiered-5way1shot    tiered-5way5shot\nSoft k-Mean [1]                     48.70+/-0.32            63.55+/-0.28           49.88+/-0.52            68.32+/-0.22\nSoft k-Mean+Cluster [1]      48.86+/-0.32            61.27+/-0.24           51.36+/-0.31            67.56+/-0.10\nMasked Soft k-Means [1]    49.04+/-0.31            62.96+/-0.14           51.38+/-0.38            69.08+/-0.25\nTPN-semi (Ours)                   50.43+/-0.84            64.95+/-0.73           53.45+/-0.93            69.93+/-0.80\n\nIt can be seen that our TPN-semi algorithm outperforms [1] in all cases, although our method is not specifically designed for the distractor-classes problem.\nWe believe with care design, the performance of our method will continue to increase. This will be the future work.", "title": "Response to experiments and methods"}, "Bkli_tYd5Q": {"type": "rebuttal", "replyto": "ryekRHiecQ", "comment": "Thanks for the comments.\nThe state-of-the-art performance on Omniglot is quite high (>99% except for 20way-1shot setting), which means this problem is nearly solved. Also, there is a tendency that recent high-quality papers do not report results on Omniglot, such as TADAM [1] (NIPS2018), Delta-encoder [2] (NIPS2018), LEO [3] (ICLR19 submission). For the 20way-1shot setting, we compare our TPN results with Relation Net and Prototypical Net as follows:\n\t\t\t\t\t20way-1shot\nPrototypical Net           96.00\nRelation Net                  97.60\nTPN\t\t\t\t        98.03\n\nAlthough zero-shot learning is not our focus, TPN can be easily adapted to zero-shot setting. The modification is similar to Prototypical network or Relation Network. First, a function g can be used to map class-level semantic feature into the same space of visual feature. Then, we can construct graph structure using both features and perform label propagation as in few-shot setting. \n\n[1] Oreshkin, Boris N., Alexandre Lacoste, and Pau Rodriguez. \"TADAM: Task dependent adaptive metric for improved few-shot learning.\" NIPS2018\n[2] Schwartz, Eli, et al. \"Delta-encoder: an effective sample synthesis method for few-shot object recognition.\" NIPS2018\n[3] Anonymous, \"Meta-Learning with Latent Embedding Optimization.\" ICLR2019 submission. ", "title": "Response to the experiments"}, "ryeo4LvecQ": {"type": "rebuttal", "replyto": "H1xbcLqkcQ", "comment": "Thanks for pointing out the related work. We would like to include this reference to our manuscript in the next version. \n\nOur paper and the mentioned paper share the same idea of using metric learning and transduction. However, the target tasks are different. We focus on few-shot learning and meta-learning while the mentioned paper deals with unsupervised domain adaptation. This distinction leads to different algorithm designs: we learn to propagate labels while the mentioned paper proposes the transduction and adaptation steps.", "title": "Thank you for pointing out related work"}}}