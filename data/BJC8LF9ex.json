{"paper": {"title": "Recurrent Neural Networks for Multivariate Time Series with Missing Values", "authors": ["Zhengping Che", "Sanjay Purushotham", "Kyunghyun Cho", "David Sontag", "Yan Liu"], "authorids": ["zche@usc.edu", "spurusho@usc.edu", "kyunghyun.cho@nyu.edu", "dsontag@cs.nyu.edu", "yanliu.cs@usc.edu"], "summary": "", "abstract": "Multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. In time series prediction and other related tasks, it has been noted that missing values and their missing patterns are often correlated with the target labels, a.k.a., informative missingness. There is very limited work on exploiting the missing patterns for effective imputation and improving prediction performance. In this paper, we develop novel deep learning models, namely GRU-D, as one of the early attempts. GRU-D is based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network. It takes two representations of missing patterns, i.e., masking and time interval, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to achieve better prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-the-art performance and provides useful insights for better understanding and utilization of missing values in time series analysis.", "keywords": ["Deep learning"]}, "meta": {"decision": "Reject", "comment": "This paper presents a modification of GRU-RNNs to handle missing data explicitly, allowing them to exploit data not missing at random. The method is presented clearly enough, but the reviewers felt that the claims were overreaching. It's also unsatisfying that the method depends on specific modifications of RNN architectures for a particular domain, instead of being a more general approach."}, "review": {"B18v0flvx": {"type": "rebuttal", "replyto": "Bk1ESnC8x", "comment": "\nThanks for your comments and follow-up questions. Please find our reply below.\n\n1) Question about Eq. 4-6.\nA: Firstly we just want to make it clear that the `lambda` mentioned in your question refers to the decay rate `gamma` in these equations. \nAs shown in Eq. 4, in GRU-D, gamma_x = exp(W_x \\dot delta + b_x) and gamma_h = exp(W_h \\dot delta + b_h), where `delta` is the time interval and W_x/h, b_x/h are model parameters.\nSay the numbers of input features and hidden states are |x_t| = D and |h_t| = H, respectively.\nW_x is a diagonal matrix with size [D,D], and W_h is a matrix with size [H,D].\nBy doing this, GRU-D decay the input feature based on its own time interval and decay the hidden states based on the time intervals of all input features.\nThe gamma_h (which is calculated based on delta_t) is different from the reset gate (which is calculated based on h_{t-1} and x_t) in a standard GRU.\n\n2) The homeostatic properties of the human body and Figure 4a).\nA: For some of the input features, simply forward imputation on the missing values can lead to reasonable prediction performance. However, forward imputation is not optimal for all features, otherwise the input decay in Figure 4a) would stay at 1 for all features at all time.\nFollowing the homeostatic properties, we do find a few variables have different decay rates which contribute to the model performance a lot, and many of them are validated by domain experts. Also, the plots in Figure 4a) show the input decay for time interval of <= 24 hours, and the decay values will be lower for longer time interval.\n\n3) `I still maintain that in your specific tests, not-at-random-missingness used for prediction isn't useful for an health care expert. However, GRU-D does use the presence of features as an additional clue for classification.`\nA: We agree that not-at-random-missingness may have not been properly used for health care experts. This may due to several reasons, e.g., limited time constraints to analyze the (missing) data, complex and even unknown reasons and sources of missing values, etc.\nHowever, not properly utilizing the missingness does not mean it is useless. Both the correlation validation like Fig. 1 and the performance improvements gained by GRU-D demonstrate that missingness can also be an informative resource rather than only cumbrance.\n\n4) `a review of better imputation methods would have been needed.`\n`you still ignore a whole field of study designed to solve the missing value problem.`\nA: Thanks for your suggestion on including a broader reviews of imputation methods. We'd like to include imputation literature survey on imputation methods in the upcoming draft.\nWe apologize for the confusion and we are not ignoring the research progress from the imputation field, but we are proposing a novel way to use missingness informativeness directly in the deep learning models which can help in training end-to-end ML/AI systems. As far as we know, imputation for missing-not-at-random is an open ongoing research problem.\n\n5) by combining two imputation methods in your model, which clearly suggests that imputation is a very important factor in learning a good model that handles missing values. \nA: Our model not only combines forward and mean imputations but actually utilizes time intervals and masking. The interval information is never considered in forward/mean imputation methods.\n\n6) `\"by incorporating masking [...] inside the GRU architecture\". With m_t being concatenated to x_t, this claim doesn't stand.`\nA: m_t is not only concatenated to the input and fed to get hidden state H, but also acts as the switch to control the flow of current input x_{t} and decayed input x_{t-1}. We updated figure 3b to make it clearer.\n\n7) Thanks for your detailed comments for the revised paper. We have made corresponding changes in a new version.\n", "title": "Re: A question + comments and rectifications"}, "H1SAfbv8l": {"type": "rebuttal", "replyto": "SywGAzlEg", "comment": "Thanks for your comments and your pre-review suggestions for us to improve this work.\nWe'd like to clarify that our model is widely applicable and competitive as well on big datasets.\nAs shown in the experiments (Section 3.3 and Appendix A.3.5 in the updated draft), the proposed GRU-D model achieves consistent improvement in both one-layer and multi-layer RNN settings on all the datasets of varied size.\nAs far as we know, the MIMIC-III dataset we used is the largest publicly available one of its kind of tens of thousands of admission records. We agreed that the datasets in healthcare is not as many as in other mature domains such as computer vision or natural language processing, where massive public datasets are available and deep learning models are demonstrated to be powerful. Data collection and sharing in healthcare has it own myriad of challenges and is beyond the scope of this paper. \nOur methodologies work irrespective of the dataset size. Also, it is quite important to make deep learning more applicable in domains with moderate amount of available data such as healthcare. Motivated by this, our work is a reasonable approach to improve the RNN performance in such condition by resorting to the missing patterns in the data.", "title": "To AnonReviewer2"}, "rJHpf-w8l": {"type": "rebuttal", "replyto": "ry1MlUM4g", "comment": "Thanks for your time and thoughtful comments! We would like to address your the questions and concerns below.\n\n\nQ1: the novelty of this work is not enough. Adding a decaying smooth factor to input and hidden layers seems to be the main modification of the architecture. \nA: We would like to clarify our contributions: we develop a reasonable and novel RNN framework for time series classification, which 1) capture missing patterns, 2) automatically imputes or accounts for missing values, and 3) consistently improves classification results. To the best of our knowledge, our work is one of the first of this kind. None of the previous works explicitly modified the RNN structure to capture and utilize missingness information for applications such as healthcare. Experiments on both synthetic and real datasets have demonstrated our proposed GRU-D model is the best among existing state-of-the-art models.\n\n\nQ2: the datasets used in this paper are small. \nA: (This question is also asked by AnonReviewer2 and we thus copied the same answer here.)\nAs far as we know, the MIMIC-III dataset we used is the largest publicly available one of its kind of tens of thousands of admission records. We agreed that the datasets in healthcare is not as many as in other mature domains such as computer vision or natural language processing, where massive public datasets are available and deep learning models are demonstrated to be powerful. Data collection and sharing in healthcare has it own myriad of challenges and is beyond the scope of this paper. \nOur methodologies work irrespective of the dataset size. Also, it is quite important to make deep learning more applicable in domains with moderate amount of available data such as healthcare. Motivated by this, we believe our work is a reasonable approach to improve the RNN performance in such condition by resorting to the missing patterns in the data.\n\n\nQ3: the decaying effect might not be able to generalize to other domains. \nA: The proposed model works in a broad set of domains where we encounter time series data missing at random or not at random. In this paper we evaluated our model for healthcare applications. Given that we're taking ICU data as example, our decay idea is very well matched to homeostatic systems (e.g., human body) that actively control themselves to revert to mean. We demonstrated it is indeed helpful in practice.\nFurthermore, our framework is flexible enough to incorporate decay functions in a variety of forms which can handle different missing mechanisms.", "title": "To AnonReviewer4"}, "HyZizbPIe": {"type": "rebuttal", "replyto": "rJcLzVQNx", "comment": "Thanks for your time and thoughtful comments!\n\nBefore addressing detailed comments, we would like to clarify our contribution in this work: we develop a reasonable and novel RNN framework for time series classification, which 1) captures missing patterns, 2) automatically imputes or accounts for missing values, and 3) consistently obtains the state-of-the-art results on the challenging healthcare domain classification problems. To the best of our knowledge, our work is one of the first of this kind. None of the previous works explicitly modify the RNN structure to capture and utilize missingness information for applications such as healthcare. Experiments on synthetic and real datasets have demonstrated our proposed GRU-D model is the best among existing state-of-the-art models.\n\nWe agree that GRU-simple (with and without intervals) models are strong baselines, but the proposed GRU-D model is definitely the best among all GRU baselines for handling and utilizing missingness.\nGRU-simple models often but not always get marginal improvement against other GRU baselines (i.e., GRU-mean/forward). Moreover, the masking and/or time interval are fed into the model same as other input values without considering their roles as `missingness indicators`.\nEmpirically we found GRU-simple baselines (with and without intervals) have quite close performance and are not as good as GRU-D. Thus we take GRU-simple as the representative in the main paper and show its variants in the appendix. It\u2019s worth noting that while time interval doesn\u2019t help much in GRU-simple, it not only improves the performance of GRU-D but also provides useful interpretations for the prediction tasks (as shown in Fig. 4).\n\nWe believe the superiority of GRU-D model over GRU baselines for the following reasons:\n1. It achieves consistent performance improvement on the difficult prediction tasks: ~0.02 AUC score in PhysioNet mortality prediction and ~0.01 AUC score in MIMIC-III mortality prediction over the best baselines.\n2. GRU-D model for healthcare applications is an example of how domain knowledge can be simply incorporated into the model architecture design to better handle and utilize missing values.\n3. GRU-D provides useful insights for raw input features from decay plots/distributions. These results help us understand the underlying variable impact on prediction tasks and also help domain experts understand and apply our model and discover new domain knowledge.\n\n\nOther detailed comments:\n\n\nQ: the relationship with the presence rate of data in the dataset and the diagnostics.\nA: Figure 1 shows the observations on missingness and prediction tasks and, thus demonstrates the usefulness of missingness. Missingness can be due to several factors, such as medical events, saving costs, anomalies, inconvenience and so on. Healthcare providers' judgments and actions may also lead to some missing values in the data. However, missing data is widespread in our dataset and it's hard to figure out the reason for the missing values of each variable.\n\n\nQ: the conclusion that since GRU models displayed the best improvement between a subsample of the dataset and the whole of it means that the improvement is going to continue to grow as more data is added. This fails to consider that non-GRU models actually started with much better results than most GRU ones.\nA: As shown in Figure 7, GRU-D has comparable performance on 2k samples and beats non-GRU models with increasing margin on 10k and 19k samples. Other GRU baselines also improve with more training samples. Given these results, we expect this trend will continue if we have even larger healthcare datasets. Especially, the performance gap between the proposed GRU-D and non-RNN baselines is expected to increase.\n\n\nQ: The Section A.2.3 really belongs in the main article as it deals with important related works. Swap it with the imprecise diagrams of the model if you need space.\nA: We have revised and reorganized the contents on related baseline models as you suggested to make it more clear.\n\n\nQ: No mention of any methods from the statistics literature.\nA: The goal of this work is to make predictions on whether a patient has a disease (or what is his/her mortality status) based on the time series observations with missing values from ICU. \nIn short, it is a time series classification task with utilization of missing patterns but not a data imputation task. Also, as discussed in the paper (Wells et al., 2013), missing not at random imputation techniques lead to suboptimal performance.", "title": "To AnonReviewer5"}, "r1CC63BQl": {"type": "rebuttal", "replyto": "ryuwQxXmg", "comment": "Thanks for your suggestion on experiments for multi-layer RNNs!\n\nAs you mentioned, The methods of handling missing values in the baseline GRU-simple and the proposed GRU-D can be generalized to multi-layer RNNs.\nWe tested the following 2-layer RNN models with similar number of parameters and with more parameters to validate the generalization and efficacy of our model.\nGRU-simple-2: A 2-layer RNN model, where the first RNN layer is GRU-simple, and the second layer is a standard GRU layer.\nGRU-D-2: A 2-layer RNN model, where the first RNN layer is GRU-D, and the second layer is a standard GRU layer.\n\nOur GRU-D model consistently outperforms other baselines. Also a deeper or larger model won't always help, given that the available dataset size is limited.\nHere's a comparison on physionet mortality prediction task:\n\t\t\t\tLayer size\t\t# of params\tAUC score\nGRU-simple\t\t\t43\t\t\t18495\t\t0.8156\nGRU-simple-2\t\t\t32-32\t\t18947\t\t0.8159\nGRU-simple-2(large)\t43-64\t\t39250\t\t0.8208\nGRU-D\t\t\t\t49\t\t\t18838\t\t0.8424\nGRU-D-2\t\t\t\t34-34\t\t18599\t\t0.8420\nGRU-D-2(large)\t\t49-64\t\t40739\t\t0.8363\n\nWe observed similar result improvements on MIMIC-III dataset, where GRU-D outperforms GRU-simple by AUC score >=0.01 with 2 RNN layers.\n\nWe will include the detailed evaluation results on multi-layer RNN models in the appendix of the updated paper.", "title": "Re: Comparison to multi-layer RNNs"}, "ryuwQxXmg": {"type": "review", "replyto": "BJC8LF9ex", "review": "Interesting work!\n\nAnother simple baseline for dealing with missing values would be to add a standard NN layer (instead of a handcrafted one) for preprocessing the input consisting of the data and the mask, \u00a0feeding into the GRU layer. It is conceivable that if the preprocessing layer happens to be some sort of gated, recurrent layer (GRU/LSTM), that it would learn to impute the missing values and compute important statistics of the mask. Therefore, it would important to address the following question:\n \u00a0\nHow does the performance of the GRU-simple baseline change if it is given more hidden layers (with same number of parameters and with increasing number of parameters)? The authors propose a RNN-method for time-series classification with missing values, that can make use of potential information in missing values. It is based on a simple linear imputation of missing values with learnable parameters. Furthermore, time-intervals between missing values are computed and used to scale the RNN computation downstream. The authors demonstrate that their method outperforms reasonable baselines on (small to mid-sized) real world datasets. The paper is clearly written.\nIMO the authors propose a reasonable approach for dealing with missing values for their intended application domain, where data is not abundant and requires smallish models. I\u2019m somewhat sceptical if the benefits would carry over to big datasets, where more general, less handcrafted multi-layer RNNs are an option. ", "title": "Comparison to multi-layer RNNs", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SywGAzlEg": {"type": "review", "replyto": "BJC8LF9ex", "review": "Interesting work!\n\nAnother simple baseline for dealing with missing values would be to add a standard NN layer (instead of a handcrafted one) for preprocessing the input consisting of the data and the mask, \u00a0feeding into the GRU layer. It is conceivable that if the preprocessing layer happens to be some sort of gated, recurrent layer (GRU/LSTM), that it would learn to impute the missing values and compute important statistics of the mask. Therefore, it would important to address the following question:\n \u00a0\nHow does the performance of the GRU-simple baseline change if it is given more hidden layers (with same number of parameters and with increasing number of parameters)? The authors propose a RNN-method for time-series classification with missing values, that can make use of potential information in missing values. It is based on a simple linear imputation of missing values with learnable parameters. Furthermore, time-intervals between missing values are computed and used to scale the RNN computation downstream. The authors demonstrate that their method outperforms reasonable baselines on (small to mid-sized) real world datasets. The paper is clearly written.\nIMO the authors propose a reasonable approach for dealing with missing values for their intended application domain, where data is not abundant and requires smallish models. I\u2019m somewhat sceptical if the benefits would carry over to big datasets, where more general, less handcrafted multi-layer RNNs are an option. ", "title": "Comparison to multi-layer RNNs", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H1qx5p7-l": {"type": "rebuttal", "replyto": "SyvsB0MZe", "comment": "Thank you!", "title": "Re: A couple of questions"}, "SyvsB0MZe": {"type": "rebuttal", "replyto": "SkF549MZg", "comment": "\nThanks for your interests in our paper and your questions!\n\n1) The best model in Lipton et al. (2016) is similar to GRU-simple (masking only) as discussed in the appendix page 12. All the input variables to our model are z-normalized to be 0 mean, so zero-filling and mean-imputation are the same.\nThe AUC score on MIMIC-III mortality prediction is 0.8367 for GRU-simple with masking only and 0.8527 for our GRU-D, and on PhysioNet2012 is 0.8226 for GRU-simple with masking only and 0.8424 for GRU-D.\n\n2) In our experiments, a model with higher AUC usually performs better in terms of other metrics. \nThe average scores of min(Se, +P) in the test folds for GRU-mean, GRU-forward, GRU-sample, and GRU-D are 0.4413, 0.4319, 0.4383, 0.4469.\n\n3) We did not explicitly handle class imbalance in our model. In evaluation, we take stratified k-fold cross validation to make sure each fold has similar percentage of samples for each class.", "title": "Re: A couple of questions"}, "SkF549MZg": {"type": "rebuttal", "replyto": "BJC8LF9ex", "comment": "Thank you for a very interesting work! \n\n\nI have a couple of questions:\n\n1) Lipton at al. (2016) achieve the best result with zero filling and indicators. As I understood from the equation 3 and the following description, you did not experiment with zero filling and used either forward or mean imputation. Is it correct?\n\n2) It will be interesting to see not only AUC but also Sensitivity and Positive Predictivity, as well as min(Se, +P) since that was the official scoring metric in the PhysioNet Challenge 2012 (although we cannot compare these score directly, obviously)\n\n3) How did you combat such high class imbalance in case of mortality prediction?\n", "title": "A couple of questions"}}}