{"paper": {"title": "Joint Multimodal Learning with Deep Generative Models", "authors": ["Masahiro Suzuki", "Kotaro Nakayama", "Yutaka Matsuo"], "authorids": ["masa@weblab.t.u-tokyo.ac.jp", "k-nakayama@weblab.t.u-tokyo.ac.jp", "matsuo@weblab.t.u-tokyo.ac.jp"], "summary": "", "abstract": "We investigate deep generative models that can exchange multiple modalities bi-directionally, e.g., generating images from corresponding texts and vice versa. Recently, some studies handle multiple modalities on deep generative models, such as variational autoencoders (VAEs). However, these models typically assume that modalities are forced to have a conditioned relation, i.e., we can only generate modalities in one direction. To achieve our objective, we should extract a joint representation that captures high-level concepts among all modalities and through which we can exchange them bi-directionally. As described herein, we propose a joint multimodal variational autoencoder (JMVAE), in which all modalities are independently conditioned on joint representation. In other words, it models a joint distribution of modalities. Furthermore, to be able to generate missing modalities from the remaining modalities properly, we develop an additional method, JMVAE-kl, that is trained by reducing the divergence between JMVAE's encoder and prepared networks of respective modalities. Our experiments show that our proposed method can obtain appropriate joint representation from multiple modalities and that it can generate and reconstruct them more properly than conventional VAEs. We further demonstrate that JMVAE can generate multiple modalities bi-directionally.\n", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper addresses the importance task of learning generative models of multiple modalities. There are two concerns about the paper: limited novelty, which will not have sufficient impact; ineffectiveness of evaluation. The paper extends VAEs in an interesting way, but this extension on its own does provide sufficient new insight understanding. And the log-likelihood evaluations and data sets are not enough to be convincing. As a result, the paper is not yet ready for acceptance at the conference."}, "review": {"rkP46kA7x": {"type": "rebuttal", "replyto": "BJiH4Ey7x", "comment": "Thank you for informative comments and sorry for my late reply.\n\nAs you say,  p(z | w) and p(z | x) are not strictly included in the proposed generative model since logp (x | w) + logp (w | x) and JMVAE do not have the same generative model.\n\nHowever, what we would like to claim here is that if the probability distributions are parameterized with neural networks, both maximization of logp (x | w) + logp (w | x) and maximization of JMVAE-kl have the same meaning as optimizing the same network as well, because both parameterized p(z|w;\u03b8) and q(z|w; \u03b8) can be expressed in the same network. \n\nAnyway, the way of writing the current proof is not very correct, so we will fix it as soon as possible.\n\nThanks,\n\nMasahiro", "title": "Question on Equation (5)"}, "BJiH4Ey7x": {"type": "review", "replyto": "Hk8rlUqge", "review": "I am not entirely sure whether the proof in Appendix A is correct. Especially, in the first equality in Equation (5), authors replace p(z|w) into q(z|w) (and similarly for p(z|x) and q(z|x)), but this doesn't seem correct since p(z|w) and p(z|x) are posteriors, not proposal distribution under a proposed generative model p(x|z)p(w|z)p(z).The paper introduces the joint multimodal variational autoencoder, a directed graphical model for modeling multimodal data with latent variable. the model is rather straightforward extension of standard VAE where two data modalities are generated from a shared latent representation independently. In order to deal with missing input modalities or bi-directional inference between two modalities the paper introduces modality-specific encoder that is trained to minimize the KL divergence of latent variable distributions between joint and modality-specific recognition networks. The paper demonstrates its effectiveness on MNIST and CelebA datasets, both in terms of test log-likelihoods and the conditional image generation and editing.\n\nThe proposed method is rather straightforward extension of VAE and therefore the model should inherent the probabilistic inference methods of VAE. For example, for missing data modalities, the model should be able to infer joint representation as well as filling in the missing modalities via iterative sampling as introduced by Rezende et al. (2014). Given marginal improvement, I am not convinced by the contribution of modality-specific encoders in Section 3.3. In addition, the inference methods introduced for generating Figure 5 looks somewhat unprincipled; I am wondering the conditional image generation results by following more principled approach (e.g., iterative sampling). Experimental results on joint image-attribute generation is also missing.", "title": "Question on Equation (5)", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "rJ3O99HEe": {"type": "review", "replyto": "Hk8rlUqge", "review": "I am not entirely sure whether the proof in Appendix A is correct. Especially, in the first equality in Equation (5), authors replace p(z|w) into q(z|w) (and similarly for p(z|x) and q(z|x)), but this doesn't seem correct since p(z|w) and p(z|x) are posteriors, not proposal distribution under a proposed generative model p(x|z)p(w|z)p(z).The paper introduces the joint multimodal variational autoencoder, a directed graphical model for modeling multimodal data with latent variable. the model is rather straightforward extension of standard VAE where two data modalities are generated from a shared latent representation independently. In order to deal with missing input modalities or bi-directional inference between two modalities the paper introduces modality-specific encoder that is trained to minimize the KL divergence of latent variable distributions between joint and modality-specific recognition networks. The paper demonstrates its effectiveness on MNIST and CelebA datasets, both in terms of test log-likelihoods and the conditional image generation and editing.\n\nThe proposed method is rather straightforward extension of VAE and therefore the model should inherent the probabilistic inference methods of VAE. For example, for missing data modalities, the model should be able to infer joint representation as well as filling in the missing modalities via iterative sampling as introduced by Rezende et al. (2014). Given marginal improvement, I am not convinced by the contribution of modality-specific encoders in Section 3.3. In addition, the inference methods introduced for generating Figure 5 looks somewhat unprincipled; I am wondering the conditional image generation results by following more principled approach (e.g., iterative sampling). Experimental results on joint image-attribute generation is also missing.", "title": "Question on Equation (5)", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HkhATBpGx": {"type": "rebuttal", "replyto": "HkYlp79zl", "comment": "Thank you very much for your valuable comments.\n\nAs you say, in settings such as collaborative filtering, it seems that it is common to train sparse data by randomly adding filters to the input at training time. However, the setting we tackle in our paper differs from these settings in the following points.\n\nFirst, an encoder of JMVAE is composed of a concatenation of multiple networks of \"different\" architecture corresponding to each modality (i.e, dense networks for attributes and labels, convolutional networks for images). Second, our objective is to exchange multiple modalities bi-directionally, so all inputs of a certain modality's network are \"completely\" missing at test time. In neural recommender systems, I think that all input layers are at the same level and which part of the input is missing depends on each sample.\n\nI have tried to add a binary mask to the input at training time or to put noise in place of zero value at test time, but we found that these attempts are not very effective when the completely missing input has large dimensions such as images.\n\nThus, we think that JMVAE-kl is effective when the encoder is composed of a concatenation of multiple networks and the input of the network corresponding to a certain modality becomes completely zero.\n\nThanks,\n\nMasahiro", "title": "Masking"}, "HkYlp79zl": {"type": "review", "replyto": "Hk8rlUqge", "review": "Hello, \n\nModeling multimodal data is indeed an interesting problem. However, I fail to understand what motivated you to pick a JMVAE-kl model over, for example, training a VAE with random masking. You could use random binary mask b_x and b_w and then use an encoder q(z|b_x * x, b_w * w, b_x, b_w). This kind of modeling using binary masks as input is common practice in neural recommender systems and seems more principled. Did you try that approach ? How does it compare to the current path you are taking ?This paper proposes an alternative to Conditional Variational Auto-Encoders and Conditional MultiModal Auto-Encoders to perform inference of missing modalities in dataset with multiple modalities. The proposed approach is a Variational Auto-Encoder jointly on all the modalities  with additional KL divergence penalties between the approximate posterior given all the modalities and the approximate posterior given a subset of the modalities. The approach is named Joint Multimodal Variational Auto-Encoder.\nThe authors make a connection between this approach and the Variation of Information. It is unclear why the authors chose the JMVAE approach instead of a more elegant Variation of Information approach.\nAnother unaddressed issue is the scalability of the method. As far as I can tell (given that no code is provided and the specification of the encoder is missing), this approach requires a new encoder per subset of missing modalities. Right now this approach seems to scale since there are only two modalities.\nThe fact that the estimating the log-likelihood log(p(x)) using multiple modalities provide a lower value than with just one in Table 1 is a bit odd. Could you explain that ?\nThe comparison with between the representation learned by JMVAE and CVAE might be unfair given that the representation of CVAE is learned conditionally, on the label in the case of MNIST, and should therefore not consider the label in this representation. Intuitively, this representation could represent \"style\" as shown in (Kingma et al., 2014) in their conditional generation figure.\nFor CelebA, comparing log-likelihood on models that use GANs is probably not significant since GAN does not optimizes log-likelihood. \nOverall this is an interesting problem and there are also interesting ideas worth exploring further, but the execution of the paper requires more work.", "title": "Masking", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H1Sfbi-Nl": {"type": "review", "replyto": "Hk8rlUqge", "review": "Hello, \n\nModeling multimodal data is indeed an interesting problem. However, I fail to understand what motivated you to pick a JMVAE-kl model over, for example, training a VAE with random masking. You could use random binary mask b_x and b_w and then use an encoder q(z|b_x * x, b_w * w, b_x, b_w). This kind of modeling using binary masks as input is common practice in neural recommender systems and seems more principled. Did you try that approach ? How does it compare to the current path you are taking ?This paper proposes an alternative to Conditional Variational Auto-Encoders and Conditional MultiModal Auto-Encoders to perform inference of missing modalities in dataset with multiple modalities. The proposed approach is a Variational Auto-Encoder jointly on all the modalities  with additional KL divergence penalties between the approximate posterior given all the modalities and the approximate posterior given a subset of the modalities. The approach is named Joint Multimodal Variational Auto-Encoder.\nThe authors make a connection between this approach and the Variation of Information. It is unclear why the authors chose the JMVAE approach instead of a more elegant Variation of Information approach.\nAnother unaddressed issue is the scalability of the method. As far as I can tell (given that no code is provided and the specification of the encoder is missing), this approach requires a new encoder per subset of missing modalities. Right now this approach seems to scale since there are only two modalities.\nThe fact that the estimating the log-likelihood log(p(x)) using multiple modalities provide a lower value than with just one in Table 1 is a bit odd. Could you explain that ?\nThe comparison with between the representation learned by JMVAE and CVAE might be unfair given that the representation of CVAE is learned conditionally, on the label in the case of MNIST, and should therefore not consider the label in this representation. Intuitively, this representation could represent \"style\" as shown in (Kingma et al., 2014) in their conditional generation figure.\nFor CelebA, comparing log-likelihood on models that use GANs is probably not significant since GAN does not optimizes log-likelihood. \nOverall this is an interesting problem and there are also interesting ideas worth exploring further, but the execution of the paper requires more work.", "title": "Masking", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Bk11Z8xGl": {"type": "rebuttal", "replyto": "SJQ0eh2-e", "comment": "Hi Diane.\n \nThank you very much for your feedback.\n \nWe didn't cite this Cadena's paper in our paper, but this is an application study with MAE which we cited in our paper.  The main differences between theirs and our method, JMVAE, are as follows:\n \n\u30fbJMVAE is based on VAEs, deep generative models, while MAE used in Cadena's paper is based on AEs,  deep discriminative models.  For this reason, JMVAE can obtain joint representation which captures manifold as well as is suitable for reconstruction.  Therefore, we can generate modalities from corresponding other modalities even if they are varied (such as Fig. 5).\n\n\u30fbBoth of Cadena's and our papers have same goal of trying to generate missing modalities at test time, but we found that they can't be generated properly when each modality has different dimensions (e.g. images and texts) and architectures (e.g. dense and convolutional), which are shown in our results. So we proposed JMVAE-kl and our results showed that it works well even if we use both convolutional and dense networks. In Cadena's paper, they only used dense and relatively shallow networks for each modality and considered that each modality had same dimensions and architectures.\n\nThanks,\n\nMasahiro", "title": "Related work"}, "SJQ0eh2-e": {"type": "rebuttal", "replyto": "Hk8rlUqge", "comment": "Have you considered the following related work : \"Multi-modal Auto-Encoders as Joint Estimators for\nRobotics Scene Understanding\" Cadena et al. ? Could you explain how your method differs from theirs ?\n\nThanks,\n\nDiane", "title": "Related work"}}}