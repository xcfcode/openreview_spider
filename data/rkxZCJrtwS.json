{"paper": {"title": "D3PG: Deep Differentiable Deterministic Policy Gradients", "authors": ["Tao Du", "Yunfei Li", "Jie Xu", "Andrew Spielberg", "Kui Wu", "Daniela Rus", "Wojciech Matusik"], "authorids": ["taodu@csail.mit.edu", "l-yf16@mails.tsinghua.edu.cn", "jiex@csail.mit.edu", "aespielberg@csail.mit.edu", "walker.kui.wu@gmail.com", "rus@csail.mit.edu", "wojciech@csail.mit.edu"], "summary": "We propose a novel method that leverages the gradients from differentiable simulators to improve the performance of RL for robotics control", "abstract": "Over the last decade, two competing control strategies have emerged for solving complex control tasks with high efficacy. Model-based control algorithms, such as model-predictive control (MPC) and trajectory optimization, peer into the gradients of underlying system dynamics in order to solve control tasks with high sample efficiency.  However, like all gradient-based numerical optimization methods,model-based control methods are sensitive to intializations and are prone to becoming trapped in local minima. Deep reinforcement learning (DRL), on the other hand, can somewhat alleviate these issues by exploring the solution space through sampling \u2014 at the expense of computational cost. In this paper, we present a hybrid method that combines the best aspects of gradient-based methods and DRL. We base our algorithm on the deep deterministic policy gradients (DDPG) algorithm and propose a simple modification that uses true gradients from a differentiable physical simulator to increase the convergence rate of both the actor and the critic.  We demonstrate our algorithm on seven 2D robot control tasks, with the most complex one being a differentiable half cheetah with hard contact constraints. Empirical results show that our method boosts the performance of DDPGwithout sacrificing its robustness to local minima.", "keywords": ["differentiable simulator", "model-based control", "policy gradients"]}, "meta": {"decision": "Reject", "comment": "This paper proposes a hybrid RL algorithm that uses model based gradients from a differentiable simulator to accelerate learning of a model-free policy.  While the method seems sound, the reviewers raised concerns about the experimental evaluation, particularly lack of comparisons to prior works, and that the experiments do not show a clear improvement over the base algorithms that do not make use of the differentiable dynamics. I recommend rejecting this paper, since it is not obvious from the results that the increased complexity of the method can be justified by a better performance, particularly since the method requires access to a simulator, which is not available for real world experiments where sample complexity matters more."}, "review": {"SJek1Z91cS": {"type": "review", "replyto": "rkxZCJrtwS", "review": "This paper studies optimal control problems where a physical simulator of the system is available, which outputs the gradient of the dynamics. Using the gradients proposed by the model, the authors propose to add two additional terms in the loss function for critic training in DDPG, where these to terms corresponding to the prediction error of $\\nabla_{a} Q(s,a)$ and $\\nabla_b Q(s,a)$, respectively. However, my main concern is that the form of gradient given in equation (2) might contains an error.\n\n1. Equation (2). Note that in DDPG, the action is given by a deterministic policy. Thus, we have $a_t = \\pi(s_t)$ for all $t\\geq 0$. For critic estimation, it seems you are basing on the Bellman equation \n$ Q(s,a) = r(s,a) + Q(s', \\pi(s'))$, where $s'$ is the next state following $(s,a)$. Then, it seems that Equation (2) is obtained by taking gradient with respect to $(s,a)$. However, I cannot understand what $\\nabla_{\\pi} Q$ stands for. If it is $\\nabla_a Q(s_{i+1}, a_{i+1}) \\cdot \\nabla_s \\pi(s_{i+1}) $, then that makes sense. \n\n2. Based on the experiments, it seems that the proposed method does not always outperform MPC or DDPG, even in a small-scale control problem Mountaincar. Moreover, it seems that the performance is similar to that of the DDPG. \n\n3. Here the model-based gradient in equation (2) is defined by only unroll one-step forward by going from $s_i, a_i$ to $s_{i+1}$. It would be interesting to see how the number of unroll steps affect the algorithm, which is a gradient version of TD($\\lambda$).\n\n4. Missing reference: Differential Temporal Difference Learning https://arxiv.org/abs/1812.11137", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 4}, "SJgwyeiooH": {"type": "rebuttal", "replyto": "rkxZCJrtwS", "comment": "We thank all reviewers again for their feedback. We have uploaded a new manuscript based on the review and we summarized our updates below:\n\n1. We added clarification to the notations in Equation (2) and updated the computational graph (Figure 1) to better explain the computation of our gradients.\n2. We reported experimental results in the Appendix (Section A.1) about using cosine similarity instead of L2 norm in all examples. These experiments showed that our method is not sensitive to the choice between these two norms, and both norms are viable options for our examples.\n3. We implemented our proposed method in another actor-critic method (SAC) and reported the experimental results in five of our examples. We use these experiments to demonstrate that it is possible to apply our method to other actor-critic methods besides DDPG. \n\nWe hope these updates can help articulate the benefits of incorporating gradient information in RL training whenever a differentiable simulator is available. Please feel free to leave more comments and thank you again for your review!", "title": "Paper update"}, "rklgox5jiS": {"type": "rebuttal", "replyto": "BJxm08CJcr", "comment": "Thank you for your constructive review!\n\n== New actor-critic methods ==\nWe agree that TD3 and SAC are good candidates to try besides DDPG. We have implemented a variant of SAC and reported the results in five examples in Section A.2 of the updated manuscript. Our experiments showed that the proposed method helped improve the performance of the original SAC in three examples and obtained similar performance in the other two.\n\n== Q function approximation ==\nThe Q network does not fit the ground-truth closely because 1) The RL algorithm only explored and used a very small part of the whole domain of ($s$, $a$) for fitting. Specifically, since samples were extracted from perturbing $\\pi$, most of them clustered around the curve $(s, \\pi(s))$; 2) Due to the design of this problem, regions far away from the initial ($s=-0.5$) and final ($s=0$) positions of the mass point are rarely visited during training and not needed in the final solution. Due to these two reasons, the Q network attempted to fit the ground-truth Q well only in the banded region between $s=-0.5$ and $s=0$, and it can be observed that adding weighted loss on gradient differences helped the Q network converge to the ground-truth in this banded area faster.\n\n== Slower convergence when both weights are available ==\nDue to the empirical nature of our method, we are not able to justify this phenomenon on a theoretical basis. We suspect it might be related to the fact that the Q function in this example has different sensitivity to its two inputs $s$ and $a$. In particular, if we slice the ground-truth Q surface at a given $s$, the resulting $Q-a$ curve is very flat, so more gradient information about $\\partial Q/\\partial a$ might be unnecessary and not helpful for fitting it well.\n", "title": "Experimenting with SAC and interpreting results from the motivating example"}, "B1eIgYQjsr": {"type": "rebuttal", "replyto": "SJek1Z91cS", "comment": "We agree that using the gradients from TD($\\lambda$) in Equation (2) is an interesting direction to explore. However, unrolling more steps to estimate $\\hat{Q}_i$ requires more on-policy samples: for example, unrolling one more step in line 9 of algorithm 1 would require access to $s_{i+2}$ computed by simulating the robot from $(s_{i+1}, \\pi\u2019(s_{i+1}))$. These new samples are not directly available from the off-policy replay buffer in DDPG and have to be regenerated on the fly, which hurts the sampling efficiency of the algorithm.\n\nWe did think about applying the same technique to on-policy RL algorithms and have implemented the TD($\\lambda$) version of equation (2) in PPO. Our preliminary results showed that it did not improve the performance of PPO even after hyperparameter tuning. We suspect the reason is that Equation (2) assumes the policy is deterministic in nature while PPO uses stochastic policies. Still, we think it is possible that a proper combination of TD($\\lambda$) gradients and RL baseline algorithms could lead to an improvement in performance.\n", "title": "Using a gradient version of TD($\\lambda$)"}, "B1eFQ9Lcor": {"type": "rebuttal", "replyto": "rJxqEfvvcS", "comment": "Thank you for your constructive feedback!\n\nThank you for sharing Fig. 1 in \"Gradient Estimators for Implicit Models\". We agree that such an example would highlight the shortcomings of estimating only the Q function and the benefit of training with simulation gradients whenever they are available. We will consider including this argument in a stronger motivating example and adding a similar figure in the manuscript.\n\nWe agree comparing other norms is an interesting idea and norms like L1 and cosine would both be interesting to try.  For now, we have tested the cosine norm on all of our examples.  For some of our examples (the Acrobot and MountainCar), we found that the cosine norm dominates the L2 norm.  For the CartPoleSwingUp, the L2 norm still dominates.  For the remaining problems, both norms work approximately equally well. We will include these quantitative results in the revised manuscript. We stress that in all cases, both regularization variants achieve performance similar to or better than pure DDPG.  It is difficult to give a precise theoretical reason as to why one norm outperforms the other for certain problems, however, we can gladly report extensive empirical findings in a final version of the manuscript.\n\nSimulator gradients are unfortunately difficult to use to directly improve exploration since they always point in a greedy direction.  In the classic exploration/exploitation tradeoff, the gradient provides exploitation.  It is possible that one could devise an algorithm that may improve exploration by sampling updates which deviate from the deterministic gradient (e.g. a gradient-based variant of https://arxiv.org/pdf/1706.01905.pdf).  However, such an algorithm could be tried with or without gradient fitting.  It is possible that gradient-fitting would improve the efficacy of such a technique, but this is all introducing a new, potentially complex algorithm, worthy of its own manuscript and study.", "title": "Experimental results on cosine similarity and thoughts on using gradients for exploration"}, "rJgVDqggoB": {"type": "rebuttal", "replyto": "SJek1Z91cS", "comment": "Thank you for your constructive review!\n\nWe really appreciate your comments and are happy to discuss them during this rebuttal period. But for now, we just want to make a quick clarification on Equation (2) and justify our gradient computation:\n\nYou are correct that we base our computation on the Bellman equation. To be precise, we use $\\hat{Q}(s_i,a_i)=r(s_i,a_i,s_{i+1})+\\gamma Q'(s_{i+1},\\pi'(s_{i+1}))$. This is line 9 in Algorithm 1 in our paper, and it is also consistent with line 12 in Algorithm 1 in the original DDPG paper.\n\nFor brevity, in Equation (2) we use the neural network names to refer to its output value. So $\\pi'$ in Equation (2) stands for $\\pi'(s_{i+1})$ and $\\nabla_{\\pi'}Q'$ in Equation (2) stands for:\n$$\n\\nabla_aQ'(s,a)|_{s=s_{i+1},a=\\pi'(s_{i+1})}\n$$\nYou can also check the correctness of Equation (2) by comparing it to the computation graph in Figure 1, where the upper right $\\mu$ stands for $\\pi'(s_{i+1})$. In Figure 1, $\\nabla_{\\pi'}Q'$ corresponds to the gradient back-propagated along the arrow $\\mu\\rightarrow Q'$ (\"if we change $\\mu$, how much will $Q'$ change?\"). Similarly, the term $\\nabla_{s_{i+1}}\\pi'$ after $\\nabla_{\\pi'}Q'$ in Equation (2) corresponds to the arrow $s_{i+1}\\rightarrow\\mu$ (\"if we change $s_{i+1}$, how much will $\\mu$ change?\").\n\nPutting them together, the product $\\nabla_{\\pi'}Q'\\cdot \\nabla_{s_{i+1}}\\pi'$ in Equation (2) back-propagates the gradient along the path $s_{i+1}\\rightarrow\\mu\\rightarrow Q'$ in Figure 1. Similarly, the term $\\nabla_{s_{i+1}}Q'$ in Equation (2) corresponds to the arrow $s_{i+1}\\rightarrow Q'$ in Figure 1, and the sum $(\\nabla_{s_{i+1}}Q'+\\nabla_{\\pi'}Q'\\cdot \\nabla_{s_{i+1}}\\pi')$ computes the total derivative of $Q'$ with respect to $s_{i+1}$. The other terms in Equation (2) can be verified in the same way.\n\nWe hope this explanation can clear your concern with the correctness of Equation (2). We will revise the manuscript to clarify the notations.", "title": "Clarifications on the correctness of Equation (2)"}, "BJxm08CJcr": {"type": "review", "replyto": "rkxZCJrtwS", "review": "This paper shows how the derivatives from a differentiable\nenvironment can be used to improve the convergence rate of\nthe actor and critic in DDPG.\nThis is useful information to use as most physics simulators\nhave derivative information available that would be useful\nto leverage when training models.\nThe empirical results show that their method of adding\nthis information (D3PG) slightly improves DDPG's\nperformance in the tasks they consider.\nAs the contribution of this work is empirical is nature,\nI think a very promising future direction fo work is to\nadd derivative information to and evaluate similar\nvariants of some of the newer actor-critic methods\nsuch as TD3 and SAC.\n\nI have two minor questions:\n1) Figure 2(a) shows the convenrgence of regularizing states,\n   actions, and both states and actions and the text\n   describing the figure states that this is\n   \"expected to boost the convergence of Q.\"\n   However the figure shows that regularizing both states and\n   actions results in a slower convergence than doing\n   them separately. Why is this?\n2) How should I interpret the visualization of the\n   learned Q surface in Figure 2(f) in comparison to\n   the true Q function in Figure 2(g)?\n   It does not look like a good approximation.", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}, "rJxqEfvvcS": {"type": "review", "replyto": "rkxZCJrtwS", "review": "==Summary==\n\nDDPG is a popular RL method for continuous control problems. It is more widely applicable than traditional model-based approaches like MPC, since it doesn't require differentiable models of the dynamics. However, in many environments, dynamics are differentiable. This paper proposes a method for extending DDPG to exploit simulator gradients. In particular, the Bellman error objective (which is defined in terms of critic values) used for training the critic is augmented with additional terms defined in terms of gradients of the critic. This leads to faster convergence in practice on a range of benchmarks.\n\n==Overall Assessment==\n\nI recommend acceptance. The paper's contribution is well-motivated, works reasonably well, and is relatively easy to implement.\n\n==Comments==\n\nIt would be good to add an argument explaining to readers that accurately estimating Q using Q_\\phi does not mean that the gradients of Q_\\phi will be good approximations of the true gradients of Q. I found Fig 1 of arxiv.org/pdf/1705.07107.pdf informative.\n\nCan you justify the choice of euclidean norm in line 10? In terms of the critic helping teach the actor, the direction of the gradient may be more important than the norm. What if you used cosine sim?\n\nYou argue that DRL is better than MPC because DRL explores better. Could you use the simulator gradients somehow to improve exploration?\n", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 2}}}