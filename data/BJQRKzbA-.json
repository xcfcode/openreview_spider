{"paper": {"title": "Hierarchical Representations for Efficient Architecture Search", "authors": ["Hanxiao Liu", "Karen Simonyan", "Oriol Vinyals", "Chrisantha Fernando", "Koray Kavukcuoglu"], "authorids": ["hanxiaol@cs.cmu.edu", "simonyan@google.com", "vinyals@google.com", "chrisantha@google.com", "korayk@google.com"], "summary": "In this paper we propose a hierarchical architecture representation in which doing random or evolutionary architecture search yields highly competitive results using fewer computational resources than the prior art.", "abstract": "We explore efficient neural architecture search methods and show that a simple yet powerful evolutionary algorithm can discover new architectures with excellent performance. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6% on CIFAR-10 and 20.3% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches. We also present results using random search, achieving 0.3% less top-1 accuracy on CIFAR-10 and 0.1% less on ImageNet whilst reducing the search time from 36 hours down to 1 hour.", "keywords": ["deep learning", "architecture search"]}, "meta": {"decision": "Accept (Poster)", "comment": "PROS:\n1. Overall, the paper is well-written, clear in its exposition and technically sound.\n2. With some caveats, an independent team concluded that the results were \"largely reproducible\"\n3. The key idea is a smart evolution scheme. It circumvents the traditional tradeoff between search space size and complexity of the found models.\n4. The implementation seems technically sound.\n\nCONS:\n1. The results were a bit over-stated (the authors promise to correct)\n2. Could benefit from more comparison with other approaches (e.g. RL)"}, "review": {"BJNK-sdxz": {"type": "review", "replyto": "BJQRKzbA-", "review": "The fundamental contribution of the article is the explicit use of compositionality in the definition of the search space. Instead of merely defining an architecture as a Directed Acyclic Graph (DAG), with nodes corresponding to feature maps and edges to primitive operations, the approach in this paper introduces a hierarchy of architectures of this form. Each level of the hierarchy utilises the existing architectures in the preceding level as candidate operations to be applied in the edges of the DAG. As a result, this would allow the evolutionary search algorithm to design modules which might be then reused in different edges of the DAG corresponding to the final architecture, which is located at the top level in the hierarchy.\n\nManually designing novel neural architectures is a laborious, time-consuming process. Therefore, exploring new approaches to automatise this task is a problem of great relevance for the field. \n\nOverall, the paper is well-written, clear in its exposition and technically sound. While some hyperparameter and design choices could perhaps have been justified in greater detail, the paper is mostly self-contained and provides enough information to be reproducible. \n\nThe fundamental contribution of this article, when put into the context of the many recent publications on the topic of automatic neural architecture search, is the introduction of a hierarchy of architectures as a way to build the search space. Compared to existing work, this approach should emphasise modularity, making it easier for the evolutionary search algorithm to discover architectures that extensively reuse simpler blocks as part of the model. Exploiting compositionality in model design is not novel per se (e.g. [1,2]), but it is to the best of my knowledge the first explicit application of this idea in neural architecture search. \n\nNevertheless, while the idea behind the proposed approach is definitely interesting, I believe that the experimental results do not provide sufficiently compelling evidence that the resulting method substantially outperforms the non-hierarchical, flat representation of architectures used in other publications. In particular, the results highlighted in Figure 3 and Table 1 seem to indicate that the difference in performance between both paradigms is rather small. Moreover, the performance gap between the flat and hierarchical representations of the search space, as reported in Table 1, remains smaller than the performance gap between the best performing of the approaches proposed in this article and NASNet-A (Zoph et al., 2017), as reported in Tables 2 and 3.\n\nAnother concern I have is regarding the definition of the mutation operators in Section 3.1. While not explicitly stated, I assume that all sampling steps are performed uniformly at random (otherwise please clarify it). If that was indeed the case, there is a systematic asymmetry between the probability to add and remove an edge, making the former considerably more likely. This could bias the architectures towards fully-connected DAGs, as indeed seems to occur based on the motifs reported in Appendix A.\n\nFinally, while the main motivation behind neural architecture search is to automatise the design of new models, the approach here presented introduces a non-negligible number of hyperparameters that could potentially have a considerable impact and need to be selected somehow. This includes, for instance, the number of levels in the hierarchy (L), the number of motifs at each level in the hierarchy (M_l), the number of nodes in each graph at each level in the hierarchy (| G^{(l)} |), as well as the set of primitive operations. I believe the paper would be substantially strengthened if the authors explored how robust the resulting approach is with respect to perturbations of these hyperparameters, and/or provided users with a principled approach to select reasonable values.\n\nReferences:\n\n[1] Grosse, Roger, et al. \"Exploiting compositionality to explore a large space of model structures.\" UAI (2012).\n[2] Duvenaud, David, et al. \"Structure discovery in nonparametric regression through compositional kernel search.\" ICML (2013).\n", "title": "In this paper, the authors propose a novel evolutionary algorithm for neural architecture search. ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "SkbQs_cgz": {"type": "review", "replyto": "BJQRKzbA-", "review": "This work fits well into a growing body of research concerning the encoding of network topologies and training of topology via evolution or RL. The experimentation and basic results are probably sufficient for acceptance, but to this reviewer, the paper spins the actual experiments and results a too strongly.\n\nThe biggest two nitpicks:\n\n> In our work we pursue an alternative approach: instead of restricting the search space directly, we allow the architectures to have flexible network topologies (arbitrary directed acyclic graphs)\n\nThis is a gross overstatement. The architectures considered in this paper are heavily restricted to be a stack of cells of uniform content interspersed with specifically and manually designed convolution, separable convolution, and pooling layers. Only the topology of the cells themselves are designed. The work is still great, but this misleading statement in the beginning of the paper left the rest of the paper with a dishonest aftertaste. As an exercise to the authors, count the hyperparameters used just to set up the learning problem in this paper and compare them to those used in describing the entire VGG-16 network. It seems fewer hyperparameters are needed to describe VGG-16, making this paper hardly an alternative to the \"[common solution] to restrict the search space to reduce complexity and increase efficiency of architecture search.\"\n\n> Table 1\n\nWhy is the second best method on CIFAR (\u201cHier. repr-n, random search (7000 samples)\u201d) never tested on ImageNet? The omission is conspicuous. Just test it and report.\n\nSmaller nitpicks:\n\n> \u201cNew state of the art for evolutionary strategies on this task\u201d\n\n\u201cEvolutionary Strategies\u201d, at least as used in Salimans 2017, has a specific connotation of estimating and then following a gradient using random perturbations which this paper does not do. It may be more clear to change this phrase to \u201cevolutionary methods\u201d or similar.\n\n> Our evolution algorithm is similar but more generic than the binary tournament selection (K = 2) used in a recent large-scale evolutionary method (Real et al., 2017).\n\nA K=5% tournament does not seem more generic than a binary K=2 tournament. They\u2019re just different.", "title": "Great results, needlessly overstated.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkM52nagG": {"type": "review", "replyto": "BJQRKzbA-", "review": "The authors present a novel evolution scheme applied to neural network architecture search. It relies on defining an expressive search space for conducting optimization, with a constrained search space that leads to a lighter and more efficient algorithm. To balance these constraints, they grow sub-modules in a hierarchical way to form more and more complex cells. Hence, each level is limited to a small search space while the system as a whole converges toward a complex structure. To speed up the search, they focus on finding cells instead of an entire network. In evaluation time, they insert these cells between layers of a network comparable in size to known networks. They find complex cells that lead to state-of-the-art performance on benchmark dataset CIFAR-10 and ImageNet. They also claim that their method is reaching a new milestone in evolutionary search strategies performance.\n\nThe method proposed for an hierarchical representation for optimizing over neural network designs is well thought and sound. It could lead to new insight on automating design of neural networks for given problems. In addition, the authors present results that appear to be on par with the state-of-the-art with architecture search on CIFAR-10 and ImageNet benchmark datasets. The paper presents a good work and is well articulated. However, it could benefit from additional details and a deeper analysis of the results.\n\nThe key idea is a smart evolution scheme. It circumvents the traditional tradeoff between search space size and complexity of the found models. The method is also appealing for its use of some kind of emergence between two levels of hierarchy. In fact, it could be argued that nature tends to exploit the same phenomenon when building more and more complex molecules. Thought, the paper could benefit from a more detailed analysis of the architectures found by the algorithm. Do the modules always become more complex as they jump from a level to another or there is some kind of inter-level redundancy? Are the cells found interpretable? The authors should try to give their opinion about the design obtained.\n\nThe implementation seems technically sound. The experiments and results section shows that the authors are confident and the evaluation seems correct. However, paragraphs on the architectures could be a bit clearer for the reader. The diagram could be more complete and reflect better the description. During evaluation, what is a step? A batch or an epoch or other?\n\nThe method seems relatively efficient as it took 36 hours to converge in a field traditionally considered as heavy in terms of computation, but at the requirement of using 200 GPU. It raises questions on the usability of the method for small labs. At some point, we will have to use insights from this search to stop early, when no improvement is expected. Also, authors claim that their method consume less computation time than reinforcement learning. This should be supported by some quantitative results.\n\nThe paper would greatly benefit from a deeper comparison over other techniques. For instance, it could describe more the advantages over reinforcement learning. An important contribution is to show that a well-defined architecture representation could lead to efficient cells with a simple randomized search. It could have taken more spaces in the paper.\n\nI am also concerned the computational efficiency of the results obtained with this method on current processors. Indeed, the randomness of the found cells could be less efficient in terms of computation that what we can get from a well-structured network designed by hand. Exploiting the structure of the GPUs (cache size, sequential accesses, etc.) allows to get best possible performance from the hardware at hand. Does the solution obtained with the optimization can be run as efficiently? A short analysis forward pass time of optimized cells vs. popular models could be an interesting addition to the paper. This is a general comment over this kind of approach, but I think it should be addressed. \n", "title": "Good paper on searching space of network design", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJ9nV7amG": {"type": "rebuttal", "replyto": "H1yO14fzz", "comment": "Thank you for taking the effort to implement our algorithm. Your detailed comments are valuable for us to improve the paper further. We provide the clarifications below, which would hopefully be useful for reproducing our results.\n\n* \u201cit would be impossible for the depthwise convolution operation to actually have a constant number of output filters as described.\u201d\nWe did not require depthwise convolution operations to have a constant number of output filters (we\u2019ll remove \u201cof C channels\u201d in the the 2nd bullet point in Sect. 2.3, which was a typo). This is not an issue because the way that we merge the nodes (depthwise concatenation) does not require the input nodes to have the same number of filters.\n\n* \u201cseparable convolution would no longer be valid as a primitive as it could be produced by a stacked depthwise and 1x1 convolution\u201d\nIn our case, each convolutional operation comes with batch normalization (BN) and ReLU, hence the separable convolution\n3x3_depthwise_conv->1x1_conv->BN->ReLU\nis not exactly the same as the stack of \n3x3_depthwise_conv->BN->ReLU and 1x1_conv->BN->ReLU.\nWe also note that in general the algorithm remains valid if one primitive operation can be expressed in terms of the others.\n\n* \u201cthe initialization routine seems to imply that the identity operation is available at every level\u201d\nNo, the identity operation is only available at the motif level: a motif is initialised as a chain of identity operations, and a cell is initialised as a chain of motifs (note that a chain of identity chains is also an identity chain).\n\n* \u201cThe probability distributions used for random sampling in mutation are not given\u201d\nWe always use uniform distributions in all of our experiments. That being said, no hyperparameters were involved or tuned for mutation operations.\n\n* About the number of mutations during initialization.\nWe would like to point out that a large number of mutations is necessary to produce a diverse initial population of architectures. In our case we used 1000.", "title": "Clarifications for reproducing the results"}, "r1mfNm6mG": {"type": "rebuttal", "replyto": "BJQRKzbA-", "comment": "We thank all reviewers for their comments. We will incorporate the suggested revisions into the new version of the paper. Our responses below focus on the major points.\n\n* About comparing computation time with RL-based approaches (reviewer 1)\nOur approach is faster than some published RL-based methods (e.g. 2000 GPU days in Zoph et al. (2017) vs 300 GPU days in our case). Having said that, we do not claim that evolution is more efficient than RL-based approaches in general.\n\n* Efficiency of architectures found using architecture search (reviewer 1)\nIn terms of the number of parameters, our ImageNet model is comparable to Inception-Resnet-v2 but larger than NASNet-A. Although identifying fast/compact architectures was not the primary focus of this work, an interesting future direction is to include FLOPS or wall clock time as a part of the evolution fitness, letting the architecture search algorithm to discover architectures that are both accurate and computationally efficient.\n\n* \u201cDuring evaluation, what is a step?\u201d (reviewer 1)\nAn evolution step refers to training and evaluation of a single architecture. We will make the definition more explicit in the revised paper.\n\n* \u201cThe authors should try to give their opinion about the design obtained\u201d (reviewer 1)\nOur visualisation in appendix A shows that architecture search discovers a number of skip connections. For example, the cell contains a direct skip connection between input and output: nodes 1 and 5 are connected by Motif 4, which in turn contains a direct connection between input and output. The cell also contains several internal skip connections, through Motif 5 (which again comes with an input-to-output skip connection similar to Motif 4).\n\n* \u201cthe paper spins the actual experiments and results a too strongly.\u201d (reviewers 2 and 3)\nThank you for the suggested improvements. We will revise our writing and soften the claims.\n\n* Missing ImageNet results for certain methods in Table 1 (reviewer 3)\nImageNet experiments under those two settings were still running at the time of the submission deadline. Their results are as follows:\nFlat repr-n, parameter-constrained, evolution (7000 samples): 21.2 / 5.8\nHier. repr-n, random search (7000 samples): 21.0 / 5.5.\nThe latter result is due to the fact that the evolution fitness computed on CIFAR is a proxy for ImageNet performance. Computationally efficient architecture search directly on ImageNet is an interesting direction for future research.\n\n* Mutation is biased towards adding edges (reviewer 2)\nIndeed, in our implementation we don\u2019t ensure an equal probability of adding and deleting edges. We think inferring the mutation bias along with evolution is an interesting direction for future work.\n\n* Regarding a large number of hyperparameters specifying the architecture (reviewers 2 and 3)\nWe note that some hyperparameters can be adaptively tuned by evolution. Namely, M_l and |G^{(l)}| affect only the upper bounds on effective hyperparameters, since the algorithm may learn to not use a particular motif (hence the effective number of motifs becomes smaller than M_l), or to shortcut two nodes using an identity op (hence the effective number of nodes becomes smaller than |G^{(l)}|). Both behaviors have been empirically observed in our visualization (see Figure 5 & Figure 10 in Appendix A).", "title": "Responses to Reviewers"}}}