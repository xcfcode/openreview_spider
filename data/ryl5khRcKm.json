{"paper": {"title": "Human-level Protein Localization with Convolutional Neural Networks", "authors": ["Elisabeth Rumetshofer", "Markus Hofmarcher", "Clemens R\u00f6hrl", "Sepp Hochreiter", "G\u00fcnter Klambauer"], "authorids": ["rumetshofer@ml.jku.at", "hofmarcher@ml.jku.at", "clemens.roehrl@meduniwien.ac.at", "hochreit@ml.jku.at", "klambauer@ml.jku.at"], "summary": "", "abstract": "Localizing a specific protein in a human cell is essential for understanding cellular functions and biological processes of underlying diseases. A promising, low-cost,and time-efficient biotechnology for localizing proteins is high-throughput fluorescence microscopy imaging (HTI). This imaging technique stains the protein of interest in a cell with fluorescent antibodies and subsequently takes a microscopic image.  Together with images of other stained proteins or cell organelles and the annotation by the Human Protein Atlas project, these images provide a rich source of information on the protein location which can be utilized by computational methods.  It is yet unclear how precise such methods are and whether they can compete with human experts.   We here focus on deep learning image analysis methods and, in particular, on Convolutional Neural Networks (CNNs)since they showed overwhelming success across different imaging tasks. We pro-pose a novel CNN architecture \u201cGapNet-PL\u201d that has been designed to tackle the characteristics of HTI data and uses global averages of filters at different abstraction levels.   We present the largest comparison of CNN architectures including GapNet-PL for protein localization in HTI images of human cells.  GapNet-PL outperforms all other competing methods and reaches close to perfect localization in all 13 tasks with an average AUC of 98% and F1 score of 78%.  On a separate test set the performance of GapNet-PL was compared with three human experts and 25 scholars. GapNet-PL achieved an accuracy of 91%, significantly (p-value 1.1e\u22126) outperforming the best human expert with an accuracy of 72%.", "keywords": ["Convolutional Neural Networks", "High-resolution images", "Multiple-Instance Learning", "Microscopy Imaging", "Protein Localization"]}, "meta": {"decision": "Accept (Poster)", "comment": "The reviewers all agreed that the problem application is interesting, and that there is little new methodology, but disagreed as to how that should translate into a score. The highest rating seemed to heavily weight the importance of the method to biological application, whereas the lowest rating heavily weighted the lack of technical novelty. However, because the ICLR call for papers clearly calls out applications in biology, and all reviewers agreed on its strength in that regard, and it was well-written and executed, I would recommend it for acceptance."}, "review": {"SJlJ53Vqnm": {"type": "review", "replyto": "ryl5khRcKm", "review": "This paper designed a GapNet-PL architecture and applied GapNet-PL, DenseNet, Multi-scale CNN etc. to the protein image (multi-labels) classification dataset.\n\nPros:\n\n1. The proposed method has a good performance on the given task. Compared to the claimed baselines (Liimatainen et al. and human experts), the proposed architecture shows a much higher performance.\n\nCons:\n\n1. The novelty of the proposed architecture is limited. The main contribution of this work is the application of CNN-based methods to the specific biological images.\n\n2. The existing technical challenge of this task is not significant and the motivation of the proposed method could be hardly found in this paper. \n\n3. The baselines are not convincing enough. Since the performance of Liimatainen et al. is calculated on a different test dataset, the results here are not comparable. The prediction from a human expert, which may vary from individuals, fails to provide a confident performance comparison.\n\n4. Compared to the existing models (DenseNet, Multi-scale CNN etc.), the performance improvement of the proposed model is limited.", "title": "This is an application oriented paper with little technical contribution", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkxjXg6tCX": {"type": "rebuttal", "replyto": "SylHrr7cnQ", "comment": "We thank the reviewer for his encouraging statement about our work.We believe that the results are also relevant for the machine learning community because our architecture tackles the problem of weakly labeled data different than other methods before. \n\nAd relevance for deep learning\n\nOur contribution on the Deep Learning side is on the problem of dealing with weakly annotated data. Instead of single instances that are labeled, a set of instances is labeled. A machine learning method has to collect and combine hints from the instances for accurate predictions. Many hints across the image have to be collected in order to be able to correctly classify. This is rather more similar to sentiment detection than to object recognition (MNIST, CIFAR, ImageNet), in which typically a single instance has to be detected. We think that this problem is occurring in a number of different situations, such that researchers in other machine learning fields might find the GapNet-PL technique highly relevant to their problem, as well.\n\nAd comparison with human experts\n\nAs the reviewer was skeptical about comparisons with humans, so were the authors. Thus, we have taken efforts to improve the estimates of human performance and of expert performance: We recruited two more human experts to improve the performance estimate of human experts. We also tested 25 graduate and undergraduate students, we call scholars, with life science background and specific training at this task. Thus, we can now also provide a reliable estimate for knowledgeable, non-expert human performance.  \nWe adapted the respective paragraphs including these new results that provide a better view on human performance at this task.\n\nOther remarks and questions:\n\n1.) Dataset of Liimatainen et al.\n\nThey report the performance on the Challenge test set that has never been released. Therefore, we had to compare the methods on a different test set. However, we have now re-implemented the method of Liimatainen et al. and compared it on our test set, where it had a similar performance (F1 score of 0.50 on our test set and 0.51 on the challenge test set). Hence, all performance metrics are estimated on the same test set and are comparable. \n\n2.) DenseNet\n\nDue to computational constraints, the computations for DenseNet were restricted to a single GPU on which we could only fit this variant. We hypothesize that there could be small improvement of performance with larger variants.\n\n3.) Random crops\n\nIn practice, we did not observe that an empty crop is propagated through the network, which empirically confirmed by testing around 100,000 random crops. However, we agree with the reviewer about the labeling problem: the data set can in general be considered as weakly annotated (see answer above) because the whole image and not the individual instances, i.e. cells, are labeled. \n\n4.) AUC as performance measure\n\nThe ranking is important in cases, where multiple proteins are tested and the research team wants to perform subsequent test of all proteins that are only located in, say, the nucleus. In this case they would start with the highest ranked protein and test the following proteins until they have found the one with the desired properties. \n\n5.) Failure modes of human experts\n\nWe now provide a list of images (see Appendix) which were frequently mis-labeled by the human experts and how the CNN handled those images including a thorough discussion.\n", "title": "Reply to reviewer"}, "Bkx8ennFC7": {"type": "rebuttal", "replyto": "SJlJ53Vqnm", "comment": "We thank the reviewer for the assessment of our work. \n\nAd Pros 1.) \n\nWe thank the reviewer for this positive comment. We have even improved our baseline estimates a lot. \n\nAd Cons 1.) \n\nWe agree with the reviewer that this is an application paper (such as references [1-3]) - for which ICLR specifically called. However, the problem posed by these images is rather general than specific (see point 2). \nWe improved the introduction section to emphasize the main contribution of our work, which we see in the development of the architecture and that it treats the problem of weakly annotated data different than previous approaches (see also point 2). \n\n[1]Yin, B., Balvert, M., Zambrano, D., Sch\u00f6nhuth, A., & Bohte, S. (2018). An image representation based convolutional network for DNA classification. International Conference on Learning Representations (ICLR), arXiv preprint arXiv:1806.04931.\n[2] Chen, X., Liu, C., & Song, D. (2018). Towards Synthesizing Complex Programs from Input-Output Examples. International Conference on Learning Representations (ICLR), arXiv preprint arXiv:1706.01284.\n[3] Falcon, W., & Schulzrinne, H. (2018). Predicting Floor-Level for 911 Calls with Recurrent Neural Networks and Smartphone Sensor Data. International Conference on Learning Representations (ICLR), arXiv preprint arXiv:1710.11122.\n\nAd Cons 2.) \n\nThe technical challenge of this task is a difficult problem in machine learning, namely, how to deal with weakly annotated data. Instead of single instances that are labeled, a set of instances is labeled. A machine learning method has to collect and combine hints from the instances for accurate predictions. This is different from the problem setting of object recognition in MNIST, CIFAR, and ImageNet, where each image is clearly and unambiguously labeled. Current methods ameliorate this problem by learning a representation per image-patch or instance, e.g. a cell, and then pooling over those patches, such as mean- or max-pooling, or noisy-and (Kraus et al., 2016). Thus, these approaches collect hints from different patches. With GapNet-PL, we propose an alternative to these, where hints are collected by global-average pooling layers. In contrast to other methods, pooling is not done at a high-level representation nor at the prediction level but at the low-level convolutions.  \nWe apologize that we have not stated this clearer and we now improved the introduction section to explain the general problem setting.  \n\nAd Cons 3.)\n\nWe thank the reviewer for pointing out these problems. First, the test set of Liimatainen et al. was drawn from the same basic set of images and therefore the estimate should be comparable. Under the iid assumption, the expected values of the estimates (but admittedly not the variance) should be the same. Second, we re-implemented the approach of Liimatainen et al. and now provide a performance estimate on our test data set. \nThird, we recruited two more human experts to improve the performance estimate of human experts. The performance of the human experts now ranges from an accuracy of 64% to 72%. We also tested 25 scholars, i.e. graduate and undergraduate students with life science background and specific training at this task. Thus, we can now also provide a reliable estimate for non-expert human performance. The mean performance of scholars is 51% with a range from 34% to 66%.  \nWe hope that these improvements would incline the reviewer to re-think his/her current evaluation.\n\nAd Cons 4.)\n\nWe want to point out that the state-of-the-art before our work was an F1 score of ~0.50 (Liimatainen et al., 2018), which we improved very significantly by proposing GapNet-PL and by comparing other architectures. We state this clearer in the introduction and conclusion section. ", "title": "Reply to reviewer"}, "SyeqlUatC7": {"type": "rebuttal", "replyto": "HyedjVXrim", "comment": "We thank the reviewer for his/her positive comments. We have implemented and tested another method (of Liimatainen et al., 2018). Furthermore, we have put efforts in improving the human-computer competition by recruiting two additional experts to perform the task and 25 scholars, i.e. graduate undergraduate students with a life science background that were given special training. We found that the performance of experts ranges from an accuracy of 64% to 72% (average: 67%) and the performance of scholars ranges from an accuracy of to 34% to 66% (average: 51%).\n\nConcerns:\n\n1.) Generalizability of the architecture\n\nWe thank the reviewer for pointing out his concern. As communicated to the other reviewers, our architecture provides an alternative to ameliorate the problem of weakly labeled data. Instead of combining hints (pooling) of different instances at representation or prediction level, our network rather combines hints from low-level filter. We think that this problem is occurring in a number of different situations, such that researchers in other machine learning fields might find the GapNet-PL technique highly relevant to their problem, as well.\n\n2.) Model development and re-implementation of competing methods\n\nWe aimed at using the original implementations of the architectures and only made changes if the original architecture was impossible to run, which could happen if the original architecture was developed on smaller images. Otherwise, we only changed parameters if they led to an improvement of the validation metrics. \nThe reviewer is correct that we only use a single train/val/test split, where all hyper-parameter optimization is done on the validation set in order to prevent overfitting. We also took care that the amount of searched hyper-parameters is similar for all methods (e.g. we tested two different initial learning rates for all competing methods). \nThe GapNet architecture was developed on a different data set with similar characteristics (classification task of high-throughput fluorescence microscopy images) and then adapted to this dataset. We downsized the model slightly as the original task was more complex and then adjusted its most important hyper-parameters (such as learning rate) on the validation set. We will make this more explicit in the upcoming version of the paper, thanks for pointing this out.\n\n3.) Circularity of labels\n\nWe thank the reviewer for this important comment that led us to dive deeper into the actual annotation process of those images (Thul et al., 2017). It turned out that the description of the challenge data set was partly lacking with respect to the annotation process. We updated the respective information in the dataset section of the paper. \n\n4.) Biology jargon\n\nWe improved the introduction section and removed the biology jargon. Additionally, we now provide examples where humans and network disagree. \n\nMinor comments:\n\nThanks, we corrected as suggested except for the last one. We decided to keep the normalized confusion matrices to be consistent with Esteva et al. (2017) and also because the visualization (coloring of the cells) is better. \n", "title": "Reply to reviewer"}, "SylHrr7cnQ": {"type": "review", "replyto": "ryl5khRcKm", "review": "The paper proposes a CNN variant tailored for high-resolution\nimmunofluorescence confocal microscopy data.  The authors show\nthat the method outperforms a human expert.\n\nThe proposed method is evaluated on benchmark instances\ndistributed by Cyto Challenge '17, which is presumably the best\ndata source for the target application.  Indeed, the method\nperforms better than several competitors plus a single human\nexpert.\n\nThe paper is well written and easy to follow.  I could not spot any\nmajor technical issues.\n\nThis is an applicative paper targeting a problem that is very\nrelevant in bioinformatics, but it sports little methodological\ninnovation.  On the biological side, the contribution looks\nsignificant.  Why not targeting a bioinformatics venue?\n\n\nDetailed comments:\n\nPapers that stretch multiple fields are always hard to review.  On\none hand, having contributions that cross different fields is a\nhigh-risk (but potentially highly rewarding) route, and I applaud\nthe authors for taking the risk.  On the other hand, there's the risk\nof having unbalanced contributions.\n\nI think that the contribution here is mostly on the bioinformatics\nside, not on the deep learning side.  Indeed, the method boils\ndown to a variant of CNNs.  I am skeptical that this is enough to\nspark useful discussion with practitioners of deep learning\n(although I could be wrong?).\n\nFinally, I am always skeptical of \"human-level\" performance claims.\nThese are strong claims that are also hard to substantiate.  I don't\nthink that comparing to a *single* expert is quite enough.  The fact\nthat \"the human expert stated that he would be capable to localize\nproteins with the provided data\" doesn't sound quite enough.  I\nagree that the user study could be biased (and that \"It would be\na tremendous effort to find a completely fair experimental\nsetting\"), but, if this is the case, the argument that the method\nreaches human-level performance is brittle.\n\n\nOther remarks and questions:\n\n- Why wasn't the dataset of Liimatainen et al. used for the\ncomparison?\n\n- The authors say that \"due to memory restrictions, the smallest\nvariant of DenseNet was used\".  How much of an impact could have\nthis had on performance?\n\n- \"One random crop per training sample is extracted in every epoch\".\nDoesn't this potentially introduce labeling errors?  Did you observe\nthis to occur in practice?\n\n- The authors claim that the method is close to perfect in terms\nof AUC.  In decision-making applications, the AUC is a very\nindirect measure of performance, because it is independent of\nany decision threshold.  In other words, the AUC does not measure\nthe yes/no decisions suggested by the method.  Why is the AUC\nimportant in the biological application at hand?  Why is it important\nto the users (biologists, I suppose) of the system?\n\nIn particular, \"our method performs nearly perfect, achieving an\naverage AUC of 98% and an F1 score of 78%\" seems inconsistent\nto me---the F1 is indeed \"only\" 78%.\n\n- I would appreciate if there was a thorough discussion of the\nfailure mode of the expert.  What kind of errors did he/she\nmake?  How are these cases handled by the model?", "title": "high-performance method with minor methodological contributions", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HyedjVXrim": {"type": "review", "replyto": "ryl5khRcKm", "review": "This manuscript describes a deep convolutional neural network for\nassigning proteins to subcellular compartments on the basis of\nmicroscopy images.\n\nPositive points:\n\n- This is an important, well-studied problem.\n\n- The results appear to improve significantly on the state of the art.\n\n- The experimental comparison is quite extensive, including\n  reimplementations of four, competing state-of-the-art methods, and\n  lots of details about how the comparisons were carried out.\n\n- The manuscript also includes a human-computer competition, which the\n  computer soundly wins.\n\n- The manuscript is written very clearly.\n\nConcerns:\n\nThere is not much here in the way of new machine learning methods.\nThe authors describe a particular neural network architecture\n(\"GapNet-PL\") and show empirical evidence that it performs well on a\nparticular dataset.  No claims are made about the generalizability of\nthe particular model architecture used here to other datasets or other\ntasks.\n\nA significant concern is one that is common to much of the deep\nlearning literature these days, namely, that the manuscript fails to\nseparate model development from model validation. We are told only\nabout the final model that the authors propose here, with no\ndiscussion of how the model was arrived at.  The concern here is that,\nin all likelihood, the authors had to try various model topologies,\ntraining strategies, etc., before settling on this particular setup.\nIf all of this was done on the same train/validation/test split, then\nthere is a risk of overfitting.\n\nThe dataset used here is not new; it was the basis for a competition\ncarried out previously.  It is therefore somewhat strange that the\nauthors chose to report only the results from their reimplementations\nof competing methods.  There is a risk that the authors'\nreimplementations involve some suboptimal choices, relative to the\nmethods used by the originators of those methods.\n\nAnother concern is the potential circularity of the labels.  At one\npoint, we are told that \"Most importantly, these labels have not been\nderived from the given microscopy images, but from other\nbiotechnologies such as microarrays or from literature.\"  However,\nearlier we are told that the labels come from \"a large battery of\nbiotechnologies and approaches, such as microarrays, confocal\nmicroscopy, knowledge from literature, bioinformatics predictions and\nadditional experimental evidence, such as western blots, or small\ninterfering RNA knockdowns.\"  The concern is that, to the extent that\nthe labels are due to bioinformatics predictions, then we may simply\nbe learning to re-create some other image processing tool.\n\nThe manuscript contains a fair amount of biology jargon (western\nblots, small interfering RNA knockdowns, antibodies, Hoechst staining,\netc.) that will not be understandable to a typical ICLR reader.\n\nAt the end, I think it would be instructive to show some examples\nwhere the human expert and the network disagreed.\n\nMinor:\n\np. 2: \"automatic detection of malaria\" -- from images of what?\n\np. 2: Put a semicolon before \"however\" and a comma after.\n\np. 2: Change \"Linear Discriminant\" to \"linear discriminant.\" Also, remove\nthe abbreviations (SVM and LDA), since they are never used again in\nthis manuscript.\n\np. 5: Delete comma in \"assumption, that.\"\n\np. 8: \"nearly perfect\" -> \"nearly perfectly\"\n\nThe confusion matrices in Figure 5 should not be row normalized --\njust report raw counts.  Also, it would be better to order the classes\nso that confusable ones are nearby in the list.\n", "title": "A CNN that boosts the state of the art on an important image classification task in biology", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}