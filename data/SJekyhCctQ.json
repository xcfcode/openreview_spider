{"paper": {"title": "Detecting Adversarial Examples Via Neural Fingerprinting", "authors": ["Sumanth Dathathri", "Stephan Zheng", "Yisong Yue", "Richard M. Murray"], "authorids": ["sdathath@caltech.edu", "st.t.zheng@gmail.com", "yyue@caltech.edu", "murray@cds.caltech.edu"], "summary": "Novel technique for detecting adversarial examples -- robust across gradient-based and gradient-free attacks, AUC-ROC >95%", "abstract": "Deep neural networks are vulnerable to adversarial examples: input data that has been manipulated to cause dramatic model output errors. To defend against such attacks, we propose NeuralFingerprinting: a simple, yet effective method to detect adversarial examples that verifies whether model behavior is consistent with a set of fingerprints. These fingerprints are encoded into the model response during training and are inspired by the use of biometric and cryptographic signatures. In contrast to previous defenses, our method does not rely on knowledge of the adversary and can scale to large networks and input data. The benefits of our method are that 1) it is fast, 2) it is prohibitively expensive for an attacker to reverse-engineer which fingerprints were used, and 3) it does not assume knowledge of the adversary. In this work, we 1) theoretically analyze NeuralFingerprinting for linear models and 2) show that NeuralFingerprinting significantly improves on state-of-the-art detection mechanisms for deep neural networks, by detecting the strongest known adversarial attacks with 98-100% AUC-ROC scores on the MNIST, CIFAR-10 and MiniImagenet (20 classes) datasets.  In particular, we consider several threat models, including the most conservative one in which the attacker has full knowledge of the defender's strategy. In all settings, the detection accuracy of NeuralFingerprinting generalizes well to unseen test-data and is robust over a wide range of hyperparameters.", "keywords": ["Adversarial Attacks", "Deep Neural Networks"]}, "meta": {"decision": "Reject", "comment": "* Strengths\n\nThe paper proposes a novel and interesting method for detecting adversarial examples, which has the advantage of being based on general \u201cfingerprint statistics\u201d of a model and is not restricted to any specific threat model (in contrast to much of the work in the area which is restricted to adversarial examples in some L_p norm ball). The writing is clear and the experiments are extensive.\n\n* Weaknesses\n\nThe experiments are thorough. However, they contain a subtle but important flaw. During discussion it was revealed that the attacks used to evaluate the method fail to reduce accuracy even at large values of epsilon where there are simple adversarial attacks that should reduce the accuracy to zero. This casts doubt on whether the attacks at small values of epsilon really are providing a good measure of the method\u2019s robustness.\n\n* Discussion\n\nThere was substantial disagreement about the paper, with R1 feeling that the evaluation issues were serious enough to merit rejection and R3 feeling that they were not a large issue. In discussion with me, both R1 and R3 agreed that if an attack were demonstrated to break the method, that would be grounds for rejection. They also both agreed that there probably is an attack that breaks the method. A potential key difference is that R3 thinks this might be quite difficult to find and so merits publishing the paper to motivate stronger attacks.\n\nI ultimately agree with R1 that the evaluation issues are indeed serious. One reason for this is that there is by now a long record of adversarial defense papers posting impressive numbers that are often invalidated within a short period (often less than a month or so) of the paper being published. The \u201cObfuscated Gradients\u201d paper of Athalye, Carlini, and Wagner suggests several basic sanity checks to help avoid this. One of the sanity checks (which the present paper fails) is to test that attacks work when epsilon is large. This is not an arbitrary test but gets at a key issue---any given attack provides only an *upper bound* on the worst-case accuracy of a method. For instance, if an attack only brings the accuracy of a method down to 80% at epsilon=1 (when we know the true accuracy should be 0%), then at epsilon=0.01 we know that the measured accuracy of the attack comes 80% from the over-optimistic accuracy at epsilon=1 and at most 20% from the true accuracy at epsilon=0.01. If the measured accuracy at epsilon=1 is close to 100%, then accuracy at lower values of epsilon provides basically no information. This means that the experiments as currently performed give no information about the true accuracy of the method, which is a serious issue that the authors should address before the paper can be accepted."}, "review": {"HJxNopwee4": {"type": "rebuttal", "replyto": "ryxY5IDex4", "comment": "The SPSA, FGSM, PGD, BIM attacks are untargeted. This is already mentioned in the paper in the section on experiments. Please refer to the paper (end of page 7) and code for details. \n", "title": "Plenty of untargeted attacks in the paper"}, "ByeYgX16JN": {"type": "rebuttal", "replyto": "S1xVUS0hJ4", "comment": "I am assuming your question refers to the value of embedding during training. We can run the specific comparison you request. \n\nAlso, I believe the paper uses m=1000 (number of \"fingerprints\") -- it does not use the change in normalized-logits, rather it considers the label predictions at a specific set of randomly sampled points around the data-point and uses that as an ensemble classifier. Here are some initial thoughts on the need for embedding:\n\n1. We investigate the effect of adding different numbers of fingerprints in our paper. For instance, see Figure 9b: If we choose low fingerprint numbers  (say N=3) during embedding, the AUC-ROC is quite bad. In fact, we find that with an adaptive attack, we can make the adversarial examples seem more real than the real examples for such low fingerprint numbers.\n\n2. The premise for https://arxiv.org/abs/1709.05583 is that adversarial examples are not robust, however there is some evidence that robust adversarial examples can be synthesized (See https://arxiv.org/abs/1707.07397).  We believe high-confidence adversarial examples are more robust (in relation to random samples in the neighborhood) => increased success rates for high confidence attacks in https://arxiv.org/abs/1709.05583. We believe this would not be the case if the fingerprints are embedded, we will run tests to confirm this. \n\n3. Assuming a non-adaptive attacker, the success rates reported in the paper you cite (e.g. 78% adversarial success rate for JSMA on CIFAR-10) are significantly higher than those reported in our paper.  \n\n4.  https://arxiv.org/abs/1709.05583 uses randomized ensembles, so I suspect an adaptive attack with EOT (see Athayle et al. ICML'18) might succeed to some extent against the defense proposed in https://arxiv.org/abs/1709.05583.", "title": "Effect of fingerprinting"}, "SyxblBd2yV": {"type": "rebuttal", "replyto": "BkgSEnvn1V", "comment": "We have tried this earlier, and the AUC-ROC drops to ~~0.5. The adversarial examples are virtually indistinguishable from real ones if you do not embed the fingerprints.\n\nDoes that answer your question? You could probably try to learn the \"fingerprints\" naturally embedded but I am not confident that the performance would be comparable.", "title": "Adv eg. vs clean indistinguishable"}, "Hyg3sqeSyE": {"type": "rebuttal", "replyto": "H1xcUXJNkE", "comment": "We agree that we could be more forceful about emphasizing the difficulty of the non-convex problem the adversary has to solve, we will revise to enforce this point during revision. We would like to further add two more points of discussion to your comment:\n\n1. Using more fingerprints leads to more robustness to adaptive attacks, and poses a much harder optimization problem as you mentioned. We did find some evidence that directly points to this during our tests.\n\nFor MNIST with hyper-parameters N=6, \\vareps=0.1, against non-adaptive attacks NeuralFP achieves >95% AUC-ROC detection. When evaluated with adaptive-CW-l2, this number drops to 70% or less (with some variance due to the random dx chosen).\n\nWith N=20, eps=0.05, for the non-adaptive attacks AUC-ROC is near 100% and when evaluated with the adaptive-CW-l2 attack AUC-ROC is still >95%. We see a similar effect with CIFAR-10 as well (in the appendix).\n \n2. To provide extra intuition we discuss some observations from our experiments.\n\nHypothesis: NFP has a flat loss-surface away from the data. \n\nIf we look at Figure 3, most regions away from the training data have a very high fingerprint loss and these regions are mostly flat. The loss surface is essentially flat everywhere, except for a few valleys of low-loss around the data-distribution. We believe the loss surface for the fp-loss looks similar to this plot for MNIST and CIFAR-10 as well.  At large distances, for randomly sampled points, the gradient is essentially not providing much useful information. This interpretation is to some extent justified by what we observe for the random adversary (end of the appendix).\n\nIf we interpret \u201cgradient masking\u201d as a phenomena of supplying the adversary with misleading gradients, we do not think NeuralFP does gradient masking. Also, optimization methods like SPSA that \u201clocally\u201d smoothen out the gradients may not yield much useful information on how to traverse the loss-surface at these \u201cflat\u201d regions to converge to something that fools NeuralFP.\n\nSome form of macro-scale loss-approximation or search techniques could perhaps be useful, and would advance the debate, leading to better attacks. We agree that evaluating the effects of the different hyper-parameters, particularly N would be a very interesting direction for future investigation.\n", "title": "More discussion, numbers and intuition based on your comment"}, "Hyeh8SgMy4": {"type": "rebuttal", "replyto": "rylIKYfn2m", "comment": "Dear Reviewer, \nWe would appreciate additional feedback on the new experiments (with more iterations/random restarts) as requested. We hope the following points and our revised paper address your concerns stated above. \n\n1. We have run adaptive-PGD (which is a strong gradient based attack) with random restarts and many iterations (Appendix H), and adaptive-CW-l2 with *many* iterations (20000) (Appendix I), and see that NeuralFP is robust even for these settings. \n\n2. CW-l2 attack has been shown to fail, even when there exist adversarial examples for certain. For example, in the paper https://arxiv.org/abs/1802.00420 (see Appendix) show that against Defense-GAN the attack is only able to reduce the accuracy to 55% even though adversarial examples exist on the projection manifold. As such, there is no formal guarantee that the CW-l2 attack will succeed.\n\nThis is also discussed in their ICML talk ( https://nicholas.carlini.com/talks/2018_icml_obfuscatedgradients.mp4 ) at minute 15:00.", "title": "Revision with random seeds/many more iterations"}, "HygINeIsRm": {"type": "rebuttal", "replyto": "SJekyhCctQ", "comment": "https://www.dropbox.com/sh/iq0yub74gquz1od/AADZXUVabMvZasPrt-6-c-Wpa?dl=0", "title": "Source Code and Model Weights at the following link"}, "ryxlKRJ60X": {"type": "rebuttal", "replyto": "SkxcMTJ60Q", "comment": "The attacks are applied to unseen test data (mentioned in the experiments section 3.1). ", "title": "Unseen test data"}, "r1xCwpgqAQ": {"type": "rebuttal", "replyto": "SJekyhCctQ", "comment": "We thank the reviewers again for the comments and insightful feedback. We report results on additional experiments to evaluate whether adversarial attacks with more iterations/random restarts can degrade the detection performance for NeuralFP. These experiments were run (for CIFAR-10 samples) following the feedback/suggestions provided in the reviews.\n\nAttack                                                             Distortion        Iteration         Bisection       Step-size       AUC-ROC\n                                                                           Bound                Steps              Steps                                     (%)\nCarliniL2-FP (N=30, vareps=0.003)           Unbounded        20000                 20, 10              --                    94.12\nCarliniL2-FP (N=30, vareps=0.003)           Unbounded        15000                 15,  9               --                    95.48\nCarliniL2-FP (N=50, vareps=0.003)           Unbounded        20000                 20,  10             --                    95.56\nAdaptive-PGD (l-2)                                            10.0             Steps = 100            6                    0.5                  99.37\n                                                                                                  Restarts=5           \nAdaptive-PGD (l-2)                                            60.0             Steps = 150            6                    0.5                  99.55\n                                                                                                  Restarts=5           \nAdaptive-PGD (l-inf)                                     16/255             Steps = 50              6                   0.005              99.21\n                                                                                                  Restarts=50           \nAdaptive-PGD (l-inf)                                     16/255             Steps = 1000          6                   0.005              99.71\n                                                                                                   Restarts=1      \nAdaptive-PGD (l-inf)                                      0.25                Steps = 1000           6                   0.005             99.48\n                                                                                                   Restarts=1   \nAdaptive-PGD (l-inf)                                      1.0                  Steps = 150             6                    0.01               99.74\n                                                                                                   Restarts=5 \nAdaptive-SPSA                                                 0.05               Steps = 1000           20                  lr=0.01          99.84\n\t\t\t\t\t\t\t\t            \t\t\t\t\t\t\t\t               delta=0.01\nAdaptive-SPSA                                                 0.05               Steps = 1000           20                  lr=0.005        99.71\n\t\t\t\t\t\t\t\t            \t\t\t\t\t\t\t\t               delta=0.005\nAdaptive-SPSA                                                 0.25              Steps = 1000           20                  lr=0.01           99.32\n\t\t\t\t\t\t\t\t            \t\t\t\t\t\t\t\t            delta=0.01   \n\n1. To conclude, we see that state-of-the-art attacks with more iterations and varying hyperparameters search do *not* generate significantly more successful adversarial perturbations. Together with the results in our paper, this strongly suggests that current attacks are not able to effectively find adversarial perturbations that can fool NFP. \n\n2. Moreover, generating adversarial perturbations by randomly sampling around the data is not successful. This is another sanity check that indicates that NFP does *not* merely implement gradient masking.\n\n\n", "title": "New experiments and revision"}, "rJxPKmksCQ": {"type": "rebuttal", "replyto": "S1gxPy_ham", "comment": "We have posted a general comment summarizing the results from our new experiments. Apologies for the delay as these experiments took a while to run, particularly CW-fp (with large iterations) and PGD with multiple restarts. We have also updated the paper to include these results (please see Appendix for detailed discussion/more numbers). \n\nWe are happy to continue providing technical support with the implementation/models, and hope our experiments have been able to alleviate some of your concerns. Please let us know if you have further questions/comments! \n\nMany thanks for the review, and feedback on new experiments. ", "title": "Updated experiments"}, "B1gGtTl9RX": {"type": "rebuttal", "replyto": "r1xCwpgqAQ", "comment": "We have included these results and discussion in the appendix in the revised submission.\n\nWith that, we would like to emphasize a few points in addition to our earlier comments.\n\n** Black-box/grey-box settings\nWe believe NeuralFP is a strong defense in the black-box/grey-box settings. In these settings, NeuralFP achieves AUC-ROCs of 99-100% (near-perfect detection) consistently across attacks and in practical applications where security is a concern in the black-box setting, we believe NeuralFP is a significant step towards solving the problem of adversarial robustness. \n\n** Adaptive white-box setting\nIn the adaptive white-box setting, we show that the approach is robust over the full range of currently known attacks, even when run for a very large number of iterations, and with randomized restarts. We believe that running the attacks for much longer is not likely going to change the numbers significantly. (See Appendix H, Appendix I, Appendix J)\n\n** Weakness of current attacks\nWe do not claim that NeuralFP is entirely robust to all possible attacks, but against attacks that defenses are currently evaluated against, NeuralFP is able to detect such attacks quite successfully. These attacks fail even in scenarios when adversarial examples definitely exist, pointing to weaknesses in the current optimization methods.\n\n** Gradient masking unlikely\nIn addition to evaluation against black-box adversaries and adaptive white-box adversaries, we further evaluate against an adversary that functions by random sampling. This is suggested as a test for obfuscated gradients in (https://arxiv.org/pdf/1802.00420.pdf ). For a set of MNIST and CIFAR-10 unseen data-points, we sample large numbers of points (5e5 to 1e6 points for each input) uniformly randomly in epsilon-balls around the unseen data-points. For these points we analyse the fingerprint-losses and we find that for the entire set of points sampled, we cannot find a single adversarial point with a low fingerprint loss (See Appendix K).\n\nFurther,  we  would  like to note that our defense does not  introduce any non-differentiable functions/optimization procedures that have frequently been associated with obfuscated gradients.\n\n** Current attacks are not effective, even with more computation and hyperparameter search.\nFurther, that current attacks fail even when allowed a large perturbation budget indicates the need for stronger attacks/improved optimization methods. \n\nIn short, we believe that NeuralFP is a strong detection method for current attacks and serves as a useful baseline for developing a new range of adversarial attacks and thereby, robust defenses.\n", "title": "Contd."}, "rylIKYfn2m": {"type": "review", "replyto": "SJekyhCctQ", "review": "This work introduces a novel defense method \"Neural Fingerprinting\" against adversarial examples.\nIn the training process, this method embeds a set of characteristic labeled samples so that responses of the model around real data show a specific pattern.  The defender can detect if a given query is adversarial or not by checking the pattern at test time.\n\nStrong point:\nThe strong point is that the proposed method seems to be appropriate and technically original. The performance is well investigated and compared with several competitors.  The organization is good and the idea is clearly stated. \n  \nWeak point:\nOne question is that why the proposed method can be protective against the adaptive CW attack. In the public discussion, the authors mention that the defense works successfully because the landscape of the fingerprinting loss is non-convex and no gradient method is guaranteed to find a suitable solution. If this is correct, did you repeatedly try the gradient-based attack with changing random seeds? By doing so, the attack might work successfully with a certain probability.\n\nComments:\nThe presented method seems to have a certain similarity with digital watermarking of deep neural networks, for example:\nhttps://gzs715.github.io/pubs/WATERMARK_ASIACCS18.pdf\nIt would be interesting to mention to these methods in the related work section.\n\n", "title": "On the adaptive CW attack", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HkgIwaDrRX": {"type": "rebuttal", "replyto": "ByeJoMCmCX", "comment": "We have seen that the detection AUC-ROC for a set of correctly classified vs adversarial examples is usually a bit higher than that for a set of clean (includes misclassified) vs adversarial.  Here, the correctly classified examples being from the clean set. This seems to suggest that the distribution of losses in misclassified samples is generally higher than that of correctly classified samples. Specifically how much higher is something we have not explicitly looked at, we can probably take a look at that after the rebuttal phase is complete.", "title": "Interesting Question!"}, "Hyx_7PKyT7": {"type": "rebuttal", "replyto": "Syg8JVYkTm", "comment": "Before, we address your other issues we would like to immediately get you to be able to run our code. We run our code on a machine with 2 GPUs, so the OOM error is quite surprising. \n\nCould you please let us know if you have access to AWS instances? We have been able to run our experiments on the AWS DeepLearning AMI. I can make a public AMI and share it with you if that's easier. \n\n\nI am able to run our code for MNIST on an Ubuntu 16.04 machine with a 16GB RAM (no GPU). The commands I am running on the Ubuntu machine are as follows:\n1. cd into the directory with run.sh after extracting the files from the tar-ball\n2. Run the command 'export PYTHONPATH=$(pwd):$PYTHONPATH'\n3.  Run the command './run.sh mnist train attack eval nogrid 50 0.1 10'\n\nAlso, this runs with python 2.7 and we have not tested with python 3 or above.\n\nWe have tested our code on multiple linux platforms with the following specific software versions:\n\n>>> torch.__version__\n'0.3.0.post4'\n\n>>> torchvision.__version__\n'0.2.0'\n\n>>> tensorflow \n'1.4.0'\n\n>>> keras\n'2.1.0'\n\nWe are actively currently working on running PGD (with random restarts) and SPSA with many more iterations. We will provide detailed answers to your questions, and do our best to get you running with the code ASAP! Engineering the whole project is considerable effort, we strongly recommend trying to run our scripts which we have shared. \n\n", "title": "Help Running Our Code"}, "rJlVieO2Tm": {"type": "rebuttal", "replyto": "S1gxPy_ham", "comment": "Minor clarification: Do you have a preferred step-size for the bounded and unbounded l2 PGD attacks? I am struggling to find literature/code that recommends a good step-size.\n\nThanks again for the help and feedback!", "title": "Clarification -- step-size"}, "BkgOSwznaX": {"type": "rebuttal", "replyto": "ryxH6LfhaQ", "comment": "\u201cIt is then unclear if this inability is due to computational hardness, or simply an extremely non-convex landscape that requires \"tricks\" to be able to navigate.\u201d\n      \nWe feel that this an extremely interesting question, and we have extensively shown that the current set of optimization methods used to effectively evaluate adversarial attacks do not convincingly answer this question. Further (theoretical) analysis of this question is an exciting direction for future research.\n\n\"- PGD using L2-normalized gradients is an L2 attack which does not minimize l2 distance (i.e. taking gradient steps (step size) * grad / ||grad||. (In the same vein, the authors should also verify that the L-inf PGD attacks they are using normalize the gradient via sign, i.e. (step size) * sign(grad).)\n      \nConcretely, it would be very helpful for the authors to show:\n- Figure 5 for Linf and L2 (using PGD [this means sign(grad) and grad/||grad|| respectively] in both cases), extending from eta=0 to eta=1.\"\n\n*** Attack normalization. \nWe use the default Cleverhans implementation for the SPSA/PGD attacks and (https://github.com/carlini/nn_robust_attacks)  for the CW attack. We did verify that Cleverhans normalizes the gradient via sign for PGD. \n\n*** Experiments and compute budget.\nThanks for the suggestions for new experiments. Running PGD (both versions) with the above mentioned #iterations/bisection steps, at a coarseness of \\delta eta = 0.05, requires about 40 GPUs for 5 days, which equates to ~$10000 of AWS credits. Since we work in an academic setting, this is beyond our compute budget. \n\nWe will try PGD with the normalized L2 norm and do our best within the allowed time frame and our financial constraints. However, we do feel our current extensive experiments already stand on their own as strong positive validation of NeuralFP.\n\n*** Additional tests for gradient masking\nWe are also working on an additional test for gradient masking with a large set of random-points described in [1]. We will update the page and our manuscript with these results shortly.\n\nPlease let us know if you need any additional technical help with your evaluation and if you have other questions.\n", "title": "Response to response (part 2)"}, "ryxH6LfhaQ": {"type": "rebuttal", "replyto": "rye8BOm8aX", "comment": "Thanks for the quick reply and the heads up about the experiments.\n\n\u201c- R.e. papers that turn 2s into 6s etc, see this paper:     https://arxiv.org/abs/1805.12152, which I believe may be useful.\u201d\n\n\nThanks for this interesting reference. We respectfully note that the adversarial  examples illustrated in the paper can clearly be deciphered to be altered and unnatural images even with the human eye (e.g. Figure 3 in the paper). This by itself is not convincing evidence that PGD/SPSA can generate \u201crealistic\u201d images from the \u201ctrue\u201d distribution.\n\n\u201c- R.e. high eta leading to failures: this should actually be true both for detection and prediction. For example, at eta = 1.0, an attack that selects the nearest neighbor from another class in the training set should suffice to get 0% detection. In the same vein, an attack that simply uses a generative model of some kind to find images of a class while minimizing distance to an original image should suffice to find high-eta examples of other images that are not adversarially perturbed in the first place.\u201d\n\nWe agree that \u201crealistic\u201d unbounded attacks undoubtedly exist, e.g., images from the training dataset. We would like to clarify that most of our earlier comments with regards to finding realistic images in the rebuttal were in regard to the attacks that we use to  evaluate NeuralFP, such as SPSA/PGD/FGSM/CW. \n\nWe would like to emphasize that our contribution focuses on detecting *bounded* attacks from the literature. Note that we do *not* evaluate or claim robust detection of (yet unpublished) unbounded attacks, e.g., based on generative models such as GANs (e.g., Song et al, NIPS 2018  https://arxiv.org/abs/1805.07894). Although we plan to evaluate on detection of unbounded (generative) attacks shortly, the scope of the current paper is limited to bounded adversarial attacks.\n\nFurthermore, for bounded adversarial attacks (e.g., eta<0.25), robust prediction is certainly a more difficult task than detection. For instance, Schmidt et al, NIPS 2018 (https://arxiv.org/abs/1804.11285) argue that robust prediction requires significantly larger amounts of data.\n\n\u201cIn any case, if extending Figure 5 to eta = 1.0 still yields a high success rate at detection with the current attacks, this indicates that it is a failure of current optimization methods, rather than a true detection mechanism, that might be generating this success, so this would be useful.\"\n\n\nWe agree that failure at eta=1.0 would indicate flaws in the evaluated attacks, e.g., their inability might be due to the computationally hardness or might need improved optimization algorithms. We will try to address your suggestion to evaluate up to eta=1.0. However, we note that is computationally demanding.\n\nNote that for each eta, the PGD attack evaluation (with the bisection search  and 1000 iterations) for 112 samples is expected to take 4-5 days on a single GPU. We will try to run as many cases as possible before the revision dates, but we have a limited compute budget and time constraints.\n\nFurthermore, although evaluating at large eta is interesting, note that we do *not* claim NeuralFP can detect all (unbounded) adversarial attacks. However, given its excellent detection performance on both bounded gradient and non-gradient based attacks, we do believe it is a very strong baseline. As such, NeuralFP highlights the need for better attacks. \n     \nAlso, as we discussed in our previous replies, we do not believe our experimental results show that NeuralFP provides robustness merely through gradient masking. We feel that failure of the attacks at eta=1.0 also would not provide conclusive evidence of gradient masking, but rather a failure of the current attacks. However we are running experiments with larger eta-s and will update the manuscript/the page shortly.\n", "title": "Response to response (part 1)"}, "SJgi7QLHpQ": {"type": "rebuttal", "replyto": "HJx4wGIHpQ", "comment": "*** Extra experiments: \n\nTo alleviate your concerns regarding attacks with more iterations, we are running more experiments as suggested. Note that our previous experiments were already extensive -- as noted by the reviewer, we have put in significant effort into our evaluation.\n\nFor 112 unseen CIFAR-10 test points we are running: \n1. CW-fp with 20,000 iterations and bisection search with 25 steps for gamma-1 and 12 steps for gamma-2 (roughly 6,000,000 gradient steps per point!).\n\n2. Adaptive-PGD attack (similar to the other adaptive attacks) with \na) 1,000 steps and 1 randomized restart with 6 bisection steps, eps = 16.0/255.0, eps_iter=0.005 (same parameters as https://arxiv.org/pdf/1807.10272.pdf)\nb) 50 steps and 50 randomized restarts with 6 bisection steps, eps = 16.0/255.0, eps_iter=0.005 \nWe believe 6 bisection steps suffice here because we are searching for gamma in the range (0.01,10) -- at gamma>10, misclassification starts to fail for a significant fraction of the samples. The fact that increasing gamma causes misclassification to fail indicates that the gradients from L_{fp} are likely not exploding or vanishing. \n\n3. We ran SPSA with 1,000 iterations and 20 bisection steps (with default parameters for eps=0.05, lr=0.01 and delta=0.01). The AUC-ROC is 99.97%. We are running experiments with \n1. lr=0.005 and delta=0.005 to see if that leads to better performance. \n2. eps=0.25, 1000 iterations, lr=0.01 and delta=0.01 to see if SPSA degrades accuracy at larger eta.\n\nOnce we have all the numbers, we will compile the results and update our paper and the page. If you would like to suggest other relevant experiments, please let us know. \n\n*** Specific comments: \n\n-- \u201cFigure 5 shows adversarial robustness even against eta = 0.25\u2026.. hard to find rather than removing them. The authors should address this issue.\u201d\n\nWe have not seen any paper that has been able to generate \u201crealistic\u201d images (for e.g. starting with a \u20182\u2019 and ending up with a \u20186\u2019) using any of the attacks we test against. It would be very useful if you could point us to related work that has been successful in doing so. We believe traversing the manifold of natural images in a high-dimensional spaces is an inherently challenging optimization problem (assuming a reasonable compute budget). There is no guarantee that any of the current attack techniques we are aware of will find other realistic images in a computationally tractable way (e.g., within a reasonable number of iterations/random restarts), starting from the vicinity of one image. \n\n-- \u201c-Showing that at a sufficiently high eta attacks start to succeed is also useful\u201d\n\nThis makes sense in the context of robust prediction, where for large perturbations you can turn realistic images into noise where classification makes no sense. The functioning of our method is entirely different to robust prediction methods, since noise is unlikely to pass through NeuralFP. One way to definitely fool NeuralFP is to generate images from the true data-distribution, but as discussed, generating \u201crealistic\u201d images from the true data-distribution is not necessarily an easy task.", "title": "Response to Review (part 2)"}, "HJx4wGIHpQ": {"type": "rebuttal", "replyto": "Syg8JVYkTm", "comment": "We sincerely thank the reviewer for taking time to implement the paper and run our experimental code,  we believe that openness of any defense is a key necessary step in the evaluation process and we will do our best to support your efforts. \n \nWhile we (and possibly, the reviewer) run experiments with a large number of iterations, with large number of steps for the bisection search (and for PGD, *many* randomized restarts), to encourage discussion we are posting an initial response addressing the reviewer\u2019s concerns.\n\n*** Regarding gradient masking:\n\nFirst, we would like to address your main concern, i.e., gradient masking.\n\nOther than the fact that FGSM and SPSA fail with large eta, NeuralFP does not show any other signs of gradient masking. The reference [1] mentions a number of features of gradient masking. We note that NeuralFP behaves *contrary* to defenses that rely on gradient masking in many ways: it is \n\n1) robust to black-box attacks, SPSA attack and adaptive attacks, and \n2) single step attacks do worse than multi-step attacks.\n\nThis indicates that NeuralFP may not rely on gradient-masking. With the current set of available attacks it seems hard to conclude (beyond reasonable doubt) about NeuralFP\u2019s reliance on gradient masking, one way or another. \n\n\u201c The two reasons I strongly believe this is the case are (a) Figure 5 and (b) the authors themselves state that their loss is highly non-convex and that no gradient-based method may be able to find a solution. This, however, does not guarantee robustness (see [1] for why such \u201cunfriendly\u201d landscapes can usually be circumvented)\u201d\n\nFirst we note that the function L_{fp} is continuous and differentiable with respect to the logits. Also, please note all of the attacks used in [1] are indeed gradient based. \n\nSecond, [1] (See section 4) introduces three strategies to traverse \u201cunfriendly\u201d landscapes: \n  1. BPDA\n  2. Expectation over transformation\n  3. Reparameterization\n\nHowever, we note that none of these techniques seem directly applicable to our defense: \na) BPDA is applicable in settings where gradient information is masked with either non-differentiable operations or optimization procedures that are likely to cause the gradients to not be readily available. Our loss is continuous and differentiable. This leads us to believe BPDA is not relevant to this setting.\nb) Expectation over transformation -- This mechanism is not relevant as NeuralFP involves no randomization during detection.\nc) Reparameterization -- There is no reparameterization involved in the defense. The network *directly* acts on the image space. \n\nPhenomena such as label leaking (https://openreview.net/forum?id=BJm4T4Kgx) are not likely because we do not train using any information about adversarial examples. \n\nHowever, we understand that it might be possible that there exist attacks that break our defense since we do not provide formal guarantees. We have conducted experiments to extensively evaluate our defense, and are continuing to work on evaluating the defense in more detail. \n\nIn this light, NeuralFP sheds light on possible weak spots in several of the attacks we evaluated against. This implies that either NeuralFP is indeed strong and/or we need stronger attacks for performing accurate evaluations -- both situations being beneficial to the adversarial ML community.  We are happy to summarize these discussion points in the final paper.\n", "title": "Response to review (part 1)"}, "rJehrXIBpQ": {"type": "rebuttal", "replyto": "SJgi7QLHpQ", "comment": "-- \u201c- Running SPSA, CarliniL2-FP, and PGD for *many* more iterations and using *many* \u2026\u2026 these results)\u201d\n\n*** Regarding Bisection Search: \nWe use a starting point of 1e-2 (<5k) for gamma (please see our hyper-parameters at the end of the public discussion) during the bisection search. Please note that this same bisection search is used in the original CW-L2 attack. Further, larger the gamma, more preference is given to minimizing L_{fp}. \n\n\n*** Regarding SPSA: \n\nWe directly inquired with the authors of (https://arxiv.org/abs/1802.05666) about choosing the optimal Lagrange parameter to use for the SPSA attack. We were informed that the Lagrange coefficients for their experiments were hand-picked loosely based on the values of the losses. They further recommended that the simplest thing to do, in the context of their paper, is to just optimize J = C * min(1, J_adv) + L_CNN (for large C). This will cause the optimizer to ensure the image is misclassified, but stop optimizing the misclassification loss once it is already misclassified, and then focus on the likelihood loss. In summary, we believe the Lagrange parameter itself does not affect the results to that great an extent, and a coarse search should suffice. However, we are doing a finer search now as detailed at the end of our comment. \n\n*** Regarding CarliniL2-FP: \n\nNote that the loss in CW-fp has a similar form as that used in the SPSA paper (the loss in the SPSA paper was modelled after the loss from the CW attack), hence we believe that the optimization of L_{CW} and L_fp is not too sensitive to the exact Lagrange coefficients; a coarse search should suffice in this case as well. In the paper where the CW-loss function was introduced, the authors argue that the coefficient matters the most in producing the smaller adversarial perturbation. However, as suggested by the reviewer, we are running experiments with an increased number of bisection steps and iterations to be sure of the result. \n\n*** Regarding PGD:\nWe are currently running PGD with a large number of iterations and restarts as discussed above. We will post the results shortly. \n\n-- \u201cThe AUC should monotonically degrade with eta (this is another indication the attacks might not be running for long enough)\u201d\n\nWe agree that with large eta this statement holds true for robust prediction, but it is not obvious that this should indeed be the case for *detection* with NeuralFP, as explained earlier. Large eta can be used to generate noisy images, but it is not entirely obvious that one would be able to generate realistic images from the true distribution by simply allowing for a large eta using the current attack schemes in a computationally tractable manner. \n\n--  \u201cThe method does not seem to be specific to L-infinity constraint. To this end, a version of Figure 5 in the L2 case would be extremely useful in understanding the detection method.\u201d\n\nIndeed, our method is not specific to any adversarial attack. We test against the JSMA attack, which is L-0 based and we have near perfect detection. The CW attack attempts to minimize the L-2 perturbation and we cannot vary the L-2 perturbation as a hyperparameter. Do you recommend any specific L2 based attacks to test-against?\n\n[1] https://arxiv.org/pdf/1802.00420.pdf\n", "title": "Response to review (part 3)"}, "B1g_CLIBp7": {"type": "rebuttal", "replyto": "rylIKYfn2m", "comment": "Thank you for your feedback. As detailed in our response to Reviewer 1, we are working on new experiments with randomized restarts for PGD with *many* iterations. We will report back with the numbers for our experiments with randomized restarts soon. Please see our response to Reviewer 1 for a detailed discussion with regard to the details of our new experiments and specific comments on gradient masking.\t\n\nEven though our loss is non-convex, the fingerprint loss L_{fp} is differentiable and continuous, and it is not obvious to us that any of the attacks we study are likely to succeed against NeuralFP, even with a larger distortion budget. We believe this is because finding \u201crealistic\u201d images in a high-dimensional space starting from the vicinity of the current image may not be an easy task . \n\nBesides the experiments with randomized restarts, which we are currently running, if you have further specific suggestions to improve the paper and the evaluation, we would be glad to hear them.\n\nWe would like to also point out that the Carlini-Wagner attack (https://arxiv.org/abs/1608.04644) was compared with that of exhaustive search using a solver-based approach (https://arxiv.org/abs/1709.10207). It was shown that the Carlini-Wagner attack does not necessarily succeed in finding the optimal attack. Note that exhaustive search comes with tremendous computation costs (the problem is NP-hard!). This implies that there is no guarantee that the CW attack does indeed find the global minima, and finding the global optima through exhaustive search might be intractable.\n\nAlso, we would like to note again that the SPSA attack and black-box attacks have been shown to overcome defenses that mask gradients. However, NeuralFP performs quite well against these attacks.\n", "title": "Response to Reviewer 2"}, "rkgs9ELr6X": {"type": "rebuttal", "replyto": "Bkg6qVsa2X", "comment": "Thank you for the comments and the feedback, we are glad you found our contributions interesting and promising. \n\nAddressing your questions:\n\n-- \u201cCan an adversarial strategy be developed that could execute a successful attack while minimizing the fingerprint loss.\u201d\n\nThis is an interesting question, if one can indeed develop a new attack to break NeuralFP. We are actively investigating this, and we believe that NeuralFP will result in better attacks being formulated and eventually, better defenses.\n\n-- \u201cAnother issue is whether the NeuralFP would work on more challenging data sets where the classes are highly fragmented - at what rate would the benefits of NeuralFP fade as the classification performance degrades?\u201d\n\nFrom the few experiments we report on MiniImagenet-20, we see good detection rates on FGSM and BIM. Also, when moving from MNIST to CIFAR-10, the classification performance degrades quite a bit, while NeuralFP\u2019s performance nearly stays the same. We think these are promising signs that NeuralFP would be beneficial even on fragmented data-sets where classification is harder. However, conclusively answering this question concretely would need a thorough investigation.\n\n-- \u201cWhat happens to performance if the perturbation directions are chosen so as to better conform with the local sub-manifolds... would fewer perturbations be required? (It would seem that reducing the number of perturbations needed could have a significant effect on training time.)\u201d\n\nThe increased training time (and memory) is indeed one of the biggest drawbacks of our approach, and the suggested idea of using a more principled approach for choosing the perturbation directions could indeed result in faster training, and possibly, better detection of outliers. We believe this is the immediate next step towards making NeuralFP more accessible on more complex tasks like ImageNet.\n\nThanks for pointing out the connection to K-means clustering, that is an insightful interpretation. We will consider this and see if it sheds more light on our approach, and helps us improve NeuralFP.\n", "title": "Response to Reviewer 3"}, "r1e6HZUHaX": {"type": "rebuttal", "replyto": "SJekyhCctQ", "comment": "We thank all reviewers for their thoughtful comments and reviews. We would like to briefly summarize and highlight our contributions here, and address specifics in individual comments to the reviewers.\n\nSome merits of our contribution we would like to highlight:\n\n1. NFP is a blackbox defense, i.e. it is attack agnostic. To the best of our knowledge, this is the first such defense that does not use adversarial examples to learn to distinguish between real and adversarial images, but rather relies on the training data alone and still manages to achieve near perfect detection in the experiments described. \n\n2. NFP does not require extensive fine-tuning to be effective. Indeed, our experiments show that NFP is effective against the strongest state-of-the-art attacks over a wide range of hyperparameters.\n\nDespite the lack of formal guarantees, we present an abundant amount of empirical evidence that our method is robust against a full-range of the strongest available attacks in the literature. In sum, NFP serves as a strong baseline for developing stronger attacks. In turn, this can eventually lead to better defenses.\n\n3. NFP is a promising step towards the problem of detecting and defending against  adversarial attacks with unbounded perturbations (we test SPSA, FGSM with eps=0.25 for CIFAR-10) -- particularly, when the fingerprint is not known to the adversary (for e.g., consider the challenge https://github.com/google/unrestricted-adversarial-examples).", "title": "Summary and general comments"}, "Bkehwiggpm": {"type": "rebuttal", "replyto": "SJxOmdee6X", "comment": "We have shared one set of well performing model weights, the defense parameters in the link below (and also, some adversarial examples we generated as well). \n\nhttps://www.dropbox.com/sh/0ow62124skbzy1y/AABOHL0Y0EeAdEmPfHItpOt_a?dl=0 \n\n\nThank you for the comment!  We are actively working with the reviewer to get them to be able to run our code, and will support anyone else who wishes to do the same during the review process.", "title": "Thanks! We are committed to openness about our defense! "}, "Syg8JVYkTm": {"type": "review", "replyto": "SJekyhCctQ", "review": "This paper proposes a new technique for detecting adversarial examples by introducing \"fingerprints\" into the landscape while training, and exploiting the fingerprints at test time to detect adversarial examples. The idea is novel and the paper is well-written, but concerns about gradient masking prevent me from recommending acceptance just yet.\n\nPositives: \nThe paper is extremely well-written, and the approach is clear and presented well. The authors also clearly put significant effort into the evaluation, and accurately/consistently describe threat models that they consider. The approach is also clearly novel, and is interesting. \n\nConcerns:\nMy biggest concern is that this detection mechanism masks gradients in its loss function. The two reasons I strongly believe this is the case are (a) Figure 5 and (b) the authors themselves state that their loss is highly non-convex and that no gradient-based method may be able to find a solution. This, however, does not guarantee robustness (see [1] for why such \u201cunfriendly\u201d landscapes can usually be circumvented)\n\nSome concrete evaluation concerns and experiments that the authors can run to alleviate them:\n- Figure 5 shows adversarial robustness even against eta = 0.25---at this value of epsilon, the attacker should be able to create realistic images of other classes (even without PGD), so this suggests that the loss is somehow making examples hard to find rather than removing them. The authors should address this issue.\n- Showing that at a sufficiently high eta attacks start to succeed is also useful\n- Running SPSA, CarliniL2-FP, and PGD for *many* more iterations and using *many* more steps for binary search (right now it looks like the binary search is looking in a space of size 10^6 with 10 steps, which only has a granularity of about 5k, which means you never see any value < 5k in a bisection search, which casts into doubt all of these results)\n- The AUC should monotonically degrade with eta (this is another indication the attacks might not be running for long enough)\n- The method does not seem to be specific to L-infinity constraint. To this end, a version of Figure 5 in the L2 case would be extremely useful in understanding the detection method.\n\nI also apologize if some of these concerns about gradient masking seem unsubstantiated; that said, I tried to run the code given in the paper, but got several OOM and other errors (utils modules not found, and PyTorch deprecations), even on a machine with 8 12GB-memory GPUs. If the authors can provide instructions for running the code I will be happy to test it and alleviate some of my own concerns. I also tried to reimplement the approach, but did not manage to finish before the review deadline. If I am able to reimplement the approach I will update my review accordingly.\n\nSome smaller comments on the paper:\nA consolidated set of tables for attack parameters in an appendix is needed\n- Page 4 last paragraph line 4 find the subset that \u201csatisfies\u201d instead of \u201csatisfy\u201d\n- Page 5 paragraph 1 line 1 defender,for needs a space before for\n- Page 5 paragraph right before theorem 1 last line Here, for detection needs a , after detection \n- Page 7 paragraph 1 line 2 (2 hidden layers the 2 should be written two\n- Page 7 second last paragraph line 2 \u201cis chosen\u201d instead of \u201care chosen\u201d\n- Page 7 last paragraph line 2 \u201cacross attacks\u201d needs a , after \n- Page 9 table 6 label line 2 \u201cdoes not shown\u201d should be \u201cshow\u201d instead of \u201cshown\u201d \n- Page 9 last line \u201cmeasure of robustness\u201d remove \"of\"\n\n[1] https://arxiv.org/pdf/1802.00420.pdf", "title": "Review", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Bkg6qVsa2X": {"type": "review", "replyto": "SJekyhCctQ", "review": "This paper proposes a method for the detection of adversarial examples via what the authors term \"neural fingerprinting\" (NeuralFP). Essentially, a reference collection of perturbations are applied to the training data so as to learn the effects on the classification decision. The premise here is that on average, normal examples from a common class would have similar changes in the classification decision when reference perturbations are applied, whereas adversarial examples (particularly those off the local submanifold) may have a markedly different set of changes from what was expected for the targeted class. These reference perturbations as well as the anticipated output perturbations together form the \"fingerprints\".\n\nTo measure the difference between observed outputs and fingerprints, the average (squared?) Euclidean distance is used. Given a fixed set of input fingerprints (presumably chosen so as to provide coverage of the range of possible perturbation directions), the authors use the distance formula as a regression loss (\"fingerprint loss\") to train the choice of output fingerprints. Although the authors do not explicitly state it this way, this secondary training objective encourages a K-Means style clustering of output perturbations where the output fingerprints serve as cluster representatives. \n\nThis learning formulation is to my mind is both very innovative and extremely effective, as demonstrated by the authors' experimental results. Their experiments show superlative performance (near perfect detection!) against essentially the full range of state-of-the-art attacks. They give careful attention to the mode of attack, and show excellent performance even for adaptive white-box attacks, in which existing attack methods are given the opportunity to minimize the fingerprint loss.\n\nThe presentation of the paper is excellent - clear, well-motivated, and detailed, with careful attention given to experimental concerns such as the choice of perturbation directions (the recommendation is to choose them at random), and the number of fingerprints to pick.\n\nOverall, the reported results are so good, and the approach so convincing, that one wonders what the weaknesses of the approach might be (if any). Questions that do come to mind are:\n* Can an adversarial strategy can be developed that could execute a successful attack while minimizing the fingerprint loss. \n* Another issue is whether the NeuralFP would work on more challenging data sets where the classes are highly fragmented - at what rate would the benefits of NeuralFP fade as the classification performance degrades? \n* What happens to performance if the perturbation directions are chosen so as to better conform with the local sub-manifolds... would fewer perturbations be required? (It would seem that reducing the number of perturbations needed could have a significant effect on training time.)\n\nOverall, this is a very strong and important result, fully deserving of acceptance.\n\nP.S. Two sets of typos that need attention:\n* In Equation 3, the Euclidean norm is taken. In Equation 5, the squared Euclidean norm is taken. Presumably, one of these is a typo. Which?\n* In the definition of delta-min and delta-max in the first paragraph of Section 2.2, y-hat should be w-hat.\n\n", "title": "Innovative perturbation-based learning strategy leads to very impressive performance in adversarial detection", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "B1x7tfDT27": {"type": "rebuttal", "replyto": "B1eAQp4a27", "comment": "Thanks for the suggestions for sanity checks. We will get back in more detail on this, but some initial comments are below:\n\n1. During our initial tests we have tried this for FGSM. It turns out that the perturbations become nearly indistinguishable  (AUC-ROC ~~0.5-0.6) but the adversarial success rate drops to 0. We will try this for the other attacks (and FGSM again to confirm) and get back shortly. \n\nAttacking with only \\gamma*L_{fp} is loosely equivalent to choosing a very large coefficient for L_fp (at which point the misclassification starts to fail).  \n\n2. We have tried detecting random images (every pixel drawn uniformly) and the AUC-ROC was ~~100%. We will get back on the detection of gaussian noise with varying eta shortly. ", "title": "Reply to Suggestions"}, "BJeg3ZP8hm": {"type": "rebuttal", "replyto": "Skldi0wrhm", "comment": "Some additional information for the OP and the interested reader:\n\nThe SPSA attack is designed for when the gradients do not point in useful directions -- essentially, to check for gradient obfuscation. Note that our method uses no information about the adversarial examples or the attack mechanism during train, so things like label leaking cannot even occur. For SPSA, we use the cleverhans implementation and the hyper-parameters we test against are:\ndelta=0.01\nlearning rate=0.01\niterations=100\n\n(Same as the parameters from the SPSA [3] paper used to attack the CIFAR-10 dataset). \n\n[3] https://arxiv.org/abs/1802.05666 (ICML'18)", "title": "SPSA parameters"}, "Skldi0wrhm": {"type": "rebuttal", "replyto": "HJeJ4hUr2Q", "comment": ">It's concerning to me that the attack still is failing even at very large epsilons of 64/255 on CIFAR-10. No prior defense has achieved \n>robustness at even 16/255. It might be useful to verify that the attack does >eventually succeed if epsilon is allowed to be 128/255.  \n>(Not because this is an actual valid attack -- but just \n>to make sure the attack is functioning correctly.)\n\n The optimization problem of finding adversarial examples that both misclassify and have a low-fingerprint loss is non-convex, and no gradient method is guaranteed to find a suitable solution (i.e. good local minimum). Though we do not provide *formal* guarantees, it is not obvious if a \u201ccomputationally tractable functional\u201d attack exists that with increasing epsilon will fool neural fingerprinting. We have evaluated with a variety of currently known attacks, and find that neural-fingerprinting is robust across these attacks. \n\nTo evaluate if the defense is simply based on gradient masking, we evaluate both with the adaptive-SPSA attack and black-box adversarial examples, and observe that the detection AUCs do not degrade. Please note that SPSA is gradient-free. \n\nFurther, we do notice a small degradation in AUCs for the adaptive BIM-b and CW-L2, which suggests that the adaptive attacks are not entirely dysfunctional -- which would have likely been the case if the defense was working solely based on masked gradients. \n\nAnother sign against gradient masking is that one step attacks do strictly worse than iterative attacks, unlike [1] cited above. \n\n>> It is not obvious if \n>>1) there are points that successfully produce a misclassification and have a small fingerprint loss (corresponding to the true label), and \n\n>These points definitely exist: we know that there exists at least one image correctly classified as each class, \n>so given an unlimited perturbation budget, there will always exist an \"adversarial example\" with \n>unbounded distortion.\n\n\nThank you for pointing this out, we agree that with unbounded perturbations, such examples exist. For instance, an unbounded \u201csuccessful attack\u201d could transform a natural image of a dog to a natural image of a cat. But, coming up with an attack that finds such \"successful\" unbounded transformations  is likely hard. \n\nAn intuition for why finding such \u201csuccessful\u201d unbounded transformation is likely hard might be that real images are thought to occupy a very low-dimensional subspace in the high-dimensional input space, and so the space of \u201cgood\u201d perturbations might be a very small fraction of the space of all allowed perturbations that a CW attacker searches over.\n\n>> 2) if the CW attack will find it. \n\n>It may not. But clearly an optimal attack should find it, and so if detection rate is not 0% then we know the \n>attack is performing sub-optimally.\n\nAs stated earlier, an optimal attack may be intractable. For reference, in our experiments for the CW-attack, we used the following settings:\n\nL2_BINARY_SEARCH_STEPS_1 = 9 (corresponding to gamma-1)  \nL2_BINARY_SEARCH_STEPS_2 = 5 (corresponding to gamma-2) \nL2_MAX_ITERATIONS = 1000    \nL2_ABORT_EARLY = True      \nL2_LEARNING_RATE = 1e-2     \nL2_TARGETED = True          \nL2_CONFIDENCE = 0           \nL2_INITIAL_CONST = 1e-3    (gamma-1 initial value)\nL2_INITIAL_CONST_2 = 0.1   (gamma-2 initial value)\n\nMeanwhile, we will run experiments with larger L2_MAX_ITERATIONS to see if there is a significant degradation in AUC-ROC. We welcome suggestions from you (and other readers) for better settings or modifications to consider.\n", "title": "Addressing comments on gradient masking"}, "B1eZuarS3m": {"type": "rebuttal", "replyto": "r1edIXVBhX", "comment": "Thank you for the questions, and catching the missing detail in Figure 5. \n\n\u201cIn Figure 5, it appears that AUC is uncorrelated with the magnitude of the adversarial perturbation. If you increase the perturbation budget to something much larger does the AUC begin to decrease? (Also: what is the dataset for this figure? MNIST?)\u201d\n\nThe dataset for Figure 5 is CIFAR-10 \u2014 we will include this detail in the paper. 0.25 is already a significantly large perturbation, since our pixel values in the input images are scaled from [-0.5,0.5]. We will run experiments with larger perturbations and report the numbers for this shortly.\n\n\u201cThe adaptive CW attack is unbounded, and should always eventually succeed. That is, for any fixed decision threshold that could be used for detection, the attack should always eventually produce an adversarial example that can fool the detector by introducing a very large perturbation. However, it looks like you claim robustness even at very small thresholds (e.g., a 80+% TPR @ 0%FPR). Do you know what is going on here?\u201d\n\nThe adaptive CW attack is not always guaranteed to work for an arbitrary detection rule for an arbitrary point. For example, the adaptive CW attacks formulated in [1,2] do not succeed. However, we do note that the adaptive CW attack fails in [1] due to gradient obfuscation and [2] has not been tested extensively for gradient obfuscation. \n\nFor the adaptive CW attacks we consider, we do not set up a fixed threshold, instead we attempt to misclassify the point and minimize the fingerprint loss corresponding to the *true class*. From all the successful attacks that produce a misclassification during the iterations, we choose the one with the smallest fingerprint loss. \n\nAlso, we would like to clarify the intuition: At smaller thresholds it is actually harder for the attacker to fool the detector. This is because the fingerprint loss under smaller thresholds admits smaller regions while larger thresholds admit larger regions.\n\nThe fingerprints provide a characterization of the data-distribution and reject regions away from the data-distribution corresponding to the training data . It is not obvious if \n1) there are points that successfully produce a misclassification and have a small fingerprint loss (corresponding to the true label), and \n2) if the CW attack will find it. \n\nFor instance, consider Figure 3 \u2014 there exist no \u201cadversarial regions\u201d that produce a misclassification and have a small fingerprint loss corresponding to the true label. We conjecture that the networks from our experiments on the image dataset (MNIST, CIFAR-10\u2026) show similar behavior \u2014 the \u201cadversarial regions\u201d producing both a misclassification and a small fingerprint loss do not exist/are either quite small and difficult to find. \n\nWe have also considered a variant where the adaptive CW attacker can determine both the target-class for the label-misclassification and the fingerprint loss to be minimized \u2014 constrained to both being the same. This attack performed worse than the variant we report in the paper. \n  \nOur source code (link provided in the paper) includes the implementation for the adaptive CW attack discussed in the paper. If you would like to suggest other variants that are interesting, we would be happy to try them out. \n\n[1] https://openreview.net/forum?id=B1gJ1L2aW (ICLR\u201918)\n[2] https://arxiv.org/abs/1807.03888 (NIPS\u201918)\n", "title": "Addressing evaluation questions"}}}