{"paper": {"title": "Countering Adversarial Images using Input Transformations", "authors": ["Chuan Guo", "Mayank Rana", "Moustapha Cisse", "Laurens van der Maaten"], "authorids": ["cg563@cornell.edu", "mayankrana@fb.com", "moustaphacisse@fb.com", "lvdmaaten@gmail.com"], "summary": "We apply a model-agnostic defense strategy against adversarial examples and achieve 60% white-box accuracy and 90% black-box accuracy against major attack algorithms.", "abstract": "This paper investigates strategies that defend against adversarial-example attacks on image-classification systems by transforming the inputs before feeding them to the system. Specifically, we study applying image transformations such as bit-depth reduction, JPEG compression, total variance minimization, and image quilting before feeding the image to a convolutional network classifier. Our experiments on ImageNet show that total variance minimization and image quilting are very effective defenses in practice, in particular, when the network is trained on transformed images. The strength of those defenses lies in their non-differentiable nature and their inherent randomness, which makes it difficult for an adversary to circumvent the defenses. Our best defense eliminates 60% of strong gray-box and 90% of strong black-box attacks by a variety of major attack methods.", "keywords": ["adversarial example", "machine learning security", "computer vision", "image classification"]}, "meta": {"decision": "Accept (Poster)", "comment": "A well written paper proposing some reasonable approaches to counter adversarial images. Proposed approaches include non-differentiable and randomized methods. Anonymous commentators pushed upon and cleared up some important issues regarding white, black and gray \"box\" settings. The approach appears to be a plausible defence strategy. One reviewers is a hold out on acceptance, but is open to the idea. The authors responded to the points of this reviewer sufficiently. The AC recommends accept."}, "review": {"ryi9KVKBG": {"type": "rebuttal", "replyto": "Bk1rU7YSz", "comment": "Thanks for pointing us to this work! We will update our paper to refer to it.", "title": "Thanks"}, "B1gREqS4f": {"type": "rebuttal", "replyto": "HyulgtSVz", "comment": "We agree that security through obscurity is inherently weak and ineffective. However, the defenses that we evaluated, in particular TV minimization and image quilting, are stochastic in nature. This allows the defense to randomize its transformation, and hence prevents the adversary from knowing the exact transformation being applied even if the defense strategy is known. The image quilting defense enjoys the additional property that the patch database used to construct the quilted images can be considered as the secret key, which obeys Kerckhoff's principle: even if the defense strategy is known, the adversary cannot apply the same quilting transformation if he does not have access to the patch database.", "title": "Re: Unreasonable threat model assumptions"}, "S1wXIrVgM": {"type": "review", "replyto": "SyJ7ClWCb", "review": "To increase robustness to adversarial attacks, the paper fundamentally proposes to transform an input image before feeding it to a convolutional network classifier. The purpose of the transformation is to erase the high-frequency signals potentially embedded by an adversarial attack.\n\nStrong points:\n\n* To my knowledge, the proposed defense strategy is novel (even if the idea of transformation has been introduced at https://arxiv.org/abs/1612.01401). \n\n* The writing is reasonably clear (up to the terminology issues discussed among the weak points), and introduces properly the adversarial attacks considered in the work.\n\n* The proposed approach really helps in a black-box scenario (Figure 4). As explained below, the presented investigation is however insufficient to assess whether the proposed defense helps in a true white-box scenario. \n\n\nWeak points:\n\n* The black-box versus white-box terminology is not appropriate, and confusing. In general, black-box means that the adversary ignores everything from the decision process. Hence, in this case, the adversary does not know about the classification model, nor the defensive method, when used. This corresponds to Figure 3. On the contrary, white-box means that the adversary knows everything about the classification method, including the transformation implemented to make it more robust to attacks. Assimilating the parameters of the transform to a secret key is not correct because those parameters could be inferred by presenting many image samples to the transform and looking at the outcome of the transformation (which is supposed to be available in a 'white-box' paradigm) for those samples. \n\n* Using block diagrams would definitely help in presenting the training/testing and attack/defense schemes investigated in Figure 3, 4, and 5.\n\n* The paper does not discuss the impact of the denfense strategy on the classification performance in absence of adversity.\n\n* The paper lacks of positioning with respect to recent related works, e.g. 'Adversary Resistant Deep Neural Networks with an Application to Malware Detection' in KDD 2017, or 'Building Adversary-Resistant Deep Neural Networks without\nSecurity through Obscurity' at https://arxiv.org/abs/1612.01401. \n\n* In a white-box scenario, the adversary knows about the transformation and the classification model. Hence, an effective and realistic attack should exploit this knowledge. Designing an attack in case of a non differentiable transformation is obviously not trivial since back-propagation can not be used. However, since the proposed transformation primarily aim at removing the high frequency pattern induced by the attack, one could for example design an attack that account for a (linear and differentiable) low-pass filter transformation. Another example of attack that account for transformation knowledge (and would hopefully be more robust than the attacks considered in the manuscript) could be one that alternates between a conventional attack and the transformation.\n\n* If I understand correctly, the classification model considered in Figure 3 has been trained on original images, while the one in Figure 4 has been trained on transformed images. However, in absence of attack, they both achieve 76% accuracy. Is it correct? Does it mean that the transformation does not affect the classification accuracy at all?\n\n\nOverall, the works investigates an interesting idea, but lacks maturity to be accepted. Therefore, I would only recommend acceptation if room.\n\nMinor issues:\n\nTypo on p7: to change*s*\nClarify poor formulations:\n* p1: 'enforce model-specific strategies that enforce model properties such as invariance and smoothness via the learning algorithm or regularization schemes'. \n* p1: 'too simple to remove adversarial perturbations from input images sufficiently'", "title": "Valuable idea but immature contribution.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "Sk47YIYlM": {"type": "review", "replyto": "SyJ7ClWCb", "review": "Summary: This works proposes strategies to make neural networks less sensitive to adversarial attacks. They consist into applying different transformations to the images, such as quantization, JPEG compression, total variation minimization and image quilting. Four adversarial attacks strategies are considered to attack a Resnet50 model for classification of Imagenet images.\nExperiments are conducted in a black box setting (when the model to attack is unknown by the adversary) or white box setting (the model and defense strategy are known by the adversary).\n60% of attacks are countered in this last most difficult setting.\nThe previous best approach for this task consists in ensemble training and is attack specific. It is therefore pretty robust to the attack it was trained on but is largely outperformed by the authors methods that manage to reduce the classifier error drop below 25%.  \n\nComments: The paper is well written, the proposed methods are well adapted to the task and lead to satisfying results.\n \nThe discussion remarks are particularly interesting: the non differentiability of the total variation and image quilting methods seems to be the key to their best performance in practice.\nMinor: the bibliography should be uniformed.", "title": "Well argumented, solid work", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJzYnEqef": {"type": "review", "replyto": "SyJ7ClWCb", "review": " The paper investigates using input transformation techniques as a defence against adversarial examples. The authors evaluate a number of simple defences that are based on input transformations such TV minimization and image quilting and compare it against previously proposed ideas of JPEG compression and decompression and random crops.  The authors have evaluated their defences against four main kinds of adversarial attacks.\n\nThe main takeaways of the paper are to incorporate transformations that are non-differentiable and randomised. Both TV minimisation and image quilting have that property and show good performance in withstanding adversarial attacks in various settings. \n\nOne argument that I am not sure would be applicable perhaps and could be used by adversarial attacks is as follows: If the defence uses image quilting for instance and obtains an image $P$ that approximates the original observation $X$, it could be possible to use a model based approach that obtains an observation $Q$ that is close to $P$ which can be attacked using adversarial attacks. Would this observation then be vulnerable to such attacks? This could perhaps be explored in future.\n\nThe paper provides useful contributions in forming model agnostic defences that could be further investigated. The authors show that the simple input transformations advocated work against the major kind of attacks. The input transformations of TV minimization and image quilting share varying characteristics in terms of being sensitive to various kinds of attacks and therefore can be combined. The evaluation is carried out on ImageNet dataset with large number of examples.", "title": "Review", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HkKhk67Nf": {"type": "rebuttal", "replyto": "rkqFThmEM", "comment": "Apologies for leaving out this detail! We used an overlap of 2 pixels between the 5x5 patches. We will update the paper with this information.", "title": "The overlap size is 2 pixels"}, "rJX6P09fz": {"type": "rebuttal", "replyto": "S1wXIrVgM", "comment": "Thank you for your insightful comments on our work, which have been very helpful in improving the paper!\n\n* The black-box versus white-box terminology is not appropriate...\n\nAs several public comments have pointed out, the white-box terminology can be misleading. Some of our experiments are performed in a \"gray-box\" setting in which the adversary has access to the network parameters, but not to the quilting database that acts as a kind of \"secret key\". We believe that this gray-box setting is of practical interest because the quilting process is stochastic and because the adversary never directly observes the quilted images themselves: this makes it very difficult for the adversary to exactly reproduce the quilted images that the defender produces. Per your suggestion, we have clarified the learning-setting terminology in the revised version of the paper.\n\n* Using block diagrams would definitely help in presenting the training/testing and attack/defense schemes investigated in Figure 3, 4, and 5.\n\nPer your suggestion, we have added block diagrams clarifying the workflow of our attack/defense schemes in the revised version of the paper.\n\n* The paper does not discuss the impact of the defense strategy on the classification performance in absence of adversity.\n\nThe first row of Tables 1 and 2 present the accuracy of various defenses on non-adversarial images (\"no attack\"). In Figures 3, 4 and 5, the y-axis value corresponding to normalized L2-dissimilarity of 0 corresponds to the accuracy on non-adversarial images. We have emphasized this point in the table and figure captions in the revised version of the paper.\n\n* The paper lacks of positioning with respect to recent related works, e.g. 'Adversary Resistant Deep Neural Networks with an Application to Malware Detection' in KDD 2017, or 'Building Adversary-Resistant Deep Neural Networks without Security through Obscurity' at https://arxiv.org/abs/1612.01401.\n\nThank you for pointing out these references, which we were unaware of at the time of submission. Both approaches are similar to our defenses in the sense that they focus on non-differentiable, stochastic transformations. Having said that, there are also substantial differences between our study and those related works. The first paper relies on LLE to represent data points as a linear combination of nearest neighbors: this approach may certainly be suitable for certain kinds of data, but is unlikely to work very well in extremely high-dimensional spaces such as the ImageNet pixel space. The second paper's approach of randomly removing blocks of pixels is related to our image-cropping baseline defense, which is one of our baselines. We have included positioning with respect to these works in the revised version of the paper.\n\n* In a white-box scenario, the adversary knows about the transformation and the classification model. Hence, an effective and realistic attack should exploit this knowledge...\n\nIn white-box settings, it may, indeed, be possible to devise attacks that are tailored towards a particular defense. In our work, we have tried to make the development of such attacks non-trivial by making our defenses non-differentiable and stochastic. Having said that, it may certainly be possible to devise attack strategies that are successful nevertheless (such as the strategy sketched in our response to AnonReviewer3). We leave the investigation of attacks that are tailored to our defenses to future work.\n\n* If I understand correctly, the classification model considered in Figure 3 has been trained on original images, while the one in Figure 4 has been trained on transformed images. However, in absence of attack, they both achieve 76% accuracy. Is it correct? Does it mean that the transformation does not affect the classification accuracy at all?\n\nThe 76% accuracy is obtained by a convolutional network that is trained and tested on images on which no defense (i.e., input transformation) is applied. The \"no defense\" baseline is this exactly the same in both Figures 3 and 4. For defenses such as TV minimization and quilting, the accuracy on non-adversarial images is lower (both in Figure 3 and 4), which shows that the transformations, indeed, do negatively impact classification accuracy on non-adversarial images.", "title": "Re: Valuable idea but immature contribution."}, "r1XVPRqzM": {"type": "rebuttal", "replyto": "SJzYnEqef", "comment": "Thank you for your insightful comments, and positive evaluation of work! \n\nRegarding model-based approaches for attacking our quilting defense: we agree this may the most viable option for attacking our defense. As you suggest, it may be possible for the adversary to construct its own patch database, and use it to construct quilted images that may be sufficiently similar to the quilted image created using our \"secret database\". The remaining issue for the adversary is then to backpropagate gradients through the quilting transformation: the adversary may be able to do this by training a pixel-to-pixel network that learns to produce the quilted image given an original image, and using this network to approximate gradients. We intend to investigate such attack approaches in future work. We have updated our paragraph describing future work to reflect this.\n", "title": "Re: Review"}, "SyNlDC9GM": {"type": "rebuttal", "replyto": "Sk47YIYlM", "comment": "Thanks for your positive evaluation of our paper! Per your suggestion, we have updated the bibliography entries to make them uniform.", "title": "Re: Well argumented, solid work"}, "ry4zt5Vyf": {"type": "rebuttal", "replyto": "HyOUaNzkz", "comment": "The transformations we studied, indeed, all have some hyper-parameter that controls how lossy the transformation is, and can be used to trade off clean accuracy and adversarial accuracy. These hyper-parameters are: crop ratio for the crop-rescale transform, pixel drop rate and regularization parameter for total variation minimization, and patch size for image quilting. For instance, using a larger patch size in image quilting will remove more of the adversarial perturbation (which likely leads to higher adversarial accuracy), but also affects clean images which deteriorates clean accuracy. \n\nWe selected hyper-parameter that achieve high adversarial accuracy, but one may choose to set them differently depending on the user's needs. We surmise it may be possible to achieve high clear and adversarial accuracy by ensembling predictions over multiple hyper-parameter settings, but further experimentation is needed to confirm this hypothesis.", "title": "Re: Performance on clean examples"}, "H1dO8OqC-": {"type": "rebuttal", "replyto": "BkDPS_tAb", "comment": "Thank you for your comment! We were unaware of [1] at the time of submission. We will include it as a citation. Based on our understanding, [2] applies Carlini-Wagner's attack against a target loss that averages the prediction over multiple fixed models with random network weight dropout rather than random pixel dropout, but the two techniques are certainly related.\n\nIn regards to attacking the transformation defenses, independent of [1], we have observed that it is possible to produce adversarial examples that are invariant to crop location and scale. By randomly selecting a crop of size 135x135 each iteration to compute the loss function for CW-L2, the resulting adversarial examples can reduce the accuracy of crop defense to 30% at an average L2-dissimilarity of 0.045. \n\nEnhancing the attack with random pixel dropping can, indeed, reduce the effectiveness of TV minimization significantly. Using a pixel dropout mask with drop probability 0.1 each iteration, CW-L2 can reduce the accuracy of TV minimization defense to 9% at an average L2-dissimilarity of 0.06. However, we do not have a good idea of how to backpropagate through the quilting transformation, as the construction is stochastic and non-differentiable in nature. Nevertheless, section 5.5 of our paper does show that it is possible to successfully attack the quilting defense in some cases even without knowledge of this transformation; we presume because the convolutional filters reveal some information about the patch database used.\n\nThe attacks we use for the white-box setting do not have knowledge of the defense mechanism used. \n\nWe will clarify these points in the paper.", "title": "Re: White-box attacks with knowledge of defense transformation"}, "HktFIuqC-": {"type": "rebuttal", "replyto": "SJ67F79R-", "comment": "For our experiments, we selected four attacks that we believe are representative of the large number of attacks that people have proposed. Since PGD is related to I-FGSM (the main difference between the two is the projection step after every iteration), we expect that our defenses will have similar performance against PGD. \n\nWe performed a small experiment with PGD to confirm this. We created attacks with an average L2-dissimilarity of 0.06, and find that the accuracy of our defenses against white-box I-FGSM/PGD attacks are: no defense 0%/0%, crop ensemble 44%/48%, TVM 29%/36%, and image quilting 35%/37%. These results suggest that the effectiveness of our defenses against PGD is similar to their effectiveness against I-FGSM. We will add these results to the paper.", "title": "Re: PGD evaluation missing"}, "HJkZJjhAb": {"type": "rebuttal", "replyto": "Hk1TeF9RZ", "comment": "We agree that the terminology of white-box is ambiguous, in particular, in the context of randomized defenses (is the adversary allowed access to the random seed?). In cryptography, Kerckhoffs's principle prescribes the use of a \"secret key\". One could make the argument that the patch database used by image quilting is an implementation of such a secret key. To the best of our knowledge, there is no consensus yet in the community yet on what a \"secret key\" may and may not contain in the context of defenses against adversarial examples.\n\nIn our current paper, \"white-box\" refers to access the model parameters; other parameters (random seed, patch database) are considered to be part of the secret key. We will add a section to our paper clarifying the terminology.", "title": "Re: White-box attacks with knowledge of defense transformation"}, "ByXm1shC-": {"type": "rebuttal", "replyto": "ryuYj65C-", "comment": "1) We have performed experiments with our defenses against PGD; see our previous comment for the results of those experiments, which we will add to the paper. Our results do not suggest substantial differences in the effectiveness of our defenses between PGD and I-FGSM. \n\n2) Our proposed defenses are not intended to compete with adversarial-training-based defenses: the two defenses can be used together and are likely to be complementary. We chose ImageNet to conduct our experiment for the following two reasons:\n\n- The interest in adversarial examples mainly stems from the concern of use of computer vision models in real-world applications such as self-driving cars and image-classification services. In these settings, the input to the model has high resolution and diverse content; ImageNet more closely resembles this scenario than MNIST or CIFAR.\n\n- Defending a model that performs classification on ImageNet is inherently more difficult than defending a MNIST or CIFAR classification model, since the model must output very diverse class labels and the model's prediction is often uncertain. Moreover, the input dimensionality for ImageNet is much higher (~150000 compared to 768 for MNIST and 3072 for CIFAR-10), which gives the attacker much more maneuverability.\n\n3) We agree that the terminology of white-box is ambiguous, in particular, in the context of randomized defenses (is the adversary allowed access to the random seed?). In cryptography, Kerckhoffs's principle prescribes the use of a \"secret key\". One could make the argument that the patch database used by image quilting is an implementation of such a secret key. To the best of our knowledge, there is no consensus yet in the community yet on what a \"secret key\" may and may not contain in the context of defenses against adversarial examples. In our current paper, \"white-box\" refers to access the model parameters; other parameters (random seed, patch database) are considered to be part of the secret key. We will add a section to our paper clarifying the terminology.", "title": "Re: Three comments"}}}