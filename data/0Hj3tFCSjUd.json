{"paper": {"title": "Energy-based View of Retrosynthesis", "authors": ["Ruoxi Sun", "Hanjun Dai", "Li Li", "Steven Kearnes", "Bo Dai"], "authorids": ["~Ruoxi_Sun2", "~Hanjun_Dai1", "~Li_Li8", "~Steven_Kearnes1", "~Bo_Dai1"], "summary": "The paper proposed new energy-based methods for retrosynthesis.", "abstract": "Retrosynthesis\u2014the process of identifying a set of reactants to synthesize a target molecule\u2014is of vital importance to material design and drug discovery. Existing machine learning approaches based on language models and graph neural networks have achieved encouraging results. However, the inner connections of these models are rarely discussed, and rigorous evaluations of these models are largely in need. In this paper, we propose a framework that unifies sequence- and graph-based methods as energy-based models (EBMs) with different energy functions. This unified point of view establishes connections between different models and identifies the differences between them, thereby promoting the understanding of model design. We also provide a comprehensive assessment of performance to the community. Moreover, we present a novel \u201cdual\u201d variant within the framework that performs consistent training over Bayesian forward- and backward-prediction by constraining the agreement between the two directions. This model improves the state of the art for template-free approaches where the reaction type is unknown and known.\n", "keywords": ["Applications", "Retrosynthesis", "Energy-based Model"]}, "meta": {"decision": "Reject", "comment": "Before the discussion phase nearly all reviewers had doubts about the comparison of the current work with state-of-the-art works (notably Yan et al., 2020, RetroXpert, and GraphRETRO). The authors then compared with these works and emphasized that these works rely on hand-crafted features. They argue that the fairest comparison is the one where each method uses the same sort of features during train/test time. This is because in certain real world settings we may not have accurate estimates of such features (e.g., atom mappings, templates, reaction centers). However, in the revised version of the paper the authors did not adhere to this concept of fair comparison in Table 4 of Appendix A.4. Here their method uses reaction centers as input while baselines do not. While the authors claimed that the comparison here was designed to show how reaction centers provided as input improved performance, this doesn't seem like a good way to show it: to isolate the improvement due to reaction center inputs you should fix everything else, i.e., the rest of the method. \n\nApart from the above contradiction, I buy the arguments of reviewers that distinguishing between methods that use hand-crafted features and those that do not is not a meaningful distinction. One can apply atom-mapping or reaction center discovery algorithms as data preprocessing before applying other methods. Ablation studies where such preprocessing is added or removed are interesting, but it is completely fair for any method to use such preprocessing before applying their method, it is up to the modeller. \n\nI would have argued for acceptance had the authors either (a) just included results from SOTA methods (one, RetroXpert was published 1 month after the ICLR submission deadline), and/or (b) reran their approach with such preprocessing. However the authors ended up hurting the submission by emphasizing a difference between using handcrafted features and not, then contradicting their experimental setup in Table 4.\n\nThis is a good paper, but I agree it is not ready to be accepted at ICLR. I recommend the authors do the following: (a) use any preprocessing they want for their method and compare with the state-of-the-art, (b) if they want they can run their method without any preprocessing as an interesting ablation study, (c) remove Table 4 (as (b) already does this type of an ablation study), (d) describe recent work through the lense of EBM, (e) resubmit to a strong ML conference. The new submission will be much stronger."}, "review": {"jHp9SfIkHw": {"type": "review", "replyto": "0Hj3tFCSjUd", "review": "### Summary of the paper\nThis paper proposes an energy based model (EBM) for retrosynthesis. The best model (dual model) leverages the duality of retrosynthesis and reaction prediction. The EBM contains three factors: prior on reactants $p(X)$, forward reaction probability $p(y | X) and backward posterior $P(X|y)$. The duality loss penalizes the KL divergence of the two directions. The forward predictor is trained on a mixture of original data and samples drawn from the backward predictor. The dual model shows improvement over template-based and template-free baselines.\n\n### Strength\n1. The dual model is novel and interesting. EBM provides a unified view of forward and backward reaction prediction. It will be interesting to see how this improves the forward prediction performance.\n2. EBM is a flexible framework, which can be applied to both template-based and template-free approaches.\n\n### Weakness (and questions)\n1. Clarity: The paper describes EBM view of many methods, ranging from sequence and graph based methods. As a result, each subsection is too sketchy and lots of details are missing. For example:\n 1. For Dual-TB (ours), what is the exact model architecture? I understand that the candidates are generated from reaction templates, but what's the parametrization of $p_\\alpha, p_\\gamma, p_\\eta$? Is it GLN?\n 2. For Dual-TF (ours), what is the model architecture? Is it transformer?\n 3. What is the augmented USPTO 50k? Is it randomized SMILES? For Dual-TB (ours), is it trained on augmented USPTO 50k? Why SMILES randomization matters for templated based methods?\n 4. $X$ is a set of compounds, how do you generate a set of compounds in an order-invariant manner?\n 5. For duality constraint, you sample from backward predictor using beam search. Why not the other way around? Why not sample from forward predictor and train your backward predictor on the additional samples?\n\n2. At inference time, it is hard to sample from EBM. Therefore, authors propose to rank the candidates generated from other models (reaction templates or transformers). This really limits the performance (and applicability) of the approach. To my knowledge, you can do MCMC based on Langevin dynamics to sample from EBM. Is this not possible for retrosynthesis?\n\n3. The best model is this paper actually performs much worse than the state-of-the-art models. For instance, RetroXpert (Yan et al., NeurIPS 2020) achieves 65.6% top-1 accuracy (reaction type unknown) and 70.4% top-1 accuracy (reaction type known). This is much better than the dual model (55.2%, 67.7%). Somnath et al, 2020 also achieves 64.2% top-1 accuracy (reaction type unknown), which is much higher than dual model (55.2%).\n\n### Overall evaluation\nI vote for weak reject of the paper, primarily due to the weaker result and lack of clarity. I believe the dual formulation can be applied to these above state-of-the-art models (if code is available). I suspect the weak result is primarily due to the base model (e.g., transformer). I am happy to adjust my score if there are stronger results and the clarity can be improved. One suggestion for clarity is to put perturbed / bidirectional models into appendix since they are not helpful anyway... \n\n### Post Rebuttal\nI would like to thank authors for their response. I think the paper needs to be improved further to get accepted. I do believe that the proposed dual learning method is promising, but empirical evaluation is still lacking. So my review score stays the same.\n\n[1] Yan et al., RetroXpert: Decompose Retrosynthesis Prediction like A Chemist, NeurIPS 2020.\n\n[2] Somnath et al., Learning Graph Models for Template-Free Retrosynthesis, 2020", "title": "Clarity needs to be improved", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "9HHX_D2k10c": {"type": "rebuttal", "replyto": "bqRvrAv0VSy", "comment": "We are sorry for the confusion. We appreciate both of your great questions. Please see our clarification below. We uploaded a new revision with one edit marked in magenta on page 13.      \n\n**Q1: \u201cDual-TB (Table 1) performance is still lower than RetroXpert and GraphRETRO.\u201d**\n\nWe did not have time to touch template-based experiments (Dual-TB). We did not do experiments on adding reaction centers to Dual-TB (Dual-TB part is not updated). We only use Template-free setup to show the effect of incorporating \u201creaction center\u201d.  But based on our previous experiences under the same setup, Dual-TB usually performs similar with Dual-TF but a bit better, as they have the same training procedure (Only difference between the two are how to propose candidate list during testing). Our guess is Dual-DB with reaction center known should be similar with Table 4 Appendix A4, if not a bit better.\n\nTo be clear, Dual-TB does not use template or reaction centers during training. Dual-TB only uses \u201ctemplates\u201d to propose a list of candidates during testing. So Dual-TB is template-free during training. \n\nWe appreciate RetroXpert and GraphRETRO\u2019s impressive results. Because of their great results and our A4 appendix results, we are convinced that compared with directly matching templates (\u201ctemplate-based methods\u201d), reaction centers actually are more accessible/easier features for the algorithm to learn (directly points the key information without introducing noise from templates). Just for this paper, we are more interested in the modeling part of retrosynthesis than chasing SOTA by using additional information during training.  \n\n**Q2:  \u201cYour new experiments in Appendix A4 (70.1% and 73.4%) are NOT head-to-head comparisons with RetroXpert or GraphRETRO\u201d.**\n\nYou are absolutely correct -- Appendix A4 experiments are using \u201ctrue reaction centers\u201d, but RetroXpert or GraphRETRO has to infer them, hence not head-to-head comparison. We apologize for the misunderstanding; **our intent was simply to demonstrate that adding this information would improve performance (given as an upper bound) rather than to serve as a head-to-head comparison. We think the upper bound is still informative to show the features are critical for improving accuracy.** As we stated in Appendix in our previous edition (third to last paragraph). \n\n> \u201cAlthough in the above experiments, we are given the reaction center, that is we do not infer them, Table 4 still serves well as a proof of concept that incorporating useful handcrafted chemistry features can significantly boost accuracy regardless of model design.\u201d \n\nDue to the limited time of rebuttal, we do not have time to infer the reaction center first and use the inferred ones to our experiments. **We can consider 70.1% and 73.4% to be upper bounds on our experiments when we can infer the reaction center perfectly**; we made this clear in the newly revised paper. However, we would like to also emphasize that inferring centers (using additional information \u201creactor center\u201d as supervision during training) is not the direction we would like to pursue as these models rely on more information than the \u201cfull automatic template-free\u201d models we are proposing here. The experiments we did in A4 appendix are just to alert the community and also prove to reviewers that we should compare models with the access of same information, not to claim SOTA. \n\n**Q3: \u201cWith true reaction center, your Dual-TF with true reaction center given (70.1%, 73.2%) is not template free anymore\u201d**\n\nSorry for not making the following points clear in the rebuttal, in our revised paper: \n**We do not claim 70.1% is the performance of our Dual-TF**. Please kindly refer to our previous revised paper (Table 1), 55.4% is still our evaluation (in black). We put 70.1% in the appendix, not the main text, as an \u201cauxiliary study\u201d proof of concept that gives an upper bound on performance for the case where we are incorporating reaction centers as additional features.   We think the upper bound is still informative to show the additional features are critical for improving accuracy.\n\nWe did not have time to do experiments on adding other chemistry features to our dual model, such as \u201cproduct side of template\u201d which are demonstrated to be useful in RetroXpert.  Doing so may boost accuracy for dual-semi-template-based methods even further. \n\nMany of the above experiments we did not do is due to rebuttal's time constraints, and also more importantly they are not the main focus of our paper: full automatic template free methods. \n\nThank you again for the great review and questions! We are happy to answer more questions. \n", "title": "Re R2: \"Experimental setup is confusing\""}, "tbAjwNlv2f": {"type": "rebuttal", "replyto": "74mJtnpyS1e", "comment": "Thank you for the insightful comments and helpful suggestions! We have loaded a revised paper to address the comments, and the changes are highlighted in blue. Please see our response: \n\n**Q1: Ablation Experiments on with or without dual constraint.** \n\nThank you for the invaluable suggestions.  Your suggestions have led to precious insights. We have added the following ablation experiments in Appendix A6 of the revised paper: \n\na. Dual model (with dual constraint):  (eq6) $E_{dual}[log p(X) + log\u2061p(X|y)] + log\u2061p(y|X)$\n\nb. Without dual constraint: $log p(X) + log\u2061p(X|y) + log\u2061p(y|X)$  \n   \nc. Without prior: $log\u2061p(X|y) + log\u2061p(y|X)$     \n\nd. Only backward:  $log\u2061p(X|y)$\n\nThe results in table 5 (Appendix A6) show that every component of the dual loss contributes to the performance positively. For top 1 accuracy: (a) dual 67.7% ;  (b) 67.0%;  (c) 66.1%; (d) 60.9%. Comparing (a) and (b) is the contribution of dual constraint, which is 0.7%. It is harder to improve upon a higher accuracy range (i.e. (b) ) than a lower range (i.e. (d) ). \n\n**Q2: Please cite relevant papers**\n\nThank you for the great suggestions. We have cited all the suggested papers in the revised version.  Segler Nature 2018,  Segler & Waller 2017,  Strieth-Kalthoff et al 2020,  Johansson et al 2020, Yan et al 2020, Somnath et al 2020, Guo et al 2020, Coley et al Science 2019; Arus-Pous et al 2019. \n\nIn our first edition, we have already cited Segler & Waller 2017 (\u201cNeuralSym\u201d in Table 1), and Segler Nature 2018  (Introduction).   Segler Nature 2018 focuses on multiple-step retrosynthesis. We focus on the single-step retrosynthesis, which is a critical building block of the multi-step process. We have included a discussion about Segler & Waller 2017 in the result section. We will include more discussions with other methods in the camera-ready edition. \n\n**Q3: Table 3. With Type, do you mean the reaction type is given?**\n\nYes\n\n\n**Q4:  eq13 should be a centered dot for multiplication, not a dot.** \n\nYes. Thanks. Fixed. \n\n\n**Q5: Please change fonts for your figures.** \n\nApologize. We changed the fonts to \u201ccourier\u201d. \n\n\n**Q6: Not SOTA, compared with RetroXpert (Yan et al,  NeurIPS 2020) and GraphRETRO (Somnath et al, 2020) [from other reviewers].** \n\nPlease kindly refer to the response to all reviewers with the title \"Reply to R1, R2, R3, R4: Not SOTA...\u201d.\n\nWe have a different setup. RetroXpert and GraphRETRO are not template free methods. The lower results are due to the fact that we are not using extra chemical features. If we add those features to the dual model, we get better results than RetroXpert.  Dual: 70.1%; 73.2% RetroXpert: 65.6%; 70.4%  (Table 4 in Appendix A. 4 of our revised paper). \n\n\n \n\n\n\n\n\n\n", "title": "Reply to R1"}, "ziuy6FSvI3F": {"type": "rebuttal", "replyto": "_99sSejhs7", "comment": "Thank you for the helpful suggestions. We thank the reviewer for the positive feedbacks and for highlighting the advantages of our work. We have uploaded the revised paper, and the changes are highlighted in blue. \n\n\u201cMoving some mathematical details into supplementary\u201d is a very helpful suggestion,  as it can (1) make the paper more readable, (2) allow in-depth discussion, and (3) avoid distracting the readers from unnecessary details. \n\nWe plan to do so for the camera-ready version. However, currently, we are debating on which parts should be put into the appendix, as different readers seem to be interested in different parts.  \n\n", "title": "Reply to R4: "}, "U356aK-iAO": {"type": "rebuttal", "replyto": "Qhukq2uBvct", "comment": "Thank you for your prompt reply and helpful question!\n\n### Q1: Can researchers run an atom mapping algorithm on any real-world dataset and get good atom mapping with the same quality as USPTO50K?\n\nNot necessarily. Atom mapping algorithms do better on simple reactions, and worse on complex reactions. Real-world datasets are more likely to contain complex reactions. The accuracy of atom mapping on real-world datasets drops. In our opinion, atom mapping is in general a quite hard problem and an atom-mapping algorithm may not be perfect. \n\nhttps://chemrxiv.org/articles/preprint/Atom-to-Atom_Mapping_A_Benchmarking_Study_of_Popular_Mapping_Algorithms_and_Consensus_Strategies/13012679/1\n\nThis paper is a benchmark study of popular atom mapping algorithms on a dataset with human-labeled atom mapping ground truth. See table 1, the best algorithm RXNMapper (transformer-based atom mapping algorithm) achieves 83%, which is not perfect. Even with the best algorithm RXNMapper, the authors observed undesired performance (page 15): \n\n>\u201cwhen applied to the entire USPTO dataset errors appear. For example, out of 541 reactions of reduction of carboxyl groups to alcohols found in USPTO only 58 were mapped correctly, while 483 had wrong AAM. For reactions of methyl ester formation using methanol 493 reactions were correct but 225 were found to be incorrectly mapped by RXNMapper.\u201d\n\n**Additionally, since \u201ctemplate\u201d or \u201creaction center\u201d can be extracted from \u201catom mapping\u201d, we lean to characterize models using \u201catom mapping\u201d as \u201csemi-template-based\u201d methods (or more precisely, 'mapping-based' methods). Semi-template-based methods are not the focus of our paper.**\n\n\n### Q2 Reiterate key points: We don\u2019t want to let the accuracy gain due to incorporating additional chemical features mislead (or interfere with) the evaluation of different modeling. \n\nWe want to reiterate the key point of this response titled \u201cReply to R1,2,3,4 NOT SOTA ...\u201d is that we want to make **a fair comparison** where every model gets access to the same information. We don\u2019t want to let the accuracy gain due to incorporating additional chemical features **mislead the evaluation of different modeling**. In this paper, we are mainly interested in methods without using chemistry features, especially template-related features (reaction center, product side of template, templates, etc). We hope eventually the models can learn all this chemistry knowledge by themselves, instead of building on the output of another algorithm, say atom mapping algorithm.  \n \nLet us know if it helps. We are happy to answer more questions. Thank you for the time. ", "title": "Reply to R3: My confusing in \"hand-crafted\""}, "kQPWk5VVwar": {"type": "rebuttal", "replyto": "jHp9SfIkHw", "comment": "Thank you for the thoughtful comments. We uploaded the revised paper and changes are highlighted in blue.  Please see our response to \u201cWeakness\u201d: \n\n**Q1, 2, 3**: \n\nThe model architecture for the dual model under template-based or template-free setup are both transformers (not GLN) (See the paragraph under eq4 on page 4). $p_{\\alpha}$, $p_{\\gamma}$, $p_{\\eta}$ are transformer parameters. \nDual-TB and Dual-TF have the same training procedure (including model architecture), but different inferences. In other words, we train the same dual model in TB or TF set up, which are used to rank a list of reactant candidates, e.g.  $[X1, X2, X3 .. ]$ that are associated with a given product $y$ during inference. The proposal reactant list can be generated with templates or without templates, leading to template-based or template-free methods. \n\n**Q4**: \n\nYes.  The augment procedures of USPTO50K are as follows. For each reaction, \n1. Replace each molecule in reactants or product using random SMILES.\n2. Random permute the order of reactant molecules.\n\nWe updated it in the revised draft. \n\nWe trained Dual-TB on both USPTO 50K (Table 2 upper) and augmented USPTO 50k (Table 2 bottom). \n\nRandom SMILES matters for \u201csequence-based model\u201d, as opposed to \u201cgraph-based model\u201d.  Sequence-based models linearize the 2d molecule to a 1d sequence. The linear compression losses a rich local environment of a molecule. Random SMILES are different linearizations of the same molecule, which rescue some of the information loss. Using random SMILES also prevents transformers from overfitting. Note that as we don\u2019t add new molecules, random SMILES augmentation won\u2019t influence graph-based models\u2019 performance, as graph-based models are permutation invariant.\n\nRandom SMILES doesn\u2019t have a distinguished effect on template-based versus template-free models -- it benefits both template-based and template-free models -- as long as the model is sequenced based (Table 2 template-based vs Table 3 template-free). \n\n**Q5**: \n\nThere are two ways: \n1. Data level (we used): \nAs stated above, the Random SMILES data augmentation we used includes permuting the order of reactants, which achieves order invariant during training stochastically. \n\n2.  NN level: Modify the position encoding in transformer implementation: \nTransformer has a position encoding to mark the different locations on an input sequence. We modified the position encoding such that each molecule starts with 0 encodings, as opposed to the concatenated position in the reactants sequence.  We didn\u2019t use it in our paper, because (1) data level  has superior performance and is easy. See details in Appendix A12. Table 7 of revised paper. \n\n**Q6**: \n\nYes, we can draw samples from the forward model and train backward on the additional samples. We can also do both (the paper plus this suggestion), or even repeat them a few times.  However drawing samples from the forward direction would either require empirical reactants from data, or from the learned $p(X)$, which is harder than drawing samples from the backward direction.  \n\n**Q7**:\n\nIt is true drawing samples from EBMs can be generally challenging. Langevin requires differentiability w.r.t the observations, i.e., molecules, which is NOT applicable here since the molecule SMILES string is discrete. Fortunately, using proposal distribution to sample from EBM is a principled way in general, such as the Metropolis Hastings algorithm. In our case, using the $p(X|y)$ as a proposal is a good trade-off, as it has full support in the molecule space, while being close to the EBM distribution. In practice, $p(X|y)$ can be efficiently obtained by K-size beam search. \n\n**Q8: Not SOTA compared with RetroXpert and GraphRETRO**\n\nPlease see the response to all reviewers with the title \"Reply to R1, R2, R3, R4: Not SOTA...\u201d.\n\n**Q9: The dual formulation can be applied to these above state-of-the-art models (if code is available).**\n\nYes. The generalization is an advantage of the dual framework, as it can be adapted to different models. RetroXpert's code is not uploaded yet: https://github.com/uta-smile/RetroXpert (only data is uploaded. Not code). We are happy to adapt RetroXpert into the dual model framework if the authors upload code.  \n\n**Q10: Are weak results due to transformer?**\n\nSee the response titled\"reply to all; NOT SOTA ...\" and Table 4 in Appendix A. 4 of our revised paper.  The weak results are due to that we are not using extra chemical features. If we add those features to the dual model, we get better results than RetroXpert.  Dual: 70.1%; 73.2% RetroXpert: 65.6%; 70.4%\n\n**Q11: Move perturbed / bidirectional to appendix**\n\nGood point. Thanks. But our goal is to present a full picture of different model designs. In our opinions, both good and bad performances are valuable to trigger helpful discussions and inspire better models in the community.  But since we run out of space, we plan to move some parts into the appendix. \n\n\n", "title": "Reply to R2"}, "ef8GaySb3Wj": {"type": "rebuttal", "replyto": "FDC34zfDlvL", "comment": "Thank you for the insightful comments.  We have loaded the revised paper to address the comments, and the changes are highlighted in blue. Please see our response: \n\n### Q1: Why is the EBM framework beneficial? What do we get from unifying models into EBM? \n\nThank you for the question. EBMs provide a way to map high dimensional data (X, y) into an unnormalized probability distribution. EBM works on a wide range of problems and demonstrates good performance. The community has accumulated lots of knowledge of how to design energy scores and to train them efficiently. That prior knowledge can generalize to new problems or new model designs. \n\nEBM provides the following: \n\n**1. Principled ways for training**. \n\nEBMs have many off-the-shelf training methods that are ready to use, including but limited to contrastive divergence (Hinton, 2002), pseudo-likelihood (PL) (Besag, 1975), conditional composite likelihood (CL) (Lindsay, 1988), score matching (SM) (Hyv\u00a8arinen, 2005), minimum (diffusion) Stein kernel discrepancy estimator (DSKD) (Barp et al.,2019), non-local contrastive objectives (NLCO) (Vickrey et al., 2010), minimum probability flow (MPF) (SohlDickstein et al., 2011), and noise-contrastive estimation (NCE) (Gutmann and Hyvarinen, 2010). \n\nFor example, if we design the energy score function as the \u201cbidirectional model\u201d (eq11 in Sec 3.1.5),  with EBM in mind, it is natural to think of using pseudo-likelihood (PL) as the training approach. \n\nAdditionally, there are different ways to design loss see Sec 2.2 of \nLeCun et al tutorial http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf page 6\n\n**2. Better inference**\n\nEBM model\u2019s inference is flexible. LeCun et al tutorial Page 6 (inference). \n\nEBM can not only make inferences such as \u201cclassification\u201d or \u201cprediction\u201d, e.g. what is best y (label) for X (input image), but also EBM\u2019s inference can be used to rank possible (X, y)-pairs. From the EBM point of view, we can decouple the model, learning (training), and inference (testing). For example, one can use the autoregressive model for modeling and learning, while during inference we can use it as an energy function to get better samples (via re-ranking as we used, or doing Gibbs-sampling to refine the samples). \n\n**3. Advance understanding of different model designs.**\n\n* Extract commonalities and differences between EBM variants.\n\nOne example, RetroXpert, GraphRETRO, and G2G share the same energy score format (eq 14) but the differences are the various instantionans of $p(X|y, c)$. Another example is comparing the energy score design ordered model (eq3), perturbed model (eq8), and bidirectional model (eq11).  We see their bidirectional degree: ordered < perturbed <  bidirectional \nThe experiments show although bidirectional provides more information to the model, at the same time adds challenges in modeling $p(X|y)$ (from $p(X_i| X_{\\setminus i}, y)$ to $p(X|y)$ ) and training.  In addition, some variants share the same training procedure  (ordered model and perturbed model both use MLE). \n\n* Understand the strengths and limitations in model design.\n\nFull model versus ordered model.  The full model is more flexible that do not impose restrictions on model design, however, the expressional power adds challenges in training, say enumerating all possible molecules.\nIn contrast, the ordered model is restricted to the autoregressive model, but the training is much tractable (MLE)\n\n* Compare the complexity of learning or inference.\n\nSee appendix A.7 \"TIME AND SPACE COMPLEXITY ANALYSIS\"  \n\n\n### Q2: Not SOTA, compared with RetroXpert (Yan et al,  NeurIPS 2020) and GraphRETRO (Somnath et al, 2020). \n\nPlease kindly refer to the response to all reviewers with the title \"Reply to R1, R2, R3, R4: Not SOTA...\u201d.\n\nWe have a different setup. These RetroXpert and GraphRETRO are not template free. The lower results are due to that we are not using extra chemical features. If we add those features to the dual model, we get better results than RetroXpert.  Dual: 70.1%; 73.2% RetroXpert: 65.6%; 70.4%  (Table 4 in Appendix A. 4 of our revised paper). \n\n\n### Q3: \u201cI am interested in how the EMB can interpret these latest models.\u201d How to interpret the latest model such as RetroXpert and GraphRETRO into the EBM framework?  \nPlease also see Appendix A.1 and A2 of revised paper. \n\nYes,  RetroXpert and GraphRETRO can be integrated into our EBM framework in a way that is similar to G2G (Sec 3.2.3 in our paper. eq14). The difference between the three is parameterizing $p(X|c, y)$, which is the step that generates reactants from synthons.  Synthons are subgraphs extracted from the products by breaking the bonds in the reaction centers.  Reactants are generated by completing the \u201cmissing pieces\u201d (\u201cleaving groups\u201d in GraphRETRO) in synthon. RetroXpert uses transformers to complete the missing pieces of reactant from synthons; GraphRETRO uses a predefined list of leaving groups and selects the missing pieces; G2G uses graph generation. \n\n", "title": "Reply to R3:"}, "nIVlCorUKTW": {"type": "rebuttal", "replyto": "0Hj3tFCSjUd", "comment": "We upload the revised paper and changes in blue.  \n\n# Short answer: \n\nWe have a **different experimental setup** from RetroXpert or GraphRETRO. RetroXpert and GraphRETRO use additional important chemical information (e.g. reaction centers, and other handcrafted template-related features) during modeling and training, whereas we do not. Adding useful chemical features usually has a ~10% improvement in accuracy (for example, \u201creaction type\u201d is a well-known example that improves ~10% acc regardless of model designs). Because of the usage of chemical features, we classify RetroXpert and GraphRETRO as **semi-template based methods**, whereas our models as **template-free models** (See Appendix A1, 2, 3, 4 of revised paper). **So comparing our models with RetroXpert or GraphRETRO is not a head-to-head comparison**.  \n\nTo prove the above point further, we performed experiments to include these features in our model. The results showed these features boost our model significantly, as expected. Assuming the reaction center is known (due to time constraint of the rebuttal), the dual model improves to **70.1% from 54.5%** without reaction type, and improves to **73.2% from 66.2%** with reaction type. See appendix A.4 Table 4 of the revised version.  (70.1%, 73.2%) is higher than RetroXpert (65.6%, 70.1%). However, our paper focuses on the comparison of different models under the same setup (template-free model without any other additional chemistry features).  Adding extra features to improve accuracy is not our goal here. Apologies for not making this point clear.  \n\n# Detailed answer: \n\nThis paper aims at understanding different modeling designs under the setup of full-automatic template-free, which are defined as models without using hand-crafted chemistry features, including but not limited to templates, sub-parts of templates, reaction centers, and atom mapping (matching of the same atoms in reactants and product).  We are interested in this setup because (1) those handcrafted features are often not available for real-world large datasets. For example, without atom-mapping, reaction centers have to be manually labeled or inferred from substructure searches and/or predictive models, which may be imperfect. With atom-mapping, reaction centers can be extracted computationally. However, getting good atom mapping for large datasets is an equally challenging problem. (2) The future of retrosynthesis is to train models that can think beyond existing rules (e.g. template), and learn useful things without using prior human knowledge. \n\n**RetroXpert and GraphRETRO use lots of useful chemistry features to boost accuracy**\n\n1. RetroXpert  and GraphRETRO both use \u201creaction centers\u201d, which are almost equivalent to templates. \n\nAlthough RetroXpert and GraphRETRO claim they are template-free methods (for not explicitly performing template-matching), they are not fully automatic template-free according to our standard above. Similar with G2G (Shi et al ICML 2020), RetroXpert and GraphRETRO use \u201creaction centers\u201d as additional information to supervise their algorithm during training ( $y_{ij}$ in eq2 for RetroXpert paper;  $y_{uvk}$ in eq5 for GraphRETRO paper). The \u201creaction centers\u201d preserve almost equivalent information as templates-- once reaction centers are given, the product can be broken into two parts (denoted as \u201csynthons\u201d). The templates can be recovered by removing nonessential atoms from products and synthons (See GLN Figure 1, Dai et al NeurIPS 2019). Therefore, we denote RetroXpert or GraphRETRO as semi-template based methods. \n\n2. Besides reaction centers, what other template-related features do RetroXpert or GraphRETRO use? \n\n- **RetroXpert**: adds \u201cthe product side of templates'' as atom features (See sec \u201cAtom and bond features\u201d on page 5 of RetroXpert paper). These features take 654 dimensions out of a total of 791 atom features (Appendix B).  Ablation study shows without  \u201cproduct side of template\", the accuracy for identifying centers drops by 2% with reaction type, and 5% without reaction type (D.2 in Appendix) . The accuracy of the retrosynthesis may decrease further. In conclusion, these \u201cproduct side of templates\u201d significantly improves accuracy. \n- **GraphRETRO**:  predefines a list of leaving groups (Sec 3.2 of GraphRETRO paper). GraphRETRO constrains the leaving groups to be a list of predefined groups extracted from the entire database (including test data. page 5, line 5, \u201c a standard dataset with 50, 000 examples\u201d), which allows training to use test data. Additionally, doing so simplifies the task from a generative task (of reactants) to a classification problem, which is not able to generalize to unseen molecules. \n\n\nReference \n[1]  Yan. et al. (RetroXpert): \nNeurIPS 2020. https://arxiv.org/abs/2011.02893\n[2]  Somnath et al. (GraphRETRO) \nhttps://arxiv.org/abs/2006.07038\n[3] Shi et al. (G2G): ICML 2020. https://arxiv.org/pdf/2003.12725.pdf\n\n", "title": "Reply to R1, R2, R3, R4: \u201cNot SOTA. RetroXpert  and GraphRETRO have better performance\u201d."}, "FDC34zfDlvL": {"type": "review", "replyto": "0Hj3tFCSjUd", "review": "# Summary #\nThis paper introduces a re-interpretation of seq2seq and graph2graph retrosynthesis models with energy based models (EBMs). \nEBM is a general log-linear framework to model joint distributions of a feature variable X and a target variable y, first introduced to connect discriminative (DNN) models with generative (DNN) models. This paper shows the reformulations of typical seq2seq retrosynthesis models and graph2graph models. Also, the paper proposes a dual training model that optimizes both the forward path model and the backward path model. \nExperimental results show that the dual training models perform better than conventional retrosynthesis models in template-based and template-free retrosynthesis frameworks. \n\n# Comments #\n\nI have two major concerns. \n\nFirst, I cannot clearly understand a new insight or knowledge that is brought by the EBM-based re-formulation of retrosynthesis models. \nAny probabilistic models can be described by the full joint distribution of all variables. In my understanding, the EBM is a way of modeling the joints with the log-linear model plus the potential function. \nIt seems for me that the current manuscript is successful in re-wiring the existing retrosynthesis frameworks into EBMs, but that is all. \nPlease clarify what readers can learn about \"connections and ... differences between models, ...understanding of model design [abstract]\" from the EMB modelings. \n\nSecond, the State-of-the-Art of the retrosynthesis in the manuscript is somewhat outdated. \nTo the best of my knowledge, the current best-performing retrosynthesis models are [1] and [2], and the scores of these models are higher than the reported results of the proposed dual models. \n\nI think the current manuscript needs modifications to appropriately cite these papers and replace the standpoint of the submitted work. \n\nPersonally, I am interested in how the EMB can interpret these latest models. \n\n\n[1] Somnath+, \"Learning graph models for template-free retrosynthesis\", arXiv:2006.07038, June 2020. \n[2] Yan+, \"RetroXpert: Decompose retrosynthesis prediction like a chemist\", chemrxiv:11869692.v3, June 2020. \n\n# Evaluation points #\n\n(+) First to apply the EBM for retrosynthesis prediction models\n\n(+) Proposed a forward/backward simultaneous (dual) training\n\n(--) It is unclear what we can learn by rewriting the retrosynthesis models with EBM. \n\n(-) Reported results do not update the State-of-the-Art prediction accuracy of retrosynthesis models in the literature\n", "title": "ICLR 2021 Conference Paper2550 AnonReviewer3", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "74mJtnpyS1e": {"type": "review", "replyto": "0Hj3tFCSjUd", "review": "In this paper, the authors use the framework of energy based models to describe several known approaches for ML-based retrosynthesis in a unified way. This allows to combine retrosynthetic (backward) and reaction prediction (forward) in a principled way.  \nBased on this analysis, a dual model is proposed which used a duality constraint as a regulariser, leading to improved performance over several baselines models, but not over SOTA (see below!).\n\nThe theoretical analysis alone is very interesting, and well described. However, I have a few concerns with missing ablation experiments and the positioning, which could be addressed to make the paper stronger in my opinion.\n\nOverall, I think this paper should be accepted at ICLR after the following points have been addressed (or the authors commit to provide the additional data in the camera ready version if the time of the rebuttal phase is too short to run additional experiments). However, in the current form, the paper is not ready. My evaluation is for the current form of the paper, and I am happy to change it significantly during the rebuttal.\n\n\nAblation Experiments:\n\nIs the duality constraint actually needed? It would be important to perform the ablation experiment where $\\beta$ is set to 0.\nIn other words, what happens if you just use $\\log p(X|y) + \\log  p(y|X)$ to rank the candidates? If the authors perform these additional experiments and report the numbers, I will increase my score regardless of the outcome of the experiments.\n\n\nPrior work:\n\nThe state of the art claim is not correct. Yan et al achieve higher performance on the same dataset https://chemrxiv.org/articles/preprint/Interpretable_Retrosynthesis_Prediction_in_Two_Steps/11869692/2 which has already been published last February! \nTherefore, please remove the SOTA claim from the paper, and acknowledge the Yan et al work. \nFor this reviewer well-motivated modelling, honest analysis and proper experimentation is more important than chasing SOTA, and not achieving SOTA will not affect the evaluation negatively.\n\nAlso, the connection of forward reaction and retrosynthesis prediction via Bayes Theorem has been studied here https://arxiv.org/abs/2003.03190 which should be acknowledged.\n\nOther work preceding has also used a combination of backward and forward prediction. For example, Segler et al Nature 2018 and Coley et al Science 2019 use a model for p(X|y) to propose disconnections combined with a model for p(X,y) to remove low probability solution.\n\nThe usefulness of SMILES augmentation has been previously shown, please cite the previous work by Arus Pous et al https://jcheminf.biomedcentral.com/articles/10.1186/s13321-019-0393-0 \n\nThe Coley et al 2017a paper that the authors cite in the introduction is a wonderful paper, however, it is not concerned with retrosynthesis. I would suggest to cite the review article by Strieth-Kalthoff et al https://doi.org/10.1039/C9CS00786E instead, which provides a good overview over ML approaches for (retro)synthesis. Furthermore, I would suggest to cite Segler & Waller 2017 in the introduction, which was the first paper to suggest deep neural networks for both retrosynthesis & reaction prediction (see also https://doi.org/10.1016/j.ddtec.2020.06.002 )\nColey 2017a should be cited later in the context of forward prediction.\n\nComments:\n\nTable 3 is a bit unclear. With Type, do you mean the reaction type is given?\n\nsmall things:\n- in eq 13, the is a low dot, should this have been a centred dot for multiplication?\n\n- Please don't use Google house fonts for your figures,  to maintain anonymity.\n\n", "title": "Interesting analysis of the problem of retrosynthesis using EBMs", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "_99sSejhs7": {"type": "review", "replyto": "0Hj3tFCSjUd", "review": "SUMMARY\n\nThis paper uses the statistical physics-inspired energy-based model formalism to study the by now \"canonical\" problem of retrosynthesis using deep learning. The authors use an interesting variant that combines forward and backward prediction. The authors use template-based and template-free models.\n\n\nPROS\n\n- This reviewer believes that energy-based models have an elegance and connection to statistical mechanics that should be explored more in the area of machine learning. This work goes in this interesting direction.\n- Based on the above, and as far as the reviewer is appraised, this is a unique, non-derivative direction in the field and therefore deserving of consideration for acceptance.\n- The dual model seems to be very useful given the increase in template-based and non-template-based model performance. This could be applied to other transformer-based tasks in chemistry and graph-based ML\n- The authors compared their models to a variety of SOTA models and approaches, they also were thorough and explored both DeepSMILES and SELFIES.\n\n\nCONS\n- Some of the mathematical formalism could be moved to supplementary to allow for better discussion.\n\n\nMINOR FORMATTING\n- The authors may want to give the manuscript a pass for grammar. There are missing articles in a few sentences.\n", "title": "Review of Energy-Based view of Retrosynthesis", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}