{"paper": {"title": "Neural TTS Stylization with Adversarial and Collaborative Games", "authors": ["Shuang Ma", "Daniel Mcduff", "Yale Song"], "authorids": ["shuangma@buffalo.edu", "damcduff@microsoft.com", "yalesong@csail.mit.edu"], "summary": "a generative adversarial network for style modeling in a text-to-speech system", "abstract": "The modeling of style when synthesizing natural human speech from text has been the focus of significant attention. Some state-of-the-art approaches train an encoder-decoder network on paired text and audio samples (x_txt, x_aud) by encouraging its output to reconstruct x_aud. The synthesized audio waveform is expected to contain the verbal content of x_txt and the auditory style of x_aud. Unfortunately, modeling style in TTS is somewhat under-determined and training models with a reconstruction loss alone is insufficient to disentangle content and style from other factors of variation. In this work, we introduce an end-to-end TTS model that offers enhanced content-style disentanglement ability and controllability. We achieve this by combining a pairwise training procedure, an adversarial game, and a collaborative game into one training scheme. The adversarial game concentrates the true data distribution, and the collaborative game minimizes the distance between real samples and generated samples in both the original space and the latent space. As a result, the proposed model delivers a highly controllable generator, and a disentangled representation. Benefiting from the separate modeling of style and content, our model can generate human fidelity speech that satisfies the desired style conditions. Our model achieves start-of-the-art results across multiple tasks, including style transfer (content and style swapping), emotion modeling, and identity transfer (fitting a new speaker's voice).", "keywords": ["Text-To-Speech synthesis", "GANs"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper proposes using GANs for disentangling style information from speech content, and thereby improve style transfer in TTS. The review and responses for this paper have been especially thorough! The authors significantly improved the paper during the review process, as pointed out by the reviewers. Inclusion of additional baselines, evaluations and ablation analysis helped improve the overall quality of the paper and helped alleviate concerns raised by the reviewers. Therefore, it is recommended that the paper be accepted for publication."}, "review": {"HyeiSBJQT7": {"type": "review", "replyto": "ByzcS3AcYX", "review": "This paper proposes to use GAN to disentangle style information from speech content. The presentation of the core idea is clear but IMO there are some key missing details and experiments.\n\n* The paper mentions '....the model could simply learn to copy the waveform information from xaud to the output and ignore s....' \n--  Did you verify this is indeed the case? 1) The style embedding in Skerry-Ryan et al.'18 serves as a single bottleneck layer, which could prevent information leaking. What dimension did you use, and did you try to use smaller size? 2) The GST layer in Wang et al.'18 is an even more aggressive bottleneck layer, which could (almost) eliminate style info entangled with content info. \n\n* The sampling process to get x_{aud}^{-} needs more careful justifications/ablations.\n-- Is random sampling enough? What if the model samples a x_{aud}^{-} that has the same speaking style as x_{aud}^{+}? (which could be a common case).\n\n* Did you consider the idea in Fader Netowrks (Lample et al.'17)', which corresponds to adding a simple adversarial loss on the style embedding? It occurs to be a much simpler alternative to the proposed method.\n\n* Table 1. \"Tacotron2\" is often referred to Shen et al.'18, not Skerry-Ryan et al.'18. Consider using something like \"Prosody-Tacotron\"?\n\n* The paramerters used for comparisons with other models are not clear. Some of them are important detail (see the first point above)\n\n* The author mentioned the distance between different clusters in the t-SNE plot. Note that the distance in t-SNE visualizations typically doesn't indicate anything.\n\n* 'TTS-GAN' is too general as the name for the proposed method.", "title": "lack of details and proper comparisons", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "BJl11ws2AX": {"type": "rebuttal", "replyto": "B1lxfzs30Q", "comment": "We sincerely appreciate your constructive suggestions!\n\nre: Ablation study on the reference embedding size and the number of heads in multihead attention\nWe feel that this is a good suggestion! We agree that conducting more comprehensive analyses with different degrees of \"information bottleneck\" -- i.e., the size of reference embedding and the number of heads in the multihead attention -- will provide interesting insights on how our model behaves. We promise to include this ablation study in the Appendix.\n\nre: Fader Networks\nIt is interesting to hear that, unlike our case, adding an adversarial loss for style transfer helped achieve better results (we'd be curious to see the results; do you have a paper we can check? was your experience based on images only or also based on audio signals?) Regardless, we feel that this will be an easy-to-add baseline approach; we will include this in the final version. ", "title": "Thank you for the constructive suggestions!"}, "r1esueVMp7": {"type": "review", "replyto": "ByzcS3AcYX", "review": "Overview: This paper describes an approach to style transfer in end-to-end speech synthesis by extending the reconstruction loss function and augmenting with an adversarial component and style based loss component.\n\nSummary: This paper describes an interesting technical approach and the results show incremental improvement to matching a reference style in end-to-end speech synthesis.  The three-component adversarial loss is novel to this task.  While it has technical merit, the presentation of this paper make it unready for publication.  The technical descriptions are difficult to follow in places, it makes some incorrect statements about speech and speech synthesis and its evaluation is lacking in a number of ways.   After a substantial revision and additional evaluation, this will be a very good paper.\n\nThe title of the paper and moniker of this approach as \u201cTTS-GAN\u201d seems to preclude the fact that in the last few years there have been a number of approaches to speech synthesis using GANs.  By using such a generic term, it implies that this is the \u201cstandard\u201d way of using a GAN for TTS.  Clearly it is not. Moreover, other than the use of the term, the authors do not claim that it is. \n\nWhile the related works regarding style modeling and transfer in end-to-end TTS models are well described, prior work on using GANs in TTS is not.  (This may or may not be related to the previous point.)  For example, but not limited to:\nYang Shan, Xie Lei, Chen Xiao, Lou Xiaoyan, Zhu Xuan, Huang Dongyan, and Li Haizhou, Statistical Parametric Speech Synthesis Using Generative Adversarial Networks Under a Multi-task Learning Framework, ASRU, 2017\nYuki Saito, Shinnosuke Takamichi, Hiroshi Saruwatari, Text-to-speech Synthesis using STFT Spectra Based on Low- /multi-resolution Generative Adversarial Networks, ICASSP 2018\nSaito Yuki, Takamichi Shinnosuke, and Saruwatari Hiroshi, Training Algorithm to Deceive Anti-spoofing Verification for DNN-based Speech Synthesis, ICASSP, 2017.\n\nSection 2 describes speech synthesis as a cross-domain mapping problem F : S -> T, where S is text and T is speech. (Why a text-to-speech mapping is formalized as S->T is an irrelevant mystery.)  This is a reasonable formulation, however, this is not a bijective mapping.  There are many valid realizations s \\subset T of a text utterance t \\in S.  The true mapping F is one-to-many.    Contrary to the statement in Section 2, there should not be a one-to-one correspondence between input conditions and the output audio waveform and this should not be assumed.  This formalism can be posed as a simplification of the speech synthesis mapping problem.  Overall Section 2 lays an incorrect and unnecessary formalism over the problem, and does very little in terms of \u201cbackground\u201d information regarding speech synthesis or GANs.  I would recommend distilling the latter half of the last paragraph.  This content is important -- the goal of this paper is to disentangle the style component (s) from the \u201ceverything else\u201d component (z)  in x_{aud} by which the resultant model can be correctly conditioned on s and ignore z.\n\nSection 3.2 Style Loss: The parallel between artistic style in vision and speaking style in speech is misplaced.  Artistic style can be captured by local information by representing color choices, brush technique, etc.  Speaking style and prosodic variation more broadly is suprasegmental.  That is it spans multiple speech segments (typically defined as phonetic units, phonemes, etc.).  It is specifically not captured in local variations in the time-frequency domain.  The local statistics of a mel-spectrogram are empoverished to capture the long term variation spanning multiple syllables, words, and phrases that contribute to \u201cspeaking style\u201d.  (In addition to the poor motivation of using low-level filters to capture speaking style, the authors describe \u201cprosody\u201d as \u201crepresenting the low-level characteristics of sound\u201d. This is not correct.)  These filter activations are more likely to capture voice quality and speaker identity characteristics than prosody and speaking style.\n\nSection 3.2: Reconstruction Loss: The training in this section is difficult to follow.  Presumably, l is the explicit style label from the data, the emotion label for EMT-4 and (maybe) speaker id for VCTK.  It is a rather confusing choice to refer to this as \u201clatent\u201d since this carries a number of implications from variational techniques and bayesian inference.  Similarly, It is not clear how these are trained. Specifically, both terms are minimized w.r.t. C but the second is minimized only w.r.t G.  I would recommend that this section be rewritten to describe both the loss functions, target variables, and the dependent variables that are optimized during training.\n\nSection 3.3 How are the coefficients \\alpha and \\beta determined?\n\nSection 3.3 \u201cWe train TTS-GAN for at least 200k steps.\u201d Why be vague about the training?\n\nSection 3.3. \u201cDuring training R is fixed weights\u201d Where do these weights come from? Is it an ImageNet classifier similar with a smaller network than VGG-19?\n\nSection 5: The presentation of results into Table 1 and Table 2 is quite odd.  The text material references Table 1 in Section 5.1, then Table 2 in Section 5.2, then Table 1 in Section 5.3 and then Table 2 again in Section 5.3.  It would be preferable to include the tabular material which is being discussed in the same order as the text.\n\nSection 5: Evaluation.  It is surprising that there is no MOS or naturalness evaluation of this work.  In general increased flexibility of a style-enabled system results in decreased naturalness.  While there are WER results to show that intelligibility (at least machine intelligibility) may not suffer, the lack of an MOS result to describe TTS quality is surprising.\n\nSection 5: The captions of Tables 1 and 2 should provide appropriate context for the contained data.  There is not enough information to understand what is described here without reference to the associated text.\n\nSection 5.1: The content and style swapping is not evaluated.  While samples are provided, it is not at all clear that the claims made by the authors are supported by the data.  A listening study where subjects are asked to identify the intended emotion of the utterance would be a convincing way to demonstrate the effectiveness of this technique.  As it stands, I would recommend removing the section titled \u201cContent and style swapping\u201d as it is unempirical.  If the authors are committed to it, it could be reasonably moved to the conclusions or discussion section as anecdotal evidence.\n\nSection 5.3: Why use a pre-trained WaveNet based ASR model?  What is its performance on the ground truth audio?  This is a valuable baseline for the WER of the synthesized material.\n\nSection 5.3 Style Transfer: Without support that the subject ratings in this test follow a normal distribution a t-test is not a valid test to use here.  A non-parametric test like a Mann-Whitney U test would be more appropriate.\n\nSection 5.3 Style Transfer: \u201cEach listened to all 15 permutations of content\u201d.  From the previous paragraph there should be 60 permutations.\n\nSection 5.3 Style Transfer: Was there any difference in the results from the 10 sentences from the test set, and the 5 drawn from the web?\n\nTypos:\nSection 1 Introduction: \u201cx_{aud}^{+} is unpaired\u201d -> \u201cx_{aud}^{-} is unpaired\u201d\nSection 2: \u201cHere, We\u201d -> \u201cHere, we\u201d\nSection 5.3 \u201cTachotron\u201d -> \u201cTacotron\u201d", "title": "Good technical ideas, but suffers from clarity issues and weak evaluation.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "BygbTKVoR7": {"type": "rebuttal", "replyto": "rkeebN1oR7", "comment": "Naturalness:\nThanks for your constructive comments. It is correct that our evaluation is focused on the disentanglement of style and content, rather than directly assessing the naturalness of the TTS results, because disentangling content/style is the major focus of our work. In hindsight, however, we do agree with your point that measuring the naturalness could have provided additional insights into how our model performs compared to the baseline TTS systems. We promise to add a MOS evaluation results in the final version of our paper.\n\n\nSwapping:\nWe also agree with the reviewer on this point. We will add human classification results on the style swapping experiment.\n\n\nStyle transfer:\nWe appreciate your clarification on evaluation metrics for our subjective study. Yes, we do agree with your comments, and will modify our metric based on a non-parametric test.", "title": "naturalness, swapping and style transfer"}, "ryl7IwesAX": {"type": "rebuttal", "replyto": "ByejWaRc0Q", "comment": "Bijective mapping:\nThanks for the constructive suggestion. We do agree that the bijective constraint might be too strict; injective mapping could be more appropriate to illustrate our setting. We have incorporated your two suggestions into the new revision. (Note: Since this discussion was at the very last minute, which was past the rebuttal period, we could not upload the new version of the paper. But the change is already made and will be reflected in the final version.)\n\nStyle loss:\nThanks for the clarification. Yes, we do agree that prosody, in its entirety, cannot be captured using local statistics in the time-frequency domain. As we clarified above, our style loss is limited to capturing only certain elements of prosody. To reflect this, we have already removed our statement regarding style loss and prosody in the revision.", "title": "'bijective mapping' and 'style loss'"}, "SJeKU5Ic27": {"type": "review", "replyto": "ByzcS3AcYX", "review": "This paper proposes a method to synthesize speech from text input, with the style of an input voice provided with the text. Thus, we provide content - text - and style - voice. It leverages recent - phenomenal - progress in TTS with Deep Neural Networks as seen from exemplar works such as Tacotron (and derivatives), DeepVoice, which use seq2seq RNNs and Wavenet families of models. The work is extremely relevant in that audio data is hard to generate (expensive) and content-style modeling could be useful in a number of practical areas in synthetic voice generation. It is also quite applicable in the related problem of voice conversion. The work also uses some quite complex - (and very interesting!) - proposals to abstract style, and paste with content using generative modeling. I am VERY excited by this effort in that it puts together a number of sophisticated pieces together, in what I think is a very sensible way to implement a solution to this very difficult problem. However, I would like clarifications and explanations, especially in regards to the architecture.  \n\nDescription of problem: The paper proposes a fairly elaborate setup to inject voice style (speech) into text. At train time it takes in text samples $x_{txt}$, paired voice samples (utterances that have $x_{txt}$ as content) $s+$ and unpaired voice samples $s-$, and produces two voice samples $x+$ (for paired  <txt, utterance>) and $x-$ (for unpaired txt/utterance). The idea is that at test time, we pass in a text sample $x_{txt}$ and an UNPAIRED voice sample $x_{aud}$ and the setup produces voice in the style of $x_{aud}$ but whose content is $x_{txt}$, in other words it generates synthetic speech saying $x_{txt}$. The paper goes on to show performance metrics based on an autoencoder loss, WER and t-SNE embeddings for various attributes. \n\nContext:  The setup seems to be built upon the earlier work by Taigman et al (2016) which has the extremely interesting conception of using a {\\it ternary} discriminator loss to carry out domain adaptation between images. This previous work was prior to the seminal CycleGAN work for image translation, which many speech works have since used. Interestingly, the Taigman work also hints at a 'common' latent representation a la UNIT using coupled VAE-GANs with cycle consistency (also extremely pertinent), but done differently. In addition to the GAN framework by Taigman et al, since this work is built upon Tacotron and the GST (Global Style Tokens) work that followed it, the generative setup is a sophisticated recurrent attention based seq2seq model.\n\nFormulation:\nA conditional formulation is used wherein the content c (encoding generated by text) is passed along with other inputs in the generator and discriminator. The formulation in Taigman assumes that there is an invariant representation in both (image) domains with shared features. To this, style embeddings (audio) gets added on and then gets passed into the generator to generate the speech. Both c and s seem to be encoder outputs in the formulation. The loss components of what they call \u2018adversarial\u2019, \u2018collaborative\u2019 and \u2018style\u2019 losses. \n\nAdversarial losses\nThe ternary loss for D consists of \n\nDiscriminator output from \u2018paired\u2019 style embedding (i.e. text matching the content of paired audio sample)\nDiscriminator output from \u2018unpaired\u2019 style embedding (i.e text paired with random sample of some style)\nDiscriminator output from target ground truth style. The paper uses x_+, so I would think that it uses the paired sample (i.e. from the source) style.\n\nGenerator loss (also analogous to Taigman et al) consists of generations from paired and unpaired audio, possibly a loose analogue to source and target domains, although in this case we can\u2019t as such think of \u2018+\u2019 as the source domain, since the input is text. \n\nCollaborative losses \nThis has two components, one for style (Gatys et al 2016) and a reconstruction component. The reconstruction component again has two terms, one to reconstruct the paired audio output \u2018x+=x_audio+\u2019 - so that the input content is reproduced -  and the other to encourage reconstruction of the latent code. \n\nDatasets and Results:\nThey use two datasets: one, an internal \u2018EMT-4\u2019 dataset with 20k+ English speakers, and the other, the VCTK corpus. Comparisons are made with a few good baselines in Tacotron2, GST and DeepVoice2. \n\nOne comparison technique to test disentanglement ability is to compare autoencoder reconstructions with the idea that a setup that has learnt to disentangle would produce higher reconstruction error because it has learnt to separate style and content. \n\nt-SNE embeddings are presented to show visualizations of various emotion styles (neutral, angry, sad and happy), and separation of male and female voices. A WER metric is also presented so that generations are passed into a classifier (an ASR system trained on Wavenet). All the metrics above seem to compare excellently (better than?) with the others. \n\nQuestions and clarifications:\n\n(Minor) There\u2019s a typo in page 2, line 2. x_{aud}^+ should be x_{aud}^-.\n\nClarification on formulation: Making the analogy (is that even the right way of looking at this?) that the \u2018source\u2019 domain is \u2018+\u2019, and the target domain is \u2018-\u2019, in equation (5), the last term of the ternary discriminator has the source domain (x_{aud}^+) in it, while the Taigman et al paper uses the target term. Does this matter? I would think \u2018no\u2019, because we have a large number of terms here and each individual term in and of itself might not be relevant, nor is the current work a direct translation of the Taigman et al work. Nevertheless, I would like clarification, if possible, on the discrepancy and why we use the \u2018+\u2019 samples. \n\nClarification on reconstruction loss: I think the way it is presented, equation (8) is misleading. Apparently, we are sampling from the latent space of style and content embeddings for paired data. The notation seems to be quite consistent with that of the VAE, where we have a reconstruction and a recognition model, and in effect the equation (8) is sampling from the latent space in a stochastic way. However, as far as I can see, the latent space here produces deterministic embeddings, in that c = f(x_{txt}) and s = g(x_{aud}^+), with the distribution itself being a delta function. Also, the notation q used in this equation most definitely indicates a variational distribution, which I would think is misleading (unless I have misinterpreted what the style tokens mean). At any rate, it would help to show how the style token is computed and why it is not deterministic. \n\nClarification on latent reconstruction loss: In equation (9), how is the latent representation \u2018l\u2019 computed? While I can intuitively see that the latent space \u2018l\u2019 (or z, in more common notation) would be the \u2018same\u2019 between real audio samples and the \u2018+\u2019, \u2018-\u2019 fake samples, it seems to me that they would be related to s (as the paper says, \u2018C\u2019 and \u2018Enc_s\u2019 share all conv layers) and the text. But what, in physical terms is it producing? Is it like the shared latent space in the UNIT work, or the invariant representation in Taigman? This could be made clearer with an block diagram for the architecture. \n\n(Major) Clarification on network architecture\nThe work references Tacotron\u2019s GST work (Wang et al 2018) and the related Skerry-Ryan work as the stem architecture with separate networks for style embeddings and for content (text). While the architecture itself might be available in the stem work by Wang et al, I think we need some diagrams for the current work as well for a high level picture. Although it is mentioned in words in section 3.3, I do not get a clear idea of what the encoder/decoder architectures look like. I was also surprised in not seeing attention plots which are ubiquitous in this kind of work. Furthermore, in the notes to the \u2018inference\u2019 network \u2018C\u2019 it is stated that C and Enc_s share all conv layers. Again, a diagram might be helpful - this also applies for the discriminator. \n\nClarification on stability/mode collapse: Could the authors clarify how easily this setup trained in this adversarial setup? \n\nNote on latent representation: To put the above points in perspective, a small note on what this architecture does in regards to the meaning of the latent codes would be useful. The Taigman et al 2016 paper talks about the f-constancy condition (and 'invariance'). Likewise, in the UNIT paper by Ming-Yu Liu - which is basically a set of coupled VAEs + cycle consistency losses, there is the notion of a shared latent space. A little discussion on these aspects would make the paper much more insightful to the domain adaptation practitioner.\n\nReference: This reference - Adversarial feature matching for text generation - (https://arxiv.org/abs/1706.03850) contains a reconstruction stream (as perhaps many other papers) and might be useful for instruction. \n\nOther relevant works in speech and voice conversion: This work comes to mind, using the StarGAN setup, also containing a survey of relevant approach in voice conversion. Although the current work is for TTS, I think it would be useful to include speech papers carrying out domain adaptation for other tasks.\n\nStarGAN-VC: Non-parallel many-to-many voice conversion with star generative adversarial networks. \nhttps://arxiv.org/abs/1806.02169 \n\nI would rate this paper as being acceptable if the authors clarify my concerns, and in particular, about the architecture. It is also hard to hard to assess reproducibility in a complex architecture such as this. ", "title": "Good adversarial domain adaptation ideas for TTS - but details on architecture needed ", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "HJgZD2nYRQ": {"type": "rebuttal", "replyto": "Hklxr33Y0m", "comment": "MOS\nWe do not think MOS is a must have metric in our paper. Other relevant papers for stylization in TTS, e.g. prosody-Tacotron also do not include a MOS evaluation. \nWe have performed a number of evaluations quantitatively and qualitatively and believe that these extensive evaluations are sufficient to validate our work. The most important thing is, in our paper, how to disentangle style and content such that the encoder learns to produce effective style latent codes is the most important claim. So other than some evaluation metrics used in regular TTS, we also performed a set of experiments that do not typically appear in TTS work. \n\nTable captions  \nThanks for your suggestion, we will revise the captions.\n\nSwapping \nThe most important claim in our paper is the ability to disentangle content and style. We believe this experiment actually is most important evaluation in validating our claim. Similar experiment are typically performed in computer vision papers, e.g. \u2018Disentangling factors of variation in deep representations using adversarial training, NIPS 2016\u2019 (Fig.3). \n\nASR model\nThe ASR model is just a tool to evaluate different methods, here we just compare the relative performance.  But your suggestions are good, we will add the ground truth WER in our paper.\n\nStyle transfer \nTo validate that the test follows a normal distribution would require a large amount of subjective studies.  We followed the precedent in the most recent works (GST, and prosody-Tacotron).\n\nPermutations\nYes, you are right. Thanks for your carefully reading the paper, we will change this in our paper.\n\nResults \nThe results turn out that they are almost equivalent. \n\nTypos\nThanks, we will modify the typos in our paper.\n", "title": "evaluations"}, "Hklxr33Y0m": {"type": "rebuttal", "replyto": "r1esueVMp7", "comment": "Title\nWe appreciate this point, and removed the TTS-GAN moniker as it is quite generic.\n\nBijective mapping\nWe agree that regular speech synthesis is not a bijective mapping problem, because it may result in multiple meaningful results. We also mentioned this in our paper (Sec. 1 ln 6-7). However, we want to clarify our claim, by saying \u2018bijective\u2019, we refer to style modeling in TTS (a conditional generation), i.e. given textual string and a reference audio sample, the synthesized audio should one-to-one correspond to the given conditions (content from text and style from reference audio). If it is not a bijective mapping, e.g. one-to-many mapping, then one textual string could map to different styles, which neglects our style condition (reference audio). We have also elaborated on our claim, which can be seen in Sec. 2 (last paragraph).\n\nStyle loss\nWith all due respect, we disagree with the reviewer that prosody cannot be captured in local variations in the time-frequency domain. In fact, certain prosodic characteristics, such as emotion, are captured by local statistics in the time-frequency domain. For example, Cheang and Pell (2008) have shown that a temporary reduction in the average fundamental frequency significantly correlates with sarcasm expression. \nMore broadly, numerous past studies on prosody have been based on spectral characteristics, e.g. Wang (2015),  Soleymani, et al. (2018), Barry (2018).\nThat being said, we do agree with the reviewer that prosodic variation is often suprasegmental. Therefore, our approach to capturing speaking style can only model those prosodic variations that are characterized by local statistics. We have made this point clear in our paper in Section 3.2.\nCheang, Henry S., and Marc D. Pell. \"The sound of sarcasm.\" Speech communication 50.5 (2008): 366-381.\nKun-Ching Wang. \u201cTime-Frequency Feature Representation Using Multi-Resolution Texture Analysis and Acoustic Activity Detector for Real-Life Speech Emotion Recognition\u201d (2015).\nSobhan Soleymani, Ali Dabouei, et al. \u201cProsodic-Enhanced Siamese Convolutional Neural Networks for Cross-Device Text-Independent Speaker Verification\u201d (2018).\nShaun Barry, Youngmoo Kim. \u201cStyle Transfer for Musical Audio Using Multiple Time-Frequency Representations\u201d. (2018)\n\n\nReconstruction loss\nFirst, we have changed \u2018I\u2019 to \u2018z_c\u2019 to represent the latent code. \nIf z_c is categorical, then C could be a N-way classifier. So you are right, z_c is the emotion label for EMT-4, and identities for VCTK.\n\u2018Latent\u2019 is commonly used in encoder-decoder networks and generative work, we do not feel it is a confusing word.\nThe training details are present in the last paragraph of this section. In Eq9, the first term is minimized over C and the second term is minimized over both C and G. The hyperparameters were empirically determined.\nDifferent datasets need different numbers of training steps (for EMT-4 we trained for 200k steps, while for VCTK, we trained our model for 280k steps).\nThe detailed description of the weights and network architecture of R can be found in our paper (last paragraph in \u2018Style Loss\u2019 section and line 4-5 in page 5).\n\nPresentation of the tables\nDue the page limitation, we prefer to present our paper in a more compact way. However, we could move elements to the appendices if necessary.", "title": "clarifications"}, "rJe7lY3KCQ": {"type": "rebuttal", "replyto": "HyeiSBJQT7", "comment": "Sorry, it misleads readers by saying in this way, we will modify our description in the paper.\n\n\n1. To clarify, we were trying to communicate that when training on purely paired data the network can easily to memorize all the information from the paired audio sample, i.e. both style and content components.\nFor example, given (txt1, aud1), the network memorizes that as long as given a txt1, the result should be aud1. In this case, the style embedding tends to be neglected by the decoder, and the style encoder cannot be optimized easily. During test stage, when given (txt1, aud2), the network still produces an audio sample very similar to aud1, and the \u2018style\u2019 is not learned well. Our experiments on style transfer validate this claim. When comparing with GST, our synthesized audio is closer to the reference style.\n\n2. Through empirical experiments we found that randomly sampling is enough for training.\n\n3. Thanks for your suggestion. When we started this work, that idea was our first basic attempt. But it turns out, by simply adding an adversarial loss on the latent space did not produce good results. The most severe problem is it is not robust to various length reference audio samples. When the reference audio is longer than the input, the synthesized samples tend to have long duplicate tails, or sometime noises. It severely impairs the audio quality. \nWe suspect that, to satisfy the correct classification, the style embedding is squeezed into the same scale, which is not robust to varied length sequential signals. The Fader Network was used for processing images which are a fixed dimension, this method does not seem to work well for audio. Therefore, in our current model, we promote the disentanglement by paire-wise training, which means we do not need to add an adversarial loss directly on the latent space, but on the generated samples. Our results show that this leads to more robust outcomes for sequential signals for different lengths. We will clarify this in the paper.\n\n4. Thanks. It is a good suggestion to replace Tacotron2 with Prosody-Tacrotron. We will modify this in our paper.\n\n5. The hyperparameters for our model can be seen in our implementation details and Appendix.\nThe parameters used for other methods are the same with their original work.\n\n6. As in this experiment, we want to evaluate how well our model can learn the latent space. In other words, are the style embeddings produced by our model effectively representing any desired style. By showing the t-SNE visualization, we can see that, the latent space learned by our model can be well separated into clusters according to the testing data distribution. The same experiment was also done in GST (Wang et al).\n\n7. We appreciate that TTS-GAN is quite general.  We are happy to change the name of the paper.  ", "title": "Thanks for your thoughtful reviews and valuable comments"}, "BJebIsvXpm": {"type": "rebuttal", "replyto": "Byg1SllT37", "comment": "Thank you for the comments. We have fixed the typos in our revision.", "title": "Thank you for the comments."}, "BJxRuqPQam": {"type": "rebuttal", "replyto": "SJeKU5Ic27", "comment": "1. Typo in p2 l2.\nThanks, we fixed it.\n\n2. Clarification on formulation:\nThank you for pointing out the discrepency. We provide detailed explanation below. In short, there is a subtle yet important distinction: We use '+' samples to regularize within-domain mapping (between (c, x_aud^+) and \\tilde{x}^+), while Taigman et al., (2016) use '-' to promote cross-domain mapping (between (c, x_aud^-) and \\tilde{x}^-)).\n\nTaigman's work use a pretrained function f(.) to extract latent embeddings from both the source and the target domains, i.e., z_s = f(s), z_t = f(t). They then use a decoder to map these to the target distribution, producing s2t and t2t. The s2t drives cross-domain mapping, while the t2t regularizes within-domain mapping. They use a single function f(.) to compute the embeddings from both the source (real human face) and the target (emoji human face) because the two domains share certain structures and properties, e.g., a face has two eyes with eyebrows on top. This makes t2t -- within-domain mapping -- relatively easy compared to ours (see below on why); so they include the target term in the loss (Eqn 3 in [Taigman et al., 2016]) to further promote cross-domain mapping.\n\nIn our work, making the analogy, the source domain is '(content, style+)' and the target is '(content, style-)'. Both domains consist of two input modalities (text and sound) with very different characteristics. So we use two functions to represent each domain: Enc_c and Enc_s. Unfortunately, this makes it difficult to even ensure that within-domain mapping is successful. So, to strengthen within-domain mapping we modify the last term of the tenary discriminator to have x_aud^+ instead of the target x_aud^-. \n\n3. Clarification on reconstruction loss:\nYes, both the content c = f(x_txt) and the style s = g(x_aud^+) embeddings are deterministic. The only stochasticity comes from the data distribution. We revised the notation in the paper; please take a look. \n\n4. Clarification on latent reconstruction loss: \nWe have revised our paper with network architecture details, including a block diagram of the Inference Network 'C' that computes the latent representation 'l'; see Figure 3. The inference network is simply the style encoder (Enc_s) with a new classifier on top (one FC layer followed by softmax); all the weights are shared between C and Enc_s except for the new classifier layer.\n\nWe agree that 'z' is a more commonly used notation to represent latent codes. We have changed the notation in the paper; thanks for the suggestion! \n\n5. Clarification on network architecture\nWe have revised our paper with block diagrams of our network architecture as well as parameter settings used in our implementation (Figure 3 to 5). We have also included an attention plot (Figure 6), showing the robustness of our approach to the length of the reference audio.\n\n6. Clarification on stability/mode collapse:\nIn TTS stylization, when mode collapse happens the synthesized voice samples will exhibit the same acoustic style although different reference audio samples are provided. While it is difficult to entirely prevent the mode collapse from ever happening (as is common in GAN training), we have a number of measurements (i.e., different loss terms in our adversarial & collaborative game) to alleviate the issue and to improve stability during training. Our qualitative results show more diverse synthesized samples than Tacotron-GST when different reference audio samples are given, suggesting our work clearly improves upon the state-of-the-art.  Our learning curve (https://researchdemopage.wixsite.com/tts-gan/image) also suggests that training with our loss formulation is relatively stable, i.e., the three loss values seem to converge to a stable regime.\n\n7. Note on latent representation:\nPerhaps the most important message we want to deliver is: We are improving upon content vs. style disentanglement in acoustic signals by means of adversarial & collaborative learning. Extracting ``acoustic styles'' such as prosody has been an extremely difficult task. The state-of-the-art GST achieves this with an attention mechanism. But, as we argue in our paper, their loss construction makes it difficult to ``wipe out'' content information from acoustic signals; this is also shown in their qualitative results where prosody style transfer fails when the length of the reference audio clip is different from what is appropriate for the content to be synthesized. Our novel loss construction enables careful conditioning of our model so that the two latent representations, content 'c' and style 's' embeddings, become more precise than the previous method could obtain. In particular, our paired and unpaired input forumation, and the adversarial & collaborative game makes our model better condition the latent space so that the content information is effectively ignored in style embedding vectors. \n\n8. Reference:\nWe have incorporated those references in our revision. ", "title": "Thanks for your thoughtful reviews and valuable comments."}, "Byg1SllT37": {"type": "review", "replyto": "ByzcS3AcYX", "review": "This paper proposes to use a generative adversarial network to model speaking style in end-to-end TTS. The paper shows the effectiveness of the proposed method compared with Takotron2 and other variants of end-to-end TTS with intensive experimental verifications. The proposed method of using adversarial and collaborative games is also quite unique. The experimental part of the paper is well written, but the formulation part is difficult to follow. Also, the method seems to be very complicated, and I\u2019m concerning about the reproducibility of the method only with the description in Section 3.\n\nComments\n- Page 2, line 2: x _{aud} ^{+} -> x _{aud} ^{-} (?)\n- Section 2: $T$ is used for audio and the number of words.\n", "title": "Review of \u201cTTS-GAN: A GENERATIVE ADVERSARIAL NETWORK FOR STYLE MODELING IN A TEXT-TO-SPEECH SYSTEM\u201d", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}