{"paper": {"title": "Avoiding Catastrophic States with Intrinsic Fear", "authors": ["Zachary C. Lipton", "Kamyar Azizzadenesheli", "Abhishek Kumar", "Lihong Li", "Jianfeng Gao", "Li Deng"], "authorids": ["zlipton@cmu.edu", "kazizzad@uci.edu", "abkumar@ucsd.edu", "lihongli.cs@gmail.com", "jfgao@microsoft.com", "l.deng@ieee.org"], "summary": "Shape reward with intrinsic motivation to avoid catastrophic states and mitigate catastrophic forgetting.", "abstract": "Many practical reinforcement learning problems contain catastrophic states that the optimal policy visits infrequently or never. Even on toy problems, deep reinforcement learners periodically revisit these states, once they are forgotten under a new policy. In this paper, we introduce intrinsic fear, a learned reward shaping that accelerates deep reinforcement learning and guards oscillating policies against periodic catastrophes. Our approach incorporates a second model trained via supervised learning to predict the probability of imminent catastrophe. This score acts as a penalty on the Q-learning objective. Our theoretical analysis demonstrates that the perturbed objective yields the same average return under strong assumptions and an $\\epsilon$-close average return under weaker assumptions. Our analysis also shows robustness to classification errors. Equipped with intrinsic fear, our DQNs solve the toy environments and improve on the Atari games Seaquest, Asteroids, and Freeway.", "keywords": ["reinforcement learning", "safe exploration", "dqn"]}, "meta": {"decision": "Reject", "comment": "This paper presents an interesting idea that is related to imitation learning, safe exploration,\nand intrinsic motivation. However, in its current state the paper needs improvement in clarity. There are also some concerns about the number of hyperparameters involved. Finally, the experimental results are not completely convincing and should reflect existing baselines in one of the areas described above.\n"}, "review": {"SkYNcg5xz": {"type": "review", "replyto": "B16yEqkCZ", "review": "The paper addresses the problem of learners forgetting rare states and revisiting catastrophic danger states. The authors propose to train a predictive \u2018fear model\u2019 that penalizes states that lead to catastrophes. The proposed technique is validated both empirically and theoretically. \n\nExperiments show a clear advantage during learning when compared with a vanilla DQN. Nonetheless, there are some criticisms than can be made of both the method and the evaluations:\n\nThe fear radius threshold k_r seems to add yet another hyperparameter that needs tuning. Judging from the description of the experiments this parameter is important to the performance of the method and needs to be set experimentally. There seems to be no way of a priori determine a good distance as there is no way to know in advance when a catastrophe becomes unavoidable. No empirical results on the effect of the parameter are given.\n\nThe experimental results support the claim that this technique helps to avoid catastrophic states during initial learning.The paper however, also claims to address the longer term problem of revisiting these states once the learner forgets about them, since they are no longer part of the data generated by (close to) optimal policies.  This problem does not seem to be really solved by this method. Danger and safe state replay memories are kept, but are only used to train the catastrophe classifier. While the catastrophe classifier can be seen as an additional external memory, it seems that the learner will still drift away from the optimal policy and then need to be reminded by the classifier through penalties. As such the method wouldn\u2019t prevent catastrophic forgetting, it would just prevent the worst consequences by penalizing the agent before it reaches a danger state. It would therefore  be interesting to see some long running experiments and analyse how often catastrophic states (or those close to them) are visited. \n\nOverall, the current evaluations focus on performance and give little insight into the behaviour of the method. The paper also does not compare to any other techniques that attempt to deal with catastrophic forgetting and/or the changing state distribution ([1,2]).\n\nIn general the explanations in the paper often often use confusing and  imprecise language, even in formal derivations, e.g.  \u2018if the fear model reaches arbitrarily high accuracy\u2019 or \u2018if the probability is negligible\u2019.\n\nIt is wasn\u2019t clear to me that the properties described in Theorem 1 actually hold. The motivation in the appendix is very informal and no clear derivation is provided. The authors seem to indicate that a minimal return can be guaranteed because the optimal policy spends a maximum of epsilon amount of time in the catastrophic states and the alternative policy simply avoids these states. However, as the alternative policy is learnt on a different reward, it can have a very different state distribution, even for the non-catastrophics states.  It might attach all its weight to a very poor reward state in an effort to avoid the catastrophe penalty. It is therefore not clear to me that any claims can be made about its performance without additional assumptions.\n\nIt seems that one could construct a counterexample using a 3-state chain problem (no_reward,danger, goal) where the only way to get to the single goal state is to incur a small risk of visiting the danger state. Any optimal policy would therefore need to spend some time e in the danger state, on average. A policy that learns to avoid the danger state would then also be unable to reach the goal state and receive rewards. E.g pi* has stationary distribution (0,e,1-e) and return 0*0+e*Rmin + (1-e)*Rmax. By adding a sufficiently high penalty, policy pi~ can learn to avoid the catastrophic state with distribution (1,0,0) and then gets return 1*0+ 0*Rmin+0*Rmax= 0 < n*_M - e (Rmax - Rmin) = e*Rmin + (1-e)*Rmax - e (Rmax - Rmin).  This seems to contradict the theorem. It wasn\u2019t clear what assumptions the authors make to exclude situations like this.\n\n[1] T. de Bruin, J. Kober, K. Tuyls and R. Babu\u0161ka, \"Improved deep reinforcement learning for robotics through distribution-based experience retention,\" 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, 2016, pp. 3947-3952.\n[2] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ... & Hassabis, D. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 201611835.", "title": "Interesting idea, but some potential issues.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SyHc3kp1f": {"type": "review", "replyto": "B16yEqkCZ", "review": "\nSUMMARY\n\nThe paper proposes an RL algorithm that combines the DQN algorithm with a fear model.  The fear model is trained in parallel to predict catastrophic states.  Its output is used to penalize the Q learning target.\n\n\n\nCOMMENTS\n\nNot convinced about the fact that an agent forgets about catastrophic states. Because it does not experience it any more.  Shouldn\u2019t the agent stop learning at some point in time?  Why does it need to keep collecting good data?  How about giving more weight to catastrophic data (e.g., replicating it)\n\nIs the catastrophic scenario specific to DRL or RL in general with function approximation?\n\nWhy not specify catastrophic states with a large negative reward?\n\nIt seems that catastrophe states need to be experienced at least once.\nIs that acceptable for the autonomous car hitting a pedestrian?\n", "title": "There could be many other base-line ideas that could avoid the catastrophic scenarios.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "S117txRef": {"type": "review", "replyto": "B16yEqkCZ", "review": "The paper studies catastrophic forgetting, which is an important aspect of deep reinforcement learning (RL). The problem formulation is connected to safe RL, but the emphasis is on tasks where a DQN is able to learn to avoid catastrophic events as long as it avoids forgetting. The proposed method is novel, but perhaps the most interesting aspect of this paper is that they demonstrate that \u201cDQNs  are susceptible to periodically repeating mistakes\u201d. I believe this observation, though not entirely novel, will inspire many researchers to study catastrophic forgetting and propose improved strategies for handling these issues.\n\nThe paper is accurate, very well written (apart from a small number of grammatical mistakes) and contains appealing motivations to its key contributions. In particular, I find the basic of idea of introducing a component that represents fear natural, promising and novel. \n\nStill, many of the design choices appear quite arbitrary and can most likely be improved upon. In fact, it is not difficult to design examples for which the proposed algorithm would be far from optimal. Instead I view the proposed techniques mostly as useful inspiration for future papers to build on. As a source of inspiration, I believe that this paper will be of considerable importance and I think many people in our community will read it with great interest. The theoretical results regarding the properties of the proposed algorithm are also relevant, and points out some of its benefits, though I do not view the results as particularly strong. \n\nTo conclude, the submitted manuscript contains novel observations and results and is likely to draw additional attention to an important aspect of deep reinforcement learning. A potential weakness with the paper is that the proposed strategies appear to be simple to improve upon and that they have not convinced me that they would yield good performance on a wider set of problems. \n", "title": "DQN and catastrophic forgetting", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "H11eRromM": {"type": "rebuttal", "replyto": "SyHc3kp1f", "comment": "We are grateful to Reviewer3 for taking the time to review our paper but disagree with several of the assertions. \n\n1. The reviewer states \u201cNot convinced about the fact that an agent forgets about catastrophic states\u201d. The susceptibility of neural networks to catastrophic forgetting (not to be confused with our safety-motivated notion of a catastrophe) is well-documented in the literature. Whenever the policy is modified such that some states would never be encountered, they will eventually, as soon as they are flushed from the replay buffer, cease to influence the Q-network. If we continue to update the network as is necessary, especially in non-stationary environments (e.g. nearly all real-world settings) then nothing in the standard DQN formulation guards the agent from revisiting the catastrophic states.\n\nIn addition to being well-documented in the literature, we demonstrate this problem clearly in our paper using the simplest failure case. Even in AdventureSeeker, a 1-D environment with only two actions, the agent will eventually forget about the catastrophic states. \n\n2.  Re: \u201cShouldn\u2019t the agent stop learning at some point in time?\u201d\n\na. First, even when there is limited duration learning period, we may want an agent to make a minimal number of catastrophic errors while learning.\n\nb. Second, as stated above: in nonstationary environments, which describes most real-world environments, we want to learn continually.  Otherwise the policy will become stale and cease to perform owing to the shifting dynamics. In the case of driving, imagine new cars which appear on the road, or new street signs. In the case of a vacuum-cleaner, imagine that it is confronted with new household appliances that didn\u2019t exist previously. \n\n3. \u201cRe: \u201cWhy not specify catastrophic states with a large negative reward?\u201d\n\nUnfortunately, even large negative rewards are eventually forgotten, leading the agent to revisit the catastrophic states.  Take AdventureSeeker as an example: no matter how negative the penalty is, the same catastrophic forgetting will eventually happen. Moreover, this approach, unlike ours, has no notion of \u201cdanger zone\u201d and therefore does not benefit from reward shaping. In our approach, the agent avoids even getting *close* to a catastrophe. When this assumption is reasonable, this leads to significantly faster exploration.\n\n4. Re:  \u201cIt seems that catastrophe states need to be experienced at least once. Is that acceptable for the autonomous car hitting a pedestrian?\u201d\n\nThis is a good question that has implications for all of RL: If catastrophes can truly never be experienced even once, then is reinforcement learning off the table altogether?\n\nHowever, in many settings, perhaps even car accidents, if enough cars are on the road and the probability of an accident is nonzero, then accidents will happen. Our work addresses how to learn from these mistakes rapidly and to guard against repeating the same mistakes in the future. \n", "title": "Response to AnonReviewer3"}, "Sy1zTBomM": {"type": "rebuttal", "replyto": "SkYNcg5xz", "comment": "Thanks for the thoughtful review of our paper. \n\n1. We are glad that you noticed the issue in the proof of theorem 1. Per your feedback, we have corrected the proof and substantially revised the paper (see current revision). At a high level, the performance degradation, as described corrected theorem and proof are as follows:\n\nIf the optimal policy, \\pi^*, of the original environment, without intrinsic fear, (M), visits the fear zone with probability at most \\epsilon, then applying  pi^* on the environment with intrinsic fear (M,F), gives the return of eta^*-\\epsilon\\lambda(Rmax-Rmin) therefore, the optimal policy on (M,F), \\tilde{\\pi}, can not give a return less than \\eta^*\\epsilon-\\lambda(Rmax-Rmin) on environment (M,F). If \\tilde{\\pi} visits the fear zone with probability \\epsilon\u2019, we can rewrite its return as:\n(return from non intrinsic fears)-epsilon\u2019(\\lambda(Rmax-Rmin)\nTherefore applying \\tilde{\\pi} on original environment (M) gives a return of at least \\eta^*-\\epsilon\\lambda(Rmax-Rmin) +\\epsilon\u2019\\lambda(Rmax-Rmin) which is lower bounded by \\eta^*-\\epsilon\\lambda(Rmax-Rmin).\n\n2. Regarding the parameter k_r, as the reviewer mentioned, without any prior knowledge and posed safety constraint of the environment, this parameter needs to be chosen empirically, as with other hyper-parameters. We note however, that this is a kind of prior knowledge that might be reasonably to expect of an algorithm designer. For example, a robot should perhaps never be too close to a cliff or a ledge. \n\nIntuitively, small k_r\u2019s better preserve the original policy, but for too small a k_r, the fear model might be ignored. On the other hand, large k_r are better at preventing the agent from visiting the catastrophic states but run more risk of deviating substantially from the optimal policy. Prior knowledge of the environment can guide us to design a proper k_r, otherwise, k_r needs to be chosen experimentally.\n\n3. Regarding the (very) long term forgetting, the reviewer is correct that this paper doesn\u2019t completely alleviate catastrophic forgetting an that we instead guard against the worst consequences. We have created a video to visualize the fear probability as a red overlay on the video game play and will continue to work on other ways to qualitatively understand how our algorithm is working.  \n\n4. We thank the reviewer for suggesting baselines to compare to.  They have some relevance but are designed for different purposes.  In particular,\n[1] (IROS) uses a second experience replay buffer to store state transitions that covers the whole state space uniformly, in addition to a typical buffer used in standard DQN.  This approach aims mostly to reduce exploration, but can face the curse of dimensionality as it tries to cover the state space uniformly.  Moreover, the uniform covering idea is not efficient for avoiding catastrophic events that are rare, while our approach uses a fear classifier to target danger zones directly.\n[2] (PNAS) takes a Bayesian approach to continual learning, trying to avoid catastrophic forgetting of solutions to earlier tasks that have not occured for a long time.  In contrast, our problem is to avoid running into catastrophic states in the same task.  It is not clear how a similar, Bayesian variant of DQN (such as BBQ) can be extended to address our safe exploration challenge.\n", "title": "Response to AnonReviewer2"}, "rk6W9Si7z": {"type": "rebuttal", "replyto": "S117txRef", "comment": "We thank AnonReviewer1 for a clear and constructive review. We are encouraged that you recognize the importance of the problem addressed and the novelty of the methods. Per your suggestions, we have polished the paper, fixing several of the typos that had made it into the first draft. The reviewer\u2019s point that many aspects of the algorithm can likely be improved upon in future work is well-taken. We hope that this is just one of the first among many papers to improve with respect to these fundamental problems. ", "title": "Response to AnonReviewer1"}, "B10tYHsQG": {"type": "rebuttal", "replyto": "B16yEqkCZ", "comment": "We would like to thank the reviewers for taking the time to leave thoughtful reviews. Given this feedback, we have significantly improved the draft and hope the reviewers and area chair will take this into account when assessing the final scores. For example, the harsh score from reviewer 2 owes largely to a mistake in one theorem that has since been fixed in the newest version. We are also grateful to the folks at the reproducibility project who noted the commendable clarity and reproducibility of our paper, algorithm and empirical findings. Please find specific rebuttals to each reviewer as replies to the respective reviews. ", "title": "General reply to reviewers and area chair"}, "BkEm_rimM": {"type": "rebuttal", "replyto": "B1mRwQzQf", "comment": "Note that there are many out-of-the-box DQNs available. They do not all achieve the same performance on every game. DRL is unfortunately still rather brittle to small implementation changes. For example, if you alter (SGD vs Momentum vs ADAM), size of initial replay buffer before reducing epsilon to < 1, etc. you will notice that often each agent will do better for some games (sometimes strikingly) and worse on others. We cannot vouch for the performance of every configuration of DQN you might access, only for the specific implementation that we used. One small detail that could potentially explain some differences is that we used a smaller initial replay buffer size than in the original DQN paper. Perhaps this additional early exploration was crucial for DQN but not for the Intrinsic Fear model.", "title": "Reply"}, "BybJ2MqxG": {"type": "rebuttal", "replyto": "S16Q2IXlM", "comment": "Happy to share details - pardon the delay due to holiday travel. Need to get back home to look up exact details on hyperparameter settings as the toy environment experiments were done a while ago.\n\nYes the hyper-parameters can make a big difference on many of these problems. Optimizer, number of exploration turns, etc. There's also a large amount of variance across runs. Especially on the toy environments. That's why we run every experiment multiple times and report averages.  \n\nThanks for the questions and for holding tight, more details on toy environments coming soon!", "title": "Of course!"}, "rJckvqkgz": {"type": "rebuttal", "replyto": "rk3V5Hjkf", "comment": "Hi Nick,\n\nThanks for your interest in our paper! The code is actually open sourced now, and already one group of researchers has re-implemented our algorithm from scratch and confirmed outperformance of DQN. To preserve double blind status, we won't post the GitHub link here but it's not too hard to find.\n\nCheers,\n\nAuthors", "title": "Posted Code"}}}