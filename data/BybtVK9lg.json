{"paper": {"title": "Autoencoding Variational Inference For Topic Models", "authors": ["Akash Srivastava", "Charles Sutton"], "authorids": ["akash.srivastava@ed.ac.uk", "csutton@inf.ed.ac.uk"], "summary": "We got autoencoding variational bayes to work for latent Dirichlet  allocation using one weird trick. The new inference method then made it  easy to make a new topic model that works even better than LDA.", "abstract": "Topic models are one of the most popular methods for learning representations of\ntext, but a major challenge is that any change to the topic model requires mathematically\nderiving a new inference algorithm. A promising approach to address\nthis problem is autoencoding variational Bayes (AEVB), but it has proven diffi-\ncult to apply to topic models in practice. We present what is to our knowledge the\nfirst effective AEVB based inference method for latent Dirichlet allocation (LDA),\nwhich we call Autoencoded Variational Inference For Topic Model (AVITM). This\nmodel tackles the problems caused for AEVB by the Dirichlet prior and by component\ncollapsing. We find that AVITM matches traditional methods in accuracy\nwith much better inference time. Indeed, because of the inference network, we\nfind that it is unnecessary to pay the computational cost of running variational\noptimization on test data. Because AVITM is black box, it is readily applied\nto new topic models. As a dramatic illustration of this, we present a new topic\nmodel called ProdLDA, that replaces the mixture model in LDA with a product\nof experts. By changing only one line of code from LDA, we find that ProdLDA\nyields much more interpretable topics, even if LDA is trained via collapsed Gibbs\nsampling.", "keywords": ["Deep learning", "Unsupervised Learning", "Applications", "Optimization"]}, "meta": {"decision": "Accept (Poster)", "comment": "The reviewers agree that the approach is interesting and the paper presents useful findings. They also raise enough questions and suggestions for improvements that I believe the paper will be much stronger after further revision, though these seem straightforward to address."}, "review": {"SJMp-Kf8e": {"type": "rebuttal", "replyto": "BkiuiOfUx", "comment": "Right, and our main point is that this comparison is *not* lacking. \n\nWe present the added value of each trick separately in Figure 5 in the paper. \n\nWe pointed that out in our response, three messages above yours in this thread.\n\nThe review claims that the comparison is lacking, but the review is incorrect. (Or we simply misunderstood the review, but if so, the reviewer has not chosen to clarify.)\n\nWe're happy to add important comparisons or to clarify the paper, but to be entirely honest, at this point we're having trouble understanding what the confusion is?", "title": "More about comparison"}, "BkiuiOfUx": {"type": "rebuttal", "replyto": "BygtE3WNe", "comment": "Thanks for the clarification. The main point is that you may consider to provide the added value of each trick separately, which is currently lacking.", "title": "RE:about comparison"}, "rJJmtoZLe": {"type": "rebuttal", "replyto": "Hy-FkbfVl", "comment": "Thanks for the comments, which we\u2019ll incorporate. Here are the clarifications to your questions:\n\n1. Reason for not waiting for DMFVI to finish: In order to make sure the reported results were statistically significant we ran every method for multiple (40) times. This would have not been practically possible for DMFVI which was taking more than 24 hours for a single run.\n\n2. Perplexity vs Topic Coherence: J. Chang et al, (2009) in Reading tea leaves: How humans interpret topic models show that in fact perplexity correlates negatively with topic interpretability. Our results are in line with their findings.\n\n3. We agree and recently we found that a BN inspired reparametrization of the topic matrix helps a great deal in improving the topic coherence for NVLDA as well so we are in the process of extending  the training section of the paper with the effect of normalization, learning rate and momentum scheduling on the latent topics.", "title": "response"}, "HJNJuoWLe": {"type": "rebuttal", "replyto": "SJ2TV3CQl", "comment": "Thanks for your feedback and the correction in eq 4.1. Here are a few points that we would like to clarify to make our contributions more specific.\n\n1. Sensitivity to optimization tricks: VAEs and GANs are notorious for being sensitive to optimization parameters. One of the main contributions of our paper is to work out which optimization tricks are necessary for VAEs to work with LDA (and other mixed-membership methods). Given how important a probabilistic model  LDA is, we feel that this is an important contribution.\n\nOnce the normalizations have been applied before softmax non-linearities the training method does not seem to be sensitive to the numerical parameters of the optimization scheme. In fact, we used the same learning rate and momentum across both data set and both models. \n\n2. Another key highlight is the speed of training which is known to be quite a problem area for VAEs.  The proposed method drastically speeds up inference in one of the most celebrated ML models. On 20newsgroup dataset, it takes 46 seconds compared to several minutes and hours of inference time needed for DMFVI and collapsed gibbs. In addition, our proposed model increases the topic quality three times over the state-of-art in the same 46 seconds of inference time, which in our opinion is both highly useful and exciting. \n\n4. Simple exponential family: While proLDA can be seen as an extension to SePCA (with non-gaussian priors), that would be true of several other mixed membership models simply because SePCA is fairly general architecture. Secondly, it is not clear how would one extend the proposed inference for SePCA to prodLDA. \n\n5. Vocabulary Size: 20newsgroup = 2000 whereas RCV1 = 10,000. The method works just as well for higher dimensional  sparse representations without any change or additional adjustments. \n\n6. Batch-Normalization: We have recently been able to improve the NVLDA learning via a re-parameterization scheme for the topic matrix that is inspired by batch normalization .Now it performs significantly better than NVDM and Gibbs Sampler (in some cases) in terms of the topic coherence.", "title": "response"}, "S1IKDsWIx": {"type": "rebuttal", "replyto": "SJoekNG4g", "comment": "Thanks for the review. Here are our clarifications and answers.\n\nOur results do not conflate modelling and inference. It\u2019s true that we introduce both a new model ProdLDA and a new inference method, but we evaluate them separately. First we compare the new inference method to old inference methods on an existing model, LDA. Then we compare the new model, ProdLDA, to the old model, LDA, with the same inference method.\n\nWe included the comparison to prodLDA as an example to show how easy it is to apply our method to difficult models. ProdLDA is an interesting model for this approach precisely because it is difficult to get good baselines with other inference methods.\n\nAs far as we are aware, prodLDA is a novel topic model related to exponential family harmonium like NVDM.\n\nFigure 1 and Sparsity: theta are sampled from the posterior p(theta|doc,alpha). alpha is the concentration parameter of the prior on theta. Sparsity is shown through the amount of probability mass that the different components are allocated under the different choices of prior. For gaussian prior, it seems that all components are getting roughly the same mass, whereas, for dirichlet, fewer components get more mass and the rest of the components get hardly any. This is in fact also the motivation for the choice of dirichlet priors in the original LDA model.\n\nHyper-parameter selection:  For other LDA inference methods we used the built in hyperparameter optimization option in Mallet or scikit-learn, respectively. For NVDM we used the author\u2019s code and because our method is so fast, it\u2019s easy to embed them within an optimization scheme for alpha, like is used in mallet. So we used BO for HP selection and used it for all the experiments. \n\nUni-modal: No, Dirichlet is unimodal in the softmax basis. See Philipp Hennig, David H Stern, Ralf Herbrich, and Thore Graepel. Kernel topic models. In AISTATS, pp. 511\u2013519, 2012, Section 3.3 and David JC MacKay. Choice of basis for laplace approximation. Machine learning, 33(1):77\u201386, 1998, Section 2.\n\nStatistical significance: Yes the results are statistically significant. Across 40 different random initializations, the standard deviation is very low.\n", "title": "response"}, "ryVcLobLx": {"type": "rebuttal", "replyto": "ByNv1p1Qe", "comment": "Thanks for your question. We already tease out the difference between the model and inference. The way that we do this is by comparing different inference algorithms for LDA. We compare neural inference for LDA to standard mean-field and Gibbs sampling for LDA.\n\nWe agree that it would be interesting to compare different inference methods for ProdLDA as well, but this is more difficult than it may seem. The problem is that the Dirichlet prior p(theta) is not conjugate to the product of multinomials p(w_i | theta). This means that we cannot use collapsed Gibbs sampling, and mean field methods are significantly more difficult to apply.\n\nThe reason that we introduced prodLDA was simply to show that carrying out inference in newer models does not require the same mathematical heavy lifting as traditional VI and collapsed Gibbs methods, and the results in the paper already makes this point. ", "title": "model and the inference"}, "S1jUHn-El": {"type": "rebuttal", "replyto": "BkpbhEvQe", "comment": "Thanks for your comments. Our preprocessing steps are described in the discussion thread above. As described above, we used standard toolkits both for feature processing and for the baseline LDA inference methods, so that our results should be easy to replicate. We use the same preprocessing steps in all of the experiments that we report, so that all of the comparisons that we report are fair. We would be happy to help you reproduce these results. If you wish to ensure that your data set is processed in the same way as ours, please contact us via email and we will share our processed data with you.\n", "title": "replication"}, "BygtE3WNe": {"type": "rebuttal", "replyto": "HJRj9BwQx", "comment": "In reverse order:\n\nContribution: It is true that the contribution of this paper is mostly empirical. We have made no attempt to claim otherwise. We show how to make VAE training for LDA work, by using batch normalization, a Laplace approximation to the Dirichlet prior, and careful tuning of momentum.  We have found training a VAE for LDA is more difficult than it may seem at first; if it were not, others would have already reported results on this. Given how important the LDA model is, we feel that showing exactly how to train VAEs for LDA successfully, and showing as well that a VAE beats standard inference methods by a wide margin, is an important empirical contribution.\n\nComparing different inference methods for ProdLDA: We'll respond to this question below (as the ICLR reviewer also asks this)\n\n\"it makes more sense to compare the amortised VI with other inference method for \u2018LDA\": We agree. This is why we made that comparison in the paper. We compared amortised VI with collapsed Gibbs and decoupled mean field VI. These are, by far, the two most popular, standard inference methods for LDA. Adding a comparison to collapsed variational would add no additional insight.", "title": "about comparison"}, "HJRj9BwQx": {"type": "rebuttal", "replyto": "SJfvW3SXl", "comment": "I see your point. It is important to emphasise the contribution is a new inference method for conventional topic model (LDA). From this perspective it makes more sense to compare the amortised VI with other inference method for \u2018LDA\u2019. It is desirable to include collapsed variational inference (Teh et al.,) as well. On the other hand, it seems to me you also want to propose a new topic model by \u2018changing only one line of code of LDA\u2019, it is then important to assess the relative contribution by comparing different topic models using the same inference method. I would suggest to compare ProdLDA with LDA in a conventional inference setting to tease out the contributions and then move on to neural inference. My two cents of `fairness\u2019 are mainly raised in this regard. These tricks are rather off-the-shelf and can be applied to NVDM as well. (It seems to me that 'your optimisation techniques' are just tweaking the parameters of the ADAM optimiser, batch normalisation and dropout, although you mentioned the intuition of component collapsing. Don't over claim.)\n", "title": "comparison"}, "BkpbhEvQe": {"type": "rebuttal", "replyto": "ryb3torQl", "comment": "Thanks for the clarification. Could you please let me know if the following preprocessing steps are used: removing common stopwords, stemming, and then considering the 2000 most frequent words. Or do you mean you use the same preprocessing steps across your own models which are not in consistent with previous work?\nI ask this mainly because I am unable to reproduce the collapsed Gibbs sampling baseline either and the second author mentions (in previous posts) that your pre-processing is different from the Replicated Softmax paper.", "title": "Re: Comparison"}, "SJfvW3SXl": {"type": "rebuttal", "replyto": "r1CgOu77l", "comment": "Thanks for your review. We would like to point out a few things:\n\nThe main point of our paper is to determine what approximations/optimization techniques are necessary to make VAE inference work for LDA. So the most important comparison is between the different inference methods for LDA. The results for NVDM are mostly for reference.\n\nThat said, after the submission we have verified our results for NVDM by using the code from the original author and we'll update the paper to reflect that.\n \nAlso, we actually do explore the effect of each trick separately, as the reviewer suggests (see Tables 4, 5 and Figure 1).\n\nIt is an interesting question as to whether the optimization techniques that we use for VAE+LDA would also help the performance of NVDM. We have tried this and found that our optimisation techniques do improve the topic coherence for NVDM, but it is still less than LDA+Gibbs and prodLDA. We'll update the paper to reflect this.\n", "title": "Response to anonymous public review"}, "ryb3torQl": {"type": "rebuttal", "replyto": "SJ0Uv7Vmx", "comment": "Thanks for the comment. We use exactly the same preprocessing steps for all methods, so that the comparison is fair.", "title": "Re: Comparison"}, "SJ0Uv7Vmx": {"type": "rebuttal", "replyto": "HJITnwwbe", "comment": "It is suggested that the author could use the same pre-processing step as the Replicated Softmax paper, which NVDM follows and compares against. Otherwise the numbers are not comparable.", "title": "Comparison to NVDM and Replicated Softmax"}, "ByNv1p1Qe": {"type": "review", "replyto": "BybtVK9lg", "review": "In some of the experiments, it seems like ProdLDA + neural variational inference (NVI) works best. Given that NVI is worse than collapsed Gibbs sampling for LDA, the experiments seem to confound improvements to model vs inference. Can you evaluate a difference inference scheme for ProdLDA? That'd help tease out the relative significance of the contributions. The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)\n\nIn general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. \n\nCan you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?\n\nFigure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads \"log p(topic proportions)\" which is a bit confusing.\n\nSection 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal?\n\nNone of the numbers include error bars. Are the results statistically significant?\n\n\nMinor comments:\n\nLast term in equation (3) is not \"error\"; reconstruction accuracy or negative reconstruction error perhaps?\n\nThe idea of using an inference network is much older, cf. Helmholtz machine. \n", "title": "improvements to model (ProdLDA / LDA) vs improvements to inference (NVI / CGS)", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "SJoekNG4g": {"type": "review", "replyto": "BybtVK9lg", "review": "In some of the experiments, it seems like ProdLDA + neural variational inference (NVI) works best. Given that NVI is worse than collapsed Gibbs sampling for LDA, the experiments seem to confound improvements to model vs inference. Can you evaluate a difference inference scheme for ProdLDA? That'd help tease out the relative significance of the contributions. The authors propose NVI for LDA variants. The authors compare NVI-LDA to standard inference schemes such as CGS and online SVI. The authors also evaluate NVI on a different model ProdLDA (not sure this model has been proposed before in the topic modeling literature though?)\n\nIn general, I like the direction of this paper and NVI looks promising for LDA. The experimental results however confound model vs inference which makes it hard to understand the significance of the results. Furthermore, the authors don't discuss hyper-parameter selection which is known to significantly impact performance of topic models. This makes it hard to understand when the proposed method can be expected to work. \n\nCan you maybe generate synthetic datasets with different Dirichlet distributions and assess when the proposed method recovers the true parameters?\n\nFigure 1: Is this prior or posterior? The text talks about sparsity whereas the y-axis reads \"log p(topic proportions)\" which is a bit confusing.\n\nSection 3.2: it is not clear what you mean by unimodal in softmax basis. Consider a Dirichlet on K-dimensional simplex with concentration parameter alpha/K where alpha<1 makes it multimodal. Isn't the softmax basis still multimodal?\n\nNone of the numbers include error bars. Are the results statistically significant?\n\n\nMinor comments:\n\nLast term in equation (3) is not \"error\"; reconstruction accuracy or negative reconstruction error perhaps?\n\nThe idea of using an inference network is much older, cf. Helmholtz machine. \n", "title": "improvements to model (ProdLDA / LDA) vs improvements to inference (NVI / CGS)", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HJITnwwbe": {"type": "rebuttal", "replyto": "HJcj5wIWe", "comment": "Our pre-processing is different from the Replicated Softmax paper. See section 6, paragraph 2 (Essentially, we use the CountVectorizer with default settings and do not stem the tokens). Therefore, our mean wordcount and std. are not the same as the ones you mentioned. If that doesn't help, please feel free to contact us via email and we'd be happy to provide you our pre-processed data files and help figure out what is going on in more detail.", "title": "Re:Re:Re: Perplexity"}, "HJcj5wIWe": {"type": "rebuttal", "replyto": "HycAYjmbg", "comment": "Thanks for the detailed answer. Unfortunately I'm unable to reproduce the results on my dataset. I'm curious if we are using the same preprocessing. Could you confirm that your preprocessed dataset has roughly the same mean word count and standard deviation as reported in the original Replicated Softmax paper (Mean: 51.8, St. Dev: 70.8)?", "title": "Re:Re: Preplexity"}, "HycAYjmbg": {"type": "rebuttal", "replyto": "SJOMe3ybl", "comment": "Thanks for your question. We used the standard Mallet implementation with 100,000 iterations and 'optimise interval' set to 1000 for training on 20 Newsgroup. To evaluate perplexity we used \"left-right algorithm\" (Evaluation Methods for Topic Models) with re-sampling set to True as implemented in Mallet on the entire (original) \"test set\" split.\n\nSeveral papers (for example, Neural Variational Inference for Text Processing) have reported  higher numbers for LDA perplexity on the test set than ours. It seems that many authors have repeated the numbers from (Replicated Softmax: an Undirected Topic Model). However, their results from LDA are not comparable to ours, because in their work  only 50 (randomly sampled) held out documents were used  for computing perplexities and NOT the entire test set. Therefore, those  numbers should be borrowed with caution. Indeed, many later papers that compare against those results do not clearly state whether they  run their methods on the same subset.\n\nAnother difference between the results that we report for LDA and those in the replicated softmax paper is  our use of the left-right algorithm rather than AIS. We expect that to  be a less important difference, as (Evaluation Methods for Topic Models) showed that the LR algorithm generally agrees with AIS.", "title": "Response: Perplexity"}, "SJOMe3ybl": {"type": "rebuttal", "replyto": "BybtVK9lg", "comment": "The perplexity you're reporting for the 20 Newsgroups dataset using LDA Collapsed Gibbs is better than for any other method I've seen.  Would you mind sharing the parameters you used and/or the preprocessed dataset?", "title": "Perplexity"}}}