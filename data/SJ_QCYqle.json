{"paper": {"title": "Semi-Supervised Detection of Extreme Weather Events in Large Climate Datasets", "authors": ["Evan Racah", "Christopher Beckham", "Tegan Maharaj", "Prabhat", "Christopher Pal"], "authorids": ["eracah@lbl.gov", "christopher.beckham@polymtl.ca", "tegan.maharaj@polymtl.ca", "prabhat@lbl.gov", "christopher.pal@polymtl.ca"], "summary": "Semi-supervised 3D CNN's improve bounding box detection of weather events in climate simulations compared to supervised approaches.", "abstract": "The detection and identification of extreme weather events in large scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system.\nRecent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, there are many different types of spatially localized climate patterns of interest (including hurricanes, extra-tropical cyclones, weather fronts, blocking events, etc.)\nfound in simulation data for which labeled data is not available at large scale for all simulations of interest.\nWe present a multichannel spatiotemporal encoder-decoder CNN architecture for semi-supervised bounding box prediction and exploratory data analysis.\nThis architecture is designed to fully model multi-channel simulation data, temporal dynamics and unlabelled data within a reconstruction and prediction framework so as to improve the detection of a wide range of extreme weather events. \nOur architecture can be viewed as a 3D convolutional autoencoder with an additional modified one-pass bounding box regression loss. \nWe demonstrate that our approach is able to leverage temporal information and unlabelled data to improve localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data, and facilitate further work in understanding and mitigating the effects of climate change.", "keywords": ["Semi-Supervised Learning", "Applications", "Computer vision"]}, "meta": {"decision": "Reject", "comment": "The paper addresses the little explored domain (at least from computer vision perspective) of analyzing weather data. All reviewers found the application interesting, yet felt that the technical side was somewhat standard. On the positive note, the authors try to use semi-supervised techniques due to limited labeled data, which is less explored in the object detection domain. The also released code, which is a plus. Overall, the application is nice, however, none of the reviewers was swayed by the paper. I suggest that the authors re-submit their work to a vision conference, where applications like this may be more enthusiastically received."}, "review": {"BJLVneP8l": {"type": "rebuttal", "replyto": "Bk28SarVl", "comment": "Thank you very much for your feedback and kind words! Below, we respond to some points made:\n\nR: \u201cMinor: table 4 -- shouldn\u2019t the semi-supervised models have more parameters than the corresponding supervised ones due to the decoder layers?\u201d\n\nA: The decoder part of the model uses tied weights, so no additional parameters are introduced in the semi-supervised formulation of the network\n\nR: \u201cAs other reviewers pointed out, the use of the 0.1 IoU criterion for true positives is very loose relative to the standard 0.5 criterion.  On the other hand, if the results visualized in Figure 3 are typical, a 0.1 overlap criterion could be reasonable for this domain as the detector does seem to localize events well enough that the system could be used to expedite human review of the climate images for extreme events.  Still, it would be useful to also report results at higher overlap thresholds.\u201d\n\nA: The results are still useful to climate scientists, but the more accurate the detections the better. We have included results for IOU 0.5 in a third revision on January 13 and an interpretation of those results.\n\nThanks again!", "title": "response to review"}, "SkOUsxPUl": {"type": "rebuttal", "replyto": "HkaczsZNg", "comment": "Thank you for your review. Even though you have deemed a rejection, we appreciate that you still took the time to read it and give constructive, useful criticism. Below we respond to some points you made: \n\nR:\u201dFinally, is there any baseline approach the authors could report or compare too?\u201d\n\nA: We decided to use the 2D supervised approach as a baseline, but  we see how it could potentially be useful to show pure YOLO or SSD results.\n\n\nR: \u201cIf the goal of this work is to count the number of such instances this is defensible, but for localization this seems overly loose.Furthermore, the 3D CNN architecture (which is one of the the core novelties of this work compared to past efforts) does not seem capable of producing variable sized boxes (as noted in the last paragraph of page 7), which I imagine results in poorer performance at higher IoU thresholds (as many of the weather events appear small).\u201d\n\n\nA: Counting is useful, but climate scientists do eventually want full on detection. That being said, there is some utility in rough localization at least as a starting point. We have posted results for 0.5 IOU in the January 13 revision. You are correct in that the results were much worse due to the more stringent criteria and the shortcomings of the model. We address and interpret the decline in accuracy and these shortcomings in this revision (to save you time, almost all the changes are in section 4.3 and tables 4 and 5. The other changes are just an addition of a link to code and a link to download the data).\n\nThanks again for your time and effort!\n", "title": "response to review"}, "BylSyqZIg": {"type": "rebuttal", "replyto": "BkfVLbMEx", "comment": "Dear Reviewer2,\nThank you for your thoughtful, thorough review. Below are some responses to your review.\nR:\u201cThe biggest concern w the paper though is experimental results. Only a single figure and table of results are shown (figure 4 and table 4).The metrics are not defined (what is mean average recall?).\u201d \nA:In our revision from December 1st, we included Table 4 and Table 5 as results. Table 4 which show mAP for 2D, 3D, semi-supervised and supervised in addition to different capacities for 2D and a different lambda setting. Table 5 shows mAP for all experiments by performance on each class. The mean average recall metric was erroneously named and is removed in the December 1 revision. Figure 3 shows sample boxes and figure 4 shows some visualization of the embeddings of the \u201ccode\u201d layer.\nR:\u201dOne of the things that is unclear is how many events are actually in the training/testing data\u201d\nA: Table 3 in the December 1 revision shows the frequency of classes in train and test data\nR:  The only confusing bit is that the \u201csemi-supervised\u201d loss actually has all the labels used for the \u201csupervised\u201d loss and additionally incorporates the reconstruction loss. Hence, the \u201csemi-supervised\u201d loss is actually stronger, which makes the terminology a bit confusing.\nA: The semi-supervised network uses twice as much data, but all additional data is unlabelled. And yes, even the labelled data is subject to reconstruction penalty\nR: \u201cIt is also unclear if future researchers will be able to reproduce the experimental setting (a commitment to open-source the data or a way to reproduce the experiments would be critical for future authors).\u201d\nA: We will include the link to the github repo (https://github.com/eracah/hur-detect/) in our next revision in addition to a link for downloading the data (http://portal.nersc.gov/project/dasrepo/climate/)\nR: \u201cthe authors use both a classification loss and an \u201cobjectness\u201d loss. I\u2019ve never seen both used together like this\u201d\nA: The two are not usually used together in region proposal based methods, but are used together in the loss for YOLO, which is the method we used as our base method. Most detection methods as you said either have a two step process (RCNN-like networks) or just include classification term (SSD). We used both for two reasons: \n1, we started with YOLO and then tweaked it to improve the detection, so we started with both terms and did not have time to experiment with removing one.\n2. Like YOLO, we wanted a simple one shot detection network that would localize and classify in one pass and we liked the simplicity of the formulation of the YOLO loss, where only boxes that had an object were penalized for getting the class wrong. We thought this seemed to be simplest, logical way to formulate the loss as opposed to having a having a two step process with a post-classifier; especially because the post-classifier often involves a background class and the likelihood of having to tune the ratio of negative to positive images passed through the classifier.\n\nThanks again for your time!", "title": "reviewer response"}, "H1nFikImx": {"type": "rebuttal", "replyto": "S1Q5fKJ7e", "comment": "We picked 0.1 originally to make the accuracy criteria a little easier as we were still developing and improving the model and we never had a chance to try any other values, but in a future revision, we can show results for 0.5 which is more in line with ImageNet.\n", "title": "0.1 IoU Threshold"}, "B1uBo1Lmg": {"type": "rebuttal", "replyto": "HyQGuhJ7x", "comment": "Originally we were looking at recall and precision at a certain confidence threshold for the boxes (i.e. what is the precision and recall for model when we only take into account box guesses that have a confidence/objectness probability greater than 0.8). Those were erroneously labelled as mAP and mAR. In the revision on December 1, we corrected that to just report the true mAP (integrate over a precision-recall curve) and as you correctly said there is no analog for recall with that definition of mAP.", "title": "mAR"}, "HyQGuhJ7x": {"type": "review", "replyto": "SJ_QCYqle", "review": "How is the 'mean average recall' (mAR) metric reported in table 4 defined? (I've never heard of this metric before, and it's not obvious to me what it would mean -- a direct analog of Average Precision would not be well-defined.)This paper applies convnet-based object detection techniques to detection of weather events from 3D climate data, additionally exploring the effect of using an unsupervised autoencoder-style objective term.\n\n\nPros:\n\nThe application of object detection techniques to extreme weather event detection problem is unique, to my knowledge.\n\nThe paper is well-written and describes the method well, including a survey of the related work.\n\nThe best model makes use of 3D convolutions and unsupervised learning, both of which are relatively unexplored in the detection literature.  Both of these aspects are validated and shown to produce at least small performance improvements over a 2D and/or purely supervised approach.\n\n\nCons:\n\nThe benefits of the 3D convolutional architecture and unsupervised learning end up being a little underwhelming, with 52.92% mAP for the 3D+semi-sup result vs. 51.42% mAP for the 2D+sup result.\n\nIt\u2019s a bit strange that 3D+sup and 2D+semi-sup are each worse than the 2D+sup base result;  I\u2019d expect each aspect to give a slight improvement over the base result, given that using both together gives the best results -- perhaps there was not a thorough enough hyperparameter search for these cases.  The paper does acknowledge this and provide potential explanations in Sec. 4.3, however.\n\nAs other reviewers pointed out, the use of the 0.1 IoU criterion for true positives is very loose relative to the standard 0.5 criterion.  On the other hand, if the results visualized in Figure 3 are typical, a 0.1 overlap criterion could be reasonable for this domain as the detector does seem to localize events well enough that the system could be used to expedite human review of the climate images for extreme events.  Still, it would be useful to also report results at higher overlap thresholds.\n\nMinor: eq 6 should (probably) be the squared L2 norm (i.e. the sum of squares) rather than the L2 norm itself.\n\nMinor: table 4 -- shouldn\u2019t the semi-supervised models have more parameters than the corresponding supervised ones due to the decoder layers?\n\n\nOverall, this paper is well-written and applies some interesting underutilized techniques to a relatively unique domain.\f  The results aren't striking, but the model is ablated appropriately and shown to be beneficial.  For a final version, it would be nice to see results at higher overlap thresholds.", "title": "mAR", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "Bk28SarVl": {"type": "review", "replyto": "SJ_QCYqle", "review": "How is the 'mean average recall' (mAR) metric reported in table 4 defined? (I've never heard of this metric before, and it's not obvious to me what it would mean -- a direct analog of Average Precision would not be well-defined.)This paper applies convnet-based object detection techniques to detection of weather events from 3D climate data, additionally exploring the effect of using an unsupervised autoencoder-style objective term.\n\n\nPros:\n\nThe application of object detection techniques to extreme weather event detection problem is unique, to my knowledge.\n\nThe paper is well-written and describes the method well, including a survey of the related work.\n\nThe best model makes use of 3D convolutions and unsupervised learning, both of which are relatively unexplored in the detection literature.  Both of these aspects are validated and shown to produce at least small performance improvements over a 2D and/or purely supervised approach.\n\n\nCons:\n\nThe benefits of the 3D convolutional architecture and unsupervised learning end up being a little underwhelming, with 52.92% mAP for the 3D+semi-sup result vs. 51.42% mAP for the 2D+sup result.\n\nIt\u2019s a bit strange that 3D+sup and 2D+semi-sup are each worse than the 2D+sup base result;  I\u2019d expect each aspect to give a slight improvement over the base result, given that using both together gives the best results -- perhaps there was not a thorough enough hyperparameter search for these cases.  The paper does acknowledge this and provide potential explanations in Sec. 4.3, however.\n\nAs other reviewers pointed out, the use of the 0.1 IoU criterion for true positives is very loose relative to the standard 0.5 criterion.  On the other hand, if the results visualized in Figure 3 are typical, a 0.1 overlap criterion could be reasonable for this domain as the detector does seem to localize events well enough that the system could be used to expedite human review of the climate images for extreme events.  Still, it would be useful to also report results at higher overlap thresholds.\n\nMinor: eq 6 should (probably) be the squared L2 norm (i.e. the sum of squares) rather than the L2 norm itself.\n\nMinor: table 4 -- shouldn\u2019t the semi-supervised models have more parameters than the corresponding supervised ones due to the decoder layers?\n\n\nOverall, this paper is well-written and applies some interesting underutilized techniques to a relatively unique domain.\f  The results aren't striking, but the model is ablated appropriately and shown to be beneficial.  For a final version, it would be nice to see results at higher overlap thresholds.", "title": "mAR", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1Q5fKJ7e": {"type": "review", "replyto": "SJ_QCYqle", "review": "In the paper, you consider detections with a IoU of 0.1 or greater to be true positives. Could you discuss the choice of this threshold? How do your results change as this threshold is increased (i.e. when more accurate bounding boxes are required)? This work presents a novel 3D CNN architecture for climate event detection that combines an unsupervised auto-encoder reconstruction loss with YOLO like bounding box prediction. The approach is trained and evaluated on a large-scale, simulated climate dataset labeled by a costly heuristic approach called TECA. \n\nFor the most part, the paper is nicely written (minor comments below) and addresses an important and well motivated problem. The authors provide sufficient model details to allow reproduction (although public code would be preferred). I find the experiments a bit unconvincing (see below) but appreciate the attention to model capacity (via number of parameter) when comparing the 2D and 3D model variants.\n\nI am concerned that the evaluation may be insufficient to assess the effectiveness of this method. An IoU threshold of 0.1 allows for many rather poor detections to count as true positives. If the goal of this work is to count the number of such instances this is defensible, but for localization this seems overly loose.Furthermore, the 3D CNN architecture (which is one of the the core novelties of this work compared to past efforts) does not seem capable of producing variable sized boxes (as noted in the last paragraph of page 7), which I imagine results in poorer performance at higher IoU thresholds (as many of the weather events appear small). \nThe experiments also feel inconclusive about the effect of temporal modeling and semi-supervision. The temporal component does not seem to matter in the supervised settings (2D 51.45 mAP - 3D 51.00 mAP) but improves somewhat in the semi-supervised case (2D 51.11 mAP - 3D 52.92 mAP). Whereas the additional unlabeled data seems to hurt in the 2D case but improve results for the 3D model. Could the authors provide confidence intervals for these numbers? I would like to see further discussion of these trends especially with respect to the effect of the loss weights (alpha, beta, and gamma). \n\nI also note that it is not clear if both the 2D and 3D models were trained for equivalent time periods (seems like no from last paragraph of page 7). Could a plot of training and validation accuracy for each model be presented for comparison? \n\nFinally, is there any baseline approach the authors could report or compare too? Without one, it is difficult to evaluate the performance of the approach with respect to the difficulty of the problem.\n\nPreliminary Rating:\nI think this is an interesting paper that is well motivated but feel the experiments as presented do not seem adequate to support any conclusive trends. I would like to see the mAP trends across a wider range of IoU values and further discussion of training procedure, loss weight settings, and reasons for lack of bounding box variability in the 3D model (as stated above). \n\nClarification:\nIn the paper you say \"While climate models are run on a 3D grid, with the vertical dimension corresponding to 30 levels; we only consider surface quantities (i.e. 2D data) in this study.\" Could you elaborate on what the surface quantities correspond to? Is it the highest cloud level?\n\nMinor notes:\n\tPlease provide years for Prabhat et al. references rather than a and b.\n\tFootnote in 4.2 could be inline text with similar space.\n\t4.3 second paragraph the word table is not capitalized like elsewhere.\n\t4.3 4th paragraph the word section is not capitalized like elsewhere.\n\nEdit: I appreciate the authors responding to my questions but still feel the relatively poor localization performance at stricter IoU thresholds fails to justify the complexity of the approach. I encourage the authors to continue pursuing this line of research.", "title": "0.1 IoU Threshold", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkaczsZNg": {"type": "review", "replyto": "SJ_QCYqle", "review": "In the paper, you consider detections with a IoU of 0.1 or greater to be true positives. Could you discuss the choice of this threshold? How do your results change as this threshold is increased (i.e. when more accurate bounding boxes are required)? This work presents a novel 3D CNN architecture for climate event detection that combines an unsupervised auto-encoder reconstruction loss with YOLO like bounding box prediction. The approach is trained and evaluated on a large-scale, simulated climate dataset labeled by a costly heuristic approach called TECA. \n\nFor the most part, the paper is nicely written (minor comments below) and addresses an important and well motivated problem. The authors provide sufficient model details to allow reproduction (although public code would be preferred). I find the experiments a bit unconvincing (see below) but appreciate the attention to model capacity (via number of parameter) when comparing the 2D and 3D model variants.\n\nI am concerned that the evaluation may be insufficient to assess the effectiveness of this method. An IoU threshold of 0.1 allows for many rather poor detections to count as true positives. If the goal of this work is to count the number of such instances this is defensible, but for localization this seems overly loose.Furthermore, the 3D CNN architecture (which is one of the the core novelties of this work compared to past efforts) does not seem capable of producing variable sized boxes (as noted in the last paragraph of page 7), which I imagine results in poorer performance at higher IoU thresholds (as many of the weather events appear small). \nThe experiments also feel inconclusive about the effect of temporal modeling and semi-supervision. The temporal component does not seem to matter in the supervised settings (2D 51.45 mAP - 3D 51.00 mAP) but improves somewhat in the semi-supervised case (2D 51.11 mAP - 3D 52.92 mAP). Whereas the additional unlabeled data seems to hurt in the 2D case but improve results for the 3D model. Could the authors provide confidence intervals for these numbers? I would like to see further discussion of these trends especially with respect to the effect of the loss weights (alpha, beta, and gamma). \n\nI also note that it is not clear if both the 2D and 3D models were trained for equivalent time periods (seems like no from last paragraph of page 7). Could a plot of training and validation accuracy for each model be presented for comparison? \n\nFinally, is there any baseline approach the authors could report or compare too? Without one, it is difficult to evaluate the performance of the approach with respect to the difficulty of the problem.\n\nPreliminary Rating:\nI think this is an interesting paper that is well motivated but feel the experiments as presented do not seem adequate to support any conclusive trends. I would like to see the mAP trends across a wider range of IoU values and further discussion of training procedure, loss weight settings, and reasons for lack of bounding box variability in the 3D model (as stated above). \n\nClarification:\nIn the paper you say \"While climate models are run on a 3D grid, with the vertical dimension corresponding to 30 levels; we only consider surface quantities (i.e. 2D data) in this study.\" Could you elaborate on what the surface quantities correspond to? Is it the highest cloud level?\n\nMinor notes:\n\tPlease provide years for Prabhat et al. references rather than a and b.\n\tFootnote in 4.2 could be inline text with similar space.\n\t4.3 second paragraph the word table is not capitalized like elsewhere.\n\t4.3 4th paragraph the word section is not capitalized like elsewhere.\n\nEdit: I appreciate the authors responding to my questions but still feel the relatively poor localization performance at stricter IoU thresholds fails to justify the complexity of the approach. I encourage the authors to continue pursuing this line of research.", "title": "0.1 IoU Threshold", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}