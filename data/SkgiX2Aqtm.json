{"paper": {"title": "PIE: Pseudo-Invertible Encoder", "authors": ["Jan Jetze Beitler", "Ivan Sosnovik", "Arnold Smeulders"], "authorids": ["j.j.beitler@uva.nl", "i.sosnovik@uva.nl", "a.w.m.smeulders@uva.nl"], "summary": "New Class of Autoencoders with pseudo invertible architecture", "abstract": "We consider the problem of information compression from high dimensional data. Where many studies consider the problem of compression by non-invertible trans- formations, we emphasize the importance of invertible compression. We introduce new class of likelihood-based auto encoders with pseudo bijective architecture, which we call Pseudo Invertible Encoders. We provide the theoretical explanation of their principles. We evaluate Gaussian Pseudo Invertible Encoder on MNIST, where our model outperform WAE and VAE in sharpness of the generated images.", "keywords": ["Invertible Mappings", "Bijectives", "Dimensionality reduction", "Autoencoder"]}, "meta": {"decision": "Reject", "comment": "The presented approach demonstrates an invertible architecture for auto-encoding, which demonstrates improvements in performance relative to VAE and WAE's on MNIST. \n\nPros:\n+ R3: The idea of pseudo-inversion is interesting.\n+ R3: Manuscript is clear. \n\nCons:\n- R1,2,3: Additional experiments needed on CIFAR, ImageNet, others.\n- R1: Presentation unclear. Authors have not made any apparent attempt to improve the clarity of the manuscript, though they make their point that the method allows dimensionality reduction in their response.\n- R1, R2: Main advantages not clear.  \n- R3: Text could be compressed further to allow room for additional experiments. \n\nReviewers lean reject, and authors have not updated experiments. Authors are encouraged to continue to improve the work."}, "review": {"SylwAFovLB": {"type": "rebuttal", "replyto": "HylkOySLLB", "comment": "If it is invertible, then yes. But the details depend on the exact method you use.", "title": "Yes"}, "rJlas_tnzN": {"type": "rebuttal", "replyto": "S1xGzSv2G4", "comment": "You can choose g(z) to be a neural network or a pre-fixed analytical function.\n\nBy the way, feel free to contact us directly via email\n", "title": "g(z)=0 alternatives!"}, "Hyxvo6Osf4": {"type": "rebuttal", "replyto": "B1gy49uiMN", "comment": "You are welcome!", "title": "Good!"}, "rJgFlgSKGE": {"type": "rebuttal", "replyto": "HkgfHR4Yz4", "comment": "eps is a constant during optimization.\ndim(g(z)) = dim(r), \nso g(z) = torch.zeros_like(r)\n", "title": "eps is a fixed constant!"}, "H1lir6NKMV": {"type": "rebuttal", "replyto": "r1grhsEFGE", "comment": "eps is a scalar hyper-parameter of the method. \nIs it introduced in Eq. 10\nIts role is discussed after Eq. 17\nIn experiment 5.1 we demonstrate how it affects the process of the encoding.", "title": "eps!"}, "H1lFblNFMN": {"type": "rebuttal", "replyto": "SJlNGufOfN", "comment": "Hello\n\n>> How do I implement r mapping to Normal?\nIn order to train PIE, one maximises the function in Eq. 13\nSo r ~ N(g(z), eps^2) if |r - g(z)|_2^2 = 0\n\n>> What is function g(z) to parameterise mu, is it just a Linear layer from d dim to D-d dim?\ng(z) could be any differentiable function from d to D-d\nIn our experiments we use g(z) = 0\n\n>> If g(z) is Linear layer with input_dim =d and output_dim=D-d ,\n>> is the objective to minimize is r_loss = 0.5* -(g(z).mean() - 0.001)).mean()\nNo. It is r_loss = - ((r - g(z))**2).sum()  / (2 * eps**2)", "title": "Implementation!"}, "Hye21tiOC7": {"type": "rebuttal", "replyto": "BJg5ZZkp3Q", "comment": "Thank you for your review! \n\nFirst of all, we would like to emphasize the fact the PIE is an autoencoder and allows for dimensionality reduction.\nWe refer to PIE as an Autoencoder as it performs the encoding of the data automatically. \nIn contrast to previously published paper on invertible models, PIE allows for the compression of the data \nand chooses the main nonlinear components of the input.\nThe direct comparison to flow-based models such as Real NVP and NICE is not relevant in sense of compression, \nas these models transform the distributions of variables while preserving the dimensionality. \nIn section 3.4 we discuss the relation to flow-based models with multiscale architecture \nand demonstrate that such models may be viewed as one of the configurations of PIEs.\n\nIn our paper, we start from a necessity of the dimensionality reduction and derive a\ngeneral method for achieving this by using invertible models. The models studied in the experiments are chosen to be simple in order to demonstrate the difference between vanilla methods.\nAs always, the proposed model could be used as a backbone for a more complicated setup, \nbut it is out of the scope of the current paper.\n\nWe agree that the experimental part is limited. We will compare our model to a wider class of competitors during the next revision. Thank you for the recommended models to compare with.", "title": "Reply to AnonReviewer1"}, "SJl6C6od0X": {"type": "rebuttal", "replyto": "rJxyHtAn37", "comment": "Thank you for your detailed review!\n\nOn the one hand, flow-based models are tractable by design. They allow for data manipulation and their flexibility is mainly limited by the computational resources. However, these models do not allow one to compress the input data.\n\nOn the other hand, autoencoders provide a method for significant compression of the data.\nNevertheless, the training of an autoencoder requires the minimization of a reconstruction error as one of the terms in a total loss function. \nThe reconstruction error must be specified beforehand and is dependent on the stated requirements of the task.\n\nWhen the task is not defined in advance but the compression of the data is required, \nthe above-listed methods cannot be used. \n\nThe proposed model (PIE) is tractable by design. Moreover, it allows for data compression without specifying the reconstruction error function. The most relevant components are learned from the data.\n\nThank you for the recommended papers. \nWe will consider them during the next revision of the paper.", "title": "Reply to AnonReviewer2"}, "HJgQj2qd0Q": {"type": "rebuttal", "replyto": "SkgeFa7c37", "comment": "Thank you for your review! \n\nWe have changed Fig. 6 (b) so now it is clear. \nIt is g(z) instead of 0. Fig. 6 (b) exactly matches Eq. 20.\n \nWe have also fixed the typos in the text.\nWe agree that the experimental part is limited. \nWe will conduct the experiments on other datasets during the next revision.", "title": "Reply to  AnonReviewer3"}, "BJg5ZZkp3Q": {"type": "review", "replyto": "SkgiX2Aqtm", "review": "PIE extend NICE and Real NVP into situations which require having a smaller dimensionality of the latent variable (d) compared to the dimensionality of the observed variable (D), i.e. d < D. This is done by learning an extension function g(z) from R^d to R^{D-d} and then using the change of variables formula on x and [z, g(z)]. To model probabilistically the deterministic function g(z) is replaced by Normal distribution with mean g(z) and a small variance.\n\nPIE is used to build deep generative models and trained on the MNIST dataset. The authors show that the models learnt via PIE produce sharper samples than VAEs and Wasserstein autoencoders (WAEs). No comparison to real NVP is made, which should be the main baseline of comparison to answer the question of \"what is the advantage of having d < D?\". Further MNIST is no longer a good enough benchmark to evaluate deep generative models. Most representative work in this literature use CIFAR-10, downsampled Imagenet, or Imagenet at 256x256.\n\nThis work falls short of the standards of ICLR in a few ways:\n\n1. The presentation is unclear. The explanation of the extension-restriction idea is overly complicated. Further, the paper does very little to properly contextualize this work in the literature. Real NVP and flow-based models are mentioned but the proposed technique is not compared to it. The authors say they \"introduce new class of likelihood-based Auto-Encoders\", but this is false as far as I understood. The technique is not even an autoencoder since a separate decoder is not trained, and is obtained by exactly inverting the encoder as in real NVP.\n\n2. The experiments are weak. The samples shown are of poor quality, and on a very simplisitic dataset (MNIST). The authors compare with vanilla VAEs, but ignore more recent improvements to VAE such as VAE-IAF, flow-based models, and also autoregressive models. A heuristic is used to measure sharpness and only used to compare against VAE and WAE. Since all these models allow likelihood evaluation, likelihoods should also have been compared.\n\n3. The technique itself is a small change over real NVP and it's not clear whether this change brings any improvements or provides any insights about generative modeling.", "title": "Review", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "rJxyHtAn37": {"type": "review", "replyto": "SkgiX2Aqtm", "review": "In this paper, an invertible encoding method is proposed for learning latent representations and deep generators via inverting the encoder. The proposed method can be seen as an autoencoder without the need to learn the decoder. This can be computed by inverting the encoder. To the best of my knowledge the proposed method is novel and its building blocks are described adequately. \n\nMy main questions are the following: What is the main advantage of this model? Does it make the problem of deep generative model learning tractable? If so, under what conditions?\n\nDiscussion of prior art and relevant methods is limited in the paper and it can be extended. The authors may want to consider discussing relevant work on invertible autoencoders (e.g., https://arxiv.org/pdf/1802.06869.pdf) and methods like https://openreview.net/pdf?id=ryj38zWRb which can be seen as symmetric to the proposed one in the sense that an encoderless autoencoder is learnt. \n\nThe experimental evaluation is limited. The authors should consider to compare their method with other relevant models such as those mentioned above as well as GANs and their variants. Experiments on other more complex real-world data (e.g., faces) are also needed in order to prove the merits of the proposed model.\n", "title": "An interesting model without thorough evaluations  ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "SkgeFa7c37": {"type": "review", "replyto": "SkgiX2Aqtm", "review": "General:\nIn general, this is a well-written paper and I feel pleasant to read the paper. The paper proposed a model named Pseudo Invertible Autoencoder(PIE) which combines invertible architecture and inference model.\n\nStrength:\n1. The explanation of the paper is very clear and consistent.\n2. The idea is interesting. A lot of papers related to the inverse problem focus on perfect invertibility, but the author(s) emphasize the importance of invertible compression and relate PIE to the inference model.\n\nPossible Improvements:\n1. The experiments could have been more convincing: 1) The only competitors are VAE and WAE. 2)The only data set has been tested was MNIST data set. There are many great works mentioned in the paper and those works should also be compared in a way.\n2. The content could be more compact so that more experiments can be shown to support the paper. It seems to me there is too much explanation to previous works in the paper. \n3. The paper has 9 pages which exceed the suggestion a little bit.\n4. I am not sure if the author(s) checked the grammar of the paper carefully. I found quite few typos in the paper. Page 3: 'Rather then' should be 'Rather than' and 'As we are interested' should be as 'As we are interested in'; Page 4: 'Can me' should be 'Can be'; Page 6: 'Better then' should be 'Better than'; Fig.6 (b): Should it be '0' or 'g(z)'?\n\nConclusion:\nThis is a good and clean paper in general. It explains the related work and presents PIE with necessary details. My biggest concern is that empirical validation(experiment) is poor. As a conclusion, I tend to vote for weak rejection.\n\nMinor Suggestion:\nRefer to the conference instead of arXiv if the paper was already published.", "title": "A nice paper but needs stronger experiment results", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "S1e7sluJjX": {"type": "rebuttal", "replyto": "S1xo1vQAqX", "comment": "Thank you for your comment! \n\n1) In Fig. 3 the green lines indicate the aggregation of the variables in objective function. \nFor examples, the blue nodes of the 2nd layer which are discarded (r_k) \nand the variables which will be further processed (z_k) are aggregated in a conditional distribution\np(r_k| z_k) = \\delta(r_k - g_k(z_k)). \nWe will try to change the scheme in order to avoid confusion.\n\n2.1) Why 0?\nIn experiments with Gaussian PIE we used g(z) = 0. \nTherefore, x = [z, 0] as it is indicated in Eq. 20. \nWe will change Fig. 6 (b) in order to exactly match Eq. 20.\n\n2.2) To sample or not to sample?\nIn our experiment we demonstrate the behaviour of function G, defined in Eq. 4.\nThe operation of extension of the function is deterministic. \n\nWe conducted the experiments, where we used sampling from N(g(z), eps^2 I). \nThe obtained images were visually close to those depicted in the current version of the paper.\n\n3) Thank you for providing us with interesting and useful papers. \nWe will consider them during the next revision of the paper.", "title": "Reply"}}}