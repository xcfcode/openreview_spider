{"paper": {"title": "Collaborative Training of Balanced Random Forests for Open Set Domain Adaptation", "authors": ["Jongbin Ryu", "Jiun Bae", "Jongwoo Lim"], "authorids": ["jongbin.ryu@gmail.com", "maybe@hanyang.ac.kr", "jlim@hanyang.ac.kr"], "summary": "", "abstract": "In this paper, we introduce a collaborative training algorithm of balanced random forests for domain adaptation tasks which can avoid the overfitting problem. In real scenarios, most domain adaptation algorithms face the challenges from noisy, insufficient training data. Moreover in open set categorization, unknown or misaligned source and target categories adds difficulty. In such cases, conventional methods suffer from overfitting and fail to successfully transfer the knowledge of the source to the target domain. To address these issues, the following two techniques are proposed. First, we introduce the optimized decision tree construction method, in which the data at each node are split into equal sizes while maximizing the information gain. Compared to the conventional random forests, it generates larger and more balanced decision trees due to the even-split constraint, which contributes to enhanced discrimination power and reduced overfitting. Second, to tackle the domain misalignment problem, we propose the domain alignment loss which penalizes uneven splits of the source and target domain data. By collaboratively optimizing the information gain of the labeled source data as well as the entropy of unlabeled target data distributions, the proposed CoBRF algorithm achieves significantly better performance than the state-of-the-art methods. The proposed algorithm is extensively evaluated in various experimental setups in challenging domain adaptation tasks with noisy and small training data as well as open set domain adaptation problems, for two backbone networks of AlexNet and ResNet-50.", "keywords": []}, "meta": {"decision": "Reject", "comment": "This paper proposes new target objectives for training random forests for better cross-domain generalizability. \n\nAs reviewers mentioned, I think the idea of using random forests for domain adaptation is novel and interesting, while the proposed method has potential especially in the noisy settings. However, I think the paper can be much improved and is not ready to publish due to the following reviewers' comments:\n\n- This paper is not well-written and has too many unclear parts in the experiments and method section. The results are not guaranteed to be reproducible given the content of the paper. Also, the organization of the paper could be improved.\n\n- The open-set domain adaptation setting requires more elaboration. More carefully designed experiments should be presented. \n\n- It remains unclear how the feature extractors can be trained or fine-tuned in the DNN + tree architecture. Applying trees to high-dimensional features sacrifices the interpretability of the tree models, hampering the practical value of the approach.\n\nHence, I recommend rejection."}, "review": {"B1gHvfE_oH": {"type": "rebuttal", "replyto": "H1gbR6xTtB", "comment": "Thank you for helpful comments on our paper.\n\n1. We use noisy labels only for the \u2018source\u2019 domain in the experiments since we design this experiment to validate the robustness of the proposed algorithm. \nWe randomly change the original label to create a noisy setting. The specified portion of changed noise data is 40% in Table 5 and 60% and 80% in Table 4. This noisy setting is also described as label corruption in [1].\nAccordingly, we revised the paper to introduce this detail in page 7, as below.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Revised paper \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nIn this experiment, the training labels of the specified portion of the source domain are randomly changed for the noise condition, which is also referred to as the label corruption in Shu et al. (2019). \nCorruption levels are set to $40, 60$, and $80\\%$ of the source domain (refer to the supplementary material for full experimental results.\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014--\n\n[1] Yang Shu, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Transferable curriculum for weakly supervised domain adaptation. In AAAI Conference on Artificial Intelligence, 2019\n\n\n2. We revised the paper to describe the training process of the proposed method more clearly. As mentioned by R2, the hyperparameters in training the CoBRF and ablation studies are added in page 6 and 7 as below. \n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014--Revised paper \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nTo train the CoBRF, we use 100 trees with a maximum depth of 8.\nWhen there is no training data fallen on a node, we prune the tree at that node.\nThe number of randomly selected feature dimension for the SVM training is set to 250.\nThe input feature of SVM is normalized for the stable learning of the hyperplane.\nWe repeat the SVM training 15 times to select the optimal split in each node.\n\n   Depth | The number of trees (T)\n               |  5 | 10 | 50 | 100 |  \n      6   | 62.2 | 65.4 | 67.5 | 67.7 |\n      7   | 60.2 | 63.5 | 67.1 | 67.8 |\n      8   | 58.9 | 63.3 | 67.5 | 68.0 |\n      9   | 55.6 | 60.0 | 66.2 | 67.6 |\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\u2014-\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n \n3. Yes, for open set domain adaptation, we need to elaborate train models so that it does not simply minimize or adjust domain shifts. The source and target domains in the open set condition have a different set of classes, including unknown classes. Therefore, due to this condition of the open set domain adaptation, the overfitting problem should be suppressed. To do this, we take advantage of the good property of random forests [2], which rarely overfit to the training data since they form weak ensembles of multiple decision trees as studied in previous works [3][4]. Therefore, we argue that the proposed CoBRF is robust to the overfitting problem due to the unbalanced classes and the existence of the unknown class of the open set domain adaptation task.\n\n[2] Leo Breiman. Random forests. Machine Learning, 45(1):5\u201332, 2001.\n[3] Abraham J Wyner, Matthew Olson, Justin Bleich, and David Mease. Explaining the success of adaboost and random forests as interpolating classifiers. The Journal of Machine Learning Research, 18(1):1558\u20131590, 2017.\n[4] Heitor M Gomes, Albert Bifet, Jesse Read, Jean Paul Barddal, Fabr\u00edcio Enembreck, Bernhard Pfharinger, Geoff Holmes, and Talel Abdessalem. Adaptive random forests for evolving data stream classification. Machine Learning, 106(9-10):1469\u20131495, 2017.\n \n4. First of all, we apologize for missing the detailed description of the Openset1 experiment. We follow the setting of Saito et al. (2018) [5], which do not use 21-31 classes of the source data for training, and thus only 1-10 classes of source domain are included in the training step. \nWe detect the unknown class by thresholding the estimated probability of the CoBRF for target data. The threshold value is set to 0.3 in the experiments. \n\nWe revised the paper as follows, and you can find the revised version on page 8 of the paper.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014 Revised paper \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nall data with label 21$\\sim$31 in the target domain are used as one unknown class. According to Saito et al. (2018), the unknown class of the source data is not used in training, and the unknown class of the target data is classified by thresholding the class probability. The thresholding value is set to 0.3 in the epxeriments.\n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014---\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\n[5] Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, and Tatsuya Harada. Open set domain adaptation by backpropagation. In European Conference on Computer Vision, pp. 153\u2013168, 2018. \n\n5. We use 10% of source data and all (100%) of target data. We validate the performance of CoBRF on the small labeled training data (source domain). We also train other baseline methods with the same setting in the experiments for the fair comparison.", "title": "The answer to Reviewer #3"}, "r1gYn-4diB": {"type": "rebuttal", "replyto": "ryg6fy_ptB", "comment": "Thank you for helpful comments on our paper.\n\n1. Taking advantage of the ImageNet dataset\nAll baseline methods in this paper use pretrained models from the ImageNet dataset.\nFor example, OSVM, ATI-\u03bb + OSVM and Saito et al. in Table 6 utilize AlexNet pretrained by the ImageNet dataset. JAN, ATI-semi, and CDA in Table 7 also select ResNet-50 that were pretrained with the ImageNet dataset as their backbone networks.\n\nWe also use AlexNet for Table 6 and ResNet-50 for the other tables where two backbone networks are pretrained by ImgaeNet.\n\nThus both the proposed CoBRF and state-of-the-art baseline works take advantage of the ImageNet dataset to train domain adaptation methods.\n\n\n2. End-to-end learning of neural networks\nPlease note that the main focus of CoBRF is to improve the last classification layer in the neural network assuming that the deap features are fixed.\n\nAlthough CoBRF does not update the neural network parameters, it outperforms the state-of-the-art works in various experiments. We argue that the pre-trained network provides generalized features for generic classification, which makes CoBRF work well for domain adaptation tasks. Also, most state-of-the-art works learn their models on the pretrained networks from ImageNet, which indicates that they also depend on the capabilities of pretraining on the large dataset.\n\n3. Training from scratch\nAlthough CoBRF is mainly for learning more robust and generic classifier, we can train a network with CoBRF from scratch by combining another domain adaptation method. We first train a deep neural network with another method from scratch, and then we can construct CoBRF on the feature set for the trained deep neural networks. In addition, we can train a deep neural network with triplet sampling from the split result of random forests proposed in [1]. Using this method, a deep neural network can be trained from scratch with random forests.\n[1] Under review. Submitted to International Conference on Learning Representations, 2020\n\nWe will revise our paper based on the answer in the final version if you give the opportunity to present the paper to the conference. \n", "title": "The answer to Reviewer #1"}, "S1ecq-Ndir": {"type": "rebuttal", "replyto": "S1l_sCFXcS", "comment": "Thank you for helpful comments on our paper.\n\n1. We will revise the paper to improve readability. We will do our best to refine the rough expressions of the paper. We are working on improving the paper, and we will make it better for the final version. It would be greatly appreciated if more detailed comments could be provided.\n\n2. We added hyperparameter settings such as the number of trees, maximum depth, feature dimensionality of the SVM training, and the number of repeats in training a random forest to page 6. We also supplemented the ablation study with regard to the number of trees and maximum depths in Table 3. In the ablation study, the maximum depth is 8 with 100 decision trees to consider both accuracy and complexity. \n\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014--Revised paper \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nTo train the CoBRF, we use 100 trees with a maximum depth of 8.\nWhen there is no training data fallen on a node, we prune the tree at that node.\nThe number of randomly selected feature dimension for the SVM training is set to 250.\nThe input feature of SVM is normalized for the stable learning of the hyperplane.\nWe repeat the SVM training 15 times to select the optimal split in each node.\n\n   Depth | The number of trees (T)\n               |  5 | 10 | 50 | 100 |  \n      6   | 62.2 | 65.4 | 67.5 | 67.7 |\n      7   | 60.2 | 63.5 | 67.1 | 67.8 |\n      8   | 58.9 | 63.3 | 67.5 | 68.0 |\n      9   | 55.6 | 60.0 | 66.2 | 67.6 |\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\u2014-\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014-\n\nPlease refer to page 6, 7, and Table 3 for more information on the hyperparameters and ablation study.   \n", "title": "The answer to Reviewer #2"}, "H1gbR6xTtB": {"type": "review", "replyto": "SkeJPertPS", "review": "Summary:\nThis paper introduces a method for domain adaptation, where each domain has noisy examples. Their method is based on a decision tree in which the data at each node are split into equal sizes while maximizing the\ninformation gain.  They also proposed a way to reduce domain alignment. Their method is tested on several noisy domain adaptation settings and performs better than other baseline methods. \n\nPros:\nTheir idea to utilize a decision tree for domain adaptation sounds novel. \nExperiments indicate the effectiveness of their method.\n\nCons:\nThis paper is not well-written and has many unclear parts. \n1, The presentation of the problem set is unclear throughout this paper. In the abstract, they mentioned that they tackle the situation where both source and target domains contain noisy examples. However, they did not define the exact problem setting in any section. I could not understand what kind of problem setting motivated their method, which makes it hard to understand their method. \n2, How they actually optimized the model is also unclear. From Eq 1~4, it is hard to grasp how they trained the model. \n3, In open-set domain adaptation, simply minimizing domain-distance can harm the performance. How does the method avoid this issue? It was also unclear. \n4, Experimental setting seems to be wrong and unclear. In Openset1, they say that \"The labels from 1 to 10 of both source and target domains are marked as the known class, and all data with label 11\u223c20 in the source domain and label 21\u223c31 in the\ntarget domain are used as one unknown class\". However, Saito et al. (2018) used 21-31 classes in the target domain as one unknown class. In addition, \"According to Saito et al. (2018) the target data of the unknown class is not used in training, \", they used the 21-31 classes for training in an unsupervised way. How is this method used to detect unknown class? Is there any threshold value set for it?\n5, The experimental setting is unclear. In 4.4, \", we use only 10% of training samples\", does it mean 10 % training source examples or target examples? This setting is also unclear. \n\nFrom the cons written above, this paper has too many unclear parts in the experiments and method section. I cannot say the result is reproducible given the content of the paper and the result is a reliable one. They need to present more carefully designed experiments. \n", "title": "Official Blind Review #3", "rating": "3: Weak Reject", "confidence": 4}, "ryg6fy_ptB": {"type": "review", "replyto": "SkeJPertPS", "review": "This paper proposes a new target objects for training random forests that has better generalizability across domains. The authors demonstrated that the proposed method outperforms existing adversarial learning based domain adaptation methods.\n\n\nStrength\n\nThe paper is clearly-written. The two objectives(balanced split and common split distribution between source and target domain) are well motivated and explained in the paper.\n\nThe authors show that empirically the proposed method outperform several existing adversarial learning based domain adaptation methods.\n\n\nWeakness\n\nOne of the main draw back of the method is that it relies on the features extracted from existing pre-trained neural networks, and cannot be used to update the representation of the neural networks. While the adversarial learning based method could do end to end training.\n\nIt would be great if the authors could clarify the setup of the baseline methods(e.g. Whether the baseline methods also take benefit of imagenet dataset, and is trained end to end).\n\nWhat will happen if you do not have the imagenet models and have to train all the models from scratch?\n\nOverall I think it is a borderline paper that might be interesting to some audiences in the conference.\n", "title": "Official Blind Review #1", "rating": "3: Weak Reject", "confidence": 3}, "S1l_sCFXcS": {"type": "review", "replyto": "SkeJPertPS", "review": "This paper proposes an approach to building random forests that are\nbalanced in such a way as to facilitate domain adaptation. The authors\npropose to split nodes not only based on the Information Gain, but\nalso so that the sizes of each set passed to left and right children\nare equal. Another extension to the standard random forest training\nprocedure is the use of a collaborative term subtracted from the\ninformation gain over the source domain. This term encourages\nalignment of the source and target domains in the leaves of trees in\nthe forest. Experimental results are given on a range of standard\nand open-set domain adaptation datasets.\n\nThe paper has a number of issues:\n\n1. There are some problems with clarity, and the English is somewhat rough\n   throughout. These problems are not terribly distracting, but the\n   manuscript could use more polish.\n2. I don't see a detailed discussion anywhere about the\n   hyperparameters used for fitting the random forests. How many trees\n   are used? What is the max depth? These parameters should be\n   discussed and included in the ablations in order to appreciate the\n   complexity/performance tradeoffs.\n\nThis paper has some interesting ideas in it, and the experimental\nresults are excellent. I would encourage the authors to move salient\nmaterial from the supplementary material to the main article and to\nprovide a more thorough discussion of the complexity of the models\n(the structural parameters of the trees/forests).", "title": "Official Blind Review #2", "rating": "6: Weak Accept", "confidence": 2}}}