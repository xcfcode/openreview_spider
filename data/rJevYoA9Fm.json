{"paper": {"title": "The Singular Values of Convolutional Layers", "authors": ["Hanie Sedghi", "Vineet Gupta", "Philip M. Long"], "authorids": ["hsedghi@google.com", "vineet@google.com", "plong@google.com"], "summary": "We characterize the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer, enabling their efficient computation. ", "abstract": "We characterize the singular values of the linear transformation associated with a standard 2D multi-channel convolutional layer, enabling their efficient computation.  This characterization also leads to an algorithm for projecting a convolutional layer onto an operator-norm ball. We show that this is an effective regularizer;  for example, it improves the test error of a deep residual network using batch normalization on CIFAR-10 from 6.2% to 5.3%. ", "keywords": ["singular values", "operator norm", "convolutional layers", "regularization"]}, "meta": {"decision": "Accept (Poster)", "comment": "This paper proposes an efficient method to compute the singular values of the linear map represented by a convolutional layer. It makes uses of the special block-matrix form of convolutional layers to construct their more efficient method. Furthermore, it shows that this method can be used to devise new regularization schemes for DNNs. The reviewers did note that the diversity of the experiments could be improved, and R2 raised concerns that the wrong singular values were being computed. The authors should add a section clarifying why the singular values of a convolutional linear map are not found directly by performing SVD on the reshaped kernel - indeed the number of singular values would be wrong. A contrast with the singular values obtained by simple reshaping of the kernel would also be helpful."}, "review": {"r1l8srO-eV": {"type": "rebuttal", "replyto": "Syx8I7LcyE", "comment": "Thank you for your interest. \nIf the wrap-around (hence circulant form) is replaced by zero-padding, the operator matrix becomes a Toeplitz matrix. The analysis of the error for 2D case is investigated in the paper: \u201c On the Asymptotic Equivalence of Circulant and Toeplitz Matrices\u201d,  Zhihui Zhu, and Michael B. Wakin, IEEE Transactions on Information Theory, VOL. 63, NO. 5, MAY 2017.\nHere, similar to the 1D case, O(1/n) bound is obtained on the error.\nFor practical purposes such as regularizing the operator norm, this small error does not influence the result, while the approximation enables us to find all singular values efficiently.\n", "title": "Circular convolution vs zero-padding (2D) "}, "HJe-8Pr93m": {"type": "review", "replyto": "rJevYoA9Fm", "review": "The paper is dedicated to computation of singular values of convolutional layers. While singular values of convolutional layers represent sufficient interest for researchers, huge computational complexity made it difficult to investigate their properties in the case of layers of deep neural networks. Using the fact that operator matrix of the convolutional layer has a special form (i.e. can be represented as block-matrix, which blocks are doubly block circulant matrices) the authors proposed a more efficient method of computation of singular values. I really enjoyed reading this paper and I think that it opens a lot of interesting applications. As one of the possible applications the authors proposed a regularization method based on bounding of singular values.\n\nThe paper from my point of view has two main drawbacks:\n\n1.  Diversity of experiments. While the paper has strong theoretical component, the part dedicated to experiments is not broad enough. It would be interesting to see regularization on other architectures and other datasets.\n\n2.The system of references. I would recommend to add not only references to the sources, but also to the theorem numbers or the chapters. For example, I would recommend to replace \u2018Poposition 9 ((Lefkimmiatis et al., 2013))\u2019 with \u2018Poposition 9 ((Lefkimmiatis et al., 2013, Proposition 1))\u2019. In pure math papers, it is a standard rule to add such additional information since many papers contain a lot of theorems and it significantly simplifies reading and understanding the paper.\n\nDespite these disadvantages this is a great work with huge potential.\n", "title": "This is an interesting work with huge potential.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "BkeXWRlM0Q": {"type": "rebuttal", "replyto": "Skev-7xcTm", "comment": "Thank you for your comment. \n\nThe largest factor by which a convolutional layer blows up a signal is equal to the largest singular value of the function computed by the layer, which is the largest singular value of A.  We have shown that its singular values, including its largest, are different from those of any reshaping of K.  \n\nHowever based on your suggestion we conducted a few experiments on the CIFAR10 dataset, with somewhat surprising results: \n(1) We clipped the singular values of the reshaped K every 100 steps. We tried various constants for the clipped value, and found that the best accuracy we achieved was the same as the accuracy we achieved by clipping A.  \n(2) We clipped the singular values of K to these same values every step, and found that the best accuracy achieved was slightly worse than the accuracy achieved in step 1. So clipping after every step is not very useful. We observed similar behavior when we clipped A after every step. \n(3) Most surprisingly, we found that clipping A on a GPU was about 25% faster than clipping K --- on the same machine, 10000 batches of CIFAR10 took 14490 s when we clipped K, whereas they took 11004 s when we clipped A! One possible explanation is parallelization --- clipping K takes O(m^3 k^2) flops, whereas clipping A does m^2 FFT\u2019s, followed by n^2 m x m SVDs, which takes O(m^3 n^2) flops, but these can be parallelized and completed in as little as O(n^2 log n + m^3) time.\n\nClearly this is only one dataset, and the results may not generalize to other sets. However it does suggest that finding the full spectrum of the convolutional layer may be no worse than computing heuristic approximations, both in classification accuracy and speed.\n", "title": "experimental results"}, "rkeiYma-0X": {"type": "rebuttal", "replyto": "S1liyZ6-C7", "comment": "Thank you for your interest in the proof.\n\nIt seems like the issue might have been that openreview added the period to the URL.  We edited the earlier comment to remove the period.  Clicking on the link should work now (it seems to us to work in incognito mode).", "title": "should work now"}, "SylOsBC3pX": {"type": "rebuttal", "replyto": "Byldg4f9pm", "comment": "We have uploaded a proof that, in the case of a single input channel and a single output channel, for random 2 x 2 filter applied to an n x n signal, with probability 1, the number of singular values in the resulting linear transformation is at least n^2/2.  This proof may be found in the document titled pairs2d.pdf in https://www.dropbox.com/sh/l8adgttixljdpz5/AADV_n6uxBFSX2q_0B7J2sIza\n\nRunning the following code after importing numpy as np and defining SingularValues as in page two of our paper may also convince you that there are almost always at least n^2/2 singular values.\n\nfor _ in range(20)\n   \tkernel = np.random.randn(2,2,1,1)\n         input_shape = [4,4]\n         print np.sort(SingularValues(kernel, input_shape).flatten())\n", "title": "uploaded proof of n^2/2 lower bound"}, "rJlab0Efam": {"type": "rebuttal", "replyto": "ryxHyswZpm", "comment": "We thank the reviewer for the effort and time invested in this review. However it is clear that still quite a few misunderstandings of our work remain.  A convolutional layer applied to an n x n feature map with m input channels and m output channels is a function with n^2 m inputs and n^2 m outputs.  This function is linear.  The standard encoding of a linear function as a matrix has n^2 m rows and n^2 m columns.  The singular values of a linear function are defined to be the singular values of this matrix.  Computing these singular values is the subject of this paper.  The matrix and its singular values are a property of the function computed by the layer, no matter how it is implemented. Due to the special nature of the linear transform induced by the convolution layer, the matrix does indeed have lots of redundancy; we exploit this in computing the singular values efficiently (see Equations (1) and (6)).  We have proved that our method provides the correct singular values, and further verified this with unit tests.  We have also proved, in our first response to your review, that no reshaping of the kernel tensor can possibly provide the correct singular values.  We are therefore not surprised that our method gives different answers than a reshaping of the kernel tensor.\n", "title": "not fully connected"}, "BkggfSfzTX": {"type": "rebuttal", "replyto": "HJe-8Pr93m", "comment": "Thank you for your review. We have updated the references in the revised version as you requested.", "title": "system of references updated"}, "Hkxjbe6VhQ": {"type": "review", "replyto": "rJevYoA9Fm", "review": "This paper studies the problem on computing the spectrum of singular values of linear convolutional layers. This is an important problem with abundant applications on regularizing deep neural networks. However, there are several technical issues need to be addressed in its current form. \n\nFirst, in the section \"Summary of Results\", at first read of the paper I found it very confusing why the time complexity of computing the spectrum is a function of n, where n is the size of the input feature map. Intuitively, since the size of the convolutional kernel is m x m x k x k, it is expected that the time complexity is expressed as a function of (m, k). Later I realized that this is due to the unnecessary and redundant 0 padding in section 2.1 that leads to this artifact. I understand that in order to apply the described Fourier transform technique it is necessary to introduce the large nxn filter, which is of the same size as the input, but it also introduces redundant computation. This fact further emerges in the introduction of matrix A in Eq (1). \n\nMore importantly, I think the authors didn't perform a detailed analysis on using the basic definition of convolutional filter to compute its spectrum, and this is the reason why they reached a misleading conclusion that simple SVD takes O(n^6 m^3) time. Specifically, each convolution operation corresponds to a inner product operation, so we can reshape the input 3D tensor with shape m x n x n into a 2D matrix, with shape n^2 x mk^2, denoted as X. Note that this creates a unnecessary redundancy in the input feature map, but it does not create redundant weight for the convolutional kernel. As a comparison, the introduced matrix A in the paper is heavily redundant. Similarly, for m channels, we can reshape the 4D convolutional kernel with shape m x m x k x k into a 2D matrix, with shape mk^2 x m, denoted as K. Then the usual convolution layer can be described as the following linear system: Y = X K, where Y with shape n^2 x m is the output, and can be easily reshaped into size m x n x n. Hence to compute the spectrum of the convolution layer corresponds to computing the singular values of the 2D matrix K with size mk^2 x m. Hence a naive application of SVD directly gives us the solution in time O(m^3 k^2) (Note that the time complexity of SVD for matrix with size a x b is O(min{a^2 b, a b^2})), which is much smaller than the one given in the paper O(m^3 n^2) since k << n. \n\nIn experiment the authors made unfair comparison between their proposed method and the full matrix method: the full matrix A is fully redundant, due to its circulant pattern. As this implies a highly redundant information, nobody will form and compute matrix A explicitly in practice. So the time improvements demonstrated in the experiment section are meaningless. A valid baseline would be to compare the proposed method with the one introduced above. But in this case I would imagine the proposed method to be worse due to its unnecessary 0 padding leading to the worst time complexity. ", "title": "Review", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "r1efQ83ep7": {"type": "rebuttal", "replyto": "Byeq47sJaQ", "comment": "We agree that the reviewer has correctly described the operational implementation of a convolutional layer --- a set of k x k patches is created from the input, and the reshaped convolution matrix is applied to each patch to get the output.  However, the subject of our paper is the computation of the singular values of the (linear) function computed by the convolutional layer as a whole, which determine its potential contribution to exploding and vanishing gradients.  This is a linear map applied to the n^2 m inputs that produces n^2 m outputs, and we compute the singular values of the n^2 m x n^2 m matrix corresponding to this linear map.\n", "title": "analyzing the convolutional layer as a whole"}, "Syxj6HcJ6Q": {"type": "rebuttal", "replyto": "rkgEmITAiX", "comment": "Thank you for your kind review.\n", "title": "thank you"}, "ryexDS9J6X": {"type": "rebuttal", "replyto": "SJeHanvRhm", "comment": "We compared with the work on Miyato, et al in our submission.  We have elaborated on this comparison elsewhere in this comment section, including our response to Reviewer 2.  In that response, we demonstrate that there is no reshaping of the kernel tensor whose singular values coincide with the singular values of its linear transform.\n\nIn the case of convolutional layers, the paper by Tsuzuku, et al estimates the largest singular value to within a constant factor (see Corollary 1), whereas we characterize the exact values of all of its singular values.  \n\nThe paper by Scaman and Virmaux uses a power method for individual layers. In the form they describe, it only gives the largest singular value.  We compared our work with the paper by Gouk, et al in our submission.  They also compute an approximation to the largest singular value.  Proposition 8 in our paper (from earlier work) shows that computing the projection of a matrix onto an operator norm ball may require clipping multiple singular values.  The method of Gouk, et al scales down all of the singular values by the amount needed to bring their estimate of the largest to its desired value - this is not closest matrix in the operator norm ball.  \n\nOur method is the first to feasibly provide access to all of the singular values of convolutional layers for models commonly used in practice.  It thereby enables study of the properties of trained models, such as whether the linear transformations computed by convolutional layers are essentially full rank.  It also opens the door to a variety of regularizers.", "title": "comparisons"}, "SJlKQW9JaQ": {"type": "rebuttal", "replyto": "Hkxjbe6VhQ", "comment": "It can be proved that if a kernel of size (k, k, m, m) is applied to an input of size (n, n, m), and the entries of the kernel are chosen from a continuous probability distribution like a Gaussian, then with probability 1, the number of distinct singular values in the linear transformation of its convolutional layer is at least (m n^2)/2.   Repeatedly applying the code on page two of our paper to random inputs has always produced at least this number of distinct singular values.  (We invite the reader to try this.)  Any reshaping of an (k, k, m, m) kernel can produce at most mk singular values, which is not enough to be correct.  The reshaping outlined in your review can have at most m singular values, which is even smaller.\n\nAs we noted in our answer to a question during the review period, an example in which the reshaping does not compute even the largest singular value correctly can be obtained by executing the following commands after importing numpy as np and defining SingularValues as in page two of our paper:\n \n   kernel = np.array(np.ones(4)).reshape(2,2,1,1)\n   SingularValues(kernel, [4,4])\n   reshaped = kernel.reshape(4,1)\n   np.linalg.svd(reshaped, compute_uv=False)\n\nThe correct largest singular value of the convolutional layer is 4. (An all-ones feature map is turned into an all-fours feature map by applying this filter.) The (only) singular value of the reshaping of the kernel, which is (1,1,1,1)^T, is 2.  This is not an isolated instance --- random inputs also produce counterexamples --- we ran our algorithm with m = 1, n = 16, and various values of k. In each case we found 130 unique singular values, with ranges described below. The reshaping method produced 1 singular value in each case:\nk                Range of Singular Values                     Singular Value from reshaping\n4                [0.388872696514, 7.6799308322]        3.76770352\n5                [0.421739253704, 10.7721306924]      5.04019121\n7                [0.165159699404, 14.9556902191]      6.7304902\n7                [0.532454839614, 16.38084538]          7.20513578\n3                [0.4655241798,  6.12218595024]         3.48952531\n6                [0.418950277234, 15.2821360286]      6.1289447\n\n", "title": "the singular values of the reshaped kernel cannot be correct"}, "rkgEmITAiX": {"type": "review", "replyto": "rJevYoA9Fm", "review": "In this paper, the authors derive exact formulas for computing singular values of convolutional layers of deep neural networks. By appealing to fast FFT transformations, they show that computing the singular values can be done much faster than computing the full SVD of the convolution matrix. This obviates the needs to approximate the singular values. They use these results to then devise regularization schemes for DNN layers, and show that employing this regularization helps with model performance. \n\nThey show that the algorithm with the operator norm regularization can be solved via an alternating projection scheme. They also postulate that since this might be expensive and unnecessary, one can also perform just 2 projections after every few SG iterations, and claim that this acts as a 'warm start' for subsequent iterations. Experiments reveal that this does not degrade the performance too much. \n\n\nThe paper is well written and easy to understand. The proofs follow from standard linear algebra methods, and are easy to follow. ", "title": "Review", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rJlhx6b5hm": {"type": "rebuttal", "replyto": "ryl56Hlt3Q", "comment": "If we have a 4D tensor of dimensions (a, b, i, o) where i, o are the numbers of input and output channels respectively, and (m, n, i) is the size of the input, then the corresponding linear transformation is a matrix of size imn x mno. Thus we would expect min(mni, mno) singular values.  Except in degenerate cases, this linear transformation has full rank.\n\nIn our code, the FFT produces a tensor of dimensions (m, n, i, o) and the second line performs an SVD for the i x o matrix for each value of the first two dimensions. Thus it produces min(i, o) singular values for each on the mn matrices, thus the total number of singular values we produce is mn * min(i, o). So our method produces the correct number of singular values.  \n(In fact, our unit tests verify that our method computes the correct singular values, as proved in the paper.)\n\nSingular values are repeated due to the special structure of the linear transformation. This is true even for a 1-D convolution with only one input-output channel. For instance, consider the filter (a, b, c) applied to a 1-d input of 3 pixels. The matrix encoding its linear transformation is ((a b c), (c a b), (b c a)). Its eigenvalues are a+b+c, a + b\u03c9 + c\u03c9^2, a + b\u03c9^2 + c\u03c9, where \u03c9 = (-1 + sqrt(-3))/2. The singular values are the magnitudes of the eigenvalues, but since |a + b\u03c9 + c\u03c9^2| =  |a + b\u03c9^2 + c\u03c9| we get repeated singular values.  However, for random inputs, we have seen that the repeated singular values tend to come in pairs, so that there are Omega(mn * min(i, o)) distinct singular values.\n\nIt is clear that no reshaping of the 4D tensor of dimensions (a, b, i, o) to a 2D matrix can produce Omega(mn * min(i, o))  singular values.  An example in which the reshaping described in the ICLR\u201918 paper by Miyato, et al does not compute even the largest singular value correctly can be obtained by executing the following commands after importing numpy as np and defining SingularValues as in page two of our paper:\n \n   kernel = np.array(np.ones(4)).reshape(2,2,1,1)\n   SingularValues(kernel, [4,4])\n   reshaped = kernel.reshape(4,1)\n   np.linalg.svd(reshaped, compute_uv=False)\n\nThe correct largest singular value of the convolutional layer is 4. (An all-ones feature map is turned into an all-fours feature map by applying this filter.) The (only) singular value of the reshaping of the kernel, which is (1,1,1,1)^T, is 2.  This is not an isolated instance --- random inputs also produce counterexamples except in very rare cases.\n", "title": "repeated singular values, and differences with reshaped filter tensors"}, "HJeJYtkB3Q": {"type": "rebuttal", "replyto": "HJlsT1A42Q", "comment": "The first plot that you requested showed that different convolutional layers had significantly different numbers of non-negligible singular values.  We were curious to what extent this was due to the fact that different layers simply were of different sizes, so that the total number of their singular values, tiny or not, was different.  To look into this, instead of plotting the singular value ratios as a function of the rank of the singular values, as in the first plot, we normalized the values on the horizontal axis by dividing by the total number of singular values.  The resulting plot is available at https://www.dropbox.com/s/m0xun9jdc7kd5ry/resnet_svd_percentile_vs_ratio.png?dl=0.  \n", "title": "another plot"}, "HJlsT1A42Q": {"type": "rebuttal", "replyto": "rkep8CZXhm", "comment": "Thanks very much for your compliment and your careful reading of our paper.  Here are the answers to your questions.\n\n1) The singular values are the components of the tensor output by the code on page 2; flattening it produces them in a list.\n\n2) In Figure 4, while singular values in the first layer are much larger than the rest, many layers have a lot of singular values that are pretty big.  For example, most of the layers have at least 10000 singular values that are at least 1.  As you requested, we have created a plot of the ratios of the singular values in each layer with the largest singular value in that layer.  It can be viewed at https://www.dropbox.com/s/15ujg0qdr9didr8/resnet_svd_ratios.png?dl=0.  The effective rank of the convolutional layers is larger closer to the inputs.\n\n3) The x-axis in Figure 5 is wall-clock training time using a GPU.  The y-axis is the test error measured while training.  Applying the clipping reduces the test error at convergence from 6.2% to 5.3%, a significant amount.  \n", "title": "responses to your questions"}}}