{"paper": {"title": "Lie-Access Neural Turing Machines", "authors": ["Greg Yang", "Alexander Rush"], "authorids": ["gyang@college.harvard.edu", "srush@seas.harvard.edu"], "summary": "We generalize Turing machines to the continuous setting using Lie group actions on manifolds.", "abstract": "\n  External neural memory structures have recently become a popular tool for\n  algorithmic deep learning\n  (Graves et al. 2014; Weston et al. 2014).  These models\n  generally utilize differentiable versions of traditional discrete\n  memory-access structures (random access, stacks, tapes) to provide\n  the storage necessary for computational tasks.  In\n  this work, we argue that these neural memory systems lack specific\n  structure important for relative indexing, and propose an\n  alternative model, Lie-access memory, that is explicitly designed\n  for the neural setting.  In this paradigm, memory is accessed using\n  a continuous head in a key-space manifold. The head is moved via Lie\n  group actions, such as shifts or rotations, generated by a\n  controller, and memory access is performed by linear smoothing in\n  key space. We argue that Lie groups provide a natural generalization\n  of discrete memory structures, such as Turing machines, as they\n  provide inverse and identity operators while maintaining\n  differentiability. To experiment with this approach, we implement\n  a simplified Lie-access neural Turing machine (LANTM) with\n  different Lie groups.  We find that this approach is able to perform\n  well on a range of algorithmic tasks.", "keywords": ["Natural language processing", "Deep learning", "Supervised Learning"]}, "meta": {"decision": "Accept (Poster)", "comment": "The paper presents a Lie-(group) access neural turing machine (LANTM) architecture, and demonstrates it's utility on several problems. \n \n Pros:\n Reviewers agree that this is an interesting and clearly-presented idea.\n Overall, the paper is clearly written and presents original ideas.\n It is likely to inspire further work into more effective generalizations of NTMs.\n \n Cons:\n The true impact and capabilities of these architectures are not yet clear, although it is argued that the same can be said for NTMs. \n \n The paper has been revised to address some NTM features (sharpening) that were not included in the original version.\n The purpose and precise definition of the invNorm have also been fixed."}, "review": {"BJCiEcI8l": {"type": "rebuttal", "replyto": "Byiy-Pqlx", "comment": "We have finished results adding sharpening to the RAM/tape model. Weight sharpening only confers small advantage over vanilla on the copy, bigram flip, and double tasks, but deteriorates performance on all other tasks. These results will appear in the final version of the paper.", "title": "Sharpening results."}, "SJR4S5r8e": {"type": "rebuttal", "replyto": "ByGoHOQrl", "comment": "We have finished results adding sharpening to the RAM/tape model. Weight sharpening only confers small advantage over vanilla on the copy, bigram flip, and double tasks, but deteriorates performance on all other tasks. These results will appear in the final version of the paper.\nSince there is neither an official nor unofficial implementation replicating LSTM-controlled DNC, and implementing DNC from scratch requires more time than is available, we won\u2019t be able to compare DNC directly with LANTM. We hope to add this in a future version.", "title": "Sharpening"}, "HkKN7Q9Nl": {"type": "rebuttal", "replyto": "ryok7XQVx", "comment": "Thank you for taking the time to read and review our paper.\n\n> need to manually specify the Lie group to use (it would be better if network could learn the best way of accessing memory)\n\nWe see the choice of Lie Group as analogous to the choice of memory layout in discrete models. In a perfect world that could be learned, but we think something like an R^n key space is a fine general-layout. \n\n> not clear if this really works better than standard NTM (compared only to simplified version)\n\nWe will implement some of the tricks in a standard NTM such as sharpening. However, we see it as a benefit that these are not necessary in Lie-access. \n\n> not clear if this is useful in practice (no comparison on real tasks)\n\nWe realize that the tasks used are mainly synthetic, but this is true for most work studying NTMs. Our tasks capture essential aspects of algorithmic learning, like for loops (repeat copy), sorting (priority sort), and arithmetic. They are nontrivial, as demonstrated by the gap between baseline performances and LANTM performances.\n", "title": "reply"}, "rynvz75Vg": {"type": "rebuttal", "replyto": "rkhhu9bEg", "comment": "Thanks for reviewing our paper.\n\n> In the results, the LANTM only seems to be slightly better than the normal NTM.\n\nWe disagree with this interpretation of the results. In most tasks the best LANTM gets almost 100% coarse on generalization, but the corresponding simplified NTM never gets more than 50% coarse on generalization.\n\n> The result tables are a bit confusing.\n\nCan you clarify this comment? We closely followed past work in this area when presenting the results.\n\n> No source code available.\n\nWe plan on open sourcing the code and will include a link in the next version.\n\n> The difference to the properties of normal NTM doesn't become too clear. Esp it is said that LANTM are better than NTM because they are differentiable end-to-end and provide a robust relative indexing scheme but NTM are also differentiable end-to-end and also provide a robust indexing scheme.\n\nLet us reiterate the key motivation of the paper: the set of NTM\u2019s head movement does *not* compose a group.\n\nIn particular it is not true that every head movement has an inverse movement. This is briefly explained in footnote 1. In short, NTM\u2019s head movement is a Markov kernel, which mixes the probabilities encoded in the NTM\u2019s head. Intuitively, a mixture cannot be \u201cunmixed\u201d by mixing further; formally, it can be shown that a Markov kernel does not have an inverse that is a Markov kernel if it is not the delta kernel (all values concentrated at one point). This problem manifests as the loss of concentration in NTM\u2019s head after shifting repeatedly. This was fixed in an ad hoc way in the NTM paper via \u201csharpening coefficients\u201d that boosts concentration, but they have to be learned.\n\n> It is said that the head is discrete in NTM but actually it is in space R^n, i.e. it is already continuous. It doesn't become clear what is meant here.\n\nWe agree that NTM is continuous. Our point is that it does not form a group. \n\n> No tests on real-world tasks, only some toy tasks.\n\nWe agree that the tasks used are mainly synthetic, but this is true on most work studying NTMs. Our tasks capture essential aspects of algorithmic learning, like for loops (repeat copy), sorting (priority sort), and arithmetic. They are nontrivial, as demonstrated by the gap between baseline performances and LANTM performances.\n\n> No comparisons to some of the other NTM extensions such as D-NTM or Sparse Access Memory (SAM) (https://arxiv.org/abs/1610.09027). Although the motivations of other NTM extensions might be different, such comparisons still would have been interesting.\n\nThese are very interesting papers but they target aspects of the NTM that are orthogonal to the topology of addressing schemes.  \nFor example, the contributions of the D-NTM paper were mainly Dynamic Least Recently Used (LRU) addressing, regularization methods, and hard addressing using REINFORCE. In our paper under review here, we argue for a principled way to purpose different manifolds as memory storage and different Lie groups as relative head movement. In contrast, D-NTM\u2019s memory is still modeled after a traditional memory array and it does not have relative head movement. A priori, there is no reason that ideas from both papers cannot be combined in a coherent way. \n", "title": "Reply"}, "SJv5mhtVg": {"type": "rebuttal", "replyto": "HywzhQGNg", "comment": "Thank you for reading and reviewing our paper.\n\n> The primary limitation of this paper is its limited impact. [...I]t is not clear that this significantly contributes to building models capable of more general program learning. Instead, it is likely to make an already complex and slow model such as the NTM even slower.\n\nWhat leads you to think that Lie-Access is complex or slower? We feel that this model is simpler than NTM/DNC and contributes to program learning by providing a more natural interpretation of memory than soft versions of discrete memory. Our model requires only simple lookups in R^2 and could certainly be optimized to run fast. For example, all the advances made in the recent paper \"Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes\" by Rae et al. could be applied to Lie-Access Memory.\n\n> In general, it would seem memory topology is problem specific and should therefore be learned rather than specified.\n\nWe feel this criticism should apply as well to NTM and DNC. They specify a rigid memory system modelled after discrete memory arrays. By trying different memories topologies we were simply experimenting with types of access and not trying to advocate problem specific topology. Likely an R^n key space would be sufficient for most problems. \n\n> The baseline used for comparison is a very simple model, which does not even having the sharpening (the NTM approach to solving the problem of head distributions becoming \u2018smeared\u2019). There is also no comparison with the successor to the NTM, the DNC, which provides a more general approach to linking memories based on prior memory accesses.\n\nWe will add sharpening to the simplified NTM in our experiments. We had hoped to compare to DNC, but were not able to reimplement it. There are now reimplementations though that we will compare to. \n", "title": "Reply"}, "S1kebhYNx": {"type": "rebuttal", "replyto": "rJsnsceVg", "comment": "Thank you for reading and reviewing our paper.\n\n> I do not appreciate that the growing memory is not mentioned as a drawback. It should be stressed and a discussion on the impact it has on efficiency/scalability is needed.\n\nThanks for the comment. We agree this is an issue with key/value type approaches. We will discuss this more explicitly and mention possible future corrections.\n", "title": "Reply"}, "B1UB8GQQl": {"type": "rebuttal", "replyto": "HJfXZJCzg", "comment": "You are right we did not really explain this. We\u2019ll expand on this in the next version of the paper.\n\n\nThese two methods can exactly be thought of different kernels for computing the expectation over memories via kernel regression. Softmax is a gaussian kernel and invnorm is an inverse distance kernel.\n\n\nBehaviorally, in invnorm, the weighting vector becomes concentrated at a single memory i only when the read key k is very close to k_i, the address of memory i. On the other hand, in softmax, the weighting vector becomes concentrated at memory i as long as k is relatively closer to k_i than to other addresses k_j, j != i. Thus invnorm encourages the controller to output exact addresses, while softmax is OK with controller emitting loosely approximate addresses. The former has the advantage that, for example in R^2 with the shift group, when trying to learn relative head movement to go from k_i to k_{i+1}, the controller will learn k_{i+1} - k_i. With softmax, the controller may learn something wildly different; for example, it perhaps learns to retrieve memory i with \\tilde k_i := k_i + (k_{i+1} - k_i)/3 and to retrieve memory i+1 with \\tilde k_{i+1} = k_{i+1} - (k_{i+1} - k_i)/3, and therefore it learns the head shift \\tilde k_{i+1} - \\tilde k_i = (k_{i+1} - k_i)/3. If the memories are laid out on a straight line, with the same shift vector between each consecutive memories, then invnorm seems likely to learn the correct head movement, but softmax might not.", "title": "Motivation for invnorm"}, "rJGm3FMQe": {"type": "rebuttal", "replyto": "rJSXUzJ7g", "comment": ">Question: In the introduction in the first paragraph in the last sentence, you mean \"these works\" instead of \"this work\"?\n\n\nWe were referring to the collection of works mentioned in the previous sentence, and not our paper. Sorry for the confusion; this will be fixed in the next version of the paper.\n\n\n>Can you make the difference to NTM more clear in the introduction?\nEsp, NTM are already differentiable for end-to-end training and have a robust relative indexing scheme. Why is a Lie-Access NTM different in that regard?\n\n\nAs mentioned in the last sentence of that paragraph in the paper, the set of NTM\u2019s head movement does not compose a group. In particular it is not true that every head movement has an inverse movement. This is briefly explained in footnote 1. In short, NTM\u2019s head movement is a Markov kernel, which mixes the probabilities encoded in the NTM\u2019s head. Intuitively, a mixture cannot be \u201cunmixed\u201d by mixing further; formally, it can be shown that a Markov kernel does not have an inverse that is a Markov kernel if it is not the delta kernel (all values concentrated at one point). This problem manifests as the loss of concentration in NTM\u2019s head after shifting repeatedly. This was fixed in an ad hoc way in the NTM paper via \u201csharpening coefficients\u201d that boosts concentration, but they have to be learned.\n\n\n>In Tape-Based Memory, what is the probability simplex \\Delta^2?\n\n\nIt is the set {(x, y, z) in R^3: x, y, z >= 0, x + y + z = 1}.\n\n\n>How does it compare to Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes?\n\n\nThe D-NTM paper\u2019s results are in some sense orthogonal to ours. Its contributions were mainly Dynamic Least Recently Used (LRU) addressing, regularization methods, and hard addressing using REINFORCE. In our paper under review here, we argue for a principled way to purpose different manifolds as memory storage and different Lie groups as relative head movement. In contrast, D-NTM\u2019s memory is still modelled after a traditional memory array and it does not have relative head movement. A priori, there is no reason that ideas from both papers cannot be combined in a coherent way.\n\n\n>In chapter 3, the general space \\mathcal{K} is continuous?\n\n\nWhen we were talking about the general criteria for head movement in the first and second paragraphs, \\mathcal{K} may be any space satisfying those criteria, which may or may not be \"continuous.\"\nFor example, the classic Turing machine has \\mathcal{K} = Z.\n\n\nIn the last paragraph, when we talked about the differentiability criteria, we argued for \\mathcal{K} to be a smooth manifold (to be more technically accurate, a connected smooth manifold with dimension > 0), and thus \"continuous\".\n\n\n>What metric $d$ do you use and which have you tried?\n\n\nIn the experiments, $d$ is Euclidean distance (L_2 distance) for both the memory based in R^2 and memory based on the sphere S^2: d(x, y) = \\sqrt{\\sum_i (x_i - y_i)^2}.\nWe didn't try other distance functions for R^2, because Euclidean distance seems like the most appropriate choice.\nFor the sphere, we tried angle distance: d(x, y) = arccos(<x, y>).\nThis performs very similar to Euclidean distance because the two functions are close to proportional when x and y are close together.\nAnother choice would be cosine similarity (dot product): d(x, y) = 1 - <x, y>.\nBut as explained in footnote 5, this is the same as the Euclidean distance.\n\n\n>When you introduce InvNorm, what do you mean with \"limit value\"?\n\n\nIt is really a minor technical point.\nIt means that we define the InvNorm function when q = k_i, which a priori causes both the enumerator and denominator to be infinite, to be the limit value of the function when q approaches k_i.\nIn particular, if all k_i are distinct, then when q = k_i, InvNorm takes the value (0, ..., 0, 1, 0, ..., 0), where the 1 appears in position i, corresponding to k_i.\n\n\n>In section 4.4, what do you mean by \"L_2-normalization\"?\n\n\nL_2-normalization is the map x |--> x/\\|x\\|, i.e. divides the coordinates of a point x by the 2-norm \\|x\\|.\n\n\n>In Models and Baselines, you have \"described the background\", which is probably missing a word.\n>The first model (a), the LSTM encoder-decoder, does it have attention?\n\n\nThe LSTM encoder-decoder baseline does not have attention.\nThe RAM baseline would be roughly the same as LSTM with attention.\n\n\n", "title": "Answers to your questions"}, "rJSXUzJ7g": {"type": "review", "replyto": "Byiy-Pqlx", "review": "In the introduction in the first paragraph in the last sentence, you mean \"these works\" instead of \"this work\"?\n\nCan you make the difference to NTM more clear in the introduction?\nEsp, NTM are already differentiable for end-to-end training and have a robust relative indexing scheme. Why is a Lie-Access NTM different in that regard?\n\nIn Tape-Based Memory, what is the probability simplex \\Delta^2?\n\nHow does it compare to Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes?\n\nIn chapter 3, the general space \\mathcal{K} is continuous?\n\nWhat metric $d$ do you use and which have you tried?\n\nWhen you introduce InvNorm, what do you mean with \"limit value\"?\n\nIn section 4.4, what do you mean by \"L_2-normalization\"?\n\nIn Models and Baselines, you have \"described the background\", which is probably missing a word.\nThe first model (a), the LSTM encoder-decoder, does it have attention?\nThe paper proposes a new memory access scheme based on Lie group actions for NTMs.\n\nPros:\n* Well written\n* Novel addressing scheme as an extension to NTM.\n* Seems to work slightly better than normal NTMs.\n* Some interesting theory about the novel addressing scheme based on Lie groups.\n\nCons:\n* In the results, the LANTM only seems to be slightly better than the normal NTM.\n* The result tables are a bit confusing.\n* No source code available.\n* The difference to the properties of normal NTM doesn't become too clear. Esp it is said that LANTM are better than NTM because they are differentiable end-to-end and provide a robust relative indexing scheme but NTM are also differentiable end-to-end and also provide a robust indexing scheme.\n* It is said that the head is discrete in NTM but actually it is in space R^n, i.e. it is already continuous. It doesn't become clear what is meant here.\n* No tests on real-world tasks, only some toy tasks.\n* No comparisons to some of the other NTM extensions such as D-NTM or Sparse Access Memory (SAM) (https://arxiv.org/abs/1610.09027). Although the motivations of other NTM extensions might be different, such comparisons still would have been interesting.\n", "title": "difference to NTM or DNTM, details", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rkhhu9bEg": {"type": "review", "replyto": "Byiy-Pqlx", "review": "In the introduction in the first paragraph in the last sentence, you mean \"these works\" instead of \"this work\"?\n\nCan you make the difference to NTM more clear in the introduction?\nEsp, NTM are already differentiable for end-to-end training and have a robust relative indexing scheme. Why is a Lie-Access NTM different in that regard?\n\nIn Tape-Based Memory, what is the probability simplex \\Delta^2?\n\nHow does it compare to Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes?\n\nIn chapter 3, the general space \\mathcal{K} is continuous?\n\nWhat metric $d$ do you use and which have you tried?\n\nWhen you introduce InvNorm, what do you mean with \"limit value\"?\n\nIn section 4.4, what do you mean by \"L_2-normalization\"?\n\nIn Models and Baselines, you have \"described the background\", which is probably missing a word.\nThe first model (a), the LSTM encoder-decoder, does it have attention?\nThe paper proposes a new memory access scheme based on Lie group actions for NTMs.\n\nPros:\n* Well written\n* Novel addressing scheme as an extension to NTM.\n* Seems to work slightly better than normal NTMs.\n* Some interesting theory about the novel addressing scheme based on Lie groups.\n\nCons:\n* In the results, the LANTM only seems to be slightly better than the normal NTM.\n* The result tables are a bit confusing.\n* No source code available.\n* The difference to the properties of normal NTM doesn't become too clear. Esp it is said that LANTM are better than NTM because they are differentiable end-to-end and provide a robust relative indexing scheme but NTM are also differentiable end-to-end and also provide a robust indexing scheme.\n* It is said that the head is discrete in NTM but actually it is in space R^n, i.e. it is already continuous. It doesn't become clear what is meant here.\n* No tests on real-world tasks, only some toy tasks.\n* No comparisons to some of the other NTM extensions such as D-NTM or Sparse Access Memory (SAM) (https://arxiv.org/abs/1610.09027). Although the motivations of other NTM extensions might be different, such comparisons still would have been interesting.\n", "title": "difference to NTM or DNTM, details", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HJfXZJCzg": {"type": "review", "replyto": "Byiy-Pqlx", "review": "You introduce InvNorm as an alternative to the softmax. Maybe I missed it but I didn't really understand the motivation behind this.The Neural Turing Machine and related \u201cexternal memory models\u201d have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures. In particular, the NTM, DNC and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position.\n\nThe NTM, which is the most relevant to this work, uses a differentiable version of a Turing machine tape. The controller outputs a kernel which \u201csoftly\u201d shifts the head, allowing the machine to read and write sequences. Since this soft shift typically \u201csmears\u201d the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution.\n\nThe premise of this work is to notice that while the NTM emulates a differentiable version of a Turing tape, there is no particular reason that one is constrained to follow the topology of a Turing tape. Instead they propose memory stored at a set of points on a manifold and shift actions which form a Lie group. In this way, memory points can have have different relationships to one another, rather than being constrained to Z.\n\nThis is mathematically elegant and here they empirically test models with the shift group R^2 acting on R^2 and the rotation group acting on a sphere.\n\nOverall, the paper is well communicated and a novel idea.\n\nThe primary limitation of this paper is its limited impact. While this approach is certainly mathematically elegant, even likely beneficial for some specific problems where the problem structure matches the group structure, it is not clear that this significantly contributes to building models capable of more general program learning. Instead, it is likely to make an already complex and slow model such as the NTM even slower. In general, it would seem memory topology is problem specific and should therefore be learned rather than specified.\n\nThe baseline used for comparison is a very simple model, which does not even having the sharpening (the NTM approach to solving the problem of head distributions becoming \u2018smeared\u2019). There is also no comparison with the successor to the NTM, the DNC, which provides a more general approach to linking memories based on prior memory accesses.\n\nMinor issues:\nFootnote on page 3 is misleading regarding the DNC. While the linkage matrix explicitly excludes the identity, the controller can keep the head in the same position by gating the following of the link matrix.\nFigures on page 8 are difficult to follow.\n", "title": "Motivation for this InvNorm", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HywzhQGNg": {"type": "review", "replyto": "Byiy-Pqlx", "review": "You introduce InvNorm as an alternative to the softmax. Maybe I missed it but I didn't really understand the motivation behind this.The Neural Turing Machine and related \u201cexternal memory models\u201d have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures. In particular, the NTM, DNC and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position.\n\nThe NTM, which is the most relevant to this work, uses a differentiable version of a Turing machine tape. The controller outputs a kernel which \u201csoftly\u201d shifts the head, allowing the machine to read and write sequences. Since this soft shift typically \u201csmears\u201d the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution.\n\nThe premise of this work is to notice that while the NTM emulates a differentiable version of a Turing tape, there is no particular reason that one is constrained to follow the topology of a Turing tape. Instead they propose memory stored at a set of points on a manifold and shift actions which form a Lie group. In this way, memory points can have have different relationships to one another, rather than being constrained to Z.\n\nThis is mathematically elegant and here they empirically test models with the shift group R^2 acting on R^2 and the rotation group acting on a sphere.\n\nOverall, the paper is well communicated and a novel idea.\n\nThe primary limitation of this paper is its limited impact. While this approach is certainly mathematically elegant, even likely beneficial for some specific problems where the problem structure matches the group structure, it is not clear that this significantly contributes to building models capable of more general program learning. Instead, it is likely to make an already complex and slow model such as the NTM even slower. In general, it would seem memory topology is problem specific and should therefore be learned rather than specified.\n\nThe baseline used for comparison is a very simple model, which does not even having the sharpening (the NTM approach to solving the problem of head distributions becoming \u2018smeared\u2019). There is also no comparison with the successor to the NTM, the DNC, which provides a more general approach to linking memories based on prior memory accesses.\n\nMinor issues:\nFootnote on page 3 is misleading regarding the DNC. While the linkage matrix explicitly excludes the identity, the controller can keep the head in the same position by gating the following of the link matrix.\nFigures on page 8 are difficult to follow.\n", "title": "Motivation for this InvNorm", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "HkmlDMWWg": {"type": "rebuttal", "replyto": "S1Ps1PAxx", "comment": "Hi Tara,\n\nA new version has been submitted. Let us know if there are further style problems.\n\nThank you!", "title": "Resubmission"}, "S1Ps1PAxx": {"type": "rebuttal", "replyto": "Byiy-Pqlx", "comment": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct font for your submission to be considered. Thank you!", "title": "ICLR Paper Format"}}}