{"paper": {"title": "Consistency-based anomaly detection with adaptive multiple-hypotheses predictions", "authors": ["Duc Tam Nguyen", "Zhongyu Lou", "Michael Klar", "Thomas Brox"], "authorids": ["nguyen@cs.uni-freiburg.de", "zhongyu.lou@de.bosch.com", "michael.klar2@de.bosch.com", "brox@cs.uni-freiburg.de"], "summary": "We propose an anomaly-detection approach that combines modeling the foreground class via multiple local densities with adversarial training.", "abstract": "In one-class-learning tasks, only the normal case can be modeled with data, whereas the variation of all possible anomalies is too large to be described sufficiently by samples. Thus, due to the lack of representative data, the wide-spread discriminative approaches cannot cover such learning tasks, and rather generative models, which attempt to learn the input density of the normal cases, are used. However, generative models suffer from a large input dimensionality (as in images) and are typically inefficient learners. We propose to learn the data distribution more efficiently with a multi-hypotheses autoencoder. Moreover, the model is criticized by a discriminator, which prevents artificial data modes not supported by data, and which enforces diversity across hypotheses. This consistency-based anomaly detection (ConAD) framework allows the reliable identification of outof- distribution samples. For anomaly detection on CIFAR-10, it yields up to 3.9% points improvement over previously reported results. On a real anomaly detection task, the approach reduces the error of the baseline models from 6.8% to 1.5%.", "keywords": ["Anomaly detection", "outlier detection", "generative models", "VAE", "GAN"]}, "meta": {"decision": "Reject", "comment": "This paper proposes an anomaly-detection approach by augmenting VAE encoder with a network multiple hypothesis network and then using a discriminator in the decoder to select one of the hypothesis. The idea is interesting although the reviewers found the paper to be poorly written and the approach to be a bit confusing and complicated.\n\nRevisions and rebuttal have certainly helped to improve the quality of the work. However, the reviewers believe that the paper require more work before it can be accepted at ICLR. For this reason, I recommend to reject this paper in its current state.\n"}, "review": {"rkesDG3rCX": {"type": "rebuttal", "replyto": "ryxgISBF37", "comment": "6. Eq. 4: The last component should be negative since we trying to maximize the likelihood of the best hypothesis under WTA (right?).\n    - No, we minimize it.\n    - Figure (7) in the appendix shows how the training works. The combined best guess should be detected as fake from the Discriminator. In this way, the generator attempts to make the best guess closer to normal data distribution\n    - The Winner-takes-all loss is one additional likelihood-loss, L_{WTA}, which is a pixel-wise likelihood-loss. The loss only goes to the hypothesis branch which was closest to the input samples.\n\n99. Since experiments are very few (and not real-world for anomaly detection task) because of which the observations cannot be generalized.\n    - The Metal Anomaly Dataset is a real task from quality control in manufacturing. The goal is to detect anomalies like scratches and dents on a structured metal surface. \n    - On CIFAR-10, results are averaged over 10 x (one vs nine) anomaly detection tasks with multiple runs each.\n    - Our criteria for choosing the datasets are:\n        - Clean data for normal data distribution i.e. no multiple labels, known noise.\n        - Large number of anomalies available to assess the approaches' performance.\n        - Meaningful & hard tasks which have been used by other authors to allow a fair comparison.\n    - Finding a good dataset is a common problem for the semi-supervised anomaly detection community. For example, Ruff et al (ICML-18) only perform evaluation on \"artificial\" anomaly detection set (CIFAR-10). Our work however, extends the analysis to Metal Anomaly, a real anomaly detection task.\n*****Minor*******:\n5. Page 5: \"...D minimizes Eq. 3\": Should be 'maximizes' since the reference is to the log likelihood of real data (or, add a negative sign).\n9. Page 4 (minor) in two places: \"one-to-mapping\" -> \"one-to-many mapping\"\n10. Page 5 (minor): \"chap. 3\" -> \"section 3\"\n    - Thank you for catching these mistakes. We have fixed these issues and some other we could spot.\n", "title": "Main revision to clarify motivation and contribution (2)"}, "SJeEHmhH0m": {"type": "rebuttal", "replyto": "ryxgISBF37", "comment": "Thank you for your helpful comments. We highly appreciate your feedback to improve our paper.\n\n**General remarks***:\n    - To reduce the confusion about the motivation, the abstract and motivation have been rewritten and restructured.\n    - We increase the number of illustrative figures from 2 to 7 in the main paper. \n    - We update the reference to more recent related work (Ruff et al ICML 2018, as pointed out by reviewer2). Hence our improvement on CIFAR-10 over SOTA is now 3.9% points instead of 5% point.\n********Comments******************\n\n1. [...] ConAD offers a mild improvement and even degrades for the maximum number of hypothesis (i.e., 16).\n    (See 8.)\n2. ConAD can be compared with 'all other things being same'.\n    - Fig. 2 only illustrates the difference between single- and multiple-hypotheses networks. In our training, the networks are kept as similar as possible.\n    - Our ablation experiment is somewhat implicit as follows:\n        - Without the discriminator, the model becomes MHP-WTA \n        - Without multiple hypotheses, it becomes VAE-GAN\n        - Without multiple-hypotheses & discriminator, it becomes VAE\n        - Without pixel-wise likelihood-learning, anomaly detection is random (not reported). VAE-GAN training was also reported to require pixel-wise likelihood, in addition to perceptual metric from the discriminator.\n    - We extended the appendix to make the network architectures and training clearer.\n    \n    \n3. Page 2, para 2, last two lines: \"For simplicity, imagine an ... the real distribution. [...] too trivial and almost like a straw man argument.\n    - Due to its perception as a triviality, we have removed this argument and restructured the introduction to communicate our motivations ideas more clearly.\n4. [...] in real-world applications [...] most data is contaminated with anomalies [...]\n    - In many cases, the data is contaminated with anomalies. This is certainly an important challenge and belongs to unsupervised anomaly detection\n    - However, One-class-learning tasks are also very important (and frequent) in pratice. In this case, normal data can easily be collected and labeled. For these scenarios, the main effort comes from the data collection process, not the labeling. \n    - For example: consider setting up a new production line in manufacturing and setting up an anomaly detection model for quality control. Gathering many normal samples and to label them as such is feasible. However, to get one single real anomaly, one has to wait for a long time. This happens because anomalies are extremely rare  and do not need to appear right in the beginning.\n    - Another sample: Disease detection: Some diseases are extremely rare, finding the patients infected with exactly this disease is difficult and hence expensive. Contrary, data acquisition from healthy patients is quite easy and effortless.\n7. Table 1: The datasets are not real anomaly detection datasets (too high proportion of 'anomalies')[...]\n    - It is possible to reduce the number of anomaly samples to resemble realistic scenarios. However, the performance measures are then statistically less certain.  Furthermore, The Area-under-curve-score (AUROC) is a  class-frequency-normalized metric, i.e. imbalance between anomaly class & normal class is no problem.\n    - Note that the anomalies are only used for testing anomaly detection performance. They are not used for training.\n    - The Metal Anomaly dataset is a real anomaly detection set, where abnormality could occur on the surface. In contrast, CIFAR-10 serves as a benchmark to compare with other works (Deecke et al 2018)\n    \n8. Section 5.4: \"With our framework ConAD, anomaly detection performance remains competitive or better even with an increasing number of hypotheses available.\" Section 6: \"... and alleviates performance breakdown when the number of hypotheses is increased.\". The results for ConAD - {2, 4, 8, 16} are not consistently increasing.\n    - we added some comments in the experiments part & training details in the attachment\n    - In short: Too many hypotheses also capture much irrelevant noise.\n       Extreme case: When there are 255 hypotheses available: \n       The Winner-Takes-all-loss will encourage each hypothesis branch to predict a constant image with one value from [0,255]. The discriminator D as a regularizer will try to prevent this effect. That might be a reason why our ConAD has less severe performance breakdown, choosing the hyperparameter: hypotheses branches less sensitive.", "title": "Main revision to clarify motivation and contribution (1)"}, "r1gi9MhSAm": {"type": "rebuttal", "replyto": "SJxwF776hm", "comment": "4. There are many other sentences in the paper that I find difficult to understand, for example [...]:\n    - We have changed many parts of the paper to make it more clear  & simple to read.\n5. Which parts of the models make the main difference. [...] ablation experiment assessing the importance of the discriminator\n    - Our ablation experiment is somewhat implicit as follows:\n        - Without the discriminator, the model becomes MHP-WTA \n        - Without multiple hypotheses, it becomes VAE-GAN\n        - Without multiple-hypotheses & discriminator, it becomes VAE\n        - Without pixel-wise likelihood-learning, anomaly detection is random (not reported). VAE-GAN training was also reported to require pixel-wise likelihood, in addition to perceptual metric from the discriminator.\n6.MDN is not defined until Sec. 5, and doing so without giving any description about it.\n        - we added a more formal definition about MDN in the Appendix. Furthermore, we explain the idea behind MDN more deeply in the introduction.\n***Minor*** \n- WTA is used without being defined before (winner takes all)\n- L_[Hyps] is the same as L_[WTA]?\n- one-to-mapping --> one-to-one mapping?\n- Table 3 is never referred to.\n-  Is Table 5 reporting results on the Metal anomaly dataset? If so please mention it in the caption.\n-  \"Lfake itself consists of assessment for noise- (x\u02c6z\u223cN(0,1)) and data-conditioned (x\u02c6z\u223cN(\u00b5z|x,\u03a3z|x)) hypotheses and the best guess given by the WTA objective.\"\n    -Figure 7 illustrates how samples are fed into the discriminator.  Samples labeled as fake are: randomly-sampled images $\\hat{x}_{z\\sim \\mathcal{N}(0,1)}$, data reconstruction defined by individual hypotheses $\\hat{x}_{z\\sim \\mathcal{N}(\\mu_{z|x},\\Sigma_{z|x})}$, the best combination of hypotheses according to the Winner-takes-all-loss $\\hat{x}_{\\text{best\\_guess}}$.\n    \n    \n    - Thank you for pointing out. We fixed this and other points we could spot.\n", "title": "Main revision to clarify motivation and contribution (2) "}, "BJlsnMnS0Q": {"type": "rebuttal", "replyto": "SJxwF776hm", "comment": "Dear reviewer,\n\nThank you for your helpful comments. We highly appreciate your feedback to improve our paper.\n\n**General remarks***:\n    - To reduce the confusion about the motivation, the abstract and motivation have been rewritten and restructured.\n    - We increase the number of illustrative figures from 2 to 7 in the main paper. \n    - We update the reference to more recent related work (Ruff et al ICML 2018, as pointed out by reviewer2). Hence our improvement on CIFAR-10 over SOTA is now 3.9% points instead of 5% points.\n***Your comments***\n1.  Why do we need to parameterize a fixed set of hypotheses if we can generate as many outputs as we want just by sampling several times from the prior of the VAE? \n    - When we sample multiple times from the (data conditioned) prior, we indeed may get many outputs. However, they are not necessary different. In our experience, they are usually very similar and blurry.\n    - A more theoretical perspective: Given the VAE's expressive power, the latent code could capture the real data manifold rather well in theory. In that case, each sampling from the prior leads to one real and sharp image. \n        + But this requires a powerful encoder and/or latent code learning . Recent advances in generative modeling addresses better factorization of latent code [zhao,2017], [Rezende,2015]. Although meaningful, the models are (1) more complex (2) could lead to more data-hungry approaches.\n        + Further, learning from a pixel-wise metric such as likelihood often leads to a blurry images [Ledig -2016]  Longer training helps for training data but reduces likelihood on unseen data. Further, we consider  the blurriness as an expression of model uncertainty. When  reconstructed region is blurry, it means that the network has to regress between different modes (Multi-modality). \n        + Alternative are learning from perceptual metrics  such as GAN and VAE-GAN. But in these approaches, data reconstructions are also very different from input images. see reconstruction of inputs in [Dosovitskiy - 2016, Larsen -2015]\n    ---> a fixed set of hypotheses is an easy way for the network to address multiple modes in the datas pace. In this sense, it's a structured way to express the ambiguity from the latent code to image space.\n2. The paper is difficult to read: the motivation is not well explained [...] \n    - A hopefully more clear introduction is in the paper. \n    - In short: \n        - Autoencoder-based approaches often produce blurry reconstructions and falsify the reconstruction errors for anomaly detection. Due to likelihood-learning, they regress to the means in the data space.\n        - Better latent code factorization or perceptual metric learning could provide some help but are (1) complex to implement and (2) more data intensive. \n        - We propose a different solution: Let the network express the uncertainty directly with multiple hypotheses. Hence, instead of mean regressions, the hypotheses clusters should be spread out to cover dense data regions.\n\n3. The approach seems to build on top of Breunig et al. (2000), unfortunately, this paper is not well described, e.g. what does it mean global neighborhood?\n    - Local-outlier-factor computes an outlier score based on its intermediate neighborhood.  \n    - It is proportional to: (mean densities of the neighboring points)/(local density around the new point). Higher means more abnormal. So Local-outlier-factor only concentrates on a small neighborhood. \n    - By global neighborhood, we mean that anomaly score computation depends on ALL points in the training data, as in case of a Mixture Density Network, where a Gaussian Mixture is estimated in the image space. When the likelihood is used for anomaly score computation, it's somewhat similar to using weighted distances to ALL points available.\n\n", "title": "Main revision to clarify motivation and contribution (1) "}, "H1xmVZ2HCQ": {"type": "rebuttal", "replyto": "HyeByKbBnm", "comment": "Thank you for your helpful comments. We highly appreciate your feedback to improve our paper.\n\n**General remarks***:\n    - To reduce the confusion about the motivation, the abstract and motivation have been rewritten and restructured.\n    - We increase the number of illustrative figures from 2 to 7 in the main paper. \n    - We update the reference to more recent related work (Ruff et al ICML-2018). Hence Our improvement on CIFAR-10 over SOTA is now 3.9% points instead of 5% point.\n********Comments******************\n\nConfusing paper, some good ideas\n\n1. The details are unclear (the paper is poorly written and lacks some detailed explainations)\n    - We added details for training & architecture in the appendix.\n    - The motivation in the introduction is rewritten and hopefully becomes  clearer. \n2. I am assuming that for each hypothesis (ie region of the space) different en- and decoder parameters are learned (sharing the same variational prior??). \n    - Details are shown in an additional figure (7). \n    - The multiple-hypotheses network shares all layers but the last layer. That means, only the decoder part is changed, to have multiple heads.\n    \n3. An adversarial loss is added to the model (how that is done is not described; the relevant equation (5) uses L_hyp which is not defined) to avoid the mode collapse.\n    - L_{hyps} is a typo, it should be L_{WTA}\n    - How this is learned is described in Figure 7: ConAD: our multiple-hypotheses autoencoder and with the training discriminator train\n    - In short: \n        We feed the fake images to the discriminator D, consisting of 4 batches: \n            - real  32 real images\n            - fake: 32 random hypotheses from image reconstructions hypotheses branches \n            - fake: 32 best-combined (based on  winner hypotheses) reconstructions \n            - fake: 32 random sampled images from  latent prior N(0,1)\n        \n4. The idea of using the adv loss for avoiding mode collapse can be useful in other settings.\n    - We also expect it to be beneficial in normal use-cases of MHP-learning.  Instead of designing some additional losses to make the hypotheses  meaningful, the work can be shifted to the discriminator.\n\n5. [...] compare also again against more related work, e.g., Ruff et al ICML 2018).\n    - Thank you for pointing this out. We have added their results for comparison.\n\n99. [...] not well written and lacks some details. \n    - we have worked on the rewriting to make the motivation and idea clearer. \n    Details are added in the paper as well as attachment. ", "title": "Main revision to clarify motivation and contribution"}, "SJxwF776hm": {"type": "review", "replyto": "r1ledo0ctX", "review": "This paper proposes an anomaly detection system by proposing the combination of multiple-hypotheses approach with variational autoencoders, and using a discriminator to prevent either head of the model to produce modes that are not part of the data.\n\nThe combination between multiple-hypotheses approach with variational autoencoders seems rather artificial to me. Why do we need to parameterized a fixed set of hypothesis if we can generate as many outputs as we want just by sample several times from the prior of the VAE? Maybe I am missing something, which brings me to the following point.\n\nThe paper is difficult to read: the motivation is not well explained, the link between anomaly detection and multiple-hypothesis methods (both in the title of the paper) is not clear. The approach seems to build on top of Breunig et al. (2000), unfortunately this paper is not well described, e.g. what does it mean global neighborhood?\nThere are many other sentences in the paper that I find difficult to understand, for example:\n\"Lfake itself consists of assessment for noise- (x\u02c6z\u223cN(0,1)) and data-conditioned (x\u02c6z\u223cN(\u00b5z|x,\u03a3z|x)) hypotheses and the best guess given by the WTA objective.\"\n\nOn top of that there are many other elements in the paper hampering the comprehension of the reader. For example:\nWTA is used without being defined before (winner takes all)\none-to-mapping --> one-to-one mapping?\nL_[Hyps] is the same as L_[WTA]?\nMDN is not defined until Sec. 5, and doing so without giving any description about it.\nTable 3 is never referred to.\nIs Table 5 reporting results on the Metal anomaly dataset? If so please mention it in the caption.\n\nIn the experiments it is difficult to see which parts of the models make the main difference. For example, it would be interesting to have an ablation experiment assessing the importance of the discriminator.", "title": "Does not read well, no clear motivation", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "ryxgISBF37": {"type": "review", "replyto": "r1ledo0ctX", "review": "Summary\n-------\nThe paper proposes a technique to make generative models more robust by making them consistent with the local density. It is hypothesized that robust model will be able to detect out-of-distribution samples better and improve anomaly detection.\n\nMain comments\n-------------\n\n1. The proposed technique adds additional regularizers to the GAN loss that, in effect, state that the best hypothesis under a WTA strategy should have a high likelihood under the discriminator 'D'. This is an interesting idea and certainly a reasonable thing to try. As stated in the abstract, the generative models are inefficient; it is likely that additional structure enforced by the regularizer helps in improving the efficiency.\n\n2. The objective in GANs is to infer the underlying distribution correctly and so far it has been found that their accuracy is heavily dependent on both the architecture as well as the computational complexity (they may improve with more training, but maybe not consistently). Therefore, it becomes hard to compare the three architectures in Figure 2 since they are all different. A more rigorous comparison would try to keep as many pieces of the architecture the same as possible so that ConAD can be compared with 'all other things being same'. Some experiments seem to follow this idea such as 'MDN+ConAD-{2, 4, 8, 16}' in Table 2. But in these experiments the addition of ConAD offers a mild improvement and even degrades for the maximum number of hypothesis (i.e., 16).\n\n3. Page 2, para 2, last two lines: \"For simplicity, imagine an ... the real distribution.\"\n\nThe argument is not clear. It seems too trivial and almost like a straw man argument.\n\n4. Page 4: \"In anomaly detection, this is difficult since there is no anomalous data point contained in the training dataset.\"\n\nThis is not true in real-world applications where most data is contaminated with anomalies. This is part of the challenge in anomaly detection.\n\nThe above also applies to the following on page 6: \"During model training, only data from the normal data class is used...\"\n\n5. Page 5: \"...D minimizes Eq. 3\": Should be 'maximizes' since the reference is to the log likelihood of real data (or, add a negative sign).\n\n6. Eq. 4: The last component should be negative since we trying to maximize the likelihood of the best hypothesis under WTA (right?).\n\n7. Table 1: The datasets are not real anomaly detection datasets (too high proportion of 'anomalies') Moreover, the number of datasets is insufficient for rigor.\n\n8. Section 5.4: \"With our framework ConAD, anomaly detection performance remains competitive or better even with an increasing number of hypotheses available.\"\n\nSection 6: \"... and alleviates performance breakdown when the number of hypotheses is increased.\"\n\nThis is not entirely supported by the results in Tables 2, 3, and also 4 and 5 of supplement. The results for ConAD - {2, 4, 8, 16} are not consistently increasing.\n\nSince experiments are very few (and not real-world for anomaly detection task) because of which the observations cannot be generalized.\n\n9. Page 4 (minor) in two places: \"one-to-mapping\" -> \"one-to-many mapping\"\n\n10. Page 5 (minor): \"chap. 3\" -> \"section 3\"\n", "title": "Technique to make GANs more robust by adding MHP-based regularization so that detection of out-of-distribution samples can be improved", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "HyeByKbBnm": {"type": "review", "replyto": "r1ledo0ctX", "review": "The paper proposes a new method for anomaly detection using deep learning. It works as follows. \n\nThe method is based on the recent Multiple-Hypotheses predictions (MHP) model, the impact of which is yet unclear/questionable. The idea in MHP is to represent the data using multiple models. Depending on the part of the space where an instance falls, a different model is active. In this paper this is realized using VAEs. The details are unclear (the paper is poorly written and lacks some detailed explainations), but I am assuming that for each hypothesis (ie region of the space) different en- and decoder parameters are learned (sharing the same variational prior??). The authors mention that below this final layer all hypothesis share the same network parameters. An adversarial loss is added to the model (how that is done is not described; the relevant equation (5) uses L_hyp which is not defined) to avoid the mode collapse.\n\nWhat is interesting about the paper:\n- First of all, pushing the the MHP framework towards AD could be relevant by its own right for a very small subcommunity that is interested in this method\n- The idea of using the adv loss for avoiding mode collapse can be useful in other settings; this is def a that I learned from the paper\n- The method might actually work rather well in practice\n\nVotum. As outlined above, the paper makes some rather interesting points, but is not well written and lacks some details. I am not entirely convinced that AD and MHP is a killer combination, but the experimental results are ok, nothing to complain here (except the usual bla: make it larger, more, etc), but honestly they really fine (maybe compare also again against more related work, e.g., Ruff et al ICML 2018).", "title": "Confusing paper, some good ideas", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}}}