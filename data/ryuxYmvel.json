{"paper": {"title": "HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving", "authors": ["Cezary Kaliszyk", "Fran\u00e7ois Chollet", "Christian Szegedy"], "authorids": ["cezary.kaliszyk@uibk.ac.at", "fchollet@google.com", "szegedy@google.com"], "summary": "", "abstract": "Large computer-understandable proofs consist of millions of intermediate\nlogical steps. The vast majority of such steps originate from manually\nselected and manually guided heuristics applied to intermediate goals.\nSo far, machine learning has generally not been used to filter or\ngenerate these steps. In this paper, we introduce a new dataset based on\nHigher-Order Logic (HOL) proofs, for the purpose of developing new\nmachine learning-based theorem-proving strategies. We make this dataset\npublicly available under the BSD license. We propose various machine\nlearning tasks that can be performed on this dataset, and discuss their\nsignificance for theorem proving. We also benchmark a set of simple baseline\nmachine learning models suited for the tasks (including logistic regression\nconvolutional neural networks and recurrent neural networks). The results of our\nbaseline models show the promise of applying machine learning to HOL\ntheorem proving.\n", "keywords": []}, "meta": {"decision": "Accept (Poster)", "comment": "The paper presents a new dataset and initial machine-learning results for an interesting problem, namely, higher-order logic theorem proving. This dataset is of great potential value in the development of deep-learning approaches for (mathematical) reasoning.\n \n As a personal side note: It would be great if the camera-ready version of the paper would provide somewhat more context on how the state-of-the-art approaches in automatic theorem proving perform on the conjectures in HolStep. Also, it would be good to clarify how the dataset makes sure there is no \"overlap\" between the training and test set: for instance, a typical proof of the Cauchy-Schwarz inequality employs the Pythagorean theorem: how can we be sure that we don't have Cauchy-Schwarz in the training set and Pythagoras in the test set?"}, "review": {"SJt8tlI8l": {"type": "rebuttal", "replyto": "BkeTwcZVx", "comment": "We thank the reviewer for the insightful comments.\n\nIndeed the dataset is smaller than that of AlphaGo, as it has been created as a benchmark for machine learning technologies applied to higher-order logic. The proposed approach can be directly applied to other HOL proof corpora to create a much larger training dataset, which can be used to practically guide ATPs. We imagine that already with the current accuracy integrating the prediction of usefulness of a statement could help modern ATPs. And the difference could become more signigicant with further improvements to the proposed baselines.\n", "title": "Answer to Review 2"}, "BJiUOeIUx": {"type": "rebuttal", "replyto": "Hkl86ls4e", "comment": "We thank the reviewer for the insightful comments. We have taken all into account and attempted to adapt the paper accordingly. In particular:\n\nWe rewrote section 2 explaining and giving citations to the theorem proving terms and eliminating the unnecessary ones.\n\nIndeed, the training and test sets concern different conjectures. This is for the machine learning task to correspond to the theorem proving task. In the latter, learning from parts of proofs of a conjecture is only possible after it has been completely proved. This is now explained in the paper.\n\nThe global max pooling does reduce the sequence of vectors corresponding to the characters to a single vector. We have made this clearer in section 4.1.\n\nAn initial analysis of success and failure cases suggests that the design of future models should attempt to leverage the graph structure of HOL statements, for example by considering recursive graph models and generative models.\n\nFinally, deep learning models are compared to relatively simpler logistic regression baseline. The difference, as visible in Tables 2-3 and Figures 3-6 is significant.\n", "title": "Answer to Review 3"}, "HyNiqdHme": {"type": "rebuttal", "replyto": "SJgTvB4Xg", "comment": "Using machine learning to predict branches is already useful in\ntableaux based first-order automated provers. The HOL proofs have\na much higher branching factor and inferences are slowe, so being\nbeing able to predict branches more accurately will help more there.\n\nThe bias from learning on human proofs is to an extent present in\nthe proposed dataset. It is possible to extend the dataset to\ninclude the steps that are derived by automated proof search (we\nsuggested in future work to possibly include steps found by the\nmodel elimination automated procedure).\n\nFinally, indeed many of the proofs we consider are proofs that\nhumans find easy. ATPs are at the moment very week in comparison\nwith human intuition, and cases where they find proofs that a\nhuman cannot find quickly are rare. Therefore improving the quality\nof ATPs even for the many proofs humans do not find difficult would\nbe an important gain.\n\nWe have modified the conclusion and future work parts of the paper\nto include these remarks.\n", "title": "Re: When is this applicable?"}, "rJJjwOBml": {"type": "rebuttal", "replyto": "B1RMgGkXg", "comment": "We have now included the scripts and code which we used to generate the dataset, as well as the code used to compute the baseline models. This is also now mentioned in the paper.\n\nThe dataset generation approach could easily generalize to LCF-style interactive theorem provers. For ATPs, extraction from a dataset such as TPTP might conceptually be simpler. However, the data is typically less consistent across problems, so further techniques for matching concepts would likely be necessary to make the predictions meaningful.", "title": "Re: The dataset extraction method"}, "SJgTvB4Xg": {"type": "review", "replyto": "ryuxYmvel", "review": "Thanks for the interesting paper. In your results you report proof step classification accuracy. As in AlphaGo this reflects the ability of your models in mimicking the bias in proof structure of proofs that humans discovered. Can you comment whether this bias will affect an automated theorem prover if these models are used as subroutines? Moreover, it's possible that the class of proofs which we find difficult are exactly the proofs for which this method won't help. Thanks!The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data (in this case the sequence of subproofs) is a good launching point for possibly super-human performance. Super-human ATPs are clearly extremely valuable. Although relatively smaller than the original Go datasets, this dataset seems to be a great first step. Unfortunately, the ATP and HOL aspect of this work is not my area of expertise. I can't comment on the quality of this aspect.\n\nIt would be great to see future work scale up the baselines and integrate the networks into state of the art ATPs. The capacity of deep learning methods to scale and take advantage of larger datasets means there's a possibility of an iterative approach to improving ATPs: as the ATPs get stronger they may generate more data in the form of new theorems. This may be a long way off, but the possibility is exciting.", "title": "When is this applicable?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "BkeTwcZVx": {"type": "review", "replyto": "ryuxYmvel", "review": "Thanks for the interesting paper. In your results you report proof step classification accuracy. As in AlphaGo this reflects the ability of your models in mimicking the bias in proof structure of proofs that humans discovered. Can you comment whether this bias will affect an automated theorem prover if these models are used as subroutines? Moreover, it's possible that the class of proofs which we find difficult are exactly the proofs for which this method won't help. Thanks!The authors describe a dataset of proof steps in higher order logic derived from a set of proven theorems. The success of methods like AlphaGo suggests that for hard combinatorial style problems, having a curated set of expert data (in this case the sequence of subproofs) is a good launching point for possibly super-human performance. Super-human ATPs are clearly extremely valuable. Although relatively smaller than the original Go datasets, this dataset seems to be a great first step. Unfortunately, the ATP and HOL aspect of this work is not my area of expertise. I can't comment on the quality of this aspect.\n\nIt would be great to see future work scale up the baselines and integrate the networks into state of the art ATPs. The capacity of deep learning methods to scale and take advantage of larger datasets means there's a possibility of an iterative approach to improving ATPs: as the ATPs get stronger they may generate more data in the form of new theorems. This may be a long way off, but the possibility is exciting.", "title": "When is this applicable?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "B1RMgGkXg": {"type": "review", "replyto": "ryuxYmvel", "review": "In Section 2, you describe the extraction process for the dataset. It becomes clear that it was not a trivial undertaking and probably even more subtleties had to be taken into account to generate the final dataset. Could the same strategy/code be applied directly to other proofs?\n\nIn the conclusion, you state that other ITPs or ATPs could be targeted in future work, whereas it could be interesting to integrate even more proofs into the existing dataset. Are you planning to disclose your code for dataset generation?The authors present a dataset extraction method, dataset and first interesting results for machine-learning supported higher order logic theorem proving. The experimental results are impressively good for a first baseline and with an accuracy higher than 0.83 in relevance classification a lot better than chance, and encourage future research in this direction. The paper is well-written in terms of presentation and argumentation and leaves little room for criticism. The related work seems to be well-covered, though I have to note that I am not an expert for automated theorem proving.", "title": "The dataset extraction method", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "rkB8kiWNl": {"type": "review", "replyto": "ryuxYmvel", "review": "In Section 2, you describe the extraction process for the dataset. It becomes clear that it was not a trivial undertaking and probably even more subtleties had to be taken into account to generate the final dataset. Could the same strategy/code be applied directly to other proofs?\n\nIn the conclusion, you state that other ITPs or ATPs could be targeted in future work, whereas it could be interesting to integrate even more proofs into the existing dataset. Are you planning to disclose your code for dataset generation?The authors present a dataset extraction method, dataset and first interesting results for machine-learning supported higher order logic theorem proving. The experimental results are impressively good for a first baseline and with an accuracy higher than 0.83 in relevance classification a lot better than chance, and encourage future research in this direction. The paper is well-written in terms of presentation and argumentation and leaves little room for criticism. The related work seems to be well-covered, though I have to note that I am not an expert for automated theorem proving.", "title": "The dataset extraction method", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}}}